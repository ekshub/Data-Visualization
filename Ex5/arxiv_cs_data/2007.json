[
    {
        "title": "Attribute Value Weighting in K-Modes Clustering",
        "authors": [
            "Zengyou He",
            "Xaiofei Xu",
            "Shengchun Deng"
        ],
        "summary": "In this paper, the traditional k-modes clustering algorithm is extended by weighting attribute value matches in dissimilarity computation. The use of attribute value weighting technique makes it possible to generate clusters with stronger intra-similarities, and therefore achieve better clustering performance. Experimental results on real life datasets show that these value weighting based k-modes algorithms are superior to the standard k-modes algorithm with respect to clustering accuracy.",
        "published": "2007-01-03T09:06:03Z",
        "link": "http://arxiv.org/abs/cs/0701013v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Attribute Exploration of Discrete Temporal Transitions",
        "authors": [
            "Johannes Wollbold"
        ],
        "summary": "Discrete temporal transitions occur in a variety of domains, but this work is mainly motivated by applications in molecular biology: explaining and analyzing observed transcriptome and proteome time series by literature and database knowledge. The starting point of a formal concept analysis model is presented. The objects of a formal context are states of the interesting entities, and the attributes are the variable properties defining the current state (e.g. observed presence or absence of proteins). Temporal transitions assign a relation to the objects, defined by deterministic or non-deterministic transition rules between sets of pre- and postconditions. This relation can be generalized to its transitive closure, i.e. states are related if one results from the other by a transition sequence of arbitrary length. The focus of the work is the adaptation of the attribute exploration algorithm to such a relational context, so that questions concerning temporal dependencies can be asked during the exploration process and be answered from the computed stem base. Results are given for the abstract example of a game and a small gene regulatory network relevant to a biomedical question.",
        "published": "2007-01-04T14:10:51Z",
        "link": "http://arxiv.org/abs/q-bio/0701009v2",
        "categories": [
            "q-bio.QM",
            "cs.AI",
            "q-bio.MN"
        ]
    },
    {
        "title": "On the Complexity of the Numerically Definite Syllogistic and Related   Fragments",
        "authors": [
            "Ian Pratt-Hartmann"
        ],
        "summary": "In this paper, we determine the complexity of the satisfiability problem for various logics obtained by adding numerical quantifiers, and other constructions, to the traditional syllogistic. In addition, we demonstrate the incompleteness of some recently proposed proof-systems for these logics.",
        "published": "2007-01-06T15:00:00Z",
        "link": "http://arxiv.org/abs/cs/0701039v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Cooperative Optimization for Energy Minimization: A Case Study of Stereo   Matching",
        "authors": [
            "Xiaofei Huang"
        ],
        "summary": "Often times, individuals working together as a team can solve hard problems beyond the capability of any individual in the team. Cooperative optimization is a newly proposed general method for attacking hard optimization problems inspired by cooperation principles in team playing. It has an established theoretical foundation and has demonstrated outstanding performances in solving real-world optimization problems. With some general settings, a cooperative optimization algorithm has a unique equilibrium and converges to it with an exponential rate regardless initial conditions and insensitive to perturbations. It also possesses a number of global optimality conditions for identifying global optima so that it can terminate its search process efficiently. This paper offers a general description of cooperative optimization, addresses a number of design issues, and presents a case study to demonstrate its power.",
        "published": "2007-01-09T01:03:25Z",
        "link": "http://arxiv.org/abs/cs/0701057v1",
        "categories": [
            "cs.CV",
            "cs.AI"
        ]
    },
    {
        "title": "A Backtracking-Based Algorithm for Computing Hypertree-Decompositions",
        "authors": [
            "Georg Gottlob",
            "Marko Samer"
        ],
        "summary": "Hypertree decompositions of hypergraphs are a generalization of tree decompositions of graphs. The corresponding hypertree-width is a measure for the cyclicity and therefore tractability of the encoded computation problem. Many NP-hard decision and computation problems are known to be tractable on instances whose structure corresponds to hypergraphs of bounded hypertree-width. Intuitively, the smaller the hypertree-width, the faster the computation problem can be solved. In this paper, we present the new backtracking-based algorithm det-k-decomp for computing hypertree decompositions of small width. Our benchmark evaluations have shown that det-k-decomp significantly outperforms opt-k-decomp, the only exact hypertree decomposition algorithm so far. Even compared to the best heuristic algorithm, we obtained competitive results as long as the hypergraphs are not too large.",
        "published": "2007-01-14T01:14:25Z",
        "link": "http://arxiv.org/abs/cs/0701083v1",
        "categories": [
            "cs.DS",
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "Propositional theories are strongly equivalent to logic programs",
        "authors": [
            "Pedro Cabalar",
            "Paolo Ferraris"
        ],
        "summary": "This paper presents a property of propositional theories under the answer sets semantics (called Equilibrium Logic for this general syntax): any theory can always be reexpressed as a strongly equivalent disjunctive logic program, possibly with negation in the head. We provide two different proofs for this result: one involving a syntactic transformation, and one that constructs a program starting from the countermodels of the theory in the intermediate logic of here-and-there.",
        "published": "2007-01-16T12:29:55Z",
        "link": "http://arxiv.org/abs/cs/0701095v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Algorithmic Complexity Bounds on Future Prediction Errors",
        "authors": [
            "A. Chernov",
            "M. Hutter",
            "J. Schmidhuber"
        ],
        "summary": "We bound the future loss when predicting any (computably) stochastic sequence online. Solomonoff finitely bounded the total deviation of his universal predictor $M$ from the true distribution $mu$ by the algorithmic complexity of $mu$. Here we assume we are at a time $t>1$ and already observed $x=x_1...x_t$. We bound the future prediction performance on $x_{t+1}x_{t+2}...$ by a new variant of algorithmic complexity of $mu$ given $x$, plus the complexity of the randomness deficiency of $x$. The new complexity is monotone in its condition in the sense that this complexity can only decrease if the condition is prolonged. We also briefly discuss potential generalizations to Bayesian model classes and to classification problems.",
        "published": "2007-01-19T07:36:40Z",
        "link": "http://arxiv.org/abs/cs/0701120v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Universal Algorithmic Intelligence: A mathematical top->down approach",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Sequential decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameter-free theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline how the AIXI model can formally solve a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXItl that is still effectively more intelligent than any other time t and length l bounded agent. The computation time of AIXItl is of the order t x 2^l. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches.",
        "published": "2007-01-20T00:18:06Z",
        "link": "http://arxiv.org/abs/cs/0701125v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "A novel set of rotationally and translationally invariant features for   images based on the non-commutative bispectrum",
        "authors": [
            "Risi Kondor"
        ],
        "summary": "We propose a new set of rotationally and translationally invariant features for image or pattern recognition and classification. The new features are cubic polynomials in the pixel intensities and provide a richer representation of the original image than most existing systems of invariants. Our construction is based on the generalization of the concept of bispectrum to the three-dimensional rotation group SO(3), and a projection of the image onto the sphere.",
        "published": "2007-01-20T15:45:03Z",
        "link": "http://arxiv.org/abs/cs/0701127v3",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.4.7; I.2.10; I.5.4"
        ]
    },
    {
        "title": "Time and the Prisoner's Dilemma",
        "authors": [
            "Yishay Mor",
            "Jeffrey S. Rosenschein"
        ],
        "summary": "This paper examines the integration of computational complexity into game theoretic models. The example focused on is the Prisoner's Dilemma, repeated for a finite length of time. We show that a minimal bound on the players' computational ability is sufficient to enable cooperative behavior.   In addition, a variant of the repeated Prisoner's Dilemma game is suggested, in which players have the choice of opting out. This modification enriches the game and suggests dominance of cooperative strategies.   Competitive analysis is suggested as a tool for investigating sub-optimal (but computationally tractable) strategies and game theoretic models in general. Using competitive analysis, it is shown that for bounded players, a sub-optimal strategy might be the optimal choice, given resource limitations.",
        "published": "2007-01-22T15:23:49Z",
        "link": "http://arxiv.org/abs/cs/0701139v1",
        "categories": [
            "cs.GT",
            "cs.AI"
        ]
    },
    {
        "title": "A Prototype for Educational Planning Using Course Constraints to   Simulate Student Populations",
        "authors": [
            "T. Hadzilacos",
            "D. Kalles",
            "D. Koumanakos",
            "V. Mitsionis"
        ],
        "summary": "Distance learning universities usually afford their students the flexibility to advance their studies at their own pace. This can lead to a considerable fluctuation of student populations within a program's courses, possibly affecting the academic viability of a program as well as the related required resources. Providing a method that estimates this population could be of substantial help to university management and academic personnel. We describe how to use course precedence constraints to calculate alternative tuition paths and then use Markov models to estimate future populations. In doing so, we identify key issues of a large scale potential deployment.",
        "published": "2007-01-26T08:32:10Z",
        "link": "http://arxiv.org/abs/cs/0701174v3",
        "categories": [
            "cs.AI",
            "cs.CY",
            "cs.DS",
            "cs.SC"
        ]
    },
    {
        "title": "Structure and Problem Hardness: Goal Asymmetry and DPLL Proofs in<br>   SAT-Based Planning",
        "authors": [
            "Joerg Hoffmann",
            "Carla Gomes",
            "Bart Selman"
        ],
        "summary": "In Verification and in (optimal) AI Planning, a successful method is to formulate the application as boolean satisfiability (SAT), and solve it with state-of-the-art DPLL-based procedures. There is a lack of understanding of why this works so well. Focussing on the Planning context, we identify a form of problem structure concerned with the symmetrical or asymmetrical nature of the cost of achieving the individual planning goals. We quantify this sort of structure with a simple numeric parameter called AsymRatio, ranging between 0 and 1. We run experiments in 10 benchmark domains from the International Planning Competitions since 2000; we show that AsymRatio is a good indicator of SAT solver performance in 8 of these domains. We then examine carefully crafted synthetic planning domains that allow control of the amount of structure, and that are clean enough for a rigorous analysis of the combinatorial search space. The domains are parameterized by size, and by the amount of structure. The CNFs we examine are unsatisfiable, encoding one planning step less than the length of the optimal plan. We prove upper and lower bounds on the size of the best possible DPLL refutations, under different settings of the amount of structure, as a function of size. We also identify the best possible sets of branching variables (backdoors). With minimum AsymRatio, we prove exponential lower bounds, and identify minimal backdoors of size linear in the number of variables. With maximum AsymRatio, we identify logarithmic DPLL refutations (and backdoors), showing a doubly exponential gap between the two structural extreme cases. The reasons for this behavior -- the proof arguments -- illuminate the prototypical patterns of structure causing the empirical behavior observed in the competition benchmarks.",
        "published": "2007-01-29T12:47:08Z",
        "link": "http://arxiv.org/abs/cs/0701184v2",
        "categories": [
            "cs.AI",
            "I.2.8; I.2.3"
        ]
    },
    {
        "title": "Dealing With Logical Omniscience: Expressiveness and Pragmatics",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "summary": "We examine four approaches for dealing with the logical omniscience problem and their potential applicability: the syntactic approach, awareness, algorithmic knowledge, and impossible possible worlds. Although in some settings these approaches are equi-expressive and can capture all epistemic states, in other settings of interest (especially with probability in the picture), we show that they are not equi-expressive. We then consider the pragmatics of dealing with logical omniscience-- how to choose an approach and construct an appropriate model.",
        "published": "2007-02-01T20:06:31Z",
        "link": "http://arxiv.org/abs/cs/0702011v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Uniform and Partially Uniform Redistribution Rules",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "summary": "This short paper introduces two new fusion rules for combining quantitative basic belief assignments. These rules although very simple have not been proposed in literature so far and could serve as useful alternatives because of their low computation cost with respect to the recent advanced Proportional Conflict Redistribution rules developed in the DSmT framework.",
        "published": "2007-02-05T14:56:49Z",
        "link": "http://arxiv.org/abs/cs/0702028v2",
        "categories": [
            "cs.AI",
            "I.4.8"
        ]
    },
    {
        "title": "Markovian Entanglement Networks",
        "authors": [
            "Pierfrancesco La Mura",
            "Lukasz Swiatczak"
        ],
        "summary": "Graphical models of probabilistic dependencies have been extensively investigated in the context of classical uncertainty. However, in some domains (most notably, in computational physics and quantum computing) the nature of the relevant uncertainty is non-classical, and the laws of classical probability theory are superseded by those of quantum mechanics. In this paper we introduce Markovian Entanglement Networks (MEN), a novel class of graphical representations of quantum-mechanical dependencies in the context of such non-classical systems. MEN are the quantum-mechanical analogue of Markovian Networks, a family of undirected graphical representations which, in the classical domain, exploit a notion of conditional independence among subsystems.   After defining a notion of conditional independence appropriate to our domain (conditional separability), we prove that the conditional separabilities induced by a quantum-mechanical wave function are effectively reflected in the graphical structure of MEN. Specifically, we show that for any wave function there exists a MEN which is a perfect map of its conditional separabilities. Next, we show how the graphical structure of MEN can be used to effectively classify the pure states of three-qubit systems. We also demonstrate that, in large systems, exploiting conditional independencies may dramatically reduce the computational burden of various inference tasks. In principle, the graph-theoretic representation of conditional independencies afforded by MEN may not only facilitate the classical simulation of quantum systems, but also provide a guide to the efficient design and complexity analysis of quantum algorithms and circuits.",
        "published": "2007-02-07T18:39:42Z",
        "link": "http://arxiv.org/abs/quant-ph/0702072v1",
        "categories": [
            "quant-ph",
            "cs.AI"
        ]
    },
    {
        "title": "Logic Programming with Satisfiability",
        "authors": [
            "Michael Codish",
            "Vitaly Lagoon",
            "Peter J. Stuckey"
        ],
        "summary": "This paper presents a Prolog interface to the MiniSat satisfiability solver. Logic program- ming with satisfiability combines the strengths of the two paradigms: logic programming for encoding search problems into satisfiability on the one hand and efficient SAT solving on the other. This synergy between these two exposes a programming paradigm which we propose here as a logic programming pearl. To illustrate logic programming with SAT solving we give an example Prolog program which solves instances of Partial MAXSAT.",
        "published": "2007-02-13T03:10:02Z",
        "link": "http://arxiv.org/abs/cs/0702072v1",
        "categories": [
            "cs.PL",
            "cs.AI"
        ]
    },
    {
        "title": "Invariant template matching in systems with spatiotemporal coding: a   vote for instability",
        "authors": [
            "Ivan Tyukin",
            "Tatiana Tyukina",
            "Cees van Leeuwen"
        ],
        "summary": "We consider the design of a pattern recognition that matches templates to images, both of which are spatially sampled and encoded as temporal sequences. The image is subject to a combination of various perturbations. These include ones that can be modeled as parameterized uncertainties such as image blur, luminance, translation, and rotation as well as unmodeled ones. Biological and neural systems require that these perturbations be processed through a minimal number of channels by simple adaptation mechanisms. We found that the most suitable mathematical framework to meet this requirement is that of weakly attracting sets. This framework provides us with a normative and unifying solution to the pattern recognition problem. We analyze the consequences of its explicit implementation in neural systems. Several properties inherent to the systems designed in accordance with our normative mathematical argument coincide with known empirical facts. This is illustrated in mental rotation, visual search and blur/intensity adaptation. We demonstrate how our results can be applied to a range of practical problems in template matching and pattern recognition.",
        "published": "2007-02-14T07:01:56Z",
        "link": "http://arxiv.org/abs/cs/0702082v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "G.1.6; G.1.7; I.2.0; I.2.8; I.2.10; I.4.4; I.5.0; I.5.2"
        ]
    },
    {
        "title": "Overcoming Hierarchical Difficulty by Hill-Climbing the Building Block   Structure",
        "authors": [
            "David Iclanzan",
            "Dan Dumitrescu"
        ],
        "summary": "The Building Block Hypothesis suggests that Genetic Algorithms (GAs) are well-suited for hierarchical problems, where efficient solving requires proper problem decomposition and assembly of solution from sub-solution with strong non-linear interdependencies. The paper proposes a hill-climber operating over the building block (BB) space that can efficiently address hierarchical problems. The new Building Block Hill-Climber (BBHC) uses past hill-climb experience to extract BB information and adapts its neighborhood structure accordingly. The perpetual adaptation of the neighborhood structure allows the method to climb the hierarchical structure solving successively the hierarchical levels. It is expected that for fully non deceptive hierarchical BB structures the BBHC can solve hierarchical problems in linearithmic time. Empirical results confirm that the proposed method scales almost linearly with the problem size thus clearly outperforms population based recombinative methods.",
        "published": "2007-02-16T21:47:04Z",
        "link": "http://arxiv.org/abs/cs/0702096v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "G.1.6; I.2.8"
        ]
    },
    {
        "title": "Slope One Predictors for Online Rating-Based Collaborative Filtering",
        "authors": [
            "Daniel Lemire",
            "Anna Maclachlan"
        ],
        "summary": "Rating-based collaborative filtering is the process of predicting how a user would rate a given item from other user ratings. We propose three related slope one schemes with predictors of the form f(x) = x + b, which precompute the average difference between the ratings of one item and another for users who rated both. Slope one algorithms are easy to implement, efficient to query, reasonably accurate, and they support both online queries and dynamic updates, which makes them good candidates for real-world systems. The basic slope one scheme is suggested as a new reference scheme for collaborative filtering. By factoring in items that a user liked separately from items that a user disliked, we achieve results competitive with slower memory-based schemes over the standard benchmark EachMovie and Movielens data sets while better fulfilling the desiderata of CF applications.",
        "published": "2007-02-24T03:16:27Z",
        "link": "http://arxiv.org/abs/cs/0702144v2",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Coupling Control and Human-Centered Automation in Mathematical Models of   Complex Systems",
        "authors": [
            "Roderick V. N. Melnik"
        ],
        "summary": "In this paper we analyze mathematically how human factors can be effectively incorporated into the analysis and control of complex systems. As an example, we focus our discussion around one of the key problems in the Intelligent Transportation Systems (ITS) theory and practice, the problem of speed control, considered here as a decision making process with limited information available. The problem is cast mathematically in the general framework of control problems and is treated in the context of dynamically changing environments where control is coupled to human-centered automation. Since in this case control might not be limited to a small number of control settings, as it is often assumed in the control literature, serious difficulties arise in the solution of this problem. We demonstrate that the problem can be reduced to a set of Hamilton-Jacobi-Bellman equations where human factors are incorporated via estimations of the system Hamiltonian. In the ITS context, these estimations can be obtained with the use of on-board equipment like sensors/receivers/actuators, in-vehicle communication devices, etc. The proposed methodology provides a way to integrate human factor into the solving process of the models for other complex dynamic systems.",
        "published": "2007-02-25T11:09:12Z",
        "link": "http://arxiv.org/abs/cs/0702149v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "cs.HC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Generic Global Constraints based on MDDs",
        "authors": [
            "Peter Tiedemann",
            "Henrik Reif Andersen",
            "Rasmus Pagh"
        ],
        "summary": "Constraint Programming (CP) has been successfully applied to both constraint satisfaction and constraint optimization problems. A wide variety of specialized global constraints provide critical assistance in achieving a good model that can take advantage of the structure of the problem in the search for a solution. However, a key outstanding issue is the representation of 'ad-hoc' constraints that do not have an inherent combinatorial nature, and hence are not modeled well using narrowly specialized global constraints. We attempt to address this issue by considering a hybrid of search and compilation. Specifically we suggest the use of Reduced Ordered Multi-Valued Decision Diagrams (ROMDDs) as the supporting data structure for a generic global constraint. We give an algorithm for maintaining generalized arc consistency (GAC) on this constraint that amortizes the cost of the GAC computation over a root-to-leaf path in the search tree without requiring asymptotically more space than used for the MDD. Furthermore we present an approach for incrementally maintaining the reduced property of the MDD during the search, and show how this can be used for providing domain entailment detection. Finally we discuss how to apply our approach to other similar data structures such as AOMDDs and Case DAGs. The technique used can be seen as an extension of the GAC algorithm for the regular language constraint on finite length input.",
        "published": "2007-02-28T15:32:48Z",
        "link": "http://arxiv.org/abs/cs/0702170v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "The Laplace-Jaynes approach to induction",
        "authors": [
            "P. G. L. Porta Mana",
            "A. Månsson",
            "G. Björk"
        ],
        "summary": "An approach to induction is presented, based on the idea of analysing the context of a given problem into `circumstances'. This approach, fully Bayesian in form and meaning, provides a complement or in some cases an alternative to that based on de Finetti's representation theorem and on the notion of infinite exchangeability. In particular, it gives an alternative interpretation of those formulae that apparently involve `unknown probabilities' or `propensities'. Various advantages and applications of the presented approach are discussed, especially in comparison to that based on exchangeability. Generalisations are also discussed.",
        "published": "2007-03-12T15:44:15Z",
        "link": "http://arxiv.org/abs/physics/0703126v2",
        "categories": [
            "physics.data-an",
            "cs.AI",
            "quant-ph"
        ]
    },
    {
        "title": "Redesigning Decision Matrix Method with an indeterminacy-based inference   process",
        "authors": [
            "Jose L. Salmeron",
            "Florentin Smarandache"
        ],
        "summary": "For academics and practitioners concerned with computers, business and mathematics, one central issue is supporting decision makers. In this paper, we propose a generalization of Decision Matrix Method (DMM), using Neutrosophic logic. It emerges as an alternative to the existing logics and it represents a mathematical model of uncertainty and indeterminacy. This paper proposes the Neutrosophic Decision Matrix Method as a more realistic tool for decision making. In addition, a de-neutrosophication process is included.",
        "published": "2007-03-13T02:18:09Z",
        "link": "http://arxiv.org/abs/cs/0703060v1",
        "categories": [
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "Social Information Processing in Social News Aggregation",
        "authors": [
            "Kristina Lerman"
        ],
        "summary": "The rise of the social media sites, such as blogs, wikis, Digg and Flickr among others, underscores the transformation of the Web to a participatory medium in which users are collaboratively creating, evaluating and distributing information. The innovations introduced by social media has lead to a new paradigm for interacting with information, what we call 'social information processing'. In this paper, we study how social news aggregator Digg exploits social information processing to solve the problems of document recommendation and rating. First, we show, by tracking stories over time, that social networks play an important role in document recommendation. The second contribution of this paper consists of two mathematical models. The first model describes how collaborative rating and promotion of stories emerges from the independent decisions made by many users. The second model describes how a user's influence, the number of promoted stories and the user's social network, changes in time. We find qualitative agreement between predictions of the model and user data gathered from Digg.",
        "published": "2007-03-15T22:37:22Z",
        "link": "http://arxiv.org/abs/cs/0703087v2",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.HC",
            "cs.MA"
        ]
    },
    {
        "title": "Multimodal Meaning Representation for Generic Dialogue Systems   Architectures",
        "authors": [
            "Frédéric Landragin",
            "Alexandre Denis",
            "Annalisa Ricci",
            "Laurent Romary"
        ],
        "summary": "An unified language for the communicative acts between agents is essential for the design of multi-agents architectures. Whatever the type of interaction (linguistic, multimodal, including particular aspects such as force feedback), whatever the type of application (command dialogue, request dialogue, database querying), the concepts are common and we need a generic meta-model. In order to tend towards task-independent systems, we need to clarify the modules parameterization procedures. In this paper, we focus on the characteristics of a meta-model designed to represent meaning in linguistic and multimodal applications. This meta-model is called MMIL for MultiModal Interface Language, and has first been specified in the framework of the IST MIAMM European project. What we want to test here is how relevant is MMIL for a completely different context (a different task, a different interaction type, a different linguistic domain). We detail the exploitation of MMIL in the framework of the IST OZONE European project, and we draw the conclusions on the role of MMIL in the parameterization of task-independent dialogue managers.",
        "published": "2007-03-16T15:37:47Z",
        "link": "http://arxiv.org/abs/cs/0703091v1",
        "categories": [
            "cs.AI",
            "cs.MM"
        ]
    },
    {
        "title": "Copula Component Analysis",
        "authors": [
            "Jian Ma",
            "Zengqi Sun"
        ],
        "summary": "A framework named Copula Component Analysis (CCA) for blind source separation is proposed as a generalization of Independent Component Analysis (ICA). It differs from ICA which assumes independence of sources that the underlying components may be dependent with certain structure which is represented by Copula. By incorporating dependency structure, much accurate estimation can be made in principle in the case that the assumption of independence is invalidated. A two phrase inference method is introduced for CCA which is based on the notion of multidimensional ICA.",
        "published": "2007-03-20T14:52:52Z",
        "link": "http://arxiv.org/abs/cs/0703095v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "Mathematical model of interest matchmaking in electronic social networks",
        "authors": [
            "Andreas de Vries"
        ],
        "summary": "The problem of matchmaking in electronic social networks is formulated as an optimization problem. In particular, a function measuring the matching degree of fields of interest of a search profile with those of an advertising profile is proposed.",
        "published": "2007-03-23T12:42:20Z",
        "link": "http://arxiv.org/abs/cs/0703118v2",
        "categories": [
            "cs.CY",
            "cs.AI",
            "I.2.4; H.3.5; J.4; G.2.3; C.2.4"
        ]
    },
    {
        "title": "Modelling Complexity in Musical Rhythm",
        "authors": [
            "Cheng-Yuan Liou",
            "Tai-Hei Wu",
            "Chia-Ying Lee"
        ],
        "summary": "This paper constructs a tree structure for the music rhythm using the L-system. It models the structure as an automata and derives its complexity. It also solves the complexity for the L-system. This complexity can resolve the similarity between trees. This complexity serves as a measure of psychological complexity for rhythms. It resolves the music complexity of various compositions including the Mozart effect K488.   Keyword: music perception, psychological complexity, rhythm, L-system, automata, temporal associative memory, inverse problem, rewriting rule, bracketed string, tree similarity",
        "published": "2007-03-26T07:37:11Z",
        "link": "http://arxiv.org/abs/cs/0703124v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Space-contained conflict revision, for geographic information",
        "authors": [
            "Omar Doukari",
            "Robert Jeansoulin"
        ],
        "summary": "Using qualitative reasoning with geographic information, contrarily, for instance, with robotics, looks not only fastidious (i.e.: encoding knowledge Propositional Logics PL), but appears to be computational complex, and not tractable at all, most of the time. However, knowledge fusion or revision, is a common operation performed when users merge several different data sets in a unique decision making process, without much support. Introducing logics would be a great improvement, and we propose in this paper, means for deciding -a priori- if one application can benefit from a complete revision, under only the assumption of a conjecture that we name the \"containment conjecture\", which limits the size of the minimal conflicts to revise. We demonstrate that this conjecture brings us the interesting computational property of performing a not-provable but global, revision, made of many local revisions, at a tractable size. We illustrate this approach on an application.",
        "published": "2007-03-26T12:18:32Z",
        "link": "http://arxiv.org/abs/cs/0703130v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Dependency Parsing with Dynamic Bayesian Network",
        "authors": [
            "Virginia Savova",
            "Leonid Peshkin"
        ],
        "summary": "Exact parsing with finite state automata is deemed inappropriate because of the unbounded non-locality languages overwhelmingly exhibit. We propose a way to structure the parsing task in order to make it amenable to local classification methods. This allows us to build a Dynamic Bayesian Network which uncovers the syntactic dependency structure of English sentences. Experiments with the Wall Street Journal demonstrate that the model successfully learns from labeled data.",
        "published": "2007-03-27T22:12:25Z",
        "link": "http://arxiv.org/abs/cs/0703135v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7; I.2.1; G.3; H.3.1"
        ]
    },
    {
        "title": "Reinforcement Learning for Adaptive Routing",
        "authors": [
            "Leonid Peshkin",
            "Virginia Savova"
        ],
        "summary": "Reinforcement learning means learning a policy--a mapping of observations into actions--based on feedback from the environment. The learning can be viewed as browsing a set of policies while evaluating them by trial through interaction with the environment. We present an application of gradient ascent algorithm for reinforcement learning to a complex domain of packet routing in network communication and compare the performance of this algorithm to other routing methods on a benchmark problem.",
        "published": "2007-03-28T04:41:54Z",
        "link": "http://arxiv.org/abs/cs/0703138v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NI",
            "C.2.1; C.2.2; C.2.4; C.2.6; F.1.1; I.2.6; I.2.8; I.2.9"
        ]
    },
    {
        "title": "Case Base Mining for Adaptation Knowledge Acquisition",
        "authors": [
            "Mathieu D'Aquin",
            "Fadi Badra",
            "Sandrine Lafrogne",
            "Jean Lieber",
            "Amedeo Napoli",
            "Laszlo Szathmary"
        ],
        "summary": "In case-based reasoning, the adaptation of a source case in order to solve the target problem is at the same time crucial and difficult to implement. The reason for this difficulty is that, in general, adaptation strongly depends on domain-dependent knowledge. This fact motivates research on adaptation knowledge acquisition (AKA). This paper presents an approach to AKA based on the principles and techniques of knowledge discovery from databases and data-mining. It is implemented in CABAMAKA, a system that explores the variations within the case base to elicit adaptation knowledge. This system has been successfully tested in an application of case-based reasoning to decision support in the domain of breast cancer treatment.",
        "published": "2007-03-30T16:16:11Z",
        "link": "http://arxiv.org/abs/cs/0703156v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Intelligent location of simultaneously active acoustic emission sources:   Part I",
        "authors": [
            "T. Kosel",
            "I. Grabec"
        ],
        "summary": "The intelligent acoustic emission locator is described in Part I, while Part II discusses blind source separation, time delay estimation and location of two simultaneously active continuous acoustic emission sources.   The location of acoustic emission on complicated aircraft frame structures is a difficult problem of non-destructive testing. This article describes an intelligent acoustic emission source locator. The intelligent locator comprises a sensor antenna and a general regression neural network, which solves the location problem based on learning from examples. Locator performance was tested on different test specimens. Tests have shown that the accuracy of location depends on sound velocity and attenuation in the specimen, the dimensions of the tested area, and the properties of stored data. The location accuracy achieved by the intelligent locator is comparable to that obtained by the conventional triangulation method, while the applicability of the intelligent locator is more general since analysis of sonic ray paths is avoided. This is a promising method for non-destructive testing of aircraft frame structures by the acoustic emission method.",
        "published": "2007-04-01T13:06:50Z",
        "link": "http://arxiv.org/abs/0704.0047v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Intelligent location of simultaneously active acoustic emission sources:   Part II",
        "authors": [
            "T. Kosel",
            "I. Grabec"
        ],
        "summary": "Part I describes an intelligent acoustic emission locator, while Part II discusses blind source separation, time delay estimation and location of two continuous acoustic emission sources.   Acoustic emission (AE) analysis is used for characterization and location of developing defects in materials. AE sources often generate a mixture of various statistically independent signals. A difficult problem of AE analysis is separation and characterization of signal components when the signals from various sources and the mode of mixing are unknown. Recently, blind source separation (BSS) by independent component analysis (ICA) has been used to solve these problems. The purpose of this paper is to demonstrate the applicability of ICA to locate two independent simultaneously active acoustic emission sources on an aluminum band specimen. The method is promising for non-destructive testing of aircraft frame structures by acoustic emission analysis.",
        "published": "2007-04-01T18:53:13Z",
        "link": "http://arxiv.org/abs/0704.0050v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "The World as Evolving Information",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "This paper discusses the benefits of describing the world as information, especially in the study of the evolution of life and cognition. Traditional studies encounter problems because it is difficult to describe life and cognition in terms of matter and energy, since their laws are valid only at the physical scale. However, if matter and energy, as well as life and cognition, are described in terms of information, evolution can be described consistently as information becoming more complex.   The paper presents eight tentative laws of information, valid at multiple scales, which are generalizations of Darwinian, cybernetic, thermodynamic, psychological, philosophical, and complexity principles. These are further used to discuss the notions of life, cognition and their evolution.",
        "published": "2007-04-03T02:08:48Z",
        "link": "http://arxiv.org/abs/0704.0304v3",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT",
            "q-bio.PE",
            "H.1.1"
        ]
    },
    {
        "title": "Architecture for Pseudo Acausal Evolvable Embedded Systems",
        "authors": [
            "Mohd Abubakr",
            "R. M. Vinay"
        ],
        "summary": "Advances in semiconductor technology are contributing to the increasing complexity in the design of embedded systems. Architectures with novel techniques such as evolvable nature and autonomous behavior have engrossed lot of attention. This paper demonstrates conceptually evolvable embedded systems can be characterized basing on acausal nature. It is noted that in acausal systems, future input needs to be known, here we make a mechanism such that the system predicts the future inputs and exhibits pseudo acausal nature. An embedded system that uses theoretical framework of acausality is proposed. Our method aims at a novel architecture that features the hardware evolability and autonomous behavior alongside pseudo acausality. Various aspects of this architecture are discussed in detail along with the limitations.",
        "published": "2007-04-07T13:40:49Z",
        "link": "http://arxiv.org/abs/0704.0985v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A neural network approach to ordinal regression",
        "authors": [
            "Jianlin Cheng"
        ],
        "summary": "Ordinal regression is an important type of learning, which has properties of both classification and regression. Here we describe a simple and effective approach to adapt a traditional neural network to learn ordinal categories. Our approach is a generalization of the perceptron method for ordinal regression. On several benchmark datasets, our method (NNRank) outperforms a neural network classification method. Compared with the ordinal regression methods using Gaussian processes and support vector machines, NNRank achieves comparable performance. Moreover, NNRank has the advantages of traditional neural networks: learning in both online and batch modes, handling very large training datasets, and making rapid predictions. These features make NNRank a useful and complementary tool for large-scale data processing tasks such as information retrieval, web page ranking, collaborative filtering, and protein ranking in Bioinformatics.",
        "published": "2007-04-08T17:36:00Z",
        "link": "http://arxiv.org/abs/0704.1028v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Calculating Valid Domains for BDD-Based Interactive Configuration",
        "authors": [
            "Tarik Hadzic",
            "Rune Moller Jensen",
            "Henrik Reif Andersen"
        ],
        "summary": "In these notes we formally describe the functionality of Calculating Valid Domains from the BDD representing the solution space of valid configurations. The formalization is largely based on the CLab configuration framework.",
        "published": "2007-04-11T10:59:56Z",
        "link": "http://arxiv.org/abs/0704.1394v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Preconditioned Temporal Difference Learning",
        "authors": [
            "Yao HengShuai"
        ],
        "summary": "This paper has been withdrawn by the author. This draft is withdrawn for its poor quality in english, unfortunately produced by the author when he was just starting his science route. Look at the ICML version instead: http://icml2008.cs.helsinki.fi/papers/111.pdf",
        "published": "2007-04-11T13:17:01Z",
        "link": "http://arxiv.org/abs/0704.1409v3",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Exploiting Social Annotation for Automatic Resource Discovery",
        "authors": [
            "Anon Plangprasopchok",
            "Kristina Lerman"
        ],
        "summary": "Information integration applications, such as mediators or mashups, that require access to information resources currently rely on users manually discovering and integrating them in the application. Manual resource discovery is a slow process, requiring the user to sift through results obtained via keyword-based search. Although search methods have advanced to include evidence from document contents, its metadata and the contents and link structure of the referring pages, they still do not adequately cover information sources -- often called ``the hidden Web''-- that dynamically generate documents in response to a query. The recently popular social bookmarking sites, which allow users to annotate and share metadata about various information sources, provide rich evidence for resource discovery. In this paper, we describe a probabilistic model of the user annotation process in a social bookmarking system del.icio.us. We then use the model to automatically find resources relevant to a particular information domain. Our experimental results on data obtained from \\emph{del.icio.us} show this approach as a promising method for helping automate the resource discovery task.",
        "published": "2007-04-12T23:24:19Z",
        "link": "http://arxiv.org/abs/0704.1675v1",
        "categories": [
            "cs.AI",
            "cs.CY",
            "cs.DL"
        ]
    },
    {
        "title": "Personalizing Image Search Results on Flickr",
        "authors": [
            "Kristina Lerman",
            "Anon Plangprasopchok",
            "Chio Wong"
        ],
        "summary": "The social media site Flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. Flickr offers multiple ways of browsing or searching it. One option is tag search, which returns all images tagged with a specific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. We claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. We show how to exploit this metadata to personalize search results for the user, thereby improving search performance. First, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. Secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. The users' interests can similarly be described by the tags they used for annotating their images. The latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.",
        "published": "2007-04-12T23:31:04Z",
        "link": "http://arxiv.org/abs/0704.1676v1",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CY",
            "cs.DL",
            "cs.HC"
        ]
    },
    {
        "title": "Unicast and Multicast Qos Routing with Soft Constraint Logic Programming",
        "authors": [
            "Stefano Bistarelli",
            "Ugo Montanari",
            "Francesca Rossi",
            "Francesco Santini"
        ],
        "summary": "We present a formal model to represent and solve the unicast/multicast routing problem in networks with Quality of Service (QoS) requirements. To attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different QoS metric value (e.g. bandwidth, cost, delay, packet loss). The second step consists in writing this graph as a program in Soft Constraint Logic Programming (SCLP): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to QoS routing problems. Moreover, c-semiring structures are a convenient tool to model QoS metrics. At last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.",
        "published": "2007-04-13T15:53:44Z",
        "link": "http://arxiv.org/abs/0704.1783v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.NI",
            "D.3.2; D.3.3; C.2.3; F.4.1"
        ]
    },
    {
        "title": "A study of structural properties on profiles HMMs",
        "authors": [
            "Juliana S Bernardes",
            "Alberto Davila",
            "Vitor Santos Costa",
            "Gerson Zaverucha"
        ],
        "summary": "Motivation: Profile hidden Markov Models (pHMMs) are a popular and very useful tool in the detection of the remote homologue protein families. Unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm and tool that tries to improve pHMM performance by using structural information while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs. Each pHMM is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. Properties used were primary, secondary and tertiary structures, accessibility and packing. HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP database to perform our experiments. Throughout, we apply leave-one-family-out cross-validation over protein superfamilies. First, we used the MAMMOTH-mult structural aligner to align the training set proteins. Then, we performed two sets of experiments. In a first experiment, we compared structure weighted models against standard pHMMs and against each other. In a second experiment, we compared the voting model against individual pHMMs. We compare method performance through ROC curves and through Precision/Recall curves, and assess significance through the paired two tailed t-test. Our results show significant performance improvements of all structurally weighted models over default HMMER, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.",
        "published": "2007-04-16T13:10:35Z",
        "link": "http://arxiv.org/abs/0704.2010v2",
        "categories": [
            "cs.AI",
            "J.3"
        ]
    },
    {
        "title": "Introduction to Arabic Speech Recognition Using CMUSphinx System",
        "authors": [
            "H. Satori",
            "M. Harti",
            "N. Chenfour"
        ],
        "summary": "In this paper Arabic was investigated from the speech recognition problem point of view. We propose a novel approach to build an Arabic Automated Speech Recognition System (ASR). This system is based on the open source CMU Sphinx-4, from the Carnegie Mellon University. CMU Sphinx is a large-vocabulary; speaker-independent, continuous speech recognition system based on discrete Hidden Markov Models (HMMs). We build a model using utilities from the OpenSource CMU Sphinx. We will demonstrate the possible adaptability of this system to Arabic voice recognition.",
        "published": "2007-04-17T01:04:01Z",
        "link": "http://arxiv.org/abs/0704.2083v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Arabic Speech Recognition System using CMU-Sphinx4",
        "authors": [
            "H. Satori",
            "M. Harti",
            "N. Chenfour"
        ],
        "summary": "In this paper we present the creation of an Arabic version of Automated Speech Recognition System (ASR). This system is based on the open source Sphinx-4, from the Carnegie Mellon University. Which is a speech recognition system based on discrete hidden Markov models (HMMs). We investigate the changes that must be made to the model to adapt Arabic voice recognition.   Keywords: Speech recognition, Acoustic model, Arabic language, HMMs, CMUSphinx-4, Artificial intelligence.",
        "published": "2007-04-17T17:04:26Z",
        "link": "http://arxiv.org/abs/0704.2201v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Experimenting with recursive queries in database and logic programming   systems",
        "authors": [
            "Giorgio Terracina",
            "Nicola Leone",
            "Vincenzino Lio",
            "Claudio Panetta"
        ],
        "summary": "This paper considers the problem of reasoning on massive amounts of (possibly distributed) data. Presently, existing proposals show some limitations: {\\em (i)} the quantity of data that can be handled contemporarily is limited, due to the fact that reasoning is generally carried out in main-memory; {\\em (ii)} the interaction with external (and independent) DBMSs is not trivial and, in several cases, not allowed at all; {\\em (iii)} the efficiency of present implementations is still not sufficient for their utilization in complex reasoning tasks involving massive amounts of data. This paper provides a contribution in this setting; it presents a new system, called DLV$^{DB}$, which aims to solve these problems. Moreover, the paper reports the results of a thorough experimental analysis we have carried out for comparing our system with several state-of-the-art systems (both logic and databases) on some classical deductive problems; the other tested systems are: LDL++, XSB, Smodels and three top-level commercial DBMSs. DLV$^{DB}$ significantly outperforms even the commercial Database Systems on recursive queries. To appear in Theory and Practice of Logic Programming (TPLP)",
        "published": "2007-04-24T10:58:40Z",
        "link": "http://arxiv.org/abs/0704.3157v1",
        "categories": [
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "Direct Optimization of Ranking Measures",
        "authors": [
            "Quoc Le",
            "Alexander Smola"
        ],
        "summary": "Web page ranking and collaborative filtering require the optimization of sophisticated performance measures. Current Support Vector approaches are unable to optimize them directly and focus on pairwise comparisons instead. We present a new approach which allows direct optimization of the relevant loss functions. This is achieved via structured estimation in Hilbert spaces. It is most related to Max-Margin-Markov networks optimization of multivariate performance measures. Key to our approach is that during training the ranking problem can be viewed as a linear assignment problem, which can be solved by the Hungarian Marriage algorithm. At test time, a sort operation is sufficient, as our algorithm assigns a relevance score to every (document, query) pair. Experiments show that the our algorithm is fast and that it works very well.",
        "published": "2007-04-25T12:36:55Z",
        "link": "http://arxiv.org/abs/0704.3359v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "General-Purpose Computing on a Semantic Network Substrate",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "This article presents a model of general-purpose computing on a semantic network substrate. The concepts presented are applicable to any semantic network representation. However, due to the standards and technological infrastructure devoted to the Semantic Web effort, this article is presented from this point of view. In the proposed model of computing, the application programming interface, the run-time program, and the state of the computing virtual machine are all represented in the Resource Description Framework (RDF). The implementation of the concepts presented provides a practical computing paradigm that leverages the highly-distributed and standardized representational-layer of the Semantic Web.",
        "published": "2007-04-25T15:37:52Z",
        "link": "http://arxiv.org/abs/0704.3395v4",
        "categories": [
            "cs.AI",
            "cs.PL",
            "I.2.4; I.2.5; H.3.7; H.3.4"
        ]
    },
    {
        "title": "Bayesian approach to rough set",
        "authors": [
            "Tshilidzi Marwala",
            "Bodie Crossingham"
        ],
        "summary": "This paper proposes an approach to training rough set models using Bayesian framework trained using Markov Chain Monte Carlo (MCMC) method. The prior probabilities are constructed from the prior knowledge that good rough set models have fewer rules. Markov Chain Monte Carlo sampling is conducted through sampling in the rough set granule space and Metropolis algorithm is used as an acceptance criteria. The proposed method is tested to estimate the risk of HIV given demographic data. The results obtained shows that the proposed approach is able to achieve an average accuracy of 58% with the accuracy varying up to 66%. In addition the Bayesian rough set give the probabilities of the estimated HIV status as well as the linguistic rules describing how the demographic parameters drive the risk of HIV.",
        "published": "2007-04-25T19:50:59Z",
        "link": "http://arxiv.org/abs/0704.3433v1",
        "categories": [
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "An Adaptive Strategy for the Classification of G-Protein Coupled   Receptors",
        "authors": [
            "S. Mohamed",
            "D. Rubin",
            "T. Marwala"
        ],
        "summary": "One of the major problems in computational biology is the inability of existing classification models to incorporate expanding and new domain knowledge. This problem of static classification models is addressed in this paper by the introduction of incremental learning for problems in bioinformatics. Many machine learning tools have been applied to this problem using static machine learning structures such as neural networks or support vector machines that are unable to accommodate new information into their existing models. We utilize the fuzzy ARTMAP as an alternate machine learning system that has the ability of incrementally learning new data as it becomes available. The fuzzy ARTMAP is found to be comparable to many of the widespread machine learning systems. The use of an evolutionary strategy in the selection and combination of individual classifiers into an ensemble system, coupled with the incremental learning ability of the fuzzy ARTMAP is proven to be suitable as a pattern classifier. The algorithm presented is tested using data from the G-Coupled Protein Receptors Database and shows good accuracy of 83%. The system presented is also generally applicable, and can be used in problems in genomics and proteomics.",
        "published": "2007-04-25T21:23:31Z",
        "link": "http://arxiv.org/abs/0704.3453v1",
        "categories": [
            "cs.AI",
            "q-bio.QM"
        ]
    },
    {
        "title": "Comparing Robustness of Pairwise and Multiclass Neural-Network Systems   for Face Recognition",
        "authors": [
            "J. Uglov",
            "V. Schetinin",
            "C. Maple"
        ],
        "summary": "Noise, corruptions and variations in face images can seriously hurt the performance of face recognition systems. To make such systems robust, multiclass neuralnetwork classifiers capable of learning from noisy data have been suggested. However on large face data sets such systems cannot provide the robustness at a high level. In this paper we explore a pairwise neural-network system as an alternative approach to improving the robustness of face recognition. In our experiments this approach is shown to outperform the multiclass neural-network system in terms of the predictive accuracy on the face images corrupted by noise.",
        "published": "2007-04-26T11:29:19Z",
        "link": "http://arxiv.org/abs/0704.3515v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Ensemble Learning for Free with Evolutionary Algorithms ?",
        "authors": [
            "Christian Gagné",
            "Michèle Sebag",
            "Marc Schoenauer",
            "Marco Tomassini"
        ],
        "summary": "Evolutionary Learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. In the meanwhile, Ensemble Learning, one of the most efficient approaches in supervised Machine Learning for the last decade, proceeds by building a population of diverse classifiers. Ensemble Learning with Evolutionary Computation thus receives increasing attention. The Evolutionary Ensemble Learning (EEL) approach presented in this paper features two contributions. First, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. Further, a new selection criterion based on the classification margin is proposed. This criterion is used to extract the classifier ensemble from the final population only (Off-line) or incrementally along evolution (On-line). Experiments on a set of benchmark problems show that Off-line outperforms single-hypothesis evolutionary learning and state-of-art Boosting and generates smaller classifier ensembles.",
        "published": "2007-04-30T09:29:22Z",
        "link": "http://arxiv.org/abs/0704.3905v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A Note on Ontology and Ordinary Language",
        "authors": [
            "Walid S. Saba"
        ],
        "summary": "We argue for a compositional semantics grounded in a strongly typed ontology that reflects our commonsense view of the world and the way we talk about it. Assuming such a structure we show that the semantics of various natural language phenomena may become nearly trivial.",
        "published": "2007-04-30T17:55:39Z",
        "link": "http://arxiv.org/abs/0704.3886v6",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Can the Internet cope with stress?",
        "authors": [
            "Andreas Martin Lisewski"
        ],
        "summary": "When will the Internet become aware of itself? In this note the problem is approached by asking an alternative question: Can the Internet cope with stress? By extrapolating the psychological difference between coping and defense mechanisms a distributed software experiment is outlined which could reject the hypothesis that the Internet is not a conscious entity.",
        "published": "2007-05-01T15:44:17Z",
        "link": "http://arxiv.org/abs/0705.0025v1",
        "categories": [
            "cs.HC",
            "cs.AI"
        ]
    },
    {
        "title": "Fault Classification in Cylinders Using Multilayer Perceptrons, Support   Vector Machines and Guassian Mixture Models",
        "authors": [
            "Tshilidzi Marwala",
            "Unathi Mahola",
            "Snehashish Chakraverty"
        ],
        "summary": "Gaussian mixture models (GMM) and support vector machines (SVM) are introduced to classify faults in a population of cylindrical shells. The proposed procedures are tested on a population of 20 cylindrical shells and their performance is compared to the procedure, which uses multi-layer perceptrons (MLP). The modal properties extracted from vibration data are used to train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM produces 94% classification accuracy while the MLP produces 88% classification rates.",
        "published": "2007-05-02T03:13:28Z",
        "link": "http://arxiv.org/abs/0705.0197v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "The Parameter-Less Self-Organizing Map algorithm",
        "authors": [
            "Erik Berglund",
            "Joaquin Sitte"
        ],
        "summary": "The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network algorithm based on the Self-Organizing Map (SOM). It eliminates the need for a learning rate and annealing schemes for learning rate and neighbourhood size. We discuss the relative performance of the PLSOM and the SOM and demonstrate some tasks in which the SOM fails but the PLSOM performs satisfactory. Finally we discuss some example applications of the PLSOM and present a proof of ordering under certain limited conditions.",
        "published": "2007-05-02T04:04:51Z",
        "link": "http://arxiv.org/abs/0705.0199v2",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CV"
        ]
    },
    {
        "title": "Clustering Co-occurrence of Maximal Frequent Patterns in Streams",
        "authors": [
            "Edgar H. de Graaf",
            "Joost N. Kok",
            "Walter A. Kosters"
        ],
        "summary": "One way of getting a better view of data is using frequent patterns. In this paper frequent patterns are subsets that occur a minimal number of times in a stream of itemsets. However, the discovery of frequent patterns in streams has always been problematic. Because streams are potentially endless it is in principle impossible to say if a pattern is often occurring or not. Furthermore the number of patterns can be huge and a good overview of the structure of the stream is lost quickly. The proposed approach will use clustering to facilitate the analysis of the structure of the stream.   A clustering on the co-occurrence of patterns will give the user an improved view on the structure of the stream. Some patterns might occur so much together that they should form a combined pattern. In this way the patterns in the clustering will be the largest frequent patterns: maximal frequent patterns.   Our approach to decide if patterns occur often together will be based on a method of clustering when only the distance between pairs is known. The number of maximal frequent patterns is much smaller and combined with clustering methods these patterns provide a good view on the structure of the stream.",
        "published": "2007-05-04T10:36:53Z",
        "link": "http://arxiv.org/abs/0705.0588v1",
        "categories": [
            "cs.AI",
            "cs.DS"
        ]
    },
    {
        "title": "Clustering with Lattices in the Analysis of Graph Patterns",
        "authors": [
            "Edgar H. de Graaf",
            "Joost N. Kok",
            "Walter A. Kosters"
        ],
        "summary": "Mining frequent subgraphs is an area of research where we have a given set of graphs (each graph can be seen as a transaction), and we search for (connected) subgraphs contained in many of these graphs. In this work we will discuss techniques used in our framework Lattice2SAR for mining and analysing frequent subgraph data and their corresponding lattice information. Lattice information is provided by the graph mining algorithm gSpan; it contains all supergraph-subgraph relations of the frequent subgraph patterns -- and their supports.   Lattice2SAR is in particular used in the analysis of frequent graph patterns where the graphs are molecules and the frequent subgraphs are fragments. In the analysis of fragments one is interested in the molecules where patterns occur. This data can be very extensive and in this paper we focus on a technique of making it better available by using the lattice information in our clustering. Now we can reduce the number of times the highly compressed occurrence data needs to be accessed by the user. The user does not have to browse all the occurrence data in search of patterns occurring in the same molecules. Instead one can directly see which frequent subgraphs are of interest.",
        "published": "2007-05-04T10:52:28Z",
        "link": "http://arxiv.org/abs/0705.0593v1",
        "categories": [
            "cs.AI",
            "cs.DS"
        ]
    },
    {
        "title": "Soft constraint abstraction based on semiring homomorphism",
        "authors": [
            "Sanjiang Li",
            "Mingsheng Ying"
        ],
        "summary": "The semiring-based constraint satisfaction problems (semiring CSPs), proposed by Bistarelli, Montanari and Rossi \\cite{BMR97}, is a very general framework of soft constraints. In this paper we propose an abstraction scheme for soft constraints that uses semiring homomorphism. To find optimal solutions of the concrete problem, the idea is, first working in the abstract problem and finding its optimal solutions, then using them to solve the concrete problem.   In particular, we show that a mapping preserves optimal solutions if and only if it is an order-reflecting semiring homomorphism. Moreover, for a semiring homomorphism $\\alpha$ and a problem $P$ over $S$, if $t$ is optimal in $\\alpha(P)$, then there is an optimal solution $\\bar{t}$ of $P$ such that $\\bar{t}$ has the same value as $t$ in $\\alpha(P)$.",
        "published": "2007-05-05T08:47:31Z",
        "link": "http://arxiv.org/abs/0705.0734v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Equivalence of LP Relaxation and Max-Product for Weighted Matching in   General Graphs",
        "authors": [
            "Sujay Sanghavi"
        ],
        "summary": "Max-product belief propagation is a local, iterative algorithm to find the mode/MAP estimate of a probability distribution. While it has been successfully employed in a wide variety of applications, there are relatively few theoretical guarantees of convergence and correctness for general loopy graphs that may have many short cycles. Of these, even fewer provide exact ``necessary and sufficient'' characterizations.   In this paper we investigate the problem of using max-product to find the maximum weight matching in an arbitrary graph with edge weights. This is done by first constructing a probability distribution whose mode corresponds to the optimal matching, and then running max-product. Weighted matching can also be posed as an integer program, for which there is an LP relaxation. This relaxation is not always tight. In this paper we show that \\begin{enumerate} \\item If the LP relaxation is tight, then max-product always converges, and that too to the correct answer. \\item If the LP relaxation is loose, then max-product does not converge. \\end{enumerate} This provides an exact, data-dependent characterization of max-product performance, and a precise connection to LP relaxation, which is a well-studied optimization technique. Also, since LP relaxation is known to be tight for bipartite graphs, our results generalize other recent results on using max-product to find weighted matchings in bipartite graphs.",
        "published": "2007-05-05T18:57:47Z",
        "link": "http://arxiv.org/abs/0705.0760v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "cs.LG",
            "cs.NI",
            "math.IT"
        ]
    },
    {
        "title": "Bayesian Approach to Neuro-Rough Models",
        "authors": [
            "Tshilidzi Marwala",
            "Bodie Crossingham"
        ],
        "summary": "This paper proposes a neuro-rough model based on multi-layered perceptron and rough set. The neuro-rough model is then tested on modelling the risk of HIV from demographic data. The model is formulated using Bayesian framework and trained using Monte Carlo method and Metropolis criterion. When the model was tested to estimate the risk of HIV infection given the demographic data it was found to give the accuracy of 62%. The proposed model is able to combine the accuracy of the Bayesian MLP model and the transparency of Bayesian rough set model.",
        "published": "2007-05-06T22:55:58Z",
        "link": "http://arxiv.org/abs/0705.0761v3",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Artificial Neural Networks and Support Vector Machines for Water Demand   Time Series Forecasting",
        "authors": [
            "Ishmael S. Msiza",
            "Fulufhelo V. Nelwamondo",
            "Tshilidzi Marwala"
        ],
        "summary": "Water plays a pivotal role in many physical processes, and most importantly in sustaining human life, animal life and plant life. Water supply entities therefore have the responsibility to supply clean and safe water at the rate required by the consumer. It is therefore necessary to implement mechanisms and systems that can be employed to predict both short-term and long-term water demands. The increasingly growing field of computational intelligence techniques has been proposed as an efficient tool in the modelling of dynamic phenomena. The primary objective of this paper is to compare the efficiency of two computational intelligence techniques in water demand forecasting. The techniques under comparison are the Artificial Neural Networks (ANNs) and the Support Vector Machines (SVMs). In this study it was observed that the ANNs perform better than the SVMs. This performance is measured against the generalisation ability of the two.",
        "published": "2007-05-07T19:00:28Z",
        "link": "http://arxiv.org/abs/0705.0969v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Learning to Bluff",
        "authors": [
            "Evan Hurwitz",
            "Tshilidzi Marwala"
        ],
        "summary": "The act of bluffing confounds game designers to this day. The very nature of bluffing is even open for debate, adding further complication to the process of creating intelligent virtual players that can bluff, and hence play, realistically. Through the use of intelligent, learning agents, and carefully designed agent outlooks, an agent can in fact learn to predict its opponents reactions based not only on its own cards, but on the actions of those around it. With this wider scope of understanding, an agent can in learn to bluff its opponents, with the action representing not an illogical action, as bluffing is often viewed, but rather as an act of maximising returns through an effective statistical optimisation. By using a tee dee lambda learning algorithm to continuously adapt neural network agent intelligence, agents have been shown to be able to learn to bluff without outside prompting, and even to learn to call each others bluffs in free, competitive play.",
        "published": "2007-05-07T19:15:24Z",
        "link": "http://arxiv.org/abs/0705.0693v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Fuzzy Artmap and Neural Network Approach to Online Processing of Inputs   with Missing Values",
        "authors": [
            "F. V. Nelwamondo",
            "T. Marwala"
        ],
        "summary": "An ensemble based approach for dealing with missing data, without predicting or imputing the missing values is proposed. This technique is suitable for online operations of neural networks and as a result, is used for online condition monitoring. The proposed technique is tested in both classification and regression problems. An ensemble of Fuzzy-ARTMAPs is used for classification whereas an ensemble of multi-layer perceptrons is used for the regression problem. Results obtained using this ensemble-based technique are compared to those obtained using a combination of auto-associative neural networks and genetic algorithms and findings show that this method can perform up to 9% better in regression problems. Another advantage of the proposed technique is that it eliminates the need for finding the best estimate of the data, and hence, saves time.",
        "published": "2007-05-08T05:12:01Z",
        "link": "http://arxiv.org/abs/0705.1031v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Mining Patterns with a Balanced Interval",
        "authors": [
            "Edgar de Graaf Joost Kok Walter Kosters"
        ],
        "summary": "In many applications it will be useful to know those patterns that occur with a balanced interval, e.g., a certain combination of phone numbers are called almost every Friday or a group of products are sold a lot on Tuesday and Thursday.   In previous work we proposed a new measure of support (the number of occurrences of a pattern in a dataset), where we count the number of times a pattern occurs (nearly) in the middle between two other occurrences. If the number of non-occurrences between two occurrences of a pattern stays almost the same then we call the pattern balanced.   It was noticed that some very frequent patterns obviously also occur with a balanced interval, meaning in every transaction. However more interesting patterns might occur, e.g., every three transactions. Here we discuss a solution using standard deviation and average. Furthermore we propose a simpler approach for pruning patterns with a balanced interval, making estimating the pruning threshold more intuitive.",
        "published": "2007-05-08T15:22:38Z",
        "link": "http://arxiv.org/abs/0705.1110v1",
        "categories": [
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "Artificial Intelligence for Conflict Management",
        "authors": [
            "E. Habtemariam",
            "T. Marwala",
            "M. Lagazio"
        ],
        "summary": "Militarised conflict is one of the risks that have a significant impact on society. Militarised Interstate Dispute (MID) is defined as an outcome of interstate interactions, which result on either peace or conflict. Effective prediction of the possibility of conflict between states is an important decision support tool for policy makers. In a previous research, neural networks (NNs) have been implemented to predict the MID. Support Vector Machines (SVMs) have proven to be very good prediction techniques and are introduced for the prediction of MIDs in this study and compared to neural networks. The results show that SVMs predict MID better than NNs while NNs give more consistent and easy to interpret sensitivity analysis than SVMs.",
        "published": "2007-05-09T05:53:30Z",
        "link": "http://arxiv.org/abs/0705.1209v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Evolving Symbolic Controllers",
        "authors": [
            "Nicolas Godzik",
            "Marc Schoenauer",
            "Michèle Sebag"
        ],
        "summary": "The idea of symbolic controllers tries to bridge the gap between the top-down manual design of the controller architecture, as advocated in Brooks' subsumption architecture, and the bottom-up designer-free approach that is now standard within the Evolutionary Robotics community. The designer provides a set of elementary behavior, and evolution is given the goal of assembling them to solve complex tasks. Two experiments are presented, demonstrating the efficiency and showing the recursiveness of this approach. In particular, the sensitivity with respect to the proposed elementary behaviors, and the robustness w.r.t. generalization of the resulting controllers are studied in detail.",
        "published": "2007-05-09T09:53:31Z",
        "link": "http://arxiv.org/abs/0705.1244v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Robust Multi-Cellular Developmental Design",
        "authors": [
            "Alexandre Devert",
            "Nicolas Bredèche",
            "Marc Schoenauer"
        ],
        "summary": "This paper introduces a continuous model for Multi-cellular Developmental Design. The cells are fixed on a 2D grid and exchange \"chemicals\" with their neighbors during the growth process. The quantity of chemicals that a cell produces, as well as the differentiation value of the cell in the phenotype, are controlled by a Neural Network (the genotype) that takes as inputs the chemicals produced by the neighboring cells at the previous time step. In the proposed model, the number of iterations of the growth process is not pre-determined, but emerges during evolution: only organisms for which the growth process stabilizes give a phenotype (the stable state), others are declared nonviable. The optimization of the controller is done using the NEAT algorithm, that optimizes both the topology and the weights of the Neural Networks. Though each cell only receives local information from its neighbors, the experimental results of the proposed approach on the 'flags' problems (the phenotype must match a given 2D pattern) are almost as good as those of a direct regression approach using the same model with global information. Moreover, the resulting multi-cellular organisms exhibit almost perfect self-healing characteristics.",
        "published": "2007-05-09T15:33:34Z",
        "link": "http://arxiv.org/abs/0705.1309v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Non-Computability of Consciousness",
        "authors": [
            "Daegene Song"
        ],
        "summary": "With the great success in simulating many intelligent behaviors using computing devices, there has been an ongoing debate whether all conscious activities are computational processes. In this paper, the answer to this question is shown to be no. A certain phenomenon of consciousness is demonstrated to be fully represented as a computational process using a quantum computer. Based on the computability criterion discussed with Turing machines, the model constructed is shown to necessarily involve a non-computable element. The concept that this is solely a quantum effect and does not work for a classical case is also discussed.",
        "published": "2007-05-11T10:16:48Z",
        "link": "http://arxiv.org/abs/0705.1617v1",
        "categories": [
            "quant-ph",
            "astro-ph",
            "cs.AI"
        ]
    },
    {
        "title": "Using artificial intelligence for data reduction in mechanical   engineering",
        "authors": [
            "L. Mdlazi",
            "C. J. Stander",
            "P. S. Heyns",
            "T. Marwala"
        ],
        "summary": "In this paper artificial neural networks and support vector machines are used to reduce the amount of vibration data that is required to estimate the Time Domain Average of a gear vibration signal. Two models for estimating the time domain average of a gear vibration signal are proposed. The models are tested on data from an accelerated gear life test rig. Experimental results indicate that the required data for calculating the Time Domain Average of a gear vibration signal can be reduced by up to 75% when the proposed models are implemented.",
        "published": "2007-05-11T15:49:40Z",
        "link": "http://arxiv.org/abs/0705.1673v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "A first-order Temporal Logic for Actions",
        "authors": [
            "Camilla Schwind"
        ],
        "summary": "We present a multi-modal action logic with first-order modalities, which contain terms which can be unified with the terms inside the subsequent formulas and which can be quantified. This makes it possible to handle simultaneously time and states. We discuss applications of this language to action theory where it is possible to express many temporal aspects of actions, as for example, beginning, end, time points, delayed preconditions and results, duration and many others. We present tableaux rules for a decidable fragment of this logic.",
        "published": "2007-05-14T18:36:25Z",
        "link": "http://arxiv.org/abs/0705.1999v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Multi-Dimensional Recurrent Neural Networks",
        "authors": [
            "Alex Graves",
            "Santiago Fernandez",
            "Juergen Schmidhuber"
        ],
        "summary": "Recurrent neural networks (RNNs) have proved effective at one dimensional sequence learning tasks, such as speech and online handwriting recognition. Some of the properties that make RNNs suitable for such tasks, for example robustness to input warping, and the ability to access contextual information, are also desirable in multidimensional domains. However, there has so far been no direct way of applying RNNs to data with more than one spatio-temporal dimension. This paper introduces multi-dimensional recurrent neural networks (MDRNNs), thereby extending the potential applicability of RNNs to vision, video processing, medical imaging and many other areas, while avoiding the scaling problems that have plagued other multi-dimensional models. Experimental results are provided for two image segmentation tasks.",
        "published": "2007-05-14T19:49:56Z",
        "link": "http://arxiv.org/abs/0705.2011v1",
        "categories": [
            "cs.AI",
            "cs.CV"
        ]
    },
    {
        "title": "Response Prediction of Structural System Subject to Earthquake Motions   using Artificial Neural Network",
        "authors": [
            "S. Chakraverty",
            "T. Marwala",
            "Pallavi Gupta",
            "Thando Tettey"
        ],
        "summary": "This paper uses Artificial Neural Network (ANN) models to compute response of structural system subject to Indian earthquakes at Chamoli and Uttarkashi ground motion data. The system is first trained for a single real earthquake data. The trained ANN architecture is then used to simulate earthquakes with various intensities and it was found that the predicted responses given by ANN model are accurate for practical purposes. When the ANN is trained by a part of the ground motion data, it can also identify the responses of the structural system well. In this way the safeness of the structural systems may be predicted in case of future earthquakes without waiting for the earthquake to occur for the lessons. Time period and the corresponding maximum response of the building for an earthquake has been evaluated, which is again trained to predict the maximum response of the building at different time periods. The trained time period versus maximum response ANN model is also tested for real earthquake data of other place, which was not used in the training and was found to be in good agreement.",
        "published": "2007-05-15T20:29:06Z",
        "link": "http://arxiv.org/abs/0705.2235v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Fault Classification using Pseudomodal Energies and Neuro-fuzzy   modelling",
        "authors": [
            "Tshilidzi Marwala",
            "Thando Tettey",
            "Snehashish Chakraverty"
        ],
        "summary": "This paper presents a fault classification method which makes use of a Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the vibration signals of cylindrical shells. The calculation of Pseudomodal Energies, for the purposes of condition monitoring, has previously been found to be an accurate method of extracting features from vibration signals. This calculation is therefore used to extract features from vibration signals obtained from a diverse population of cylindrical shells. Some of the cylinders in the population have faults in different substructures. The pseudomodal energies calculated from the vibration signals are then used as inputs to a neuro-fuzzy model. A leave-one-out cross-validation process is used to test the performance of the model. It is found that the neuro-fuzzy model is able to classify faults with an accuracy of 91.62%, which is higher than the previously used multilayer perceptron.",
        "published": "2007-05-15T20:34:05Z",
        "link": "http://arxiv.org/abs/0705.2236v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Fuzzy and Multilayer Perceptron for Evaluation of HV Bushings",
        "authors": [
            "Sizwe M. Dhlamini",
            "Tshilidzi Marwala",
            "Thokozani Majozi"
        ],
        "summary": "The work proposes the application of fuzzy set theory (FST) to diagnose the condition of high voltage bushings. The diagnosis uses dissolved gas analysis (DGA) data from bushings based on IEC60599 and IEEE C57-104 criteria for oil impregnated paper (OIP) bushings. FST and neural networks are compared in terms of accuracy and computational efficiency. Both FST and NN simulations were able to diagnose the bushings condition with 10% error. By using fuzzy theory, the maintenance department can classify bushings and know the extent of degradation in the component.",
        "published": "2007-05-16T09:06:19Z",
        "link": "http://arxiv.org/abs/0705.2305v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "A Study in a Hybrid Centralised-Swarm Agent Community",
        "authors": [
            "Bradley van Aardt",
            "Tshilidzi Marwala"
        ],
        "summary": "This paper describes a systems architecture for a hybrid Centralised/Swarm based multi-agent system. The issue of local goal assignment for agents is investigated through the use of a global agent which teaches the agents responses to given situations. We implement a test problem in the form of a Pursuit game, where the Multi-Agent system is a set of captor agents. The agents learn solutions to certain board positions from the global agent if they are unable to find a solution. The captor agents learn through the use of multi-layer perceptron neural networks. The global agent is able to solve board positions through the use of a Genetic Algorithm. The cooperation between agents and the results of the simulation are discussed here. .",
        "published": "2007-05-16T09:12:09Z",
        "link": "http://arxiv.org/abs/0705.2307v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "On-Line Condition Monitoring using Computational Intelligence",
        "authors": [
            "C. B. Vilakazi",
            "T. Marwala",
            "P. Mautla",
            "E. Moloto"
        ],
        "summary": "This paper presents bushing condition monitoring frameworks that use multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector machines (SVM) classifiers. The first level of the framework determines if the bushing is faulty or not while the second level determines the type of fault. The diagnostic gases in the bushings are analyzed using the dissolve gas analysis. MLP gives superior performance in terms of accuracy and training time than SVM and RBF. In addition, an on-line bushing condition monitoring approach, which is able to adapt to newly acquired data are introduced. This approach is able to accommodate new classes that are introduced by incoming data and is implemented using an incremental learning algorithm that uses MLP. The testing results improved from 67.5% to 95.8% as new data were introduced and the testing results improved from 60% to 95.3% as new conditions were introduced. On average the confidence value of the framework on its decision was 0.92.",
        "published": "2007-05-16T09:19:00Z",
        "link": "http://arxiv.org/abs/0705.2310v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Using Genetic Algorithms to Optimise Rough Set Partition Sizes for HIV   Data Analysis",
        "authors": [
            "Bodie Crossingham",
            "Tshilidzi Marwala"
        ],
        "summary": "In this paper, we present a method to optimise rough set partition sizes, to which rule extraction is performed on HIV data. The genetic algorithm optimisation technique is used to determine the partition sizes of a rough set in order to maximise the rough sets prediction accuracy. The proposed method is tested on a set of demographic properties of individuals obtained from the South African antenatal survey. Six demographic variables were used in the analysis, these variables are; race, age of mother, education, gravidity, parity, and age of father, with the outcome or decision being either HIV positive or negative. Rough set theory is chosen based on the fact that it is easy to interpret the extracted rules. The prediction accuracy of equal width bin partitioning is 57.7% while the accuracy achieved after optimising the partitions is 72.8%. Several other methods have been used to analyse the HIV data and their results are stated and compared to that of rough set theory (RST).",
        "published": "2007-05-17T07:02:23Z",
        "link": "http://arxiv.org/abs/0705.2485v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "q-bio.QM"
        ]
    },
    {
        "title": "Condition Monitoring of HV Bushings in the Presence of Missing Data   Using Evolutionary Computing",
        "authors": [
            "Sizwe M. Dhlamini*",
            "Fulufhelo V. Nelwamondo**",
            "Tshilidzi Marwala**"
        ],
        "summary": "The work proposes the application of neural networks with particle swarm optimisation (PSO) and genetic algorithms (GA) to compensate for missing data in classifying high voltage bushings. The classification is done using DGA data from 60966 bushings based on IEEEc57.104, IEC599 and IEEE production rates methods for oil impregnated paper (OIP) bushings. PSO and GA were compared in terms of accuracy and computational efficiency. Both GA and PSO simulations were able to estimate missing data values to an average 95% accuracy when only one variable was missing. However PSO rapidly deteriorated to 66% accuracy with two variables missing simultaneously, compared to 84% for GA. The data estimated using GA was found to classify the conditions of bushings than the PSO.",
        "published": "2007-05-17T11:33:34Z",
        "link": "http://arxiv.org/abs/0705.2516v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "On the monotonization of the training set",
        "authors": [
            "Rustem Takhanov"
        ],
        "summary": "We consider the problem of minimal correction of the training set to make it consistent with monotonic constraints. This problem arises during analysis of data sets via techniques that require monotone data. We show that this problem is NP-hard in general and is equivalent to finding a maximal independent set in special orgraphs. Practically important cases of that problem considered in detail. These are the cases when a partial order given on the replies set is a total order or has a dimension 2. We show that the second case can be reduced to maximization of a quadratic convex function on a convex set. For this case we construct an approximate polynomial algorithm based on convex optimization.",
        "published": "2007-05-18T19:44:19Z",
        "link": "http://arxiv.org/abs/0705.2765v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "The Road to Quantum Artificial Intelligence",
        "authors": [
            "Kyriakos N. Sgarbas"
        ],
        "summary": "This paper overviews the basic principles and recent advances in the emerging field of Quantum Computation (QC), highlighting its potential application to Artificial Intelligence (AI). The paper provides a very brief introduction to basic QC issues like quantum registers, quantum gates and quantum algorithms and then it presents references, ideas and research guidelines on how QC can be used to deal with some basic AI problems, such as search and pattern matching, as soon as quantum computers become widely available.",
        "published": "2007-05-23T12:31:47Z",
        "link": "http://arxiv.org/abs/0705.3360v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Generalizing Consistency and other Constraint Properties to Quantified   Constraints",
        "authors": [
            "Lucas Bordeaux",
            "Marco Cadoli",
            "Toni Mancini"
        ],
        "summary": "Quantified constraints and Quantified Boolean Formulae are typically much more difficult to reason with than classical constraints, because quantifier alternation makes the usual notion of solution inappropriate. As a consequence, basic properties of Constraint Satisfaction Problems (CSP), such as consistency or substitutability, are not completely understood in the quantified case. These properties are important because they are the basis of most of the reasoning methods used to solve classical (existentially quantified) constraints, and one would like to benefit from similar reasoning methods in the resolution of quantified constraints. In this paper, we show that most of the properties that are used by solvers for CSP can be generalized to quantified CSP. This requires a re-thinking of a number of basic concepts; in particular, we propose a notion of outcome that generalizes the classical notion of solution and on which all definitions are based. We propose a systematic study of the relations which hold between these properties, as well as complexity results regarding the decision of these properties. Finally, and since these problems are typically intractable, we generalize the approach used in CSP and propose weaker, easier to check notions based on locality, which allow to detect these properties incompletely but in polynomial time.",
        "published": "2007-05-24T11:27:55Z",
        "link": "http://arxiv.org/abs/0705.3561v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "On complexity of optimized crossover for binary representations",
        "authors": [
            "Anton Eremeev"
        ],
        "summary": "We consider the computational complexity of producing the best possible offspring in a crossover, given two solutions of the parents. The crossover operators are studied on the class of Boolean linear programming problems, where the Boolean vector of variables is used as the solution representation. By means of efficient reductions of the optimized gene transmitting crossover problems (OGTC) we show the polynomial solvability of the OGTC for the maximum weight set packing problem, the minimum weight set partition problem and for one of the versions of the simple plant location problem. We study a connection between the OGTC for linear Boolean programming problem and the maximum weight independent set problem on 2-colorable hypergraph and prove the NP-hardness of several special cases of the OGTC problem in Boolean linear programming.",
        "published": "2007-05-25T13:07:18Z",
        "link": "http://arxiv.org/abs/0705.3766v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "Truecluster matching",
        "authors": [
            "Jens Oehlschlägel"
        ],
        "summary": "Cluster matching by permuting cluster labels is important in many clustering contexts such as cluster validation and cluster ensemble techniques. The classic approach is to minimize the euclidean distance between two cluster solutions which induces inappropriate stability in certain settings. Therefore, we present the truematch algorithm that introduces two improvements best explained in the crisp case. First, instead of maximizing the trace of the cluster crosstable, we propose to maximize a chi-square transformation of this crosstable. Thus, the trace will not be dominated by the cells with the largest counts but by the cells with the most non-random observations, taking into account the marginals. Second, we suggest a probabilistic component in order to break ties and to make the matching algorithm truly random on random data. The truematch algorithm is designed as a building block of the truecluster framework and scales in polynomial time. First simulation results confirm that the truematch algorithm gives more consistent truecluster results for unequal cluster sizes. Free R software is available.",
        "published": "2007-05-29T21:52:17Z",
        "link": "http://arxiv.org/abs/0705.4302v1",
        "categories": [
            "cs.AI",
            "G.3; I.5.3"
        ]
    },
    {
        "title": "Loop corrections for message passing algorithms in continuous variable   models",
        "authors": [
            "Bastian Wemmenhove",
            "Bert Kappen"
        ],
        "summary": "In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model. Using the exactness of the averages for belief propagation for Gaussian models, a different way of obtaining the covariances is found, based on Belief Propagation on cavity graphs. We discuss the relation of this loop correction algorithm to Expectation Propagation algorithms for the case in which the model is no longer Gaussian, but slightly perturbed by nonlinear terms.",
        "published": "2007-05-31T10:35:07Z",
        "link": "http://arxiv.org/abs/0705.4566v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Modeling Epidemic Spread in Synthetic Populations - Virtual Plagues in   Massively Multiplayer Online Games",
        "authors": [
            "Magnus Boman",
            "Stefan J. Johansson"
        ],
        "summary": "A virtual plague is a process in which a behavior-affecting property spreads among characters in a Massively Multiplayer Online Game (MMOG). The MMOG individuals constitute a synthetic population, and the game can be seen as a form of interactive executable model for studying disease spread, albeit of a very special kind. To a game developer maintaining an MMOG, recognizing, monitoring, and ultimately controlling a virtual plague is important, regardless of how it was initiated. The prospect of using tools, methods and theory from the field of epidemiology to do this seems natural and appealing. We will address the feasibility of such a prospect, first by considering some basic measures used in epidemiology, then by pointing out the differences between real world epidemics and virtual plagues. We also suggest directions for MMOG developer control through epidemiological modeling. Our aim is understanding the properties of virtual plagues, rather than trying to eliminate them or mitigate their effects, as would be in the case of real infectious disease.",
        "published": "2007-05-31T12:15:05Z",
        "link": "http://arxiv.org/abs/0705.4584v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.MA",
            "I.2.1"
        ]
    },
    {
        "title": "Modeling Computations in a Semantic Network",
        "authors": [
            "Marko A. Rodriguez",
            "Johan Bollen"
        ],
        "summary": "Semantic network research has seen a resurgence from its early history in the cognitive sciences with the inception of the Semantic Web initiative. The Semantic Web effort has brought forth an array of technologies that support the encoding, storage, and querying of the semantic network data structure at the world stage. Currently, the popular conception of the Semantic Web is that of a data modeling medium where real and conceptual entities are related in semantically meaningful ways. However, new models have emerged that explicitly encode procedural information within the semantic network substrate. With these new technologies, the Semantic Web has evolved from a data modeling medium to a computational medium. This article provides a classification of existing computational modeling efforts and the requirements of supporting technologies that will aid in the further growth of this burgeoning domain.",
        "published": "2007-05-31T21:56:25Z",
        "link": "http://arxiv.org/abs/0706.0022v1",
        "categories": [
            "cs.AI",
            "I.2.12; I.2.4.k"
        ]
    },
    {
        "title": "Virtual Sensor Based Fault Detection and Classification on a Plasma Etch   Reactor",
        "authors": [
            "D. A. Sofge"
        ],
        "summary": "The SEMATECH sponsored J-88-E project teaming Texas Instruments with NeuroDyne (et al.) focused on Fault Detection and Classification (FDC) on a Lam 9600 aluminum plasma etch reactor, used in the process of semiconductor fabrication. Fault classification was accomplished by implementing a series of virtual sensor models which used data from real sensors (Lam Station sensors, Optical Emission Spectroscopy, and RF Monitoring) to predict recipe setpoints and wafer state characteristics. Fault detection and classification were performed by comparing predicted recipe and wafer state values with expected values. Models utilized include linear PLS, Polynomial PLS, and Neural Network PLS. Prediction of recipe setpoints based upon sensor data provides a capability for cross-checking that the machine is maintaining the desired setpoints. Wafer state characteristics such as Line Width Reduction and Remaining Oxide were estimated on-line using these same process sensors (Lam, OES, RFM). Wafer-to-wafer measurement of these characteristics in a production setting (where typically this information may be only sparsely available, if at all, after batch processing runs with numerous wafers have been completed) would provide important information to the operator that the process is or is not producing wafers within acceptable bounds of product quality. Production yield is increased, and correspondingly per unit cost is reduced, by providing the operator with the opportunity to adjust the process or machine before etching more wafers.",
        "published": "2007-06-04T15:55:27Z",
        "link": "http://arxiv.org/abs/0706.0465v1",
        "categories": [
            "cs.AI",
            "cs.CV"
        ]
    },
    {
        "title": "A Novel Model of Working Set Selection for SMO Decomposition Methods",
        "authors": [
            "Zhendong Zhao",
            "Lei Yuan",
            "Yuxuan Wang",
            "Forrest Sheng Bao",
            "Shunyi Zhang Yanfei Sun"
        ],
        "summary": "In the process of training Support Vector Machines (SVMs) by decomposition methods, working set selection is an important technique, and some exciting schemes were employed into this field. To improve working set selection, we propose a new model for working set selection in sequential minimal optimization (SMO) decomposition methods. In this model, it selects B as working set without reselection. Some properties are given by simple proof, and experiments demonstrate that the proposed method is in general faster than existing methods.",
        "published": "2007-06-05T05:55:07Z",
        "link": "http://arxiv.org/abs/0706.0585v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Epistemic Analysis of Strategic Games with Arbitrary Strategy Sets",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "We provide here an epistemic analysis of arbitrary strategic games based on the possibility correspondences. Such an analysis calls for the use of transfinite iterations of the corresponding operators. Our approach is based on Tarski's Fixpoint Theorem and applies both to the notions of rationalizability and the iterated elimination of strictly dominated strategies.",
        "published": "2007-06-07T12:57:21Z",
        "link": "http://arxiv.org/abs/0706.1001v1",
        "categories": [
            "cs.GT",
            "cs.AI"
        ]
    },
    {
        "title": "Automatically Restructuring Practice Guidelines using the GEM DTD",
        "authors": [
            "Amanda Bouffier",
            "Thierry Poibeau"
        ],
        "summary": "This paper describes a system capable of semi-automatically filling an XML template from free texts in the clinical domain (practice guidelines). The XML template includes semantic information not explicitly encoded in the text (pairs of conditions and actions/recommendations). Therefore, there is a need to compute the exact scope of conditions over text sequences expressing the required actions. We present a system developed for this task. We show that it yields good performance when applied to the analysis of French practice guidelines.",
        "published": "2007-06-08T15:39:49Z",
        "link": "http://arxiv.org/abs/0706.1137v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Temporal Reasoning without Transitive Tables",
        "authors": [
            "Sylviane R. Schwer"
        ],
        "summary": "Representing and reasoning about qualitative temporal information is an essential part of many artificial intelligence tasks. Lots of models have been proposed in the litterature for representing such temporal information. All derive from a point-based or an interval-based framework. One fundamental reasoning task that arises in applications of these frameworks is given by the following scheme: given possibly indefinite and incomplete knowledge of the binary relationships between some temporal objects, find the consistent scenarii between all these objects. All these models require transitive tables -- or similarly inference rules-- for solving such tasks. We have defined an alternative model, S-languages - to represent qualitative temporal information, based on the only two relations of \\emph{precedence} and \\emph{simultaneity}. In this paper, we show how this model enables to avoid transitive tables or inference rules to handle this kind of problem.",
        "published": "2007-06-09T06:57:05Z",
        "link": "http://arxiv.org/abs/0706.1290v1",
        "categories": [
            "cs.AI",
            "F.4.3"
        ]
    },
    {
        "title": "A Collection of Definitions of Intelligence",
        "authors": [
            "Shane Legg",
            "Marcus Hutter"
        ],
        "summary": "This paper is a survey of a large number of informal definitions of ``intelligence'' that the authors have collected over the years. Naturally, compiling a complete list would be impossible as many definitions of intelligence are buried deep inside articles and books. Nevertheless, the 70-odd definitions presented here are, to the authors' knowledge, the largest and most well referenced collection there is.",
        "published": "2007-06-25T13:40:56Z",
        "link": "http://arxiv.org/abs/0706.3639v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Theory of Finite or Infinite Trees Revisited",
        "authors": [
            "Khalil Djelloul",
            "Thi-bich-hanh Dao",
            "Thom Fruehwirth"
        ],
        "summary": "We present in this paper a first-order axiomatization of an extended theory $T$ of finite or infinite trees, built on a signature containing an infinite set of function symbols and a relation $\\fini(t)$ which enables to distinguish between finite or infinite trees. We show that $T$ has at least one model and prove its completeness by giving not only a decision procedure, but a full first-order constraint solver which gives clear and explicit solutions for any first-order constraint satisfaction problem in $T$. The solver is given in the form of 16 rewriting rules which transform any first-order constraint $\\phi$ into an equivalent disjunction $\\phi$ of simple formulas such that $\\phi$ is either the formula $\\true$ or the formula $\\false$ or a formula having at least one free variable, being equivalent neither to $\\true$ nor to $\\false$ and where the solutions of the free variables are expressed in a clear and explicit way. The correctness of our rules implies the completeness of $T$. We also describe an implementation of our algorithm in CHR (Constraint Handling Rules) and compare the performance with an implementation in C++ and that of a recent decision procedure for decomposable theories.",
        "published": "2007-06-28T21:18:19Z",
        "link": "http://arxiv.org/abs/0706.4323v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "A Robust Linguistic Platform for Efficient and Domain specific Web   Content Analysis",
        "authors": [
            "Thierry Hamon",
            "Adeline Nazarenko",
            "Thierry Poibeau",
            "Sophie Aubin",
            "Julien Derivière"
        ],
        "summary": "Web semantic access in specific domains calls for specialized search engines with enhanced semantic querying and indexing capacities, which pertain both to information retrieval (IR) and to information extraction (IE). A rich linguistic analysis is required either to identify the relevant semantic units to index and weight them according to linguistic specific statistical distribution, or as the basis of an information extraction process. Recent developments make Natural Language Processing (NLP) techniques reliable enough to process large collections of documents and to enrich them with semantic annotations. This paper focuses on the design and the development of a text processing platform, Ogmios, which has been developed in the ALVIS project. The Ogmios platform exploits existing NLP modules and resources, which may be tuned to specific domains and produces linguistically annotated documents. We show how the three constraints of genericity, domain semantic awareness and performance can be handled all together.",
        "published": "2007-06-29T08:58:02Z",
        "link": "http://arxiv.org/abs/0706.4375v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "The Role of Time in the Creation of Knowledge",
        "authors": [
            "Roy E. Murphy"
        ],
        "summary": "This paper I assume that in humans the creation of knowledge depends on a discrete time, or stage, sequential decision-making process subjected to a stochastic, information transmitting environment. For each time-stage, this environment randomly transmits Shannon type information-packets to the decision-maker, who examines each of them for relevancy and then determines his optimal choices. Using this set of relevant information-packets, the decision-maker adapts, over time, to the stochastic nature of his environment, and optimizes the subjective expected rate-of-growth of knowledge. The decision-maker's optimal actions, lead to a decision function that involves, over time, his view of the subjective entropy of the environmental process and other important parameters at each time-stage of the process. Using this model of human behavior, one could create psychometric experiments using computer simulation and real decision-makers, to play programmed games to measure the resulting human performance.",
        "published": "2007-07-03T20:43:43Z",
        "link": "http://arxiv.org/abs/0707.0498v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Clustering and Feature Selection using Sparse Principal Component   Analysis",
        "authors": [
            "Ronny Luss",
            "Alexandre d'Aspremont"
        ],
        "summary": "In this paper, we study the application of sparse principal component analysis (PCA) to clustering and feature selection problems. Sparse PCA seeks sparse factors, or linear combinations of the data variables, explaining a maximum amount of variance in the data while having only a limited number of nonzero coefficients. PCA is often used as a simple clustering technique and sparse factors allow us here to interpret the clusters in terms of a reduced set of variables. We begin with a brief introduction and motivation on sparse PCA and detail our implementation of the algorithm in d'Aspremont et al. (2005). We then apply these results to some classic clustering and feature selection problems arising in biology.",
        "published": "2007-07-04T21:53:11Z",
        "link": "http://arxiv.org/abs/0707.0701v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.MS"
        ]
    },
    {
        "title": "Model Selection Through Sparse Maximum Likelihood Estimation",
        "authors": [
            "Onureena Banerjee",
            "Laurent El Ghaoui",
            "Alexandre d'Aspremont"
        ],
        "summary": "We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added l_1-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our first algorithm uses block coordinate descent, and can be interpreted as recursive l_1-norm penalized regression. Our second algorithm, based on Nesterov's first order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright & Jordan (2006)), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data.",
        "published": "2007-07-04T22:13:42Z",
        "link": "http://arxiv.org/abs/0707.0704v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Optimal Solutions for Sparse Principal Component Analysis",
        "authors": [
            "Alexandre d'Aspremont",
            "Francis Bach",
            "Laurent El Ghaoui"
        ],
        "summary": "Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefficients, with total complexity O(n^3), where n is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in O(n^3) per pattern. We discuss applications in subset selection and sparse recovery and show on artificial examples and biological data that our algorithm does provide globally optimal solutions in many cases.",
        "published": "2007-07-04T22:28:28Z",
        "link": "http://arxiv.org/abs/0707.0705v4",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the   Astrobiology Phone-cam",
        "authors": [
            "Alexandra Bartolo",
            "Patrick C. McGuire",
            "Kenneth P. Camilleri",
            "Christopher Spiteri",
            "Jonathan C. Borg",
            "Philip J. Farrugia",
            "Jens Ormo",
            "Javier Gomez-Elvira",
            "Jose Antonio Rodriguez-Manfredi",
            "Enrique Diaz-Martinez",
            "Helge Ritter",
            "Robert Haschke",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "We have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. This camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. We envision that the `Astrobiology Phone-cam' exploration system can be fruitfully used in other problem domains as well.",
        "published": "2007-07-05T15:19:37Z",
        "link": "http://arxiv.org/abs/0707.0808v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.NI",
            "cs.RO",
            "cs.SE"
        ]
    },
    {
        "title": "Clusters, Graphs, and Networks for Analysing Internet-Web-Supported   Communication within a Virtual Community",
        "authors": [
            "Xavier Polanco"
        ],
        "summary": "The proposal is to use clusters, graphs and networks as models in order to analyse the Web structure. Clusters, graphs and networks provide knowledge representation and organization. Clusters were generated by co-site analysis. The sample is a set of academic Web sites from the countries belonging to the European Union. These clusters are here revisited from the point of view of graph theory and social network analysis. This is a quantitative and structural analysis. In fact, the Internet is a computer network that connects people and organizations. Thus we may consider it to be a social network. The set of Web academic sites represents an empirical social network, and is viewed as a virtual community. The network structural properties are here analysed applying together cluster analysis, graph theory and social network analysis.",
        "published": "2007-07-10T13:47:32Z",
        "link": "http://arxiv.org/abs/0707.1452v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.5.3"
        ]
    },
    {
        "title": "Mixed Integer Linear Programming For Exact Finite-Horizon Planning In   Decentralized Pomdps",
        "authors": [
            "Raghav Aras",
            "Alain Dutech",
            "François Charpillet"
        ],
        "summary": "We consider the problem of finding an n-agent joint-policy for the optimal finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem of very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new mathematical programming approach for the problem. Our approach is based on two ideas: First, we represent each agent's policy in the sequence-form and not in the tree-form, thereby obtaining a very compact representation of the set of joint-policies. Second, using this compact representation, we solve this problem as an instance of combinatorial optimization for which we formulate a mixed integer linear program (MILP). The optimal solution of the MILP directly yields an optimal joint-policy for the Dec-Pomdp. Computational experience shows that formulating and solving the MILP requires significantly less time to solve benchmark Dec-Pomdp problems than existing algorithms. For example, the multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP whereas existing algorithms require several hours to solve it.",
        "published": "2007-07-17T12:49:30Z",
        "link": "http://arxiv.org/abs/0707.2506v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Optimal Design of Ad Hoc Injection Networks by Using Genetic Algorithms",
        "authors": [
            "Gregoire Danoy",
            "Pascal Bouvry",
            "Matthias R. Brust",
            "Enrique Alba"
        ],
        "summary": "This work aims at optimizing injection networks, which consist in adding a set of long-range links (called bypass links) in mobile multi-hop ad hoc networks so as to improve connectivity and overcome network partitioning. To this end, we rely on small-world network properties, that comprise a high clustering coefficient and a low characteristic path length. We investigate the use of two genetic algorithms (generational and steady-state) to optimize three instances of this topology control problem and present results that show initial evidence of their capacity to solve it.",
        "published": "2007-07-20T10:07:27Z",
        "link": "http://arxiv.org/abs/0707.3030v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.NI",
            "I.2.8"
        ]
    },
    {
        "title": "Neutrality and Many-Valued Logics",
        "authors": [
            "Andrew Schumann",
            "Florentin Smarandache"
        ],
        "summary": "In this book, we consider various many-valued logics: standard, linear, hyperbolic, parabolic, non-Archimedean, p-adic, interval, neutrosophic, etc. We survey also results which show the tree different proof-theoretic frameworks for many-valued logics, e.g. frameworks of the following deductive calculi: Hilbert's style, sequent, and hypersequent. We present a general way that allows to construct systematically analytic calculi for a large family of non-Archimedean many-valued logics: hyperrational-valued, hyperreal-valued, and p-adic valued logics characterized by a special format of semantics with an appropriate rejection of Archimedes' axiom. These logics are built as different extensions of standard many-valued logics (namely, Lukasiewicz's, Goedel's, Product, and Post's logics). The informal sense of Archimedes' axiom is that anything can be measured by a ruler. Also logical multiple-validity without Archimedes' axiom consists in that the set of truth values is infinite and it is not well-founded and well-ordered. On the base of non-Archimedean valued logics, we construct non-Archimedean valued interval neutrosophic logic INL by which we can describe neutrality phenomena.",
        "published": "2007-07-21T10:35:37Z",
        "link": "http://arxiv.org/abs/0707.3205v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "A Generalized Information Formula as the Bridge between Shannon and   Popper",
        "authors": [
            "Chenguang Lu"
        ],
        "summary": "A generalized information formula related to logical probability and fuzzy set is deduced from the classical information formula. The new information measure accords with to Popper's criterion for knowledge evolution very much. In comparison with square error criterion, the information criterion does not only reflect error of a proposition, but also reflects the particularity of the event described by the proposition. It gives a proposition with less logical probability higher evaluation. The paper introduces how to select a prediction or sentence from many for forecasts and language translations according to the generalized information criterion. It also introduces the rate fidelity theory, which comes from the improvement of the rate distortion theory in the classical information theory by replacing distortion (i.e. average error) criterion with the generalized mutual information criterion, for data compression and communication efficiency. Some interesting conclusions are obtained from the rate-fidelity function in relation to image communication. It also discusses how to improve Popper's theory.",
        "published": "2007-07-24T00:04:32Z",
        "link": "http://arxiv.org/abs/0707.3457v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT",
            "H.1.1; H.1.2; I.4.2; I.5.0"
        ]
    },
    {
        "title": "Practical Approach to Knowledge-based Question Answering with Natural   Language Understanding and Advanced Reasoning",
        "authors": [
            "Wilson Wong"
        ],
        "summary": "This research hypothesized that a practical approach in the form of a solution framework known as Natural Language Understanding and Reasoning for Intelligence (NaLURI), which combines full-discourse natural language understanding, powerful representation formalism capable of exploiting ontological information and reasoning approach with advanced features, will solve the following problems without compromising practicality factors: 1) restriction on the nature of question and response, and 2) limitation to scale across domains and to real-life natural language text.",
        "published": "2007-07-24T14:30:27Z",
        "link": "http://arxiv.org/abs/0707.3559v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.IR",
            "I.2.7; H.5.2; H.3.4; H.3.3"
        ]
    },
    {
        "title": "Bijective Faithful Translations among Default Logics",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "In this article, we study translations between variants of defaults logics such that the extensions of the theories that are the input and the output of the translation are in a bijective correspondence. We assume that a translation can introduce new variables and that the result of translating a theory can either be produced in time polynomial in the size of the theory or its output is polynomial in that size; we however restrict to the case in which the original theory has extensions. This study fills a gap between two previous pieces of work, one studying bijective translations among restrictions of default logics, and the other one studying non-bijective translations between default logics variants.",
        "published": "2007-07-25T17:03:57Z",
        "link": "http://arxiv.org/abs/0707.3781v2",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Learning Probabilistic Models of Word Sense Disambiguation",
        "authors": [
            "Ted Pedersen"
        ],
        "summary": "This dissertation presents several new methods of supervised and unsupervised learning of word sense disambiguation models. The supervised methods focus on performing model searches through a space of probabilistic models, and the unsupervised methods rely on the use of Gibbs Sampling and the Expectation Maximization (EM) algorithm. In both the supervised and unsupervised case, the Naive Bayesian model is found to perform well. An explanation for this success is presented in terms of learning rates and bias-variance decompositions.",
        "published": "2007-07-26T17:02:40Z",
        "link": "http://arxiv.org/abs/0707.3972v1",
        "categories": [
            "cs.CL",
            "cs.AI"
        ]
    },
    {
        "title": "A Leaf Recognition Algorithm for Plant Classification Using   Probabilistic Neural Network",
        "authors": [
            "Stephen Gang Wu",
            "Forrest Sheng Bao",
            "Eric You Xu",
            "Yu-Xuan Wang",
            "Yi-Fan Chang",
            "Qiao-Liang Xiang"
        ],
        "summary": "In this paper, we employ Probabilistic Neural Network (PNN) with image and data processing techniques to implement a general purpose automated leaf recognition algorithm. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the PNN. The PNN is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90%. Compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation.",
        "published": "2007-07-29T12:31:40Z",
        "link": "http://arxiv.org/abs/0707.4289v1",
        "categories": [
            "cs.AI",
            "I.5.4"
        ]
    },
    {
        "title": "A preliminary analysis on metaheuristics methods applied to the   Haplotype Inference Problem",
        "authors": [
            "Luca Di Gaspero",
            "Andrea Roli"
        ],
        "summary": "Haplotype Inference is a challenging problem in bioinformatics that consists in inferring the basic genetic constitution of diploid organisms on the basis of their genotype. This information allows researchers to perform association studies for the genetic variants involved in diseases and the individual responses to therapeutic agents.   A notable approach to the problem is to encode it as a combinatorial problem (under certain hypotheses, such as the pure parsimony criterion) and to solve it using off-the-shelf combinatorial optimization techniques. The main methods applied to Haplotype Inference are either simple greedy heuristic or exact methods (Integer Linear Programming, Semidefinite Programming, SAT encoding) that, at present, are adequate only for moderate size instances.   We believe that metaheuristic and hybrid approaches could provide a better scalability. Moreover, metaheuristics can be very easily combined with problem specific heuristics and they can also be integrated with tree-based search techniques, thus providing a promising framework for hybrid systems in which a good trade-off between effectiveness and efficiency can be reached.   In this paper we illustrate a feasibility study of the approach and discuss some relevant design issues, such as modeling and design of approximate solvers that combine constructive heuristics, local search-based improvement strategies and learning mechanisms. Besides the relevance of the Haplotype Inference problem itself, this preliminary analysis is also an interesting case study because the formulation of the problem poses some challenges in modeling and hybrid metaheuristic solver design that can be generalized to other problems.",
        "published": "2007-08-03T12:49:21Z",
        "link": "http://arxiv.org/abs/0708.0505v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.DM",
            "q-bio.QM",
            "I.2.8; J.3.x; F.2.2"
        ]
    },
    {
        "title": "Modeling Visual Information Processing in Brain: A Computer Vision Point   of View and Approach",
        "authors": [
            "Emanuel Diamant"
        ],
        "summary": "We live in the Information Age, and information has become a critically important component of our life. The success of the Internet made huge amounts of it easily available and accessible to everyone. To keep the flow of this information manageable, means for its faultless circulation and effective handling have become urgently required. Considerable research efforts are dedicated today to address this necessity, but they are seriously hampered by the lack of a common agreement about \"What is information?\" In particular, what is \"visual information\" - human's primary input from the surrounding world. The problem is further aggravated by a long-lasting stance borrowed from the biological vision research that assumes human-like information processing as an enigmatic mix of perceptual and cognitive vision faculties. I am trying to find a remedy for this bizarre situation. Relying on a new definition of \"information\", which can be derived from Kolmogorov's compexity theory and Chaitin's notion of algorithmic information, I propose a unifying framework for visual information processing, which explicitly accounts for the perceptual and cognitive image processing peculiarities. I believe that this framework will be useful to overcome the difficulties that are impeding our attempts to develop the right model of human-like intelligent image processing.",
        "published": "2007-08-07T11:16:15Z",
        "link": "http://arxiv.org/abs/0708.0927v1",
        "categories": [
            "cs.AI",
            "cs.CV"
        ]
    },
    {
        "title": "A Practical Ontology for the Large-Scale Modeling of Scholarly Artifacts   and their Usage",
        "authors": [
            "Marko A. Rodriguez",
            "Johah Bollen",
            "Herbert Van de Sompel"
        ],
        "summary": "The large-scale analysis of scholarly artifact usage is constrained primarily by current practices in usage data archiving, privacy issues concerned with the dissemination of usage data, and the lack of a practical ontology for modeling the usage domain. As a remedy to the third constraint, this article presents a scholarly ontology that was engineered to represent those classes for which large-scale bibliographic and usage data exists, supports usage research, and whose instantiation is scalable to the order of 50 million articles along with their associated artifacts (e.g. authors and journals) and an accompanying 1 billion usage events. The real world instantiation of the presented abstract ontology is a semantic network model of the scholarly community which lends the scholarly process to statistical analysis and computational support. We present the ontology, discuss its instantiation, and provide some example inference rules for calculating various scholarly artifact metrics.",
        "published": "2007-08-08T17:06:55Z",
        "link": "http://arxiv.org/abs/0708.1150v1",
        "categories": [
            "cs.DL",
            "cs.AI",
            "H.3.7; I.2.4"
        ]
    },
    {
        "title": "A Data-Parallel Version of Aleph",
        "authors": [
            "Stasinos Konstantopoulos"
        ],
        "summary": "This is to present work on modifying the Aleph ILP system so that it evaluates the hypothesised clauses in parallel by distributing the data-set among the nodes of a parallel or distributed machine. The paper briefly discusses MPI, the interface used to access message- passing libraries for parallel computers and clusters. It then proceeds to describe an extension of YAP Prolog with an MPI interface and an implementation of data-parallel clause evaluation for Aleph through this interface. The paper concludes by testing the data-parallel Aleph on artificially constructed data-sets.",
        "published": "2007-08-10T23:32:16Z",
        "link": "http://arxiv.org/abs/0708.1527v1",
        "categories": [
            "cs.AI",
            "cs.DC"
        ]
    },
    {
        "title": "Solving the subset-sum problem with a light-based device",
        "authors": [
            "Mihai Oltean",
            "Oana Muntean"
        ],
        "summary": "We propose a special computational device which uses light rays for solving the subset-sum problem. The device has a graph-like representation and the light is traversing it by following the routes given by the connections between nodes. The nodes are connected by arcs in a special way which lets us to generate all possible subsets of the given set. To each arc we assign either a number from the given set or a predefined constant. When the light is passing through an arc it is delayed by the amount of time indicated by the number placed in that arc. At the destination node we will check if there is a ray whose total delay is equal to the target value of the subset sum problem (plus some constants).",
        "published": "2007-08-14T21:46:32Z",
        "link": "http://arxiv.org/abs/0708.1964v1",
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.DC"
        ]
    },
    {
        "title": "Compositional Semantics Grounded in Commonsense Metaphysics",
        "authors": [
            "Walid S. Saba"
        ],
        "summary": "We argue for a compositional semantics grounded in a strongly typed ontology that reflects our commonsense view of the world and the way we talk about it in ordinary language. Assuming the existence of such a structure, we show that the semantics of various natural language phenomena may become nearly trivial.",
        "published": "2007-08-17T01:15:11Z",
        "link": "http://arxiv.org/abs/0708.2303v2",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "On Ullman's theorem in computer vision",
        "authors": [
            "Oliver Knill",
            "Jose Ramirez-Herran"
        ],
        "summary": "Both in the plane and in space, we invert the nonlinear Ullman transformation for 3 points and 3 orthographic cameras. While Ullman's theorem assures a unique reconstruction modulo a reflection for 3 cameras and 4 points, we find a locally unique reconstruction for 3 cameras and 3 points. Explicit reconstruction formulas allow to decide whether picture data of three cameras seeing three points can be realized as a point-camera configuration.",
        "published": "2007-08-17T21:36:08Z",
        "link": "http://arxiv.org/abs/0708.2438v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.2.10"
        ]
    },
    {
        "title": "Space and camera path reconstruction for omni-directional vision",
        "authors": [
            "Oliver Knill",
            "Jose Ramirez-Herran"
        ],
        "summary": "In this paper, we address the inverse problem of reconstructing a scene as well as the camera motion from the image sequence taken by an omni-directional camera. Our structure from motion results give sharp conditions under which the reconstruction is unique. For example, if there are three points in general position and three omni-directional cameras in general position, a unique reconstruction is possible up to a similarity. We then look at the reconstruction problem with m cameras and n points, where n and m can be large and the over-determined system is solved by least square methods. The reconstruction is robust and generalizes to the case of a dynamic environment where landmarks can move during the movie capture. Possible applications of the result are computer assisted scene reconstruction, 3D scanning, autonomous robot navigation, medical tomography and city reconstructions.",
        "published": "2007-08-17T21:53:41Z",
        "link": "http://arxiv.org/abs/0708.2442v1",
        "categories": [
            "cs.CV",
            "cs.AI"
        ]
    },
    {
        "title": "A structure from motion inequality",
        "authors": [
            "Oliver Knill",
            "Jose Ramirez-Herran"
        ],
        "summary": "We state an elementary inequality for the structure from motion problem for m cameras and n points. This structure from motion inequality relates space dimension, camera parameter dimension, the number of cameras and number points and global symmetry properties and provides a rigorous criterion for which reconstruction is not possible with probability 1. Mathematically the inequality is based on Frobenius theorem which is a geometric incarnation of the fundamental theorem of linear algebra. The paper also provides a general mathematical formalism for the structure from motion problem. It includes the situation the points can move while the camera takes the pictures.",
        "published": "2007-08-18T14:36:28Z",
        "link": "http://arxiv.org/abs/0708.2432v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.2.10"
        ]
    },
    {
        "title": "Raising a Hardness Result",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "This article presents a technique for proving problems hard for classes of the polynomial hierarchy or for PSPACE. The rationale of this technique is that some problem restrictions are able to simulate existential or universal quantifiers. If this is the case, reductions from Quantified Boolean Formulae (QBF) to these restrictions can be transformed into reductions from QBFs having one more quantifier in the front. This means that a proof of hardness of a problem at level n in the polynomial hierarchy can be split into n separate proofs, which may be simpler than a proof directly showing a reduction from a class of QBFs to the considered problem.",
        "published": "2007-08-30T14:42:50Z",
        "link": "http://arxiv.org/abs/0708.4170v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "2006: Celebrating 75 years of AI - History and Outlook: the Next 25   Years",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "When Kurt Goedel layed the foundations of theoretical computer science in 1931, he also introduced essential concepts of the theory of Artificial Intelligence (AI). Although much of subsequent AI research has focused on heuristics, which still play a major role in many practical AI applications, in the new millennium AI theory has finally become a full-fledged formal science, with important optimality results for embodied agents living in unknown environments, obtained through a combination of theory a la Goedel and probability theory. Here we look back at important milestones of AI history, mention essential recent results, and speculate about what we may expect from the next 25 years, emphasizing the significance of the ongoing dramatic hardware speedups, and discussing Goedel-inspired, self-referential, self-improving universal problem solvers.",
        "published": "2007-08-31T11:12:26Z",
        "link": "http://arxiv.org/abs/0708.4311v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "On Ultrametric Algorithmic Information",
        "authors": [
            "Fionn Murtagh"
        ],
        "summary": "How best to quantify the information of an object, whether natural or artifact, is a problem of wide interest. A related problem is the computability of an object. We present practical examples of a new way to address this problem. By giving an appropriate representation to our objects, based on a hierarchical coding of information, we exemplify how it is remarkably easy to compute complex objects. Our algorithmic complexity is related to the length of the class of objects, rather than to the length of the object.",
        "published": "2007-09-02T17:00:40Z",
        "link": "http://arxiv.org/abs/0709.0116v2",
        "categories": [
            "cs.AI",
            "cs.CL",
            "I.2.0"
        ]
    },
    {
        "title": "Effective Generation of Subjectively Random Binary Sequences",
        "authors": [
            "Yasmine B. Sanderson"
        ],
        "summary": "We present an algorithm for effectively generating binary sequences which would be rated by people as highly likely to have been generated by a random process, such as flipping a fair coin.",
        "published": "2007-09-03T09:32:28Z",
        "link": "http://arxiv.org/abs/0709.0178v2",
        "categories": [
            "cs.HC",
            "cs.AI",
            "I.2.10; I.6.8; J.4; G.3"
        ]
    },
    {
        "title": "Qualitative Belief Conditioning Rules (QBCR)",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "summary": "In this paper we extend the new family of (quantitative) Belief Conditioning Rules (BCR) recently developed in the Dezert-Smarandache Theory (DSmT) to their qualitative counterpart for belief revision. Since the revision of quantitative as well as qualitative belief assignment given the occurrence of a new event (the conditioning constraint) can be done in many possible ways, we present here only what we consider as the most appealing Qualitative Belief Conditioning Rules (QBCR) which allow to revise the belief directly with words and linguistic labels and thus avoids the introduction of ad-hoc translations of quantitative beliefs into quantitative ones for solving the problem.",
        "published": "2007-09-04T20:03:04Z",
        "link": "http://arxiv.org/abs/0709.0522v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Simple Algorithmic Principles of Discovery, Subjective Beauty, Selective   Attention, Curiosity & Creativity",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "I postulate that human or other intelligent agents function or should function as follows. They store all sensory observations as they come - the data is holy. At any time, given some agent's current coding capabilities, part of the data is compressible by a short and hopefully fast program / description / explanation / world model. In the agent's subjective eyes, such data is more regular and more \"beautiful\" than other data. It is well-known that knowledge of regularity and repeatability may improve the agent's ability to plan actions leading to external rewards. In absence of such rewards, however, known beauty is boring. Then \"interestingness\" becomes the first derivative of subjective beauty: as the learning agent improves its compression algorithm, formerly apparently random data parts become subjectively more regular and beautiful. Such progress in compressibility is measured and maximized by the curiosity drive: create action sequences that extend the observation history and yield previously unknown / unpredictable but quickly learnable algorithmic regularity. We discuss how all of the above can be naturally implemented on computers, through an extension of passive unsupervised learning to the case of active data selection: we reward a general reinforcement learner (with access to the adaptive compressor) for actions that improve the subjective compressibility of the growing data. An unusually large breakthrough in compressibility deserves the name \"discovery\". The \"creativity\" of artists, dancers, musicians, pure mathematicians can be viewed as a by-product of this principle. Several qualitative examples support this hypothesis.",
        "published": "2007-09-05T15:20:59Z",
        "link": "http://arxiv.org/abs/0709.0674v1",
        "categories": [
            "cs.AI",
            "cs.GR",
            "I.2.0"
        ]
    },
    {
        "title": "Multi-Sensor Fusion Method using Dynamic Bayesian Network for Precise   Vehicle Localization and Road Matching",
        "authors": [
            "Cherif Smaili",
            "Maan El Badaoui El Najjar",
            "François Charpillet"
        ],
        "summary": "This paper presents a multi-sensor fusion strategy for a novel road-matching method designed to support real-time navigational features within advanced driving-assistance systems. Managing multihypotheses is a useful strategy for the road-matching problem. The multi-sensor fusion and multi-modal estimation are realized using Dynamical Bayesian Network. Experimental results, using data from Antilock Braking System (ABS) sensors, a differential Global Positioning System (GPS) receiver and an accurate digital roadmap, illustrate the performances of this approach, especially in ambiguous situations.",
        "published": "2007-09-07T15:03:37Z",
        "link": "http://arxiv.org/abs/0709.1099v1",
        "categories": [
            "cs.AI",
            "cs.RO"
        ]
    },
    {
        "title": "Using RDF to Model the Structure and Process of Systems",
        "authors": [
            "Marko A. Rodriguez",
            "Jennifer H. Watkins",
            "Johan Bollen",
            "Carlos Gershenson"
        ],
        "summary": "Many systems can be described in terms of networks of discrete elements and their various relationships to one another. A semantic network, or multi-relational network, is a directed labeled graph consisting of a heterogeneous set of entities connected by a heterogeneous set of relationships. Semantic networks serve as a promising general-purpose modeling substrate for complex systems. Various standardized formats and tools are now available to support practical, large-scale semantic network models. First, the Resource Description Framework (RDF) offers a standardized semantic network data model that can be further formalized by ontology modeling languages such as RDF Schema (RDFS) and the Web Ontology Language (OWL). Second, the recent introduction of highly performant triple-stores (i.e. semantic network databases) allows semantic network models on the order of $10^9$ edges to be efficiently stored and manipulated. RDF and its related technologies are currently used extensively in the domains of computer science, digital library science, and the biological sciences. This article will provide an introduction to RDF/RDFS/OWL and an examination of its suitability to model discrete element complex systems.",
        "published": "2007-09-08T01:18:18Z",
        "link": "http://arxiv.org/abs/0709.1167v2",
        "categories": [
            "cs.AI",
            "I.6.0"
        ]
    },
    {
        "title": "Belief-Propagation for Weighted b-Matchings on Arbitrary Graphs and its   Relation to Linear Programs with Integer Solutions",
        "authors": [
            "Mohsen Bayati",
            "Christian Borgs",
            "Jennifer Chayes",
            "Riccardo Zecchina"
        ],
        "summary": "We consider the general problem of finding the minimum weight $\\bm$-matching on arbitrary graphs. We prove that, whenever the linear programming (LP) relaxation of the problem has no fractional solutions, then the belief propagation (BP) algorithm converges to the correct solution. We also show that when the LP relaxation has a fractional solution then the BP algorithm can be used to solve the LP relaxation. Our proof is based on the notion of graph covers and extends the analysis of (Bayati-Shah-Sharma 2005 and Huang-Jebara 2007}.   These results are notable in the following regards: (1) It is one of a very small number of proofs showing correctness of BP without any constraint on the graph structure. (2) Variants of the proof work for both synchronous and asynchronous BP; it is the first proof of convergence and correctness of an asynchronous BP algorithm for a combinatorial optimization problem.",
        "published": "2007-09-08T08:21:34Z",
        "link": "http://arxiv.org/abs/0709.1190v3",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT"
        ]
    },
    {
        "title": "Solving Constraint Satisfaction Problems through Belief   Propagation-guided decimation",
        "authors": [
            "Andrea Montanari",
            "Federico Ricci-Tersenghi",
            "Guilhem Semerjian"
        ],
        "summary": "Message passing algorithms have proved surprisingly successful in solving hard constraint satisfaction problems on sparse random graphs. In such applications, variables are fixed sequentially to satisfy the constraints. Message passing is run after each step. Its outcome provides an heuristic to make choices at next step. This approach has been referred to as `decimation,' with reference to analogous procedures in statistical physics.   The behavior of decimation procedures is poorly understood. Here we consider a simple randomized decimation algorithm based on belief propagation (BP), and analyze its behavior on random k-satisfiability formulae. In particular, we propose a tree model for its analysis and we conjecture that it provides asymptotically exact predictions in the limit of large instances. This conjecture is confirmed by numerical simulations.",
        "published": "2007-09-11T15:48:56Z",
        "link": "http://arxiv.org/abs/0709.1667v3",
        "categories": [
            "cs.AI",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Efficient Tabling Mechanisms for Transaction Logic Programs",
        "authors": [
            "Paul Fodor"
        ],
        "summary": "In this paper we present efficient evaluation algorithms for the Horn Transaction Logic (a generalization of the regular Horn logic programs with state updates). We present two complementary methods for optimizing the implementation of Transaction Logic. The first method is based on tabling and we modified the proof theory to table calls and answers on states (practically, equivalent to dynamic programming). The call-answer table is indexed on the call and a signature of the state in which the call was made. The answer columns contain the answer unification and a signature of the state after the call was executed. The states are signed efficiently using a technique based on tries and counting. The second method is based on incremental evaluation and it applies when the data oracle contains derived relations. The deletions and insertions (executed in the transaction oracle) change the state of the database. Using the heuristic of inertia (only a part of the state changes in response to elementary updates), most of the time it is cheaper to compute only the changes in the state than to recompute the entire state from scratch. The two methods are complementary by the fact that the first method optimizes the evaluation when a call is repeated in the same state, and the second method optimizes the evaluation of a new state when a call-state pair is not found by the tabling mechanism (i.e. the first method). The proof theory of Transaction Logic with the application of tabling and incremental evaluation is sound and complete with respect to its model theory.",
        "published": "2007-09-11T19:00:02Z",
        "link": "http://arxiv.org/abs/0709.1699v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Enrichment of Qualitative Beliefs for Reasoning under Uncertainty",
        "authors": [
            "Xinde Li",
            "Xinhan Huang",
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "summary": "This paper deals with enriched qualitative belief functions for reasoning under uncertainty and for combining information expressed in natural language through linguistic labels. In this work, two possible enrichments (quantitative and/or qualitative) of linguistic labels are considered and operators (addition, multiplication, division, etc) for dealing with them are proposed and explained. We denote them $qe$-operators, $qe$ standing for \"qualitative-enriched\" operators. These operators can be seen as a direct extension of the classical qualitative operators ($q$-operators) proposed recently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning (DSmT). $q$-operators are also justified in details in this paper. The quantitative enrichment of linguistic label is a numerical supporting degree in $[0,\\infty)$, while the qualitative enrichment takes its values in a finite ordered set of linguistic values. Quantitative enrichment is less precise than qualitative enrichment, but it is expected more close with what human experts can easily provide when expressing linguistic labels with supporting degrees. Two simple examples are given to show how the fusion of qualitative-enriched belief assignments can be done.",
        "published": "2007-09-11T19:12:25Z",
        "link": "http://arxiv.org/abs/0709.1701v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Toward Psycho-robots",
        "authors": [
            "Andrei Khrennikov"
        ],
        "summary": "We try to perform geometrization of psychology by representing mental states, <<ideas>>, by points of a metric space, <<mental space>>. Evolution of ideas is described by dynamical systems in metric mental space. We apply the mental space approach for modeling of flows of unconscious and conscious information in the human brain. In a series of models, Models 1-4, we consider cognitive systems with increasing complexity of psychological behavior determined by structure of flows of ideas. Since our models are in fact models of the AI-type, one immediately recognizes that they can be used for creation of AI-systems, which we call psycho-robots, exhibiting important elements of human psyche. Creation of such psycho-robots may be useful improvement of domestic robots. At the moment domestic robots are merely simple working devices (e.g. vacuum cleaners or lawn mowers) . However, in future one can expect demand in systems which be able not only perform simple work tasks, but would have elements of human self-developing psyche. Such AI-psyche could play an important role both in relations between psycho-robots and their owners as well as between psycho-robots. Since the presence of a huge numbers of psycho-complexes is an essential characteristic of human psychology, it would be interesting to model them in the AI-framework.",
        "published": "2007-09-13T13:06:34Z",
        "link": "http://arxiv.org/abs/0709.2065v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Autoencoder, Principal Component Analysis and Support Vector Regression   for Data Imputation",
        "authors": [
            "Vukosi N. Marivate",
            "Fulufhelo V. Nelwamodo",
            "Tshilidzi Marwala"
        ],
        "summary": "Data collection often results in records that have missing values or variables. This investigation compares 3 different data imputation models and identifies their merits by using accuracy measures. Autoencoder Neural Networks, Principal components and Support Vector regression are used for prediction and combined with a genetic algorithm to then impute missing variables. The use of PCA improves the overall performance of the autoencoder network while the use of support vector regression shows promising potential for future investigation. Accuracies of up to 97.4 % on imputation of some of the variables were achieved.",
        "published": "2007-09-16T18:15:01Z",
        "link": "http://arxiv.org/abs/0709.2506v1",
        "categories": [
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "Evolving Classifiers: Methods for Incremental Learning",
        "authors": [
            "Greg Hulley",
            "Tshilidzi Marwala"
        ],
        "summary": "The ability of a classifier to take on new information and classes by evolving the classifier without it having to be fully retrained is known as incremental learning. Incremental learning has been successfully applied to many classification problems, where the data is changing and is not all available at once. In this paper there is a comparison between Learn++, which is one of the most recent incremental learning algorithms, and the new proposed method of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has shown good incremental learning capabilities on benchmark datasets on which the new ILUGA method has been tested. ILUGA has also shown good incremental learning ability using only a few classifiers and does not suffer from catastrophic forgetting. The results obtained for ILUGA on the Optical Character Recognition (OCR) and Wine datasets are good, with an overall accuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT for the difficult multi-class OCR dataset.",
        "published": "2007-09-25T14:28:32Z",
        "link": "http://arxiv.org/abs/0709.3965v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Classification of Images Using Support Vector Machines",
        "authors": [
            "Gidudu Anthony",
            "Hulley Greg",
            "Marwala Tshilidzi"
        ],
        "summary": "Support Vector Machines (SVMs) are a relatively new supervised classification technique to the land cover mapping community. They have their roots in Statistical Learning Theory and have gained prominence because they are robust, accurate and are effective even when using a small training sample. By their nature SVMs are essentially binary classifiers, however, they can be adopted to handle the multiple classification tasks common in remote sensing studies. The two approaches commonly used are the One-Against-One (1A1) and One-Against-All (1AA) techniques. In this paper, these approaches are evaluated in as far as their impact and implication for land cover mapping. The main finding from this research is that whereas the 1AA technique is more predisposed to yielding unclassified and mixed pixels, the resulting classification accuracy is not significantly different from 1A1 approach. It is the authors conclusions that ultimately the choice of technique adopted boils down to personal preference and the uniqueness of the dataset at hand.",
        "published": "2007-09-25T14:37:40Z",
        "link": "http://arxiv.org/abs/0709.3967v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Fitness landscape of the cellular automata majority problem: View from   the Olympus",
        "authors": [
            "Sébastien Verel",
            "Philippe Collard",
            "Marco Tomassini",
            "Leonardo Vanneschi"
        ],
        "summary": "In this paper we study cellular automata (CAs) that perform the computational Majority task. This task is a good example of what the phenomenon of emergence in complex systems is. We take an interest in the reasons that make this particular fitness landscape a difficult one. The first goal is to study the landscape as such, and thus it is ideally independent from the actual heuristics used to search the space. However, a second goal is to understand the features a good search technique for this particular problem space should possess. We statistically quantify in various ways the degree of difficulty of searching this landscape. Due to neutrality, investigations based on sampling techniques on the whole landscape are difficult to conduct. So, we go exploring the landscape from the top. Although it has been proved that no CA can perform the task perfectly, several efficient CAs for this task have been found. Exploiting similarities between these CAs and symmetries in the landscape, we define the Olympus landscape which is regarded as the ''heavenly home'' of the best local optima known (blok). Then we measure several properties of this subspace. Although it is easier to find relevant CAs in this subspace than in the overall landscape, there are structural reasons that prevent a searcher from finding overfitted CAs in the Olympus. Finally, we study dynamics and performance of genetic algorithms on the Olympus in order to confirm our analysis and to find efficient CAs for the Majority problem with low computational cost.",
        "published": "2007-09-25T15:40:38Z",
        "link": "http://arxiv.org/abs/0709.3974v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Local search heuristics: Fitness Cloud versus Fitness Landscape",
        "authors": [
            "Philippe Collard",
            "Sébastien Verel",
            "Manuel Clergue"
        ],
        "summary": "This paper introduces the concept of fitness cloud as an alternative way to visualize and analyze search spaces than given by the geographic notion of fitness landscape. It is argued that the fitness cloud concept overcomes several deficiencies of the landscape representation. Our analysis is based on the correlation between fitness of solutions and fitnesses of nearest solutions according to some neighboring. We focus on the behavior of local search heuristics, such as hill climber, on the well-known NK fitness landscape. In both cases the fitness vs. fitness correlation is shown to be related to the epistatic parameter K.",
        "published": "2007-09-25T19:15:50Z",
        "link": "http://arxiv.org/abs/0709.4010v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Measuring the Evolvability Landscape to study Neutrality",
        "authors": [
            "Sébastien Verel",
            "Philippe Collard",
            "Manuel Clergue"
        ],
        "summary": "This theoretical work defines the measure of autocorrelation of evolvability in the context of neutral fitness landscape. This measure has been studied on the classical MAX-SAT problem. This work highlight a new characteristic of neutral fitness landscapes which allows to design new adapted metaheuristic.",
        "published": "2007-09-25T19:17:08Z",
        "link": "http://arxiv.org/abs/0709.4011v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "From Texts to Structured Documents: The Case of Health Practice   Guidelines",
        "authors": [
            "Amanda Bouffier"
        ],
        "summary": "This paper describes a system capable of semi-automatically filling an XML template from free texts in the clinical domain (practice guidelines). The XML template includes semantic information not explicitly encoded in the text (pairs of conditions and actions/recommendations). Therefore, there is a need to compute the exact scope of conditions over text sequences expressing the required actions. We present in this paper the rules developed for this task. We show that the system yields good performance when applied to the analysis of French practice guidelines.",
        "published": "2007-09-25T19:26:08Z",
        "link": "http://arxiv.org/abs/0709.4015v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Mining for trees in a graph is NP-complete",
        "authors": [
            "Jan Van den Bussche"
        ],
        "summary": "Mining for trees in a graph is shown to be NP-complete.",
        "published": "2007-09-28T17:08:39Z",
        "link": "http://arxiv.org/abs/0709.4655v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "Lagrangian Relaxation for MAP Estimation in Graphical Models",
        "authors": [
            "Jason K. Johnson",
            "Dmitry M. Malioutov",
            "Alan S. Willsky"
        ],
        "summary": "We develop a general framework for MAP estimation in discrete and Gaussian graphical models using Lagrangian relaxation techniques. The key idea is to reformulate an intractable estimation problem as one defined on a more tractable graph, but subject to additional constraints. Relaxing these constraints gives a tractable dual problem, one defined by a thin graph, which is then optimized by an iterative procedure. When this iterative optimization leads to a consistent estimate, one which also satisfies the constraints, then it corresponds to an optimal MAP estimate of the original model. Otherwise there is a ``duality gap'', and we obtain a bound on the optimal solution. Thus, our approach combines convex optimization with dynamic programming techniques applicable for thin graphs. The popular tree-reweighted max-product (TRMP) method may be seen as solving a particular class of such relaxations, where the intractable graph is relaxed to a set of spanning trees. We also consider relaxations to a set of small induced subgraphs, thin subgraphs (e.g. loops), and a connected tree obtained by ``unwinding'' cycles. In addition, we propose a new class of multiscale relaxations that introduce ``summary'' variables. The potential benefits of such generalizations include: reducing or eliminating the ``duality gap'' in hard problems, reducing the number or Lagrange multipliers in the dual problem, and accelerating convergence of the iterative optimization procedure.",
        "published": "2007-09-28T21:29:43Z",
        "link": "http://arxiv.org/abs/0710.0013v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Optimising the topology of complex neural networks",
        "authors": [
            "Fei Jiang",
            "Hugues Berry",
            "Marc Schoenauer"
        ],
        "summary": "In this paper, we study instances of complex neural networks, i.e. neural netwo rks with complex topologies. We use Self-Organizing Map neural networks whose n eighbourhood relationships are defined by a complex network, to classify handwr itten digits. We show that topology has a small impact on performance and robus tness to neuron failures, at least at long learning times. Performance may howe ver be increased (by almost 10%) by artificial evolution of the network topo logy. In our experimental conditions, the evolved networks are more random than their parents, but display a more heterogeneous degree distribution.",
        "published": "2007-10-01T06:51:42Z",
        "link": "http://arxiv.org/abs/0710.0213v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Bio-linguistic transition and Baldwin effect in an evolutionary   naming-game model",
        "authors": [
            "Adam Lipowski",
            "Dorota Lipowska"
        ],
        "summary": "We examine an evolutionary naming-game model where communicating agents are equipped with an evolutionarily selected learning ability. Such a coupling of biological and linguistic ingredients results in an abrupt transition: upon a small change of a model control parameter a poorly communicating group of linguistically unskilled agents transforms into almost perfectly communicating group with large learning abilities. When learning ability is kept fixed, the transition appears to be continuous. Genetic imprinting of the learning abilities proceeds via Baldwin effect: initially unskilled communicating agents learn a language and that creates a niche in which there is an evolutionary pressure for the increase of learning ability.Our model suggests that when linguistic (or cultural) processes became intensive enough, a transition took place where both linguistic performance and biological endowment of our species experienced an abrupt change that perhaps triggered the rapid expansion of human civilization.",
        "published": "2007-10-01T09:39:49Z",
        "link": "http://arxiv.org/abs/0710.0009v2",
        "categories": [
            "cs.CL",
            "cond-mat.stat-mech",
            "cs.AI",
            "physics.soc-ph",
            "q-bio.PE"
        ]
    },
    {
        "title": "What's in a Name?",
        "authors": [
            "Stasinos Konstantopoulos"
        ],
        "summary": "This paper describes experiments on identifying the language of a single name in isolation or in a document written in a different language. A new corpus has been compiled and made available, matching names against languages. This corpus is used in a series of experiments measuring the performance of general language models and names-only language models on the language identification task. Conclusions are drawn from the comparison between using general language models and names-only language models and between identifying the language of isolated names and the language of very short document fragments. Future research directions are outlined.",
        "published": "2007-10-08T08:36:32Z",
        "link": "http://arxiv.org/abs/0710.1481v1",
        "categories": [
            "cs.CL",
            "cs.AI"
        ]
    },
    {
        "title": "A Heuristic Routing Mechanism Using a New Addressing Scheme",
        "authors": [
            "Mohsen Ravanbakhsh",
            "Yasin Abbasi-Yadkori",
            "Maghsoud Abbaspour",
            "Hamid Sarbazi-Azad"
        ],
        "summary": "Current methods of routing are based on network information in the form of routing tables, in which routing protocols determine how to update the tables according to the network changes. Despite the variability of data in routing tables, node addresses are constant. In this paper, we first introduce the new concept of variable addresses, which results in a novel framework to cope with routing problems using heuristic solutions. Then we propose a heuristic routing mechanism based on the application of genes for determination of network addresses in a variable address network and describe how this method flexibly solves different problems and induces new ideas in providing integral solutions for variety of problems. The case of ad-hoc networks is where simulation results are more supportive and original solutions have been proposed for issues like mobility.",
        "published": "2007-10-10T04:29:24Z",
        "link": "http://arxiv.org/abs/0710.1924v1",
        "categories": [
            "cs.NI",
            "cs.AI"
        ]
    },
    {
        "title": "A System for Predicting Subcellular Localization of Yeast Genome Using   Neural Network",
        "authors": [
            "Sabu M. Thampi",
            "K. Chandra Sekaran"
        ],
        "summary": "The subcellular location of a protein can provide valuable information about its function. With the rapid increase of sequenced genomic data, the need for an automated and accurate tool to predict subcellular localization becomes increasingly important. Many efforts have been made to predict protein subcellular localization. This paper aims to merge the artificial neural networks and bioinformatics to predict the location of protein in yeast genome. We introduce a new subcellular prediction method based on a backpropagation neural network. The results show that the prediction within an error limit of 5 to 10 percentage can be achieved with the system.",
        "published": "2007-10-11T12:09:33Z",
        "link": "http://arxiv.org/abs/0710.2227v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "The structure of verbal sequences analyzed with unsupervised learning   techniques",
        "authors": [
            "Catherine Recanati",
            "Nicoleta Rogovschi",
            "Younès Bennani"
        ],
        "summary": "Data mining allows the exploration of sequences of phenomena, whereas one usually tends to focus on isolated phenomena or on the relation between two phenomena. It offers invaluable tools for theoretical analyses and exploration of the structure of sentences, texts, dialogues, and speech. We report here the results of an attempt at using it for inspecting sequences of verbs from French accounts of road accidents. This analysis comes from an original approach of unsupervised training allowing the discovery of the structure of sequential data. The entries of the analyzer were only made of the verbs appearing in the sentences. It provided a classification of the links between two successive verbs into four distinct clusters, allowing thus text segmentation. We give here an interpretation of these clusters by applying a statistical analysis to independent semantic annotations.",
        "published": "2007-10-12T12:44:11Z",
        "link": "http://arxiv.org/abs/0710.2446v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Geometric Analogue of Holographic Reduced Representation",
        "authors": [
            "Diederik Aerts",
            "Marek Czachor",
            "Bart De Moor"
        ],
        "summary": "Holographic reduced representations (HRR) are based on superpositions of convolution-bound $n$-tuples, but the $n$-tuples cannot be regarded as vectors since the formalism is basis dependent. This is why HRR cannot be associated with geometric structures. Replacing convolutions by geometric products one arrives at reduced representations analogous to HRR but interpretable in terms of geometry. Variable bindings occurring in both HRR and its geometric analogue mathematically correspond to two different representations of $Z_2\\times...\\times Z_2$ (the additive group of binary $n$-tuples with addition modulo 2). As opposed to standard HRR, variable binding performed by means of geometric product allows for computing exact inverses of all nonzero vectors, a procedure even simpler than approximate inverses employed in HRR. The formal structure of the new reduced representation is analogous to cartoon computation, a geometric analogue of quantum computation.",
        "published": "2007-10-15T13:56:39Z",
        "link": "http://arxiv.org/abs/0710.2611v2",
        "categories": [
            "cs.AI",
            "quant-ph"
        ]
    },
    {
        "title": "Effective linkage learning using low-order statistics and clustering",
        "authors": [
            "Leonardo Emmendorfer",
            "Aurora Pozo"
        ],
        "summary": "The adoption of probabilistic models for the best individuals found so far is a powerful approach for evolutionary computation. Increasingly more complex models have been used by estimation of distribution algorithms (EDAs), which often result better effectiveness on finding the global optima for hard optimization problems. Supervised and unsupervised learning of Bayesian networks are very effective options, since those models are able to capture interactions of high order among the variables of a problem. Diversity preservation, through niching techniques, has also shown to be very important to allow the identification of the problem structure as much as for keeping several global optima. Recently, clustering was evaluated as an effective niching technique for EDAs, but the performance of simpler low-order EDAs was not shown to be much improved by clustering, except for some simple multimodal problems. This work proposes and evaluates a combination operator guided by a measure from information theory which allows a clustered low-order EDA to effectively solve a comprehensive range of benchmark optimization problems.",
        "published": "2007-10-15T15:28:47Z",
        "link": "http://arxiv.org/abs/0710.2782v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Fuzzy Modeling of Electrical Impedance Tomography Image of the Lungs",
        "authors": [
            "Harki Tanaka",
            "Neli Regina Siqueira Ortega",
            "Mauricio Stanzione Galizia",
            "Joao Batista Borges Sobrinho",
            "Marcelo Britto Passos Amato"
        ],
        "summary": "Electrical Impedance Tomography (EIT) is a functional imaging method that is being developed for bedside use in critical care medicine. Aiming at improving the chest anatomical resolution of EIT images we developed a fuzzy model based on EIT high temporal resolution and the functional information contained in the pulmonary perfusion and ventilation signals. EIT data from an experimental animal model were collected during normal ventilation and apnea while an injection of hypertonic saline was used as a reference . The fuzzy model was elaborated in three parts: a modeling of the heart, a pulmonary map from ventilation images and, a pulmonary map from perfusion images. Image segmentation was performed using a threshold method and a ventilation/perfusion map was generated. EIT images treated by the fuzzy model were compared with the hypertonic saline injection method and CT-scan images, presenting good results in both qualitative (the image obtained by the model was very similar to that of the CT-scan) and quantitative (the ROC curve provided an area equal to 0.93) point of view. Undoubtedly, these results represent an important step in the EIT images area, since they open the possibility of developing EIT-based bedside clinical methods, which are not available nowadays. These achievements could serve as the base to develop EIT diagnosis system for some life-threatening diseases commonly found in critical care medicine.",
        "published": "2007-10-16T22:13:11Z",
        "link": "http://arxiv.org/abs/0710.3185v1",
        "categories": [
            "cs.AI",
            "cs.CV"
        ]
    },
    {
        "title": "Stationary probability density of stochastic search processes in global   optimization",
        "authors": [
            "Arturo Berrones"
        ],
        "summary": "A method for the construction of approximate analytical expressions for the stationary marginal densities of general stochastic search processes is proposed. By the marginal densities, regions of the search space that with high probability contain the global optima can be readily defined. The density estimation procedure involves a controlled number of linear operations, with a computational cost per iteration that grows linearly with problem size.",
        "published": "2007-10-18T18:04:42Z",
        "link": "http://arxiv.org/abs/0710.3561v1",
        "categories": [
            "cs.AI",
            "cond-mat.stat-mech",
            "cs.NE"
        ]
    },
    {
        "title": "Analyzing covert social network foundation behind terrorism disaster",
        "authors": [
            "Yoshiharu Maeno",
            "Yukio Ohsawa"
        ],
        "summary": "This paper addresses a method to analyze the covert social network foundation hidden behind the terrorism disaster. It is to solve a node discovery problem, which means to discover a node, which functions relevantly in a social network, but escaped from monitoring on the presence and mutual relationship of nodes. The method aims at integrating the expert investigator's prior understanding, insight on the terrorists' social network nature derived from the complex graph theory, and computational data processing. The social network responsible for the 9/11 attack in 2001 is used to execute simulation experiment to evaluate the performance of the method.",
        "published": "2007-10-23T10:40:55Z",
        "link": "http://arxiv.org/abs/0710.4231v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Computational Intelligence Characterization Method of Semiconductor   Device",
        "authors": [
            "Eric Liau",
            "Doris Schmitt-Landsiedel"
        ],
        "summary": "Characterization of semiconductor devices is used to gather as much data about the device as possible to determine weaknesses in design or trends in the manufacturing process. In this paper, we propose a novel multiple trip point characterization concept to overcome the constraint of single trip point concept in device characterization phase. In addition, we use computational intelligence techniques (e.g. neural network, fuzzy and genetic algorithm) to further manipulate these sets of multiple trip point values and tests based on semiconductor test equipments, Our experimental results demonstrate an excellent design parameter variation analysis in device characterization phase, as well as detection of a set of worst case tests that can provoke the worst case variation, while traditional approach was not capable of detecting them.",
        "published": "2007-10-25T09:41:43Z",
        "link": "http://arxiv.org/abs/0710.4734v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Node discovery problem for a social network",
        "authors": [
            "Yoshiharu Maeno"
        ],
        "summary": "Methods to solve a node discovery problem for a social network are presented. Covert nodes refer to the nodes which are not observable directly. They transmit the influence and affect the resulting collaborative activities among the persons in a social network, but do not appear in the surveillance logs which record the participants of the collaborative activities. Discovering the covert nodes is identifying the suspicious logs where the covert nodes would appear if the covert nodes became overt. The performance of the methods is demonstrated with a test dataset generated from computationally synthesized networks and a real organization.",
        "published": "2007-10-26T01:32:47Z",
        "link": "http://arxiv.org/abs/0710.4975v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Discriminated Belief Propagation",
        "authors": [
            "Uli Sorger"
        ],
        "summary": "Near optimal decoding of good error control codes is generally a difficult task. However, for a certain type of (sufficiently) good codes an efficient decoding algorithm with near optimal performance exists. These codes are defined via a combination of constituent codes with low complexity trellis representations. Their decoding algorithm is an instance of (loopy) belief propagation and is based on an iterative transfer of constituent beliefs. The beliefs are thereby given by the symbol probabilities computed in the constituent trellises. Even though weak constituent codes are employed close to optimal performance is obtained, i.e., the encoder/decoder pair (almost) achieves the information theoretic capacity. However, (loopy) belief propagation only performs well for a rather specific set of codes, which limits its applicability.   In this paper a generalisation of iterative decoding is presented. It is proposed to transfer more values than just the constituent beliefs. This is achieved by the transfer of beliefs obtained by independently investigating parts of the code space. This leads to the concept of discriminators, which are used to improve the decoder resolution within certain areas and defines discriminated symbol beliefs. It is shown that these beliefs approximate the overall symbol probabilities. This leads to an iteration rule that (below channel capacity) typically only admits the solution of the overall decoding problem. Via a Gauss approximation a low complexity version of this algorithm is derived. Moreover, the approach may then be applied to a wide range of channel maps without significant complexity increase.",
        "published": "2007-10-29T19:01:48Z",
        "link": "http://arxiv.org/abs/0710.5501v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT"
        ]
    },
    {
        "title": "Performance Bounds for Lambda Policy Iteration and Application to the   Game of Tetris",
        "authors": [
            "Bruno Scherrer"
        ],
        "summary": "We consider the discrete-time infinite-horizon optimal control problem formalized by Markov Decision Processes. We revisit the work of Bertsekas and Ioffe, that introduced $\\lambda$ Policy Iteration, a family of algorithms parameterized by $\\lambda$ that generalizes the standard algorithms Value Iteration and Policy Iteration, and has some deep connections with the Temporal Differences algorithm TD($\\lambda$) described by Sutton and Barto. We deepen the original theory developped by the authors by providing convergence rate bounds which generalize standard bounds for Value Iteration described for instance by Puterman. Then, the main contribution of this paper is to develop the theory of this algorithm when it is used in an approximate form and show that this is sound. Doing so, we extend and unify the separate analyses developped by Munos for Approximate Value Iteration and Approximate Policy Iteration. Eventually, we revisit the use of this algorithm in the training of a Tetris playing controller as originally done by Bertsekas and Ioffe. We provide an original performance bound that can be applied to such an undiscounted control problem. Our empirical results are different from those of Bertsekas and Ioffe (which were originally qualified as \"paradoxical\" and \"intriguing\"), and much more conform to what one would expect from a learning experiment. We discuss the possible reason for such a difference.",
        "published": "2007-11-05T17:07:22Z",
        "link": "http://arxiv.org/abs/0711.0694v5",
        "categories": [
            "cs.AI",
            "cs.RO"
        ]
    },
    {
        "title": "Towards a Sound Theory of Adaptation for the Simple Genetic Algorithm",
        "authors": [
            "Keki Burjorjee"
        ],
        "summary": "The pace of progress in the fields of Evolutionary Computation and Machine Learning is currently limited -- in the former field, by the improbability of making advantageous extensions to evolutionary algorithms when their capacity for adaptation is poorly understood, and in the latter by the difficulty of finding effective semi-principled reductions of hard real-world problems to relatively simple optimization problems. In this paper we explain why a theory which can accurately explain the simple genetic algorithm's remarkable capacity for adaptation has the potential to address both these limitations. We describe what we believe to be the impediments -- historic and analytic -- to the discovery of such a theory and highlight the negative role that the building block hypothesis (BBH) has played. We argue based on experimental results that a fundamental limitation which is widely believed to constrain the SGA's adaptive ability (and is strongly implied by the BBH) is in fact illusionary and does not exist. The SGA therefore turns out to be more powerful than it is currently thought to be. We give conditions under which it becomes feasible to numerically approximate and study the multivariate marginals of the search distribution of an infinite population SGA over multiple generations even when its genomes are long, and explain why this analysis is relevant to the riddle of the SGA's remarkable adaptive abilities.",
        "published": "2007-11-09T02:28:12Z",
        "link": "http://arxiv.org/abs/0711.1401v2",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.8; F.2"
        ]
    },
    {
        "title": "Predicting relevant empty spots in social interaction",
        "authors": [
            "Yoshiharu Maeno",
            "Yukio Ohsawa"
        ],
        "summary": "An empty spot refers to an empty hard-to-fill space which can be found in the records of the social interaction, and is the clue to the persons in the underlying social network who do not appear in the records. This contribution addresses a problem to predict relevant empty spots in social interaction. Homogeneous and inhomogeneous networks are studied as a model underlying the social interaction. A heuristic predictor function approach is presented as a new method to address the problem. Simulation experiment is demonstrated over a homogeneous network. A test data in the form of baskets is generated from the simulated communication. Precision to predict the empty spots is calculated to demonstrate the performance of the presented approach.",
        "published": "2007-11-09T13:54:30Z",
        "link": "http://arxiv.org/abs/0711.1466v3",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Building Rules on Top of Ontologies for the Semantic Web with Inductive   Logic Programming",
        "authors": [
            "Francesca A. Lisi"
        ],
        "summary": "Building rules on top of ontologies is the ultimate goal of the logical layer of the Semantic Web. To this aim an ad-hoc mark-up language for this layer is currently under discussion. It is intended to follow the tradition of hybrid knowledge representation and reasoning systems such as $\\mathcal{AL}$-log that integrates the description logic $\\mathcal{ALC}$ and the function-free Horn clausal language \\textsc{Datalog}. In this paper we consider the problem of automating the acquisition of these rules for the Semantic Web. We propose a general framework for rule induction that adopts the methodological apparatus of Inductive Logic Programming and relies on the expressive and deductive power of $\\mathcal{AL}$-log. The framework is valid whatever the scope of induction (description vs. prediction) is. Yet, for illustrative purposes, we also discuss an instantiation of the framework which aims at description and turns out to be useful in Ontology Refinement.   Keywords: Inductive Logic Programming, Hybrid Knowledge Representation and Reasoning Systems, Ontologies, Semantic Web.   Note: To appear in Theory and Practice of Logic Programming (TPLP)",
        "published": "2007-11-12T17:15:34Z",
        "link": "http://arxiv.org/abs/0711.1814v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Computer Model of a \"Sense of Humour\". I. General Algorithm",
        "authors": [
            "I. M. Suslov"
        ],
        "summary": "A computer model of a \"sense of humour\" is proposed. The humorous effect is interpreted as a specific malfunction in the course of information processing due to the need for the rapid deletion of the false version transmitted into consciousness. The biological function of a sense of humour consists in speeding up the bringing of information into consciousness and in fuller use of the resources of the brain.",
        "published": "2007-11-13T19:00:32Z",
        "link": "http://arxiv.org/abs/0711.2058v1",
        "categories": [
            "q-bio.NC",
            "cs.AI"
        ]
    },
    {
        "title": "Computer Model of a \"Sense of Humour\". II. Realization in Neural   Networks",
        "authors": [
            "I. M. Suslov"
        ],
        "summary": "The computer realization of a \"sense of humour\" requires the creation of an algorithm for solving the \"linguistic problem\", i.e. the problem of recognizing a continuous sequence of polysemantic images. Such algorithm may be realized in the Hopfield model of a neural network after its proper modification.",
        "published": "2007-11-13T20:15:10Z",
        "link": "http://arxiv.org/abs/0711.2061v1",
        "categories": [
            "q-bio.NC",
            "cs.AI"
        ]
    },
    {
        "title": "Can a Computer Laugh ?",
        "authors": [
            "I. M. Suslov"
        ],
        "summary": "A computer model of \"a sense of humour\" suggested previously [arXiv:0711.2058,0711.2061], relating the humorous effect with a specific malfunction in information processing, is given in somewhat different exposition. Psychological aspects of humour are elaborated more thoroughly. The mechanism of laughter is formulated on the more general level. Detailed discussion is presented for the higher levels of information processing, which are responsible for a perception of complex samples of humour. Development of a sense of humour in the process of evolution is discussed.",
        "published": "2007-11-14T18:32:09Z",
        "link": "http://arxiv.org/abs/0711.2270v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "q-bio.NC"
        ]
    },
    {
        "title": "A Compact Self-organizing Cellular Automata-based Genetic Algorithm",
        "authors": [
            "Vasileios Barmpoutis",
            "Gary F. Dargush"
        ],
        "summary": "A Genetic Algorithm (GA) is proposed in which each member of the population can change schemata only with its neighbors according to a rule. The rule methodology and the neighborhood structure employ elements from the Cellular Automata (CA) strategies. Each member of the GA population is assigned to a cell and crossover takes place only between adjacent cells, according to the predefined rule. Although combinations of CA and GA approaches have appeared previously, here we rely on the inherent self-organizing features of CA, rather than on parallelism. This conceptual shift directs us toward the evolution of compact populations containing only a handful of members. We find that the resulting algorithm can search the design space more efficiently than traditional GA strategies due to its ability to exploit mutations within this compact self-organizing population. Consequently, premature convergence is avoided and the final results often are more accurate. In order to reinforce the superior mutation capability, a re-initialization strategy also is implemented. Ten test functions and two benchmark structural engineering truss design problems are examined in order to demonstrate the performance of the method.",
        "published": "2007-11-15T18:19:39Z",
        "link": "http://arxiv.org/abs/0711.2478v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Comparing the notions of optimality in CP-nets, strategic games and soft   constraints",
        "authors": [
            "Krzysztof R. Apt",
            "Francesca Rossi",
            "Kristen Brent Venable"
        ],
        "summary": "The notion of optimality naturally arises in many areas of applied mathematics and computer science concerned with decision making. Here we consider this notion in the context of three formalisms used for different purposes in reasoning about multi-agent systems: strategic games, CP-nets, and soft constraints. To relate the notions of optimality in these formalisms we introduce a natural qualitative modification of the notion of a strategic game. We show then that the optimal outcomes of a CP-net are exactly the Nash equilibria of such games. This allows us to use the techniques of game theory to search for optimal outcomes of CP-nets and vice-versa, to use techniques developed for CP-nets to search for Nash equilibria of the considered games. Then, we relate the notion of optimality used in the area of soft constraints to that used in a generalization of strategic games, called graphical games. In particular we prove that for a natural class of soft constraints that includes weighted constraints every optimal solution is both a Nash equilibrium and Pareto efficient joint strategy. For a natural mapping in the other direction we show that Pareto efficient joint strategies coincide with the optimal solutions of soft constraints.",
        "published": "2007-11-19T12:14:27Z",
        "link": "http://arxiv.org/abs/0711.2909v2",
        "categories": [
            "cs.AI",
            "cs.GT",
            "I.2.11; D.3.3"
        ]
    },
    {
        "title": "Image Classification Using SVMs: One-against-One Vs One-against-All",
        "authors": [
            "Gidudu Anthony",
            "Hulley Gregg",
            "Marwala Tshilidzi"
        ],
        "summary": "Support Vector Machines (SVMs) are a relatively new supervised classification technique to the land cover mapping community. They have their roots in Statistical Learning Theory and have gained prominence because they are robust, accurate and are effective even when using a small training sample. By their nature SVMs are essentially binary classifiers, however, they can be adopted to handle the multiple classification tasks common in remote sensing studies. The two approaches commonly used are the One-Against-One (1A1) and One-Against-All (1AA) techniques. In this paper, these approaches are evaluated in as far as their impact and implication for land cover mapping. The main finding from this research is that whereas the 1AA technique is more predisposed to yielding unclassified and mixed pixels, the resulting classification accuracy is not significantly different from 1A1 approach. It is the authors conclusion therefore that ultimately the choice of technique adopted boils down to personal preference and the uniqueness of the dataset at hand.",
        "published": "2007-11-19T12:25:00Z",
        "link": "http://arxiv.org/abs/0711.2914v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ]
    },
    {
        "title": "How to realize \"a sense of humour\" in computers ?",
        "authors": [
            "I. M. Suslov"
        ],
        "summary": "Computer model of a \"sense of humour\" suggested previously [arXiv:0711.2058, 0711.2061, 0711.2270] is raised to the level of a realistic algorithm.",
        "published": "2007-11-20T19:57:23Z",
        "link": "http://arxiv.org/abs/0711.3197v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "q-bio.NC"
        ]
    },
    {
        "title": "A Game-Theoretic Analysis of Updating Sets of Probabilities",
        "authors": [
            "Peter D. Grunwald",
            "Joseph Y. Halpern"
        ],
        "summary": "We consider how an agent should update her uncertainty when it is represented by a set $\\P$ of probability distributions and the agent observes that a random variable $X$ takes on value $x$, given that the agent makes decisions using the minimax criterion, perhaps the best-studied and most commonly-used criterion in the literature. We adopt a game-theoretic framework, where the agent plays against a bookie, who chooses some distribution from $\\P$. We consider two reasonable games that differ in what the bookie knows when he makes his choice. Anomalies that have been observed before, like time inconsistency, can be understood as arising important because different games are being played, against bookies with different information. We characterize the important special cases in which the optimal decision rules according to the minimax criterion amount to either conditioning or simply ignoring the information. Finally, we consider the relationship between conditioning and calibration when uncertainty is described by sets of probabilities.",
        "published": "2007-11-20T23:34:12Z",
        "link": "http://arxiv.org/abs/0711.3235v1",
        "categories": [
            "cs.AI",
            "math.ST",
            "stat.TH",
            "I.2.4"
        ]
    },
    {
        "title": "Translating OWL and Semantic Web Rules into Prolog: Moving Toward   Description Logic Programs",
        "authors": [
            "Ken Samuel",
            "Leo Obrst",
            "Suzette Stoutenberg",
            "Karen Fox",
            "Paul Franklin",
            "Adrian Johnson",
            "Ken Laskey",
            "Deborah Nichols",
            "Steve Lopez",
            "Jason Peterson"
        ],
        "summary": "To appear in Theory and Practice of Logic Programming (TPLP), 2008.   We are researching the interaction between the rule and the ontology layers of the Semantic Web, by comparing two options: 1) using OWL and its rule extension SWRL to develop an integrated ontology/rule language, and 2) layering rules on top of an ontology with RuleML and OWL. Toward this end, we are developing the SWORIER system, which enables efficient automated reasoning on ontologies and rules, by translating all of them into Prolog and adding a set of general rules that properly capture the semantics of OWL. We have also enabled the user to make dynamic changes on the fly, at run time. This work addresses several of the concerns expressed in previous work, such as negation, complementary classes, disjunctive heads, and cardinality, and it discusses alternative approaches for dealing with inconsistencies in the knowledge base. In addition, for efficiency, we implemented techniques called extensionalization, avoiding reanalysis, and code minimization.",
        "published": "2007-11-21T17:36:50Z",
        "link": "http://arxiv.org/abs/0711.3419v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Knowware: the third star after Hardware and Software",
        "authors": [
            "Ruqian Lu"
        ],
        "summary": "This book proposes to separate knowledge from software and to make it a commodity that is called knowware. The architecture, representation and function of Knowware are discussed. The principles of knowware engineering and its three life cycle models: furnace model, crystallization model and spiral model are proposed and analyzed. Techniques of software/knowware co-engineering are introduced. A software component whose knowledge is replaced by knowware is called mixware. An object and component oriented development schema of mixware is introduced. In particular, the tower model and ladder model for mixware development are proposed and discussed. Finally, knowledge service and knowware based Web service are introduced and compared with Web service. In summary, knowware, software and hardware should be considered as three equally important underpinnings of IT industry.   Ruqian Lu is a professor of computer science of the Institute of Mathematics, Academy of Mathematics and System Sciences. He is a fellow of Chinese Academy of Sciences. His research interests include artificial intelligence, knowledge engineering and knowledge based software engineering. He has published more than 100 papers and 10 books. He has won two first class awards from the Academia Sinica and a National second class prize from the Ministry of Science and Technology. He has also won the sixth Hua Loo-keng Mathematics Prize.",
        "published": "2007-11-27T17:36:35Z",
        "link": "http://arxiv.org/abs/0711.4309v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.CY"
        ]
    },
    {
        "title": "The Second Law as a Cause of the Evolution",
        "authors": [
            "Oded Kafri"
        ],
        "summary": "It is a common belief that in any environment where life is possible, life will be generated. Here it is suggested that the cause for a spontaneous generation of complex systems is probability driven processes. Based on equilibrium thermodynamics, it is argued that in low occupation number statistical systems, the second law of thermodynamics yields an increase of thermal entropy and a canonic energy distribution. However, in high occupation number statistical systems, the same law for the same reasons yields an increase of information and a Benford's law/power-law energy distribution. It is therefore, plausible, that eventually the heat death is not necessarily the end of the universe.",
        "published": "2007-11-28T14:06:19Z",
        "link": "http://arxiv.org/abs/0711.4507v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT"
        ]
    },
    {
        "title": "Circumspect descent prevails in solving random constraint satisfaction   problems",
        "authors": [
            "Mikko Alava",
            "John Ardelius",
            "Erik Aurell",
            "Petteri Kaski",
            "Supriya Krishnamurthy",
            "Pekka Orponen",
            "Sakari Seitz"
        ],
        "summary": "We study the performance of stochastic local search algorithms for random instances of the $K$-satisfiability ($K$-SAT) problem. We introduce a new stochastic local search algorithm, ChainSAT, which moves in the energy landscape of a problem instance by {\\em never going upwards} in energy. ChainSAT is a \\emph{focused} algorithm in the sense that it considers only variables occurring in unsatisfied clauses. We show by extensive numerical investigations that ChainSAT and other focused algorithms solve large $K$-SAT instances almost surely in linear time, up to high clause-to-variable ratios $\\alpha$; for example, for K=4 we observe linear-time performance well beyond the recently postulated clustering and condensation transitions in the solution space. The performance of ChainSAT is a surprise given that by design the algorithm gets trapped into the first local energy minimum it encounters, yet no such minima are encountered. We also study the geometry of the solution space as accessed by stochastic local search algorithms.",
        "published": "2007-11-30T11:01:40Z",
        "link": "http://arxiv.org/abs/0711.4902v1",
        "categories": [
            "cs.DS",
            "cond-mat.stat-mech",
            "cs.AI"
        ]
    },
    {
        "title": "Ontology and Formal Semantics - Integration Overdue",
        "authors": [
            "Walid S. Saba"
        ],
        "summary": "In this note we suggest that difficulties encountered in natural language semantics are, for the most part, due to the use of mere symbol manipulation systems that are devoid of any content. In such systems, where there is hardly any link with our common-sense view of the world, and it is quite difficult to envision how one can formally account for the considerable amount of content that is often implicit, but almost never explicitly stated in our everyday discourse. The solution, in our opinion, is a compositional semantics grounded in an ontology that reflects our commonsense view of the world and the way we talk about it in ordinary language. In the compositional logic we envision there are ontological (or first-intension) concepts, and logical (or second-intension) concepts, and where the ontological concepts include not only Davidsonian events, but other abstract objects as well (e.g., states, processes, properties, activities, attributes, etc.) It will be demonstrated here that in such a framework, a number of challenges in the semantics of natural language (e.g., metonymy, intensionality, metaphor, etc.) can be properly and uniformly addressed.",
        "published": "2007-12-01T14:27:12Z",
        "link": "http://arxiv.org/abs/0712.1529v2",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "A Spectral Approach to Analyzing Belief Propagation for 3-Coloring",
        "authors": [
            "Amin Coja-Oghlan",
            "Elchanan Mossel",
            "Dan Vilenchik"
        ],
        "summary": "Contributing to the rigorous understanding of BP, in this paper we relate the convergence of BP to spectral properties of the graph. This encompasses a result for random graphs with a ``planted'' solution; thus, we obtain the first rigorous result on BP for graph coloring in the case of a complex graphical structure (as opposed to trees). In particular, the analysis shows how Belief Propagation breaks the symmetry between the $3!$ possible permutations of the color classes.",
        "published": "2007-12-02T19:34:59Z",
        "link": "http://arxiv.org/abs/0712.0171v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "cs.DM"
        ]
    },
    {
        "title": "A Reactive Tabu Search Algorithm for Stimuli Generation in   Psycholinguistics",
        "authors": [
            "Alejandro Chinea Manrique De Lara"
        ],
        "summary": "The generation of meaningless \"words\" matching certain statistical and/or linguistic criteria is frequently needed for experimental purposes in Psycholinguistics. Such stimuli receive the name of pseudowords or nonwords in the Cognitive Neuroscience literatue. The process for building nonwords sometimes has to be based on linguistic units such as syllables or morphemes, resulting in a numerical explosion of combinations when the size of the nonwords is increased. In this paper, a reactive tabu search scheme is proposed to generate nonwords of variables size. The approach builds pseudowords by using a modified Metaheuristic algorithm based on a local search procedure enhanced by a feedback-based scheme. Experimental results show that the new algorithm is a practical and effective tool for nonword generation.",
        "published": "2007-12-04T08:52:46Z",
        "link": "http://arxiv.org/abs/0712.0451v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DM",
            "cs.LG"
        ]
    },
    {
        "title": "Computational Chemotaxis in Ants and Bacteria over Dynamic Environments",
        "authors": [
            "Vitorino Ramos",
            "C. M. Fernandes",
            "A. C. Rosa",
            "A. Abraham"
        ],
        "summary": "Chemotaxis can be defined as an innate behavioural response by an organism to a directional stimulus, in which bacteria, and other single-cell or multicellular organisms direct their movements according to certain chemicals in their environment. This is important for bacteria to find food (e.g., glucose) by swimming towards the highest concentration of food molecules, or to flee from poisons. Based on self-organized computational approaches and similar stigmergic concepts we derive a novel swarm intelligent algorithm. What strikes from these observations is that both eusocial insects as ant colonies and bacteria have similar natural mechanisms based on stigmergy in order to emerge coherent and sophisticated patterns of global collective behaviour. Keeping in mind the above characteristics we will present a simple model to tackle the collective adaptation of a social swarm based on real ant colony behaviors (SSA algorithm) for tracking extrema in dynamic environments and highly multimodal complex functions described in the well-know De Jong test suite. Later, for the purpose of comparison, a recent model of artificial bacterial foraging (BFOA algorithm) based on similar stigmergic features is described and analyzed. Final results indicate that the SSA collective intelligence is able to cope and quickly adapt to unforeseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes, while outperforming BFOA in adaptive speed. Results indicate that the present approach deals well in severe Dynamic Optimization problems.",
        "published": "2007-12-05T15:02:19Z",
        "link": "http://arxiv.org/abs/0712.0744v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "q-bio.PE",
            "q-bio.QM",
            "I.2; I.2.11; G.1.6"
        ]
    },
    {
        "title": "Evolving localizations in reaction-diffusion cellular automata",
        "authors": [
            "Andrew Adamatzky",
            "Larry Bull",
            "Pierre Collet",
            "Emmanuel Sapin"
        ],
        "summary": "We consider hexagonal cellular automata with immediate cell neighbourhood and three cell-states. Every cell calculates its next state depending on the integral representation of states in its neighbourhood, i.e. how many neighbours are in each one state. We employ evolutionary algorithms to breed local transition functions that support mobile localizations (gliders), and characterize sets of the functions selected in terms of quasi-chemical systems. Analysis of the set of functions evolved allows to speculate that mobile localizations are likely to emerge in the quasi-chemical systems with limited diffusion of one reagent, a small number of molecules is required for amplification of travelling localizations, and reactions leading to stationary localizations involve relatively equal amount of quasi-chemical species. Techniques developed can be applied in cascading signals in nature-inspired spatially extended computing devices, and phenomenological studies and classification of non-linear discrete systems.",
        "published": "2007-12-05T22:07:04Z",
        "link": "http://arxiv.org/abs/0712.0836v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Automatic Pattern Classification by Unsupervised Learning Using   Dimensionality Reduction of Data with Mirroring Neural Networks",
        "authors": [
            "Dasika Ratna Deepthi",
            "G. R. Aditya Krishna",
            "K. Eswaran"
        ],
        "summary": "This paper proposes an unsupervised learning technique by using Multi-layer Mirroring Neural Network and Forgy's clustering algorithm. Multi-layer Mirroring Neural Network is a neural network that can be trained with generalized data inputs (different categories of image patterns) to perform non-linear dimensionality reduction and the resultant low-dimensional code is used for unsupervised pattern classification using Forgy's algorithm. By adapting the non-linear activation function (modified sigmoidal function) and initializing the weights and bias terms to small random values, mirroring of the input pattern is initiated. In training, the weights and bias terms are changed in such a way that the input presented is reproduced at the output by back propagating the error. The mirroring neural network is capable of reducing the input vector to a great degree (approximately 1/30th the original size) and also able to reconstruct the input pattern at the output layer from this reduced code units. The feature set (output of central hidden layer) extracted from this network is fed to Forgy's algorithm, which classify input data patterns into distinguishable classes. In the implementation of Forgy's algorithm, initial seed points are selected in such a way that they are distant enough to be perfectly grouped into different categories. Thus a new method of unsupervised learning is formulated and demonstrated in this paper. This method gave impressive results when applied to classification of different image patterns.",
        "published": "2007-12-06T13:52:04Z",
        "link": "http://arxiv.org/abs/0712.0938v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Dimensionality Reduction and Reconstruction using Mirroring Neural   Networks and Object Recognition based on Reduced Dimension Characteristic   Vector",
        "authors": [
            "Dasika Ratna Deepthi",
            "Sujeet Kuchibhotla",
            "K. Eswaran"
        ],
        "summary": "In this paper, we present a Mirroring Neural Network architecture to perform non-linear dimensionality reduction and Object Recognition using a reduced lowdimensional characteristic vector. In addition to dimensionality reduction, the network also reconstructs (mirrors) the original high-dimensional input vector from the reduced low-dimensional data. The Mirroring Neural Network architecture has more number of processing elements (adalines) in the outer layers and the least number of elements in the central layer to form a converging-diverging shape in its configuration. Since this network is able to reconstruct the original image from the output of the innermost layer (which contains all the information about the input pattern), these outputs can be used as object signature to classify patterns. The network is trained to minimize the discrepancy between actual output and the input by back propagating the mean squared error from the output layer to the input layer. After successfully training the network, it can reduce the dimension of input vectors and mirror the patterns fed to it. The Mirroring Neural Network architecture gave very good results on various test patterns.",
        "published": "2007-12-06T14:11:07Z",
        "link": "http://arxiv.org/abs/0712.0932v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "A Common View on Strong, Uniform, and Other Notions of Equivalence in   Answer-Set Programming",
        "authors": [
            "Stefan Woltran"
        ],
        "summary": "Logic programming under the answer-set semantics nowadays deals with numerous different notions of program equivalence. This is due to the fact that equivalence for substitution (known as strong equivalence) and ordinary equivalence are different concepts. The former holds, given programs P and Q, iff P can be faithfully replaced by Q within any context R, while the latter holds iff P and Q provide the same output, that is, they have the same answer sets. Notions in between strong and ordinary equivalence have been introduced as theoretical tools to compare incomplete programs and are defined by either restricting the syntactic structure of the considered context programs R or by bounding the set A of atoms allowed to occur in R (relativized equivalence).For the latter approach, different A yield properly different equivalence notions, in general. For the former approach, however, it turned out that any ``reasonable'' syntactic restriction to R coincides with either ordinary, strong, or uniform equivalence. In this paper, we propose a parameterization for equivalence notions which takes care of both such kinds of restrictions simultaneously by bounding, on the one hand, the atoms which are allowed to occur in the rule heads of the context and, on the other hand, the atoms which are allowed to occur in the rule bodies of the context. We introduce a general semantical characterization which includes known ones as SE-models (for strong equivalence) or UE-models (for uniform equivalence) as special cases. Moreover,we provide complexity bounds for the problem in question and sketch a possible implementation method.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2007-12-06T14:26:42Z",
        "link": "http://arxiv.org/abs/0712.0948v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "On Using Unsatisfiability for Solving Maximum Satisfiability",
        "authors": [
            "Joao Marques-Silva",
            "Jordi Planes"
        ],
        "summary": "Maximum Satisfiability (MaxSAT) is a well-known optimization pro- blem, with several practical applications. The most widely known MAXS AT algorithms are ineffective at solving hard problems instances from practical application domains. Recent work proposed using efficient Boolean Satisfiability (SAT) solvers for solving the MaxSAT problem, based on identifying and eliminating unsatisfiable subformulas. However, these algorithms do not scale in practice. This paper analyzes existing MaxSAT algorithms based on unsatisfiable subformula identification. Moreover, the paper proposes a number of key optimizations to these MaxSAT algorithms and a new alternative algorithm. The proposed optimizations and the new algorithm provide significant performance improvements on MaxSAT instances from practical applications. Moreover, the efficiency of the new generation of unsatisfiability-based MaxSAT solvers becomes effectively indexed to the ability of modern SAT solvers to proving unsatisfiability and identifying unsatisfiable subformulas.",
        "published": "2007-12-07T09:21:58Z",
        "link": "http://arxiv.org/abs/0712.1097v1",
        "categories": [
            "cs.AI",
            "cs.DS"
        ]
    },
    {
        "title": "Cumulative and Averaging Fission of Beliefs",
        "authors": [
            "Audun Josang"
        ],
        "summary": "Belief fusion is the principle of combining separate beliefs or bodies of evidence originating from different sources. Depending on the situation to be modelled, different belief fusion methods can be applied. Cumulative and averaging belief fusion is defined for fusing opinions in subjective logic, and for fusing belief functions in general. The principle of fission is the opposite of fusion, namely to eliminate the contribution of a specific belief from an already fused belief, with the purpose of deriving the remaining belief. This paper describes fission of cumulative belief as well as fission of averaging belief in subjective logic. These operators can for example be applied to belief revision in Bayesian belief networks, where the belief contribution of a given evidence source can be determined as a function of a given fused belief and its other contributing beliefs.",
        "published": "2007-12-07T16:42:07Z",
        "link": "http://arxiv.org/abs/0712.1182v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "About Algorithm for Transformation of Logic Functions (ATLF)",
        "authors": [
            "Lev Cherbanski"
        ],
        "summary": "In this article the algorithm for transformation of logic functions which are given by truth tables is considered. The suggested algorithm allows the transformation of many-valued logic functions with the required number of variables and can be looked in this sense as universal.",
        "published": "2007-12-08T22:36:44Z",
        "link": "http://arxiv.org/abs/0712.1310v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Sequential operators in computability logic",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Computability logic (CL) (see http://www.cis.upenn.edu/~giorgi/cl.html) is a semantical platform and research program for redeveloping logic as a formal theory of computability, as opposed to the formal theory of truth which it has more traditionally been. Formulas in CL stand for (interactive) computational problems, understood as games between a machine and its environment; logical operators represent operations on such entities; and \"truth\" is understood as existence of an effective solution, i.e., of an algorithmic winning strategy.   The formalism of CL is open-ended, and may undergo series of extensions as the study of the subject advances. The main groups of operators on which CL has been focused so far are the parallel, choice, branching, and blind operators. The present paper introduces a new important group of operators, called sequential. The latter come in the form of sequential conjunction and disjunction, sequential quantifiers, and sequential recurrences. As the name may suggest, the algorithmic intuitions associated with this group are those of sequential computations, as opposed to the intuitions of parallel computations associated with the parallel group of operations: playing a sequential combination of games means playing its components in a sequential fashion, one after one.   The main technical result of the present paper is a sound and complete axiomatization of the propositional fragment of computability logic whose vocabulary, together with negation, includes all three -- parallel, choice and sequential -- sorts of conjunction and disjunction. An extension of this result to the first-order level is also outlined.",
        "published": "2007-12-09T16:59:35Z",
        "link": "http://arxiv.org/abs/0712.1345v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Population stratification using a statistical model on hypergraphs",
        "authors": [
            "Alexei Vazquez"
        ],
        "summary": "Population stratification is a problem encountered in several areas of biology and public health. We tackle this problem by mapping a population and its elements attributes into a hypergraph, a natural extension of the concept of graph or network to encode associations among any number of elements. On this hypergraph, we construct a statistical model reflecting our intuition about how the elements attributes can emerge from a postulated population structure. Finally, we introduce the concept of stratification representativeness as a mean to identify the simplest stratification already containing most of the information about the population structure. We demonstrate the power of this framework stratifying an animal and a human population based on phenotypic and genotypic properties, respectively.",
        "published": "2007-12-09T20:53:45Z",
        "link": "http://arxiv.org/abs/0712.1365v1",
        "categories": [
            "q-bio.PE",
            "cs.AI",
            "physics.data-an"
        ]
    },
    {
        "title": "Numerical Sensitivity and Efficiency in the Treatment of Epistemic and   Aleatory Uncertainty",
        "authors": [
            "Eric Chojnacki",
            "Jean Baccou",
            "Sébastien Destercke"
        ],
        "summary": "The treatment of both aleatory and epistemic uncertainty by recent methods often requires an high computational effort. In this abstract, we propose a numerical sampling method allowing to lighten the computational burden of treating the information by means of so-called fuzzy random variables.",
        "published": "2007-12-13T12:49:30Z",
        "link": "http://arxiv.org/abs/0712.2141v1",
        "categories": [
            "cs.AI",
            "math.PR"
        ]
    },
    {
        "title": "Decomposition During Search for Propagation-Based Constraint Solvers",
        "authors": [
            "Martin Mann",
            "Guido Tack",
            "Sebastian Will"
        ],
        "summary": "We describe decomposition during search (DDS), an integration of And/Or tree search into propagation-based constraint solvers. The presented search algorithm dynamically decomposes sub-problems of a constraint satisfaction problem into independent partial problems, avoiding redundant work.   The paper discusses how DDS interacts with key features that make propagation-based solvers successful: constraint propagation, especially for global constraints, and dynamic search heuristics.   We have implemented DDS for the Gecode constraint programming library. Two applications, solution counting in graph coloring and protein structure prediction, exemplify the benefits of DDS in practice.",
        "published": "2007-12-14T18:08:26Z",
        "link": "http://arxiv.org/abs/0712.2389v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Common knowledge logic in a higher order proof assistant?",
        "authors": [
            "Pierre Lescanne"
        ],
        "summary": "This paper presents experiments on common knowledge logic, conducted with the help of the proof assistant Coq. The main feature of common knowledge logic is the eponymous modality that says that a group of agents shares a knowledge about a certain proposition in a inductive way. This modality is specified by using a fixpoint approach. Furthermore, from these experiments, we discuss and compare the structure of theorems that can be proved in specific theories that use common knowledge logic. Those structures manifests the interplay between the theory (as implemented in the proof assistant Coq) and the metatheory.",
        "published": "2007-12-19T10:25:34Z",
        "link": "http://arxiv.org/abs/0712.3147v2",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Universal Intelligence: A Definition of Machine Intelligence",
        "authors": [
            "Shane Legg",
            "Marcus Hutter"
        ],
        "summary": "A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.",
        "published": "2007-12-20T05:50:54Z",
        "link": "http://arxiv.org/abs/0712.3329v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Improving the Performance of PieceWise Linear Separation Incremental   Algorithms for Practical Hardware Implementations",
        "authors": [
            "Alejandro Chinea Manrique De Lara",
            "Juan Manuel Moreno",
            "Arostegui Jordi Madrenas",
            "Joan Cabestany"
        ],
        "summary": "In this paper we shall review the common problems associated with Piecewise Linear Separation incremental algorithms. This kind of neural models yield poor performances when dealing with some classification problems, due to the evolving schemes used to construct the resulting networks. So as to avoid this undesirable behavior we shall propose a modification criterion. It is based upon the definition of a function which will provide information about the quality of the network growth process during the learning phase. This function is evaluated periodically as the network structure evolves, and will permit, as we shall show through exhaustive benchmarks, to considerably improve the performance(measured in terms of network complexity and generalization capabilities) offered by the networks generated by these incremental models.",
        "published": "2007-12-21T10:05:52Z",
        "link": "http://arxiv.org/abs/0712.3654v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Tests of Machine Intelligence",
        "authors": [
            "Shane Legg",
            "Marcus Hutter"
        ],
        "summary": "Although the definition and measurement of intelligence is clearly of fundamental importance to the field of artificial intelligence, no general survey of definitions and tests of machine intelligence exists. Indeed few researchers are even aware of alternatives to the Turing test and its many derivatives. In this paper we fill this gap by providing a short survey of the many tests of machine intelligence that have been proposed.",
        "published": "2007-12-22T01:17:24Z",
        "link": "http://arxiv.org/abs/0712.3825v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "TRUST-TECH based Methods for Optimization and Learning",
        "authors": [
            "Chandan K. Reddy"
        ],
        "summary": "Many problems that arise in machine learning domain deal with nonlinearity and quite often demand users to obtain global optimal solutions rather than local optimal ones. Optimization problems are inherent in machine learning algorithms and hence many methods in machine learning were inherited from the optimization literature. Popularly known as the initialization problem, the ideal set of parameters required will significantly depend on the given initialization values. The recently developed TRUST-TECH (TRansformation Under STability-reTaining Equilibria CHaracterization) methodology systematically explores the subspace of the parameters to obtain a complete set of local optimal solutions. In this thesis work, we propose TRUST-TECH based methods for solving several optimization and machine learning problems. Two stages namely, the local stage and the neighborhood-search stage, are repeated alternatively in the solution space to achieve improvements in the quality of the solutions. Our methods were tested on both synthetic and real datasets and the advantages of using this novel framework are clearly manifested. This framework not only reduces the sensitivity to initialization, but also allows the flexibility for the practitioners to use various global and local methods that work well for a particular problem of interest. Other hierarchical stochastic algorithms like evolutionary algorithms and smoothing algorithms are also studied and frameworks for combining these methods with TRUST-TECH have been proposed and evaluated on several test systems.",
        "published": "2007-12-25T03:14:32Z",
        "link": "http://arxiv.org/abs/0712.4126v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.MS",
            "cs.NA",
            "cs.NE",
            "G.1.6; I.5.3; I.5.1"
        ]
    },
    {
        "title": "Convergence of Expected Utilities with Algorithmic Probability   Distributions",
        "authors": [
            "Peter de Blanc"
        ],
        "summary": "We consider an agent interacting with an unknown environment. The environment is a function which maps natural numbers to natural numbers; the agent's set of hypotheses about the environment contains all such functions which are computable and compatible with a finite set of known input-output pairs, and the agent assigns a positive probability to each such hypothesis. We do not require that this probability distribution be computable, but it must be bounded below by a positive computable function. The agent has a utility function on outputs from the environment. We show that if this utility function is bounded below in absolute value by an unbounded computable function, then the expected utility of any input is undefined. This implies that a computable utility function will have convergent expected utilities iff that function is bounded.",
        "published": "2007-12-28T07:50:00Z",
        "link": "http://arxiv.org/abs/0712.4318v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Judgment",
        "authors": [
            "Ruadhan O'Flanagan"
        ],
        "summary": "The concept of a judgment as a logical action which introduces new information into a deductive system is examined. This leads to a way of mathematically representing implication which is distinct from the familiar material implication, according to which \"If A then B\" is considered to be equivalent to \"B or not-A\". This leads, in turn, to a resolution of the paradox of the raven.",
        "published": "2007-12-28T21:00:01Z",
        "link": "http://arxiv.org/abs/0712.4402v3",
        "categories": [
            "math.PR",
            "cs.AI",
            "math.LO",
            "60A05 (Primary) 03B42, 03B48 (Secondary)"
        ]
    },
    {
        "title": "Does intelligence imply contradiction?",
        "authors": [
            "Patrizio Frosini"
        ],
        "summary": "Contradiction is often seen as a defect of intelligent systems and a dangerous limitation on efficiency. In this paper we raise the question of whether, on the contrary, it could be considered a key tool in increasing intelligence in biological structures. A possible way of answering this question in a mathematical context is shown, formulating a proposition that suggests a link between intelligence and contradiction.   A concrete approach is presented in the well-defined setting of cellular automata. Here we define the models of ``observer'', ``entity'', ``environment'', ``intelligence'' and ``contradiction''. These definitions, which roughly correspond to the common meaning of these words, allow us to deduce a simple but strong result about these concepts in an unbiased, mathematical manner. Evidence for a real-world counterpart to the demonstrated formal link between intelligence and contradiction is provided by three computational experiments.",
        "published": "2007-12-31T19:07:22Z",
        "link": "http://arxiv.org/abs/0801.0232v2",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Time Series Forecasting: Obtaining Long Term Trends with Self-Organizing   Maps",
        "authors": [
            "Geoffroy Simon",
            "Amaury Lendasse",
            "Marie Cottrell",
            "Jean-Claude Fort",
            "Michel Verleysen"
        ],
        "summary": "Kohonen self-organisation maps are a well know classification tool, commonly used in a wide variety of problems, but with limited applications in time series forecasting context. In this paper, we propose a forecasting method specifically designed for multi-dimensional long-term trends prediction, with a double application of the Kohonen algorithm. Practical applications of the method are also presented.",
        "published": "2007-01-08T17:03:31Z",
        "link": "http://arxiv.org/abs/cs/0701052v1",
        "categories": [
            "cs.LG",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Strategies for prediction under imperfect monitoring",
        "authors": [
            "Gabor Lugosi",
            "Shie Mannor",
            "Gilles Stoltz"
        ],
        "summary": "We propose simple randomized strategies for sequential prediction under imperfect monitoring, that is, when the forecaster does not have access to the past outcomes but rather to a feedback signal. The proposed strategies are consistent in the sense that they achieve, asymptotically, the best possible average reward. It was Rustichini (1999) who first proved the existence of such consistent predictors. The forecasters presented here offer the first constructive proof of consistency. Moreover, the proposed algorithms are computationally efficient. We also establish upper bounds for the rates of convergence. In the case of deterministic feedback, these rates are optimal up to logarithmic terms.",
        "published": "2007-01-15T16:45:38Z",
        "link": "http://arxiv.org/abs/math/0701419v4",
        "categories": [
            "math.ST",
            "cs.LG",
            "stat.TH",
            "91A20, 62L12, 68Q32"
        ]
    },
    {
        "title": "A Delta Debugger for ILP Query Execution",
        "authors": [
            "Remko Troncon",
            "Gerda Janssens"
        ],
        "summary": "Because query execution is the most crucial part of Inductive Logic Programming (ILP) algorithms, a lot of effort is invested in developing faster execution mechanisms. These execution mechanisms typically have a low-level implementation, making them hard to debug. Moreover, other factors such as the complexity of the problems handled by ILP algorithms and size of the code base of ILP data mining systems make debugging at this level a very difficult job. In this work, we present the trace-based debugging approach currently used in the development of new execution mechanisms in hipP, the engine underlying the ACE Data Mining system. This debugger uses the delta debugging algorithm to automatically reduce the total time needed to expose bugs in ILP execution, thus making manual debugging step much lighter.",
        "published": "2007-01-17T13:35:35Z",
        "link": "http://arxiv.org/abs/cs/0701105v1",
        "categories": [
            "cs.PL",
            "cs.LG"
        ]
    },
    {
        "title": "Algorithmic Complexity Bounds on Future Prediction Errors",
        "authors": [
            "A. Chernov",
            "M. Hutter",
            "J. Schmidhuber"
        ],
        "summary": "We bound the future loss when predicting any (computably) stochastic sequence online. Solomonoff finitely bounded the total deviation of his universal predictor $M$ from the true distribution $mu$ by the algorithmic complexity of $mu$. Here we assume we are at a time $t>1$ and already observed $x=x_1...x_t$. We bound the future prediction performance on $x_{t+1}x_{t+2}...$ by a new variant of algorithmic complexity of $mu$ given $x$, plus the complexity of the randomness deficiency of $x$. The new complexity is monotone in its condition in the sense that this complexity can only decrease if the condition is prolonged. We also briefly discuss potential generalizations to Bayesian model classes and to classification problems.",
        "published": "2007-01-19T07:36:40Z",
        "link": "http://arxiv.org/abs/cs/0701120v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Universal Algorithmic Intelligence: A mathematical top->down approach",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Sequential decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameter-free theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline how the AIXI model can formally solve a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXItl that is still effectively more intelligent than any other time t and length l bounded agent. The computation time of AIXItl is of the order t x 2^l. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches.",
        "published": "2007-01-20T00:18:06Z",
        "link": "http://arxiv.org/abs/cs/0701125v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "The Loss Rank Principle for Model Selection",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "We introduce a new principle for model selection in regression and classification. Many regression models are controlled by some smoothness or flexibility or complexity parameter c, e.g. the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials. Let f_D^c be the (best) regressor of complexity c on data D. A more flexible regressor can fit more data D' well than a more rigid one. If something (here small loss) is easy to achieve it's typically worth less. We define the loss rank of f_D^c as the number of other (fictitious) data D' that are fitted better by f_D'^c than D is fitted by f_D^c. We suggest selecting the model complexity c that has minimal loss rank (LoRP). Unlike most penalized maximum likelihood variants (AIC,BIC,MDL), LoRP only depends on the regression function and loss function. It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN. In this paper we formalize, discuss, and motivate LoRP, study it for specific regression problems, in particular linear ones, and compare it to other model selection schemes.",
        "published": "2007-02-27T03:31:34Z",
        "link": "http://arxiv.org/abs/math/0702804v1",
        "categories": [
            "math.ST",
            "cs.LG",
            "stat.ME",
            "stat.ML",
            "stat.TH"
        ]
    },
    {
        "title": "Support and Quantile Tubes",
        "authors": [
            "Kristiaan Pelckmans",
            "Jos De Brabanter",
            "Johan A. K. Suykens",
            "Bart De Moor"
        ],
        "summary": "This correspondence studies an estimator of the conditional support of a distribution underlying a set of i.i.d. observations. The relation with mutual information is shown via an extension of Fano's theorem in combination with a generalization bound based on a compression argument. Extensions to estimating the conditional quantile interval, and statistical guarantees on the minimal convex hull are given.",
        "published": "2007-03-12T19:14:23Z",
        "link": "http://arxiv.org/abs/cs/0703055v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Bandit Algorithms for Tree Search",
        "authors": [
            "Pierre-Arnaud Coquelin",
            "Rémi Munos"
        ],
        "summary": "Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT algorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper Confidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is too ``optimistic'' in some cases, leading to a regret O(exp(exp(D))) where D is the depth of the tree. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially with the horizon depth is proven to have a regret O(2^D \\sqrt{n}), but does not adapt to possible smoothness in the tree. We then analyze Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth Trees which takes into account actual smoothness of the rewards for performing efficient ``cuts'' of sub-optimal branches with high confidence. Finally, we present an incremental tree search version which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, essentially only the optimal branches is indefinitely developed. We illustrate these methods on a global optimization problem of a Lipschitz function, given noisy data.",
        "published": "2007-03-13T08:53:41Z",
        "link": "http://arxiv.org/abs/cs/0703062v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Intrinsic dimension of a dataset: what properties does one expect?",
        "authors": [
            "Vladimir Pestov"
        ],
        "summary": "We propose an axiomatic approach to the concept of an intrinsic dimension of a dataset, based on a viewpoint of geometry of high-dimensional structures. Our first axiom postulates that high values of dimension be indicative of the presence of the curse of dimensionality (in a certain precise mathematical sense). The second axiom requires the dimension to depend smoothly on a distance between datasets (so that the dimension of a dataset and that of an approximating principal manifold would be close to each other). The third axiom is a normalization condition: the dimension of the Euclidean $n$-sphere $\\s^n$ is $\\Theta(n)$. We give an example of a dimension function satisfying our axioms, even though it is in general computationally unfeasible, and discuss a computationally cheap function satisfying most but not all of our axioms (the ``intrinsic dimensionality'' of Ch\\'avez et al.)",
        "published": "2007-03-25T01:19:14Z",
        "link": "http://arxiv.org/abs/cs/0703125v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Structure induction by lossless graph compression",
        "authors": [
            "Leonid Peshkin"
        ],
        "summary": "This work is motivated by the necessity to automate the discovery of structure in vast and evergrowing collection of relational data commonly represented as graphs, for example genomic networks. A novel algorithm, dubbed Graphitour, for structure induction by lossless graph compression is presented and illustrated by a clear and broadly known case of nested structure in a DNA molecule. This work extends to graphs some well established approaches to grammatical inference previously applied only to strings. The bottom-up graph compression problem is related to the maximum cardinality (non-bipartite) maximum cardinality matching problem. The algorithm accepts a variety of graph types including directed graphs and graphs with labeled nodes and arcs. The resulting structure could be used for representation and classification of graphs.",
        "published": "2007-03-27T05:46:31Z",
        "link": "http://arxiv.org/abs/cs/0703132v1",
        "categories": [
            "cs.DS",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "I.2.6; G.2.2; E.1; E.4; F.4.2; G.2.3; I.3.5; I.4.2; I.5.3; J.3"
        ]
    },
    {
        "title": "Reinforcement Learning for Adaptive Routing",
        "authors": [
            "Leonid Peshkin",
            "Virginia Savova"
        ],
        "summary": "Reinforcement learning means learning a policy--a mapping of observations into actions--based on feedback from the environment. The learning can be viewed as browsing a set of policies while evaluating them by trial through interaction with the environment. We present an application of gradient ascent algorithm for reinforcement learning to a complex domain of packet routing in network communication and compare the performance of this algorithm to other routing methods on a benchmark problem.",
        "published": "2007-03-28T04:41:54Z",
        "link": "http://arxiv.org/abs/cs/0703138v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NI",
            "C.2.1; C.2.2; C.2.4; C.2.6; F.1.1; I.2.6; I.2.8; I.2.9"
        ]
    },
    {
        "title": "Learning from compressed observations",
        "authors": [
            "Maxim Raginsky"
        ],
        "summary": "The problem of statistical learning is to construct a predictor of a random variable $Y$ as a function of a related random variable $X$ on the basis of an i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable predictors are drawn from some specified class, and the goal is to approach asymptotically the performance (expected loss) of the best predictor in the class. We consider the setting in which one has perfect observation of the $X$-part of the sample, while the $Y$-part has to be communicated at some finite bit rate. The encoding of the $Y$-values is allowed to depend on the $X$-values. Under suitable regularity conditions on the admissible predictors, the underlying family of probability distributions and the loss function, we give an information-theoretic characterization of achievable predictor performance in terms of conditional distortion-rate functions. The ideas are illustrated on the example of nonparametric regression in Gaussian noise.",
        "published": "2007-04-05T02:57:15Z",
        "link": "http://arxiv.org/abs/0704.0671v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Sensor Networks with Random Links: Topology Design for Distributed   Consensus",
        "authors": [
            "Soummya Kar",
            "Jose M. F. Moura"
        ],
        "summary": "In a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. The signal-to-noise ratio (SNR) is usually a main factor in determining the probability of error (or of communication failure) in a link. These probabilities are then a proxy for the SNR under which the links operate. The paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. To consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. With these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. We show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.",
        "published": "2007-04-06T21:58:52Z",
        "link": "http://arxiv.org/abs/0704.0954v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "The on-line shortest path problem under partial monitoring",
        "authors": [
            "Andras Gyorgy",
            "Tamas Linder",
            "Gabor Lugosi",
            "Gyorgy Ottucsak"
        ],
        "summary": "The on-line shortest path problem is considered under various models of partial monitoring. Given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. In a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. For this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on the number of edges of the graph. The algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges. An extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. Another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. A version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. Applications to routing in packet switched networks along with simulation results are also presented.",
        "published": "2007-04-08T10:15:54Z",
        "link": "http://arxiv.org/abs/0704.1020v1",
        "categories": [
            "cs.LG",
            "cs.SC",
            "C.2.1; C.2.2; F.1.1; I.2.6; I.2.8"
        ]
    },
    {
        "title": "A neural network approach to ordinal regression",
        "authors": [
            "Jianlin Cheng"
        ],
        "summary": "Ordinal regression is an important type of learning, which has properties of both classification and regression. Here we describe a simple and effective approach to adapt a traditional neural network to learn ordinal categories. Our approach is a generalization of the perceptron method for ordinal regression. On several benchmark datasets, our method (NNRank) outperforms a neural network classification method. Compared with the ordinal regression methods using Gaussian processes and support vector machines, NNRank achieves comparable performance. Moreover, NNRank has the advantages of traditional neural networks: learning in both online and batch modes, handling very large training datasets, and making rapid predictions. These features make NNRank a useful and complementary tool for large-scale data processing tasks such as information retrieval, web page ranking, collaborative filtering, and protein ranking in Bioinformatics.",
        "published": "2007-04-08T17:36:00Z",
        "link": "http://arxiv.org/abs/0704.1028v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Parametric Learning and Monte Carlo Optimization",
        "authors": [
            "David H. Wolpert",
            "Dev G. Rajnarayan"
        ],
        "summary": "This paper uncovers and explores the close relationship between Monte Carlo Optimization of a parametrized integral (MCO), Parametric machine-Learning (PL), and `blackbox' or `oracle'-based optimization (BO). We make four contributions. First, we prove that MCO is mathematically identical to a broad class of PL problems. This identity potentially provides a new application domain for all broadly applicable PL techniques: MCO. Second, we introduce immediate sampling, a new version of the Probability Collectives (PC) algorithm for blackbox optimization. Immediate sampling transforms the original BO problem into an MCO problem. Accordingly, by combining these first two contributions, we can apply all PL techniques to BO. In our third contribution we validate this way of improving BO by demonstrating that cross-validation and bagging improve immediate sampling. Finally, conventional MC and MCO procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. We demonstrate that one can exploit the sample location information using PL techniques, for example by forming a fit of the sample locations to the associated values of the integrand. This provides an additional way to apply PL techniques to improve MCO.",
        "published": "2007-04-10T17:01:07Z",
        "link": "http://arxiv.org/abs/0704.1274v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Preconditioned Temporal Difference Learning",
        "authors": [
            "Yao HengShuai"
        ],
        "summary": "This paper has been withdrawn by the author. This draft is withdrawn for its poor quality in english, unfortunately produced by the author when he was just starting his science route. Look at the ICML version instead: http://icml2008.cs.helsinki.fi/papers/111.pdf",
        "published": "2007-04-11T13:17:01Z",
        "link": "http://arxiv.org/abs/0704.1409v3",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "A Note on the Inapproximability of Correlation Clustering",
        "authors": [
            "Jinsong Tan"
        ],
        "summary": "We consider inapproximability of the correlation clustering problem defined as follows: Given a graph $G = (V,E)$ where each edge is labeled either \"+\" (similar) or \"-\" (dissimilar), correlation clustering seeks to partition the vertices into clusters so that the number of pairs correctly (resp. incorrectly) classified with respect to the labels is maximized (resp. minimized). The two complementary problems are called MaxAgree and MinDisagree, respectively, and have been studied on complete graphs, where every edge is labeled, and general graphs, where some edge might not have been labeled. Natural edge-weighted versions of both problems have been studied as well. Let S-MaxAgree denote the weighted problem where all weights are taken from set S, we show that S-MaxAgree with weights bounded by $O(|V|^{1/2-\\delta})$ essentially belongs to the same hardness class in the following sense: if there is a polynomial time algorithm that approximates S-MaxAgree within a factor of $\\lambda = O(\\log{|V|})$ with high probability, then for any choice of S', S'-MaxAgree can be approximated in polynomial time within a factor of $(\\lambda + \\epsilon)$, where $\\epsilon > 0$ can be arbitrarily small, with high probability. A similar statement also holds for $S-MinDisagree. This result implies it is hard (assuming $NP \\neq RP$) to approximate unweighted MaxAgree within a factor of $80/79-\\epsilon$, improving upon a previous known factor of $116/115-\\epsilon$ by Charikar et. al. \\cite{Chari05}.",
        "published": "2007-04-17T03:52:41Z",
        "link": "http://arxiv.org/abs/0704.2092v2",
        "categories": [
            "cs.LG",
            "cs.DS",
            "F.2.0"
        ]
    },
    {
        "title": "Joint universal lossy coding and identification of stationary mixing   sources",
        "authors": [
            "Maxim Raginsky"
        ],
        "summary": "The problem of joint universal source coding and modeling, treated in the context of lossless codes by Rissanen, was recently generalized to fixed-rate lossy coding of finitely parametrized continuous-alphabet i.i.d. sources. We extend these results to variable-rate lossy block coding of stationary ergodic sources and show that, for bounded metric distortion measures, any finitely parametrized family of stationary sources satisfying suitable mixing, smoothness and Vapnik-Chervonenkis learnability conditions admits universal schemes for joint lossy source coding and identification. We also give several explicit examples of parametric sources satisfying the regularity conditions.",
        "published": "2007-04-20T01:25:22Z",
        "link": "http://arxiv.org/abs/0704.2644v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Supervised Feature Selection via Dependence Estimation",
        "authors": [
            "Le Song",
            "Alex Smola",
            "Arthur Gretton",
            "Karsten Borgwardt",
            "Justin Bedo"
        ],
        "summary": "We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.",
        "published": "2007-04-20T08:26:29Z",
        "link": "http://arxiv.org/abs/0704.2668v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Equivalence of LP Relaxation and Max-Product for Weighted Matching in   General Graphs",
        "authors": [
            "Sujay Sanghavi"
        ],
        "summary": "Max-product belief propagation is a local, iterative algorithm to find the mode/MAP estimate of a probability distribution. While it has been successfully employed in a wide variety of applications, there are relatively few theoretical guarantees of convergence and correctness for general loopy graphs that may have many short cycles. Of these, even fewer provide exact ``necessary and sufficient'' characterizations.   In this paper we investigate the problem of using max-product to find the maximum weight matching in an arbitrary graph with edge weights. This is done by first constructing a probability distribution whose mode corresponds to the optimal matching, and then running max-product. Weighted matching can also be posed as an integer program, for which there is an LP relaxation. This relaxation is not always tight. In this paper we show that \\begin{enumerate} \\item If the LP relaxation is tight, then max-product always converges, and that too to the correct answer. \\item If the LP relaxation is loose, then max-product does not converge. \\end{enumerate} This provides an exact, data-dependent characterization of max-product performance, and a precise connection to LP relaxation, which is a well-studied optimization technique. Also, since LP relaxation is known to be tight for bipartite graphs, our results generalize other recent results on using max-product to find weighted matchings in bipartite graphs.",
        "published": "2007-05-05T18:57:47Z",
        "link": "http://arxiv.org/abs/0705.0760v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "cs.LG",
            "cs.NI",
            "math.IT"
        ]
    },
    {
        "title": "HMM Speaker Identification Using Linear and Non-linear Merging   Techniques",
        "authors": [
            "Unathi Mahola",
            "Fulufhelo V. Nelwamondo",
            "Tshilidzi Marwala"
        ],
        "summary": "Speaker identification is a powerful, non-invasive and in-expensive biometric technique. The recognition accuracy, however, deteriorates when noise levels affect a specific band of frequency. In this paper, we present a sub-band based speaker identification that intends to improve the live testing performance. Each frequency sub-band is processed and classified independently. We also compare the linear and non-linear merging techniques for the sub-bands recognizer. Support vector machines and Gaussian Mixture models are the non-linear merging techniques that are investigated. Results showed that the sub-band based method used with linear merging techniques enormously improved the performance of the speaker identification over the performance of wide-band recognizers when tested live. A live testing improvement of 9.78% was achieved",
        "published": "2007-05-11T04:54:54Z",
        "link": "http://arxiv.org/abs/0705.1585v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Statistical Mechanics of Nonlinear On-line Learning for Ensemble   Teachers",
        "authors": [
            "Hideto Utsumi",
            "Seiji Miyoshi",
            "Masato Okada"
        ],
        "summary": "We analyze the generalization performance of a student in a model composed of nonlinear perceptrons: a true teacher, ensemble teachers, and the student. We calculate the generalization error of the student analytically or numerically using statistical mechanics in the framework of on-line learning. We treat two well-known learning rules: Hebbian learning and perceptron learning. As a result, it is proven that the nonlinear model shows qualitatively different behaviors from the linear model. Moreover, it is clarified that Hebbian learning and perceptron learning show qualitatively different behaviors from each other. In Hebbian learning, we can analytically obtain the solutions. In this case, the generalization error monotonically decreases. The steady value of the generalization error is independent of the learning rate. The larger the number of teachers is and the more variety the ensemble teachers have, the smaller the generalization error is. In perceptron learning, we have to numerically obtain the solutions. In this case, the dynamical behaviors of the generalization error are non-monotonic. The smaller the learning rate is, the larger the number of teachers is; and the more variety the ensemble teachers have, the smaller the minimum value of the generalization error is.",
        "published": "2007-05-16T09:58:39Z",
        "link": "http://arxiv.org/abs/0705.2318v1",
        "categories": [
            "cs.LG",
            "cond-mat.dis-nn"
        ]
    },
    {
        "title": "On the monotonization of the training set",
        "authors": [
            "Rustem Takhanov"
        ],
        "summary": "We consider the problem of minimal correction of the training set to make it consistent with monotonic constraints. This problem arises during analysis of data sets via techniques that require monotone data. We show that this problem is NP-hard in general and is equivalent to finding a maximal independent set in special orgraphs. Practically important cases of that problem considered in detail. These are the cases when a partial order given on the replies set is a total order or has a dimension 2. We show that the second case can be reduced to maximization of a quadratic convex function on a convex set. For this case we construct an approximate polynomial algorithm based on convex optimization.",
        "published": "2007-05-18T19:44:19Z",
        "link": "http://arxiv.org/abs/0705.2765v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "Mixed membership stochastic blockmodels",
        "authors": [
            "Edoardo M Airoldi",
            "David M Blei",
            "Stephen E Fienberg",
            "Eric P Xing"
        ],
        "summary": "Observations consisting of measurements on relationships for pairs of objects arise in many settings, such as protein interaction and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold. In this paper, we describe a latent variable model of such data called the mixed membership stochastic blockmodel. This model extends blockmodels for relational data to ones which capture mixed membership latent relational structure, thus providing an object-specific low-dimensional representation. We develop a general variational inference algorithm for fast approximate posterior inference. We explore applications to social and protein interaction networks.",
        "published": "2007-05-30T23:22:59Z",
        "link": "http://arxiv.org/abs/0705.4485v1",
        "categories": [
            "stat.ME",
            "cs.LG",
            "math.ST",
            "physics.soc-ph",
            "stat.ML",
            "stat.TH"
        ]
    },
    {
        "title": "Loop corrections for message passing algorithms in continuous variable   models",
        "authors": [
            "Bastian Wemmenhove",
            "Bert Kappen"
        ],
        "summary": "In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model. Using the exactness of the averages for belief propagation for Gaussian models, a different way of obtaining the covariances is found, based on Belief Propagation on cavity graphs. We discuss the relation of this loop correction algorithm to Expectation Propagation algorithms for the case in which the model is no longer Gaussian, but slightly perturbed by nonlinear terms.",
        "published": "2007-05-31T10:35:07Z",
        "link": "http://arxiv.org/abs/0705.4566v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "A Novel Model of Working Set Selection for SMO Decomposition Methods",
        "authors": [
            "Zhendong Zhao",
            "Lei Yuan",
            "Yuxuan Wang",
            "Forrest Sheng Bao",
            "Shunyi Zhang Yanfei Sun"
        ],
        "summary": "In the process of training Support Vector Machines (SVMs) by decomposition methods, working set selection is an important technique, and some exciting schemes were employed into this field. To improve working set selection, we propose a new model for working set selection in sequential minimal optimization (SMO) decomposition methods. In this model, it selects B as working set without reselection. Some properties are given by simple proof, and experiments demonstrate that the proposed method is in general faster than existing methods.",
        "published": "2007-06-05T05:55:07Z",
        "link": "http://arxiv.org/abs/0706.0585v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Getting started in probabilistic graphical models",
        "authors": [
            "Edoardo M Airoldi"
        ],
        "summary": "Probabilistic graphical models (PGMs) have become a popular tool for computational analysis of biological data in a variety of domains. But, what exactly are they and how do they work? How can we use PGMs to discover patterns that are biologically relevant? And to what extent can PGMs help us formulate new hypotheses that are testable at the bench? This note sketches out some answers and illustrates the main ideas behind the statistical approach to biological pattern discovery.",
        "published": "2007-06-14T14:52:06Z",
        "link": "http://arxiv.org/abs/0706.2040v2",
        "categories": [
            "q-bio.QM",
            "cs.LG",
            "physics.soc-ph",
            "stat.ME",
            "stat.ML"
        ]
    },
    {
        "title": "A tutorial on conformal prediction",
        "authors": [
            "Glenn Shafer",
            "Vladimir Vovk"
        ],
        "summary": "Conformal prediction uses past experience to determine precise levels of confidence in new predictions. Given an error probability $\\epsilon$, together with a method that makes a prediction $\\hat{y}$ of a label $y$, it produces a set of labels, typically containing $\\hat{y}$, that also contains $y$ with probability $1-\\epsilon$. Conformal prediction can be applied to any method for producing $\\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge regression, etc.   Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right $1-\\epsilon$ of the time, even though they are based on an accumulating dataset rather than on independent datasets.   In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these.   This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in \"Algorithmic Learning in a Random World\", by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).",
        "published": "2007-06-21T16:40:06Z",
        "link": "http://arxiv.org/abs/0706.3188v1",
        "categories": [
            "cs.LG",
            "stat.ML"
        ]
    },
    {
        "title": "Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers   Taking Values in R^Q",
        "authors": [
            "Yann Guermeur"
        ],
        "summary": "Bounds on the risk play a crucial role in statistical learning theory. They usually involve as capacity measure of the model studied the VC dimension or one of its extensions. In classification, such \"VC dimensions\" exist for models taking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations appropriate for the missing case, the one of models with values in R^Q. This provides us with a new guaranteed risk for M-SVMs which appears superior to the existing one.",
        "published": "2007-06-25T17:28:57Z",
        "link": "http://arxiv.org/abs/0706.3679v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "The Role of Time in the Creation of Knowledge",
        "authors": [
            "Roy E. Murphy"
        ],
        "summary": "This paper I assume that in humans the creation of knowledge depends on a discrete time, or stage, sequential decision-making process subjected to a stochastic, information transmitting environment. For each time-stage, this environment randomly transmits Shannon type information-packets to the decision-maker, who examines each of them for relevancy and then determines his optimal choices. Using this set of relevant information-packets, the decision-maker adapts, over time, to the stochastic nature of his environment, and optimizes the subjective expected rate-of-growth of knowledge. The decision-maker's optimal actions, lead to a decision function that involves, over time, his view of the subjective entropy of the environmental process and other important parameters at each time-stage of the process. Using this model of human behavior, one could create psychometric experiments using computer simulation and real decision-makers, to play programmed games to measure the resulting human performance.",
        "published": "2007-07-03T20:43:43Z",
        "link": "http://arxiv.org/abs/0707.0498v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Clustering and Feature Selection using Sparse Principal Component   Analysis",
        "authors": [
            "Ronny Luss",
            "Alexandre d'Aspremont"
        ],
        "summary": "In this paper, we study the application of sparse principal component analysis (PCA) to clustering and feature selection problems. Sparse PCA seeks sparse factors, or linear combinations of the data variables, explaining a maximum amount of variance in the data while having only a limited number of nonzero coefficients. PCA is often used as a simple clustering technique and sparse factors allow us here to interpret the clusters in terms of a reduced set of variables. We begin with a brief introduction and motivation on sparse PCA and detail our implementation of the algorithm in d'Aspremont et al. (2005). We then apply these results to some classic clustering and feature selection problems arising in biology.",
        "published": "2007-07-04T21:53:11Z",
        "link": "http://arxiv.org/abs/0707.0701v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.MS"
        ]
    },
    {
        "title": "Model Selection Through Sparse Maximum Likelihood Estimation",
        "authors": [
            "Onureena Banerjee",
            "Laurent El Ghaoui",
            "Alexandre d'Aspremont"
        ],
        "summary": "We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added l_1-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our first algorithm uses block coordinate descent, and can be interpreted as recursive l_1-norm penalized regression. Our second algorithm, based on Nesterov's first order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright & Jordan (2006)), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data.",
        "published": "2007-07-04T22:13:42Z",
        "link": "http://arxiv.org/abs/0707.0704v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Optimal Solutions for Sparse Principal Component Analysis",
        "authors": [
            "Alexandre d'Aspremont",
            "Francis Bach",
            "Laurent El Ghaoui"
        ],
        "summary": "Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefficients, with total complexity O(n^3), where n is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in O(n^3) per pattern. We discuss applications in subset selection and sparse recovery and show on artificial examples and biological data that our algorithm does provide globally optimal solutions in many cases.",
        "published": "2007-07-04T22:28:28Z",
        "link": "http://arxiv.org/abs/0707.0705v4",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "A New Generalization of Chebyshev Inequality for Random Vectors",
        "authors": [
            "Xinjia Chen"
        ],
        "summary": "In this article, we derive a new generalization of Chebyshev inequality for random vectors. We demonstrate that the new generalization is much less conservative than the classical generalization.",
        "published": "2007-07-05T15:28:05Z",
        "link": "http://arxiv.org/abs/0707.0805v2",
        "categories": [
            "math.ST",
            "cs.LG",
            "math.PR",
            "stat.AP",
            "stat.TH"
        ]
    },
    {
        "title": "Clusters, Graphs, and Networks for Analysing Internet-Web-Supported   Communication within a Virtual Community",
        "authors": [
            "Xavier Polanco"
        ],
        "summary": "The proposal is to use clusters, graphs and networks as models in order to analyse the Web structure. Clusters, graphs and networks provide knowledge representation and organization. Clusters were generated by co-site analysis. The sample is a set of academic Web sites from the countries belonging to the European Union. These clusters are here revisited from the point of view of graph theory and social network analysis. This is a quantitative and structural analysis. In fact, the Internet is a computer network that connects people and organizations. Thus we may consider it to be a social network. The set of Web academic sites represents an empirical social network, and is viewed as a virtual community. The network structural properties are here analysed applying together cluster analysis, graph theory and social network analysis.",
        "published": "2007-07-10T13:47:32Z",
        "link": "http://arxiv.org/abs/0707.1452v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.5.3"
        ]
    },
    {
        "title": "Universal Reinforcement Learning",
        "authors": [
            "Vivek F. Farias",
            "Ciamac C. Moallemi",
            "Tsachy Weissman",
            "Benjamin Van Roy"
        ],
        "summary": "We consider an agent interacting with an unmodeled environment. At each time, the agent makes an observation, takes an action, and incurs a cost. Its actions can influence future observations and costs. The goal is to minimize the long-term average cost. We propose a novel algorithm, known as the active LZ algorithm, for optimal control based on ideas from the Lempel-Ziv scheme for universal data compression and prediction. We establish that, under the active LZ algorithm, if there exists an integer $K$ such that the future is conditionally independent of the past given a window of $K$ consecutive actions and observations, then the average cost converges to the optimum. Experimental results involving the game of Rock-Paper-Scissors illustrate merits of the algorithm.",
        "published": "2007-07-20T14:51:39Z",
        "link": "http://arxiv.org/abs/0707.3087v3",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Consistency of the group Lasso and multiple kernel learning",
        "authors": [
            "Francis Bach"
        ],
        "summary": "We consider the least-square regression problem with regularization by a block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1-norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic model consistency of the group Lasso. We derive necessary and sufficient conditions for the consistency of group Lasso under practical assumptions, such as model misspecification. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this infinite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisfied.",
        "published": "2007-07-23T14:35:20Z",
        "link": "http://arxiv.org/abs/0707.3390v2",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Quantum Algorithms for Learning and Testing Juntas",
        "authors": [
            "Alp Atici",
            "Rocco A. Servedio"
        ],
        "summary": "In this article we develop quantum algorithms for learning and testing juntas, i.e. Boolean functions which depend only on an unknown set of k out of n input variables. Our aim is to develop efficient algorithms:   - whose sample complexity has no dependence on n, the dimension of the domain the Boolean functions are defined over;   - with no access to any classical or quantum membership (\"black-box\") queries. Instead, our algorithms use only classical examples generated uniformly at random and fixed quantum superpositions of such classical examples;   - which require only a few quantum examples but possibly many classical random examples (which are considered quite \"cheap\" relative to quantum examples).   Our quantum algorithms are based on a subroutine FS which enables sampling according to the Fourier spectrum of f; the FS subroutine was used in earlier work of Bshouty and Jackson on quantum learning. Our results are as follows:   - We give an algorithm for testing k-juntas to accuracy $\\epsilon$ that uses $O(k/\\epsilon)$ quantum examples. This improves on the number of examples used by the best known classical algorithm.   - We establish the following lower bound: any FS-based k-junta testing algorithm requires $\\Omega(\\sqrt{k})$ queries.   - We give an algorithm for learning $k$-juntas to accuracy $\\epsilon$ that uses $O(\\epsilon^{-1} k\\log k)$ quantum examples and $O(2^k \\log(1/\\epsilon))$ random examples. We show that this learning algorithms is close to optimal by giving a related lower bound.",
        "published": "2007-07-24T13:17:55Z",
        "link": "http://arxiv.org/abs/0707.3479v1",
        "categories": [
            "quant-ph",
            "cs.LG"
        ]
    },
    {
        "title": "Virtual screening with support vector machines and structure kernels",
        "authors": [
            "Pierre Mahé",
            "Jean-Philippe Vert"
        ],
        "summary": "Support vector machines and kernel methods have recently gained considerable attention in chemoinformatics. They offer generally good performance for problems of supervised classification or regression, and provide a flexible and computationally efficient framework to include relevant information and prior knowledge about the data and problems to be handled. In particular, with kernel methods molecules do not need to be represented and stored explicitly as vectors or fingerprints, but only to be compared to each other through a comparison function technically called a kernel. While classical kernels can be used to compare vector or fingerprint representations of molecules, completely new kernels were developed in the recent years to directly compare the 2D or 3D structures of molecules, without the need for an explicit vectorization step through the extraction of molecular descriptors. While still in their infancy, these approaches have already demonstrated their relevance on several toxicity prediction and structure-activity relationship problems.",
        "published": "2007-08-01T19:13:52Z",
        "link": "http://arxiv.org/abs/0708.0171v1",
        "categories": [
            "q-bio.QM",
            "cs.LG"
        ]
    },
    {
        "title": "Structure or Noise?",
        "authors": [
            "Susanne Still",
            "James P. Crutchfield"
        ],
        "summary": "We show how rate-distortion theory provides a mechanism for automated theory building by naturally distinguishing between regularity and randomness. We start from the simple principle that model variables should, as much as possible, render the future and past conditionally independent. From this, we construct an objective function for model making whose extrema embody the trade-off between a model's structural complexity and its predictive power. The solutions correspond to a hierarchy of models that, at each level of complexity, achieve optimal predictive power at minimal cost. In the limit of maximal prediction the resulting optimal model identifies a process's intrinsic organization by extracting the underlying causal states. In this limit, the model's complexity is given by the statistical complexity, which is known to be minimal for achieving maximum prediction. Examples show how theory building can profit from analyzing a process's causal compressibility, which is reflected in the optimal models' rate-distortion curve--the process's characteristic for optimally balancing structure and noise at different levels of representation.",
        "published": "2007-08-05T01:37:53Z",
        "link": "http://arxiv.org/abs/0708.0654v2",
        "categories": [
            "physics.data-an",
            "cond-mat.stat-mech",
            "cs.IT",
            "cs.LG",
            "math-ph",
            "math.IT",
            "math.MP",
            "math.ST",
            "nlin.CD",
            "stat.TH"
        ]
    },
    {
        "title": "Cost-minimising strategies for data labelling : optimal stopping and   active learning",
        "authors": [
            "Christos Dimitrakakis",
            "Christian Savu-Krohn"
        ],
        "summary": "Supervised learning deals with the inference of a distribution over an output or label space $\\CY$ conditioned on points in an observation space $\\CX$, given a training dataset $D$ of pairs in $\\CX \\times \\CY$. However, in a lot of applications of interest, acquisition of large amounts of observations is easy, while the process of generating labels is time-consuming or costly. One way to deal with this problem is {\\em active} learning, where points to be labelled are selected with the aim of creating a model with better performance than that of an model trained on an equal number of randomly sampled points. In this paper, we instead propose to deal with the labelling cost directly: The learning goal is defined as the minimisation of a cost which is a function of the expected model performance and the total cost of the labels used. This allows the development of general strategies and specific algorithms for (a) optimal stopping, where the expected cost dictates whether label acquisition should continue (b) empirical evaluation, where the cost is used as a performance metric for a given combination of inference, stopping and sampling methods. Though the main focus of the paper is optimal stopping, we also aim to provide the background for further developments and discussion in the related field of active learning.",
        "published": "2007-08-09T10:21:34Z",
        "link": "http://arxiv.org/abs/0708.1242v3",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Defensive forecasting for optimal prediction with expert advice",
        "authors": [
            "Vladimir Vovk"
        ],
        "summary": "The method of defensive forecasting is applied to the problem of prediction with expert advice for binary outcomes. It turns out that defensive forecasting is not only competitive with the Aggregating Algorithm but also handles the case of \"second-guessing\" experts, whose advice depends on the learner's prediction; this paper assumes that the dependence on the learner's prediction is continuous.",
        "published": "2007-08-10T19:19:54Z",
        "link": "http://arxiv.org/abs/0708.1503v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Optimal Causal Inference: Estimating Stored Information and   Approximating Causal Architecture",
        "authors": [
            "Susanne Still",
            "James P. Crutchfield",
            "Christopher J. Ellison"
        ],
        "summary": "We introduce an approach to inferring the causal architecture of stochastic dynamical systems that extends rate distortion theory to use causal shielding---a natural principle of learning. We study two distinct cases of causal inference: optimal causal filtering and optimal causal estimation.   Filtering corresponds to the ideal case in which the probability distribution of measurement sequences is known, giving a principled method to approximate a system's causal structure at a desired level of representation. We show that, in the limit in which a model complexity constraint is relaxed, filtering finds the exact causal architecture of a stochastic dynamical system, known as the causal-state partition. From this, one can estimate the amount of historical information the process stores. More generally, causal filtering finds a graded model-complexity hierarchy of approximations to the causal architecture. Abrupt changes in the hierarchy, as a function of approximation, capture distinct scales of structural organization.   For nonideal cases with finite data, we show how the correct number of underlying causal states can be found by optimal causal estimation. A previously derived model complexity control term allows us to correct for the effect of statistical fluctuations in probability estimates and thereby avoid over-fitting.",
        "published": "2007-08-11T19:13:29Z",
        "link": "http://arxiv.org/abs/0708.1580v2",
        "categories": [
            "cs.IT",
            "cond-mat.stat-mech",
            "cs.LG",
            "math.IT",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "On Semimeasures Predicting Martin-Loef Random Sequences",
        "authors": [
            "Marcus Hutter",
            "Andrej Muchnik"
        ],
        "summary": "Solomonoff's central result on induction is that the posterior of a universal semimeasure M converges rapidly and with probability 1 to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal sequence predictor in case of unknown mu. Despite some nearby results and proofs in the literature, the stronger result of convergence for all (Martin-Loef) random sequences remained open. Such a convergence result would be particularly interesting and natural, since randomness can be defined in terms of M itself. We show that there are universal semimeasures M which do not converge for all random sequences, i.e. we give a partial negative answer to the open problem. We also provide a positive answer for some non-universal semimeasures. We define the incomputable measure D as a mixture over all computable measures and the enumerable semimeasure W as a mixture over all enumerable nearly-measures. We show that W converges to D and D to mu on all random sequences. The Hellinger distance measuring closeness of two distributions plays a central role.",
        "published": "2007-08-17T06:39:11Z",
        "link": "http://arxiv.org/abs/0708.2319v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.PR"
        ]
    },
    {
        "title": "Continuous and randomized defensive forecasting: unified view",
        "authors": [
            "Vladimir Vovk"
        ],
        "summary": "Defensive forecasting is a method of transforming laws of probability (stated in game-theoretic terms as strategies for Sceptic) into forecasting algorithms. There are two known varieties of defensive forecasting: \"continuous\", in which Sceptic's moves are assumed to depend on the forecasts in a (semi)continuous manner and which produces deterministic forecasts, and \"randomized\", in which the dependence of Sceptic's moves on the forecasts is arbitrary and Forecaster's moves are allowed to be randomized. This note shows that the randomized variety can be obtained from the continuous variety by smearing Sceptic's moves to make them continuous.",
        "published": "2007-08-17T12:18:24Z",
        "link": "http://arxiv.org/abs/0708.2353v2",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "A Dichotomy Theorem for General Minimum Cost Homomorphism Problem",
        "authors": [
            "Rustem Takhanov"
        ],
        "summary": "In the constraint satisfaction problem ($CSP$), the aim is to find an assignment of values to a set of variables subject to specified constraints. In the minimum cost homomorphism problem ($MinHom$), one is additionally given weights $c_{va}$ for every variable $v$ and value $a$, and the aim is to find an assignment $f$ to the variables that minimizes $\\sum_{v} c_{vf(v)}$. Let $MinHom(\\Gamma)$ denote the $MinHom$ problem parameterized by the set of predicates allowed for constraints. $MinHom(\\Gamma)$ is related to many well-studied combinatorial optimization problems, and concrete applications can be found in, for instance, defence logistics and machine learning. We show that $MinHom(\\Gamma)$ can be studied by using algebraic methods similar to those used for CSPs. With the aid of algebraic techniques, we classify the computational complexity of $MinHom(\\Gamma)$ for all choices of $\\Gamma$. Our result settles a general dichotomy conjecture previously resolved only for certain classes of directed graphs, [Gutin, Hell, Rafiey, Yeo, European J. of Combinatorics, 2008].",
        "published": "2007-08-23T18:26:21Z",
        "link": "http://arxiv.org/abs/0708.3226v7",
        "categories": [
            "cs.LG",
            "cs.CC",
            "F.4.1; G.2.2; I.2.6"
        ]
    },
    {
        "title": "Filtering Additive Measurement Noise with Maximum Entropy in the Mean",
        "authors": [
            "Henryk Gzyl",
            "Enrique ter Horst"
        ],
        "summary": "The purpose of this note is to show how the method of maximum entropy in the mean (MEM) may be used to improve parametric estimation when the measurements are corrupted by large level of noise. The method is developed in the context on a concrete example: that of estimation of the parameter in an exponential distribution. We compare the performance of our method with the bayesian and maximum likelihood approaches.",
        "published": "2007-09-04T19:36:22Z",
        "link": "http://arxiv.org/abs/0709.0509v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "On Universal Prediction and Bayesian Confirmation",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "The Bayesian framework is a well-studied and successful framework for inductive reasoning, which includes hypothesis testing and confirmation, parameter estimation, sequence prediction, classification, and regression. But standard statistical guidelines for choosing the model class and prior are not always available or fail, in particular in complex situations. Solomonoff completed the Bayesian framework by providing a rigorous, unique, formal, and universal choice for the model class and the prior. We discuss in breadth how and in which sense universal (non-i.i.d.) sequence prediction solves various (philosophical) problems of traditional Bayesian sequence prediction. We show that Solomonoff's model possesses many desirable properties: Strong total and weak instantaneous bounds, and in contrast to most classical continuous prior densities has no zero p(oste)rior problem, i.e. can confirm universal hypotheses, is reparametrization and regrouping invariant, and avoids the old-evidence and updating problem. It even performs well (actually better) in non-computable environments.",
        "published": "2007-09-11T01:39:20Z",
        "link": "http://arxiv.org/abs/0709.1516v1",
        "categories": [
            "math.ST",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "stat.ML",
            "stat.TH"
        ]
    },
    {
        "title": "Learning for Dynamic Bidding in Cognitive Radio Resources",
        "authors": [
            "Fangwen Fu",
            "Mihaela van der Schaar"
        ],
        "summary": "In this paper, we model the various wireless users in a cognitive radio network as a collection of selfish, autonomous agents that strategically interact in order to acquire the dynamically available spectrum opportunities. Our main focus is on developing solutions for wireless users to successfully compete with each other for the limited and time-varying spectrum opportunities, given the experienced dynamics in the wireless network. We categorize these dynamics into two types: one is the disturbance due to the environment (e.g. wireless channel conditions, source traffic characteristics, etc.) and the other is the impact caused by competing users. To analyze the interactions among users given the environment disturbance, we propose a general stochastic framework for modeling how the competition among users for spectrum opportunities evolves over time. At each stage of the dynamic resource allocation, a central spectrum moderator auctions the available resources and the users strategically bid for the required resources. The joint bid actions affect the resource allocation and hence, the rewards and future strategies of all users. Based on the observed resource allocation and corresponding rewards from previous allocations, we propose a best response learning algorithm that can be deployed by wireless users to improve their bidding policy at each stage. The simulation results show that by deploying the proposed best response learning algorithm, the wireless users can significantly improve their own performance in terms of both the packet loss rate and the incurred cost for the used resources.",
        "published": "2007-09-15T20:48:57Z",
        "link": "http://arxiv.org/abs/0709.2446v1",
        "categories": [
            "cs.LG",
            "cs.GT"
        ]
    },
    {
        "title": "Mutual information for the selection of relevant variables in   spectrometric nonlinear modelling",
        "authors": [
            "Fabrice Rossi",
            "Amaury Lendasse",
            "Damien François",
            "Vincent Wertz",
            "Michel Verleysen"
        ],
        "summary": "Data from spectrophotometers form vectors of a large number of exploitable variables. Building quantitative models using these variables most often requires using a smaller set of variables than the initial one. Indeed, a too large number of input variables to a model results in a too large number of parameters, leading to overfitting and poor generalization abilities. In this paper, we suggest the use of the mutual information measure to select variables from the initial set. The mutual information measures the information content in input variables with respect to the model output, without making any assumption on the model that will be used; it is thus suitable for nonlinear modelling. In addition, it leads to the selection of variables among the initial set, and not to linear or nonlinear combinations of them. Without decreasing the model performances compared to other variable projection methods, it allows therefore a greater interpretability of the results.",
        "published": "2007-09-21T12:49:47Z",
        "link": "http://arxiv.org/abs/0709.3427v1",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.AP"
        ]
    },
    {
        "title": "Fast Algorithm and Implementation of Dissimilarity Self-Organizing Maps",
        "authors": [
            "Brieuc Conan-Guez",
            "Fabrice Rossi",
            "Aïcha El Golli"
        ],
        "summary": "In many real world applications, data cannot be accurately represented by vectors. In those situations, one possible solution is to rely on dissimilarity measures that enable sensible comparison between observations. Kohonen's Self-Organizing Map (SOM) has been adapted to data described only through their dissimilarity matrix. This algorithm provides both non linear projection and clustering of non vector data. Unfortunately, the algorithm suffers from a high cost that makes it quite difficult to use with voluminous data sets. In this paper, we propose a new algorithm that provides an important reduction of the theoretical cost of the dissimilarity SOM without changing its outcome (the results are exactly the same as the ones obtained with the original algorithm). Moreover, we introduce implementation methods that result in very short running times. Improvements deduced from the theoretical cost model are validated on simulated and real world data (a word list clustering problem). We also demonstrate that the proposed implementation methods reduce by a factor up to 3 the running time of the fast algorithm over a standard implementation.",
        "published": "2007-09-21T15:20:07Z",
        "link": "http://arxiv.org/abs/0709.3461v1",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Une adaptation des cartes auto-organisatrices pour des données   décrites par un tableau de dissimilarités",
        "authors": [
            "Aïcha El Golli",
            "Fabrice Rossi",
            "Brieuc Conan-Guez",
            "Yves Lechevallier"
        ],
        "summary": "Many data analysis methods cannot be applied to data that are not represented by a fixed number of real values, whereas most of real world observations are not readily available in such a format. Vector based data analysis methods have therefore to be adapted in order to be used with non standard complex data. A flexible and general solution for this adaptation is to use a (dis)similarity measure. Indeed, thanks to expert knowledge on the studied data, it is generally possible to define a measure that can be used to make pairwise comparison between observations. General data analysis methods are then obtained by adapting existing methods to (dis)similarity matrices. In this article, we propose an adaptation of Kohonen's Self Organizing Map (SOM) to (dis)similarity data. The proposed algorithm is an adapted version of the vector based batch SOM. The method is validated on real world data: we provide an analysis of the usage patterns of the web site of the Institut National de Recherche en Informatique et Automatique, constructed thanks to web log mining method.",
        "published": "2007-09-22T15:53:54Z",
        "link": "http://arxiv.org/abs/0709.3586v1",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Self-organizing maps and symbolic data",
        "authors": [
            "Aïcha El Golli",
            "Brieuc Conan-Guez",
            "Fabrice Rossi"
        ],
        "summary": "In data analysis new forms of complex data have to be considered like for example (symbolic data, functional data, web data, trees, SQL query and multimedia data, ...). In this context classical data analysis for knowledge discovery based on calculating the center of gravity can not be used because input are not $\\mathbb{R}^p$ vectors. In this paper, we present an application on real world symbolic data using the self-organizing map. To this end, we propose an extension of the self-organizing map that can handle symbolic data.",
        "published": "2007-09-22T15:54:37Z",
        "link": "http://arxiv.org/abs/0709.3587v1",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Fast Selection of Spectral Variables with B-Spline Compression",
        "authors": [
            "Fabrice Rossi",
            "Damien François",
            "Vincent Wertz",
            "Marc Meurens",
            "Michel Verleysen"
        ],
        "summary": "The large number of spectral variables in most data sets encountered in spectral chemometrics often renders the prediction of a dependent variable uneasy. The number of variables hopefully can be reduced, by using either projection techniques or selection methods; the latter allow for the interpretation of the selected variables. Since the optimal approach of testing all possible subsets of variables with the prediction model is intractable, an incremental selection approach using a nonparametric statistics is a good option, as it avoids the computationally intensive use of the model itself. It has two drawbacks however: the number of groups of variables to test is still huge, and colinearities can make the results unstable. To overcome these limitations, this paper presents a method to select groups of spectral variables. It consists in a forward-backward procedure applied to the coefficients of a B-Spline representation of the spectra. The criterion used in the forward-backward procedure is the mutual information, allowing to find nonlinear dependencies between variables, on the contrary of the generally used correlation. The spline representation is used to get interpretability of the results, as groups of consecutive spectral variables will be selected. The experiments conducted on NIR spectra from fescue grass and diesel fuels show that the method provides clearly identified groups of selected variables, making interpretation easy, while keeping a low computational load. The prediction performances obtained using the selected coefficients are higher than those obtained by the same method applied directly to the original variables and similar to those obtained using traditional models, although using significantly less spectral variables.",
        "published": "2007-09-23T14:08:51Z",
        "link": "http://arxiv.org/abs/0709.3639v1",
        "categories": [
            "cs.LG",
            "stat.AP"
        ]
    },
    {
        "title": "Resampling methods for parameter-free and robust feature selection with   mutual information",
        "authors": [
            "Damien François",
            "Fabrice Rossi",
            "Vincent Wertz",
            "Michel Verleysen"
        ],
        "summary": "Combining the mutual information criterion with a forward feature selection strategy offers a good trade-off between optimality of the selected feature subset and computation time. However, it requires to set the parameter(s) of the mutual information estimator and to determine when to halt the forward procedure. These two choices are difficult to make because, as the dimensionality of the subset increases, the estimation of the mutual information becomes less and less reliable. This paper proposes to use resampling methods, a K-fold cross-validation and the permutation test, to address both issues. The resampling methods bring information about the variance of the estimator, information which can then be used to automatically set the parameter and to calculate a threshold to stop the forward procedure. The procedure is illustrated on a synthetic dataset as well as on real-world examples.",
        "published": "2007-09-23T14:09:28Z",
        "link": "http://arxiv.org/abs/0709.3640v1",
        "categories": [
            "cs.LG",
            "stat.AP"
        ]
    },
    {
        "title": "Evolving Classifiers: Methods for Incremental Learning",
        "authors": [
            "Greg Hulley",
            "Tshilidzi Marwala"
        ],
        "summary": "The ability of a classifier to take on new information and classes by evolving the classifier without it having to be fully retrained is known as incremental learning. Incremental learning has been successfully applied to many classification problems, where the data is changing and is not all available at once. In this paper there is a comparison between Learn++, which is one of the most recent incremental learning algorithms, and the new proposed method of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has shown good incremental learning capabilities on benchmark datasets on which the new ILUGA method has been tested. ILUGA has also shown good incremental learning ability using only a few classifiers and does not suffer from catastrophic forgetting. The results obtained for ILUGA on the Optical Character Recognition (OCR) and Wine datasets are good, with an overall accuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT for the difficult multi-class OCR dataset.",
        "published": "2007-09-25T14:28:32Z",
        "link": "http://arxiv.org/abs/0709.3965v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Classification of Images Using Support Vector Machines",
        "authors": [
            "Gidudu Anthony",
            "Hulley Greg",
            "Marwala Tshilidzi"
        ],
        "summary": "Support Vector Machines (SVMs) are a relatively new supervised classification technique to the land cover mapping community. They have their roots in Statistical Learning Theory and have gained prominence because they are robust, accurate and are effective even when using a small training sample. By their nature SVMs are essentially binary classifiers, however, they can be adopted to handle the multiple classification tasks common in remote sensing studies. The two approaches commonly used are the One-Against-One (1A1) and One-Against-All (1AA) techniques. In this paper, these approaches are evaluated in as far as their impact and implication for land cover mapping. The main finding from this research is that whereas the 1AA technique is more predisposed to yielding unclassified and mixed pixels, the resulting classification accuracy is not significantly different from 1A1 approach. It is the authors conclusions that ultimately the choice of technique adopted boils down to personal preference and the uniqueness of the dataset at hand.",
        "published": "2007-09-25T14:37:40Z",
        "link": "http://arxiv.org/abs/0709.3967v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Prediction with expert advice for the Brier game",
        "authors": [
            "Vladimir Vovk",
            "Fedor Zhdanov"
        ],
        "summary": "We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches. The theoretical performance guarantee turns out to be rather tight on these data sets, especially in the case of the more extensive tennis data.",
        "published": "2007-10-02T10:08:41Z",
        "link": "http://arxiv.org/abs/0710.0485v2",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Association Rules in the Relational Calculus",
        "authors": [
            "Oliver Schulte",
            "Flavia Moser",
            "Martin Ester",
            "Zhiyong Lu"
        ],
        "summary": "One of the most utilized data mining tasks is the search for association rules. Association rules represent significant relationships between items in transactions. We extend the concept of association rule to represent a much broader class of associations, which we refer to as \\emph{entity-relationship rules.} Semantically, entity-relationship rules express associations between properties of related objects. Syntactically, these rules are based on a broad subclass of safe domain relational calculus queries. We propose a new definition of support and confidence for entity-relationship rules and for the frequency of entity-relationship queries. We prove that the definition of frequency satisfies standard probability axioms and the Apriori property.",
        "published": "2007-10-10T18:00:44Z",
        "link": "http://arxiv.org/abs/0710.2083v1",
        "categories": [
            "cs.DB",
            "cs.LG",
            "cs.LO"
        ]
    },
    {
        "title": "The structure of verbal sequences analyzed with unsupervised learning   techniques",
        "authors": [
            "Catherine Recanati",
            "Nicoleta Rogovschi",
            "Younès Bennani"
        ],
        "summary": "Data mining allows the exploration of sequences of phenomena, whereas one usually tends to focus on isolated phenomena or on the relation between two phenomena. It offers invaluable tools for theoretical analyses and exploration of the structure of sentences, texts, dialogues, and speech. We report here the results of an attempt at using it for inspecting sequences of verbs from French accounts of road accidents. This analysis comes from an original approach of unsupervised training allowing the discovery of the structure of sequential data. The entries of the analyzer were only made of the verbs appearing in the sentences. It provided a classification of the links between two successive verbs into four distinct clusters, allowing thus text segmentation. We give here an interpretation of these clusters by applying a statistical analysis to independent semantic annotations.",
        "published": "2007-10-12T12:44:11Z",
        "link": "http://arxiv.org/abs/0710.2446v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Consistency of trace norm minimization",
        "authors": [
            "Francis Bach"
        ],
        "summary": "Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufficient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulfilled.",
        "published": "2007-10-15T15:38:33Z",
        "link": "http://arxiv.org/abs/0710.2848v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "An efficient reduction of ranking to classification",
        "authors": [
            "Nir Ailon",
            "Mehryar Mohri"
        ],
        "summary": "This paper describes an efficient reduction of the learning problem of ranking to binary classification. The reduction guarantees an average pairwise misranking regret of at most that of the binary classifier regret, improving a recent result of Balcan et al which only guarantees a factor of 2. Moreover, our reduction applies to a broader class of ranking loss functions, admits a simpler proof, and the expected running time complexity of our algorithm in terms of number of calls to a classifier or preference function is improved from $\\Omega(n^2)$ to $O(n \\log n)$. In addition, when the top $k$ ranked elements only are required ($k \\ll n$), as in many applications in information extraction or search engines, the time complexity of our algorithm can be further reduced to $O(k \\log k + n)$. Our reduction and algorithm are thus practical for realistic applications where the number of points to rank exceeds several thousands. Much of our results also extend beyond the bipartite case previously studied.   Our rediction is a randomized one. To complement our result, we also derive lower bounds on any deterministic reduction from binary (preference) classification to ranking, implying that our use of a randomized reduction is essentially necessary for the guarantees we provide.",
        "published": "2007-10-15T18:25:15Z",
        "link": "http://arxiv.org/abs/0710.2889v2",
        "categories": [
            "cs.LG",
            "cs.IR",
            "K.3.2"
        ]
    },
    {
        "title": "Combining haplotypers",
        "authors": [
            "Matti Kääriäinen",
            "Niels Landwehr",
            "Sampsa Lappalainen",
            "Taneli Mielikäinen"
        ],
        "summary": "Statistically resolving the underlying haplotype pair for a genotype measurement is an important intermediate step in gene mapping studies, and has received much attention recently. Consequently, a variety of methods for this problem have been developed. Different methods employ different statistical models, and thus implicitly encode different assumptions about the nature of the underlying haplotype structure. Depending on the population sample in question, their relative performance can vary greatly, and it is unclear which method to choose for a particular sample. Instead of choosing a single method, we explore combining predictions returned by different methods in a principled way, and thereby circumvent the problem of method selection.   We propose several techniques for combining haplotype reconstructions and analyze their computational properties. In an experimental study on real-world haplotype data we show that such techniques can provide more accurate and robust reconstructions, and are useful for outlier detection. Typically, the combined prediction is at least as accurate as or even more accurate than the best individual method, effectively circumventing the method selection problem.",
        "published": "2007-10-26T15:13:21Z",
        "link": "http://arxiv.org/abs/0710.5116v1",
        "categories": [
            "cs.LG",
            "cs.CE",
            "q-bio.QM",
            "F.2.2; I.2.6; J.3"
        ]
    },
    {
        "title": "A Tutorial on Spectral Clustering",
        "authors": [
            "Ulrike von Luxburg"
        ],
        "summary": "In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.",
        "published": "2007-11-01T19:04:43Z",
        "link": "http://arxiv.org/abs/0711.0189v1",
        "categories": [
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Building Rules on Top of Ontologies for the Semantic Web with Inductive   Logic Programming",
        "authors": [
            "Francesca A. Lisi"
        ],
        "summary": "Building rules on top of ontologies is the ultimate goal of the logical layer of the Semantic Web. To this aim an ad-hoc mark-up language for this layer is currently under discussion. It is intended to follow the tradition of hybrid knowledge representation and reasoning systems such as $\\mathcal{AL}$-log that integrates the description logic $\\mathcal{ALC}$ and the function-free Horn clausal language \\textsc{Datalog}. In this paper we consider the problem of automating the acquisition of these rules for the Semantic Web. We propose a general framework for rule induction that adopts the methodological apparatus of Inductive Logic Programming and relies on the expressive and deductive power of $\\mathcal{AL}$-log. The framework is valid whatever the scope of induction (description vs. prediction) is. Yet, for illustrative purposes, we also discuss an instantiation of the framework which aims at description and turns out to be useful in Ontology Refinement.   Keywords: Inductive Logic Programming, Hybrid Knowledge Representation and Reasoning Systems, Ontologies, Semantic Web.   Note: To appear in Theory and Practice of Logic Programming (TPLP)",
        "published": "2007-11-12T17:15:34Z",
        "link": "http://arxiv.org/abs/0711.1814v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Empirical Evaluation of Four Tensor Decomposition Algorithms",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Higher-order tensor decompositions are analogous to the familiar Singular Value Decomposition (SVD), but they transcend the limitations of matrices (second-order tensors). SVD is a powerful tool that has achieved impressive results in information retrieval, collaborative filtering, computational linguistics, computational vision, and other fields. However, SVD is limited to two-dimensional arrays of data (two modes), and many potential applications have three or more modes, which require higher-order tensor decompositions. This paper evaluates four algorithms for higher-order tensor decomposition: Higher-Order Singular Value Decomposition (HO-SVD), Higher-Order Orthogonal Iteration (HOOI), Slice Projection (SP), and Multislice Projection (MP). We measure the time (elapsed run time), space (RAM and disk space requirements), and fit (tensor reconstruction accuracy) of the four algorithms, under a variety of conditions. We find that standard implementations of HO-SVD and HOOI do not scale up to larger tensors, due to increasing RAM requirements. We recommend HOOI for tensors that are small enough for the available RAM and MP for larger tensors.",
        "published": "2007-11-13T16:28:47Z",
        "link": "http://arxiv.org/abs/0711.2023v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; I.2.6; I.2.7; E.1; G.1.3"
        ]
    },
    {
        "title": "Inverse Sampling for Nonasymptotic Sequential Estimation of Bounded   Variable Means",
        "authors": [
            "Xinjia Chen"
        ],
        "summary": "In this paper, we consider the nonasymptotic sequential estimation of means of random variables bounded in between zero and one. We have rigorously demonstrated that, in order to guarantee prescribed relative precision and confidence level, it suffices to continue sampling until the sample sum is no less than a certain bound and then take the average of samples as an estimate for the mean of the bounded random variable. We have developed an explicit formula and a bisection search method for the determination of such bound of sample sum, without any knowledge of the bounded variable. Moreover, we have derived bounds for the distribution of sample size. In the special case of Bernoulli random variables, we have established analytical and numerical methods to further reduce the bound of sample sum and thus improve the efficiency of sampling. Furthermore, the fallacy of existing results are detected and analyzed.",
        "published": "2007-11-18T17:28:23Z",
        "link": "http://arxiv.org/abs/0711.2801v2",
        "categories": [
            "math.ST",
            "cs.LG",
            "math.PR",
            "stat.TH",
            "62L12; 62D05; 65C05"
        ]
    },
    {
        "title": "Image Classification Using SVMs: One-against-One Vs One-against-All",
        "authors": [
            "Gidudu Anthony",
            "Hulley Gregg",
            "Marwala Tshilidzi"
        ],
        "summary": "Support Vector Machines (SVMs) are a relatively new supervised classification technique to the land cover mapping community. They have their roots in Statistical Learning Theory and have gained prominence because they are robust, accurate and are effective even when using a small training sample. By their nature SVMs are essentially binary classifiers, however, they can be adopted to handle the multiple classification tasks common in remote sensing studies. The two approaches commonly used are the One-Against-One (1A1) and One-Against-All (1AA) techniques. In this paper, these approaches are evaluated in as far as their impact and implication for land cover mapping. The main finding from this research is that whereas the 1AA technique is more predisposed to yielding unclassified and mixed pixels, the resulting classification accuracy is not significantly different from 1A1 approach. It is the authors conclusion therefore that ultimately the choice of technique adopted boils down to personal preference and the uniqueness of the dataset at hand.",
        "published": "2007-11-19T12:25:00Z",
        "link": "http://arxiv.org/abs/0711.2914v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ]
    },
    {
        "title": "Clustering with Transitive Distance and K-Means Duality",
        "authors": [
            "Chunjing Xu",
            "Jianzhuang Liu",
            "Xiaoou Tang"
        ],
        "summary": "Recent spectral clustering methods are a propular and powerful technique for data clustering. These methods need to solve the eigenproblem whose computational complexity is $O(n^3)$, where $n$ is the number of data samples. In this paper, a non-eigenproblem based clustering method is proposed to deal with the clustering problem. Its performance is comparable to the spectral clustering algorithms but it is more efficient with computational complexity $O(n^2)$. We show that with a transitive distance and an observed property, called K-means duality, our algorithm can be used to handle data sets with complex cluster shapes, multi-scale clusters, and noise. Moreover, no parameters except the number of clusters need to be set in our algorithm.",
        "published": "2007-11-22T15:05:35Z",
        "link": "http://arxiv.org/abs/0711.3594v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Derivations of Normalized Mutual Information in Binary Classifications",
        "authors": [
            "Yong Wang",
            "Bao-Gang Hu"
        ],
        "summary": "This correspondence studies the basic problem of classifications - how to evaluate different classifiers. Although the conventional performance indexes, such as accuracy, are commonly used in classifier selection or evaluation, information-based criteria, such as mutual information, are becoming popular in feature/model selections. In this work, we propose to assess classifiers in terms of normalized mutual information (NI), which is novel and well defined in a compact range for classifier evaluation. We derive close-form relations of normalized mutual information with respect to accuracy, precision, and recall in binary classifications. By exploring the relations among them, we reveal that NI is actually a set of nonlinear functions, with a concordant power-exponent form, to each performance index. The relations can also be expressed with respect to precision and recall, or to false alarm and hitting rate (recall).",
        "published": "2007-11-23T07:45:52Z",
        "link": "http://arxiv.org/abs/0711.3675v1",
        "categories": [
            "cs.LG",
            "cs.IT",
            "math.IT",
            "I.5.1"
        ]
    },
    {
        "title": "Covariance and PCA for Categorical Variables",
        "authors": [
            "Hirotaka Niitsuma",
            "Takashi Okada"
        ],
        "summary": "Covariances from categorical variables are defined using a regular simplex expression for categories. The method follows the variance definition by Gini, and it gives the covariance as a solution of simultaneous equations. The calculated results give reasonable values for test data. A method of principal component analysis (RS-PCA) is also proposed using regular simplex expressions, which allows easy interpretation of the principal components. The proposed methods apply to variable selection problem of categorical data USCensus1990 data. The proposed methods give appropriate criterion for the variable selection problem of categorical",
        "published": "2007-11-28T12:05:47Z",
        "link": "http://arxiv.org/abs/0711.4452v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "On the Relationship between the Posterior and Optimal Similarity",
        "authors": [
            "Thomas M. Breuel"
        ],
        "summary": "For a classification problem described by the joint density $P(\\omega,x)$, models of $P(\\omega\\eq\\omega'|x,x')$ (the ``Bayesian similarity measure'') have been shown to be an optimal similarity measure for nearest neighbor classification. This paper analyzes demonstrates several additional properties of that conditional distribution. The paper first shows that we can reconstruct, up to class labels, the class posterior distribution $P(\\omega|x)$ given $P(\\omega\\eq\\omega'|x,x')$, gives a procedure for recovering the class labels, and gives an asymptotically Bayes-optimal classification procedure. It also shows, given such an optimal similarity measure, how to construct a classifier that outperforms the nearest neighbor classifier and achieves Bayes-optimal classification rates. The paper then analyzes Bayesian similarity in a framework where a classifier faces a number of related classification tasks (multitask learning) and illustrates that reconstruction of the class posterior distribution is not possible in general. Finally, the paper identifies a distinct class of classification problems using $P(\\omega\\eq\\omega'|x,x')$ and shows that using $P(\\omega\\eq\\omega'|x,x')$ to solve those problems is the Bayes optimal solution.",
        "published": "2007-12-02T09:38:26Z",
        "link": "http://arxiv.org/abs/0712.0130v1",
        "categories": [
            "cs.LG",
            "I.5.3; I.5.2"
        ]
    },
    {
        "title": "A Reactive Tabu Search Algorithm for Stimuli Generation in   Psycholinguistics",
        "authors": [
            "Alejandro Chinea Manrique De Lara"
        ],
        "summary": "The generation of meaningless \"words\" matching certain statistical and/or linguistic criteria is frequently needed for experimental purposes in Psycholinguistics. Such stimuli receive the name of pseudowords or nonwords in the Cognitive Neuroscience literatue. The process for building nonwords sometimes has to be based on linguistic units such as syllables or morphemes, resulting in a numerical explosion of combinations when the size of the nonwords is increased. In this paper, a reactive tabu search scheme is proposed to generate nonwords of variables size. The approach builds pseudowords by using a modified Metaheuristic algorithm based on a local search procedure enhanced by a feedback-based scheme. Experimental results show that the new algorithm is a practical and effective tool for nonword generation.",
        "published": "2007-12-04T08:52:46Z",
        "link": "http://arxiv.org/abs/0712.0451v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DM",
            "cs.LG"
        ]
    },
    {
        "title": "Equations of States in Singular Statistical Estimation",
        "authors": [
            "Sumio Watanabe"
        ],
        "summary": "Learning machines which have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither the Bayes a posteriori distribution converges to the normal distribution nor the maximum likelihood estimator satisfies asymptotic normality. This is the main reason why it has been difficult to predict their generalization performances from trained states. In this paper, we study four errors, (1) Bayes generalization error, (2) Bayes training error, (3) Gibbs generalization error, and (4) Gibbs training error, and prove that there are mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that Bayes and Gibbs generalization errors are estimated by Bayes and Gibbs training errors, and propose widely applicable information criteria which can be applied to both regular and singular statistical models.",
        "published": "2007-12-05T05:39:07Z",
        "link": "http://arxiv.org/abs/0712.0653v2",
        "categories": [
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "A Universal Kernel for Learning Regular Languages",
        "authors": [
            "Leonid",
            "Kontorovich"
        ],
        "summary": "We give a universal kernel that renders all the regular languages linearly separable. We are not able to compute this kernel efficiently and conjecture that it is intractable, but we do have an efficient $\\eps$-approximation.",
        "published": "2007-12-05T22:25:03Z",
        "link": "http://arxiv.org/abs/0712.0840v1",
        "categories": [
            "cs.LG",
            "cs.DM",
            "F.1.1; D.3.1; F.4.3"
        ]
    },
    {
        "title": "Automatic Pattern Classification by Unsupervised Learning Using   Dimensionality Reduction of Data with Mirroring Neural Networks",
        "authors": [
            "Dasika Ratna Deepthi",
            "G. R. Aditya Krishna",
            "K. Eswaran"
        ],
        "summary": "This paper proposes an unsupervised learning technique by using Multi-layer Mirroring Neural Network and Forgy's clustering algorithm. Multi-layer Mirroring Neural Network is a neural network that can be trained with generalized data inputs (different categories of image patterns) to perform non-linear dimensionality reduction and the resultant low-dimensional code is used for unsupervised pattern classification using Forgy's algorithm. By adapting the non-linear activation function (modified sigmoidal function) and initializing the weights and bias terms to small random values, mirroring of the input pattern is initiated. In training, the weights and bias terms are changed in such a way that the input presented is reproduced at the output by back propagating the error. The mirroring neural network is capable of reducing the input vector to a great degree (approximately 1/30th the original size) and also able to reconstruct the input pattern at the output layer from this reduced code units. The feature set (output of central hidden layer) extracted from this network is fed to Forgy's algorithm, which classify input data patterns into distinguishable classes. In the implementation of Forgy's algorithm, initial seed points are selected in such a way that they are distant enough to be perfectly grouped into different categories. Thus a new method of unsupervised learning is formulated and demonstrated in this paper. This method gave impressive results when applied to classification of different image patterns.",
        "published": "2007-12-06T13:52:04Z",
        "link": "http://arxiv.org/abs/0712.0938v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Reconstruction of Markov Random Fields from Samples: Some Easy   Observations and Algorithms",
        "authors": [
            "Guy Bresler",
            "Elchanan Mossel",
            "Allan Sly"
        ],
        "summary": "Markov random fields are used to model high dimensional distributions in a number of applied areas. Much recent interest has been devoted to the reconstruction of the dependency structure from independent samples from the Markov random fields. We analyze a simple algorithm for reconstructing the underlying graph defining a Markov random field on $n$ nodes and maximum degree $d$ given observations. We show that under mild non-degeneracy conditions it reconstructs the generating graph with high probability using $\\Theta(d \\epsilon^{-2}\\delta^{-4} \\log n)$ samples where $\\epsilon,\\delta$ depend on the local interactions. For most local interaction $\\eps,\\delta$ are of order $\\exp(-O(d))$.   Our results are optimal as a function of $n$ up to a multiplicative constant depending on $d$ and the strength of the local interactions. Our results seem to be the first results for general models that guarantee that {\\em the} generating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2} \\epsilon^{-2}\\delta^{-4} \\log n)$ running time bound. In cases where the measure on the graph has correlation decay, the running time is $O(n^2 \\log n)$ for all fixed $d$. We also discuss the effect of observing noisy samples and show that as long as the noise level is low, our algorithm is effective. On the other hand, we construct an example where large noise implies non-identifiability even for generic noise and interactions. Finally, we briefly show that in some simple cases, models with hidden nodes can also be recovered.",
        "published": "2007-12-10T06:50:36Z",
        "link": "http://arxiv.org/abs/0712.1402v2",
        "categories": [
            "cs.CC",
            "cs.LG"
        ]
    },
    {
        "title": "A New Theoretic Foundation for Cross-Layer Optimization",
        "authors": [
            "Fangwen Fu",
            "Mihaela van der Schaar"
        ],
        "summary": "Cross-layer optimization solutions have been proposed in recent years to improve the performance of network users operating in a time-varying, error-prone wireless environment. However, these solutions often rely on ad-hoc optimization approaches, which ignore the different environmental dynamics experienced at various layers by a user and violate the layered network architecture of the protocol stack by requiring layers to provide access to their internal protocol parameters to other layers. This paper presents a new theoretic foundation for cross-layer optimization, which allows each layer to make autonomous decisions individually, while maximizing the utility of the wireless user by optimally determining what information needs to be exchanged among layers. Hence, this cross-layer framework does not change the current layered architecture. Specifically, because the wireless user interacts with the environment at various layers of the protocol stack, the cross-layer optimization problem is formulated as a layered Markov decision process (MDP) in which each layer adapts its own protocol parameters and exchanges information (messages) with other layers in order to cooperatively maximize the performance of the wireless user. The message exchange mechanism for determining the optimal cross-layer transmission strategies has been designed for both off-line optimization and on-line dynamic adaptation. We also show that many existing cross-layer optimization algorithms can be formulated as simplified, sub-optimal, versions of our layered MDP framework.",
        "published": "2007-12-15T06:50:43Z",
        "link": "http://arxiv.org/abs/0712.2497v1",
        "categories": [
            "cs.NI",
            "cs.LG"
        ]
    },
    {
        "title": "Density estimation in linear time",
        "authors": [
            "Satyaki Mahalanabis",
            "Daniel Stefankovic"
        ],
        "summary": "We consider the problem of choosing a density estimate from a set of distributions F, minimizing the L1-distance to an unknown distribution (Devroye, Lugosi 2001). Devroye and Lugosi analyze two algorithms for the problem: Scheffe tournament winner and minimum distance estimate. The Scheffe tournament estimate requires fewer computations than the minimum distance estimate, but has strictly weaker guarantees than the latter.   We focus on the computational aspect of density estimation. We present two algorithms, both with the same guarantee as the minimum distance estimate. The first one, a modification of the minimum distance estimate, uses the same number (quadratic in |F|) of computations as the Scheffe tournament. The second one, called ``efficient minimum loss-weight estimate,'' uses only a linear number of computations, assuming that F is preprocessed.   We also give examples showing that the guarantees of the algorithms cannot be improved and explore randomized algorithms for density estimation.",
        "published": "2007-12-18T03:30:05Z",
        "link": "http://arxiv.org/abs/0712.2869v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Graph kernels between point clouds",
        "authors": [
            "Francis Bach"
        ],
        "summary": "Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow to use kernel methods for such ob jects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.",
        "published": "2007-12-20T13:06:50Z",
        "link": "http://arxiv.org/abs/0712.3402v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Improving the Performance of PieceWise Linear Separation Incremental   Algorithms for Practical Hardware Implementations",
        "authors": [
            "Alejandro Chinea Manrique De Lara",
            "Juan Manuel Moreno",
            "Arostegui Jordi Madrenas",
            "Joan Cabestany"
        ],
        "summary": "In this paper we shall review the common problems associated with Piecewise Linear Separation incremental algorithms. This kind of neural models yield poor performances when dealing with some classification problems, due to the evolving schemes used to construct the resulting networks. So as to avoid this undesirable behavior we shall propose a modification criterion. It is based upon the definition of a function which will provide information about the quality of the network growth process during the learning phase. This function is evaluated periodically as the network structure evolves, and will permit, as we shall show through exhaustive benchmarks, to considerably improve the performance(measured in terms of network complexity and generalization capabilities) offered by the networks generated by these incremental models.",
        "published": "2007-12-21T10:05:52Z",
        "link": "http://arxiv.org/abs/0712.3654v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Improved Collaborative Filtering Algorithm via Information   Transformation",
        "authors": [
            "Jian-Guo Liu",
            "Bing-Hong Wang",
            "Qiang Guo"
        ],
        "summary": "In this paper, we propose a spreading activation approach for collaborative filtering (SA-CF). By using the opinion spreading process, the similarity between any users can be obtained. The algorithm has remarkably higher accuracy than the standard collaborative filtering (CF) using Pearson correlation. Furthermore, we introduce a free parameter $\\beta$ to regulate the contributions of objects to user-user correlations. The numerical results indicate that decreasing the influence of popular objects can further improve the algorithmic accuracy and personality. We argue that a better algorithm should simultaneously require less computation and generate higher accuracy. Accordingly, we further propose an algorithm involving only the top-$N$ similar neighbors for each target user, which has both less computational complexity and higher algorithmic accuracy.",
        "published": "2007-12-26T14:25:18Z",
        "link": "http://arxiv.org/abs/0712.3807v3",
        "categories": [
            "cs.LG",
            "cs.CY"
        ]
    },
    {
        "title": "Online EM Algorithm for Latent Data Models",
        "authors": [
            "Olivier Cappé",
            "Eric Moulines"
        ],
        "summary": "In this contribution, we propose a generic online (also sometimes called adaptive or recursive) version of the Expectation-Maximisation (EM) algorithm applicable to latent variable models of independent observations. Compared to the algorithm of Titterington (1984), this approach is more directly connected to the usual EM algorithm and does not rely on integration with respect to the complete data distribution. The resulting algorithm is usually simpler and is shown to achieve convergence to the stationary points of the Kullback-Leibler divergence between the marginal distribution of the observation and the model distribution at the optimal rate, i.e., that of the maximum likelihood estimator. In addition, the proposed approach is also suitable for conditional (or regression) models, as illustrated in the case of the mixture of linear regressions model.",
        "published": "2007-12-27T19:44:34Z",
        "link": "http://arxiv.org/abs/0712.4273v4",
        "categories": [
            "stat.CO",
            "cs.LG"
        ]
    },
    {
        "title": "An algebraic approach to complexity of data stream computations",
        "authors": [
            "Sumit Ganguly"
        ],
        "summary": "We consider a basic problem in the general data streaming model, namely, to estimate a vector $f \\in \\Z^n$ that is arbitrarily updated (i.e., incremented or decremented) coordinate-wise. The estimate $\\hat{f} \\in \\Z^n$ must satisfy $\\norm{\\hat{f}-f}_{\\infty}\\le \\epsilon\\norm{f}_1 $, that is, $\\forall i ~(\\abs{\\hat{f}_i - f_i} \\le \\epsilon \\norm{f}_1)$. It is known to have $\\tilde{O}(\\epsilon^{-1})$ randomized space upper bound \\cite{cm:jalgo}, $\\Omega(\\epsilon^{-1} \\log (\\epsilon n))$ space lower bound \\cite{bkmt:sirocco03} and deterministic space upper bound of $\\tilde{\\Omega}(\\epsilon^{-2})$ bits.\\footnote{The $\\tilde{O}$ and $\\tilde{\\Omega}$ notations suppress poly-logarithmic factors in $n, \\log \\epsilon^{-1}, \\norm{f}_{\\infty}$ and $\\log \\delta^{-1}$, where, $\\delta$ is the error probability (for randomized algorithm).} We show that any deterministic algorithm for this problem requires space $\\Omega(\\epsilon^{-2} (\\log \\norm{f}_1))$ bits.",
        "published": "2007-01-02T18:12:52Z",
        "link": "http://arxiv.org/abs/cs/0701004v4",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A Reply to Hofman On: \"Why LP cannot solve large instances of   NP-complete problems in polynomial time\"",
        "authors": [
            "Moustapha Diaby"
        ],
        "summary": "Using an approach that seems to be patterned after that of Yannakakis, Hofman argues that an NP-complete problem cannot be formulated as a polynomial bounded-sized linear programming problem. He then goes on to propose a \"construct\" that he claims to be a counter-example to recently published linear programming formulations of the Traveling Salesman Problem (TSP) and the Quadratic Assignment Problems (QAP), respectively. In this paper, we show that Hofman's construct is flawed, and provide further proof that his \"counter-example\" is invalid.",
        "published": "2007-01-03T11:26:26Z",
        "link": "http://arxiv.org/abs/cs/0701014v2",
        "categories": [
            "cs.CC",
            "F.2; F.2.2"
        ]
    },
    {
        "title": "Almost Euclidean sections of the N-dimensional cross-polytope using O(N)   random bits",
        "authors": [
            "Shachar Lovett",
            "Sasha Sodin"
        ],
        "summary": "It is well known that R^N has subspaces of dimension proportional to N on which the \\ell_1 norm is equivalent to the \\ell_2 norm; however, no explicit constructions are known. Extending earlier work by Artstein--Avidan and Milman, we prove that such a subspace can be generated using O(N) random bits.",
        "published": "2007-01-03T18:25:26Z",
        "link": "http://arxiv.org/abs/math/0701102v3",
        "categories": [
            "math.FA",
            "cs.CC",
            "math.MG"
        ]
    },
    {
        "title": "A Polynomial Time Algorithm for 3-SAT",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Article describes a class of efficient algorithms for 3SAT and their generalizations on SAT.",
        "published": "2007-01-04T18:16:30Z",
        "link": "http://arxiv.org/abs/cs/0701023v4",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.LO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Polygraphic programs and polynomial-time functions",
        "authors": [
            "Guillaume Bonfante",
            "Yves Guiraud"
        ],
        "summary": "We study the computational model of polygraphs. For that, we consider polygraphic programs, a subclass of these objects, as a formal description of first-order functional programs. We explain their semantics and prove that they form a Turing-complete computational model. Their algebraic structure is used by analysis tools, called polygraphic interpretations, for complexity analysis. In particular, we delineate a subclass of polygraphic programs that compute exactly the functions that are Turing-computable in polynomial time.",
        "published": "2007-01-05T18:05:48Z",
        "link": "http://arxiv.org/abs/cs/0701032v4",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.CT",
            "F.1.1; F.4"
        ]
    },
    {
        "title": "A Counterexample to a Proposed Proof of P=NP by S. Gubin",
        "authors": [
            "Blake Hegerle"
        ],
        "summary": "In a recent paper by S. Gubin [cs/0701023v1], a polynomial-time solution to the 3SAT problem was presented as proof that P=NP. The proposed algorithm cannot be made to work, which I shall demonstrate.",
        "published": "2007-01-05T19:16:46Z",
        "link": "http://arxiv.org/abs/cs/0701033v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On the Complexity of the Numerically Definite Syllogistic and Related   Fragments",
        "authors": [
            "Ian Pratt-Hartmann"
        ],
        "summary": "In this paper, we determine the complexity of the satisfiability problem for various logics obtained by adding numerical quantifiers, and other constructions, to the traditional syllogistic. In addition, we demonstrate the incompleteness of some recently proposed proof-systems for these logics.",
        "published": "2007-01-06T15:00:00Z",
        "link": "http://arxiv.org/abs/cs/0701039v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "On the Complexity of a Derivative Chess Problem",
        "authors": [
            "Barnaby Martin"
        ],
        "summary": "We introduce QUEENS, a derivative chess problem based on the classical n-queens problem. We prove that QUEENS is NP-complete, with respect to polynomial-time reductions.",
        "published": "2007-01-08T16:45:55Z",
        "link": "http://arxiv.org/abs/cs/0701049v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Nearly-Exponential Size Lower Bounds for Symbolic Quantifier Elimination   Algorithms and OBDD-Based Proofs of Unsatisfiability",
        "authors": [
            "Nathan Segerlind"
        ],
        "summary": "We demonstrate a family of propositional formulas in conjunctive normal form so that a formula of size $N$ requires size $2^{\\Omega(\\sqrt[7]{N/logN})}$ to refute using the tree-like OBDD refutation system of Atserias, Kolaitis and Vardi with respect to all variable orderings. All known symbolic quantifier elimination algorithms for satisfiability generate tree-like proofs when run on unsatisfiable CNFs, so this lower bound applies to the run-times of these algorithms. Furthermore, the lower bound generalizes earlier results on OBDD-based proofs of unsatisfiability in that it applies for all variable orderings, it applies when the clauses are processed according to an arbitrary schedule, and it applies when variables are eliminated via quantification.",
        "published": "2007-01-09T01:16:00Z",
        "link": "http://arxiv.org/abs/cs/0701054v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.2.2"
        ]
    },
    {
        "title": "Constructive Dimension and Turing Degrees",
        "authors": [
            "Laurent Bienvenu",
            "David Doty",
            "Frank Stephan"
        ],
        "summary": "This paper examines the constructive Hausdorff and packing dimensions of Turing degrees. The main result is that every infinite sequence S with constructive Hausdorff dimension dim_H(S) and constructive packing dimension dim_P(S) is Turing equivalent to a sequence R with dim_H(R) <= (dim_H(S) / dim_P(S)) - epsilon, for arbitrary epsilon > 0. Furthermore, if dim_P(S) > 0, then dim_P(R) >= 1 - epsilon. The reduction thus serves as a *randomness extractor* that increases the algorithmic randomness of S, as measured by constructive dimension.   A number of applications of this result shed new light on the constructive dimensions of Turing degrees. A lower bound of dim_H(S) / dim_P(S) is shown to hold for the Turing degree of any sequence S. A new proof is given of a previously-known zero-one law for the constructive packing dimension of Turing degrees. It is also shown that, for any regular sequence S (that is, dim_H(S) = dim_P(S)) such that dim_H(S) > 0, the Turing degree of S has constructive Hausdorff and packing dimension equal to 1.   Finally, it is shown that no single Turing reduction can be a universal constructive Hausdorff dimension extractor, and that bounded Turing reductions cannot extract constructive Hausdorff dimension. We also exhibit sequences on which weak truth-table and bounded Turing reductions differ in their ability to extract dimension.",
        "published": "2007-01-15T04:41:00Z",
        "link": "http://arxiv.org/abs/cs/0701089v4",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Feasible Depth",
        "authors": [
            "David Doty",
            "Philippe Moser"
        ],
        "summary": "This paper introduces two complexity-theoretic formulations of Bennett's logical depth: finite-state depth and polynomial-time depth. It is shown that for both formulations, trivial and random infinite sequences are shallow, and a slow growth law holds, implying that deep sequences cannot be created easily from shallow sequences. Furthermore, the E analogue of the halting language is shown to be polynomial-time deep, by proving a more general result: every language to which a nonnegligible subset of E can be reduced in uniform exponential time is polynomial-time deep.",
        "published": "2007-01-19T22:10:50Z",
        "link": "http://arxiv.org/abs/cs/0701123v3",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Interference Automata",
        "authors": [
            "M. V. Panduranga Rao"
        ],
        "summary": "We propose a computing model, the Two-Way Optical Interference Automata (2OIA), that makes use of the phenomenon of optical interference. We introduce this model to investigate the increase in power, in terms of language recognition, of a classical Deterministic Finite Automaton (DFA) when endowed with the facility of optical interference. The question is in the spirit of Two-Way Finite Automata With Quantum and Classical States (2QCFA) [A. Ambainis and J. Watrous, Two-way Finite Automata With Quantum and Classical States, Theoretical Computer Science, 287 (1), 299-311, (2002)] wherein the classical DFA is augmented with a quantum component of constant size. We test the power of 2OIA against the languages mentioned in the above paper. We give efficient 2OIA algorithms to recognize languages for which 2QCFA machines have been shown to exist, as well as languages whose status vis-a-vis 2QCFA has been posed as open questions. Finally we show the existence of a language that cannot be recognized by a 2OIA but can be recognized by an $O(n^3)$ space Turing machine.",
        "published": "2007-01-22T09:14:11Z",
        "link": "http://arxiv.org/abs/cs/0701128v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A polynomial time algorithm to approximate the mixed volume within a   simply exponential factor",
        "authors": [
            "Leonid Gurvits"
        ],
        "summary": "Let ${\\bf K} = (K_1, ..., K_n)$ be an $n$-tuple of convex compact subsets in the Euclidean space $\\R^n$, and let $V(\\cdot)$ be the Euclidean volume in $\\R^n$. The Minkowski polynomial $V_{{\\bf K}}$ is defined as $V_{{\\bf K}}(\\lambda_1, ... ,\\lambda_n) = V(\\lambda_1 K_1 +, ..., + \\lambda_n K_n)$ and the mixed volume $V(K_1, ..., K_n)$ as $$ V(K_1, ..., K_n) = \\frac{\\partial^n}{\\partial \\lambda_1...\\partial \\lambda_n} V_{{\\bf K}}(\\lambda_1 K_1 +, ..., + \\lambda_n K_n). $$ Our main result is a poly-time algorithm which approximates $V(K_1, ..., K_n)$ with multiplicative error $e^n$ and with better rates if the affine dimensions of most of the sets $K_i$ are small. Our approach is based on a particular approximation of $\\log(V(K_1, ..., K_n))$ by a solution of some convex minimization problem. We prove the mixed volume analogues of the Van der Waerden and Schrijver-Valiant conjectures on the permanent. These results, interesting on their own, allow us to justify the abovementioned approximation by a convex minimization, which is solved using the ellipsoid method and a randomized poly-time time algorithm for the approximation of the volume of a convex set.",
        "published": "2007-02-02T01:09:36Z",
        "link": "http://arxiv.org/abs/cs/0702013v4",
        "categories": [
            "cs.CG",
            "cs.CC",
            "math.CO"
        ]
    },
    {
        "title": "Longest Common Separable Pattern between Permutations",
        "authors": [
            "Mathilde Bouvel",
            "Dominique Rossin",
            "Stephane Vialette"
        ],
        "summary": "In this article, we study the problem of finding the longest common separable pattern between several permutations. We give a polynomial-time algorithm when the number of input permutations is fixed and show that the problem is NP-hard for an arbitrary number of input permutations even if these permutations are separable. On the other hand, we show that the NP-hard problem of finding the longest common pattern between two permutations cannot be approximated better than within a ratio of $sqrt{Opt}$ (where $Opt$ is the size of an optimal solution) when taking common patterns belonging to pattern-avoiding classes of permutations.",
        "published": "2007-02-05T16:30:35Z",
        "link": "http://arxiv.org/abs/math/0702109v1",
        "categories": [
            "math.CO",
            "cs.CC",
            "05A05 - 05C12 - 05C85 - 05C05- 90C39"
        ]
    },
    {
        "title": "Design and Analysis of the REESSE1+ Public Key Cryptosystem v2.21",
        "authors": [
            "Shenghui Su",
            "Shuwang Lv"
        ],
        "summary": "In this paper, the authors give the definitions of a coprime sequence and a lever function, and describe the five algorithms and six characteristics of a prototypal public key cryptosystem which is used for encryption and signature, and based on three new problems and one existent problem: the multivariate permutation problem (MPP), the anomalous subset product problem (ASPP), the transcendental logarithm problem (TLP), and the polynomial root finding problem (PRFP). Prove by reduction that MPP, ASPP, and TLP are computationally at least equivalent to the discrete logarithm problem (DLP) in the same prime field, and meanwhile find some evidence which inclines people to believe that the new problems are harder than DLP each, namely unsolvable in DLP subexponential time. Demonstrate the correctness of the decryption and the verification, deduce the probability of a plaintext solution being nonunique is nearly zero, and analyze the exact securities of the cryptosystem against recovering a plaintext from a ciphertext, extracting a private key from a public key or a signature, and forging a signature through known signatures, public keys, and messages on the assumption that IFP, DLP, and LSSP can be solved. Studies manifest that the running times of effectual attack tasks are greater than or equal to O(2^n) so far when n = 80, 96, 112, or 128 with lgM = 696, 864, 1030, or 1216. As viewed from utility, it should be researched further how to decrease the length of a modulus and to increase the speed of the decryption.",
        "published": "2007-02-08T15:00:16Z",
        "link": "http://arxiv.org/abs/cs/0702046v18",
        "categories": [
            "cs.CR",
            "cs.CC",
            "F.1.3; F.2.1"
        ]
    },
    {
        "title": "Hierarchical Unambiguity",
        "authors": [
            "Holger Spakowski",
            "Rahul Tripathi"
        ],
        "summary": "We develop techniques to investigate relativized hierarchical unambiguous computation. We apply our techniques to generalize known constructs involving relativized unambiguity based complexity classes (UP and \\mathcal{UP}) to new constructs involving arbitrary higher levels of the relativized unambiguous polynomial hierarchy (UPH). Our techniques are developed on constraints imposed by hierarchical arrangement of unambiguous nondeterministic polynomial-time Turing machines, and so they differ substantially, in applicability and in nature, from standard methods (such as the switching lemma [Hastad, Computational Limitations of Small-Depth Circuits, MIT Press, 1987]), which play roles in carrying out similar generalizations.   Aside from achieving these generalizations, we resolve a question posed by Cai, Hemachandra, and Vyskoc [J. Cai, L. Hemachandra, and J. Vyskoc, Promises and fault-tolerant database access, In K. Ambos-Spies, S. Homer, and U. Schoening, editors, Complexity Theory, pages 101-146. Cambridge University Press, 1993] on an issue related to nonadaptive Turing access to UP and adaptive smart Turing access to \\mathcal{UP}.",
        "published": "2007-02-08T16:00:25Z",
        "link": "http://arxiv.org/abs/cs/0702047v1",
        "categories": [
            "cs.CC",
            "F.1.2; F.1.3"
        ]
    },
    {
        "title": "The DFAs of Finitely Different Languages",
        "authors": [
            "Andrew Badr",
            "Ian Shipman"
        ],
        "summary": "Two languages are \"finitely different\" if their symmetric difference is finite. We consider the DFAs of finitely different regular languages and find major structural similarities. We proceed to consider the smallest DFAs that recognize a language finitely different from some given DFA. Such \"f-minimal\" DFAs are not unique, and this non-uniqueness is characterized. Finally, we offer a solution to the minimization problem of finding such f-minimal DFAs.",
        "published": "2007-02-09T03:42:51Z",
        "link": "http://arxiv.org/abs/cs/0702053v1",
        "categories": [
            "cs.CC",
            "F.1.1; F.4.3"
        ]
    },
    {
        "title": "Exploring k-Colorability",
        "authors": [
            "Kia Kai Li"
        ],
        "summary": "An introductory paper to the graph k-colorability problem.",
        "published": "2007-02-09T23:07:30Z",
        "link": "http://arxiv.org/abs/cs/0702058v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "An Example of Pi^0_3-complete Infinitary Rational Relation",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We give in this paper an example of infinitary rational relation, accepted by a 2-tape B\\\"{u}chi automaton, which is Pi^0_3-complete in the Borel hierarchy. Moreover the example of infinitary rational relation given in this paper has a very simple structure and can be easily described by its sections.",
        "published": "2007-02-12T16:11:46Z",
        "link": "http://arxiv.org/abs/math/0702334v1",
        "categories": [
            "math.LO",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Tradeoff between decoding complexity and rate for codes on graphs",
        "authors": [
            "Pulkit Grover"
        ],
        "summary": "We consider transmission over a general memoryless channel, with bounded decoding complexity per bit under message passing decoding. We show that the achievable rate is bounded below capacity if there is a finite success in the decoding in a specified number of operations per bit at the decoder for some codes on graphs. These codes include LDPC and LDGM codes. Good performance with low decoding complexity suggests strong local structures in the graphs of these codes, which are detrimental to the code rate asymptotically. The proof method leads to an interesting necessary condition on the code structures which could achieve capacity with bounded decoding complexity. We also show that if a code sequence achieves a rate epsilon close to the channel capacity, the decoding complexity scales at least as O(log(1/epsilon).",
        "published": "2007-02-13T03:39:56Z",
        "link": "http://arxiv.org/abs/cs/0702073v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "A Local Algorithm for Finding Dense Subgraphs",
        "authors": [
            "Reid Andersen"
        ],
        "summary": "We present a local algorithm for finding dense subgraphs of bipartite graphs, according to the definition of density proposed by Kannan and Vinay. Our algorithm takes as input a bipartite graph with a specified starting vertex, and attempts to find a dense subgraph near that vertex. We prove that for any subgraph S with k vertices and density theta, there are a significant number of starting vertices within S for which our algorithm produces a subgraph S' with density theta / O(log n) on at most O(D k^2) vertices, where D is the maximum degree. The running time of the algorithm is O(D k^2), independent of the number of vertices in the graph.",
        "published": "2007-02-13T23:41:46Z",
        "link": "http://arxiv.org/abs/cs/0702078v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "A Hike in the Phases of the 1-in-3 Satisfiability",
        "authors": [
            "Elitza Maneva",
            "Talya Meltzer",
            "Jack Raymond",
            "Andrea Sportiello",
            "Lenka Zdeborová"
        ],
        "summary": "We summarise our results for the random $\\epsilon$--1-in-3 satisfiability problem, where $\\epsilon$ is a probability of negation of the variable. We employ both rigorous and heuristic methods to describe the SAT/UNSAT and Hard/Easy transitions.",
        "published": "2007-02-18T16:27:49Z",
        "link": "http://arxiv.org/abs/cond-mat/0702421v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "On the decidability and complexity of Metric Temporal Logic over finite   words",
        "authors": [
            "Joel Ouaknine",
            "James Worrell"
        ],
        "summary": "Metric Temporal Logic (MTL) is a prominent specification formalism for real-time systems. In this paper, we show that the satisfiability problem for MTL over finite timed words is decidable, with non-primitive recursive complexity. We also consider the model-checking problem for MTL: whether all words accepted by a given Alur-Dill timed automaton satisfy a given MTL formula. We show that this problem is decidable over finite words. Over infinite words, we show that model checking the safety fragment of MTL--which includes invariance and time-bounded response properties--is also decidable. These results are quite surprising in that they contradict various claims to the contrary that have appeared in the literature.",
        "published": "2007-02-21T16:14:34Z",
        "link": "http://arxiv.org/abs/cs/0702120v3",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "A Sequential Algorithm for Generating Random Graphs",
        "authors": [
            "Mohsen Bayati",
            "Jeong Han Kim",
            "Amin saberi"
        ],
        "summary": "We present a nearly-linear time algorithm for counting and randomly generating simple graphs with a given degree sequence in a certain range. For degree sequence $(d_i)_{i=1}^n$ with maximum degree $d_{\\max}=O(m^{1/4-\\tau})$, our algorithm generates almost uniform random graphs with that degree sequence in time $O(m\\,d_{\\max})$ where $m=\\f{1}{2}\\sum_id_i$ is the number of edges in the graph and $\\tau$ is any positive constant. The fastest known algorithm for uniform generation of these graphs McKay Wormald (1990) has a running time of $O(m^2d_{\\max}^2)$. Our method also gives an independent proof of McKay's estimate McKay (1985) for the number of such graphs.   We also use sequential importance sampling to derive fully Polynomial-time Randomized Approximation Schemes (FPRAS) for counting and uniformly generating random graphs for the same range of $d_{\\max}=O(m^{1/4-\\tau})$.   Moreover, we show that for $d = O(n^{1/2-\\tau})$, our algorithm can generate an asymptotically uniform $d$-regular graph. Our results improve the previous bound of $d = O(n^{1/3-\\tau})$ due to Kim and Vu (2004) for regular graphs.",
        "published": "2007-02-22T10:00:53Z",
        "link": "http://arxiv.org/abs/cs/0702124v5",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Tree Automata and Essential Input Variables",
        "authors": [
            "Slavcho Shtrakov"
        ],
        "summary": "We introduce and study the essential inputs (variables) for terms (trees) and tree automata.",
        "published": "2007-02-22T15:12:33Z",
        "link": "http://arxiv.org/abs/cs/0702129v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Tree automata and separable sets of input variables",
        "authors": [
            "Slavcho Shtrakov",
            "Vladimir Shtrakov"
        ],
        "summary": "We consider the computational complexity of tree transducers, depending on their separable sets of input variables.",
        "published": "2007-02-22T15:16:56Z",
        "link": "http://arxiv.org/abs/cs/0702123v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.1; F.1.3"
        ]
    },
    {
        "title": "Fast Exact Method for Solving the Travelling Salesman Problem",
        "authors": [
            "Vadim Yatsenko"
        ],
        "summary": "This paper describes TSP exact solution of polynomial complexity. It is considered properties of proposed method. Effectiveness of proposed solution is illustrated by outcomes of computer modeling.",
        "published": "2007-02-23T02:39:05Z",
        "link": "http://arxiv.org/abs/cs/0702133v1",
        "categories": [
            "cs.CC",
            "I.2.8; G.1.6"
        ]
    },
    {
        "title": "Essential Inputs and Minimal Tree Automata",
        "authors": [
            "Ivo Damyanov",
            "Slavcho Shtrakov"
        ],
        "summary": "We continue studying essential inputs of trees and automata. Strongly essential inputs of trees are introduced and studied. Various examples for application in Computer Science are shown.",
        "published": "2007-02-23T12:48:41Z",
        "link": "http://arxiv.org/abs/cs/0702136v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Tree Automata and Essential Subtrees",
        "authors": [
            "Slavcho Shtrakov"
        ],
        "summary": "We introduce essential subtrees for terms (trees) and tree automata . There are some results concerning independent sets of subtrees and separable sets for a tree and an automaton.",
        "published": "2007-02-23T13:20:58Z",
        "link": "http://arxiv.org/abs/cs/0702137v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "A Landscape Analysis of Constraint Satisfaction Problems",
        "authors": [
            "Florent Krzakala",
            "Jorge Kurchan"
        ],
        "summary": "We discuss an analysis of Constraint Satisfaction problems, such as Sphere Packing, K-SAT and Graph Coloring, in terms of an effective energy landscape. Several intriguing geometrical properties of the solution space become in this light familiar in terms of the well-studied ones of rugged (glassy) energy landscapes. A `benchmark' algorithm naturally suggested by this construction finds solutions in polynomial time up to a point beyond the `clustering' and in some cases even the `thermodynamic' transitions. This point has a simple geometric meaning and can be in principle determined with standard Statistical Mechanical methods, thus pushing the analytic bound up to which problems are guaranteed to be easy. We illustrate this for the graph three and four-coloring problem. For Packing problems the present discussion allows to better characterize the `J-point', proposed as a systematic definition of Random Close Packing, and to place it in the context of other theories of glasses.",
        "published": "2007-02-23T15:25:30Z",
        "link": "http://arxiv.org/abs/cond-mat/0702546v4",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC",
            "nlin.CD"
        ]
    },
    {
        "title": "Games on the Sperner Triangle",
        "authors": [
            "Kyle Burke",
            "Shang-Hua Teng"
        ],
        "summary": "We create a new two-player game on the Sperner Triangle based on Sperner's lemma. Our game has simple rules and several desirable properties. First, the game is always certain to have a winner. Second, like many other interesting games such as Hex and Geography, we prove that deciding whether one can win our game is a PSPACE-complete problem. Third, there is an elegant balance in the game such that neither the first nor the second player always has a decisive advantage. We provide a web-based version of the game, playable at: http://cs-people.bu.edu/paithan/spernerGame/ . In addition we propose other games, also based on fixed-point theorems.",
        "published": "2007-02-26T05:00:45Z",
        "link": "http://arxiv.org/abs/cs/0702153v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Finding long cycles in graphs",
        "authors": [
            "Enzo Marinari",
            "Guilhem Semerjian",
            "Valery Van Kerrebroeck"
        ],
        "summary": "We analyze the problem of discovering long cycles inside a graph. We propose and test two algorithms for this task. The first one is based on recent advances in statistical mechanics and relies on a message passing procedure. The second follows a more standard Monte Carlo Markov Chain strategy. Special attention is devoted to Hamiltonian cycles of (non-regular) random graphs of minimal connectivity equal to three.",
        "published": "2007-02-26T20:33:20Z",
        "link": "http://arxiv.org/abs/cond-mat/0702613v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC",
            "math.PR"
        ]
    },
    {
        "title": "A Quantifier-Free String Theory for ALOGTIME Reasoning",
        "authors": [
            "François Pitt"
        ],
        "summary": "The main contribution of this work is the definition of a quantifier-free string theory T_1 suitable for formalizing ALOGTIME reasoning. After describing L_1 -- a new, simple, algebraic characterization of the complexity class ALOGTIME based on strings instead of numbers -- the theory T_1 is defined (based on L_1), and a detailed formal development of T_1 is given.   Then, theorems of T_1 are shown to translate into families of propositional tautologies that have uniform polysize Frege proofs, T_1 is shown to prove the soundness of a particular Frege system F, and F is shown to provably p-simulate any proof system whose soundness can be proved in T_1. Finally, T_1 is compared with other theories for ALOGTIME reasoning in the literature.   To our knowledge, this is the first formal theory for ALOGTIME reasoning whose basic objects are strings instead of numbers, and the first quantifier-free theory formalizing ALOGTIME reasoning in which a direct proof of the soundness of some Frege system has been given (in the case of first-order theories, such a proof was first given by Arai for his theory AID). Also, the polysize Frege proofs we give for the propositional translations of theorems of T_1 are considerably simpler than those for other theories, and so is our proof of the soundness of a particular F-system in T_1. Together with the simplicity of T_1's recursion schemes, axioms, and rules these facts suggest that T_1 is one of the most natural theories available for ALOGTIME reasoning.",
        "published": "2007-02-28T02:59:36Z",
        "link": "http://arxiv.org/abs/cs/0702160v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "Integral Biomathics: A Post-Newtonian View into the Logos of Bios (On   the New Meaning, Relations and Principles of Life in Science)",
        "authors": [
            "Plamen L. Simeonov"
        ],
        "summary": "This work is an attempt for a state-of-the-art survey of natural and life sciences with the goal to define the scope and address the central questions of an original research program. It is focused on the phenomena of emergence, adaptive dynamics and evolution of self-assembling, self-organizing, self-maintaining and self-replicating biosynthetic systems viewed from a newly-arranged perspective and understanding of computation and communication in the living nature.",
        "published": "2007-02-28T22:52:43Z",
        "link": "http://arxiv.org/abs/cs/0703002v9",
        "categories": [
            "cs.NE",
            "cs.CC",
            "F.1.1; F.4.0; H.1.1; I.2.0; J.3"
        ]
    },
    {
        "title": "Intensional properties of polygraphs",
        "authors": [
            "Guillaume Bonfante",
            "Yves Guiraud"
        ],
        "summary": "We present polygraphic programs, a subclass of Albert Burroni's polygraphs, as a computational model, showing how these objects can be seen as first-order functional programs. We prove that the model is Turing complete. We use polygraphic interpretations, a termination proof method introduced by the second author, to characterize polygraphic programs that compute in polynomial time. We conclude with a characterization of polynomial time functions and non-deterministic polynomial time functions.",
        "published": "2007-03-02T09:21:20Z",
        "link": "http://arxiv.org/abs/cs/0703007v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.CT",
            "F.1.1; F.4"
        ]
    },
    {
        "title": "Can we Compute the Similarity Between Surfaces?",
        "authors": [
            "Helmut Alt",
            "Maike Buchin"
        ],
        "summary": "A suitable measure for the similarity of shapes represented by parameterized curves or surfaces is the Fr\\'echet distance. Whereas efficient algorithms are known for computing the Fr\\'echet distance of polygonal curves, the same problem for triangulated surfaces is NP-hard. Furthermore, it remained open whether it is computable at all. Here, using a discrete approximation we show that it is {\\em upper semi-computable}, i.e., there is a non-halting Turing machine which produces a monotone decreasing sequence of rationals converging to the result. It follows that the decision problem, whether the Fr\\'echet distance of two given surfaces lies below some specified value, is recursively enumerable.   Furthermore, we show that a relaxed version of the problem, the computation of the {\\em weak Fr\\'echet distance} can be solved in polynomial time. For this, we give a computable characterization of the weak Fr\\'echet distance in a geometric data structure called the {\\em free space diagram}.",
        "published": "2007-03-02T17:00:18Z",
        "link": "http://arxiv.org/abs/cs/0703011v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "F.2.2; F.1.1"
        ]
    },
    {
        "title": "Algorithmic Information Theory: a brief non-technical guide to the field",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "This article is a brief guide to the field of algorithmic information theory (AIT), its underlying philosophy, and the most important concepts. AIT arises by mixing information theory and computation theory to obtain an objective and absolute notion of information in an individual object, and in so doing gives rise to an objective and robust notion of randomness of individual objects. This is in contrast to classical information theory that is based on random variables and communication, and has no bearing on information and randomness of individual objects. After a brief overview, the major subfields, applications, history, and a map of the field are presented.",
        "published": "2007-03-06T09:54:47Z",
        "link": "http://arxiv.org/abs/cs/0703024v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "FPRAS for computing a lower bound for weighted matching polynomial of   graphs",
        "authors": [
            "Shmuel Friedland"
        ],
        "summary": "We give a fully polynomial randomized approximation scheme to compute a lower bound for the matching polynomial of any weighted graph at a positive argument. For the matching polynomial of complete bipartite graphs with bounded weights these lower bounds are asymptotically optimal.",
        "published": "2007-03-06T22:59:30Z",
        "link": "http://arxiv.org/abs/cs/0703029v2",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Sofic Trace of a Cellular Automaton",
        "authors": [
            "Julien Cervelle",
            "Enrico Formenti",
            "Pierre Guillon"
        ],
        "summary": "The trace subshift of a cellular automaton is the subshift of all possible columns that may appear in a space-time diagram, ie the infinite sequence of states of a particular cell of a configuration; in the language of symbolic dynamics one says that it is a factor system. In this paper we study conditions for a sofic subshift to be the trace of a cellular automaton.",
        "published": "2007-03-08T20:48:25Z",
        "link": "http://arxiv.org/abs/math/0703241v1",
        "categories": [
            "math.DS",
            "cs.CC",
            "cs.DM",
            "37B15"
        ]
    },
    {
        "title": "Geometry and the complexity of matrix multiplication",
        "authors": [
            "J. M. Landsberg"
        ],
        "summary": "We survey results in algebraic complexity theory, focusing on matrix multiplication. Our goals are   (i.) to show how open questions in algebraic complexity theory are naturally posed as questions in geometry and representation theory, (ii.) to motivate researchers to work on these questions, and (iii.) to point out relations with more general problems in geometry. The key geometric objects for our study are the secant varieties of Segre varieties. We explain how these varieties are also useful for algebraic statistics, the study of phylogenetic invariants, and quantum computing.",
        "published": "2007-03-12T22:11:55Z",
        "link": "http://arxiv.org/abs/cs/0703059v1",
        "categories": [
            "cs.CC",
            "math.AG",
            "math.RT"
        ]
    },
    {
        "title": "Satisfying assignments of Random Boolean CSP: Clusters and Overlaps",
        "authors": [
            "Gabriel Istrate"
        ],
        "summary": "The distribution of overlaps of solutions of a random CSP is an indicator of the overall geometry of its solution space. For random $k$-SAT, nonrigorous methods from Statistical Physics support the validity of the ``one step replica symmetry breaking'' approach. Some of these predictions were rigorously confirmed in \\cite{cond-mat/0504070/prl} \\cite{cond-mat/0506053}. There it is proved that the overlap distribution of random $k$-SAT, $k\\geq 9$, has discontinuous support. Furthermore, Achlioptas and Ricci-Tersenghi proved that, for random $k$-SAT, $k\\geq 8$. and constraint densities close enough to the phase transition there exists an exponential number of clusters of satisfying assignments; moreover, the distance between satisfying assignments in different clusters is linear.   We aim to understand the structural properties of random CSP that lead to solution clustering. To this end, we prove two results on the cluster structure of solutions for binary CSP under the random model from Molloy (STOC 2002)   1. For all constraint sets $S$ (described explicitly in Creignou and Daude (2004), Istrate (2005)) s.t. $SAT(S)$ has a sharp threshold and all $q\\in (0,1]$, $q$-overlap-$SAT(S)$ has a sharp threshold (i.e. the first step of the approach in Mora et al. works in all nontrivial cases). 2. For any constraint density value $c<1$, the set of solutions of a random instance of 2-SAT form, w.h.p., a single cluster. Also, for and any $q\\in (0,1]$ such an instance has w.h.p. two satisfying assignment of overlap $\\sim q$. Thus, as expected from Statistical Physics predictions, the second step of the approach in Mora et al. fails for 2-SAT.",
        "published": "2007-03-13T15:45:08Z",
        "link": "http://arxiv.org/abs/cs/0703065v1",
        "categories": [
            "cs.DM",
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "Randomized Computations on Large Data Sets: Tight Lower Bounds",
        "authors": [
            "Martin Grohe",
            "Andre Hernich",
            "Nicole Schweikardt"
        ],
        "summary": "We study the randomized version of a computation model (introduced by Grohe, Koch, and Schweikardt (ICALP'05); Grohe and Schweikardt (PODS'05)) that restricts random access to external memory and internal memory space. Essentially, this model can be viewed as a powerful version of a data stream model that puts no cost on sequential scans of external memory (as other models for data streams) and, in addition, (like other external memory models, but unlike streaming models), admits several large external memory devices that can be read and written to in parallel.   We obtain tight lower bounds for the decision problems set equality, multiset equality, and checksort. More precisely, we show that any randomized one-sided-error bounded Monte Carlo algorithm for these problems must perform Omega(log N) random accesses to external memory devices, provided that the internal memory size is at most O(N^(1/4)/log N), where N denotes the size of the input data.   From the lower bound on the set equality problem we can infer lower bounds on the worst case data complexity of query evaluation for the languages XQuery, XPath, and relational algebra on streaming data. More precisely, we show that there exist queries in XQuery, XPath, and relational algebra, such that any (randomized) Las Vegas algorithm that evaluates these queries must perform Omega(log N) random accesses to external memory devices, provided that the internal memory size is at most O(N^(1/4)/log N).",
        "published": "2007-03-15T12:58:36Z",
        "link": "http://arxiv.org/abs/cs/0703081v1",
        "categories": [
            "cs.DB",
            "cs.CC",
            "F.1.3; F.1.1"
        ]
    },
    {
        "title": "Semidefinite programming characterization and spectral adversary method   for quantum complexity with noncommuting unitary queries",
        "authors": [
            "Howard N. Barnum"
        ],
        "summary": "Generalizing earlier work characterizing the quantum query complexity of computing a function of an unknown classical ``black box'' function drawn from some set of such black box functions, we investigate a more general quantum query model in which the goal is to compute functions of N by N ``black box'' unitary matrices drawn from a set of such matrices, a problem with applications to determining properties of quantum physical systems. We characterize the existence of an algorithm for such a query problem, with given error and number of queries, as equivalent to the feasibility of a certain set of semidefinite programming constraints, or equivalently the infeasibility of a dual of these constraints, which we construct. Relaxing the primal constraints to correspond to mere pairwise near-orthogonality of the final states of a quantum computer, conditional on black-box inputs having distinct function values, rather than bounded-error determinability of the function value via a single measurement on the output states, we obtain a relaxed primal program the feasibility of whose dual still implies the nonexistence of a quantum algorithm. We use this to obtain a generalization, to our not-necessarily-commutative setting, of the ``spectral adversary method'' for quantum query lower bounds.",
        "published": "2007-03-15T18:43:56Z",
        "link": "http://arxiv.org/abs/quant-ph/0703141v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Dimension and Relative Frequencies",
        "authors": [
            "Xiaoyang Gu",
            "Jack H. Lutz"
        ],
        "summary": "We show how to calculate the finite-state dimension (equivalently, the finite-state compressibility) of a saturated sets $X$ consisting of {\\em all} infinite sequences $S$ over a finite alphabet $\\Sigma_m$ satisfying some given condition $P$ on the asymptotic frequencies with which various symbols from $\\Sigma_m$ appear in $S$. When the condition $P$ completely specifies an empirical probability distribution $\\pi$ over $\\Sigma_m$, i.e., a limiting frequency of occurrence for {\\em every} symbol in $\\Sigma_m$, it has been known since 1949 that the Hausdorff dimension of $X$ is precisely $\\CH(\\pi)$, the Shannon entropy of $\\pi$, and the finite-state dimension was proven to have this same value in 2001.   The saturated sets were studied by Volkmann and Cajar decades ago. It got attention again only with the recent developments in multifractal analysis by Barreira, Saussol, Schmeling, and separately Olsen. However, the powerful methods they used -- ergodic theory and multifractal analysis -- do not yield a value for the finite-state (or even computable) dimension in an obvious manner.   We give a pointwise characterization of finite-state dimensions of saturated sets. Simultaneously, we also show that their finite-state dimension and strong dimension coincide with their Hausdorff and packing dimension respectively, though the techniques we use are completely elementary. Our results automatically extend to less restrictive effective settings (e.g., constructive, computable, and polynomial-time dimensions).",
        "published": "2007-03-16T02:42:53Z",
        "link": "http://arxiv.org/abs/cs/0703085v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Convex Discrete Optimization",
        "authors": [
            "Shmuel Onn"
        ],
        "summary": "We develop an algorithmic theory of convex optimization over discrete sets. Using a combination of algebraic and geometric tools we are able to provide polynomial time algorithms for solving broad classes of convex combinatorial optimization problems and convex integer programming problems in variable dimension. We discuss some of the many applications of this theory including to quadratic programming, matroids, bin packing and cutting-stock problems, vector partitioning and clustering, multiway transportation problems, and privacy and confidential statistical data disclosure. Highlights of our work include a strongly polynomial time algorithm for convex and linear combinatorial optimization over any family presented by a membership oracle when the underlying polytope has few edge-directions; a new theory of so-termed n-fold integer programming, yielding polynomial time solution of important and natural classes of convex and linear integer programming problems in variable dimension; and a complete complexity classification of high dimensional transportation problems, with practical applications to fundamental problems in privacy and confidential statistical data disclosure.",
        "published": "2007-03-20T08:47:46Z",
        "link": "http://arxiv.org/abs/math/0703575v1",
        "categories": [
            "math.OC",
            "cs.CC",
            "cs.DM",
            "math.CO"
        ]
    },
    {
        "title": "On Approximating Optimal Weighted Lobbying, and Frequency of Correctness   versus Average-Case Polynomial Time",
        "authors": [
            "Gabor Erdelyi",
            "Lane A. Hemaspaandra",
            "Joerg Rothe",
            "Holger Spakowski"
        ],
        "summary": "We investigate issues related to two hard problems related to voting, the optimal weighted lobbying problem and the winner problem for Dodgson elections. Regarding the former, Christian et al. [CFRS06] showed that optimal lobbying is intractable in the sense of parameterized complexity. We provide an efficient greedy algorithm that achieves a logarithmic approximation ratio for this problem and even for a more general variant--optimal weighted lobbying. We prove that essentially no better approximation ratio than ours can be proven for this greedy algorithm.   The problem of determining Dodgson winners is known to be complete for parallel access to NP [HHR97]. Homan and Hemaspaandra [HH06] proposed an efficient greedy heuristic for finding Dodgson winners with a guaranteed frequency of success, and their heuristic is a ``frequently self-knowingly correct algorithm.'' We prove that every distributional problem solvable in polynomial time on the average with respect to the uniform distribution has a frequently self-knowingly correct polynomial-time algorithm. Furthermore, we study some features of probability weight of correctness with respect to Procaccia and Rosenschein's junta distributions [PR07].",
        "published": "2007-03-20T20:35:02Z",
        "link": "http://arxiv.org/abs/cs/0703097v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Polynomial time algorithm for 3-SAT. Examples of use",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "The algorithm checks the propositional formulas for patterns of unsatisfiability.",
        "published": "2007-03-21T06:46:09Z",
        "link": "http://arxiv.org/abs/cs/0703098v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.LO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Approximation Algorithms for Multiprocessor Scheduling under Uncertainty",
        "authors": [
            "Guolong Lin",
            "Rajmohan Rajaraman"
        ],
        "summary": "Motivated by applications in grid computing and project management, we study multiprocessor scheduling in scenarios where there is uncertainty in the successful execution of jobs when assigned to processors. We consider the problem of multiprocessor scheduling under uncertainty, in which we are given n unit-time jobs and m machines, a directed acyclic graph C giving the dependencies among the jobs, and for every job j and machine i, the probability p_{ij} of the successful completion of job j when scheduled on machine i in any given particular step. The goal of the problem is to find a schedule that minimizes the expected makespan, that is, the expected completion time of all the jobs.   The problem of multiprocessor scheduling under uncertainty was introduced by Malewicz and was shown to be NP-hard even when all the jobs are independent. In this paper, we present polynomial-time approximation algorithms for the problem, for special cases of the dag C. We obtain an O(log(n))-approximation for the case of independent jobs, an O(log(m)log(n)log(n+m)/loglog(n+m))-approximation when C is a collection of disjoint chains, an O(log(m)log^2(n))-approximation when C is a collection of directed out- or in-trees, and an O(log(m)log^2(n)log(n+m)/loglog(n+m))-approximation when C is a directed forest.",
        "published": "2007-03-21T20:35:40Z",
        "link": "http://arxiv.org/abs/cs/0703100v1",
        "categories": [
            "cs.DC",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "A Note on Approximate Nearest Neighbor Methods",
        "authors": [
            "Thomas M. Breuel"
        ],
        "summary": "A number of authors have described randomized algorithms for solving the epsilon-approximate nearest neighbor problem. In this note I point out that the epsilon-approximate nearest neighbor property often fails to be a useful approximation property, since epsilon-approximate solutions fail to satisfy the necessary preconditions for using nearest neighbors for classification and related tasks.",
        "published": "2007-03-21T20:47:33Z",
        "link": "http://arxiv.org/abs/cs/0703101v1",
        "categories": [
            "cs.IR",
            "cs.CC",
            "cs.CV"
        ]
    },
    {
        "title": "New List Decoding Algorithms for Reed-Solomon and BCH Codes",
        "authors": [
            "Yingquan Wu"
        ],
        "summary": "In this paper we devise a rational curve fitting algorithm and apply it to the list decoding of Reed-Solomon and BCH codes. The proposed list decoding algorithms exhibit the following significant properties. 1 The algorithm corrects up to $n(1-\\sqrt{1-D})$ errors for a (generalized) $(n, k, d=n-k+1)$ Reed-Solomon code, which matches the Johnson bound, where $D\\eqdef \\frac{d}{n}$ denotes the normalized minimum distance. In comparison with the Guruswami-Sudan algorithm, which exhibits the same list correction capability, the former requires multiplicity, which dictates the algorithmic complexity, $O(n(1-\\sqrt{1-D}))$, whereas the latter requires multiplicity $O(n^2(1-D))$. With the up-to-date most efficient implementation, the former has complexity $O(n^{6}(1-\\sqrt{1-D})^{7/2})$, whereas the latter has complexity $O(n^{10}(1-D)^4)$. 2. With the multiplicity set to one, the derivative list correction capability precisely sits in between the conventional hard-decision decoding and the optimal list decoding. Moreover, the number of candidate codewords is upper bounded by a constant for a fixed code rate and thus, the derivative algorithm exhibits quadratic complexity $O(n^2)$. 3. By utilizing the unique properties of the Berlekamp algorithm, the algorithm corrects up to $\\frac{n}{2}(1-\\sqrt{1-2D})$ errors for a narrow-sense $(n, k, d)$ binary BCH code, which matches the Johnson bound for binary codes. The algorithmic complexity is $O(n^{6}(1-\\sqrt{1-2D})^7)$.",
        "published": "2007-03-22T05:29:48Z",
        "link": "http://arxiv.org/abs/cs/0703105v3",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "Geometric Complexity Theory IV: nonstandard quantum group for the   Kronecker problem",
        "authors": [
            "Jonah Blasiak",
            "Ketan D. Mulmuley",
            "Milind Sohoni"
        ],
        "summary": "The Kronecker coefficient g_{\\lambda \\mu \\nu} is the multiplicity of the GL(V)\\times GL(W)-irreducible V_\\lambda \\otimes W_\\mu in the restriction of the GL(X)-irreducible X_\\nu via the natural map GL(V)\\times GL(W) \\to GL(V \\otimes W), where V, W are \\mathbb{C}-vector spaces and X = V \\otimes W. A fundamental open problem in algebraic combinatorics is to find a positive combinatorial formula for these coefficients.   We construct two quantum objects for this problem, which we call the nonstandard quantum group and nonstandard Hecke algebra. We show that the nonstandard quantum group has a compact real form and its representations are completely reducible, that the nonstandard Hecke algebra is semisimple, and that they satisfy an analog of quantum Schur-Weyl duality.   Using these nonstandard objects as a guide, we follow the approach of Adsul, Sohoni, and Subrahmanyam to construct, in the case dim(V) = dim(W) =2, a representation \\check{X}_\\nu of the nonstandard quantum group that specializes to Res_{GL(V) \\times GL(W)} X_\\nu at q=1. We then define a global crystal basis +HNSTC(\\nu) of \\check{X}_\\nu that solves the two-row Kronecker problem: the number of highest weight elements of +HNSTC(\\nu) of weight (\\lambda,\\mu) is the Kronecker coefficient g_{\\lambda \\mu \\nu}. We go on to develop the beginnings of a graphical calculus for this basis, along the lines of the U_q(\\sl_2) graphical calculus, and use this to organize the crystal components of +HNSTC(\\nu) into eight families. This yields a fairly simple, explicit and positive formula for two-row Kronecker coefficients, generalizing a formula of Brown, van Willigenburg, and Zabrocki. As a byproduct of the approach, we also obtain a rule for the decomposition of Res_{GL_2 \\times GL_2 \\rtimes \\S_2} X_\\nu into irreducibles.",
        "published": "2007-03-22T15:35:29Z",
        "link": "http://arxiv.org/abs/cs/0703110v4",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Classical Interaction Cannot Replace a Quantum Message",
        "authors": [
            "Dmytro Gavinsky"
        ],
        "summary": "We demonstrate a two-player communication problem that can be solved in the one-way quantum model by a 0-error protocol of cost O (log n) but requires exponentially more communication in the classical interactive (bounded error) model.",
        "published": "2007-03-23T01:38:13Z",
        "link": "http://arxiv.org/abs/quant-ph/0703215v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Physarum machine: Implementation of Kolmogorov-Uspensky machine in   biological substrat",
        "authors": [
            "Andrew Adamatzky"
        ],
        "summary": "We implement Kolmogorov-Uspensky machine on a plasmodium of true slime mold {\\em Physarum polycephalum}. We provide experimental findings on realization of the machine instructions, illustrate basic operations, and elements of programming.",
        "published": "2007-03-26T01:51:25Z",
        "link": "http://arxiv.org/abs/cs/0703128v2",
        "categories": [
            "cs.AR",
            "cs.CC",
            "F.1.1"
        ]
    },
    {
        "title": "The Simultaneous Triple Product Property and Group-theoretic Results for   the Exponent of Matrix Multiplication",
        "authors": [
            "Sandeep Murthy"
        ],
        "summary": "We describe certain special consequences of certain elementary methods from group theory for studying the algebraic complexity of matrix multiplication, as developed by H. Cohn, C. Umans et. al. in 2003 and 2005. The measure of complexity here is the exponent of matrix multiplication, a real parameter between 2 and 3, which has been conjectured to be 2. More specifically, a finite group may simultaneously \"realize\" several independent matrix multiplications via its regular algebra if it has a family of triples of \"index\" subsets which satisfy the so-called simultaneous triple product property (STPP), in which case the complexity of these several multiplications does not exceed the rank (complexity) of the algebra. This leads to bounds for the exponent in terms of the size of the group and the sizes of its STPP triples, as well as the dimensions of its distinct irreducible representations. Wreath products of Abelian with symmetric groups appear especially important, in this regard, and we give an example of such a group which shows that the exponent is less than 2.84, and could be possibly be as small as 2.02 depending on the number of simultaneous matrix multiplications it realizes.",
        "published": "2007-03-29T02:55:17Z",
        "link": "http://arxiv.org/abs/cs/0703145v4",
        "categories": [
            "cs.DS",
            "cs.CC",
            "math.GR",
            "F.2.1"
        ]
    },
    {
        "title": "A Polynomial Time Algorithm for SAT",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Article presents the compatibility matrix method and illustrates it with the application to P vs NP problem. The method is a generalization of descriptive geometry: in the method, we draft problems and solve them utilizing the image creation technique. The method reveals: P = NP = PSPACE",
        "published": "2007-03-29T07:36:30Z",
        "link": "http://arxiv.org/abs/cs/0703146v4",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.LO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Reducing SAT to 2-SAT",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Description of a polynomial time reduction of SAT to 2-SAT of polynomial size.",
        "published": "2007-04-01T23:16:27Z",
        "link": "http://arxiv.org/abs/0704.0108v1",
        "categories": [
            "cs.CC",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Geometric Complexity Theory V: On deciding nonvanishing of a generalized   Littlewood-Richardson coefficient",
        "authors": [
            "Ketan D. Mulmuley Hariharan Narayanan"
        ],
        "summary": "This article has been withdrawn because it has been merged with the earlier article GCT3 (arXiv: CS/0501076 [cs.CC]) in the series. The merged article is now available as:   Geometric Complexity Theory III: on deciding nonvanishing of a Littlewood-Richardson Coefficient, Journal of Algebraic Combinatorics, vol. 36, issue 1, 2012, pp. 103-110. (Authors: Ketan Mulmuley, Hari Narayanan and Milind Sohoni)   The new article in this GCT5 slot in the series is:   Geometric Complexity Theory V: Equivalence between blackbox derandomization of polynomial identity testing and derandomization of Noether's Normalization Lemma, in the Proceedings of FOCS 2012 (abstract), arXiv:1209.5993 [cs.CC] (full version) (Author: Ketan Mulmuley)",
        "published": "2007-04-02T15:13:27Z",
        "link": "http://arxiv.org/abs/0704.0213v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Geometric Complexity Theory VI: the flip via saturated and positive   integer programming in representation theory and algebraic geometry",
        "authors": [
            "Ketan D. Mulmuley"
        ],
        "summary": "This article belongs to a series on geometric complexity theory (GCT), an approach to the P vs. NP and related problems through algebraic geometry and representation theory. The basic principle behind this approach is called the flip. In essence, it reduces the negative hypothesis in complexity theory (the lower bound problems), such as the P vs. NP problem in characteristic zero, to the positive hypothesis in complexity theory (the upper bound problems): specifically, to showing that the problems of deciding nonvanishing of the fundamental structural constants in representation theory and algebraic geometry, such as the well known plethysm constants--or rather certain relaxed forms of these decision probelms--belong to the complexity class P. In this article, we suggest a plan for implementing the flip, i.e., for showing that these relaxed decision problems belong to P. This is based on the reduction of the preceding complexity-theoretic positive hypotheses to mathematical positivity hypotheses: specifically, to showing that there exist positive formulae--i.e. formulae with nonnegative coefficients--for the structural constants under consideration and certain functions associated with them. These turn out be intimately related to the similar positivity properties of the Kazhdan-Lusztig polynomials and the multiplicative structural constants of the canonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum groups. The known proofs of these positivity properties depend on the Riemann hypothesis over finite fields and the related results. Thus the reduction here, in conjunction with the flip, in essence, says that the validity of the P vs. NP conjecture in characteristic zero is intimately linked to the Riemann hypothesis over finite fields and related problems.",
        "published": "2007-04-02T16:41:38Z",
        "link": "http://arxiv.org/abs/0704.0229v4",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On Punctured Pragmatic Space-Time Codes in Block Fading Channel",
        "authors": [
            "Samuele Bandi",
            "Luca Stabellini",
            "Andrea Conti",
            "Velio Tralli"
        ],
        "summary": "This paper considers the use of punctured convolutional codes to obtain pragmatic space-time trellis codes over block-fading channel. We show that good performance can be achieved even when puncturation is adopted and that we can still employ the same Viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations.",
        "published": "2007-04-02T22:44:17Z",
        "link": "http://arxiv.org/abs/0704.0282v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "The Complexity of HCP in Digraps with Degree Bound Two",
        "authors": [
            "Guohun Zhu"
        ],
        "summary": "The Hamiltonian cycle problem (HCP) in digraphs D with degree bound two is solved by two mappings in this paper. The first bijection is between an incidence matrix C_{nm} of simple digraph and an incidence matrix F of balanced bipartite undirected graph G; The second mapping is from a perfect matching of G to a cycle of D. It proves that the complexity of HCP in D is polynomial, and finding a second non-isomorphism Hamiltonian cycle from a given Hamiltonian digraph with degree bound two is also polynomial. Lastly it deduces P=NP base on the results.",
        "published": "2007-04-03T03:50:43Z",
        "link": "http://arxiv.org/abs/0704.0309v3",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Differential Recursion and Differentially Algebraic Functions",
        "authors": [
            "Akitoshi Kawamura"
        ],
        "summary": "Moore introduced a class of real-valued \"recursive\" functions by analogy with Kleene's formulation of the standard recursive functions. While his concise definition inspired a new line of research on analog computation, it contains some technical inaccuracies. Focusing on his \"primitive recursive\" functions, we pin down what is problematic and discuss possible attempts to remove the ambiguity regarding the behavior of the differential recursion operator on partial functions. It turns out that in any case the purported relation to differentially algebraic functions, and hence to Shannon's model of analog computation, fails.",
        "published": "2007-04-03T19:50:14Z",
        "link": "http://arxiv.org/abs/0704.0301v1",
        "categories": [
            "cs.CC",
            "F.1.1"
        ]
    },
    {
        "title": "Inapproximability of Maximum Weighted Edge Biclique and Its Applications",
        "authors": [
            "Jinsong Tan"
        ],
        "summary": "Given a bipartite graph $G = (V_1,V_2,E)$ where edges take on {\\it both} positive and negative weights from set $\\mathcal{S}$, the {\\it maximum weighted edge biclique} problem, or $\\mathcal{S}$-MWEB for short, asks to find a bipartite subgraph whose sum of edge weights is maximized. This problem has various applications in bioinformatics, machine learning and databases and its (in)approximability remains open. In this paper, we show that for a wide range of choices of $\\mathcal{S}$, specifically when $| \\frac{\\min\\mathcal{S}} {\\max \\mathcal{S}} | \\in \\Omega(\\eta^{\\delta-1/2}) \\cap O(\\eta^{1/2-\\delta})$ (where $\\eta = \\max\\{|V_1|, |V_2|\\}$, and $\\delta \\in (0,1/2]$), no polynomial time algorithm can approximate $\\mathcal{S}$-MWEB within a factor of $n^{\\epsilon}$ for some $\\epsilon > 0$ unless $\\mathsf{RP = NP}$. This hardness result gives justification of the heuristic approaches adopted for various applied problems in the aforementioned areas, and indicates that good approximation algorithms are unlikely to exist. Specifically, we give two applications by showing that: 1) finding statistically significant biclusters in the SAMBA model, proposed in \\cite{Tan02} for the analysis of microarray data, is $n^{\\epsilon}$-inapproximable; and 2) no polynomial time algorithm exists for the Minimum Description Length with Holes problem \\cite{Bu05} unless $\\mathsf{RP=NP}$.",
        "published": "2007-04-03T21:39:11Z",
        "link": "http://arxiv.org/abs/0704.0468v2",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.2.0"
        ]
    },
    {
        "title": "On the Kolmogorov-Chaitin Complexity for short sequences",
        "authors": [
            "Jean-Paul Delahaye",
            "Hector Zenil"
        ],
        "summary": "A drawback of Kolmogorov-Chaitin complexity (K) as a function from s to the shortest program producing s is its noncomputability which limits its range of applicability. Moreover, when strings are short, the dependence of K on a particular universal Turing machine U can be arbitrary. In practice one can approximate it by computable compression methods. However, such compression methods do not always provide meaningful approximations--for strings shorter, for example, than typical compiler lengths. In this paper we suggest an empirical approach to overcome this difficulty and to obtain a stable definition of the Kolmogorov-Chaitin complexity for short sequences. Additionally, a correlation in terms of distribution frequencies was found across the output of two models of abstract machines, namely unidimensional cellular automata and deterministic Turing machine.",
        "published": "2007-04-08T20:01:47Z",
        "link": "http://arxiv.org/abs/0704.1043v5",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Phase Transitions in the Coloring of Random Graphs",
        "authors": [
            "Lenka Zdeborová",
            "Florent Krzakala"
        ],
        "summary": "We consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. Using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions).   We show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. First, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. Afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. Another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. Eventually, above the coloring threshold, no more solutions are available. We compute all the critical connectivities for Erdos-Renyi and regular random graphs and determine their asymptotic values for large number of colors.   Finally, we discuss the algorithmic consequences of our findings. We argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. We also discuss the performance of a simple local Walk-COL algorithm and of the belief propagation algorithm in the light of our results.",
        "published": "2007-04-10T16:42:15Z",
        "link": "http://arxiv.org/abs/0704.1269v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Settling the Complexity of Computing Two-Player Nash Equilibria",
        "authors": [
            "Xi Chen",
            "Xiaotie Deng",
            "Shang-Hua Teng"
        ],
        "summary": "We settle a long-standing open question in algorithmic game theory. We prove that Bimatrix, the problem of finding a Nash equilibrium in a two-player game, is complete for the complexity class PPAD Polynomial Parity Argument, Directed version) introduced by Papadimitriou in 1991.   This is the first of a series of results concerning the complexity of Nash equilibria. In particular, we prove the following theorems:   Bimatrix does not have a fully polynomial-time approximation scheme unless every problem in PPAD is solvable in polynomial time. The smoothed complexity of the classic Lemke-Howson algorithm and, in fact, of any algorithm for Bimatrix is not polynomial unless every problem in PPAD is solvable in randomized polynomial time. Our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. Our results also have two broad complexity implications in mathematical economics and operations research: Arrow-Debreu market equilibria are PPAD-hard to compute. The P-Matrix Linear Complementary Problem is computationally harder than convex programming unless every problem in PPAD is solvable in polynomial time.",
        "published": "2007-04-12T23:54:30Z",
        "link": "http://arxiv.org/abs/0704.1678v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "F.1.2; F.1.3; F.2; F.2.3"
        ]
    },
    {
        "title": "Locally Decodable Codes From Nice Subsets of Finite Fields and Prime   Factors of Mersenne Numbers",
        "authors": [
            "Kiran S. Kedlaya",
            "Sergey Yekhanin"
        ],
        "summary": "A k-query Locally Decodable Code (LDC) encodes an n-bit message x as an N-bit codeword C(x), such that one can probabilistically recover any bit x_i of the message by querying only k bits of the codeword C(x), even after some constant fraction of codeword bits has been corrupted. The major goal of LDC related research is to establish the optimal trade-off between length and query complexity of such codes.   Recently [Y] introduced a novel technique for constructing locally decodable codes and vastly improved the upper bounds for code length. The technique is based on Mersenne primes. In this paper we extend the work of [Y] and argue that further progress via these methods is tied to progress on an old number theory question regarding the size of the largest prime factors of Mersenne numbers.   Specifically, we show that every Mersenne number m=2^t-1 that has a prime factor p>m^\\gamma yields a family of k(\\gamma)-query locally decodable codes of length Exp(n^{1/t}). Conversely, if for some fixed k and all \\epsilon > 0 one can use the technique of [Y] to obtain a family of k-query LDCs of length Exp(n^\\epsilon); then infinitely many Mersenne numbers have prime factors arger than known currently.",
        "published": "2007-04-13T04:18:19Z",
        "link": "http://arxiv.org/abs/0704.1694v1",
        "categories": [
            "cs.CC",
            "math.NT"
        ]
    },
    {
        "title": "Bounded Pushdown dimension vs Lempel Ziv information density",
        "authors": [
            "Pilar Albert",
            "Elvira Mayordomo",
            "Philippe Moser"
        ],
        "summary": "In this paper we introduce a variant of pushdown dimension called bounded pushdown (BPD) dimension, that measures the density of information contained in a sequence, relative to a BPD automata, i.e. a finite state machine equipped with an extra infinite memory stack, with the additional requirement that every input symbol only allows a bounded number of stack movements. BPD automata are a natural real-time restriction of pushdown automata. We show that BPD dimension is a robust notion by giving an equivalent characterization of BPD dimension in terms of BPD compressors. We then study the relationships between BPD compression, and the standard Lempel-Ziv (LZ) compression algorithm, and show that in contrast to the finite-state compressor case, LZ is not universal for bounded pushdown compressors in a strong sense: we construct a sequence that LZ fails to compress signicantly, but that is compressed by at least a factor 2 by a BPD compressor. As a corollary we obtain a strong separation between finite-state and BPD dimension.",
        "published": "2007-04-18T16:49:48Z",
        "link": "http://arxiv.org/abs/0704.2386v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "The Complexity of Simple Stochastic Games",
        "authors": [
            "Jonas Dieckelmann"
        ],
        "summary": "In this paper we survey the computational time complexity of assorted simple stochastic game problems, and we give an overview of the best known algorithms associated with each problem.",
        "published": "2007-04-20T19:51:36Z",
        "link": "http://arxiv.org/abs/0704.2779v1",
        "categories": [
            "cs.CC",
            "cs.GT"
        ]
    },
    {
        "title": "Computing modular polynomials in quasi-linear time",
        "authors": [
            "Andreas Enge"
        ],
        "summary": "We analyse and compare the complexity of several algorithms for computing modular polynomials. We show that an algorithm relying on floating point evaluation of modular functions and on interpolation, which has received little attention in the literature, has a complexity that is essentially (up to logarithmic factors) linear in the size of the computed polynomials. In particular, it obtains the classical modular polynomials $\\Phi_\\ell$ of prime level $\\ell$ in time O (\\ell^3 \\log^4 \\ell \\log \\log \\ell). Besides treating modular polynomials for $\\Gamma^0 (\\ell)$, which are an important ingredient in many algorithms dealing with isogenies of elliptic curves, the algorithm is easily adapted to more general situations. Composite levels are handled just as easily as prime levels, as well as polynomials between a modular function and its transform of prime level, such as the Schl\\\"afli polynomials and their generalisations. Our distributed implementation of the algorithm confirms the theoretical analysis by computing modular equations of record level around 10000 in less than two weeks on ten processors.",
        "published": "2007-04-24T12:27:39Z",
        "link": "http://arxiv.org/abs/0704.3177v2",
        "categories": [
            "math.NT",
            "cs.CC"
        ]
    },
    {
        "title": "Polynomial algorithms for protein similarity search for restricted mRNA   structures",
        "authors": [
            "Frank Gurski"
        ],
        "summary": "In this paper we consider the problem of computing an mRNA sequence of maximal similarity for a given mRNA of secondary structure constraints, introduced by Backofen et al. in [BNS02] denoted as the MRSO problem. The problem is known to be NP-complete for planar associated implied structure graphs of vertex degree at most 3. In [BFHV05] a first polynomial dynamic programming algorithms for MRSO on implied structure graphs with maximum vertex degree 3 of bounded cut-width is shown. We give a simple but more general polynomial dynamic programming solution for the MRSO problem for associated implied structure graphs of bounded clique-width. Our result implies that MRSO is polynomial for graphs of bounded tree-width, co-graphs, $P_4$-sparse graphs, and distance hereditary graphs. Further we conclude that the problem of comparing two solutions for MRSO is hard for the class of problems which can be solved in polynomial time with a number of parallel queries to an oracle in NP.",
        "published": "2007-04-26T08:30:14Z",
        "link": "http://arxiv.org/abs/0704.3496v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "The Complexity of Weighted Boolean #CSP",
        "authors": [
            "Martin Dyer",
            "Leslie Ann Goldberg",
            "Mark Jerrum"
        ],
        "summary": "This paper gives a dichotomy theorem for the complexity of computing the partition function of an instance of a weighted Boolean constraint satisfaction problem. The problem is parameterised by a finite set F of non-negative functions that may be used to assign weights to the configurations (feasible solutions) of a problem instance. Classical constraint satisfaction problems correspond to the special case of 0,1-valued functions. We show that the partition function, i.e. the sum of the weights of all configurations, can be computed in polynomial time if either (1) every function in F is of ``product type'', or (2) every function in F is ``pure affine''. For every other fixed set F, computing the partition function is FP^{#P}-complete.",
        "published": "2007-04-27T13:19:32Z",
        "link": "http://arxiv.org/abs/0704.3683v2",
        "categories": [
            "cs.CC",
            "math.CO",
            "F.2.2; F.4.1; G.2.1"
        ]
    },
    {
        "title": "Minimizing Unsatisfaction in Colourful Neighbourhoods",
        "authors": [
            "K. Y. Michael Wong",
            "David Saad"
        ],
        "summary": "Colouring sparse graphs under various restrictions is a theoretical problem of significant practical relevance. Here we consider the problem of maximizing the number of different colours available at the nodes and their neighbourhoods, given a predetermined number of colours. In the analytical framework of a tree approximation, carried out at both zero and finite temperatures, solutions obtained by population dynamics give rise to estimates of the threshold connectivity for the incomplete to complete transition, which are consistent with those of existing algorithms. The nature of the transition as well as the validity of the tree approximation are investigated.",
        "published": "2007-04-29T10:03:00Z",
        "link": "http://arxiv.org/abs/0704.3835v3",
        "categories": [
            "cs.DS",
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "Iterative Rounding for the Closest String Problem",
        "authors": [
            "Jing-Chao Chen"
        ],
        "summary": "The closest string problem is an NP-hard problem, whose task is to find a string that minimizes maximum Hamming distance to a given set of strings. This can be reduced to an integer program (IP). However, to date, there exists no known polynomial-time algorithm for IP. In 2004, Meneses et al. introduced a branch-and-bound (B & B) method for solving the IP problem. Their algorithm is not always efficient and has the exponential time complexity. In the paper, we attempt to solve efficiently the IP problem by a greedy iterative rounding technique. The proposed algorithm is polynomial time and much faster than the existing B & B IP for the CSP. If the number of strings is limited to 3, the algorithm is provably at most 1 away from the optimum. The empirical results show that in many cases we can find an exact solution. Even though we fail to find an exact solution, the solution found is very close to exact solution.",
        "published": "2007-05-04T03:01:42Z",
        "link": "http://arxiv.org/abs/0705.0561v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Epsilon-Distortion Complexity for Cantor Sets",
        "authors": [
            "C. Bonanno",
            "J. -R. Chazottes",
            "P. Collet"
        ],
        "summary": "We define the epsilon-distortion complexity of a set as the shortest program, running on a universal Turing machine, which produces this set at the precision epsilon in the sense of Hausdorff distance. Then, we estimate the epsilon-distortion complexity of various central Cantor sets on the line generated by iterated function systems (IFS's). In particular, the epsilon-distortion complexity of a C^k Cantor set depends, in general, on k and on its box counting dimension, contrarily to Cantor sets generated by polynomial IFS or random affine Cantor sets.",
        "published": "2007-05-07T12:16:57Z",
        "link": "http://arxiv.org/abs/0705.0895v1",
        "categories": [
            "math.DS",
            "cs.CC",
            "math.MG"
        ]
    },
    {
        "title": "Satisfiability Parsimoniously Reduces to the Tantrix(TM) Rotation Puzzle   Problem",
        "authors": [
            "Dorothea Baumeister",
            "Joerg Rothe"
        ],
        "summary": "Holzer and Holzer (Discrete Applied Mathematics 144(3):345--358, 2004) proved that the Tantrix(TM) rotation puzzle problem is NP-complete. They also showed that for infinite rotation puzzles, this problem becomes undecidable. We study the counting version and the unique version of this problem. We prove that the satisfiability problem parsimoniously reduces to the Tantrix(TM) rotation puzzle problem. In particular, this reduction preserves the uniqueness of the solution, which implies that the unique Tantrix(TM) rotation puzzle problem is as hard as the unique satisfiability problem, and so is DP-complete under polynomial-time randomized reductions, where DP is the second level of the boolean hierarchy over NP.",
        "published": "2007-05-07T14:23:20Z",
        "link": "http://arxiv.org/abs/0705.0915v2",
        "categories": [
            "cs.CC",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "Improved Analysis of Kannan's Shortest Lattice Vector Algorithm",
        "authors": [
            "Guillaume Hanrot",
            "Damien Stehlé"
        ],
        "summary": "The security of lattice-based cryptosystems such as NTRU, GGH and Ajtai-Dwork essentially relies upon the intractability of computing a shortest non-zero lattice vector and a closest lattice vector to a given target vector in high dimensions. The best algorithms for these tasks are due to Kannan, and, though remarkably simple, their complexity estimates have not been improved since more than twenty years. Kannan's algorithm for solving the shortest vector problem is in particular crucial in Schnorr's celebrated block reduction algorithm, on which are based the best known attacks against the lattice-based encryption schemes mentioned above. Understanding precisely Kannan's algorithm is of prime importance for providing meaningful key-sizes. In this paper we improve the complexity analyses of Kannan's algorithms and discuss the possibility of improving the underlying enumeration strategy.",
        "published": "2007-05-07T18:44:05Z",
        "link": "http://arxiv.org/abs/0705.0965v2",
        "categories": [
            "cs.CR",
            "cs.CC"
        ]
    },
    {
        "title": "Does P=NP?",
        "authors": [
            "Karlen Garnik Gharibyan"
        ],
        "summary": "This paper has been withdrawn Abstract: This paper has been withdrawn by the author due to the publication.",
        "published": "2007-05-10T11:41:26Z",
        "link": "http://arxiv.org/abs/0705.1442v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On the freezing of variables in random constraint satisfaction problems",
        "authors": [
            "Guilhem Semerjian"
        ],
        "summary": "The set of solutions of random constraint satisfaction problems (zero energy groundstates of mean-field diluted spin glasses) undergoes several structural phase transitions as the amount of constraints is increased. This set first breaks down into a large number of well separated clusters. At the freezing transition, which is in general distinct from the clustering one, some variables (spins) take the same value in all solutions of a given cluster. In this paper we study the critical behavior around the freezing transition, which appears in the unfrozen phase as the divergence of the sizes of the rearrangements induced in response to the modification of a variable. The formalism is developed on generic constraint satisfaction problems and applied in particular to the random satisfiability of boolean formulas and to the coloring of random graphs. The computation is first performed in random tree ensembles, for which we underline a connection with percolation models and with the reconstruction problem of information theory. The validity of these results for the original random ensembles is then discussed in the framework of the cavity method.",
        "published": "2007-05-15T13:53:10Z",
        "link": "http://arxiv.org/abs/0705.2147v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC",
            "math.PR"
        ]
    },
    {
        "title": "Parallelized approximation algorithms for minimum routing cost spanning   trees",
        "authors": [
            "Ching-Lueh Chang",
            "Yuh-Dauh Lyuu"
        ],
        "summary": "We parallelize several previously proposed algorithms for the minimum routing cost spanning tree problem and some related problems.",
        "published": "2007-05-15T17:48:42Z",
        "link": "http://arxiv.org/abs/0705.2125v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "On tractability and congruence distributivity",
        "authors": [
            "Emil Kiss",
            "Matthew Valeriote"
        ],
        "summary": "Constraint languages that arise from finite algebras have recently been the object of study, especially in connection with the Dichotomy Conjecture of Feder and Vardi. An important class of algebras are those that generate congruence distributive varieties and included among this class are lattices, and more generally, those algebras that have near-unanimity term operations. An algebra will generate a congruence distributive variety if and only if it has a sequence of ternary term operations, called Jonsson terms, that satisfy certain equations.   We prove that constraint languages consisting of relations that are invariant under a short sequence of Jonsson terms are tractable by showing that such languages have bounded relational width.",
        "published": "2007-05-15T20:01:32Z",
        "link": "http://arxiv.org/abs/0705.2229v2",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3; F.4.1"
        ]
    },
    {
        "title": "Improved Approximability Result for Test Set with Small Redundancy",
        "authors": [
            "Peng Cui"
        ],
        "summary": "Test set with redundancy is one of the focuses in recent bioinformatics research. Set cover greedy algorithm (SGA for short) is a commonly used algorithm for test set with redundancy. This paper proves that the approximation ratio of SGA can be $(2-\\frac{1}{2r})\\ln n+{3/2}\\ln r+O(\\ln\\ln n)$ by using the potential function technique. This result is better than the approximation ratio $2\\ln n$ which directly derives from set multicover, when $r=o(\\frac{\\ln n}{\\ln\\ln n})$, and is an extension of the approximability results for plain test set.",
        "published": "2007-05-17T09:53:20Z",
        "link": "http://arxiv.org/abs/0705.2503v4",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Voronoi Diagram of Polygonal Chains under the Discrete Fréchet   Distance",
        "authors": [
            "Sergey Bereg",
            "Marina Gavrilova",
            "Binhai Zhu"
        ],
        "summary": "Polygonal chains are fundamental objects in many applications like pattern recognition and protein structure alignment. A well-known measure to characterize the similarity of two polygonal chains is the famous Fr\\`{e}chet distance. In this paper, for the first time, we consider the Voronoi diagram of polygonal chains in $d$-dimension ($d=2,3$) under the discrete Fr\\`{e}chet distance. Given $n$ polygonal chains ${\\cal C}$ in $d$-dimension ($d=2,3$), each with at most $k$ vertices, we prove fundamental properties of such a Voronoi diagram {\\em VD}$_F({\\cal C})$ by presenting the first known upper and lower bounds for {\\em VD}$_F({\\cal C})$.",
        "published": "2007-05-19T18:35:18Z",
        "link": "http://arxiv.org/abs/0705.2835v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Computability of simple games: A characterization and application to the   core",
        "authors": [
            "Masahiro Kumabe",
            "H. Reiju Mihara"
        ],
        "summary": "The class of algorithmically computable simple games (i) includes the class of games that have finite carriers and (ii) is included in the class of games that have finite winning coalitions. This paper characterizes computable games, strengthens the earlier result that computable games violate anonymity, and gives examples showing that the above inclusions are strict. It also extends Nakamura's theorem about the nonemptyness of the core and shows that computable games have a finite Nakamura number, implying that the number of alternatives that the players can deal with rationally is restricted.",
        "published": "2007-05-22T17:49:15Z",
        "link": "http://arxiv.org/abs/0705.3227v2",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.LO",
            "math.LO",
            "F.4.1; F.1.1"
        ]
    },
    {
        "title": "On the Obfuscation Complexity of Planar Graphs",
        "authors": [
            "Oleg Verbitsky"
        ],
        "summary": "Being motivated by John Tantalo's Planarity Game, we consider straight line plane drawings of a planar graph $G$ with edge crossings and wonder how obfuscated such drawings can be. We define $obf(G)$, the obfuscation complexity of $G$, to be the maximum number of edge crossings in a drawing of $G$. Relating $obf(G)$ to the distribution of vertex degrees in $G$, we show an efficient way of constructing a drawing of $G$ with at least $obf(G)/3$ edge crossings. We prove bounds $(\\delta(G)^2/24-o(1))n^2 < \\obf G <3 n^2$ for an $n$-vertex planar graph $G$ with minimum vertex degree $\\delta(G)\\ge 2$.   The shift complexity of $G$, denoted by $shift(G)$, is the minimum number of vertex shifts sufficient to eliminate all edge crossings in an arbitrarily obfuscated drawing of $G$ (after shifting a vertex, all incident edges are supposed to be redrawn correspondingly). If $\\delta(G)\\ge 3$, then $shift(G)$ is linear in the number of vertices due to the known fact that the matching number of $G$ is linear. However, in the case $\\delta(G)\\ge2$ we notice that $shift(G)$ can be linear even if the matching number is bounded. As for computational complexity, we show that, given a drawing $D$ of a planar graph, it is NP-hard to find an optimum sequence of shifts making $D$ crossing-free.",
        "published": "2007-05-25T11:19:03Z",
        "link": "http://arxiv.org/abs/0705.3748v3",
        "categories": [
            "cs.DM",
            "cs.CC"
        ]
    },
    {
        "title": "On the expressive power of planar perfect matching and permanents of   bounded treewidth matrices",
        "authors": [
            "Laurent Lyaudet",
            "Pascal Koiran",
            "Uffe Flarup"
        ],
        "summary": "Valiant introduced some 25 years ago an algebraic model of computation along with the complexity classes VP and VNP, which can be viewed as analogues of the classical classes P and NP. They are defined using non-uniform sequences of arithmetic circuits and provides a framework to study the complexity for sequences of polynomials. Prominent examples of difficult (that is, VNP-complete) problems in this model includes the permanent and hamiltonian polynomials. While the permanent and hamiltonian polynomials in general are difficult to evaluate, there have been research on which special cases of these polynomials admits efficient evaluation. For instance, Barvinok has shown that if the underlying matrix has bounded rank, both the permanent and the hamiltonian polynomials can be evaluated in polynomial time, and thus are in VP. Courcelle, Makowsky and Rotics have shown that for matrices of bounded treewidth several difficult problems (including evaluating the permanent and hamiltonian polynomials) can be solved efficiently. An earlier result of this flavour is Kasteleyn's theorem which states that the sum of weights of perfect matchings of a planar graph can be computed in polynomial time, and thus is in VP also. For general graphs this problem is VNP-complete. In this paper we investigate the expressive power of the above results. We show that the permanent and hamiltonian polynomials for matrices of bounded treewidth both are equivalent to arithmetic formulas. Also, arithmetic weakly skew circuits are shown to be equivalent to the sum of weights of perfect matchings of planar graphs.",
        "published": "2007-05-25T11:34:13Z",
        "link": "http://arxiv.org/abs/0705.3751v1",
        "categories": [
            "cs.DM",
            "cs.CC"
        ]
    },
    {
        "title": "The Battery-Discharge-Model: A Class of Stochastic Finite Automata to   Simulate Multidimensional Continued Fraction Expansion",
        "authors": [
            "Michael Vielhaber",
            "Monica del Pilar Canales"
        ],
        "summary": "We define an infinite stochastic state machine, the Battery-Discharge-Model (BDM), which simulates the behaviour of linear and jump complexity of the continued fraction expansion of multidimensional formal power series, a relevant security measure in the cryptanalysis of stream ciphers.   We also obtain finite approximations to the infinite BDM, where polynomially many states suffice to approximate with an exponentially small error the probabilities and averages for linear and jump complexity of M-multisequences of length n over the finite field F_q, for any M, n, q.",
        "published": "2007-05-29T02:50:42Z",
        "link": "http://arxiv.org/abs/0705.4134v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "cs.CR",
            "math.IT"
        ]
    },
    {
        "title": "The Asymptotic Normalized Linear Complexity of Multisequences",
        "authors": [
            "Michael Vielhaber",
            "Monica del Pilar Canales"
        ],
        "summary": "We show that the asymptotic linear complexity of a multisequence a in F_q^\\infty that is I := liminf L_a(n)/n and S := limsup L_a(n)/n satisfy the inequalities M/(M+1) <= S <= 1 and M(1-S) <= I <= 1-S/M, if all M sequences have nonzero discrepancy infinitely often, and all pairs (I,S) satisfying these conditions are met by 2^{\\aleph_0} multisequences a.   This answers an Open Problem by Dai, Imamura, and Yang.   Keywords: Linear complexity, multisequence, Battery Discharge Model, isometry.",
        "published": "2007-05-29T03:41:21Z",
        "link": "http://arxiv.org/abs/0705.4138v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "cs.CR",
            "math.IT"
        ]
    },
    {
        "title": "Two sources are better than one for increasing the Kolmogorov complexity   of infinite sequences",
        "authors": [
            "Marius Zimand"
        ],
        "summary": "The randomness rate of an infinite binary sequence is characterized by the sequence of ratios between the Kolmogorov complexity and the length of the initial segments of the sequence. It is known that there is no uniform effective procedure that transforms one input sequence into another sequence with higher randomness rate. By contrast, we display such a uniform effective procedure having as input two independent sequences with positive but arbitrarily small constant randomness rate. Moreover the transformation is a truth-table reduction and the output has randomness rate arbitrarily close to 1.",
        "published": "2007-05-31T17:38:04Z",
        "link": "http://arxiv.org/abs/0705.4658v2",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "Moving Vertices to Make Drawings Plane",
        "authors": [
            "Xavier Goaoc",
            "Jan Kratochvil",
            "Yoshio Okamoto",
            "Chan-Su Shin",
            "Alexander Wolff"
        ],
        "summary": "A straight-line drawing $\\delta$ of a planar graph $G$ need not be plane, but can be made so by moving some of the vertices. Let shift$(G,\\delta)$ denote the minimum number of vertices that need to be moved to turn $\\delta$ into a plane drawing of $G$. We show that shift$(G,\\delta)$ is NP-hard to compute and to approximate, and we give explicit bounds on shift$(G,\\delta)$ when $G$ is a tree or a general planar graph. Our hardness results extend to 1BendPointSetEmbeddability, a well-known graph-drawing problem.",
        "published": "2007-06-07T13:57:52Z",
        "link": "http://arxiv.org/abs/0706.1002v3",
        "categories": [
            "cs.CG",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "VPSPACE and a transfer theorem over the complex field",
        "authors": [
            "Pascal Koiran",
            "Sylvain Perifel"
        ],
        "summary": "We extend the transfer theorem of [KP2007] to the complex field. That is, we investigate the links between the class VPSPACE of families of polynomials and the Blum-Shub-Smale model of computation over C. Roughly speaking, a family of polynomials is in VPSPACE if its coefficients can be computed in polynomial space. Our main result is that if (uniform, constant-free) VPSPACE families can be evaluated efficiently then the class PAR of decision problems that can be solved in parallel polynomial time over the complex field collapses to P. As a result, one must first be able to show that there are VPSPACE families which are hard to evaluate in order to separate P from NP over C, or even from PAR.",
        "published": "2007-06-11T13:59:31Z",
        "link": "http://arxiv.org/abs/0706.1477v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Critique of Feinstein's Proof that P is not Equal to NP",
        "authors": [
            "Kyle Sabo",
            "Ryan Schmitt",
            "Michael Silverman"
        ],
        "summary": "We examine a proof by Craig Alan Feinstein that P is not equal to NP. We present counterexamples to claims made in his paper and expose a flaw in the methodology he uses to make his assertions. The fault in his argument is the incorrect use of reduction. Feinstein makes incorrect assumptions about the complexity of a problem based on the fact that there is a more complex problem that can be used to solve it. His paper introduces the terminology \"imaginary processor\" to describe how it is possible to beat the brute force reduction he offers to solve the Subset-Sum problem. The claims made in the paper would not be validly established even were imaginary processors to exist.",
        "published": "2007-06-14T13:15:39Z",
        "link": "http://arxiv.org/abs/0706.2035v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Dualheap Selection Algorithm: Efficient, Inherently Parallel and   Somewhat Mysterious",
        "authors": [
            "Greg Sepesi"
        ],
        "summary": "An inherently parallel algorithm is proposed that efficiently performs selection: finding the K-th largest member of a set of N members. Selection is a common component of many more complex algorithms and therefore is a widely studied problem.   Not much is new in the proposed dualheap selection algorithm: the heap data structure is from J.W.J.Williams, the bottom-up heap construction is from R.W. Floyd, and the concept of a two heap data structure is from J.W.J. Williams and D.E. Knuth. The algorithm's novelty is limited to a few relatively minor implementation twists: 1) the two heaps are oriented with their roots at the partition values rather than at the minimum and maximum values, 2)the coding of one of the heaps (the heap of smaller values) employs negative indexing, and 3) the exchange phase of the algorithm is similar to a bottom-up heap construction, but navigates the heap with a post-order tree traversal.   When run on a single processor, the dualheap selection algorithm's performance is competitive with quickselect with median estimation, a common variant of C.A.R. Hoare's quicksort algorithm. When run on parallel processors, the dualheap selection algorithm is superior due to its subtasks that are easily partitioned and innately balanced.",
        "published": "2007-06-14T16:11:24Z",
        "link": "http://arxiv.org/abs/0706.2155v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DC",
            "C.1.4; F.2.0; G.2.2"
        ]
    },
    {
        "title": "The Complexity of Determining Existence a Hamiltonian Cycle is $O(n^3)$",
        "authors": [
            "Guohun Zhu"
        ],
        "summary": "The Hamiltonian cycle problem in digraph is mapped into a matching cover bipartite graph. Based on this mapping, it is proved that determining existence a Hamiltonian cycle in graph is $O(n^3)$.",
        "published": "2007-06-19T07:57:51Z",
        "link": "http://arxiv.org/abs/0706.2725v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Abstract Storage Devices",
        "authors": [
            "Robert Koenig",
            "Ueli Maurer",
            "Stefano Tessaro"
        ],
        "summary": "A quantum storage device differs radically from a conventional physical storage device. Its state can be set to any value in a certain (infinite) state space, but in general every possible read operation yields only partial information about the stored state.   The purpose of this paper is to initiate the study of a combinatorial abstraction, called abstract storage device (ASD), which models deterministic storage devices with the property that only partial information about the state can be read, but that there is a degree of freedom as to which partial information should be retrieved.   This concept leads to a number of interesting problems which we address, like the reduction of one device to another device, the equivalence of devices, direct products of devices, as well as the factorization of a device into primitive devices. We prove that every ASD has an equivalent ASD with minimal number of states and of possible read operations. Also, we prove that the reducibility problem for ASD's is NP-complete, that the equivalence problem is at least as hard as the graph isomorphism problem, and that the factorization into binary-output devices (if it exists) is unique.",
        "published": "2007-06-19T17:14:24Z",
        "link": "http://arxiv.org/abs/0706.2746v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Dualheap Sort Algorithm: An Inherently Parallel Generalization of   Heapsort",
        "authors": [
            "Greg Sepesi"
        ],
        "summary": "A generalization of the heapsort algorithm is proposed. At the expense of about 50% more comparison and move operations for typical cases, the dualheap sort algorithm offers several advantages over heapsort: improved cache performance, better performance if the input happens to be already sorted, and easier parallel implementations.",
        "published": "2007-06-20T14:42:45Z",
        "link": "http://arxiv.org/abs/0706.2893v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DC"
        ]
    },
    {
        "title": "On Canonical Forms of Complete Problems via First-order Projections",
        "authors": [
            "Nerio Borges",
            "Blai Bonet"
        ],
        "summary": "The class of problems complete for NP via first-order reductions is known to be characterized by existential second-order sentences of a fixed form. All such sentences are built around the so-called generalized IS-form of the sentence that defines Independent-Set. This result can also be understood as that every sentence that defines a NP-complete problem P can be decomposed in two disjuncts such that the first one characterizes a fragment of P as hard as Independent-Set and the second the rest of P. That is, a decomposition that divides every such sentence into a quotient and residue modulo Independent-Set.   In this paper, we show that this result can be generalized over a wide collection of complexity classes, including the so-called nice classes. Moreover, we show that such decomposition can be done for any complete problem with respect to the given class, and that two such decompositions are non-equivalent in general. Interestingly, our results are based on simple and well-known properties of first-order reductions.ow that this result can be generalized over a wide collection of complexity classes, including the so-called nice classes. Moreover, we show that such decomposition can be done for any complete problem with respect to the given class, and that two such decompositions are non-equivalent in general. Interestingly, our results are based on simple and well-known properties of first-order reductions.",
        "published": "2007-06-22T21:27:06Z",
        "link": "http://arxiv.org/abs/0706.3412v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "NP by means of lifts and shadows",
        "authors": [
            "Gabor Kun",
            "Jaroslav Nesetril"
        ],
        "summary": "We show that every NP problem is polynomially equivalent to a simple combinatorial problem: the membership problem for a special class of digraphs. These classes are defined by means of shadows (projections) and by finitely many forbidden colored (lifted) subgraphs. Our characterization is motivated by the analysis of syntactical subclasses with the full computational power of NP, which were first studied by Feder and Vardi.   Our approach applies to many combinatorial problems and it induces the characterization of coloring problems (CSP) defined by means of shadows. This turns out to be related to homomorphism dualities. We prove that a class of digraphs (relational structures) defined by finitely many forbidden colored subgraphs (i.e. lifted substructures) is a CSP class if and only if all the the forbidden structures are homomorphically equivalent to trees. We show a surprising richness of coloring problems when restricted to most frequent graph classes. Using results of Ne\\v{s}et\\v{r}il and Ossona de Mendez for bounded expansion classes (which include bounded degree and proper minor closed classes) we prove that the restriction of every class defined as the shadow of finitely many colored subgraphs equals to the restriction of a coloring (CSP) class.",
        "published": "2007-06-23T15:40:43Z",
        "link": "http://arxiv.org/abs/0706.3459v1",
        "categories": [
            "cs.CC",
            "math.CO",
            "05C15; 68R05"
        ]
    },
    {
        "title": "There Exist some Omega-Powers of Any Borel Rank",
        "authors": [
            "Dominique Lecomte",
            "Olivier Finkel"
        ],
        "summary": "Omega-powers of finitary languages are languages of infinite words (omega-languages) in the form V^omega, where V is a finitary language over a finite alphabet X. They appear very naturally in the characterizaton of regular or context-free omega-languages. Since the set of infinite words over a finite alphabet X can be equipped with the usual Cantor topology, the question of the topological complexity of omega-powers of finitary languages naturally arises and has been posed by Niwinski (1990), Simonnet (1992) and Staiger (1997). It has been recently proved that for each integer n > 0, there exist some omega-powers of context free languages which are Pi^0_n-complete Borel sets, that there exists a context free language L such that L^omega is analytic but not Borel, and that there exists a finitary language V such that V^omega is a Borel set of infinite rank. But it was still unknown which could be the possible infinite Borel ranks of omega-powers. We fill this gap here, proving the following very surprising result which shows that omega-powers exhibit a great topological complexity: for each non-null countable ordinal alpha, there exist some Sigma^0_alpha-complete omega-powers, and some Pi^0_alpha-complete omega-powers.",
        "published": "2007-06-25T16:03:36Z",
        "link": "http://arxiv.org/abs/0706.3523v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "PSPACE Bounds for Rank-1 Modal Logics",
        "authors": [
            "Lutz Schröder",
            "Dirk Pattinson"
        ],
        "summary": "For lack of general algorithmic methods that apply to wide classes of logics, establishing a complexity bound for a given modal logic is often a laborious task. The present work is a step towards a general theory of the complexity of modal logics. Our main result is that all rank-1 logics enjoy a shallow model property and thus are, under mild assumptions on the format of their axiomatisation, in PSPACE. This leads to a unified derivation of tight PSPACE-bounds for a number of logics including K, KD, coalition logic, graded modal logic, majority logic, and probabilistic modal logic. Our generic algorithm moreover finds tableau proofs that witness pleasant proof-theoretic properties including a weak subformula property. This generality is made possible by a coalgebraic semantics, which conveniently abstracts from the details of a given model class and thus allows covering a broad range of logics in a uniform way.",
        "published": "2007-06-27T15:15:57Z",
        "link": "http://arxiv.org/abs/0706.4044v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "Some Quantitative Aspects of Fractional Computability",
        "authors": [
            "Ilya Kapovich",
            "Paul Schupp"
        ],
        "summary": "Motivated by results on generic-case complexity in group theory, we apply the ideas of effective Baire category and effective measure theory to study complexity classes of functions which are \"fractionally computable\" by a partial algorithm. For this purpose it is crucial to specify an allowable effective density, $\\delta$, of convergence for a partial algorithm. The set $\\mathcal{FC}(\\delta)$ consists of all total functions $ f: \\Sigma^\\ast \\to \\{0,1 \\}$ where $\\Sigma$ is a finite alphabet with $|\\Sigma| \\ge 2$ which are \"fractionally computable at density $\\delta$\". The space $\\mathcal{FC}(\\delta) $ is effectively of the second category while any fractional complexity class, defined using $\\delta$ and any computable bound $\\beta$ with respect to an abstract Blum complexity measure, is effectively meager. A remarkable result of Kautz and Miltersen shows that relative to an algorithmically random oracle $A$, the relativized class $\\mathcal{NP}^A$ does not have effective polynomial measure zero in $\\mathcal{E}^A$, the relativization of strict exponential time. We define the class $\\mathcal{UFP}^A$ of all languages which are fractionally decidable in polynomial time at ``a uniform rate'' by algorithms with an oracle for $A$. We show that this class does have effective polynomial measure zero in $\\mathcal{E}^A$ for every oracle $A$. Thus relaxing the requirement of polynomial time decidability to hold only for a fraction of possible inputs does not compensate for the power of nondeterminism in the case of random oracles.",
        "published": "2007-06-27T20:26:19Z",
        "link": "http://arxiv.org/abs/0706.4095v1",
        "categories": [
            "math.GR",
            "cs.CC",
            "Primary 68Q, Secondary 20P05"
        ]
    },
    {
        "title": "Directed Feedback Vertex Set is Fixed-Parameter Tractable",
        "authors": [
            "Igor Razgon",
            "Barry O'Sullivan"
        ],
        "summary": "We resolve positively a long standing open question regarding the fixed-parameter tractability of the parameterized Directed Feedback Vertex Set problem. In particular, we propose an algorithm which solves this problem in $O(8^kk!*poly(n))$.",
        "published": "2007-07-02T17:56:53Z",
        "link": "http://arxiv.org/abs/0707.0282v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "The $k$-anonymity Problem is Hard",
        "authors": [
            "Paola Bonizzoni",
            "Gianluca Della Vedova",
            "Riccardo Dondi"
        ],
        "summary": "The problem of publishing personal data without giving up privacy is becoming increasingly important. An interesting formalization recently proposed is the k-anonymity. This approach requires that the rows in a table are clustered in sets of size at least k and that all the rows in a cluster become the same tuple, after the suppression of some records. The natural optimization problem, where the goal is to minimize the number of suppressed entries, is known to be NP-hard when the values are over a ternary alphabet, k = 3 and the rows length is unbounded. In this paper we give a lower bound on the approximation factor that any polynomial-time algorithm can achive on two restrictions of the problem,namely (i) when the records values are over a binary alphabet and k = 3, and (ii) when the records have length at most 8 and k = 4, showing that these restrictions of the problem are APX-hard.",
        "published": "2007-07-03T14:17:49Z",
        "link": "http://arxiv.org/abs/0707.0421v2",
        "categories": [
            "cs.DB",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Assisted Problem Solving and Decompositions of Finite Automata",
        "authors": [
            "Peter Gaži",
            "Branislav Rovan"
        ],
        "summary": "A study of assisted problem solving formalized via decompositions of deterministic finite automata is initiated. The landscape of new types of decompositions of finite automata this study uncovered is presented. Languages with various degrees of decomposability between undecomposable and perfectly decomposable are shown to exist.",
        "published": "2007-07-03T14:54:53Z",
        "link": "http://arxiv.org/abs/0707.0430v1",
        "categories": [
            "cs.CC",
            "F.1.1; F.2.0"
        ]
    },
    {
        "title": "The Nash Equilibrium Revisited: Chaos and Complexity Hidden in   Simplicity",
        "authors": [
            "Philip V. Fellman"
        ],
        "summary": "The Nash Equilibrium is a much discussed, deceptively complex, method for the analysis of non-cooperative games. If one reads many of the commonly available definitions the description of the Nash Equilibrium is deceptively simple in appearance. Modern research has discovered a number of new and important complex properties of the Nash Equilibrium, some of which remain as contemporary conundrums of extraordinary difficulty and complexity. Among the recently discovered features which the Nash Equilibrium exhibits under various conditions are heteroclinic Hamiltonian dynamics, a very complex asymptotic structure in the context of two-player bi-matrix games and a number of computationally complex or computationally intractable features in other settings. This paper reviews those findings and then suggests how they may inform various market prediction strategies.",
        "published": "2007-07-06T00:55:01Z",
        "link": "http://arxiv.org/abs/0707.0891v1",
        "categories": [
            "cs.GT",
            "cs.CC"
        ]
    },
    {
        "title": "Expressing an NP-Complete Problem as the Solvability of a Polynomial   Equation",
        "authors": [
            "Deepak Chermakani"
        ],
        "summary": "We demonstrate a polynomial approach to express the decision version of the directed Hamiltonian Cycle Problem (HCP), which is NP-Complete, as the Solvability of a Polynomial Equation with a constant number of variables, within a bounded real space. We first introduce four new Theorems for a set of periodic Functions with irrational periods, based on which we then use a trigonometric substitution, to show how the HCP can be expressed as the Solvability of a single polynomial Equation with a constant number of variables. The feasible solution of each of these variables is bounded within two real numbers. We point out what future work is necessary to prove that P=NP.",
        "published": "2007-07-09T04:49:47Z",
        "link": "http://arxiv.org/abs/0707.1176v2",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Report on Generic Case Complexity",
        "authors": [
            "Robert Gilman",
            "Alexei G. Miasnikov",
            "Alexey D. Myasnikov",
            "Alexander Ushakov"
        ],
        "summary": "This article is a short introduction to generic case complexity, which is a recently developed way of measuring the difficulty of a computational problem while ignoring atypical behavior on a small set of inputs. Generic case complexity applies to both recursively solvable and recursively unsolvable problems.",
        "published": "2007-07-10T18:57:08Z",
        "link": "http://arxiv.org/abs/0707.1364v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "A Characterisation of First-Order Constraint Satisfaction Problems",
        "authors": [
            "Benoit Larose",
            "Cynthia Loten",
            "Claude Tardif"
        ],
        "summary": "We describe simple algebraic and combinatorial characterisations of finite relational core structures admitting finitely many obstructions. As a consequence, we show that it is decidable to determine whether a constraint satisfaction problem is first-order definable: we show the general problem to be NP-complete, and give a polynomial-time algorithm in the case of cores. A slight modification of this algorithm provides, for first-order definable CSP's, a simple poly-time algorithm to produce a solution when one exists. As an application of our algebraic characterisation of first order CSP's, we describe a large family of L-complete CSP's.",
        "published": "2007-07-17T16:23:45Z",
        "link": "http://arxiv.org/abs/0707.2562v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Faster subsequence recognition in compressed strings",
        "authors": [
            "Alexander Tiskin"
        ],
        "summary": "Computation on compressed strings is one of the key approaches to processing massive data sets. We consider local subsequence recognition problems on strings compressed by straight-line programs (SLP), which is closely related to Lempel--Ziv compression. For an SLP-compressed text of length $\\bar m$, and an uncompressed pattern of length $n$, C{\\'e}gielski et al. gave an algorithm for local subsequence recognition running in time $O(\\bar mn^2 \\log n)$. We improve the running time to $O(\\bar mn^{1.5})$. Our algorithm can also be used to compute the longest common subsequence between a compressed text and an uncompressed pattern in time $O(\\bar mn^{1.5})$; the same problem with a compressed pattern is known to be NP-hard.",
        "published": "2007-07-23T16:26:24Z",
        "link": "http://arxiv.org/abs/0707.3407v4",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Faster exon assembly by sparse spliced alignment",
        "authors": [
            "Alexander Tiskin"
        ],
        "summary": "Assembling a gene from candidate exons is an important problem in computational biology. Among the most successful approaches to this problem is \\emph{spliced alignment}, proposed by Gelfand et al., which scores different candidate exon chains within a DNA sequence of length $m$ by comparing them to a known related gene sequence of length n, $m = \\Theta(n)$. Gelfand et al.\\ gave an algorithm for spliced alignment running in time O(n^3). Kent et al.\\ considered sparse spliced alignment, where the number of candidate exons is O(n), and proposed an algorithm for this problem running in time O(n^{2.5}). We improve on this result, by proposing an algorithm for sparse spliced alignment running in time O(n^{2.25}). Our approach is based on a new framework of \\emph{quasi-local string comparison}.",
        "published": "2007-07-23T16:35:54Z",
        "link": "http://arxiv.org/abs/0707.3409v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.CE",
            "q-bio.QM"
        ]
    },
    {
        "title": "Complexity of Propositional Proofs under a Promise",
        "authors": [
            "Nachum Dershowitz",
            "Iddo Tzameret"
        ],
        "summary": "We study -- within the framework of propositional proof complexity -- the problem of certifying unsatisfiability of CNF formulas under the promise that any satisfiable formula has many satisfying assignments, where ``many'' stands for an explicitly specified function $\\Lam$ in the number of variables $n$. To this end, we develop propositional proof systems under different measures of promises (that is, different $\\Lam$) as extensions of resolution. This is done by augmenting resolution with axioms that, roughly, can eliminate sets of truth assignments defined by Boolean circuits. We then investigate the complexity of such systems, obtaining an exponential separation in the average-case between resolution under different size promises:   1. Resolution has polynomial-size refutations for all unsatisfiable 3CNF formulas when the promise is $\\eps\\cd2^n$, for any constant $0<\\eps<1$.   2. There are no sub-exponential size resolution refutations for random 3CNF formulas, when the promise is $2^{\\delta n}$ (and the number of clauses is $o(n^{3/2})$), for any constant $0<\\delta<1$.",
        "published": "2007-07-28T18:36:01Z",
        "link": "http://arxiv.org/abs/0707.4255v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.2.2; F.4.1"
        ]
    },
    {
        "title": "On sparse representations of linear operators and the approximation of   matrix products",
        "authors": [
            "Mohamed-Ali Belabbas",
            "Patrick J. Wolfe"
        ],
        "summary": "Thus far, sparse representations have been exploited largely in the context of robustly estimating functions in a noisy environment from a few measurements. In this context, the existence of a basis in which the signal class under consideration is sparse is used to decrease the number of necessary measurements while controlling the approximation error. In this paper, we instead focus on applications in numerical analysis, by way of sparse representations of linear operators with the objective of minimizing the number of operations needed to perform basic operations (here, multiplication) on these operators. We represent a linear operator by a sum of rank-one operators, and show how a sparse representation that guarantees a low approximation error for the product can be obtained from analyzing an induced quadratic form. This construction in turn yields new algorithms for computing approximate matrix products.",
        "published": "2007-07-30T17:20:23Z",
        "link": "http://arxiv.org/abs/0707.4448v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Small weakly universal Turing machines",
        "authors": [
            "Turlough Neary",
            "Damien Woods"
        ],
        "summary": "We give small universal Turing machines with state-symbol pairs of (6, 2), (3, 3) and (2, 4). These machines are weakly universal, which means that they have an infinitely repeated word to the left of their input and another to the right. They simulate Rule 110 and are currently the smallest known weakly universal Turing machines.",
        "published": "2007-07-30T21:40:55Z",
        "link": "http://arxiv.org/abs/0707.4489v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On the Complexity of the Interlace Polynomial",
        "authors": [
            "Markus Bläser",
            "Christian Hoffmann"
        ],
        "summary": "We consider the two-variable interlace polynomial introduced by Arratia, Bollobas and Sorkin (2004). We develop graph transformations which allow us to derive point-to-point reductions for the interlace polynomial. Exploiting these reductions we obtain new results concerning the computational complexity of evaluating the interlace polynomial at a fixed point. Regarding exact evaluation, we prove that the interlace polynomial is #P-hard to evaluate at every point of the plane, except on one line, where it is trivially polynomial time computable, and four lines, where the complexity is still open. This solves a problem posed by Arratia, Bollobas and Sorkin (2004). In particular, three specializations of the two-variable interlace polynomial, the vertex-nullity interlace polynomial, the vertex-rank interlace polynomial and the independent set polynomial, are almost everywhere #P-hard to evaluate, too. For the independent set polynomial, our reductions allow us to prove that it is even hard to approximate at any point except at 0.",
        "published": "2007-07-31T12:23:13Z",
        "link": "http://arxiv.org/abs/0707.4565v3",
        "categories": [
            "cs.CC",
            "math.CO",
            "F.2.1; F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Nonlinear Matroid Optimization and Experimental Design",
        "authors": [
            "Yael Berstein",
            "Jon Lee",
            "Hugo Maruri-Aguilar",
            "Shmuel Onn",
            "Eva Riccomagno",
            "Robert Weismantel",
            "Henry Wynn"
        ],
        "summary": "We study the problem of optimizing nonlinear objective functions over matroids presented by oracles or explicitly. Such functions can be interpreted as the balancing of multi-criteria optimization. We provide a combinatorial polynomial time algorithm for arbitrary oracle-presented matroids, that makes repeated use of matroid intersection, and an algebraic algorithm for vectorial matroids.   Our work is partly motivated by applications to minimum-aberration model-fitting in experimental design in statistics, which we discuss and demonstrate in detail.",
        "published": "2007-07-31T13:54:07Z",
        "link": "http://arxiv.org/abs/0707.4618v1",
        "categories": [
            "math.CO",
            "cs.CC",
            "cs.DM",
            "math.OC",
            "90C11, 52B55, 90B06, 68R05, 15A39, 62H17"
        ]
    },
    {
        "title": "Physical limits of inference",
        "authors": [
            "David H. Wolpert"
        ],
        "summary": "I show that physical devices that perform observation, prediction, or recollection share an underlying mathematical structure. I call devices with that structure \"inference devices\". I present a set of existence and impossibility results concerning inference devices. These results hold independent of the precise physical laws governing our universe. In a limited sense, the impossibility results establish that Laplace was wrong to claim that even in a classical, non-chaotic universe the future can be unerringly predicted, given sufficient knowledge of the present. Alternatively, these impossibility results can be viewed as a non-quantum mechanical \"uncertainty principle\". Next I explore the close connections between the mathematics of inference devices and of Turing Machines. In particular, the impossibility results for inference devices are similar to the Halting theorem for TM's. Furthermore, one can define an analog of Universal TM's (UTM's) for inference devices. I call those analogs \"strong inference devices\". I use strong inference devices to define the \"inference complexity\" of an inference task, which is the analog of the Kolmogorov complexity of computing a string. However no universe can contain more than one strong inference device. So whereas the Kolmogorov complexity of a string is arbitrary up to specification of the UTM, there is no such arbitrariness in the inference complexity of an inference task. I end by discussing the philosophical implications of these results, e.g., for whether the universe \"is\" a computer.",
        "published": "2007-08-10T03:19:38Z",
        "link": "http://arxiv.org/abs/0708.1362v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CC",
            "cs.IT",
            "gr-qc",
            "math.IT"
        ]
    },
    {
        "title": "Resolution over Linear Equations and Multilinear Proofs",
        "authors": [
            "Ran Raz",
            "Iddo Tzameret"
        ],
        "summary": "We develop and study the complexity of propositional proof systems of varying strength extending resolution by allowing it to operate with disjunctions of linear equations instead of clauses. We demonstrate polynomial-size refutations for hard tautologies like the pigeonhole principle, Tseitin graph tautologies and the clique-coloring tautologies in these proof systems. Using the (monotone) interpolation by a communication game technique we establish an exponential-size lower bound on refutations in a certain, considerably strong, fragment of resolution over linear equations, as well as a general polynomial upper bound on (non-monotone) interpolants in this fragment.   We then apply these results to extend and improve previous results on multilinear proofs (over fields of characteristic 0), as studied in [RazTzameret06]. Specifically, we show the following:   1. Proofs operating with depth-3 multilinear formulas polynomially simulate a certain, considerably strong, fragment of resolution over linear equations.   2. Proofs operating with depth-3 multilinear formulas admit polynomial-size refutations of the pigeonhole principle and Tseitin graph tautologies. The former improve over a previous result that established small multilinear proofs only for the \\emph{functional} pigeonhole principle. The latter are different than previous proofs, and apply to multilinear proofs of Tseitin mod p graph tautologies over any field of characteristic 0.   We conclude by connecting resolution over linear equations with extensions of the cutting planes proof system.",
        "published": "2007-08-10T23:23:10Z",
        "link": "http://arxiv.org/abs/0708.1529v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.2.2; F.4.1"
        ]
    },
    {
        "title": "Hybrid Branching-Time Logics",
        "authors": [
            "Volker Weber"
        ],
        "summary": "Hybrid branching-time logics are introduced as extensions of CTL-like logics with state variables and the downarrow-binder. Following recent work in the linear framework, only logics with a single variable are considered. The expressive power and the complexity of satisfiability of the resulting logics is investigated.   As main result, the satisfiability problem for the hybrid versions of several branching-time logics is proved to be 2EXPTIME-complete. These branching-time logics range from strict fragments of CTL to extensions of CTL that can talk about the past and express fairness-properties. The complexity gap relative to CTL is explained by a corresponding succinctness result.   To prove the upper bound, the automata-theoretic approach to branching-time logics is extended to hybrid logics, showing that non-emptiness of alternating one-pebble Buchi tree automata is 2EXPTIME-complete.",
        "published": "2007-08-13T15:04:12Z",
        "link": "http://arxiv.org/abs/0708.1723v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Lower Bounds for the Complexity of the Voronoi Diagram of Polygonal   Curves under the Discrete Frechet Distance",
        "authors": [
            "Kevin Buchin",
            "Maike Buchin"
        ],
        "summary": "We give lower bounds for the combinatorial complexity of the Voronoi diagram of polygonal curves under the discrete Frechet distance. We show that the Voronoi diagram of n curves in R^d with k vertices each, has complexity Omega(n^{dk}) for dimension d=1,2 and Omega(n^{d(k-1)+2}) for d>2.",
        "published": "2007-08-14T14:53:44Z",
        "link": "http://arxiv.org/abs/0708.1909v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Attribute Estimation and Testing Quasi-Symmetry",
        "authors": [
            "Krzysztof Majewski",
            "Nicholas Pippenger"
        ],
        "summary": "A Boolean function is symmetric if it is invariant under all permutations of its arguments; it is quasi-symmetric if it is symmetric with respect to the arguments on which it actually depends. We present a test that accepts every quasi-symmetric function and, except with an error probability at most delta>0, rejects every function that differs from every quasi-symmetric function on at least a fraction epsilon>0 of the inputs. For a function of n arguments, the test probes the function at O((n/epsilon)\\log(n/delta)) inputs. Our quasi-symmetry test acquires information concerning the arguments on which the function actually depends. To do this, it employs a generalization of the property testing paradigm that we call attribute estimation. Like property testing, attribute estimation uses random sampling to obtain results that have only \"one-sided'' errors and that are close to accurate with high probability.",
        "published": "2007-08-15T20:56:22Z",
        "link": "http://arxiv.org/abs/0708.2105v1",
        "categories": [
            "cs.CC",
            "F.1.2; F.2.2"
        ]
    },
    {
        "title": "Unsatisfiable Linear k-CNFs Exist, for every k",
        "authors": [
            "Dominik Scheder"
        ],
        "summary": "We call a CNF formula linear if any two clauses have at most one variable in common. Let Linear k-SAT be the problem of deciding whether a given linear k-CNF formula is satisfiable. Here, a k-CNF formula is a CNF formula in which every clause has size exactly k. It was known that for k >= 3, Linear k-SAT is NP-complete if and only if an unsatisfiable linear k-CNF formula exists, and that they do exist for k >= 4. We prove that unsatisfiable linear k-CNF formulas exist for every k. Let f(k) be the minimum number of clauses in an unsatisfiable linear k-CNF formula. We show that f(k) is Omega(k2^k) and O(4^k*k^4), i.e., minimum size unsatisfiable linear k-CNF formulas are significantly larger than minimum size unsatisfiable k-CNF formulas. Finally, we prove that, surprisingly, linear k-CNF formulas do not allow for a larger fraction of clauses to be satisfied than general k-CNF formulas.",
        "published": "2007-08-17T09:44:21Z",
        "link": "http://arxiv.org/abs/0708.2336v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Minimum Cost Homomorphisms to Reflexive Digraphs",
        "authors": [
            "Arvind Gupta",
            "Pavol Hell",
            "Mehdi Karimi",
            "Arash Rafiey"
        ],
        "summary": "For digraphs $G$ and $H$, a homomorphism of $G$ to $H$ is a mapping $f:\\ V(G)\\dom V(H)$ such that $uv\\in A(G)$ implies $f(u)f(v)\\in A(H)$. If moreover each vertex $u \\in V(G)$ is associated with costs $c_i(u), i \\in V(H)$, then the cost of a homomorphism $f$ is $\\sum_{u\\in V(G)}c_{f(u)}(u)$. For each fixed digraph $H$, the {\\em minimum cost homomorphism problem} for $H$, denoted MinHOM($H$), is the following problem. Given an input digraph $G$, together with costs $c_i(u)$, $u\\in V(G)$, $i\\in V(H)$, and an integer $k$, decide if $G$ admits a homomorphism to $H$ of cost not exceeding $k$. We focus on the minimum cost homomorphism problem for {\\em reflexive} digraphs $H$ (every vertex of $H$ has a loop). It is known that the problem MinHOM($H$) is polynomial time solvable if the digraph $H$ has a {\\em Min-Max ordering}, i.e., if its vertices can be linearly ordered by $<$ so that $i<j, s<r$ and $ir, js \\in A(H)$ imply that $is \\in A(H)$ and $jr \\in A(H)$. We give a forbidden induced subgraph characterization of reflexive digraphs with a Min-Max ordering; our characterization implies a polynomial time test for the existence of a Min-Max ordering. Using this characterization, we show that for a reflexive digraph $H$ which does not admit a Min-Max ordering, the minimum cost homomorphism problem is NP-complete. Thus we obtain a full dichotomy classification of the complexity of minimum cost homomorphism problems for reflexive digraphs.",
        "published": "2007-08-18T23:34:45Z",
        "link": "http://arxiv.org/abs/0708.2514v2",
        "categories": [
            "cs.DM",
            "cs.CC"
        ]
    },
    {
        "title": "Claw Finding Algorithms Using Quantum Walk",
        "authors": [
            "Seiichiro Tani"
        ],
        "summary": "The claw finding problem has been studied in terms of query complexity as one of the problems closely connected to cryptography. For given two functions, f and g, as an oracle which have domains of size N and M (N<=M), respectively, and the same range, the goal of the problem is to find x and y such that f(x)=g(y). This paper describes an optimal algorithm using quantum walk that solves this problem. Our algorithm can be generalized to find a claw of k functions for any constant integer k>1, where the domains of the functions may have different size.",
        "published": "2007-08-20T07:56:12Z",
        "link": "http://arxiv.org/abs/0708.2584v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Algorithmic Semi-algebraic Geometry and Topology -- Recent Progress and   Open Problems",
        "authors": [
            "Saugata Basu"
        ],
        "summary": "We give a survey of algorithms for computing topological invariants of semi-algebraic sets with special emphasis on the more recent developments in designing algorithms for computing the Betti numbers of semi-algebraic sets. Aside from describing these results, we discuss briefly the background as well as the importance of these problems, and also describe the main tools from algorithmic semi-algebraic geometry, as well as algebraic topology, which make these advances possible. We end with a list of open problems.",
        "published": "2007-08-21T14:55:56Z",
        "link": "http://arxiv.org/abs/0708.2854v2",
        "categories": [
            "math.GT",
            "cs.CC",
            "cs.CG",
            "math.AG",
            "math.AT",
            "14P10, 14P25 (Primary) 68W30 (Secondary)"
        ]
    },
    {
        "title": "Explicit formulas for efficient multiplication in F_{3^{6m}}",
        "authors": [
            "Elisa Gorla",
            "Christoph Puttmann",
            "Jamshid Shokrollahi"
        ],
        "summary": "Efficient computation of the Tate pairing is an important part of pairing-based cryptography. Recently with the introduction of the Duursma-Lee method special attention has been given to the fields of characteristic 3. Especially multiplication in F_{3^{6m}}, where m is prime, is an important operation in the above method. In this paper we propose a new method to reduce the number of F_{3^m} multiplications for multiplication in F_{3^{6m}} from 18 in recent implementations to 15. The method is based on the fast Fourier tranmsform and explicit formulas are given. The execution times of our software implementations for F_{3^{6m}} show the efficiency of our results.",
        "published": "2007-08-22T13:52:09Z",
        "link": "http://arxiv.org/abs/0708.3014v1",
        "categories": [
            "cs.CR",
            "cs.CC"
        ]
    },
    {
        "title": "A Dichotomy Theorem for General Minimum Cost Homomorphism Problem",
        "authors": [
            "Rustem Takhanov"
        ],
        "summary": "In the constraint satisfaction problem ($CSP$), the aim is to find an assignment of values to a set of variables subject to specified constraints. In the minimum cost homomorphism problem ($MinHom$), one is additionally given weights $c_{va}$ for every variable $v$ and value $a$, and the aim is to find an assignment $f$ to the variables that minimizes $\\sum_{v} c_{vf(v)}$. Let $MinHom(\\Gamma)$ denote the $MinHom$ problem parameterized by the set of predicates allowed for constraints. $MinHom(\\Gamma)$ is related to many well-studied combinatorial optimization problems, and concrete applications can be found in, for instance, defence logistics and machine learning. We show that $MinHom(\\Gamma)$ can be studied by using algebraic methods similar to those used for CSPs. With the aid of algebraic techniques, we classify the computational complexity of $MinHom(\\Gamma)$ for all choices of $\\Gamma$. Our result settles a general dichotomy conjecture previously resolved only for certain classes of directed graphs, [Gutin, Hell, Rafiey, Yeo, European J. of Combinatorics, 2008].",
        "published": "2007-08-23T18:26:21Z",
        "link": "http://arxiv.org/abs/0708.3226v7",
        "categories": [
            "cs.LG",
            "cs.CC",
            "F.4.1; G.2.2; I.2.6"
        ]
    },
    {
        "title": "A Polynomial-time Algorithm for Computing the Permanent in GF(3^q)",
        "authors": [
            "Vadim Tarin"
        ],
        "summary": "A polynomial-time algorithm for computing the permanent in any field of characteristic 3 is presented in this article. The principal objects utilized for that purpose are the Cauchy and Vandermonde matrices, the discriminant function and their generalizations of various types. Classical theorems on the permanent such as the Binet-Minc identity and Borchadt's formula are widely applied, while a special new technique involving the notion of limit re-defined for fields of finite characteristics and corresponding computational methods was developed in order to deal with a number of polynomial-time reductions. All the constructions preserve a strictly algebraic nature ignoring the structure of the basic field, while applying its infinite extensions for calculating limits.   A natural corollary of the polynomial-time computability of the permanent in a field of a characteristic different from 2 is the non-uniform equality between the complexity classes P and NP what is equivalent to RP=NP (Ref. [1]).",
        "published": "2007-08-27T15:47:49Z",
        "link": "http://arxiv.org/abs/0708.3568v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Graph Isomorphism is PSPACE-complete",
        "authors": [
            "Matthew Delacorte"
        ],
        "summary": "Combining the the results of A.R. Meyer and L.J. Stockmeyer \"The Equivalence Problem for Regular Expressions with Squaring Requires Exponential Space\", and K.S. Booth \"Isomorphism testing for graphs, semigroups, and finite automata are polynomiamlly equivalent problems\" shows that graph isomorphism is PSPACE-complete.",
        "published": "2007-08-30T05:06:39Z",
        "link": "http://arxiv.org/abs/0708.4075v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Raising a Hardness Result",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "This article presents a technique for proving problems hard for classes of the polynomial hierarchy or for PSPACE. The rationale of this technique is that some problem restrictions are able to simulate existential or universal quantifiers. If this is the case, reductions from Quantified Boolean Formulae (QBF) to these restrictions can be transformed into reductions from QBFs having one more quantifier in the front. This means that a proof of hardness of a problem at level n in the polynomial hierarchy can be split into n separate proofs, which may be simpler than a proof directly showing a reduction from a class of QBFs to the considered problem.",
        "published": "2007-08-30T14:42:50Z",
        "link": "http://arxiv.org/abs/0708.4170v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Classical and Effective Descriptive Complexities of omega-Powers",
        "authors": [
            "Olivier Finkel",
            "Dominique Lecomte"
        ],
        "summary": "We prove that, for each non null countable ordinal alpha, there exist some Sigma^0_alpha-complete omega-powers, and some Pi^0_alpha-complete omega-powers, extending previous works on the topological complexity of omega-powers. We prove effective versions of these results. In particular, for each non null recursive ordinal alpha, there exists a recursive finitary language A such that A^omega is Sigma^0_alpha-complete (respectively, Pi^0_alpha-complete). To do this, we prove effective versions of a result by Kuratowski, describing a Borel set as the range of a closed subset of the Baire space by a continuous bijection. This leads us to prove closure properties for the classes Effective-Pi^0_alpha and Effective-Sigma^0_alpha of the hyperarithmetical hierarchy in arbitrary recursively presented Polish spaces. We apply our existence results to get better computations of the topological complexity of some sets of dictionaries considered by the second author in [Omega-Powers and Descriptive Set Theory, Journal of Symbolic Logic, Volume 70 (4), 2005, p. 1210-1232].",
        "published": "2007-08-30T14:56:24Z",
        "link": "http://arxiv.org/abs/0708.4176v2",
        "categories": [
            "math.LO",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Relationship between clustering and algorithmic phase transitions in the   random k-XORSAT model and its NP-complete extensions",
        "authors": [
            "Fabrizio Altarelli",
            "Remi Monasson",
            "Francesco Zamponi"
        ],
        "summary": "We study the performances of stochastic heuristic search algorithms on Uniquely Extendible Constraint Satisfaction Problems with random inputs. We show that, for any heuristic preserving the Poissonian nature of the underlying instance, the (heuristic-dependent) largest ratio $\\alpha_a$ of constraints per variables for which a search algorithm is likely to find solutions is smaller than the critical ratio $\\alpha_d$ above which solutions are clustered and highly correlated. In addition we show that the clustering ratio can be reached when the number k of variables per constraints goes to infinity by the so-called Generalized Unit Clause heuristic.",
        "published": "2007-09-04T08:56:27Z",
        "link": "http://arxiv.org/abs/0709.0367v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On the Complexity of Protein Local Structure Alignment Under the   Discrete Fréchet Distance",
        "authors": [
            "Binhai Zhu"
        ],
        "summary": "We show that given $m$ proteins (or protein backbones, which are modeled as 3D polygonal chains each of length O(n)) the problem of protein local structure alignment under the discrete Fr\\'{e}chet distance is as hard as Independent Set. So the problem does not admit any approximation of factor $n^{1-\\epsilon}$. This is the strongest negative result regarding the protein local structure alignment problem. On the other hand, if $m$ is a constant, then the problem can be solved in polygnomial time.",
        "published": "2007-09-05T15:30:54Z",
        "link": "http://arxiv.org/abs/0709.0677v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Geometric Complexity Theory: Introduction",
        "authors": [
            "Ketan D. Mulmuley",
            "Milind Sohoni"
        ],
        "summary": "These are lectures notes for the introductory graduate courses on geometric complexity theory (GCT) in the computer science department, the university of Chicago. Part I consists of the lecture notes for the course given by the first author in the spring quarter, 2007. It gives introduction to the basic structure of GCT. Part II consists of the lecture notes for the course given by the second author in the spring quarter, 2003. It gives introduction to invariant theory with a view towards GCT. No background in algebraic geometry or representation theory is assumed. These lecture notes in conjunction with the article \\cite{GCTflip1}, which describes in detail the basic plan of GCT based on the principle called the flip, should provide a high level picture of GCT assuming familiarity with only basic notions of algebra, such as groups, rings, fields etc.",
        "published": "2007-09-05T21:54:52Z",
        "link": "http://arxiv.org/abs/0709.0746v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On P vs. NP, Geometric Complexity Theory, and the Flip I: a high level   view",
        "authors": [
            "Ketan D. Mulmuley"
        ],
        "summary": "Geometric complexity theory (GCT) is an approach to the $P$ vs. $NP$ and related problems through algebraic geometry and representation theory. This article gives a high-level exposition of the basic plan of GCT based on the principle, called the flip, without assuming any background in algebraic geometry or representation theory.",
        "published": "2007-09-05T22:10:31Z",
        "link": "http://arxiv.org/abs/0709.0748v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Geometric Complexity Theory VII: Nonstandard quantum group for the   plethysm problem",
        "authors": [
            "Ketan D. Mulmuley"
        ],
        "summary": "This article describes a {\\em nonstandard} quantum group that may be used to derive a positive formula for the plethysm problem, just as the standard (Drinfeld-Jimbo) quantum group can be used to derive the positive Littlewood-Richardson rule for arbitrary complex semisimple Lie groups. The sequel \\cite{GCT8} gives conjecturally correct algorithms to construct canonical bases of the coordinate rings of these nonstandard quantum groups and canonical bases of the dually paired nonstandard deformations of the symmetric group algebra. A positive $#P$-formula for the plethysm constant follows from the conjectural properties of these canonical bases and the duality and reciprocity conjectures herein.",
        "published": "2007-09-05T22:23:15Z",
        "link": "http://arxiv.org/abs/0709.0749v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Geometric Complexity Theory VIII: On canonical bases for the nonstandard   quantum groups",
        "authors": [
            "Ketan D. Mulmuley"
        ],
        "summary": "This article gives conjecturally correct algorithms to construct canonical bases of the irreducible polynomial representations and the matrix coordinate rings of the nonstandard quantum groups in GCT4 and GCT7, and canonical bases of the dually paired nonstandard deformations of the symmetric group algebra therein. These are generalizations of the canonical bases of the irreducible polynomial representations and the matrix coordinate ring of the standard quantum group, as constructed by Kashiwara and Lusztig, and the Kazhdan-Lusztig basis of the Hecke algebra. A positive ($#P$-) formula for the well-known plethysm constants follows from their conjectural properties and the duality and reciprocity conjectures in \\cite{GCT7}.",
        "published": "2007-09-05T22:30:50Z",
        "link": "http://arxiv.org/abs/0709.0751v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Liquid State Machines in Adbiatic Quantum Computers for General   Computation",
        "authors": [
            "Joshua Jay Herman"
        ],
        "summary": "Major mistakes do not read",
        "published": "2007-09-06T16:04:42Z",
        "link": "http://arxiv.org/abs/0709.0883v5",
        "categories": [
            "cs.CC",
            "cs.NE",
            "C.1.3; F.1.3"
        ]
    },
    {
        "title": "Finding Paths and Cycles in Graphs",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "A polynomial time algorithm which detects all paths and cycles of all lengths in form of vertex pairs (start, finish).",
        "published": "2007-09-07T00:04:20Z",
        "link": "http://arxiv.org/abs/0709.0974v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS",
            "math.CO",
            "G.2.2; G.2.0; F.1.3; F.1.0"
        ]
    },
    {
        "title": "Constraint optimization and landscapes",
        "authors": [
            "Florent Krzakala",
            "Jorge Kurchan"
        ],
        "summary": "We describe an effective landscape introduced in [1] for the analysis of Constraint Satisfaction problems, such as Sphere Packing, K-SAT and Graph Coloring. This geometric construction reexpresses these problems in the more familiar terms of optimization in rugged energy landscapes. In particular, it allows one to understand the puzzling fact that unsophisticated programs are successful well beyond what was considered to be the `hard' transition, and suggests an algorithm defining a new, higher, easy-hard frontier.",
        "published": "2007-09-07T08:49:38Z",
        "link": "http://arxiv.org/abs/0709.1023v1",
        "categories": [
            "quant-ph",
            "cond-mat.stat-mech",
            "cs.CC",
            "nlin.AO"
        ]
    },
    {
        "title": "On the Proof Complexity of Deep Inference",
        "authors": [
            "Paola Bruscoli",
            "Alessio Guglielmi"
        ],
        "summary": "We obtain two results about the proof complexity of deep inference: 1) deep-inference proof systems are as powerful as Frege ones, even when both are extended with the Tseitin extension rule or with the substitution rule; 2) there are analytic deep-inference proof systems that exhibit an exponential speed-up over analytic Gentzen proof systems that they polynomially simulate.",
        "published": "2007-09-08T11:35:28Z",
        "link": "http://arxiv.org/abs/0709.1201v3",
        "categories": [
            "cs.CC",
            "cs.LO",
            "math.LO",
            "F.2.2; F.4.1"
        ]
    },
    {
        "title": "The P versus NP Brief",
        "authors": [
            "Mikael Franzen"
        ],
        "summary": "This paper discusses why P and NP are likely to be different. It analyses the essence of the concepts and points out that P and NP might be diverse by sheer definition. It also speculates that P and NP may be unequal due to natural laws.",
        "published": "2007-09-08T12:45:51Z",
        "link": "http://arxiv.org/abs/0709.1207v3",
        "categories": [
            "cs.CC",
            "F.1.2; F.4.1"
        ]
    },
    {
        "title": "The Graver Complexity of Integer Programming",
        "authors": [
            "Yael Berstein",
            "Shmuel Onn"
        ],
        "summary": "In this article we establish an exponential lower bound on the Graver complexity of integer programs. This provides new type of evidence supporting the presumable intractability of integer programming. Specifically, we show that the Graver complexity of the incidence matrix of the complete bipartite graph $K_{3,m}$ satisfies $g(m)=\\Omega(2^m)$, with $g(m)\\geq 17\\cdot 2^{m-3}-7$ for every $m>3$ .",
        "published": "2007-09-10T22:19:06Z",
        "link": "http://arxiv.org/abs/0709.1500v2",
        "categories": [
            "math.CO",
            "cs.CC",
            "cs.DM",
            "math.AC",
            "05A, 15A, 51M, 52A, 52B, 52C, 62H, 68Q, 68R, 68U, 68W, 90B, 90C"
        ]
    },
    {
        "title": "Solving Constraint Satisfaction Problems through Belief   Propagation-guided decimation",
        "authors": [
            "Andrea Montanari",
            "Federico Ricci-Tersenghi",
            "Guilhem Semerjian"
        ],
        "summary": "Message passing algorithms have proved surprisingly successful in solving hard constraint satisfaction problems on sparse random graphs. In such applications, variables are fixed sequentially to satisfy the constraints. Message passing is run after each step. Its outcome provides an heuristic to make choices at next step. This approach has been referred to as `decimation,' with reference to analogous procedures in statistical physics.   The behavior of decimation procedures is poorly understood. Here we consider a simple randomized decimation algorithm based on belief propagation (BP), and analyze its behavior on random k-satisfiability formulae. In particular, we propose a tree model for its analysis and we conjecture that it provides asymptotically exact predictions in the limit of large instances. This conjecture is confirmed by numerical simulations.",
        "published": "2007-09-11T15:48:56Z",
        "link": "http://arxiv.org/abs/0709.1667v3",
        "categories": [
            "cs.AI",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "CD(4) has bounded width",
        "authors": [
            "Catarina Carvalho",
            "Víctor Dalmau",
            "Petar Marković",
            "Miklós Maróti"
        ],
        "summary": "We prove that the constraint languages invariant under a short sequence of J\\'onsson terms (containing at most three non-trivial ternary terms) are tractable by showing that they have bounded width. This improves the previous result by Kiss and Valeriote and presents some evidence that the Larose-Zadori conjecture holds in the congruence-distributive case.",
        "published": "2007-09-12T17:26:10Z",
        "link": "http://arxiv.org/abs/0709.1934v1",
        "categories": [
            "math.LO",
            "cs.CC",
            "68N17 (Primary) 08A70, 08B10, 08B05, 03B70, 68T20 (Secondary)"
        ]
    },
    {
        "title": "Pushdown Compression",
        "authors": [
            "Pilar Albert",
            "Elvira Mayordomo",
            "Philippe Moser",
            "Sylvain Perifel"
        ],
        "summary": "The pressing need for eficient compression schemes for XML documents has recently been focused on stack computation [6, 9], and in particular calls for a formulation of information-lossless stack or pushdown compressors that allows a formal analysis of their performance and a more ambitious use of the stack in XML compression, where so far it is mainly connected to parsing mechanisms. In this paper we introduce the model of pushdown compressor, based on pushdown transducers that compute a single injective function while keeping the widest generality regarding stack computation. The celebrated Lempel-Ziv algorithm LZ78 [10] was introduced as a general purpose compression algorithm that outperforms finite-state compressors on all sequences. We compare the performance of the Lempel-Ziv algorithm with that of the pushdown compressors, or compression algorithms that can be implemented with a pushdown transducer. This comparison is made without any a priori assumption on the data's source and considering the asymptotic compression ratio for infinite sequences. We prove that Lempel-Ziv is incomparable with pushdown compressors.",
        "published": "2007-09-14T17:00:09Z",
        "link": "http://arxiv.org/abs/0709.2346v2",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "Quantifying Homology Classes II: Localization and Stability",
        "authors": [
            "Chao Chen",
            "Daniel Freedman"
        ],
        "summary": "In the companion paper, we measured homology classes and computed the optimal homology basis. This paper addresses two related problems, namely, localization and stability. We localize a class with the cycle minimizing a certain objective function. We explore three different objective functions, namely, volume, diameter and radius. We show that it is NP-hard to compute the smallest cycle using the former two. We also prove that the measurement defined in the companion paper is stable with regard to small changes of the geometry of the concerned space.",
        "published": "2007-09-16T20:48:49Z",
        "link": "http://arxiv.org/abs/0709.2512v2",
        "categories": [
            "cs.CG",
            "cs.CC",
            "math.AT",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "A New Lower Bound on Guard Placement for Wireless Localization",
        "authors": [
            "Mirela Damian",
            "Robin Flatland",
            "Joseph O'Rourke",
            "Suneeta Ramaswami"
        ],
        "summary": "The problem of wireless localization asks to place and orient stations in the plane, each of which broadcasts a unique key within a fixed angular range, so that each point in the plane can determine whether it is inside or outside a given polygonal region. The primary goal is to minimize the number of stations. In this paper we establish a lower bound of 2n/3 - 1 stations for polygons in general position, for the case in which the placement of stations is restricted to polygon vertices, improving upon the existing n/2 lower bound.",
        "published": "2007-09-22T00:35:28Z",
        "link": "http://arxiv.org/abs/0709.3554v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Deciding Unambiguity and Sequentiality starting from a Finitely   Ambiguous Max-Plus Automaton",
        "authors": [
            "Ines Klimann",
            "Sylvain Lombardy",
            "Jean Mairesse",
            "Christophe Prieur"
        ],
        "summary": "Finite automata with weights in the max-plus semiring are considered. The main result is: it is decidable in an effective way whether a series that is recognized by a finitely ambiguous max-plus automaton is unambiguous, or is sequential. A collection of examples is given to illustrate the hierarchy of max-plus series with respect to ambiguity.",
        "published": "2007-09-26T09:51:28Z",
        "link": "http://arxiv.org/abs/0709.4117v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Set Matrices and The Path/Cycle Problem",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Presentation of set matrices and demonstration of their efficiency as a tool using the path/cycle problem.",
        "published": "2007-09-26T21:44:10Z",
        "link": "http://arxiv.org/abs/0709.4273v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS",
            "math.CO",
            "G.2.2; G.2.0; F.1.3; F.1.0"
        ]
    },
    {
        "title": "The complexity of nonrepetitive edge coloring of graphs",
        "authors": [
            "Fedor Manin"
        ],
        "summary": "A squarefree word is a sequence $w$ of symbols such that there are no strings $x, y$, and $z$ for which $w=xyyz$. A nonrepetitive coloring of a graph is an edge coloring in which the sequence of colors along any open path is squarefree. We show that determining whether a graph $G$ has a nonrepetitive $k$-coloring is $\\Sigma_2^p$-complete. When we restrict to paths of lengths at most $n$, the problem becomes NP-complete for fixed $n$.",
        "published": "2007-09-27T20:56:42Z",
        "link": "http://arxiv.org/abs/0709.4497v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Fast minimum-weight double-tree shortcutting for Metric TSP: Is the best   one good enough?",
        "authors": [
            "Vladimir Deineko",
            "Alexander Tiskin"
        ],
        "summary": "The Metric Traveling Salesman Problem (TSP) is a classical NP-hard optimization problem. The double-tree shortcutting method for Metric TSP yields an exponentially-sized space of TSP tours, each of which approximates the optimal solution within at most a factor of 2. We consider the problem of finding among these tours the one that gives the closest approximation, i.e.\\ the \\emph{minimum-weight double-tree shortcutting}. Burkard et al. gave an algorithm for this problem, running in time $O(n^3+2^d n^2)$ and memory $O(2^d n^2)$, where $d$ is the maximum node degree in the rooted minimum spanning tree. We give an improved algorithm for the case of small $d$ (including planar Euclidean TSP, where $d \\leq 4$), running in time $O(4^d n^2)$ and memory $O(4^d n)$. This improvement allows one to solve the problem on much larger instances than previously attempted. Our computational experiments suggest that in terms of the time-quality tradeoff, the minimum-weight double-tree shortcutting method provides one of the best known tour-constructing heuristics.",
        "published": "2007-10-01T15:25:18Z",
        "link": "http://arxiv.org/abs/0710.0318v3",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Interpolation in Valiant's theory",
        "authors": [
            "Pascal Koiran",
            "Sylvain Perifel"
        ],
        "summary": "We investigate the following question: if a polynomial can be evaluated at rational points by a polynomial-time boolean algorithm, does it have a polynomial-size arithmetic circuit? We argue that this question is certainly difficult. Answering it negatively would indeed imply that the constant-free versions of the algebraic complexity classes VP and VNP defined by Valiant are different. Answering this question positively would imply a transfer theorem from boolean to algebraic complexity. Our proof method relies on Lagrange interpolation and on recent results connecting the (boolean) counting hierarchy to algebraic complexity classes. As a byproduct we obtain two additional results: (i) The constant-free, degree-unbounded version of Valiant's hypothesis that VP and VNP differ implies the degree-bounded version. This result was previously known to hold for fields of positive characteristic only. (ii) If exponential sums of easy to compute polynomials can be computed efficiently, then the same is true of exponential products. We point out an application of this result to the P=NP problem in the Blum-Shub-Smale model of computation over the field of complex numbers.",
        "published": "2007-10-01T18:58:19Z",
        "link": "http://arxiv.org/abs/0710.0360v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A Novel Solution to the ATT48 Benchmark Problem",
        "authors": [
            "Anthony A. Ruffa"
        ],
        "summary": "A solution to the benchmark ATT48 Traveling Salesman Problem (from the TSPLIB95 library) results from isolating the set of vertices into ten open-ended zones with nine lengthwise boundaries. In each zone, a minimum-length Hamiltonian Path (HP) is found for each combination of boundary vertices, leading to an approximation for the minimum-length Hamiltonian Cycle (HC). Determination of the optimal HPs for subsequent zones has the effect of automatically filtering out non-optimal HPs from earlier zones. Although the optimal HC for ATT48 involves only two crossing edges between all zones (with one exception), adding inter-zone edges can accommodate more complex problems.",
        "published": "2007-10-02T14:26:33Z",
        "link": "http://arxiv.org/abs/0710.0539v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "A Fast Heuristic Algorithm Based on Verification and Elimination Methods   for Maximum Clique Problem",
        "authors": [
            "Murali Krishna P",
            "Sabu . M Thampi"
        ],
        "summary": "A clique in an undirected graph G= (V, E) is a subset V' V of vertices, each pair of which is connected by an edge in E. The clique problem is an optimization problem of finding a clique of maximum size in graph. The clique problem is NP-Complete. We have succeeded in developing a fast algorithm for maximum clique problem by employing the method of verification and elimination. For a graph of size N there are 2N sub graphs, which may be cliques and hence verifying all of them, will take a long time. Idea is to eliminate a major number of sub graphs, which cannot be cliques and verifying only the remaining sub graphs. This heuristic algorithm runs in polynomial time and executes successfully for several examples when applied to random graphs and DIMACS benchmark graphs.",
        "published": "2007-10-03T09:33:20Z",
        "link": "http://arxiv.org/abs/0710.0748v1",
        "categories": [
            "cs.DM",
            "cs.CC"
        ]
    },
    {
        "title": "On the Satisfiability Threshold and Clustering of Solutions of Random   3-SAT Formulas",
        "authors": [
            "Elitza Maneva",
            "Alistair Sinclair"
        ],
        "summary": "We study the structure of satisfying assignments of a random 3-SAT formula. In particular, we show that a random formula of density 4.453 or higher almost surely has no non-trivial \"core\" assignments. Core assignments are certain partial assignments that can be extended to satisfying assignments, and have been studied recently in connection with the Survey Propagation heuristic for random SAT. Their existence implies the presence of clusters of solutions, and they have been shown to exist with high probability below the satisfiability threshold for k-SAT with k>8, by Achlioptas and Ricci-Tersenghi, STOC 2006. Our result implies that either this does not hold for 3-SAT or the threshold density for satisfiability in 3-SAT lies below 4.453.   The main technical tool that we use is a novel simple application of the first moment method.",
        "published": "2007-10-03T19:04:44Z",
        "link": "http://arxiv.org/abs/0710.0805v3",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Verification of Ptime Reducibility for system F Terms: Type Inference   in<br> Dual Light Affine Logic",
        "authors": [
            "Vincent Atassi",
            "Patrick Baillot",
            "Kazushige Terui"
        ],
        "summary": "In a previous work Baillot and Terui introduced Dual light affine logic (DLAL) as a variant of Light linear logic suitable for guaranteeing complexity properties on lambda calculus terms: all typable terms can be evaluated in polynomial time by beta reduction and all Ptime functions can be represented. In the present work we address the problem of typing lambda-terms in second-order DLAL. For that we give a procedure which, starting with a term typed in system F, determines whether it is typable in DLAL and outputs a concrete typing if there exists any. We show that our procedure can be run in time polynomial in the size of the original Church typed system F term.",
        "published": "2007-10-05T09:24:31Z",
        "link": "http://arxiv.org/abs/0710.1153v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.2.2; D.1.1"
        ]
    },
    {
        "title": "Cyclotomic FFTs with Reduced Additive Complexities Based on a Novel   Common Subexpression Elimination Algorithm",
        "authors": [
            "Ning Chen",
            "Zhiyuan Yan"
        ],
        "summary": "In this paper, we first propose a novel common subexpression elimination (CSE) algorithm for matrix-vector multiplications over characteristic-2 fields. As opposed to previously proposed CSE algorithms, which usually focus on complexity savings due to recurrences of subexpressions, our CSE algorithm achieves two types of complexity reductions, differential savings and recurrence savings, by taking advantage of the cancelation property of characteristic-2 fields. Using our CSE algorithm, we reduce the additive complexities of cyclotomic fast Fourier transforms (CFFTs). Using a weighted sum of the numbers of multiplications and additions as a metric, our CFFTs achieve smaller total complexities than previously proposed CFFTs and other FFTs, requiring both fewer multiplications and fewer additions in many cases.",
        "published": "2007-10-10T08:13:44Z",
        "link": "http://arxiv.org/abs/0710.1879v4",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.CO",
            "math.IT"
        ]
    },
    {
        "title": "Approximation algorithms and hardness for domination with propagation",
        "authors": [
            "Ashkan Aazami",
            "Michael D. Stilp"
        ],
        "summary": "The power dominating set (PDS) problem is the following extension of the well-known dominating set problem: find a smallest-size set of nodes $S$ that power dominates all the nodes, where a node $v$ is power dominated if (1) $v$ is in $S$ or $v$ has a neighbor in $S$, or (2) $v$ has a neighbor $w$ such that $w$ and all of its neighbors except $v$ are power dominated. We show a hardness of approximation threshold of $2^{\\log^{1-\\epsilon}{n}}$ in contrast to the logarithmic hardness for the dominating set problem. We give an $O(\\sqrt{n})$ approximation algorithm for planar graphs, and show that our methods cannot improve on this approximation guarantee. Finally, we initiate the study of PDS on directed graphs, and show the same hardness threshold of $2^{\\log^{1-\\epsilon}{n}}$ for directed \\emph{acyclic} graphs. Also we show that the directed PDS problem can be solved optimally in linear time if the underlying undirected graph has bounded tree-width.",
        "published": "2007-10-10T23:30:49Z",
        "link": "http://arxiv.org/abs/0710.2139v1",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Probabilistic communication complexity over the reals",
        "authors": [
            "Dima Grigoriev"
        ],
        "summary": "Deterministic and probabilistic communication protocols are introduced in which parties can exchange the values of polynomials (rather than bits in the usual setting). It is established a sharp lower bound $2n$ on the communication complexity of recognizing the $2n$-dimensional orthant, on the other hand the probabilistic communication complexity of its recognizing does not exceed 4. A polyhedron and a union of hyperplanes are constructed in $\\RR^{2n}$ for which a lower bound $n/2$ on the probabilistic communication complexity of recognizing each is proved. As a consequence this bound holds also for the EMPTINESS and the KNAPSACK problems.",
        "published": "2007-10-15T08:03:38Z",
        "link": "http://arxiv.org/abs/0710.2732v1",
        "categories": [
            "cs.CC",
            "68W40"
        ]
    },
    {
        "title": "P-matrix recognition is co-NP-complete",
        "authors": [
            "Jan Foniok"
        ],
        "summary": "This is a summary of the proof by G.E. Coxson that P-matrix recognition is co-NP-complete. The result follows by a reduction from the MAX CUT problem using results of S. Poljak and J. Rohn.",
        "published": "2007-10-18T14:14:26Z",
        "link": "http://arxiv.org/abs/0710.3519v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On the Complexity of Spill Everywhere under SSA Form",
        "authors": [
            "Florent Bouchez",
            "Alain Darte",
            "Fabrice Rastello"
        ],
        "summary": "Compilation for embedded processors can be either aggressive (time consuming cross-compilation) or just in time (embedded and usually dynamic). The heuristics used in dynamic compilation are highly constrained by limited resources, time and memory in particular. Recent results on the SSA form open promising directions for the design of new register allocation heuristics for embedded systems and especially for embedded compilation. In particular, heuristics based on tree scan with two separated phases -- one for spilling, then one for coloring/coalescing -- seem good candidates for designing memory-friendly, fast, and competitive register allocators. Still, also because of the side effect on power consumption, the minimization of loads and stores overhead (spilling problem) is an important issue. This paper provides an exhaustive study of the complexity of the ``spill everywhere'' problem in the context of the SSA form. Unfortunately, conversely to our initial hopes, many of the questions we raised lead to NP-completeness results. We identify some polynomial cases but that are impractical in JIT context. Nevertheless, they can give hints to simplify formulations for the design of aggressive allocators.",
        "published": "2007-10-19T07:24:58Z",
        "link": "http://arxiv.org/abs/0710.3642v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Random subcubes as a toy model for constraint satisfaction problems",
        "authors": [
            "Thierry Mora",
            "Lenka Zdeborova"
        ],
        "summary": "We present an exactly solvable random-subcube model inspired by the structure of hard constraint satisfaction and optimization problems. Our model reproduces the structure of the solution space of the random k-satisfiability and k-coloring problems, and undergoes the same phase transitions as these problems. The comparison becomes quantitative in the large-k limit. Distance properties, as well the x-satisfiability threshold, are studied. The model is also generalized to define a continuous energy landscape useful for studying several aspects of glassy dynamics.",
        "published": "2007-10-19T23:33:18Z",
        "link": "http://arxiv.org/abs/0710.3804v2",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn"
        ]
    },
    {
        "title": "On a New Type of Information Processing for Efficient Management of   Complex Systems",
        "authors": [
            "Victor Korotkikh",
            "Galina Korotkikh"
        ],
        "summary": "It is a challenge to manage complex systems efficiently without confronting NP-hard problems. To address the situation we suggest to use self-organization processes of prime integer relations for information processing. Self-organization processes of prime integer relations define correlation structures of a complex system and can be equivalently represented by transformations of two-dimensional geometrical patterns determining the dynamics of the system and revealing its structural complexity. Computational experiments raise the possibility of an optimality condition of complex systems presenting the structural complexity of a system as a key to its optimization.   From this perspective the optimization of a system could be all about the control of the structural complexity of the system to make it consistent with the structural complexity of the problem. The experiments also indicate that the performance of a complex system may behave as a concave function of the structural complexity. Therefore, once the structural complexity could be controlled as a single entity, the optimization of a complex system would be potentially reduced to a one-dimensional concave optimization irrespective of the number of variables involved its description. This might open a way to a new type of information processing for efficient management of complex systems.",
        "published": "2007-10-22T01:10:02Z",
        "link": "http://arxiv.org/abs/0710.3961v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "An approximation trichotomy for Boolean #CSP",
        "authors": [
            "Martin Dyer",
            "Leslie Ann Goldberg",
            "Mark Jerrum"
        ],
        "summary": "We give a trichotomy theorem for the complexity of approximately counting the number of satisfying assignments of a Boolean CSP instance. Such problems are parameterised by a constraint language specifying the relations that may be used in constraints. If every relation in the constraint language is affine then the number of satisfying assignments can be exactly counted in polynomial time. Otherwise, if every relation in the constraint language is in the co-clone IM_2 from Post's lattice, then the problem of counting satisfying assignments is complete with respect to approximation-preserving reductions in the complexity class #RH\\Pi_1. This means that the problem of approximately counting satisfying assignments of such a CSP instance is equivalent in complexity to several other known counting problems, including the problem of approximately counting the number of independent sets in a bipartite graph. For every other fixed constraint language, the problem is complete for #P with respect to approximation-preserving reductions, meaning that there is no fully polynomial randomised approximation scheme for counting satisfying assignments unless NP=RP.",
        "published": "2007-10-23T14:35:05Z",
        "link": "http://arxiv.org/abs/0710.4272v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A Numerical Algorithm for Zero Counting. I: Complexity and Accuracy",
        "authors": [
            "Felipe Cucker",
            "Teresa Krick",
            "Gregorio Malajovich",
            "Mario Wschebor"
        ],
        "summary": "We describe an algorithm to count the number of distinct real zeros of a polynomial (square) system f. The algorithm performs O(n D kappa(f)) iterations where n is the number of polynomials (as well as the dimension of the ambient space), D is a bound on the polynomials' degree, and kappa(f) is a condition number for the system. Each iteration uses an exponential number of operations. The algorithm uses finite-precision arithmetic and a polynomial bound for the precision required to ensure the returned output is correct is exhibited. This bound is a major feature of our algorithm since it is in contrast with the exponential precision required by the existing (symbolic) algorithms for counting real zeros. The algorithm parallelizes well in the sense that each iteration can be computed in parallel polynomial time with an exponential number of processors.",
        "published": "2007-10-24T16:33:07Z",
        "link": "http://arxiv.org/abs/0710.4508v2",
        "categories": [
            "cs.CC",
            "cs.NA",
            "cs.SC",
            "math.NA",
            "F.2.1; G.1; I.1.2"
        ]
    },
    {
        "title": "Energy Bounds for Fault-Tolerant Nanoscale Designs",
        "authors": [
            "Diana Marculescu"
        ],
        "summary": "The problem of determining lower bounds for the energy cost of a given nanoscale design is addressed via a complexity theory-based approach. This paper provides a theoretical framework that is able to assess the trade-offs existing in nanoscale designs between the amount of redundancy needed for a given level of resilience to errors and the associated energy cost. Circuit size, logic depth and error resilience are analyzed and brought together in a theoretical framework that can be seamlessly integrated with automated synthesis tools and can guide the design process of nanoscale systems comprised of failure prone devices. The impact of redundancy addition on the switching energy and its relationship with leakage energy is modeled in detail. Results show that 99% error resilience is possible for fault-tolerant designs, but at the expense of at least 40% more energy if individual gates fail independently with probability of 1%.",
        "published": "2007-10-25T09:04:27Z",
        "link": "http://arxiv.org/abs/0710.4680v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Weighted Random Popular Matchings",
        "authors": [
            "Toshiya Itoh",
            "Osamu Watanabe"
        ],
        "summary": "For a set A of n applicants and a set I of m items, we consider a problem of computing a matching of applicants to items, i.e., a function M mapping A to I; here we assume that each applicant $x \\in A$ provides a preference list on items in I. We say that an applicant $x \\in A$ prefers an item p than an item q if p is located at a higher position than q in its preference list, and we say that x prefers a matching M over a matching M' if x prefers M(x) over M'(x). For a given matching problem A, I, and preference lists, we say that M is more popular than M' if the number of applicants preferring M over M' is larger than that of applicants preferring M' over M, and M is called a popular matching if there is no other matching that is more popular than M. Here we consider the situation that A is partitioned into $A_{1},A_{2},...,A_{k}$, and that each $A_{i}$ is assigned a weight $w_{i}>0$ such that w_{1}>w_{2}>...>w_{k}>0$. For such a matching problem, we say that M is more popular than M' if the total weight of applicants preferring M over M' is larger than that of applicants preferring M' over M, and we call M an k-weighted popular matching if there is no other matching that is more popular than M. In this paper, we analyze the 2-weighted matching problem, and we show that (lower bound) if $m/n^{4/3}=o(1)$, then a random instance of the 2-weighted matching problem with $w_{1} \\geq 2w_{2}$ has a 2-weighted popular matching with probability o(1); and (upper bound) if $n^{4/3}/m = o(1)$, then a random instance of the 2-weighted matching problem with $w_{1} \\geq 2w_{2}$ has a 2-weighted popular matching with probability 1-o(1).",
        "published": "2007-10-29T04:41:03Z",
        "link": "http://arxiv.org/abs/0710.5338v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "F.1.2; G.2.3"
        ]
    },
    {
        "title": "Computing Equilibria in Anonymous Games",
        "authors": [
            "Constantinos Daskalakis",
            "Christos Papadimitriou"
        ],
        "summary": "We present efficient approximation algorithms for finding Nash equilibria in anonymous games, that is, games in which the players utilities, though different, do not differentiate between other players. Our results pertain to such games with many players but few strategies. We show that any such game has an approximate pure Nash equilibrium, computable in polynomial time, with approximation O(s^2 L), where s is the number of strategies and L is the Lipschitz constant of the utilities. Finally, we show that there is a PTAS for finding an epsilon",
        "published": "2007-10-30T07:32:37Z",
        "link": "http://arxiv.org/abs/0710.5582v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Convex and linear models of NP-problems",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Reducing the NP-problems to the convex/linear analysis on the Birkhoff polytope.",
        "published": "2007-11-01T08:33:07Z",
        "link": "http://arxiv.org/abs/0711.0086v2",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS",
            "math.CO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Phase Transitions and Computational Difficulty in Random Constraint   Satisfaction Problems",
        "authors": [
            "Florent Krzakala",
            "Lenka Zdeborová"
        ],
        "summary": "We review the understanding of the random constraint satisfaction problems, focusing on the q-coloring of large random graphs, that has been achieved using the cavity method of the physicists. We also discuss the properties of the phase diagram in temperature, the connections with the glass transition phenomenology in physics, and the related algorithmic issues.",
        "published": "2007-11-01T13:42:44Z",
        "link": "http://arxiv.org/abs/0711.0110v1",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech"
        ]
    },
    {
        "title": "Noise threshold for universality of 2-input gates",
        "authors": [
            "Falk Unger"
        ],
        "summary": "Evans and Pippenger showed in 1998 that noisy gates with 2 inputs are universal for arbitrary computation (i.e. can compute any function with bounded error), if all gates fail independently with probability epsilon and epsilon<theta, where theta is roughly 8.856%.   We show that formulas built from gates with 2 inputs, in which each gate fails with probability at least theta cannot be universal. Hence, there is a threshold on the tolerable noise for formulas with 2-input gates and it is theta. We conjecture that the same threshold also holds for circuits.",
        "published": "2007-11-02T16:58:21Z",
        "link": "http://arxiv.org/abs/0711.0351v2",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "Considerations on P vs NP",
        "authors": [
            "Alfredo von Reckow"
        ],
        "summary": "In order to prove that the P of problems is different to the NP class, we consider the satisfability problem of propositional calculus formulae, which is an NP-complete problem. It is shown that, for every search algorithm A, there is a set E(A) containing propositional calculus formulae, each of which requires the algorithm A to take non-polynomial time to find the truth-values of its propositional letters satisfying it. Moreover, E(A)'s size is an exponential function of n, which makes it impossible to detect such formulae in a polynomial time. Hence, the satisfability problem does not have a polynomial complexity",
        "published": "2007-11-07T22:32:41Z",
        "link": "http://arxiv.org/abs/0711.1177v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3"
        ]
    },
    {
        "title": "An analysis of a random algorithm for estimating all the matchings",
        "authors": [
            "Jinshan Zhang",
            "Yan Huo",
            "Fengshan Bai"
        ],
        "summary": "Counting the number of all the matchings on a bipartite graph has been transformed into calculating the permanent of a matrix obtained from the extended bipartite graph by Yan Huo, and Rasmussen presents a simple approach (RM) to approximate the permanent, which just yields a critical ratio O($n\\omega(n)$) for almost all the 0-1 matrices, provided it's a simple promising practical way to compute this #P-complete problem. In this paper, the performance of this method will be shown when it's applied to compute all the matchings based on that transformation. The critical ratio will be proved to be very large with a certain probability, owning an increasing factor larger than any polynomial of $n$ even in the sense for almost all the 0-1 matrices. Hence, RM fails to work well when counting all the matchings via computing the permanent of the matrix. In other words, we must carefully utilize the known methods of estimating the permanent to count all the matchings through that transformation.",
        "published": "2007-11-12T08:15:39Z",
        "link": "http://arxiv.org/abs/0711.1723v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "The Three-Color and Two-Color Tantrix(TM) Rotation Puzzle Problems are   NP-Complete via Parsimonious Reductions",
        "authors": [
            "Dorothea Baumeister",
            "Joerg Rothe"
        ],
        "summary": "Holzer and Holzer (Discrete Applied Mathematics 144(3):345--358, 2004) proved that the Tantrix(TM) rotation puzzle problem with four colors is NP-complete, and they showed that the infinite variant of this problem is undecidable. In this paper, we study the three-color and two-color Tantrix(TM) rotation puzzle problems (3-TRP and 2-TRP) and their variants. Restricting the number of allowed colors to three (respectively, to two) reduces the set of available Tantrix(TM) tiles from 56 to 14 (respectively, to 8). We prove that 3-TRP and 2-TRP are NP-complete, which answers a question raised by Holzer and Holzer in the affirmative. Since our reductions are parsimonious, it follows that the problems Unique-3-TRP and Unique-2-TRP are DP-complete under randomized reductions. We also show that the another-solution problems associated with 4-TRP, 3-TRP, and 2-TRP are NP-complete. Finally, we prove that the infinite variants of 3-TRP and 2-TRP are undecidable.",
        "published": "2007-11-12T17:44:45Z",
        "link": "http://arxiv.org/abs/0711.1827v3",
        "categories": [
            "cs.CC",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "A Polynomial Time Algorithm for Graph Isomorphism",
        "authors": [
            "Reiner Czerwinski"
        ],
        "summary": "We claimed that there is a polynomial algorithm to test if two graphs are isomorphic. But the algorithm is wrong. It only tests if the adjacency matrices of two graphs have the same eigenvalues. There is a counterexample of two non-isomorphic graphs with the same eigenvalues.",
        "published": "2007-11-13T15:51:33Z",
        "link": "http://arxiv.org/abs/0711.2010v5",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Algorithmic Arithmetic Fewnomial Theory I: One Variable",
        "authors": [
            "Ashraf Ibrahim",
            "J. Maurice Rojas",
            "Korben Rusek"
        ],
        "summary": "Withdrawn by the authors due to an error in the proof of the finite field result (Thm. 1.5): The random primes used in the proof need NOT avoid the exceptional primes from Lemma 2.7, thus leaving Thm. 1.5 unproved.",
        "published": "2007-11-16T06:15:35Z",
        "link": "http://arxiv.org/abs/0711.2562v2",
        "categories": [
            "math.NT",
            "cs.CC",
            "math.AG"
        ]
    },
    {
        "title": "Complexity of the conditional colorability of graphs",
        "authors": [
            "Xueliang Li",
            "Xiangmei Yao",
            "Wenli Zhou"
        ],
        "summary": "For an integer $r>0$, a conditional $(k,r)$-coloring of a graph $G$ is a proper $k$-coloring of the vertices of $G$ such that every vertex $v$ of degree $d(v)$ in $G$ is adjacent to vertices with at least $min\\{r, d(v)\\}$ different colors. The smallest integer $k$ for which a graph $G$ has a conditional $(k,r)$-coloring is called the $r$th order conditional chromatic number, denoted by $\\chi_r(G)$. It is easy to see that the conditional coloring is a generalization of the traditional vertex coloring for which $r=1$. In this paper, we consider the complexity of the conditional colorings of graphs. The main result is that the conditional $(3,2)$-colorability is $NP$-complete for triangle-free graphs with maximum degree at most 3, which is different from the old result that the traditional 3-colorability is polynomial solvable for graphs with maximum degree at most 3. This also implies that it is $NP$-complete to determine if a graph of maximum degree 3 is $(3,2)$- or $(4,2)$-colorable. Also we have proved that some old complexity results for traditional colorings still hold for the conditional colorings.",
        "published": "2007-11-19T05:41:36Z",
        "link": "http://arxiv.org/abs/0711.2843v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "G.2.2; F.2.2"
        ]
    },
    {
        "title": "Dynamic 3-Coloring of Claw-free Graphs",
        "authors": [
            "Xueliang Li",
            "Wenli Zhou"
        ],
        "summary": "A {\\it dynamic $k$-coloring} of a graph $G$ is a proper $k$-coloring of the vertices of $G$ such that every vertex of degree at least 2 in $G$ will be adjacent to vertices with at least 2 different colors. The smallest number $k$ for which a graph $G$ can have a dynamic $k$-coloring is the {\\it dynamic chromatic number}, denoted by $\\chi_d(G)$. In this paper, we investigate the dynamic 3-colorings of claw-free graphs. First, we prove that it is $NP$-complete to determine if a claw-free graph with maximum degree 3 is dynamically 3-colorable. Second, by forbidding a kind of subgraphs, we find a reasonable subclass of claw-free graphs with maximum degree 3, for which the dynamically 3-colorable problem can be solved in linear time. Third, we give a linear time algorithm to recognize this subclass of graphs, and a linear time algorithm to determine whether it is dynamically 3-colorable. We also give a linear time algorithm to color the graphs in the subclass by 3 colors.",
        "published": "2007-11-19T05:56:01Z",
        "link": "http://arxiv.org/abs/0711.2844v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "G.2.2; F.2.2"
        ]
    },
    {
        "title": "Recognizing Members of the Tournament Equilibrium Set is NP-hard",
        "authors": [
            "Felix Brandt",
            "Felix Fischer",
            "Paul Harrenstein"
        ],
        "summary": "A recurring theme in the mathematical social sciences is how to select the \"most desirable\" elements given a binary dominance relation on a set of alternatives. Schwartz's tournament equilibrium set (TEQ) ranks among the most intriguing, but also among the most enigmatic, tournament solutions that have been proposed so far in this context. Due to its unwieldy recursive definition, little is known about TEQ. In particular, its monotonicity remains an open problem up to date. Yet, if TEQ were to satisfy monotonicity, it would be a very attractive tournament solution concept refining both the Banks set and Dutta's minimal covering set. We show that the problem of deciding whether a given alternative is contained in TEQ is NP-hard.",
        "published": "2007-11-19T15:48:46Z",
        "link": "http://arxiv.org/abs/0711.2961v2",
        "categories": [
            "cs.CC",
            "cs.GT",
            "cs.MA"
        ]
    },
    {
        "title": "On Low Complexity Maximum Likelihood Decoding of Convolutional Codes",
        "authors": [
            "Jie Luo"
        ],
        "summary": "This paper considers the average complexity of maximum likelihood (ML) decoding of convolutional codes. ML decoding can be modeled as finding the most probable path taken through a Markov graph. Integrated with the Viterbi algorithm (VA), complexity reduction methods such as the sphere decoder often use the sum log likelihood (SLL) of a Markov path as a bound to disprove the optimality of other Markov path sets and to consequently avoid exhaustive path search. In this paper, it is shown that SLL-based optimality tests are inefficient if one fixes the coding memory and takes the codeword length to infinity. Alternatively, optimality of a source symbol at a given time index can be testified using bounds derived from log likelihoods of the neighboring symbols. It is demonstrated that such neighboring log likelihood (NLL)-based optimality tests, whose efficiency does not depend on the codeword length, can bring significant complexity reduction to ML decoding of convolutional codes. The results are generalized to ML sequence detection in a class of discrete-time hidden Markov systems.",
        "published": "2007-11-20T07:27:30Z",
        "link": "http://arxiv.org/abs/0711.3077v3",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "Detecting palindromes, patterns, and borders in regular languages",
        "authors": [
            "Terry Anderson",
            "John Loftus",
            "Narad Rampersad",
            "Nicolae Santean",
            "Jeffrey Shallit"
        ],
        "summary": "Given a language L and a nondeterministic finite automaton M, we consider whether we can determine efficiently (in the size of M) if M accepts at least one word in L, or infinitely many words. Given that M accepts at least one word in L, we consider how long a shortest word can be. The languages L that we examine include the palindromes, the non-palindromes, the k-powers, the non-k-powers, the powers, the non-powers (also called primitive words), the words matching a general pattern, the bordered words, and the unbordered words.",
        "published": "2007-11-20T17:38:47Z",
        "link": "http://arxiv.org/abs/0711.3183v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.FL",
            "F.4.3"
        ]
    },
    {
        "title": "Representation and Measure of Structural Information",
        "authors": [
            "Hiroshi Ishikawa"
        ],
        "summary": "We introduce a uniform representation of general objects that captures the regularities with respect to their structure. It allows a representation of a general class of objects including geometric patterns and images in a sparse, modular, hierarchical, and recursive manner. The representation can exploit any computable regularity in objects to compactly describe them, while also being capable of representing random objects as raw data. A set of rules uniformly dictates the interpretation of the representation into raw signal, which makes it possible to ask what pattern a given raw signal contains. Also, it allows simple separation of the information that we wish to ignore from that which we measure, by using a set of maps to delineate the a priori parts of the objects, leaving only the information in the structure.   Using the representation, we introduce a measure of information in general objects relative to structures defined by the set of maps. We point out that the common prescription of encoding objects by strings to use Kolmogorov complexity is meaningless when, as often is the case, the encoding is not specified in any way other than that it exists. Noting this, we define the measure directly in terms of the structures of the spaces in which the objects reside. As a result, the measure is defined relative to a set of maps that characterize the structures. It turns out that the measure is equivalent to Kolmogorov complexity when it is defined relative to the maps characterizing the structure of natural numbers. Thus, the formulation gives the larger class of objects a meaningful measure of information that generalizes Kolmogorov complexity.",
        "published": "2007-11-28T18:41:30Z",
        "link": "http://arxiv.org/abs/0711.4508v2",
        "categories": [
            "cs.CC",
            "cs.CV",
            "cs.IT",
            "math.IT",
            "F.1.1; H.1.1; I.2.10"
        ]
    },
    {
        "title": "Copeland Voting Fully Resists Constructive Control",
        "authors": [
            "Piotr Faliszewski",
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Jörg Rothe"
        ],
        "summary": "Control and bribery are settings in which an external agent seeks to influence the outcome of an election. Faliszewski et al. [FHHR07] proved that Llull voting (which is here denoted by Copeland^1) and a variant (here denoted by Copeland^0) of Copeland voting are computationally resistant to many, yet not all, types of constructive control and that they also provide broad resistance to bribery. We study a parameterized version of Copeland voting, denoted by Copeland^alpha where the parameter alpha is a rational number between 0 and 1 that specifies how ties are valued in the pairwise comparisons of candidates in Copeland elections. We establish resistance or vulnerability results, in every previously studied control scenario, for Copeland^alpha, for each rational alpha, 0 <alpha < 1. In particular, we prove that Copeland^0.5, the system commonly referred to as ``Copeland voting,'' provides full resistance to constructive control. Among the systems with a polynomial-time winner problem, this is the first natural election system proven to have full resistance to constructive control. Results on bribery and fixed-parameter tractability of bounded-case control proven for Copeland^0 and Copeland^1 in [FHHR07] are extended to Copeland^alpha for each rational alpha, 0 < alpha < 1; we also give results in more flexible models such as microbribery and extended control.",
        "published": "2007-11-29T16:12:25Z",
        "link": "http://arxiv.org/abs/0711.4759v2",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Nonuniform Bribery",
        "authors": [
            "Piotr Faliszewski"
        ],
        "summary": "We study the concept of bribery in the situation where voters are willing to change their votes as we ask them, but where their prices depend on the nature of the change we request. Our model is an extension of the one of Faliszewski et al. [FHH06], where each voter has a single price for any change we may ask for. We show polynomial-time algorithms for our version of bribery for a broad range of voting protocols, including plurality, veto, approval, and utility based voting. In addition to our polynomial-time algorithms we provide NP-completeness results for a couple of our nonuniform bribery problems for weighted voters, and a couple of approximation algorithms for NP-complete bribery problems defined in [FHH06] (in particular, an FPTAS for plurality-weighted-$bribery problem).",
        "published": "2007-11-30T12:47:35Z",
        "link": "http://arxiv.org/abs/0711.4924v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "From vectors to mnesors",
        "authors": [
            "Gilles Champenois"
        ],
        "summary": "The mnesor theory is the adaptation of vectors to artificial intelligence. The scalar field is replaced by a lattice. Addition becomes idempotent and multiplication is interpreted as a selection operation. We also show that mnesors can be the foundation for a linear calculus.",
        "published": "2007-12-01T14:37:07Z",
        "link": "http://arxiv.org/abs/0712.0084v4",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.3"
        ]
    },
    {
        "title": "On the Accepting Power of 2-Tape Büchi Automata",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We show that, from a topological point of view, 2-tape B\\\"uchi automata have the same accepting power than Turing machines equipped with a B\\\"uchi acceptance condition. In particular, we show that for every non null recursive ordinal alpha, there exist some Sigma^0_alpha-complete and some Pi^0_alpha-complete infinitary rational relations accepted by 2-tape B\\\"uchi automata. This very surprising result gives answers to questions of W. Thomas [Automata and Quantifier Hierarchies, in: Formal Properties of Finite automata and Applications, Ramatuelle, 1988, LNCS 386, Springer, 1989, p.104-119], of P. Simonnet [Automates et Th\\'eorie Descriptive, Ph. D. Thesis, Universit\\'e Paris 7, March 1992], and of H. Lescow and W. Thomas [Logical Specifications of Infinite Computations, In: \"A Decade of Concurrency\", LNCS 803, Springer, 1994, p. 583-621].",
        "published": "2007-12-02T18:36:34Z",
        "link": "http://arxiv.org/abs/0712.0165v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "A Spectral Approach to Analyzing Belief Propagation for 3-Coloring",
        "authors": [
            "Amin Coja-Oghlan",
            "Elchanan Mossel",
            "Dan Vilenchik"
        ],
        "summary": "Contributing to the rigorous understanding of BP, in this paper we relate the convergence of BP to spectral properties of the graph. This encompasses a result for random graphs with a ``planted'' solution; thus, we obtain the first rigorous result on BP for graph coloring in the case of a complex graphical structure (as opposed to trees). In particular, the analysis shows how Belief Propagation breaks the symmetry between the $3!$ possible permutations of the color classes.",
        "published": "2007-12-02T19:34:59Z",
        "link": "http://arxiv.org/abs/0712.0171v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "cs.DM"
        ]
    },
    {
        "title": "A Reactive Tabu Search Algorithm for Stimuli Generation in   Psycholinguistics",
        "authors": [
            "Alejandro Chinea Manrique De Lara"
        ],
        "summary": "The generation of meaningless \"words\" matching certain statistical and/or linguistic criteria is frequently needed for experimental purposes in Psycholinguistics. Such stimuli receive the name of pseudowords or nonwords in the Cognitive Neuroscience literatue. The process for building nonwords sometimes has to be based on linguistic units such as syllables or morphemes, resulting in a numerical explosion of combinations when the size of the nonwords is increased. In this paper, a reactive tabu search scheme is proposed to generate nonwords of variables size. The approach builds pseudowords by using a modified Metaheuristic algorithm based on a local search procedure enhanced by a feedback-based scheme. Experimental results show that the new algorithm is a practical and effective tool for nonword generation.",
        "published": "2007-12-04T08:52:46Z",
        "link": "http://arxiv.org/abs/0712.0451v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DM",
            "cs.LG"
        ]
    },
    {
        "title": "Undecidable Problems About Timed Automata",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We solve some decision problems for timed automata which were recently raised by S. Tripakis in [ Folk Theorems on the Determinization and Minimization of Timed Automata, in the Proceedings of the International Workshop FORMATS'2003, LNCS, Volume 2791, p. 182-188, 2004 ] and by E. Asarin in [ Challenges in Timed Languages, From Applied Theory to Basic Theory, Bulletin of the EATCS, Volume 83, p. 106-120, 2004 ]. In particular, we show that one cannot decide whether a given timed automaton is determinizable or whether the complement of a timed regular language is timed regular. We show that the problem of the minimization of the number of clocks of a timed automaton is undecidable. It is also undecidable whether the shuffle of two timed regular languages is timed regular. We show that in the case of timed B\\\"uchi automata accepting infinite timed words some of these problems are Pi^1_1-hard, hence highly undecidable (located beyond the arithmetical hierarchy).",
        "published": "2007-12-09T20:11:42Z",
        "link": "http://arxiv.org/abs/0712.1363v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "Reconstruction of Markov Random Fields from Samples: Some Easy   Observations and Algorithms",
        "authors": [
            "Guy Bresler",
            "Elchanan Mossel",
            "Allan Sly"
        ],
        "summary": "Markov random fields are used to model high dimensional distributions in a number of applied areas. Much recent interest has been devoted to the reconstruction of the dependency structure from independent samples from the Markov random fields. We analyze a simple algorithm for reconstructing the underlying graph defining a Markov random field on $n$ nodes and maximum degree $d$ given observations. We show that under mild non-degeneracy conditions it reconstructs the generating graph with high probability using $\\Theta(d \\epsilon^{-2}\\delta^{-4} \\log n)$ samples where $\\epsilon,\\delta$ depend on the local interactions. For most local interaction $\\eps,\\delta$ are of order $\\exp(-O(d))$.   Our results are optimal as a function of $n$ up to a multiplicative constant depending on $d$ and the strength of the local interactions. Our results seem to be the first results for general models that guarantee that {\\em the} generating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2} \\epsilon^{-2}\\delta^{-4} \\log n)$ running time bound. In cases where the measure on the graph has correlation decay, the running time is $O(n^2 \\log n)$ for all fixed $d$. We also discuss the effect of observing noisy samples and show that as long as the noise level is low, our algorithm is effective. On the other hand, we construct an example where large noise implies non-identifiability even for generic noise and interactions. Finally, we briefly show that in some simple cases, models with hidden nodes can also be recovered.",
        "published": "2007-12-10T06:50:36Z",
        "link": "http://arxiv.org/abs/0712.1402v2",
        "categories": [
            "cs.CC",
            "cs.LG"
        ]
    },
    {
        "title": "On the computational complexity of cut-reduction",
        "authors": [
            "Klaus Aehlig",
            "Arnold Beckmann"
        ],
        "summary": "Using appropriate notation systems for proofs, cut-reduction can often be rendered feasible on these notations, and explicit bounds can be given. Developing a suitable notation system for Bounded Arithmetic, and applying these bounds, all the known results on definable functions of certain such theories can be reobtained in a uniform way.",
        "published": "2007-12-10T14:58:27Z",
        "link": "http://arxiv.org/abs/0712.1499v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Hard constraint satisfaction problems have hard gaps at location 1",
        "authors": [
            "Peter Jonsson",
            "Andrei Krokhin",
            "Fredrik Kuivinen"
        ],
        "summary": "An instance of Max CSP is a finite collection of constraints on a set of variables, and the goal is to assign values to the variables that maximises the number of satisfied constraints. Max CSP captures many well-known problems (such as Max k-SAT and Max Cut) and is consequently NP-hard. Thus, it is natural to study how restrictions on the allowed constraint types (or constraint languages) affect the complexity and approximability of Max CSP. The PCP theorem is equivalent to the existence of a constraint language for which Max CSP has a hard gap at location 1, i.e. it is NP-hard to distinguish between satisfiable instances and instances where at most some constant fraction of the constraints are satisfiable. All constraint languages, for which the CSP problem (i.e., the problem of deciding whether all constraints can be satisfied) is currently known to be NP-hard, have a certain algebraic property. We prove that any constraint language with this algebraic property makes Max CSP have a hard gap at location 1 which, in particular, implies that such problems cannot have a PTAS unless P = NP. We then apply this result to Max CSP restricted to a single constraint type; this class of problems contains, for instance, Max Cut and Max DiCut. Assuming P $\\neq$ NP, we show that such problems do not admit PTAS except in some trivial cases. Our results hold even if the number of occurrences of each variable is bounded by a constant. We use these results to partially answer open questions and strengthen results by Engebretsen et al. [Theor. Comput. Sci., 312 (2004), pp. 17--45], Feder et al. [Discrete Math., 307 (2007), pp. 386--392], Krokhin and Larose [Proc. Principles and Practice of Constraint Programming (2005), pp. 388--402], and Jonsson and Krokhin [J. Comput. System Sci., 73 (2007), pp. 691--702]",
        "published": "2007-12-10T16:42:18Z",
        "link": "http://arxiv.org/abs/0712.1532v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A case study of the difficulty of quantifier elimination in constraint   databases: the alibi query in moving object databases",
        "authors": [
            "Bart Kuijpers",
            "Walied Othman",
            "Rafael Grimson"
        ],
        "summary": "In the constraint database model, spatial and spatio-temporal data are stored by boolean combinations of polynomial equalities and inequalities over the real numbers. The relational calculus augmented with polynomial constraints is the standard first-order query language for constraint databases. Although the expressive power of this query language has been studied extensively, the difficulty of the efficient evaluation of queries, usually involving some form of quantifier elimination, has received considerably less attention. The inefficiency of existing quantifier-elimination software and the intrinsic difficulty of quantifier elimination have proven to be a bottle-neck for for real-world implementations of constraint database systems. In this paper, we focus on a particular query, called the \\emph{alibi query}, that asks whether two moving objects whose positions are known at certain moments in time, could have possibly met, given certain speed constraints. This query can be seen as a constraint database query and its evaluation relies on the elimination of a block of three existential quantifiers. Implementations of general purpose elimination algorithms are in the specific case, for practical purposes, too slow in answering the alibi query and fail completely in the parametric case. The main contribution of this paper is an analytical solution to the parametric alibi query, which can be used to answer this query in the specific case in constant time. We also give an analytic solution to the alibi query at a fixed moment in time. The solutions we propose are based on geometric argumentation and they illustrate the fact that some practical problems require creative solutions, where at least in theory, existing systems could provide a solution.",
        "published": "2007-12-12T18:05:41Z",
        "link": "http://arxiv.org/abs/0712.1996v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.DB"
        ]
    },
    {
        "title": "Distinguishing Short Quantum Computations",
        "authors": [
            "Bill Rosgen"
        ],
        "summary": "Distinguishing logarithmic depth quantum circuits on mixed states is shown to be complete for QIP, the class of problems having quantum interactive proof systems. Circuits in this model can represent arbitrary quantum processes, and thus this result has implications for the verification of implementations of quantum algorithms. The distinguishability problem is also complete for QIP on constant depth circuits containing the unbounded fan-out gate. These results are shown by reducing a QIP-complete problem to a logarithmic depth version of itself using a parallelization technique.",
        "published": "2007-12-16T21:40:41Z",
        "link": "http://arxiv.org/abs/0712.2595v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Automata-based Adaptive Behavior for Economical Modelling Using Game   Theory",
        "authors": [
            "Rawan Ghnemat",
            "Saleh Oqeili",
            "Cyrille Bertelle",
            "Gérard Henry Edmond Duchamp"
        ],
        "summary": "In this chapter, we deal with some specific domains of applications to game theory. This is one of the major class of models in the new approaches of modelling in the economic domain. For that, we use genetic automata which allow to build adaptive strategies for the players. We explain how the automata-based formalism proposed - matrix representation of automata with multiplicities - allows to define semi-distance between the strategy behaviors. With that tools, we are able to generate an automatic processus to compute emergent systems of entities whose behaviors are represented by these genetic automata.",
        "published": "2007-12-17T07:07:54Z",
        "link": "http://arxiv.org/abs/0712.2644v1",
        "categories": [
            "cs.GT",
            "cs.CC"
        ]
    },
    {
        "title": "Phase transition and computational complexity in a stochastic prime   number generator",
        "authors": [
            "Lucas Lacasa",
            "Bartolo Luque",
            "Octavio Miramontes"
        ],
        "summary": "We introduce a prime number generator in the form of a stochastic algorithm. The character of such algorithm gives rise to a continuous phase transition which distinguishes a phase where the algorithm is able to reduce the whole system of numbers into primes and a phase where the system reaches a frozen state with low prime density. In this paper we firstly pretend to give a broad characterization of this phase transition, both in terms of analytical and numerical analysis. Critical exponents are calculated, and data collapse is provided. Further on we redefine the model as a search problem, fitting it in the hallmark of computational complexity theory. We suggest that the system belongs to the class NP. The computational cost is maximal around the threshold, as common in many algorithmic phase transitions, revealing the presence of an easy-hard-easy pattern. We finally relate the nature of the phase transition to an average-case classification of the problem.",
        "published": "2007-12-19T10:00:32Z",
        "link": "http://arxiv.org/abs/0712.3137v1",
        "categories": [
            "cs.CC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Solving Medium-Density Subset Sum Problems in Expected Polynomial Time:   An Enumeration Approach",
        "authors": [
            "Changlin Wan",
            "Zhongzhi Shi"
        ],
        "summary": "The subset sum problem (SSP) can be briefly stated as: given a target integer $E$ and a set $A$ containing $n$ positive integer $a_j$, find a subset of $A$ summing to $E$. The \\textit{density} $d$ of an SSP instance is defined by the ratio of $n$ to $m$, where $m$ is the logarithm of the largest integer within $A$. Based on the structural and statistical properties of subset sums, we present an improved enumeration scheme for SSP, and implement it as a complete and exact algorithm (EnumPlus). The algorithm always equivalently reduces an instance to be low-density, and then solve it by enumeration. Through this approach, we show the possibility to design a sole algorithm that can efficiently solve arbitrary density instance in a uniform way. Furthermore, our algorithm has considerable performance advantage over previous algorithms. Firstly, it extends the density scope, in which SSP can be solved in expected polynomial time. Specifically, It solves SSP in expected $O(n\\log{n})$ time when density $d \\geq c\\cdot \\sqrt{n}/\\log{n}$, while the previously best density scope is $d \\geq c\\cdot n/(\\log{n})^{2}$. In addition, the overall expected time and space requirement in the average case are proven to be $O(n^5\\log n)$ and $O(n^5)$ respectively. Secondly, in the worst case, it slightly improves the previously best time complexity of exact algorithms for SSP. Specifically, the worst-case time complexity of our algorithm is proved to be $O((n-6)2^{n/2}+n)$, while the previously best result is $O(n2^{n/2})$.",
        "published": "2007-12-19T14:43:50Z",
        "link": "http://arxiv.org/abs/0712.3203v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.CR",
            "F.2.2; G.2.1; G.1.6; I.2.8; E.3"
        ]
    },
    {
        "title": "On Exponential Time Lower Bound of Knapsack under Backtracking",
        "authors": [
            "Xin Li",
            "Tian Liu"
        ],
        "summary": "M.Aleknovich et al. have recently proposed a model of algorithms, called BT model, which generalizes both the priority model of Borodin, Nielson and Rackoff, as well as a simple dynamic programming model by Woeginger. BT model can be further divided into three kinds of fixed, adaptive and fully adaptive ones. They have proved exponential time lower bounds of exact and approximation algorithms under adaptive BT model for Knapsack problem. Their exact lower bound is $\\Omega(2^{0.5n}/\\sqrt{n})$, in this paper, we slightly improve the exact lower bound to about $\\Omega(2^{0.69n}/\\sqrt{n})$, by the same technique, with related parameters optimized.",
        "published": "2007-12-20T09:15:17Z",
        "link": "http://arxiv.org/abs/0712.3348v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Accurate and Efficient Expression Evaluation and Linear Algebra",
        "authors": [
            "James Demmel",
            "Ioana Dumitriu",
            "Olga Holtz",
            "Plamen Koev"
        ],
        "summary": "We survey and unify recent results on the existence of accurate algorithms for evaluating multivariate polynomials, and more generally for accurate numerical linear algebra with structured matrices. By \"accurate\" we mean that the computed answer has relative error less than 1, i.e., has some correct leading digits. We also address efficiency, by which we mean algorithms that run in polynomial time in the size of the input. Our results will depend strongly on the model of arithmetic: Most of our results will use the so-called Traditional Model (TM). We give a set of necessary and sufficient conditions to decide whether a high accuracy algorithm exists in the TM, and describe progress toward a decision procedure that will take any problem and provide either a high accuracy algorithm or a proof that none exists. When no accurate algorithm exists in the TM, it is natural to extend the set of available accurate operations by a library of additional operations, such as $x+y+z$, dot products, or indeed any enumerable set which could then be used to build further accurate algorithms. We show how our accurate algorithms and decision procedure for finding them extend to this case. Finally, we address other models of arithmetic, and the relationship between (im)possibility in the TM and (in)efficient algorithms operating on numbers represented as bit strings.",
        "published": "2007-12-24T20:14:50Z",
        "link": "http://arxiv.org/abs/0712.4027v1",
        "categories": [
            "math.NA",
            "cs.CC",
            "cs.DS",
            "math.RA",
            "65Y20, 68Q05, 68Q25, 65F30, 68W40, 68W25"
        ]
    },
    {
        "title": "Disjointness is hard in the multi-party number on the forehead model",
        "authors": [
            "Troy Lee",
            "Adi Shraibman"
        ],
        "summary": "We show that disjointness requires randomized communication Omega(n^{1/(k+1)}/2^{2^k}) in the general k-party number-on-the-forehead model of complexity. The previous best lower bound for k >= 3 was log(n)/(k-1). Our results give a separation between nondeterministic and randomized multiparty number-on-the-forehead communication complexity for up to k=log log n - O(log log log n) many players. Also by a reduction of Beame, Pitassi, and Segerlind, these results imply subexponential lower bounds on the size of proofs needed to refute certain unsatisfiable CNFs in a broad class of proof systems, including tree-like Lovasz-Schrijver proofs.",
        "published": "2007-12-27T20:45:53Z",
        "link": "http://arxiv.org/abs/0712.4279v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Definable functions in the simply typed lambda-calculus",
        "authors": [
            "Mateusz Zakrzewski"
        ],
        "summary": "It is a common knowledge that the integer functions definable in simply typed lambda-calculus are exactly the extended polynomials. This is indeed the case when one interprets integers over the type (p->p)->p->p where p is a base type and/or equality is taken as beta-conversion. It is commonly believed that the same holds for beta-eta equality and for integers represented over any fixed type of the form (t->t)->t->t. In this paper we show that this opinion is not quite true.   We prove that the class of functions strictly definable in simply typed lambda-calculus is considerably larger than the extended polynomials. Namely, we define F as the class of strictly definable functions and G as a class that contains extended polynomials and two additional functions, or more precisely, two function schemas, and is closed under composition. We prove that G is a subset of F.   We conjecture that G exactly characterizes strictly definable functions, i.e. G=F, and we gather some evidence for this conjecture proving, for example, that every skewly representable finite range function is strictly representable over (t->t)->t->t for some t.",
        "published": "2007-01-04T17:50:01Z",
        "link": "http://arxiv.org/abs/cs/0701022v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A Polynomial Time Algorithm for 3-SAT",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Article describes a class of efficient algorithms for 3SAT and their generalizations on SAT.",
        "published": "2007-01-04T18:16:30Z",
        "link": "http://arxiv.org/abs/cs/0701023v4",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.LO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "The Inhabitation Problem for Rank Two Intersection Types",
        "authors": [
            "Dariusz Kusmierek"
        ],
        "summary": "We prove that the inhabitation problem for rank two intersection types is decidable, but (contrary to common belief) EXPTIME-hard. The exponential time hardness is shown by reduction from the in-place acceptance problem for alternating Turing machines.",
        "published": "2007-01-05T08:38:14Z",
        "link": "http://arxiv.org/abs/cs/0701029v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "On the implementation of construction functions for non-free concrete   data types",
        "authors": [
            "Frédéric Blanqui",
            "Thérèse Hardin",
            "Pierre Weis"
        ],
        "summary": "Many algorithms use concrete data types with some additional invariants. The set of values satisfying the invariants is often a set of representatives for the equivalence classes of some equational theory. For instance, a sorted list is a particular representative wrt commutativity. Theories like associativity, neutral element, idempotence, etc. are also very common. Now, when one wants to combine various invariants, it may be difficult to find the suitable representatives and to efficiently implement the invariants. The preservation of invariants throughout the whole program is even more difficult and error prone. Classically, the programmer solves this problem using a combination of two techniques: the definition of appropriate construction functions for the representatives and the consistent usage of these functions ensured via compiler verifications. The common way of ensuring consistency is to use an abstract data type for the representatives; unfortunately, pattern matching on representatives is lost. A more appealing alternative is to define a concrete data type with private constructors so that both compiler verification and pattern matching on representatives are granted. In this paper, we detail the notion of private data type and study the existence of construction functions. We also describe a prototype, called Moca, that addresses the entire problem of...",
        "published": "2007-01-05T16:54:35Z",
        "link": "http://arxiv.org/abs/cs/0701031v1",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Polygraphic programs and polynomial-time functions",
        "authors": [
            "Guillaume Bonfante",
            "Yves Guiraud"
        ],
        "summary": "We study the computational model of polygraphs. For that, we consider polygraphic programs, a subclass of these objects, as a formal description of first-order functional programs. We explain their semantics and prove that they form a Turing-complete computational model. Their algebraic structure is used by analysis tools, called polygraphic interpretations, for complexity analysis. In particular, we delineate a subclass of polygraphic programs that compute exactly the functions that are Turing-computable in polynomial time.",
        "published": "2007-01-05T18:05:48Z",
        "link": "http://arxiv.org/abs/cs/0701032v4",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.CT",
            "F.1.1; F.4"
        ]
    },
    {
        "title": "On the Complexity of the Numerically Definite Syllogistic and Related   Fragments",
        "authors": [
            "Ian Pratt-Hartmann"
        ],
        "summary": "In this paper, we determine the complexity of the satisfiability problem for various logics obtained by adding numerical quantifiers, and other constructions, to the traditional syllogistic. In addition, we demonstrate the incompleteness of some recently proposed proof-systems for these logics.",
        "published": "2007-01-06T15:00:00Z",
        "link": "http://arxiv.org/abs/cs/0701039v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Nearly-Exponential Size Lower Bounds for Symbolic Quantifier Elimination   Algorithms and OBDD-Based Proofs of Unsatisfiability",
        "authors": [
            "Nathan Segerlind"
        ],
        "summary": "We demonstrate a family of propositional formulas in conjunctive normal form so that a formula of size $N$ requires size $2^{\\Omega(\\sqrt[7]{N/logN})}$ to refute using the tree-like OBDD refutation system of Atserias, Kolaitis and Vardi with respect to all variable orderings. All known symbolic quantifier elimination algorithms for satisfiability generate tree-like proofs when run on unsatisfiable CNFs, so this lower bound applies to the run-times of these algorithms. Furthermore, the lower bound generalizes earlier results on OBDD-based proofs of unsatisfiability in that it applies for all variable orderings, it applies when the clauses are processed according to an arbitrary schedule, and it applies when variables are eliminated via quantification.",
        "published": "2007-01-09T01:16:00Z",
        "link": "http://arxiv.org/abs/cs/0701054v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.2.2"
        ]
    },
    {
        "title": "A decision procedure for linear \"big O\" equations",
        "authors": [
            "Jeremy Avigad",
            "Kevin Donnelly"
        ],
        "summary": "Let $F$ be the set of functions from an infinite set, $S$, to an ordered ring, $R$. For $f$, $g$, and $h$ in $F$, the assertion $f = g + O(h)$ means that for some constant $C$, $|f(x) - g(x)| \\leq C |h(x)|$ for every $x$ in $S$. Let $L$ be the first-order language with variables ranging over such functions, symbols for $0, +, -, \\min, \\max$, and absolute value, and a ternary relation $f = g + O(h)$. We show that the set of quantifier-free formulas in this language that are valid in the intended class of interpretations is decidable, and does not depend on the underlying set, $S$, or the ordered ring, $R$. If $R$ is a subfield of the real numbers, we can add a constant 1 function, as well as multiplication by constants from any computable subfield. We obtain further decidability results for certain situations in which one adds symbols denoting the elements of a fixed sequence of functions of strictly increasing rates of growth.",
        "published": "2007-01-10T19:46:21Z",
        "link": "http://arxiv.org/abs/cs/0701073v1",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Time-complexity semantics for feasible affine recursions (extended   abstract)",
        "authors": [
            "Norman Danner",
            "James S. Royer"
        ],
        "summary": "The authors' ATR programming formalism is a version of call-by-value PCF under a complexity-theoretically motivated type system. ATR programs run in type-2 polynomial-time and all standard type-2 basic feasible functionals are ATR-definable (ATR types are confined to levels 0, 1, and 2). A limitation of the original version of ATR is that the only directly expressible recursions are tail-recursions. Here we extend ATR so that a broad range of affine recursions are directly expressible. In particular, the revised ATR can fairly naturally express the classic insertion- and selection-sort algorithms, thus overcoming a sticking point of most prior implicit-complexity-based formalisms. The paper's main work is in extending and simplifying the original time-complexity semantics for ATR to develop a set of tools for extracting and solving the higher-type recurrences arising from feasible affine recursions.",
        "published": "2007-01-11T16:22:11Z",
        "link": "http://arxiv.org/abs/cs/0701076v2",
        "categories": [
            "cs.LO",
            "F.3.3; F.1.3"
        ]
    },
    {
        "title": "Recurrence with affine level mappings is P-time decidable for CLP(R)",
        "authors": [
            "Fred Mesnard",
            "Alexander Serebrenik"
        ],
        "summary": "In this paper we introduce a class of constraint logic programs such that their termination can be proved by using affine level mappings. We show that membership to this class is decidable in polynomial time.",
        "published": "2007-01-12T18:43:48Z",
        "link": "http://arxiv.org/abs/cs/0701082v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; F.3.2"
        ]
    },
    {
        "title": "A Theory and Calculus for Reasoning about Sequential Behavior",
        "authors": [
            "Frederick Furtek"
        ],
        "summary": "Basic results in combinatorial mathematics provide the foundation for a theory and calculus for reasoning about sequential behavior. A key concept of the theory is a generalization of Boolean implicant which deals with statements of the form:   A sequence of Boolean expressions alpha is an implicant of a set of sequences of Boolean expressions A   This notion of a generalized implicant takes on special significance when each of the sequences in the set A describes a disallowed pattern of behavior. That is because a disallowed sequence of Boolean expressions represents a logical/temporal dependency, and because the implicants of a set of disallowed Boolean sequences A are themselves disallowed and represent precisely those dependencies that follow as a logical consequence from the dependencies represented by A. The main result of the theory is a necessary and sufficient condition for a sequence of Boolean expressions to be an implicant of a regular set of sequences of Boolean expressions. This result is the foundation for two new proof methods. Sequential resolution is a generalization of Boolean resolution which allows new logical/temporal dependencies to be inferred from existing dependencies. Normalization starts with a model (system) and a set of logical/temporal dependencies and determines which of those dependencies are satisfied by the model.",
        "published": "2007-01-15T18:57:14Z",
        "link": "http://arxiv.org/abs/cs/0701088v2",
        "categories": [
            "cs.LO",
            "cs.DM",
            "F.4.1; D.2.4; G.2.1"
        ]
    },
    {
        "title": "Propositional theories are strongly equivalent to logic programs",
        "authors": [
            "Pedro Cabalar",
            "Paolo Ferraris"
        ],
        "summary": "This paper presents a property of propositional theories under the answer sets semantics (called Equilibrium Logic for this general syntax): any theory can always be reexpressed as a strongly equivalent disjunctive logic program, possibly with negation in the head. We provide two different proofs for this result: one involving a syntactic transformation, and one that constructs a program starting from the countermodels of the theory in the intermediate logic of here-and-there.",
        "published": "2007-01-16T12:29:55Z",
        "link": "http://arxiv.org/abs/cs/0701095v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "A presentation of Quantum Logic based on an \"and then\" connective",
        "authors": [
            "Daniel Lehmann"
        ],
        "summary": "When a physicist performs a quantic measurement, new information about the system at hand is gathered. This paper studies the logical properties of how this new information is combined with previous information. It presents Quantum Logic as a propositional logic under two connectives: negation and the \"and then\" operation that combines old and new information. The \"and then\" connective is neither commutative nor associative. Many properties of this logic are exhibited, and some small elegant subset is shown to imply all the properties considered. No independence or completeness result is claimed. Classical physical systems are exactly characterized by the commutativity, the associativity, or the monotonicity of the \"and then\" connective. Entailment is defined in this logic and can be proved to be a partial order. In orthomodular lattices, the operation proposed by Finch (1969) satisfies all the properties studied in this paper. All properties satisfied by Finch's operation in modular lattices are valid in Hilbert Space Quantum Logic. It is not known whether all properties of Hilbert Space Quantum Logic are satisfied by Finch's operation in modular lattices. Non-commutative, non-associative algebraic structures generalizing Boolean algebras are defined, ideals are characterized and a homomorphism theorem is proved.",
        "published": "2007-01-16T17:55:20Z",
        "link": "http://arxiv.org/abs/quant-ph/0701113v1",
        "categories": [
            "quant-ph",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "On factorisation forests",
        "authors": [
            "Thomas Colcombet"
        ],
        "summary": "The theorem of factorisation forests shows the existence of nested factorisations -- a la Ramsey -- for finite words. This theorem has important applications in semigroup theory, and beyond. The purpose of this paper is to illustrate the importance of this approach in the context of automata over infinite words and trees. We extend the theorem of factorisation forest in two directions: we show that it is still valid for any word indexed by a linear ordering; and we show that it admits a deterministic variant for words indexed by well-orderings. A byproduct of this work is also an improvement on the known bounds for the original result. We apply the first variant for giving a simplified proof of the closure under complementation of rational sets of words indexed by countable scattered linear orderings. We apply the second variant in the analysis of monadic second-order logic over trees, yielding new results on monadic interpretations over trees. Consequences of it are new caracterisations of prefix-recognizable structures and of the Caucal hierarchy.",
        "published": "2007-01-17T16:48:28Z",
        "link": "http://arxiv.org/abs/cs/0701113v1",
        "categories": [
            "cs.LO",
            "F.4"
        ]
    },
    {
        "title": "Real-Time Model-Checking: Parameters everywhere",
        "authors": [
            "Veronique Bruyere",
            "Jean-Francois Raskin"
        ],
        "summary": "In this paper, we study the model-checking and parameter synthesis problems of the logic TCTL over discrete-timed automata where parameters are allowed both in the model (timed automaton) and in the property (temporal formula). Our results are as follows. On the negative side, we show that the model-checking problem of TCTL extended with parameters is undecidable over discrete-timed automata with only one parametric clock. The undecidability result needs equality in the logic. On the positive side, we show that the model-checking and the parameter synthesis problems become decidable for a fragment of the logic where equality is not allowed. Our method is based on automata theoretic principles and an extension of our method to express durations of runs in timed automata using Presburger arithmetic.",
        "published": "2007-01-22T15:20:20Z",
        "link": "http://arxiv.org/abs/cs/0701138v2",
        "categories": [
            "cs.LO",
            "F.1.1"
        ]
    },
    {
        "title": "Turning the Liar paradox into a metatheorem of Basic logic",
        "authors": [
            "Paola A. Zizzi"
        ],
        "summary": "We show that self-reference can be formalized in Basic logic by means of the new connective @, called \"entanglement\". In fact, the property of non-idempotence of the connective @ is a metatheorem, which states that a self-entangled sentence loses its own identity. This prevents having self-referential paradoxes in the corresponding metalanguage. In this context, we introduce a generalized definition of self-reference, which is needed to deal with the multiplicative connectives of substructural logics.",
        "published": "2007-01-23T18:36:47Z",
        "link": "http://arxiv.org/abs/quant-ph/0701171v2",
        "categories": [
            "quant-ph",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "Logic Meets Algebra: the Case of Regular Languages",
        "authors": [
            "Pascal Tesson",
            "Denis Therien"
        ],
        "summary": "The study of finite automata and regular languages is a privileged meeting point of algebra and logic. Since the work of Buchi, regular languages have been classified according to their descriptive complexity, i.e. the type of logical formalism required to define them. The algebraic point of view on automata is an essential complement of this classification: by providing alternative, algebraic characterizations for the classes, it often yields the only opportunity for the design of algorithms that decide expressibility in some logical fragment.   We survey the existing results relating the expressibility of regular languages in logical fragments of MSO[S] with algebraic properties of their minimal automata. In particular, we show that many of the best known results in this area share the same underlying mechanics and rely on a very strong relation between logical substitutions and block-products of pseudovarieties of monoid. We also explain the impact of these connections on circuit complexity theory.",
        "published": "2007-01-25T20:46:36Z",
        "link": "http://arxiv.org/abs/cs/0701154v2",
        "categories": [
            "cs.LO",
            "D.3.1; F.1.1; F.1.3; F.4.1; F.4.3"
        ]
    },
    {
        "title": "Deterministic modal Bayesian Logic: derive the Bayesian inference within   the modal logic T",
        "authors": [
            "Frederic Dambreville"
        ],
        "summary": "In this paper a conditional logic is defined and studied. This conditional logic, DmBL, is constructed as a deterministic counterpart to the Bayesian conditional. The logic is unrestricted, so that any logical operations are allowed. A notion of logical independence is also defined within the logic itself. This logic is shown to be non-trivial and is not reduced to classical propositions. A model is constructed for the logic. Completeness results are proved. It is shown that any unconditioned probability can be extended to the whole logic DmBL. The Bayesian conditional is then recovered from the probabilistic DmBL. At last, it is shown why DmBL is compliant with Lewis' triviality.",
        "published": "2007-01-28T08:57:01Z",
        "link": "http://arxiv.org/abs/math/0701801v1",
        "categories": [
            "math.LO",
            "cs.LO",
            "math.PR"
        ]
    },
    {
        "title": "Noncomputable Spectral Sets",
        "authors": [
            "Jason Teutsch"
        ],
        "summary": "It is possible to enumerate all computer programs. In particular, for every partial computable function, there is a shortest program which computes that function. f-MIN is the set of indices for shortest programs. In 1972, Meyer showed that f-MIN is Turing equivalent to 0'', the halting set with halting set oracle.   This paper generalizes the notion of shortest programs, and we use various measures from computability theory to describe the complexity of the resulting \"spectral sets.\" We show that under certain Godel numberings, the spectral sets are exactly the canonical sets 0', 0'', 0''', ... up to Turing equivalence. This is probably not true in general, however we show that spectral sets always contain some useful information. We show that immunity, or \"thinness\" is a useful characteristic for distinguishing between spectral sets.   In the final chapter, we construct a set which neither contains nor is disjoint from any infinite arithmetic set, yet it is 0-majorized and contains a natural spectral set. Thus a pathological set becomes a bit more friendly. Finally, a number of interesting open problems are left for the inspired reader.",
        "published": "2007-01-31T02:38:37Z",
        "link": "http://arxiv.org/abs/math/0701904v1",
        "categories": [
            "math.LO",
            "cs.LO",
            "03D15; 03D45"
        ]
    },
    {
        "title": "Dealing With Logical Omniscience: Expressiveness and Pragmatics",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "summary": "We examine four approaches for dealing with the logical omniscience problem and their potential applicability: the syntactic approach, awareness, algorithmic knowledge, and impossible possible worlds. Although in some settings these approaches are equi-expressive and can capture all epistemic states, in other settings of interest (especially with probability in the picture), we show that they are not equi-expressive. We then consider the pragmatics of dealing with logical omniscience-- how to choose an approach and construct an appropriate model.",
        "published": "2007-02-01T20:06:31Z",
        "link": "http://arxiv.org/abs/cs/0702011v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "A multivariate interlace polynomial",
        "authors": [
            "Bruno Courcelle"
        ],
        "summary": "We define a multivariate polynomial that generalizes several interlace polynomials defined by Arratia, Bollobas and Sorkin on the one hand, and Aigner and van der Holst on the other. We follow the route traced by Sokal, who defined a multivariate generalization of Tutte's polynomial. We also show that bounded portions of our interlace polynomial can be evaluated in polynomial time for graphs of bounded clique-width. Its full evaluation is necessarly exponential just because of the size of the result.",
        "published": "2007-02-02T12:38:50Z",
        "link": "http://arxiv.org/abs/cs/0702016v2",
        "categories": [
            "cs.LO",
            "cs.DM"
        ]
    },
    {
        "title": "The Suspension Calculus and its Relationship to Other Explicit   Treatments of Substitution in Lambda Calculi",
        "authors": [
            "Andrew Gacek"
        ],
        "summary": "The intrinsic treatment of binding in the lambda calculus makes it an ideal data structure for representing syntactic objects with binding such as formulas, proofs, types, and programs. Supporting such a data structure in an implementation is made difficult by the complexity of the substitution operation relative to lambda terms. In this paper we present the suspension calculus, an explicit treatment of meta level binding in the lambda calculus. We prove properties of this calculus which make it a suitable replacement for the lambda calculus in implementation. Finally, we compare the suspension calculus with other explicit treatments of substitution.",
        "published": "2007-02-05T14:35:01Z",
        "link": "http://arxiv.org/abs/cs/0702027v1",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Efficient First-Order Temporal Logic for Infinite-State Systems",
        "authors": [
            "Clare Dixon",
            "Michael Fisher",
            "Boris Konev",
            "Alexei Lisitsa"
        ],
        "summary": "In this paper we consider the specification and verification of infinite-state systems using temporal logic. In particular, we describe parameterised systems using a new variety of first-order temporal logic that is both powerful enough for this form of specification and tractable enough for practical deductive verification. Importantly, the power of the temporal language allows us to describe (and verify) asynchronous systems, communication delays and more complex properties such as liveness and fairness properties. These aspects appear difficult for many other approaches to infinite-state verification.",
        "published": "2007-02-06T15:10:51Z",
        "link": "http://arxiv.org/abs/cs/0702036v1",
        "categories": [
            "cs.LO",
            "F.4.1; F.3.1; D.2.2; D.2.4"
        ]
    },
    {
        "title": "Polygraphs for termination of left-linear term rewriting systems",
        "authors": [
            "Yves Guiraud"
        ],
        "summary": "We present a methodology for proving termination of left-linear term rewriting systems (TRSs) by using Albert Burroni's polygraphs, a kind of rewriting systems on algebraic circuits. We translate the considered TRS into a polygraph of minimal size whose termination is proven with a polygraphic interpretation, then we get back the property on the TRS. We recall Yves Lafont's general translation of TRSs into polygraphs and known links between their termination properties. We give several conditions on the original TRS, including being a first-order functional program, that ensure that we can reduce the size of the polygraphic translation. We also prove sufficient conditions on the polygraphic interpretations of a minimal translation to imply termination of the original TRS. Examples are given to compare this method with usual polynomial interpretations.",
        "published": "2007-02-07T10:18:09Z",
        "link": "http://arxiv.org/abs/cs/0702040v1",
        "categories": [
            "cs.LO",
            "math.CT",
            "F.4"
        ]
    },
    {
        "title": "The Fibers and Range of Reduction Graphs in Ciliates",
        "authors": [
            "Robert Brijder",
            "Hendrik Jan Hoogeboom"
        ],
        "summary": "The biological process of gene assembly has been modeled based on three types of string rewriting rules, called string pointer rules, defined on so-called legal strings. It has been shown that reduction graphs, graphs that are based on the notion of breakpoint graph in the theory of sorting by reversal, for legal strings provide valuable insights into the gene assembly process. We characterize which legal strings obtain the same reduction graph (up to isomorphism), and moreover we characterize which graphs are (isomorphic to) reduction graphs.",
        "published": "2007-02-07T12:59:22Z",
        "link": "http://arxiv.org/abs/cs/0702041v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Feasible reactivity in a synchronous pi-calculus",
        "authors": [
            "Roberto Amadio",
            "Frederique Dabrowski"
        ],
        "summary": "Reactivity is an essential property of a synchronous program. Informally, it guarantees that at each instant the program fed with an input will `react' producing an output. In the present work, we consider a refined property that we call ` feasible reactivity'. Beyond reactivity, this property guarantees that at each instant both the size of the program and its reaction time are bounded by a polynomial in the size of the parameters at the beginning of the computation and the size of the largest input. We propose a method to annotate programs and we develop related static analysis techniques that guarantee feasible reactivity for programs expressed in the S-pi-calculus. The latter is a synchronous version of the pi-calculus based on the SL synchronous programming model.",
        "published": "2007-02-11T15:43:26Z",
        "link": "http://arxiv.org/abs/cs/0702069v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "An Example of Pi^0_3-complete Infinitary Rational Relation",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We give in this paper an example of infinitary rational relation, accepted by a 2-tape B\\\"{u}chi automaton, which is Pi^0_3-complete in the Borel hierarchy. Moreover the example of infinitary rational relation given in this paper has a very simple structure and can be easily described by its sections.",
        "published": "2007-02-12T16:11:46Z",
        "link": "http://arxiv.org/abs/math/0702334v1",
        "categories": [
            "math.LO",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Mapping the Object-Role Modeling language ORM2 into Description Logic   language DLRifd",
        "authors": [
            "C. Maria Keet"
        ],
        "summary": "In recent years, several efforts have been made to enhance conceptual data modelling with automated reasoning to improve the model's quality and derive implicit information. One approach to achieve this in implementations, is to constrain the language. Advances in Description Logics can help choosing the right language to have greatest expressiveness yet to remain within the decidable fragment of first order logic to realise a workable implementation with good performance using DL reasoners. The best fit DL language appears to be the ExpTime-complete DLRifd. To illustrate trade-offs and highlight features of the modelling languages, we present a precise transformation of the mappable features of the very expressive (undecidable) ORM/ORM2 conceptual data modelling languages to exactly DLRifd. Although not all ORM2 features can be mapped, this is an interesting fragment because it has been shown that DLRifd can also encode UML Class Diagrams and EER, and therefore can foster interoperation between conceptual data models and research into ontological aspects of the modelling languages.",
        "published": "2007-02-15T23:45:46Z",
        "link": "http://arxiv.org/abs/cs/0702089v2",
        "categories": [
            "cs.LO",
            "I.2.4"
        ]
    },
    {
        "title": "The Bedwyr system for model checking over syntactic expressions",
        "authors": [
            "David Baelde",
            "Andrew Gacek",
            "Dale Miller",
            "Gopalan Nadathur",
            "Alwen Tiu"
        ],
        "summary": "Bedwyr is a generalization of logic programming that allows model checking directly on syntactic expressions possibly containing bindings. This system, written in OCaml, is a direct implementation of two recent advances in the theory of proof search. The first is centered on the fact that both finite success and finite failure can be captured in the sequent calculus by incorporating inference rules for definitions that allow fixed points to be explored. As a result, proof search in such a sequent calculus can capture simple model checking problems as well as may and must behavior in operational semantics. The second is that higher-order abstract syntax is directly supported using term-level $\\lambda$-binders and the $\\nabla$ quantifier. These features allow reasoning directly on expressions containing bound variables.",
        "published": "2007-02-20T10:08:24Z",
        "link": "http://arxiv.org/abs/cs/0702116v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "On the decidability and complexity of Metric Temporal Logic over finite   words",
        "authors": [
            "Joel Ouaknine",
            "James Worrell"
        ],
        "summary": "Metric Temporal Logic (MTL) is a prominent specification formalism for real-time systems. In this paper, we show that the satisfiability problem for MTL over finite timed words is decidable, with non-primitive recursive complexity. We also consider the model-checking problem for MTL: whether all words accepted by a given Alur-Dill timed automaton satisfy a given MTL formula. We show that this problem is decidable over finite words. Over infinite words, we show that model checking the safety fragment of MTL--which includes invariance and time-bounded response properties--is also decidable. These results are quite surprising in that they contradict various claims to the contrary that have appeared in the literature.",
        "published": "2007-02-21T16:14:34Z",
        "link": "http://arxiv.org/abs/cs/0702120v3",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "A Simplified Suspension Calculus and its Relationship to Other Explicit   Substitution Calculi",
        "authors": [
            "Andrew Gacek",
            "Gopalan Nadathur"
        ],
        "summary": "This paper concerns the explicit treatment of substitutions in the lambda calculus. One of its contributions is the simplification and rationalization of the suspension calculus that embodies such a treatment. The earlier version of this calculus provides a cumbersome encoding of substitution composition, an operation that is important to the efficient realization of reduction. This encoding is simplified here, resulting in a treatment that is easy to use directly in applications. The rationalization consists of the elimination of a practically inconsequential flexibility in the unravelling of substitutions that has the inadvertent side effect of losing contextual information in terms; the modified calculus now has a structure that naturally supports logical analyses, such as ones related to the assignment of types, over lambda terms. The overall calculus is shown to have pleasing theoretical properties such as a strongly terminating sub-calculus for substitution and confluence even in the presence of term meta variables that are accorded a grafting interpretation. Another contribution of the paper is the identification of a broad set of properties that are desirable for explicit substitution calculi to support and a classification of a variety of proposed systems based on these. The suspension calculus is used as a tool in this study. In particular, mappings are described between it and the other calculi towards understanding the characteristics of the latter.",
        "published": "2007-02-26T02:16:57Z",
        "link": "http://arxiv.org/abs/cs/0702152v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Bistable Biorders: A Sequential Domain Theory",
        "authors": [
            "James Laird"
        ],
        "summary": "We give a simple order-theoretic construction of a Cartesian closed category of sequential functions. It is based on bistable biorders, which are sets with a partial order -- the extensional order -- and a bistable coherence, which captures equivalence of program behaviour, up to permutation of top (error) and bottom (divergence). We show that monotone and bistable functions (which are required to preserve bistably bounded meets and joins) are strongly sequential, and use this fact to prove universality results for the bistable biorder semantics of the simply-typed lambda-calculus (with atomic constants), and an extension with arithmetic and recursion.   We also construct a bistable model of SPCF, a higher-order functional programming language with non-local control. We use our universality result for the lambda-calculus to show that the semantics of SPCF is fully abstract. We then establish a direct correspondence between bistable functions and sequential algorithms by showing that sequential data structures give rise to bistable biorders, and that each bistable function between such biorders is computed by a sequential algorithm.",
        "published": "2007-02-28T14:57:27Z",
        "link": "http://arxiv.org/abs/cs/0702169v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "How Overlap Determines the Macronuclear Genes in Ciliates",
        "authors": [
            "Robert Brijder",
            "Hendrik Jan Hoogeboom",
            "Grzegorz Rozenberg"
        ],
        "summary": "Formal models for gene assembly in ciliates have been developed, in particular the string pointer reduction system (SPRS) and the graph pointer reduction system (GPRS). The reduction graph is a valuable tool within the SPRS, revealing much information about how gene assembly is performed for a given gene. The GPRS is more abstract than the SPRS and not all information present in the SPRS is retained in the GPRS. As a consequence the reduction graph cannot be defined for the GPRS in general, but we show that it can be defined (in an equivalent manner as defined for the SPRS) if we restrict ourselves to so-called realistic overlap graphs. Fortunately, only these graphs correspond to genes occurring in nature. Defining the reduction graph within the GPRS allows one to carry over several results within the SPRS that rely on the reduction graph.",
        "published": "2007-02-28T15:51:26Z",
        "link": "http://arxiv.org/abs/cs/0702171v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Intensional properties of polygraphs",
        "authors": [
            "Guillaume Bonfante",
            "Yves Guiraud"
        ],
        "summary": "We present polygraphic programs, a subclass of Albert Burroni's polygraphs, as a computational model, showing how these objects can be seen as first-order functional programs. We prove that the model is Turing complete. We use polygraphic interpretations, a termination proof method introduced by the second author, to characterize polygraphic programs that compute in polynomial time. We conclude with a characterization of polynomial time functions and non-deterministic polynomial time functions.",
        "published": "2007-03-02T09:21:20Z",
        "link": "http://arxiv.org/abs/cs/0703007v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.CT",
            "F.1.1; F.4"
        ]
    },
    {
        "title": "Graph representation of context-free grammars",
        "authors": [
            "Alex Shkotin"
        ],
        "summary": "In modern mathematics, graphs figure as one of the better-investigated class of mathematical objects. Various properties of graphs, as well as graph-processing algorithms, can be useful if graphs of a certain kind are used as denotations for CF-grammars. Furthermore, graph are well adapted to various extensions (one kind of such extensions being attributes).",
        "published": "2007-03-03T13:12:51Z",
        "link": "http://arxiv.org/abs/cs/0703015v1",
        "categories": [
            "cs.LO",
            "D.3.1; E.1"
        ]
    },
    {
        "title": "A Coding Theoretic Study on MLL proof nets",
        "authors": [
            "Satoshi Matsuoka"
        ],
        "summary": "Coding theory is very useful for real world applications. A notable example is digital television. Basically, coding theory is to study a way of detecting and/or correcting data that may be true or false. Moreover coding theory is an area of mathematics, in which there is an interplay between many branches of mathematics, e.g., abstract algebra, combinatorics, discrete geometry, information theory, etc. In this paper we propose a novel approach for analyzing proof nets of Multiplicative Linear Logic (MLL) by coding theory. We define families of proof structures and introduce a metric space for each family. In each family, 1. an MLL proof net is a true code element; 2. a proof structure that is not an MLL proof net is a false (or corrupted) code element. The definition of our metrics reflects the duality of the multiplicative connectives elegantly. In this paper we show that in the framework one error-detecting is possible but one error-correcting not. Our proof of the impossibility of one error-correcting is interesting in the sense that a proof theoretical property is proved using a graph theoretical argument. In addition, we show that affine logic and MLL + MIX are not appropriate for this framework. That explains why MLL is better than such similar logics.",
        "published": "2007-03-05T07:49:55Z",
        "link": "http://arxiv.org/abs/cs/0703018v24",
        "categories": [
            "cs.LO",
            "cs.DM"
        ]
    },
    {
        "title": "Transforming structures by set interpretations",
        "authors": [
            "Thomas Colcombet",
            "Christof Löding"
        ],
        "summary": "We consider a new kind of interpretation over relational structures: finite sets interpretations. Those interpretations are defined by weak monadic second-order (WMSO) formulas with free set variables. They transform a given structure into a structure with a domain consisting of finite sets of elements of the orignal structure. The definition of these interpretations directly implies that they send structures with a decidable WMSO theory to structures with a decidable first-order theory. In this paper, we investigate the expressive power of such interpretations applied to infinite deterministic trees. The results can be used in the study of automatic and tree-automatic structures.",
        "published": "2007-03-08T12:52:55Z",
        "link": "http://arxiv.org/abs/cs/0703039v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "An ExpTime Procedure for Description Logic $\\mathcal{ALCQI}$ (Draft)",
        "authors": [
            "Yu Ding"
        ],
        "summary": "A worst-case ExpTime tableau-based decision procedure is outlined for the satisfiability problem in $\\mathcal{ALCQI}$ w.r.t. general axioms.",
        "published": "2007-03-12T01:55:23Z",
        "link": "http://arxiv.org/abs/cs/0703051v3",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Automatic Structures: Richness and Limitations",
        "authors": [
            "Bakhadyr Khoussainov",
            "Andre Nies",
            "Sasha Rubin",
            "Frank Stephan"
        ],
        "summary": "We study the existence of automatic presentations for various algebraic structures. An automatic presentation of a structure is a description of the universe of the structure by a regular set of words, and the interpretation of the relations by synchronised automata. Our first topic concerns characterising classes of automatic structures. We supply a characterisation of the automatic Boolean algebras, and it is proven that the free Abelian group of infinite rank, as well as certain Fraisse limits, do not have automatic presentations. In particular, the countably infinite random graph and the random partial order do not have automatic presentations. Furthermore, no infinite integral domain is automatic. Our second topic is the isomorphism problem. We prove that the complexity of the isomorphism problem for the class of all automatic structures is \\Sigma_1^1-complete.",
        "published": "2007-03-13T09:38:05Z",
        "link": "http://arxiv.org/abs/cs/0703064v2",
        "categories": [
            "cs.DM",
            "cs.LO",
            "F.1.1; F.4.3"
        ]
    },
    {
        "title": "Automata with Nested Pebbles Capture First-Order Logic with Transitive   Closure",
        "authors": [
            "Joost Engelfriet",
            "Hendrik Jan Hoogeboom"
        ],
        "summary": "String languages recognizable in (deterministic) log-space are characterized either by two-way (deterministic) multi-head automata, or following Immerman, by first-order logic with (deterministic) transitive closure. Here we elaborate this result, and match the number of heads to the arity of the transitive closure. More precisely, first-order logic with k-ary deterministic transitive closure has the same power as deterministic automata walking on their input with k heads, additionally using a finite set of nested pebbles. This result is valid for strings, ordered trees, and in general for families of graphs having a fixed automaton that can be used to traverse the nodes of each of the graphs in the family. Other examples of such families are grids, toruses, and rectangular mazes. For nondeterministic automata, the logic is restricted to positive occurrences of transitive closure.   The special case of k=1 for trees, shows that single-head deterministic tree-walking automata with nested pebbles are characterized by first-order logic with unary deterministic transitive closure. This refines our earlier result that placed these automata between first-order and monadic second-order logic on trees.",
        "published": "2007-03-15T10:27:05Z",
        "link": "http://arxiv.org/abs/cs/0703079v2",
        "categories": [
            "cs.LO",
            "F.1.1; F.4.1; F.4.3"
        ]
    },
    {
        "title": "Polynomial time algorithm for 3-SAT. Examples of use",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "The algorithm checks the propositional formulas for patterns of unsatisfiability.",
        "published": "2007-03-21T06:46:09Z",
        "link": "http://arxiv.org/abs/cs/0703098v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.LO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "On the Design of Generic Static Analyzers for Modern Imperative   Languages",
        "authors": [
            "Roberto Bagnara",
            "Patricia M. Hill",
            "Andrea Pescetti",
            "Enea Zaffanella"
        ],
        "summary": "The design and implementation of precise static analyzers for significant fragments of modern imperative languages like C, C++, Java and Python is a challenging problem. In this paper, we consider a core imperative language that has several features found in mainstream languages such as those including recursive functions, run-time system and user-defined exceptions, and a realistic data and memory model. For this language we provide a concrete semantics --characterizing both finite and infinite computations-- and a generic abstract semantics that we prove sound with respect to the concrete one. We say the abstract semantics is generic since it is designed to be completely parametric on the analysis domains: in particular, it provides support for \\emph{relational} domains (i.e., abstract domains that can capture the relationships between different data objects). We also sketch how the proposed methodology can be extended to accommodate a larger language that includes pointers, compound data objects and non-structured control flow mechanisms. The approach, which is based on structured, big-step $\\mathrm{G}^\\infty\\mathrm{SOS}$ operational semantics and on abstract interpretation, is modular in that the overall static analyzer is naturally partitioned into components with clearly identified responsibilities and interfaces, something that greatly simplifies both the proof of correctness and the implementation.",
        "published": "2007-03-23T09:47:15Z",
        "link": "http://arxiv.org/abs/cs/0703116v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.1; F.3.2"
        ]
    },
    {
        "title": "The logic of message passing",
        "authors": [
            "J. R. B. Cockett",
            "Craig Pastro"
        ],
        "summary": "Message passing is a key ingredient of concurrent programming. The purpose of this paper is to describe the equivalence between the proof theory, the categorical semantics, and term calculus of message passing. In order to achieve this we introduce the categorical notion of a linear actegory and the related polycategorical notion of a poly-actegory. Not surprisingly the notation used for the term calculus borrows heavily from the (synchronous) pi-calculus. The cut elimination procedure for the system provides an operational semantics.",
        "published": "2007-03-23T21:17:42Z",
        "link": "http://arxiv.org/abs/math/0703713v3",
        "categories": [
            "math.CT",
            "cs.LO"
        ]
    },
    {
        "title": "A Polynomial Time Algorithm for SAT",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Article presents the compatibility matrix method and illustrates it with the application to P vs NP problem. The method is a generalization of descriptive geometry: in the method, we draft problems and solve them utilizing the image creation technique. The method reveals: P = NP = PSPACE",
        "published": "2007-03-29T07:36:30Z",
        "link": "http://arxiv.org/abs/cs/0703146v4",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.LO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Quantum Lambda Calculi with Classical Control: Syntax and Expressive   Power",
        "authors": [
            "Ugo Dal Lago",
            "Andrea Masini",
            "Margherita Zorzi"
        ],
        "summary": "We study an untyped lambda calculus with quantum data and classical control. This work stems from previous proposals by Selinger and Valiron and by Van Tonder. We focus on syntax and expressiveness, rather than (denotational) semantics. We prove subject reduction, confluence and a standardization theorem. Moreover, we prove the computational equivalence of the proposed calculus with a suitable class of quantum circuit families.",
        "published": "2007-03-30T08:59:43Z",
        "link": "http://arxiv.org/abs/cs/0703152v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "On Almost Periodicity Criteria for Morphic Sequences in Some Particular   Cases",
        "authors": [
            "Yuri Pritykin"
        ],
        "summary": "In some particular cases we give criteria for morphic sequences to be almost periodic (=uniformly recurrent). Namely, we deal with fixed points of non-erasing morphisms and with automatic sequences. In both cases a polynomial-time algorithm solving the problem is found. A result more or less supporting the conjecture of decidability of the general problem is given.",
        "published": "2007-04-02T15:45:37Z",
        "link": "http://arxiv.org/abs/0704.0218v1",
        "categories": [
            "cs.DM",
            "cs.LO",
            "G.2.1; F.2.2; F.4.3"
        ]
    },
    {
        "title": "A Cut-free Sequent Calculus for Bi-Intuitionistic Logic: Extended   Version",
        "authors": [
            "Linda Buisman",
            "Rajeev Goré"
        ],
        "summary": "Bi-intuitionistic logic is the extension of intuitionistic logic with a connective dual to implication. Bi-intuitionistic logic was introduced by Rauszer as a Hilbert calculus with algebraic and Kripke semantics. But her subsequent ``cut-free'' sequent calculus for BiInt has recently been shown by Uustalu to fail cut-elimination. We present a new cut-free sequent calculus for BiInt, and prove it sound and complete with respect to its Kripke semantics. Ensuring completeness is complicated by the interaction between implication and its dual, similarly to future and past modalities in tense logic. Our calculus handles this interaction using extended sequents which pass information from premises to conclusions using variables instantiated at the leaves of failed derivation trees. Our simple termination argument allows our calculus to be used for automated deduction, although this is not its main purpose.",
        "published": "2007-04-13T07:29:31Z",
        "link": "http://arxiv.org/abs/0704.1707v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Unicast and Multicast Qos Routing with Soft Constraint Logic Programming",
        "authors": [
            "Stefano Bistarelli",
            "Ugo Montanari",
            "Francesca Rossi",
            "Francesco Santini"
        ],
        "summary": "We present a formal model to represent and solve the unicast/multicast routing problem in networks with Quality of Service (QoS) requirements. To attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different QoS metric value (e.g. bandwidth, cost, delay, packet loss). The second step consists in writing this graph as a program in Soft Constraint Logic Programming (SCLP): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to QoS routing problems. Moreover, c-semiring structures are a convenient tool to model QoS metrics. At last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.",
        "published": "2007-04-13T15:53:44Z",
        "link": "http://arxiv.org/abs/0704.1783v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.NI",
            "D.3.2; D.3.3; C.2.3; F.4.1"
        ]
    },
    {
        "title": "An algebraic generalization of Kripke structures",
        "authors": [
            "Sérgio Marcelino",
            "Pedro Resende"
        ],
        "summary": "The Kripke semantics of classical propositional normal modal logic is made algebraic via an embedding of Kripke structures into the larger class of pointed stably supported quantales. This algebraic semantics subsumes the traditional algebraic semantics based on lattices with unary operators, and it suggests natural interpretations of modal logic, of possible interest in the applications, in structures that arise in geometry and analysis, such as foliated manifolds and operator algebras, via topological groupoids and inverse semigroups. We study completeness properties of the quantale based semantics for the systems K, T, K4, S4, and S5, in particular obtaining an axiomatization for S5 which does not use negation or the modal necessity operator. As additional examples we describe intuitionistic propositional modal logic, the logic of programs PDL, and the ramified temporal logic CTL.",
        "published": "2007-04-14T19:59:18Z",
        "link": "http://arxiv.org/abs/0704.1886v1",
        "categories": [
            "math.LO",
            "cs.LO",
            "math.RA",
            "03B45 (Primary) 03G25, 06F07 (Secondary)"
        ]
    },
    {
        "title": "Light Logics and Optimal Reduction: Completeness and Complexity",
        "authors": [
            "Patrick Baillot",
            "Paolo Coppola",
            "Ugo Dal Lago"
        ],
        "summary": "Typing of lambda-terms in Elementary and Light Affine Logic (EAL, LAL, resp.) has been studied for two different reasons: on the one hand the evaluation of typed terms using LAL (EAL, resp.) proof-nets admits a guaranteed polynomial (elementary, resp.) bound; on the other hand these terms can also be evaluated by optimal reduction using the abstract version of Lamping's algorithm. The first reduction is global while the second one is local and asynchronous. We prove that for LAL (EAL, resp.) typed terms, Lamping's abstract algorithm also admits a polynomial (elementary, resp.) bound. We also show its soundness and completeness (for EAL and LAL with type fixpoints), by using a simple geometry of interaction model (context semantics).",
        "published": "2007-04-19T01:17:29Z",
        "link": "http://arxiv.org/abs/0704.2448v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.4.1"
        ]
    },
    {
        "title": "Higher-order theories",
        "authors": [
            "Andre' Hirschowitz",
            "Marco Maggesi"
        ],
        "summary": "We extend our approach to abstract syntax (with binding constructions) through modules and linearity. First we give a new general definition of arity, yielding the companion notion of signature. Then we obtain a modularity result as requested by Ghani and Uustalu (2003): in our setting, merging two extensions of syntax corresponds to building an amalgamated sum. Finally we define a natural notion of equation concerning a signature and prove the existence of an initial semantics for a so-called representable signature equipped with a set of equations.",
        "published": "2007-04-22T18:19:46Z",
        "link": "http://arxiv.org/abs/0704.2900v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Alternative axiomatics and complexity of deliberative STIT theories",
        "authors": [
            "Philippe Balbiani",
            "Andreas Herzig",
            "Nicolas Troquard"
        ],
        "summary": "We propose two alternatives to Xu's axiomatization of the Chellas STIT. The first one also provides an alternative axiomatization of the deliberative STIT. The second one starts from the idea that the historic necessity operator can be defined as an abbreviation of operators of agency, and can thus be eliminated from the logic of the Chellas STIT. The second axiomatization also allows us to establish that the problem of deciding the satisfiability of a STIT formula without temporal operators is NP-complete in the single-agent case, and is NEXPTIME-complete in the multiagent case, both for the deliberative and the Chellas' STIT.",
        "published": "2007-04-24T16:36:13Z",
        "link": "http://arxiv.org/abs/0704.3238v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "The Complexity of Model Checking Higher-Order Fixpoint Logic",
        "authors": [
            "Roland Axelsson",
            "Martin Lange",
            "Rafal Somla"
        ],
        "summary": "Higher-Order Fixpoint Logic (HFL) is a hybrid of the simply typed \\lambda-calculus and the modal \\lambda-calculus. This makes it a highly expressive temporal logic that is capable of expressing various interesting correctness properties of programs that are not expressible in the modal \\lambda-calculus.   This paper provides complexity results for its model checking problem. In particular we consider those fragments of HFL built by using only types of bounded order k and arity m. We establish k-fold exponential time completeness for model checking each such fragment. For the upper bound we use fixpoint elimination to obtain reachability games that are singly-exponential in the size of the formula and k-fold exponential in the size of the underlying transition system. These games can be solved in deterministic linear time. As a simple consequence, we obtain an exponential time upper bound on the expression complexity of each such fragment.   The lower bound is established by a reduction from the word problem for alternating (k-1)-fold exponential space bounded Turing Machines. Since there are fixed machines of that type whose word problems are already hard with respect to k-fold exponential time, we obtain, as a corollary, k-fold exponential time completeness for the data complexity of our fragments of HFL, provided m exceeds 3. This also yields a hierarchy result in expressive power.",
        "published": "2007-04-30T13:09:20Z",
        "link": "http://arxiv.org/abs/0704.3931v2",
        "categories": [
            "cs.LO",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Undirected Graphs of Entanglement Two",
        "authors": [
            "Walid Belkhir",
            "Luigi Santocanale"
        ],
        "summary": "Entanglement is a complexity measure of directed graphs that origins in fixed point theory. This measure has shown its use in designing efficient algorithms to verify logical properties of transition systems. We are interested in the problem of deciding whether a graph has entanglement at most k. As this measure is defined by means of games, game theoretic ideas naturally lead to design polynomial algorithms that, for fixed k, decide the problem. Known characterizations of directed graphs of entanglement at most 1 lead, for k = 1, to design even faster algorithms. In this paper we present an explicit characterization of undirected graphs of entanglement at most 2. With such a characterization at hand, we devise a linear time algorithm to decide whether an undirected graph has this property.",
        "published": "2007-05-03T08:07:11Z",
        "link": "http://arxiv.org/abs/0705.0419v2",
        "categories": [
            "cs.LO",
            "cs.GT"
        ]
    },
    {
        "title": "Logic Column 18: Alternative Logics: A Book Review",
        "authors": [
            "Riccardo Pucella"
        ],
        "summary": "This article discusses two books on the topic of alternative logics in science: \"Deviant Logic\", by Susan Haack, and \"Alternative Logics: Do Sciences Need Them?\", edited by Paul Weingartner.",
        "published": "2007-05-09T21:56:15Z",
        "link": "http://arxiv.org/abs/0705.1367v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A first-order Temporal Logic for Actions",
        "authors": [
            "Camilla Schwind"
        ],
        "summary": "We present a multi-modal action logic with first-order modalities, which contain terms which can be unified with the terms inside the subsequent formulas and which can be quantified. This makes it possible to handle simultaneously time and states. We discuss applications of this language to action theory where it is possible to express many temporal aspects of actions, as for example, beginning, end, time points, delayed preconditions and results, duration and many others. We present tableaux rules for a decidable fragment of this logic.",
        "published": "2007-05-14T18:36:25Z",
        "link": "http://arxiv.org/abs/0705.1999v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "From Nondeterministic Büchi and Streett Automata to Deterministic   Parity Automata",
        "authors": [
            "Nir Piterman"
        ],
        "summary": "In this paper we revisit Safra's determinization constructions for automata on infinite words. We show how to construct deterministic automata with fewer states and, most importantly, parity acceptance conditions. Determinization is used in numerous applications, such as reasoning about tree automata, satisfiability of CTL*, and realizability and synthesis of logical specifications. The upper bounds for all these applications are reduced by using the smaller deterministic automata produced by our construction. In addition, the parity acceptance conditions allows to use more efficient algorithms (when compared to handling Rabin or Streett acceptance conditions).",
        "published": "2007-05-15T18:08:15Z",
        "link": "http://arxiv.org/abs/0705.2205v2",
        "categories": [
            "cs.LO",
            "cs.FL",
            "F.1.1; F.4.3"
        ]
    },
    {
        "title": "On tractability and congruence distributivity",
        "authors": [
            "Emil Kiss",
            "Matthew Valeriote"
        ],
        "summary": "Constraint languages that arise from finite algebras have recently been the object of study, especially in connection with the Dichotomy Conjecture of Feder and Vardi. An important class of algebras are those that generate congruence distributive varieties and included among this class are lattices, and more generally, those algebras that have near-unanimity term operations. An algebra will generate a congruence distributive variety if and only if it has a sequence of ternary term operations, called Jonsson terms, that satisfy certain equations.   We prove that constraint languages consisting of relations that are invariant under a short sequence of Jonsson terms are tractable by showing that such languages have bounded relational width.",
        "published": "2007-05-15T20:01:32Z",
        "link": "http://arxiv.org/abs/0705.2229v2",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3; F.4.1"
        ]
    },
    {
        "title": "Computability of simple games: A characterization and application to the   core",
        "authors": [
            "Masahiro Kumabe",
            "H. Reiju Mihara"
        ],
        "summary": "The class of algorithmically computable simple games (i) includes the class of games that have finite carriers and (ii) is included in the class of games that have finite winning coalitions. This paper characterizes computable games, strengthens the earlier result that computable games violate anonymity, and gives examples showing that the above inclusions are strict. It also extends Nakamura's theorem about the nonemptyness of the core and shows that computable games have a finite Nakamura number, implying that the number of alternatives that the players can deal with rationally is restricted.",
        "published": "2007-05-22T17:49:15Z",
        "link": "http://arxiv.org/abs/0705.3227v2",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.LO",
            "math.LO",
            "F.4.1; F.1.1"
        ]
    },
    {
        "title": "Acyclicity of Preferences, Nash Equilibria, and Subgame Perfect   Equilibria: a Formal and Constructive Equivalence",
        "authors": [
            "Stéphane Le Roux"
        ],
        "summary": "In 1953, Kuhn showed that every sequential game has a Nash equilibrium by showing that a procedure, named ``backward induction'' in game theory, yields a Nash equilibrium. It actually yields Nash equilibria that define a proper subclass of Nash equilibria. In 1965, Selten named this proper subclass subgame perfect equilibria. In game theory, payoffs are rewards usually granted at the end of a game. Although traditional game theory mainly focuses on real-valued payoffs that are implicitly ordered by the usual total order over the reals, works of Simon or Blackwell already involved partially ordered payoffs. This paper generalises the notion of sequential game by replacing real-valued payoff functions with abstract atomic objects, called outcomes, and by replacing the usual total order over the reals with arbitrary binary relations over outcomes, called preferences. This introduces a general abstract formalism where Nash equilibrium, subgame perfect equilibrium, and ``backward induction'' can still be defined. This paper proves that the following three propositions are equivalent: 1) Preferences over the outcomes are acyclic. 2) Every sequential game has a Nash equilibrium. 3) Every sequential game has a subgame perfect equilibrium. The result is fully computer-certified using Coq. Beside the additional guarantee of correctness, the activity of formalisation using Coq also helps clearly identify the useful definitions and the main articulations of the proof.",
        "published": "2007-05-23T09:30:08Z",
        "link": "http://arxiv.org/abs/0705.3316v1",
        "categories": [
            "cs.DM",
            "cs.GT",
            "cs.LO"
        ]
    },
    {
        "title": "Making Random Choices Invisible to the Scheduler",
        "authors": [
            "Konstantinos Chatzikokolakis",
            "Catuscia Palamidessi"
        ],
        "summary": "When dealing with process calculi and automata which express both nondeterministic and probabilistic behavior, it is customary to introduce the notion of scheduler to solve the nondeterminism. It has been observed that for certain applications, notably those in security, the scheduler needs to be restricted so not to reveal the outcome of the protocol's random choices, or otherwise the model of adversary would be too strong even for ``obviously correct'' protocols. We propose a process-algebraic framework in which the control on the scheduler can be specified in syntactic terms, and we show how to apply it to solve the problem mentioned above. We also consider the definition of (probabilistic) may and must preorders, and we show that they are precongruences with respect to the restricted schedulers. Furthermore, we show that all the operators of the language, except replication, distribute over probabilistic summation, which is a useful property for verification.",
        "published": "2007-05-24T04:28:47Z",
        "link": "http://arxiv.org/abs/0705.3503v1",
        "categories": [
            "cs.CR",
            "cs.LO",
            "D.1.3; D.2.4; D.3.2; D.4.6; F.1.2; F.3.2; F.3.3"
        ]
    },
    {
        "title": "Generalizing Consistency and other Constraint Properties to Quantified   Constraints",
        "authors": [
            "Lucas Bordeaux",
            "Marco Cadoli",
            "Toni Mancini"
        ],
        "summary": "Quantified constraints and Quantified Boolean Formulae are typically much more difficult to reason with than classical constraints, because quantifier alternation makes the usual notion of solution inappropriate. As a consequence, basic properties of Constraint Satisfaction Problems (CSP), such as consistency or substitutability, are not completely understood in the quantified case. These properties are important because they are the basis of most of the reasoning methods used to solve classical (existentially quantified) constraints, and one would like to benefit from similar reasoning methods in the resolution of quantified constraints. In this paper, we show that most of the properties that are used by solvers for CSP can be generalized to quantified CSP. This requires a re-thinking of a number of basic concepts; in particular, we propose a notion of outcome that generalizes the classical notion of solution and on which all definitions are based. We propose a systematic study of the relations which hold between these properties, as well as complexity results regarding the decision of these properties. Finally, and since these problems are typically intractable, we generalize the approach used in CSP and propose weaker, easier to check notions based on locality, which allow to detect these properties incompletely but in polynomial time.",
        "published": "2007-05-24T11:27:55Z",
        "link": "http://arxiv.org/abs/0705.3561v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "Linearly bounded infinite graphs",
        "authors": [
            "Arnaud Carayol",
            "Antoine Meyer"
        ],
        "summary": "Linearly bounded Turing machines have been mainly studied as acceptors for context-sensitive languages. We define a natural class of infinite automata representing their observable computational behavior, called linearly bounded graphs. These automata naturally accept the same languages as the linearly bounded machines defining them. We present some of their structural properties as well as alternative characterizations in terms of rewriting systems and context-sensitive transductions. Finally, we compare these graphs to rational graphs, which are another class of automata accepting the context-sensitive languages, and prove that in the bounded-degree case, rational graphs are a strict sub-class of linearly bounded graphs.",
        "published": "2007-05-24T15:29:21Z",
        "link": "http://arxiv.org/abs/0705.3487v1",
        "categories": [
            "cs.LO",
            "F.1.1; F.4.3"
        ]
    },
    {
        "title": "A Logic of Reachable Patterns in Linked Data-Structures",
        "authors": [
            "Greta Yorsh",
            "Alexander Rabinovich",
            "Mooly Sagiv",
            "Antoine Meyer",
            "Ahmed Bouajjani"
        ],
        "summary": "We define a new decidable logic for expressing and checking invariants of programs that manipulate dynamically-allocated objects via pointers and destructive pointer updates. The main feature of this logic is the ability to limit the neighborhood of a node that is reachable via a regular expression from a designated node. The logic is closed under boolean operations (entailment, negation) and has a finite model property. The key technical result is the proof of decidability. We show how to express precondition, postconditions, and loop invariants for some interesting programs. It is also possible to express properties such as disjointness of data-structures, and low-level heap mutations. Moreover, our logic can express properties of arbitrary data-structures and of an arbitrary number of pointer fields. The latter provides a way to naturally specify postconditions that relate the fields on entry to a procedure to the fields on exit. Therefore, it is possible to use the logic to automatically prove partial correctness of programs performing low-level heap mutations.",
        "published": "2007-05-24T16:10:52Z",
        "link": "http://arxiv.org/abs/0705.3610v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Translating a first-order modal language to relational algebra",
        "authors": [
            "Yeb Havinga"
        ],
        "summary": "This paper is about Kripke structures that are inside a relational database and queried with a modal language. At first the modal language that is used is introduced, followed by a definition of the database and relational algebra. Based on these definitions two things are presented: a mapping from components of the modal structure to a relational database schema and instance, and a translation from queries in the modal language to relational algebra queries.",
        "published": "2007-05-27T12:36:58Z",
        "link": "http://arxiv.org/abs/0705.3949v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "H.2.3; I.2.4"
        ]
    },
    {
        "title": "On Term Rewriting Systems Having a Rational Derivation",
        "authors": [
            "Antoine Meyer"
        ],
        "summary": "Several types of term rewriting systems can be distinguished by the way their rules overlap. In particular, we define the classes of prefix, suffix, bottom-up and top-down systems, which generalize similar classes on words. Our aim is to study the derivation relation of such systems (i.e. the reflexive and transitive closure of their rewriting relation) and, if possible, to provide a finite mechanism characterizing it. Using a notion of rational relations based on finite graph grammars, we show that the derivation of any bottom-up, top-down or suffix systems is rational, while it can be non recursive for prefix systems.",
        "published": "2007-05-28T16:30:27Z",
        "link": "http://arxiv.org/abs/0705.4064v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Symbolic Reachability Analysis of Higher-Order Context-Free Processes",
        "authors": [
            "Ahmed Bouajjani",
            "Antoine Meyer"
        ],
        "summary": "We consider the problem of symbolic reachability analysis of higher-order context-free processes. These models are generalizations of the context-free processes (also called BPA processes) where each process manipulates a data structure which can be seen as a nested stack of stacks. Our main result is that, for any higher-order context-free process, the set of all predecessors of a given regular set of configurations is regular and effectively constructible. This result generalizes the analogous result which is known for level 1 context-free processes. We show that this result holds also in the case of backward reachability analysis under a regular constraint on configurations. As a corollary, we obtain a symbolic model checking algorithm for the temporal logic E(U,X) with regular atomic predicates, i.e., the fragment of CTL restricted to the EU and EX modalities.",
        "published": "2007-05-28T16:31:04Z",
        "link": "http://arxiv.org/abs/0705.3888v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Second-Order Type Isomorphisms Through Game Semantics",
        "authors": [
            "Joachim De Lataillade"
        ],
        "summary": "The characterization of second-order type isomorphisms is a purely syntactical problem that we propose to study under the enlightenment of game semantics. We study this question in the case of second-order &#955;$\\mu$-calculus, which can be seen as an extension of system F to classical logic, and for which we de&#64257;ne a categorical framework: control hyperdoctrines. Our game model of &#955;$\\mu$-calculus is based on polymorphic arenas (closely related to Hughes' hyperforests) which evolve during the play (following the ideas of Murawski-Ong). We show that type isomorphisms coincide with the \"equality\" on arenas associated with types. Finally we deduce the equational characterization of type isomorphisms from this equality. We also recover from the same model Roberto Di Cosmo's characterization of type isomorphisms for system F. This approach leads to a geometrical comprehension on the question of second order type isomorphisms, which can be easily extended to some other polymorphic calculi including additional programming features.",
        "published": "2007-05-29T14:26:26Z",
        "link": "http://arxiv.org/abs/0705.4226v1",
        "categories": [
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "Curry-style type Isomorphisms and Game Semantics",
        "authors": [
            "Joachim De Lataillade"
        ],
        "summary": "Curry-style system F, ie. system F with no explicit types in terms, can be seen as a core presentation of polymorphism from the point of view of programming languages. This paper gives a characterisation of type isomorphisms for this language, by using a game model whose intuitions come both from the syntax and from the game semantics universe. The model is composed of: an untyped part to interpret terms, a notion of game to interpret types, and a typed part to express the fact that an untyped strategy plays on a game. By analysing isomorphisms in the model, we prove that the equational system corresponding to type isomorphisms for Curry-style system F is the extension of the equational system for Church-style isomorphisms with a new, non-trivial equation: forall X.A = A[forall Y.Y/X] if X appears only positively in A.",
        "published": "2007-05-29T14:31:02Z",
        "link": "http://arxiv.org/abs/0705.4228v1",
        "categories": [
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "Temporal Runtime Verification using Monadic Difference Logic",
        "authors": [
            "Henrik Reif Andersen",
            "Kaare J. Kristoffersen"
        ],
        "summary": "In this paper we present an algorithm for performing runtime verification of a bounded temporal logic over timed runs. The algorithm consists of three elements. First, the bounded temporal formula to be verified is translated into a monadic first-order logic over difference inequalities, which we call monadic difference logic. Second, at each step of the timed run, the monadic difference formula is modified by computing a quotient with the state and time of that step. Third, the resulting formula is checked for being a tautology or being unsatisfiable by a decision procedure for monadic difference logic.   We further provide a simple decision procedure for monadic difference logic based on the data structure Difference Decision Diagrams. The algorithm is complete in a very strong sense on a subclass of temporal formulae characterized as homogeneously monadic and it is approximate on other formulae. The approximation comes from the fact that not all unsatisfiable or tautological formulae are recognised at the earliest possible time of the runtime verification.   Contrary to existing approaches, the presented algorithms do not work by syntactic rewriting but employ efficient decision structures which make them applicable in real applications within for instance business software.",
        "published": "2007-05-31T13:22:02Z",
        "link": "http://arxiv.org/abs/0705.4604v1",
        "categories": [
            "cs.LO",
            "D.2.4; D.2.5"
        ]
    },
    {
        "title": "An Improved Tight Closure Algorithm for Integer Octagonal Constraints",
        "authors": [
            "Roberto Bagnara",
            "Patricia M. Hill",
            "Enea Zaffanella"
        ],
        "summary": "Integer octagonal constraints (a.k.a. ``Unit Two Variables Per Inequality'' or ``UTVPI integer constraints'') constitute an interesting class of constraints for the representation and solution of integer problems in the fields of constraint programming and formal analysis and verification of software and hardware systems, since they couple algorithms having polynomial complexity with a relatively good expressive power. The main algorithms required for the manipulation of such constraints are the satisfiability check and the computation of the inferential closure of a set of constraints. The latter is called `tight' closure to mark the difference with the (incomplete) closure algorithm that does not exploit the integrality of the variables. In this paper we present and fully justify an O(n^3) algorithm to compute the tight closure of a set of UTVPI integer constraints.",
        "published": "2007-05-31T14:32:46Z",
        "link": "http://arxiv.org/abs/0705.4618v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "cs.LO"
        ]
    },
    {
        "title": "Many concepts and two logics of algorithmic reduction",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Within the program of finding axiomatizations for various parts of computability logic, it was proved earlier that the logic of interactive Turing reduction is exactly the implicative fragment of Heyting's intuitionistic calculus. That sort of reduction permits unlimited reusage of the computational resource represented by the antecedent. An at least equally basic and natural sort of algorithmic reduction, however, is the one that does not allow such reusage. The present article shows that turning the logic of the first sort of reduction into the logic of the second sort of reduction takes nothing more than just deleting the contraction rule from its Gentzen-style axiomatization. The first (Turing) sort of interactive reduction is also shown to come in three natural versions. While those three versions are very different from each other, their logical behaviors (in isolation) turn out to be indistinguishable, with that common behavior being precisely captured by implicative intuitionistic logic. Among the other contributions of the present article is an informal introduction of a series of new -- finite and bounded -- versions of recurrence operations and the associated reduction operations. An online source on computability logic can be found at http://www.cis.upenn.edu/~giorgi/cl.html",
        "published": "2007-06-01T09:07:20Z",
        "link": "http://arxiv.org/abs/0706.0103v4",
        "categories": [
            "cs.LO",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Relating two standard notions of secrecy",
        "authors": [
            "Veronique Cortier",
            "Michael Rusinovitch",
            "Eugen Zalinescu"
        ],
        "summary": "Two styles of definitions are usually considered to express that a security protocol preserves the confidentiality of a data s. Reachability-based secrecy means that s should never be disclosed while equivalence-based secrecy states that two executions of a protocol with distinct instances for s should be indistinguishable to an attacker. Although the second formulation ensures a higher level of security and is closer to cryptographic notions of secrecy, decidability results and automatic tools have mainly focused on the first definition so far.   This paper initiates a systematic investigation of the situations where syntactic secrecy entails strong secrecy. We show that in the passive case, reachability-based secrecy actually implies equivalence-based secrecy for digital signatures, symmetric and asymmetric encryption provided that the primitives are probabilistic. For active adversaries, we provide sufficient (and rather tight) conditions on the protocol for this implication to hold.",
        "published": "2007-06-04T19:30:33Z",
        "link": "http://arxiv.org/abs/0706.0502v2",
        "categories": [
            "cs.CR",
            "cs.LO"
        ]
    },
    {
        "title": "Interpolant-Based Transition Relation Approximation",
        "authors": [
            "Ranjit Jhala",
            "Kenneth L. McMillan"
        ],
        "summary": "In predicate abstraction, exact image computation is problematic, requiring in the worst case an exponential number of calls to a decision procedure. For this reason, software model checkers typically use a weak approximation of the image. This can result in a failure to prove a property, even given an adequate set of predicates. We present an interpolant-based method for strengthening the abstract transition relation in case of such failures. This approach guarantees convergence given an adequate set of predicates, without requiring an exact image computation. We show empirically that the method converges more rapidly than an earlier method based on counterexample analysis.",
        "published": "2007-06-04T20:07:54Z",
        "link": "http://arxiv.org/abs/0706.0523v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SE",
            "D.2.4; F.3.1"
        ]
    },
    {
        "title": "Probabilistic Interval Temporal Logic and Duration Calculus with   Infinite Intervals: Complete Proof Systems",
        "authors": [
            "Dimitar P. Guelev"
        ],
        "summary": "The paper presents probabilistic extensions of interval temporal logic (ITL) and duration calculus (DC) with infinite intervals and complete Hilbert-style proof systems for them. The completeness results are a strong completeness theorem for the system of probabilistic ITL with respect to an abstract semantics and a relative completeness theorem for the system of probabilistic DC with respect to real-time semantics. The proposed systems subsume probabilistic real-time DC as known from the literature. A correspondence between the proposed systems and a system of probabilistic interval temporal logic with finite intervals and expanding modalities is established too.",
        "published": "2007-06-05T15:46:09Z",
        "link": "http://arxiv.org/abs/0706.0692v2",
        "categories": [
            "cs.LO",
            "F.3.1"
        ]
    },
    {
        "title": "Asynchronous games: innocence without alternation",
        "authors": [
            "Paul-André Melliès",
            "Samuel Mimram"
        ],
        "summary": "The notion of innocent strategy was introduced by Hyland and Ong in order to capture the interactive behaviour of lambda-terms and PCF programs. An innocent strategy is defined as an alternating strategy with partial memory, in which the strategy plays according to its view. Extending the definition to non-alternating strategies is problematic, because the traditional definition of views is based on the hypothesis that Opponent and Proponent alternate during the interaction. Here, we take advantage of the diagrammatic reformulation of alternating innocence in asynchronous games, in order to provide a tentative definition of innocence in non-alternating games. The task is interesting, and far from easy. It requires the combination of true concurrency and game semantics in a clean and organic way, clarifying the relationship between asynchronous games and concurrent games in the sense of Abramsky and Melli\\`es. It also requires an interactive reformulation of the usual acyclicity criterion of linear logic, as well as a directed variant, as a scheduling criterion.",
        "published": "2007-06-08T06:56:31Z",
        "link": "http://arxiv.org/abs/0706.1118v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A Finite Semantics of Simply-Typed Lambda Terms for Infinite Runs of<br>   Automata",
        "authors": [
            "Klaus Aehlig"
        ],
        "summary": "Model checking properties are often described by means of finite automata. Any particular such automaton divides the set of infinite trees into finitely many classes, according to which state has an infinite run. Building the full type hierarchy upon this interpretation of the base type gives a finite semantics for simply-typed lambda-trees.   A calculus based on this semantics is proven sound and complete. In particular, for regular infinite lambda-trees it is decidable whether a given automaton has a run or not. As regular lambda-trees are precisely recursion schemes, this decidability result holds for arbitrary recursion schemes of arbitrary level, without any syntactical restriction.",
        "published": "2007-06-14T09:57:27Z",
        "link": "http://arxiv.org/abs/0706.2076v3",
        "categories": [
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "Resource control of object-oriented programs",
        "authors": [
            "Jean-Yves Marion",
            "Romain Pechoux"
        ],
        "summary": "A sup-interpretation is a tool which provides an upper bound on the size of a value computed by some symbol of a program. Sup-interpretations have shown their interest to deal with the complexity of first order functional programs. For instance, they allow to characterize all the functions bitwise computable in Alogtime. This paper is an attempt to adapt the framework of sup-interpretations to a fragment of oriented-object programs, including distinct encodings of numbers through the use of constructor symbols, loop and while constructs and non recursive methods with side effects. We give a criterion, called brotherly criterion, which ensures that each brotherly program computes objects whose size is polynomially bounded by the inputs sizes.",
        "published": "2007-06-15T13:39:12Z",
        "link": "http://arxiv.org/abs/0706.2293v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.2"
        ]
    },
    {
        "title": "Abstract machines for dialogue games",
        "authors": [
            "Pierre-Louis Curien",
            "Hugo Herbelin"
        ],
        "summary": "The notion of abstract Boehm tree has arisen as an operationally-oriented distillation of works on game semantics, and has been investigated in two papers. This paper revisits the notion, providing more syntactic support and more examples (like call-by-value evaluation) illustrating the generality of the underlying computing device. Precise correspondences between various formulations of the evaluation mechanism of abstract Boehm trees are established.",
        "published": "2007-06-18T08:01:12Z",
        "link": "http://arxiv.org/abs/0706.2544v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Decisive Markov Chains",
        "authors": [
            "Parosh Aziz Abdulla",
            "Noomene Ben Henda",
            "Richard Mayr"
        ],
        "summary": "We consider qualitative and quantitative verification problems for infinite-state Markov chains. We call a Markov chain decisive w.r.t. a given set of target states F if it almost certainly eventually reaches either F or a state from which F can no longer be reached. While all finite Markov chains are trivially decisive (for every set F), this also holds for many classes of infinite Markov chains. Infinite Markov chains which contain a finite attractor are decisive w.r.t. every set F. In particular, this holds for probabilistic lossy channel systems (PLCS). Furthermore, all globally coarse Markov chains are decisive. This class includes probabilistic vector addition systems (PVASS) and probabilistic noisy Turing machines (PNTM). We consider both safety and liveness problems for decisive Markov chains, i.e., the probabilities that a given set of states F is eventually reached or reached infinitely often, respectively. 1. We express the qualitative problems in abstract terms for decisive Markov chains, and show an almost complete picture of its decidability for PLCS, PVASS and PNTM. 2. We also show that the path enumeration algorithm of Iyer and Narasimha terminates for decisive Markov chains and can thus be used to solve the approximate quantitative safety problem. A modified variant of this algorithm solves the approximate quantitative liveness problem. 3. Finally, we show that the exact probability of (repeatedly) reaching F cannot be effectively expressed (in a uniform way) in Tarski-algebra for either PLCS, PVASS or (P)NTM.",
        "published": "2007-06-18T11:50:16Z",
        "link": "http://arxiv.org/abs/0706.2585v2",
        "categories": [
            "cs.LO",
            "cs.DM",
            "G.3; D.2.4; F.4.1"
        ]
    },
    {
        "title": "Algorithms for Omega-Regular Games with Imperfect Information",
        "authors": [
            "Krishnendu Chatterjee",
            "Laurent Doyen",
            "Thomas A. Henzinger",
            "<br> Jean-Francois Raskin"
        ],
        "summary": "We study observation-based strategies for two-player turn-based games on graphs with omega-regular objectives. An observation-based strategy relies on imperfect information about the history of a play, namely, on the past sequence of observations. Such games occur in the synthesis of a controller that does not see the private state of the plant. Our main results are twofold. First, we give a fixed-point algorithm for computing the set of states from which a player can win with a deterministic observation-based strategy for any omega-regular objective. The fixed point is computed in the lattice of antichains of state sets. This algorithm has the advantages of being directed by the objective and of avoiding an explicit subset construction on the game graph. Second, we give an algorithm for computing the set of states from which a player can win with probability 1 with a randomized observation-based strategy for a Buechi objective. This set is of interest because in the absence of perfect information, randomized strategies are more powerful than deterministic ones. We show that our algorithms are optimal by proving matching lower bounds.",
        "published": "2007-06-18T15:02:02Z",
        "link": "http://arxiv.org/abs/0706.2619v3",
        "categories": [
            "cs.LO",
            "cs.GT",
            "F.4.1"
        ]
    },
    {
        "title": "A Sequent Calculus for Modelling Interferences",
        "authors": [
            "Christophe Fouqueré"
        ],
        "summary": "A logic calculus is presented that is a conservative extension of linear logic. The motivation beneath this work concerns lazy evaluation, true concurrency and interferences in proof search. The calculus includes two new connectives to deal with multisequent structures and has the cut-elimination property. Extensions are proposed that give first results concerning our objectives.",
        "published": "2007-06-22T14:24:34Z",
        "link": "http://arxiv.org/abs/0706.3341v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "There Exist some Omega-Powers of Any Borel Rank",
        "authors": [
            "Dominique Lecomte",
            "Olivier Finkel"
        ],
        "summary": "Omega-powers of finitary languages are languages of infinite words (omega-languages) in the form V^omega, where V is a finitary language over a finite alphabet X. They appear very naturally in the characterizaton of regular or context-free omega-languages. Since the set of infinite words over a finite alphabet X can be equipped with the usual Cantor topology, the question of the topological complexity of omega-powers of finitary languages naturally arises and has been posed by Niwinski (1990), Simonnet (1992) and Staiger (1997). It has been recently proved that for each integer n > 0, there exist some omega-powers of context free languages which are Pi^0_n-complete Borel sets, that there exists a context free language L such that L^omega is analytic but not Borel, and that there exists a finitary language V such that V^omega is a Borel set of infinite rank. But it was still unknown which could be the possible infinite Borel ranks of omega-powers. We fill this gap here, proving the following very surprising result which shows that omega-powers exhibit a great topological complexity: for each non-null countable ordinal alpha, there exist some Sigma^0_alpha-complete omega-powers, and some Pi^0_alpha-complete omega-powers.",
        "published": "2007-06-25T16:03:36Z",
        "link": "http://arxiv.org/abs/0706.3523v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "Order-Invariant MSO is Stronger than Counting MSO in the Finite",
        "authors": [
            "Tobias Ganzow",
            "Sasha Rubin"
        ],
        "summary": "We compare the expressiveness of two extensions of monadic second-order logic (MSO) over the class of finite structures. The first, counting monadic second-order logic (CMSO), extends MSO with first-order modulo-counting quantifiers, allowing the expression of queries like ``the number of elements in the structure is even''. The second extension allows the use of an additional binary predicate, not contained in the signature of the queried structure, that must be interpreted as an arbitrary linear order on its universe, obtaining order-invariant MSO.   While it is straightforward that every CMSO formula can be translated into an equivalent order-invariant MSO formula, the converse had not yet been settled. Courcelle showed that for restricted classes of structures both order-invariant MSO and CMSO are equally expressive, but conjectured that, in general, order-invariant MSO is stronger than CMSO.   We affirm this conjecture by presenting a class of structures that is order-invariantly definable in MSO but not definable in CMSO.",
        "published": "2007-06-26T12:16:18Z",
        "link": "http://arxiv.org/abs/0706.3723v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "PSPACE Bounds for Rank-1 Modal Logics",
        "authors": [
            "Lutz Schröder",
            "Dirk Pattinson"
        ],
        "summary": "For lack of general algorithmic methods that apply to wide classes of logics, establishing a complexity bound for a given modal logic is often a laborious task. The present work is a step towards a general theory of the complexity of modal logics. Our main result is that all rank-1 logics enjoy a shallow model property and thus are, under mild assumptions on the format of their axiomatisation, in PSPACE. This leads to a unified derivation of tight PSPACE-bounds for a number of logics including K, KD, coalition logic, graded modal logic, majority logic, and probabilistic modal logic. Our generic algorithm moreover finds tableau proofs that witness pleasant proof-theoretic properties including a weak subformula property. This generality is made possible by a coalgebraic semantics, which conveniently abstracts from the details of a given model class and thus allows covering a broad range of logics in a uniform way.",
        "published": "2007-06-27T15:15:57Z",
        "link": "http://arxiv.org/abs/0706.4044v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "Theory of Finite or Infinite Trees Revisited",
        "authors": [
            "Khalil Djelloul",
            "Thi-bich-hanh Dao",
            "Thom Fruehwirth"
        ],
        "summary": "We present in this paper a first-order axiomatization of an extended theory $T$ of finite or infinite trees, built on a signature containing an infinite set of function symbols and a relation $\\fini(t)$ which enables to distinguish between finite or infinite trees. We show that $T$ has at least one model and prove its completeness by giving not only a decision procedure, but a full first-order constraint solver which gives clear and explicit solutions for any first-order constraint satisfaction problem in $T$. The solver is given in the form of 16 rewriting rules which transform any first-order constraint $\\phi$ into an equivalent disjunction $\\phi$ of simple formulas such that $\\phi$ is either the formula $\\true$ or the formula $\\false$ or a formula having at least one free variable, being equivalent neither to $\\true$ nor to $\\false$ and where the solutions of the free variables are expressed in a clear and explicit way. The correctness of our rules implies the completeness of $T$. We also describe an implementation of our algorithm in CHR (Constraint Handling Rules) and compare the performance with an implementation in C++ and that of a recent decision procedure for decomposable theories.",
        "published": "2007-06-28T21:18:19Z",
        "link": "http://arxiv.org/abs/0706.4323v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Determinacy in a synchronous pi-calculus",
        "authors": [
            "Roberto Amadio",
            "Mehdi Dogguy"
        ],
        "summary": "The S-pi-calculus is a synchronous pi-calculus which is based on the SL model. The latter is a relaxation of the Esterel model where the reaction to the absence of a signal within an instant can only happen at the next instant. In the present work, we present and characterise a compositional semantics of the S-pi-calculus based on suitable notions of labelled transition system and bisimulation. Based on this semantic framework, we explore the notion of determinacy and the related one of (local) confluence.",
        "published": "2007-07-04T08:12:17Z",
        "link": "http://arxiv.org/abs/0707.0556v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "On a Non-Context-Free Extension of PDL",
        "authors": [
            "Stefan Göller",
            "Dirk Nowotka"
        ],
        "summary": "Over the last 25 years, a lot of work has been done on seeking for decidable non-regular extensions of Propositional Dynamic Logic (PDL). Only recently, an expressive extension of PDL, allowing visibly pushdown automata (VPAs) as a formalism to describe programs, was introduced and proven to have a satisfiability problem complete for deterministic double exponential time. Lately, the VPA formalism was extended to so called k-phase multi-stack visibly pushdown automata (k-MVPAs). Similarly to VPAs, it has been shown that the language of k-MVPAs have desirable effective closure properties and that the emptiness problem is decidable. On the occasion of introducing k-MVPAs, it has been asked whether the extension of PDL with k-MVPAs still leads to a decidable logic. This question is answered negatively here. We prove that already for the extension of PDL with 2-phase MVPAs with two stacks satisfiability becomes \\Sigma_1^1-complete.",
        "published": "2007-07-04T09:33:21Z",
        "link": "http://arxiv.org/abs/0707.0562v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A process algebra based framework for promise theory",
        "authors": [
            "Jan Bergstra",
            "Inge Bethke",
            "Mark Burgess"
        ],
        "summary": "We present a process algebra based approach to formalize the interactions of computing devices such as the representation of policies and the resolution of conflicts. As an example we specify how promises may be used in coming to an agreement regarding a simple though practical transportation problem.",
        "published": "2007-07-05T09:39:14Z",
        "link": "http://arxiv.org/abs/0707.0744v1",
        "categories": [
            "cs.LO",
            "D.2.4"
        ]
    },
    {
        "title": "Are there Hilbert-style Pure Type Systems?",
        "authors": [
            "M. W. Bunder",
            "W. M. J. Dekkers"
        ],
        "summary": "For many a natural deduction style logic there is a Hilbert-style logic that is equivalent to it in that it has the same theorems (i.e. valid judgements with empty contexts). For intuitionistic logic, the axioms of the equivalent Hilbert-style logic can be propositions which are also known as the types of the combinators I, K and S. Hilbert-style versions of illative combinatory logic have formulations with axioms that are actual type statements for I, K and S. As pure type systems (PTSs)are, in a sense, equivalent to systems of illative combinatory logic, it might be thought that Hilbert-style PTSs (HPTSs) could be based in a similar way. This paper shows that some PTSs have very trivial equivalent HPTSs, with only the axioms as theorems and that for many PTSs no equivalent HPTS can exist. Most commonly used PTSs belong to these two classes. For some PTSs however, including lambda* and the PTS at the basis of the proof assistant Coq, there is a nontrivial equivalent HPTS, with axioms that are type statements for I, K and S.",
        "published": "2007-07-06T00:22:59Z",
        "link": "http://arxiv.org/abs/0707.0890v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Theorem proving support in programming language semantics",
        "authors": [
            "Yves Bertot"
        ],
        "summary": "We describe several views of the semantics of a simple programming language as formal documents in the calculus of inductive constructions that can be verified by the Coq proof system. Covered aspects are natural semantics, denotational semantics, axiomatic semantics, and abstract interpretation. Descriptions as recursive functions are also provided whenever suitable, thus yielding a a verification condition generator and a static analyser that can be run inside the theorem prover for use in reflective proofs. Extraction of an interpreter from the denotational semantics is also described. All different aspects are formally proved sound with respect to the natural semantics specification.",
        "published": "2007-07-06T08:55:26Z",
        "link": "http://arxiv.org/abs/0707.0926v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Building Decision Procedures in the Calculus of Inductive Constructions",
        "authors": [
            "Frédéric Blanqui",
            "Jean-Pierre Jouannaud",
            "Pierre-Yves Strub"
        ],
        "summary": "It is commonly agreed that the success of future proof assistants will rely on their ability to incorporate computations within deduction in order to mimic the mathematician when replacing the proof of a proposition P by the proof of an equivalent proposition P' obtained from P thanks to possibly complex calculations. In this paper, we investigate a new version of the calculus of inductive constructions which incorporates arbitrary decision procedures into deduction via the conversion rule of the calculus. The novelty of the problem in the context of the calculus of inductive constructions lies in the fact that the computation mechanism varies along proof-checking: goals are sent to the decision procedure together with the set of user hypotheses available from the current context. Our main result shows that this extension of the calculus of constructions does not compromise its main properties: confluence, subject reduction, strong normalization and consistency are all preserved.",
        "published": "2007-07-09T14:35:14Z",
        "link": "http://arxiv.org/abs/0707.1266v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Computability Closure: Ten Years Later",
        "authors": [
            "Frédéric Blanqui"
        ],
        "summary": "The notion of computability closure has been introduced for proving the termination of higher-order rewriting with first-order matching by Jean-Pierre Jouannaud and Mitsuhiro Okada in a 1997 draft which later served as a basis for the author's PhD. In this paper, we show how this notion can also be used for dealing with beta-normalized rewriting with matching modulo beta-eta (on patterns \\`a la Miller), rewriting with matching modulo some equational theory, and higher-order data types (types with constructors having functional recursive arguments). Finally, we show how the computability closure can easily be turned into a reduction ordering which, in the higher-order case, contains Jean-Pierre Jouannaud and Albert Rubio's higher-order recursive path ordering and, in the first-order case, is equal to the usual first-order recursive path ordering.",
        "published": "2007-07-10T06:33:52Z",
        "link": "http://arxiv.org/abs/0707.1372v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A Normalizing Intuitionistic Set Theory with Inaccessible Sets",
        "authors": [
            "Wojciech Moczydlowski"
        ],
        "summary": "We propose a set theory strong enough to interpret powerful type theories underlying proof assistants such as LEGO and also possibly Coq, which at the same time enables program extraction from its constructive proofs. For this purpose, we axiomatize an impredicative constructive version of Zermelo-Fraenkel set theory IZF with Replacement and $\\omega$-many inaccessibles, which we call \\izfio. Our axiomatization utilizes set terms, an inductive definition of inaccessible sets and the mutually recursive nature of equality and membership relations. It allows us to define a weakly-normalizing typed lambda calculus corresponding to proofs in \\izfio according to the Curry-Howard isomorphism principle. We use realizability to prove the normalization theorem, which provides a basis for program extraction capability.",
        "published": "2007-07-13T12:02:10Z",
        "link": "http://arxiv.org/abs/0707.1981v3",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "A Characterisation of First-Order Constraint Satisfaction Problems",
        "authors": [
            "Benoit Larose",
            "Cynthia Loten",
            "Claude Tardif"
        ],
        "summary": "We describe simple algebraic and combinatorial characterisations of finite relational core structures admitting finitely many obstructions. As a consequence, we show that it is decidable to determine whether a constraint satisfaction problem is first-order definable: we show the general problem to be NP-complete, and give a polynomial-time algorithm in the case of cores. A slight modification of this algorithm provides, for first-order definable CSP's, a simple poly-time algorithm to produce a solution when one exists. As an application of our algebraic characterisation of first order CSP's, we describe a large family of L-complete CSP's.",
        "published": "2007-07-17T16:23:45Z",
        "link": "http://arxiv.org/abs/0707.2562v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Inductive Definition and Domain Theoretic Properties of Fully Abstract",
        "authors": [
            "Vladimir Sazonov"
        ],
        "summary": "A construction of fully abstract typed models for PCF and PCF^+ (i.e., PCF + \"parallel conditional function\"), respectively, is presented. It is based on general notions of sequential computational strategies and wittingly consistent non-deterministic strategies introduced by the author in the seventies. Although these notions of strategies are old, the definition of the fully abstract models is new, in that it is given level-by-level in the finite type hierarchy. To prove full abstraction and non-dcpo domain theoretic properties of these models, a theory of computational strategies is developed. This is also an alternative and, in a sense, an analogue to the later game strategy semantics approaches of Abramsky, Jagadeesan, and Malacaria; Hyland and Ong; and Nickau. In both cases of PCF and PCF^+ there are definable universal (surjective) functionals from numerical functions to any given type, respectively, which also makes each of these models unique up to isomorphism. Although such models are non-omega-complete and therefore not continuous in the traditional terminology, they are also proved to be sequentially complete (a weakened form of omega-completeness), \"naturally\" continuous (with respect to existing directed \"pointwise\", or \"natural\" lubs) and also \"naturally\" omega-algebraic and \"naturally\" bounded complete -- appropriate generalisation of the ordinary notions of domain theory to the case of non-dcpos.",
        "published": "2007-07-21T00:29:38Z",
        "link": "http://arxiv.org/abs/0707.3170v3",
        "categories": [
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "Neutrality and Many-Valued Logics",
        "authors": [
            "Andrew Schumann",
            "Florentin Smarandache"
        ],
        "summary": "In this book, we consider various many-valued logics: standard, linear, hyperbolic, parabolic, non-Archimedean, p-adic, interval, neutrosophic, etc. We survey also results which show the tree different proof-theoretic frameworks for many-valued logics, e.g. frameworks of the following deductive calculi: Hilbert's style, sequent, and hypersequent. We present a general way that allows to construct systematically analytic calculi for a large family of non-Archimedean many-valued logics: hyperrational-valued, hyperreal-valued, and p-adic valued logics characterized by a special format of semantics with an appropriate rejection of Archimedes' axiom. These logics are built as different extensions of standard many-valued logics (namely, Lukasiewicz's, Goedel's, Product, and Post's logics). The informal sense of Archimedes' axiom is that anything can be measured by a ruler. Also logical multiple-validity without Archimedes' axiom consists in that the set of truth values is infinite and it is not well-founded and well-ordered. On the base of non-Archimedean valued logics, we construct non-Archimedean valued interval neutrosophic logic INL by which we can describe neutrality phenomena.",
        "published": "2007-07-21T10:35:37Z",
        "link": "http://arxiv.org/abs/0707.3205v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "A Knowledge-Based Analysis of Global Function Computation",
        "authors": [
            "Joseph Y. Halpern",
            "Sabina Petride"
        ],
        "summary": "Consider a distributed system N in which each agent has an input value and each communication link has a weight. Given a global function, that is, a function f whose value depends on the whole network, the goal is for every agent to eventually compute the value f(N). We call this problem global function computation. Various solutions for instances of this problem, such as Boolean function computation, leader election, (minimum) spanning tree construction, and network determination, have been proposed, each under particular assumptions about what processors know about the system and how this knowledge can be acquired. We give a necessary and sufficient condition for the problem to be solvable that generalizes a number of well-known results. We then provide a knowledge-based (kb) program (like those of Fagin, Halpern, Moses, and Vardi) that solves global function computation whenever possible. Finally, we improve the message overhead inherent in our initial kb program by giving a counterfactual belief-based program that also solves the global function computation whenever possible, but where agents send messages only when they believe it is necessary to do so. The latter program is shown to be implemented by a number of well-known algorithms for solving leader election.",
        "published": "2007-07-23T18:49:28Z",
        "link": "http://arxiv.org/abs/0707.3435v1",
        "categories": [
            "cs.DC",
            "cs.LO"
        ]
    },
    {
        "title": "Bijective Faithful Translations among Default Logics",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "In this article, we study translations between variants of defaults logics such that the extensions of the theories that are the input and the output of the translation are in a bijective correspondence. We assume that a translation can introduce new variables and that the result of translating a theory can either be produced in time polynomial in the size of the theory or its output is polynomial in that size; we however restrict to the case in which the original theory has extensions. This study fills a gap between two previous pieces of work, one studying bijective translations among restrictions of default logics, and the other one studying non-bijective translations between default logics variants.",
        "published": "2007-07-25T17:03:57Z",
        "link": "http://arxiv.org/abs/0707.3781v2",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Interactive Small-Step Algorithms I: Axiomatization",
        "authors": [
            "Andreas Blass",
            "Yuri Gurevich",
            "Dean Rosenzweig",
            "Benjamin Rossman"
        ],
        "summary": "In earlier work, the Abstract State Machine Thesis -- that arbitrary algorithms are behaviorally equivalent to abstract state machines -- was established for several classes of algorithms, including ordinary, interactive, small-step algorithms. This was accomplished on the basis of axiomatizations of these classes of algorithms. Here we extend the axiomatization and, in a companion paper, the proof, to cover interactive small-step algorithms that are not necessarily ordinary. This means that the algorithms (1) can complete a step without necessarily waiting for replies to all queries from that step and (2) can use not only the environment's replies but also the order in which the replies were received.",
        "published": "2007-07-25T17:04:26Z",
        "link": "http://arxiv.org/abs/0707.3782v2",
        "categories": [
            "cs.LO",
            "F.1.1; F.1.2; F.3.1"
        ]
    },
    {
        "title": "Interactive Small-Step Algorithms II: Abstract State Machines and   the<br> Characterization Theorem",
        "authors": [
            "Andreas Blass",
            "Yuri Gurevich",
            "Dean Rosenzweig",
            "Benjamin Rossman"
        ],
        "summary": "In earlier work, the Abstract State Machine Thesis -- that arbitrary algorithms are behaviorally equivalent to abstract state machines -- was established for several classes of algorithms, including ordinary, interactive, small-step algorithms. This was accomplished on the basis of axiomatizations of these classes of algorithms. In Part I (Interactive Small-Step Algorithms I: Axiomatization), the axiomatization was extended to cover interactive small-step algorithms that are not necessarily ordinary. This means that the algorithms (1) can complete a step without necessarily waiting for replies to all queries from that step and (2) can use not only the environment's replies but also the order in which the replies were received. In order to prove the thesis for algorithms of this generality, we extend here the definition of abstract state machines to incorporate explicit attention to the relative timing of replies and to the possible absence of replies. We prove the characterization theorem for extended abstract state machines with respect to general algorithms as axiomatized in Part I.",
        "published": "2007-07-25T17:35:46Z",
        "link": "http://arxiv.org/abs/0707.3789v2",
        "categories": [
            "cs.LO",
            "F.1.1; F.1.2; F.3.1"
        ]
    },
    {
        "title": "Complexity of Propositional Proofs under a Promise",
        "authors": [
            "Nachum Dershowitz",
            "Iddo Tzameret"
        ],
        "summary": "We study -- within the framework of propositional proof complexity -- the problem of certifying unsatisfiability of CNF formulas under the promise that any satisfiable formula has many satisfying assignments, where ``many'' stands for an explicitly specified function $\\Lam$ in the number of variables $n$. To this end, we develop propositional proof systems under different measures of promises (that is, different $\\Lam$) as extensions of resolution. This is done by augmenting resolution with axioms that, roughly, can eliminate sets of truth assignments defined by Boolean circuits. We then investigate the complexity of such systems, obtaining an exponential separation in the average-case between resolution under different size promises:   1. Resolution has polynomial-size refutations for all unsatisfiable 3CNF formulas when the promise is $\\eps\\cd2^n$, for any constant $0<\\eps<1$.   2. There are no sub-exponential size resolution refutations for random 3CNF formulas, when the promise is $2^{\\delta n}$ (and the number of clauses is $o(n^{3/2})$), for any constant $0<\\delta<1$.",
        "published": "2007-07-28T18:36:01Z",
        "link": "http://arxiv.org/abs/0707.4255v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.2.2; F.4.1"
        ]
    },
    {
        "title": "A Note on Shortest Developments",
        "authors": [
            "Morten Heine Sørensen"
        ],
        "summary": "De Vrijer has presented a proof of the finite developments theorem which, in addition to showing that all developments are finite, gives an effective reduction strategy computing longest developments as well as a simple formula computing their length.   We show that by applying a rather simple and intuitive principle of duality to de Vrijer's approach one arrives at a proof that some developments are finite which in addition yields an effective reduction strategy computing shortest developments as well as a simple formula computing their length. The duality fails for general beta-reduction.   Our results simplify previous work by Khasidashvili.",
        "published": "2007-08-01T17:22:48Z",
        "link": "http://arxiv.org/abs/0708.0200v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Edit and verify",
        "authors": [
            "Radu Grigore",
            "Michał Moskal"
        ],
        "summary": "Automated theorem provers are used in extended static checking, where they are the performance bottleneck. Extended static checkers are run typically after incremental changes to the code. We propose to exploit this usage pattern to improve performance. We present two approaches of how to do so and a full solution.",
        "published": "2007-08-06T07:47:34Z",
        "link": "http://arxiv.org/abs/0708.0713v1",
        "categories": [
            "cs.LO",
            "D.2.4"
        ]
    },
    {
        "title": "Valid formulas, games and network protocols",
        "authors": [
            "Jean-Louis Krivine",
            "Yves Legrandgérard"
        ],
        "summary": "We describe a remarkable relation between the notion of valid formula of predicate logic and the specification of network protocols. We give several examples such as the acknowledgement of one packet or of a sequence of packets. We show how to specify the composition of protocols.",
        "published": "2007-08-10T16:17:00Z",
        "link": "http://arxiv.org/abs/0708.1480v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Resolution over Linear Equations and Multilinear Proofs",
        "authors": [
            "Ran Raz",
            "Iddo Tzameret"
        ],
        "summary": "We develop and study the complexity of propositional proof systems of varying strength extending resolution by allowing it to operate with disjunctions of linear equations instead of clauses. We demonstrate polynomial-size refutations for hard tautologies like the pigeonhole principle, Tseitin graph tautologies and the clique-coloring tautologies in these proof systems. Using the (monotone) interpolation by a communication game technique we establish an exponential-size lower bound on refutations in a certain, considerably strong, fragment of resolution over linear equations, as well as a general polynomial upper bound on (non-monotone) interpolants in this fragment.   We then apply these results to extend and improve previous results on multilinear proofs (over fields of characteristic 0), as studied in [RazTzameret06]. Specifically, we show the following:   1. Proofs operating with depth-3 multilinear formulas polynomially simulate a certain, considerably strong, fragment of resolution over linear equations.   2. Proofs operating with depth-3 multilinear formulas admit polynomial-size refutations of the pigeonhole principle and Tseitin graph tautologies. The former improve over a previous result that established small multilinear proofs only for the \\emph{functional} pigeonhole principle. The latter are different than previous proofs, and apply to multilinear proofs of Tseitin mod p graph tautologies over any field of characteristic 0.   We conclude by connecting resolution over linear equations with extensions of the cutting planes proof system.",
        "published": "2007-08-10T23:23:10Z",
        "link": "http://arxiv.org/abs/0708.1529v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.2.2; F.4.1"
        ]
    },
    {
        "title": "Hybrid Branching-Time Logics",
        "authors": [
            "Volker Weber"
        ],
        "summary": "Hybrid branching-time logics are introduced as extensions of CTL-like logics with state variables and the downarrow-binder. Following recent work in the linear framework, only logics with a single variable are considered. The expressive power and the complexity of satisfiability of the resulting logics is investigated.   As main result, the satisfiability problem for the hybrid versions of several branching-time logics is proved to be 2EXPTIME-complete. These branching-time logics range from strict fragments of CTL to extensions of CTL that can talk about the past and express fairness-properties. The complexity gap relative to CTL is explained by a corresponding succinctness result.   To prove the upper bound, the automata-theoretic approach to branching-time logics is extended to hybrid logics, showing that non-emptiness of alternating one-pebble Buchi tree automata is 2EXPTIME-complete.",
        "published": "2007-08-13T15:04:12Z",
        "link": "http://arxiv.org/abs/0708.1723v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Collection analysis for Horn clause programs",
        "authors": [
            "Dale Miller"
        ],
        "summary": "We consider approximating data structures with collections of the items that they contain. For examples, lists, binary trees, tuples, etc, can be approximated by sets or multisets of the items within them. Such approximations can be used to provide partial correctness properties of logic programs. For example, one might wish to specify than whenever the atom $sort(t,s)$ is proved then the two lists $t$ and $s$ contain the same multiset of items (that is, $s$ is a permutation of $t$). If sorting removes duplicates, then one would like to infer that the sets of items underlying $t$ and $s$ are the same. Such results could be useful to have if they can be determined statically and automatically. We present a scheme by which such collection analysis can be structured and automated. Central to this scheme is the use of linear logic as a omputational logic underlying the logic of Horn clauses.",
        "published": "2007-08-16T15:45:06Z",
        "link": "http://arxiv.org/abs/0708.2230v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Focusing and Polarization in Intuitionistic Logic",
        "authors": [
            "Chuck Liang",
            "Dale Miller"
        ],
        "summary": "A focused proof system provides a normal form to cut-free proofs that structures the application of invertible and non-invertible inference rules. The focused proof system of Andreoli for linear logic has been applied to both the proof search and the proof normalization approaches to computation. Various proof systems in literature exhibit characteristics of focusing to one degree or another. We present a new, focused proof system for intuitionistic logic, called LJF, and show how other proof systems can be mapped into the new system by inserting logical connectives that prematurely stop focusing. We also use LJF to design a focused proof system for classical logic. Our approach to the design and analysis of these systems is based on the completeness of focusing in linear logic and on the notion of polarity that appears in Girard's LC and LU proof systems.",
        "published": "2007-08-16T17:36:28Z",
        "link": "http://arxiv.org/abs/0708.2252v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Unsatisfiable Linear k-CNFs Exist, for every k",
        "authors": [
            "Dominik Scheder"
        ],
        "summary": "We call a CNF formula linear if any two clauses have at most one variable in common. Let Linear k-SAT be the problem of deciding whether a given linear k-CNF formula is satisfiable. Here, a k-CNF formula is a CNF formula in which every clause has size exactly k. It was known that for k >= 3, Linear k-SAT is NP-complete if and only if an unsatisfiable linear k-CNF formula exists, and that they do exist for k >= 4. We prove that unsatisfiable linear k-CNF formulas exist for every k. Let f(k) be the minimum number of clauses in an unsatisfiable linear k-CNF formula. We show that f(k) is Omega(k2^k) and O(4^k*k^4), i.e., minimum size unsatisfiable linear k-CNF formulas are significantly larger than minimum size unsatisfiable k-CNF formulas. Finally, we prove that, surprisingly, linear k-CNF formulas do not allow for a larger fraction of clauses to be satisfied than general k-CNF formulas.",
        "published": "2007-08-17T09:44:21Z",
        "link": "http://arxiv.org/abs/0708.2336v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "The Church Synthesis Problem with Parameters",
        "authors": [
            "Alexander Rabinovich"
        ],
        "summary": "For a two-variable formula &psi;(X,Y) of Monadic Logic of Order (MLO) the Church Synthesis Problem concerns the existence and construction of an operator Y=F(X) such that &psi;(X,F(X)) is universally valid over Nat.   B\\\"{u}chi and Landweber proved that the Church synthesis problem is decidable; moreover, they showed that if there is an operator F that solves the Church Synthesis Problem, then it can also be solved by an operator defined by a finite state automaton or equivalently by an MLO formula. We investigate a parameterized version of the Church synthesis problem. In this version &psi; might contain as a parameter a unary predicate P. We show that the Church synthesis problem for P is computable if and only if the monadic theory of <Nat,<,P> is decidable. We prove that the B\\\"{u}chi-Landweber theorem can be extended only to ultimately periodic parameters. However, the MLO-definability part of the B\\\"{u}chi-Landweber theorem holds for the parameterized version of the Church synthesis problem.",
        "published": "2007-08-26T12:08:30Z",
        "link": "http://arxiv.org/abs/0708.3477v2",
        "categories": [
            "cs.LO",
            "F.4.1; F.4.3"
        ]
    },
    {
        "title": "Deciding security properties for cryptographic protocols. Application to   key cycles",
        "authors": [
            "Hubert Comon-Lundh",
            "Véronique Cortier",
            "Eugen Zalinescu"
        ],
        "summary": "There is a large amount of work dedicated to the formal verification of security protocols. In this paper, we revisit and extend the NP-complete decision procedure for a bounded number of sessions. We use a, now standard, deducibility constraints formalism for modeling security protocols. Our first contribution is to give a simple set of constraint simplification rules, that allows to reduce any deducibility constraint system to a set of solved forms, representing all solutions (within the bound on sessions).   As a consequence, we prove that deciding the existence of key cycles is NP-complete for a bounded number of sessions. The problem of key-cycles has been put forward by recent works relating computational and symbolic models. The so-called soundness of the symbolic model requires indeed that no key cycle (e.g., enc(k,k)) ever occurs in the execution of the protocol. Otherwise, stronger security assumptions (such as KDM-security) are required.   We show that our decision procedure can also be applied to prove again the decidability of authentication-like properties and the decidability of a significant fragment of protocols with timestamps.",
        "published": "2007-08-27T11:20:33Z",
        "link": "http://arxiv.org/abs/0708.3564v2",
        "categories": [
            "cs.LO",
            "cs.CR",
            "F.3.1"
        ]
    },
    {
        "title": "HORPO with Computability Closure : A Reconstruction",
        "authors": [
            "Frédéric Blanqui",
            "Jean-Pierre Jouannaud",
            "Albert Rubio"
        ],
        "summary": "This paper provides a new, decidable definition of the higher- order recursive path ordering in which type comparisons are made only when needed, therefore eliminating the need for the computability clo- sure, and bound variables are handled explicitly, making it possible to handle recursors for arbitrary strictly positive inductive types.",
        "published": "2007-08-27T12:23:16Z",
        "link": "http://arxiv.org/abs/0708.3582v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Verified Real Number Calculations: A Library for Interval Arithmetic",
        "authors": [
            "Marc Daumas",
            "David Lester",
            "César Muñoz"
        ],
        "summary": "Real number calculations on elementary functions are remarkably difficult to handle in mechanical proofs. In this paper, we show how these calculations can be performed within a theorem prover or proof assistant in a convenient and highly automated as well as interactive way. First, we formally establish upper and lower bounds for elementary functions. Then, based on these bounds, we develop a rational interval arithmetic where real number calculations take place in an algebraic setting. In order to reduce the dependency effect of interval arithmetic, we integrate two techniques: interval splitting and taylor series expansions. This pragmatic approach has been developed, and formally verified, in a theorem prover. The formal development also includes a set of customizable strategies to automate proofs involving explicit calculations over real numbers. Our ultimate goal is to provide guaranteed proofs of numerical properties with minimal human theorem-prover interaction.",
        "published": "2007-08-28T07:14:29Z",
        "link": "http://arxiv.org/abs/0708.3721v1",
        "categories": [
            "cs.MS",
            "cs.LO"
        ]
    },
    {
        "title": "Raising a Hardness Result",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "This article presents a technique for proving problems hard for classes of the polynomial hierarchy or for PSPACE. The rationale of this technique is that some problem restrictions are able to simulate existential or universal quantifiers. If this is the case, reductions from Quantified Boolean Formulae (QBF) to these restrictions can be transformed into reductions from QBFs having one more quantifier in the front. This means that a proof of hardness of a problem at level n in the polynomial hierarchy can be split into n separate proofs, which may be simpler than a proof directly showing a reduction from a class of QBFs to the considered problem.",
        "published": "2007-08-30T14:42:50Z",
        "link": "http://arxiv.org/abs/0708.4170v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Classical and Effective Descriptive Complexities of omega-Powers",
        "authors": [
            "Olivier Finkel",
            "Dominique Lecomte"
        ],
        "summary": "We prove that, for each non null countable ordinal alpha, there exist some Sigma^0_alpha-complete omega-powers, and some Pi^0_alpha-complete omega-powers, extending previous works on the topological complexity of omega-powers. We prove effective versions of these results. In particular, for each non null recursive ordinal alpha, there exists a recursive finitary language A such that A^omega is Sigma^0_alpha-complete (respectively, Pi^0_alpha-complete). To do this, we prove effective versions of a result by Kuratowski, describing a Borel set as the range of a closed subset of the Baire space by a continuous bijection. This leads us to prove closure properties for the classes Effective-Pi^0_alpha and Effective-Sigma^0_alpha of the hyperarithmetical hierarchy in arbitrary recursively presented Polish spaces. We apply our existence results to get better computations of the topological complexity of some sets of dictionaries considered by the second author in [Omega-Powers and Descriptive Set Theory, Journal of Symbolic Logic, Volume 70 (4), 2005, p. 1210-1232].",
        "published": "2007-08-30T14:56:24Z",
        "link": "http://arxiv.org/abs/0708.4176v2",
        "categories": [
            "math.LO",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Logic Column 19: Symbolic Model Checking for Temporal-Epistemic Logics",
        "authors": [
            "Alessio Lomuscio",
            "Wojciech Penczek"
        ],
        "summary": "This article surveys some of the recent work in verification of temporal epistemic logic via symbolic model checking, focusing on OBDD-based and SAT-based approaches for epistemic logics built on discrete and real-time branching time temporal logics.",
        "published": "2007-09-04T14:38:10Z",
        "link": "http://arxiv.org/abs/0709.0446v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "On the Protocol Composition Logic PCL",
        "authors": [
            "Cas Cremers"
        ],
        "summary": "A recent development in formal security protocol analysis is the Protocol Composition Logic (PCL). We identify a number of problems with this logic as well as with extensions of the logic, as defined in [DDMP05,HSD+05,He05,Dat05,Der06,DDMR07]. The identified problems imply strong restrictions on the scope of PCL, and imply that some currently claimed PCL proofs cannot be proven within the logic, or make use of unsound axioms. Where possible, we propose solutions for these problems.",
        "published": "2007-09-07T13:32:06Z",
        "link": "http://arxiv.org/abs/0709.1080v7",
        "categories": [
            "cs.CR",
            "cs.LO"
        ]
    },
    {
        "title": "On the Proof Complexity of Deep Inference",
        "authors": [
            "Paola Bruscoli",
            "Alessio Guglielmi"
        ],
        "summary": "We obtain two results about the proof complexity of deep inference: 1) deep-inference proof systems are as powerful as Frege ones, even when both are extended with the Tseitin extension rule or with the substitution rule; 2) there are analytic deep-inference proof systems that exhibit an exponential speed-up over analytic Gentzen proof systems that they polynomially simulate.",
        "published": "2007-09-08T11:35:28Z",
        "link": "http://arxiv.org/abs/0709.1201v3",
        "categories": [
            "cs.CC",
            "cs.LO",
            "math.LO",
            "F.2.2; F.4.1"
        ]
    },
    {
        "title": "Normalisation Control in Deep Inference via Atomic Flows",
        "authors": [
            "Alessio Guglielmi",
            "Tom Gundersen"
        ],
        "summary": "We introduce `atomic flows': they are graphs obtained from derivations by tracing atom occurrences and forgetting the logical structure. We study simple manipulations of atomic flows that correspond to complex reductions on derivations. This allows us to prove, for propositional logic, a new and very general normalisation theorem, which contains cut elimination as a special case. We operate in deep inference, which is more general than other syntactic paradigms, and where normalisation is more difficult to control. We argue that atomic flows are a significant technical advance for normalisation theory, because 1) the technique they support is largely independent of syntax; 2) indeed, it is largely independent of logical inference rules; 3) they constitute a powerful geometric formalism, which is more intuitive than syntax.",
        "published": "2007-09-08T12:24:19Z",
        "link": "http://arxiv.org/abs/0709.1205v3",
        "categories": [
            "math.LO",
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "A proof of strong normalisation using domain theory",
        "authors": [
            "Thierry Coquand",
            "Arnaud Spiwack"
        ],
        "summary": "Ulrich Berger presented a powerful proof of strong normalisation using domains, in particular it simplifies significantly Tait's proof of strong normalisation of Spector's bar recursion. The main contribution of this paper is to show that, using ideas from intersection types and Martin-Lof's domain interpretation of type theory one can in turn simplify further U. Berger's argument. We build a domain model for an untyped programming language where U. Berger has an interpretation only for typed terms or alternatively has an interpretation for untyped terms but need an extra condition to deduce strong normalisation. As a main application, we show that Martin-L\\\"{o}f dependent type theory extended with a program for Spector double negation shift.",
        "published": "2007-09-10T14:08:26Z",
        "link": "http://arxiv.org/abs/0709.1401v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.4.1"
        ]
    },
    {
        "title": "Cirquent calculus deepened",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Cirquent calculus is a new proof-theoretic and semantic framework, whose main distinguishing feature is being based on circuits, as opposed to the more traditional approaches that deal with tree-like objects such as formulas or sequents. Among its advantages are greater efficiency, flexibility and expressiveness. This paper presents a detailed elaboration of a deep-inference cirquent logic, which is naturally and inherently resource conscious. It shows that classical logic, both syntactically and semantically, is just a special, conservative fragment of this more general and, in a sense, more basic logic -- the logic of resources in the form of cirquent calculus. The reader will find various arguments in favor of switching to the new framework, such as arguments showing the insufficiency of the expressive power of linear logic or other formula-based approaches to developing resource logics, exponential improvements over the traditional approaches in both representational and proof complexities offered by cirquent calculus, and more. Among the main purposes of this paper is to provide an introductory-style starting point for what, as the author wishes to hope, might have a chance to become a new line of research in proof theory -- a proof theory based on circuits instead of formulas.",
        "published": "2007-09-10T18:00:51Z",
        "link": "http://arxiv.org/abs/0709.1308v3",
        "categories": [
            "cs.LO",
            "math.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Efficient Tabling Mechanisms for Transaction Logic Programs",
        "authors": [
            "Paul Fodor"
        ],
        "summary": "In this paper we present efficient evaluation algorithms for the Horn Transaction Logic (a generalization of the regular Horn logic programs with state updates). We present two complementary methods for optimizing the implementation of Transaction Logic. The first method is based on tabling and we modified the proof theory to table calls and answers on states (practically, equivalent to dynamic programming). The call-answer table is indexed on the call and a signature of the state in which the call was made. The answer columns contain the answer unification and a signature of the state after the call was executed. The states are signed efficiently using a technique based on tries and counting. The second method is based on incremental evaluation and it applies when the data oracle contains derived relations. The deletions and insertions (executed in the transaction oracle) change the state of the database. Using the heuristic of inertia (only a part of the state changes in response to elementary updates), most of the time it is cheaper to compute only the changes in the state than to recompute the entire state from scratch. The two methods are complementary by the fact that the first method optimizes the evaluation when a call is repeated in the same state, and the second method optimizes the evaluation of a new state when a call-state pair is not found by the tabling mechanism (i.e. the first method). The proof theory of Transaction Logic with the application of tabling and incremental evaluation is sound and complete with respect to its model theory.",
        "published": "2007-09-11T19:00:02Z",
        "link": "http://arxiv.org/abs/0709.1699v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Incremental Satisfiability and Implication for UTVPI Constraints",
        "authors": [
            "Andreas Schutt",
            "Peter J. Stuckey"
        ],
        "summary": "Unit two-variable-per-inequality (UTVPI) constraints form one of the largest class of integer constraints which are polynomial time solvable (unless P=NP). There is considerable interest in their use for constraint solving, abstract interpretation, spatial databases, and theorem proving. In this paper we develop a new incremental algorithm for UTVPI constraint satisfaction and implication checking that requires O(m + n log n + p) time and O(n+m+p) space to incrementally check satisfiability of m UTVPI constraints on n variables and check implication of p UTVPI constraints.",
        "published": "2007-09-19T06:58:05Z",
        "link": "http://arxiv.org/abs/0709.2961v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "cs.LO",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Algebraic characterization of logically defined tree languages",
        "authors": [
            "Zoltan Esik",
            "Pascal Weil"
        ],
        "summary": "We give an algebraic characterization of the tree languages that are defined by logical formulas using certain Lindstr\\\"om quantifiers. An important instance of our result concerns first-order definable tree languages. Our characterization relies on the usage of preclones, an algebraic structure introduced by the authors in a previous paper, and of the block product operation on preclones. Our results generalize analogous results on finite word languages, but it must be noted that, as they stand, they do not yield an algorithm to decide whether a given regular tree language is first-order definable.",
        "published": "2007-09-19T07:14:08Z",
        "link": "http://arxiv.org/abs/0709.2962v3",
        "categories": [
            "cs.LO",
            "math.LO",
            "F.4.3; F.4.1"
        ]
    },
    {
        "title": "Query Evaluation in P2P Systems of Taxonomy-based Sources: Algorithms,   Complexity, and Optimizations",
        "authors": [
            "Carlo Meghini",
            "Yannis Tzitzikas",
            "Anastasia Analyti"
        ],
        "summary": "In this study, we address the problem of answering queries over a peer-to-peer system of taxonomy-based sources. A taxonomy states subsumption relationships between negation-free DNF formulas on terms and negation-free conjunctions of terms. To the end of laying the foundations of our study, we first consider the centralized case, deriving the complexity of the decision problem and of query evaluation. We conclude by presenting an algorithm that is efficient in data complexity and is based on hypergraphs. More expressive forms of taxonomies are also investigated, which however lead to intractability. We then move to the distributed case, and introduce a logical model of a network of taxonomy-based sources. On such network, a distributed version of the centralized algorithm is then presented, based on a message passing paradigm, and its correctness is proved. We finally discuss optimization issues, and relate our work to the literature.",
        "published": "2007-09-19T15:10:05Z",
        "link": "http://arxiv.org/abs/0709.3034v1",
        "categories": [
            "cs.DB",
            "cs.DC",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "An efficient simulation algorithm based on abstract interpretation",
        "authors": [
            "Francesco Ranzato",
            "Francesco Tapparo"
        ],
        "summary": "A number of algorithms for computing the simulation preorder are available. Let Sigma denote the state space, -> the transition relation and Psim the partition of Sigma induced by simulation equivalence. The algorithms by Henzinger, Henzinger, Kopke and by Bloom and Paige run in O(|Sigma||->|)-time and, as far as time-complexity is concerned, they are the best available algorithms. However, these algorithms have the drawback of a space complexity that is more than quadratic in the size of the state space. The algorithm by Gentilini, Piazza, Policriti--subsequently corrected by van Glabbeek and Ploeger--appears to provide the best compromise between time and space complexity. Gentilini et al.'s algorithm runs in O(|Psim|^2|->|)-time while the space complexity is in O(|Psim|^2 + |Sigma|log|Psim|). We present here a new efficient simulation algorithm that is obtained as a modification of Henzinger et al.'s algorithm and whose correctness is based on some techniques used in applications of abstract interpretation to model checking. Our algorithm runs in O(|Psim||->|)-time and O(|Psim||Sigma|log|Sigma|)-space. Thus, this algorithm improves the best known time bound while retaining an acceptable space complexity that is in general less than quadratic in the size of the state space. An experimental evaluation showed good comparative results with respect to Henzinger, Henzinger and Kopke's algorithm.",
        "published": "2007-09-26T09:54:31Z",
        "link": "http://arxiv.org/abs/0709.4118v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "On the interaction between sharing and linearity",
        "authors": [
            "Gianluca Amato",
            "Francesca Scozzari"
        ],
        "summary": "In the analysis of logic programs, abstract domains for detecting sharing and linearity information are widely used. Devising abstract unification algorithms for such domains has proved to be rather hard. At the moment, the available algorithms are correct but not optimal, i.e., they cannot fully exploit the information conveyed by the abstract domains. In this paper, we define a new (infinite) domain ShLin-w which can be thought of as a general framework from which other domains can be easily derived by abstraction. ShLin-w makes the interaction between sharing and linearity explicit. We provide a constructive characterization of the optimal abstract unification operator on ShLin-w and we lift it to two well-known abstractions of ShLin-w. Namely, to the classical Sharing X Lin abstract domain and to the more precise ShLin-2 abstract domain by Andy King. In the case of single binding substitutions, we obtain optimal abstract unification algorithms for such domains.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2007-10-02T13:29:28Z",
        "link": "http://arxiv.org/abs/0710.0528v2",
        "categories": [
            "cs.PL",
            "cs.LO"
        ]
    },
    {
        "title": "Two algorithms in search of a type system",
        "authors": [
            "Norman Danner",
            "James S. Royer"
        ],
        "summary": "The authors' ATR programming formalism is a version of call-by-value PCF under a complexity-theoretically motivated type system. ATR programs run in type-2 polynomial-time and all standard type-2 basic feasible functionals are ATR-definable (ATR types are confined to levels 0, 1, and 2). A limitation of the original version of ATR is that the only directly expressible recursions are tail-recursions. Here we extend ATR so that a broad range of affine recursions are directly expressible. In particular, the revised ATR can fairly naturally express the classic insertion- and selection-sort algorithms, thus overcoming a sticking point of most prior implicit-complexity-based formalisms. The paper's main work is in refining the original time-complexity semantics for ATR to show that these new recursion schemes do not lead out of the realm of feasibility.",
        "published": "2007-10-03T16:04:59Z",
        "link": "http://arxiv.org/abs/0710.0824v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.3; F.1.3"
        ]
    },
    {
        "title": "Verification of Ptime Reducibility for system F Terms: Type Inference   in<br> Dual Light Affine Logic",
        "authors": [
            "Vincent Atassi",
            "Patrick Baillot",
            "Kazushige Terui"
        ],
        "summary": "In a previous work Baillot and Terui introduced Dual light affine logic (DLAL) as a variant of Light linear logic suitable for guaranteeing complexity properties on lambda calculus terms: all typable terms can be evaluated in polynomial time by beta reduction and all Ptime functions can be represented. In the present work we address the problem of typing lambda-terms in second-order DLAL. For that we give a procedure which, starting with a term typed in system F, determines whether it is typable in DLAL and outputs a concrete typing if there exists any. We show that our procedure can be run in time polynomial in the size of the original Church typed system F term.",
        "published": "2007-10-05T09:24:31Z",
        "link": "http://arxiv.org/abs/0710.1153v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.2.2; D.1.1"
        ]
    },
    {
        "title": "Diagrammatic Inference",
        "authors": [
            "Dominique Duval"
        ],
        "summary": "Diagrammatic logics were introduced in 2002, with emphasis on the notions of specifications and models. In this paper we improve the description of the inference process, which is seen as a Yoneda functor on a bicategory of fractions. A diagrammatic logic is defined from a morphism of limit sketches (called a propagator) which gives rise to an adjunction, which in turn determines a bicategory of fractions. The propagator, the adjunction and the bicategory provide respectively the syntax, the models and the inference process for the logic. Then diagrammatic logics and their morphisms are applied to the semantics of side effects in computer languages.",
        "published": "2007-10-05T12:36:02Z",
        "link": "http://arxiv.org/abs/0710.1208v2",
        "categories": [
            "cs.LO",
            "math.CT"
        ]
    },
    {
        "title": "Association Rules in the Relational Calculus",
        "authors": [
            "Oliver Schulte",
            "Flavia Moser",
            "Martin Ester",
            "Zhiyong Lu"
        ],
        "summary": "One of the most utilized data mining tasks is the search for association rules. Association rules represent significant relationships between items in transactions. We extend the concept of association rule to represent a much broader class of associations, which we refer to as \\emph{entity-relationship rules.} Semantically, entity-relationship rules express associations between properties of related objects. Syntactically, these rules are based on a broad subclass of safe domain relational calculus queries. We propose a new definition of support and confidence for entity-relationship rules and for the frequency of entity-relationship queries. We prove that the definition of frequency satisfies standard probability axioms and the Apriori property.",
        "published": "2007-10-10T18:00:44Z",
        "link": "http://arxiv.org/abs/0710.2083v1",
        "categories": [
            "cs.DB",
            "cs.LG",
            "cs.LO"
        ]
    },
    {
        "title": "The Variable Hierarchy for the Games mu-Calculus",
        "authors": [
            "Walid Belkhir",
            "Luigi Santocanale"
        ],
        "summary": "Parity games are combinatorial representations of closed Boolean mu-terms. By adding to them draw positions, they have been organized by Arnold and one of the authors into a mu-calculus. As done by Berwanger et al. for the propositional modal mu-calculus, it is possible to classify parity games into levels of a hierarchy according to the number of fixed-point variables. We ask whether this hierarchy collapses w.r.t. the standard interpretation of the games mu-calculus into the class of all complete lattices. We answer this question negatively by providing, for each n >= 1, a parity game Gn with these properties: it unravels to a mu-term built up with n fixed-point variables, it is semantically equivalent to no game with strictly less than n-2 fixed-point variables.",
        "published": "2007-10-12T09:50:43Z",
        "link": "http://arxiv.org/abs/0710.2419v2",
        "categories": [
            "cs.LO",
            "cs.GT",
            "math.LO"
        ]
    },
    {
        "title": "Generic Trace Semantics via Coinduction",
        "authors": [
            "Ichiro Hasuo",
            "Bart Jacobs",
            "Ana Sokolova"
        ],
        "summary": "Trace semantics has been defined for various kinds of state-based systems, notably with different forms of branching such as non-determinism vs. probability. In this paper we claim to identify one underlying mathematical structure behind these \"trace semantics,\" namely coinduction in a Kleisli category. This claim is based on our technical result that, under a suitably order-enriched setting, a final coalgebra in a Kleisli category is given by an initial algebra in the category Sets. Formerly the theory of coalgebras has been employed mostly in Sets where coinduction yields a finer process semantics of bisimilarity. Therefore this paper extends the application field of coalgebras, providing a new instance of the principle \"process semantics via coinduction.\"",
        "published": "2007-10-12T16:28:43Z",
        "link": "http://arxiv.org/abs/0710.2505v2",
        "categories": [
            "cs.LO",
            "F.3.1; F.3.2; G.3"
        ]
    },
    {
        "title": "Automatic Methods for Analyzing Non-Repudiation Protocols with an Active   Intruder",
        "authors": [
            "Francis Klay",
            "Judson Santiago",
            "Laurent Vigneron"
        ],
        "summary": "Non-repudiation protocols have an important role in many areas where secured transactions with proofs of participation are necessary. Formal methods are clever and without error, therefore using them for verifying such protocols is crucial. In this purpose, we show how to partially represent non-repudiation as a combination of authentications on the Fair Zhou-Gollmann protocol. After discussing its limits, we define a new method based on the handling of the knowledge of protocol participants. This method is very general and is of natural use, as it consists in adding simple annotations, like for authentication problems. The method is very easy to implement in tools able to handle participants knowledge. We have implemented it in the AVISPA Tool and analyzed the optimistic Cederquist-Corin- Dashti protocol, discovering two unknown attacks. This extension of the AVISPA Tool for handling non-repudiation opens a highway to the specification of many other properties, without any more change in the tool itself.",
        "published": "2007-10-17T14:16:25Z",
        "link": "http://arxiv.org/abs/0710.3305v2",
        "categories": [
            "cs.LO",
            "cs.CR"
        ]
    },
    {
        "title": "Model and Program Repair via SAT Solving",
        "authors": [
            "Paul C. Attie",
            "Jad Saklawi"
        ],
        "summary": "We consider the following \\emph{model repair problem}: given a finite Kripke structure $M$ and a specification formula $\\eta$ in some modal or temporal logic, determine if $M$ contains a substructure $M'$ (with the same initial state) that satisfies $\\eta$. Thus, $M$ can be ``repaired'' to satisfy the specification $\\eta$ by deleting some transitions.   We map an instance $(M, \\eta)$ of model repair to a boolean formula $\\repfor(M,\\eta)$ such that $(M, \\eta)$ has a solution iff $\\repfor(M,\\eta)$ is satisfiable. Furthermore, a satisfying assignment determines which transitions must be removed from $M$ to generate a model $M'$ of $\\eta$. Thus, we can use any SAT solver to repair Kripke structures. Furthermore, using a complete SAT solver yields a complete algorithm: it always finds a repair if one exists.   We extend our method to repair finite-state shared memory concurrent programs, to solve the discrete event supervisory control problem \\cite{RW87,RW89}, to check for the existence of symmettric solutions \\cite{ES93}, and to accomodate any boolean constraint on the existence of states and transitions in the repaired model.   Finally, we show that model repair is NP-complete for CTL, and logics with polynomial model checking algorithms to which CTL can be reduced in polynomial time. A notable example of such a logic is Alternating-Time Temporal Logic (ATL).",
        "published": "2007-10-17T16:56:56Z",
        "link": "http://arxiv.org/abs/0710.3332v4",
        "categories": [
            "cs.LO",
            "F.3.1; F.4.1; D.2.2"
        ]
    },
    {
        "title": "Design of a Distributed Reachability Algorithm for Analysis of Linear   Hybrid Automata",
        "authors": [
            "Sumit Kumar Jha"
        ],
        "summary": "This paper presents the design of a novel distributed algorithm d-IRA for the reachability analysis of linear hybrid automata. Recent work on iterative relaxation abstraction (IRA) is leveraged to distribute the computational problem among multiple computational nodes in a non-redundant manner by performing careful infeasibility analysis of linear programs corresponding to spurious counterexamples. The d-IRA algorithm is resistant to failure of multiple computational nodes. The experimental results provide promising evidence for the possible successful application of this technique.",
        "published": "2007-10-19T19:32:39Z",
        "link": "http://arxiv.org/abs/0710.3764v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Remarks on Jurdzinski and Lorys' proof that palindromes are not a   Church-Rosser language",
        "authors": [
            "Colm O. Dunlaing",
            "Natalie Schluter"
        ],
        "summary": "In 2002 Jurdzinski and Lorys settled a long-standing conjecture that palindromes are not a Church-Rosser language. Their proof required a sophisticated theory about computation graphs of 2-stack automata. We present their proof in terms of 1-tape Turing machines.We also provide an alternative proof of Buntrock and Otto's result that the set of non-square bitstrings, which is context-free, is not Church-Rosser.",
        "published": "2007-10-24T15:40:32Z",
        "link": "http://arxiv.org/abs/0710.4499v2",
        "categories": [
            "cs.LO",
            "F.4.2; F.4.3"
        ]
    },
    {
        "title": "Space-Efficient Bounded Model Checking",
        "authors": [
            "Jacob Katz",
            "Ziyad Hanna",
            "Nachum Dershowitz"
        ],
        "summary": "Current algorithms for bounded model checking use SAT methods for checking satisfiability of Boolean formulae. These methods suffer from the potential memory explosion problem. Methods based on the validity of Quantified Boolean Formulae (QBF) allow an exponentially more succinct representation of formulae to be checked, because no \"unrolling\" of the transition relation is required. These methods have not been widely used, because of the lack of an efficient decision procedure for QBF. We evaluate the usage of QBF in bounded model checking (BMC), using general-purpose SAT and QBF solvers. We develop a special-purpose decision procedure for QBF used in BMC, and compare our technique with the methods using general-purpose SAT and QBF solvers on real-life industrial benchmarks.",
        "published": "2007-10-25T08:06:59Z",
        "link": "http://arxiv.org/abs/0710.4629v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Verification of Embedded Memory Systems using Efficient Memory Modeling",
        "authors": [
            "Malay K. Ganai",
            "Aarti Gupta",
            "Pranav Ashar"
        ],
        "summary": "We describe verification techniques for embedded memory systems using efficient memory modeling (EMM), without explicitly modeling each memory bit. We extend our previously proposed approach of EMM in Bounded Model Checking (BMC) for a single read/write port single memory system, to more commonly occurring systems with multiple memories, having multiple read and write ports. More importantly, we augment such EMM to providing correctness proofs, in addition to finding real bugs as before. The novelties of our verification approach are in a) combining EMM with proof-based abstraction that preserves the correctness of a property up to a certain analysis depth of SAT-based BMC, and b) modeling arbitrary initial memory state precisely and thereby, providing inductive proofs using SAT-based BMC for embedded memory systems. Similar to the previous approach, we construct a verification model by eliminating memory arrays, but retaining the memory interface signals with their control logic and adding constraints on those signals at every analysis depth to preserve the data forwarding semantics. The size of these EMM constraints depends quadratically on the number of memory accesses and the number of read and write ports; and linearly on the address and data widths and the number of memories. We show the effectiveness of our approach on several industry designs and software programs.",
        "published": "2007-10-25T08:44:05Z",
        "link": "http://arxiv.org/abs/0710.4666v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Functional Equivalence Checking for Verification of Algebraic   Transformations on Array-Intensive Source Code",
        "authors": [
            "K. C. Shashidhar",
            "Maurice Bruynooghe",
            "Francky Catthoor",
            "Gerda Janssens"
        ],
        "summary": "Development of energy and performance-efficient embedded software is increasingly relying on application of complex transformations on the critical parts of the source code. Designers applying such nontrivial source code transformations are often faced with the problem of ensuring functional equivalence of the original and transformed programs. Currently they have to rely on incomplete and time-consuming simulation. Formal automatic verification of the transformed program against the original is instead desirable. This calls for equivalence checking tools similar to the ones available for comparing digital circuits. We present such a tool to compare array-intensive programs related through a combination of important global transformations like expression propagations, loop and algebraic transformations. When the transformed program fails to pass the equivalence check, the tool provides specific feedback on the possible locations of errors.",
        "published": "2007-10-25T09:09:59Z",
        "link": "http://arxiv.org/abs/0710.4689v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Exact Synthesis of 3-Qubit Quantum Circuits from Non-Binary Quantum   Gates Using Multiple-Valued Logic and Group Theory",
        "authors": [
            "Guowu Yang",
            "William N. N. Hung",
            "Xiaoyu Song",
            "Marek Perkowski"
        ],
        "summary": "We propose an approach to optimally synthesize quantum circuits from non-permutative quantum gates such as Controlled-Square-Root-of-Not (i.e. Controlled-V). Our approach reduces the synthesis problem to multiple-valued optimization and uses group theory. We devise a novel technique that transforms the quantum logic synthesis problem from a multi-valued constrained optimization problem to a group permutation problem. The transformation enables us to utilize group theory to exploit the properties of the synthesis problem. Assuming a cost of one for each two-qubit gate, we found all reversible circuits with quantum costs of 4, 5, 6, etc, and give another algorithm to realize these reversible circuits with quantum gates.",
        "published": "2007-10-25T09:14:41Z",
        "link": "http://arxiv.org/abs/0710.4694v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "SAT-Based Complete Don't-Care Computation for Network Optimization",
        "authors": [
            "Alan Mishchenko",
            "Robert K. Brayton"
        ],
        "summary": "This paper describes an improved approach to Boolean network optimization using internal don't-cares. The improvements concern the type of don't-cares computed, their scope, and the computation method. Instead of the traditionally used compatible observability don't-cares (CODCs), we introduce and justify the use of complete don't-cares (CDC). To ensure the robustness of the don't-care computation for very large industrial networks, a optional windowing scheme is implemented that computes substantial subsets of the CDCs in reasonable time. Finally, we give a SAT-based don't-care computation algorithm that is more efficient than BDD-based algorithms. Experimental results confirm that these improvements work well in practice. Complete don't-cares allow for a reduction in the number of literals compared to the CODCs. Windowing guarantees robustness, even for very large benchmarks on which previous methods could not be applied. SAT reduces the runtime and enhances robustness, making don't-cares affordable for a variety of other Boolean methods applied to the network.",
        "published": "2007-10-25T09:15:10Z",
        "link": "http://arxiv.org/abs/0710.4695v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Automated Synthesis of Assertion Monitors using Visual Specifications",
        "authors": [
            "Ambar A. Gadkari",
            "S. Ramesh"
        ],
        "summary": "Automated synthesis of monitors from high-level properties plays a significant role in assertion-based verification. We present here a methodology to synthesize assertion monitors from visual specifications given in CESC (Clocked Event Sequence Chart). CESC is a visual language designed for specifying system level interactions involving single and multiple clock domains. It has well-defined graphical and textual syntax and formal semantics based on synchronous language paradigm enabling formal analysis of specifications. In this paper we provide an overview of CESC language with few illustrative examples. The algorithm for automated synthesis of assertion monitors from CESC specifications is described. A few examples from standard bus protocols (OCP-IP and AMBA) are presented to demonstrate the application of monitor synthesis algorithm.",
        "published": "2007-10-25T09:18:46Z",
        "link": "http://arxiv.org/abs/0710.4698v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Efficient Solution of Language Equations Using Partitioned   Representations",
        "authors": [
            "Alan Mishchenko",
            "Robert Brayton",
            "Roland Jiang",
            "Tiziano Villa",
            "Nina Yevtushenko"
        ],
        "summary": "A class of discrete event synthesis problems can be reduced to solving language equations f . X &sube; S, where F is the fixed component and S the specification. Sequential synthesis deals with FSMs when the automata for F and S are prefix closed, and are naturally represented by multi-level networks with latches. For this special case, we present an efficient computation, using partitioned representations, of the most general prefix-closed solution of the above class of language equations. The transition and the output relations of the FSMs for F and S in their partitioned form are represented by the sets of output and next state functions of the corresponding networks. Experimentally, we show that using partitioned representations is much faster than using monolithic representations, as well as applicable to larger problem instances.",
        "published": "2007-10-25T09:45:31Z",
        "link": "http://arxiv.org/abs/0710.4743v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Verifying Safety-Critical Timing and Memory-Usage Properties of Embedded   Software by Abstract Interpretation",
        "authors": [
            "Reinhold Heckmann",
            "Christian Ferdinand"
        ],
        "summary": "Static program analysis by abstract interpretation is an efficient method to determine properties of embedded software. One example is value analysis, which determines the values stored in the processor registers. Its results are used as input to more advanced analyses, which ultimately yield information about the stack usage and the timing behavior of embedded software.",
        "published": "2007-10-25T09:52:50Z",
        "link": "http://arxiv.org/abs/0710.4753v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "An Integrated Design and Verification Methodology for Reconfigurable   Multimedia Systems",
        "authors": [
            "M. Borgatti",
            "A. Capello",
            "U. Rossi",
            "J. -L. Lambert",
            "I. Moussa",
            "F. Fummi",
            "G. Pravadelli"
        ],
        "summary": "Recently a lot of multimedia applications are emerging on portable appliances. They require both the flexibility of upgradeable devices (traditionally software based) and a powerful computing engine (typically hardware). In this context, programmable HW and dynamic reconfiguration allow novel approaches to the migration of algorithms from SW to HW. Thus, in the frame of the Symbad project, we propose an industrial design flow for reconfigurable SoC's. The goal of Symbad consists of developing a system level design platform for hardware and software SoC systems including formal and semi-formal verification techniques.",
        "published": "2007-10-25T12:24:16Z",
        "link": "http://arxiv.org/abs/0710.4846v1",
        "categories": [
            "cs.MM",
            "cs.LO"
        ]
    },
    {
        "title": "A Formal Verification Methodology for Checking Data Integrity",
        "authors": [
            "Yasushi Umezawa",
            "Takeshi Shimizu"
        ],
        "summary": "Formal verification techniques have been playing an important role in pre-silicon validation processes. One of the most important points considered in performing formal verification is to define good verification scopes; we should define clearly what to be verified formally upon designs under tests. We considered the following three practical requirements when we defined the scope of formal verification. They are (a) hard to verify (b) small to handle, and (c) easy to understand. Our novel approach is to break down generic properties for system into stereotype properties in block level and to define requirements for Verifiable RTL. Consequently, each designer instead of verification experts can describe properties of the design easily, and formal model checking can be applied systematically and thoroughly to all the leaf modules. During the development of a component chip for server platforms, we focused on RAS (Reliability, Availability, and Serviceability) features and described more than 2000 properties in PSL. As a result of the formal verification, we found several critical logic bugs in a short time with limited resources, and successfully verified all of them. This paper presents a study of the functional verification methodology.",
        "published": "2007-10-25T12:24:36Z",
        "link": "http://arxiv.org/abs/0710.4848v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Common Reusable Verification Environment for BCA and RTL Models",
        "authors": [
            "Giuseppe Falconeri",
            "Walid Naifer",
            "Nizar Romdhane"
        ],
        "summary": "This paper deals with a common verification methodology and environment for SystemC BCA and RTL models. The aim is to save effort by avoiding the same work done twice by different people and to reuse the same environment for the two design views. Applying this methodology the verification task starts as soon as the functional specification is signed off and it runs in parallel to the models and design development. The verification environment is modeled with the aid of dedicated verification languages and it is applied to both the models. The test suite is exactly the same and thus it's possible to verify the alignment between the two models. In fact the final step is to check the cycle-by-cycle match of the interface behavior. A regression tool and a bus analyzer have been developed to help the verification and the alignment process. The former is used to automate the testbench generation and to run the two test suites. The latter is used to verify the alignment between the two models comparing the waveforms obtained in each run. The quality metrics used to validate the flow are full functional coverage and full alignment at each IP port.",
        "published": "2007-10-25T12:25:32Z",
        "link": "http://arxiv.org/abs/0710.4851v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A Proof of the Factorization Forest Theorem",
        "authors": [
            "Manfred Kufleitner"
        ],
        "summary": "We show that for every homomorphism $\\Gamma^+ \\to S$ where $S$ is a finite semigroup there exists a factorization forest of height $\\leq 3 \\abs{S}$. The proof is based on Green's relations.",
        "published": "2007-10-26T15:59:56Z",
        "link": "http://arxiv.org/abs/0710.5130v1",
        "categories": [
            "cs.LO",
            "F.4.3"
        ]
    },
    {
        "title": "Model Checking Synchronized Products of Infinite Transition Systems",
        "authors": [
            "Stefan Wöhrle",
            "Wolfgang Thomas"
        ],
        "summary": "Formal verification using the model checking paradigm has to deal with two aspects: The system models are structured, often as products of components, and the specification logic has to be expressive enough to allow the formalization of reachability properties. The present paper is a study on what can be achieved for infinite transition systems under these premises. As models we consider products of infinite transition systems with different synchronization constraints. We introduce finitely synchronized transition systems, i.e. product systems which contain only finitely many (parameterized) synchronized transitions, and show that the decidability of FO(R), first-order logic extended by reachability predicates, of the product system can be reduced to the decidability of FO(R) of the components. This result is optimal in the following sense: (1) If we allow semifinite synchronization, i.e. just in one component infinitely many transitions are synchronized, the FO(R)-theory of the product system is in general undecidable. (2) We cannot extend the expressive power of the logic under consideration. Already a weak extension of first-order logic with transitive closure, where we restrict the transitive closure operators to arity one and nesting depth two, is undecidable for an asynchronous (and hence finitely synchronized) product, namely for the infinite grid.",
        "published": "2007-10-30T14:39:09Z",
        "link": "http://arxiv.org/abs/0710.5659v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Source-to-source optimizing transformations of Prolog programs based on   abstract interpretation",
        "authors": [
            "Francois Gobert",
            "Baudouin Le Charlier"
        ],
        "summary": "Making a Prolog program more efficient by transforming its source code, without changing its operational semantics, is not an obvious task. It requires the user to have a clear understanding of how the Prolog compiler works, and in particular, of the effects of impure features like the cut. The way a Prolog code is written - e.g., the order of clauses, the order of literals in a clause, the use of cuts or negations - influences its efficiency. Furthermore, different optimization techniques may be redundant or conflicting when they are applied together, depending on the way a procedure is called - e.g., inserting cuts and enabling indexing. We present an optimiser, based on abstract interpretation, that automatically performs safe code transformations of Prolog procedures in the context of some class of input calls. The method is more effective if procedures are annotated with additional information about modes, types, sharing, number of solutions and the like. Thus the approach is similar to Mercury. It applies to any Prolog program, however.",
        "published": "2007-10-31T15:59:50Z",
        "link": "http://arxiv.org/abs/0710.5895v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Coinductive Proof Principles for Stochastic Processes",
        "authors": [
            "Dexter Kozen"
        ],
        "summary": "We give an explicit coinduction principle for recursively-defined stochastic processes. The principle applies to any closed property, not just equality, and works even when solutions are not unique. The rule encapsulates low-level analytic arguments, allowing reasoning about such processes at a higher algebraic level. We illustrate the use of the rule in deriving properties of a simple coin-flip process.",
        "published": "2007-11-01T19:25:13Z",
        "link": "http://arxiv.org/abs/0711.0194v2",
        "categories": [
            "cs.LO",
            "F.4.1; F.3.1; I.1.3; I.2.3"
        ]
    },
    {
        "title": "An interface group for process components",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "We take a process component as a pair of an interface and a behaviour. We study the composition of interacting process components in the setting of process algebra. We formalize the interfaces of interacting process components by means of an interface group. An interesting feature of the interface group is that it allows for distinguishing between expectations and promises in interfaces of process components. This distinction comes into play in case components with both client and server behaviour are involved.",
        "published": "2007-11-06T10:45:02Z",
        "link": "http://arxiv.org/abs/0711.0834v2",
        "categories": [
            "cs.LO",
            "D.2.1; D.2.2; D.2.4; F.1.2; F.3.1"
        ]
    },
    {
        "title": "A thread calculus with molecular dynamics",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "We present a theory of threads, interleaving of threads, and interaction between threads and services with features of molecular dynamics, a model of computation that bears on computations in which dynamic data structures are involved. Threads can interact with services of which the states consist of structured data objects and computations take place by means of actions which may change the structure of the data objects. The features introduced include restriction of the scope of names used in threads to refer to data objects. Because that feature makes it troublesome to provide a model based on structural operational semantics and bisimulation, we construct a projective limit model for the theory.",
        "published": "2007-11-06T11:25:20Z",
        "link": "http://arxiv.org/abs/0711.0840v2",
        "categories": [
            "cs.LO",
            "D.1.3; D.1.5; D.3.3; F.1.1; F.1.2; F.3.2"
        ]
    },
    {
        "title": "An On-the-fly Tableau-based Decision Procedure for PDL-Satisfiability",
        "authors": [
            "Pietro Abate",
            "Rajeev Goré",
            "Florian Widmann"
        ],
        "summary": "We present a tableau-based algorithm for deciding satisfiability for propositional dynamic logic (PDL) which builds a finite rooted tree with ancestor loops and passes extra information from children to parents to separate good loops from bad loops during backtracking. It is easy to implement, with potential for parallelisation, because it constructs a pseudo-model ``on the fly'' by exploring each tableau branch independently. But its worst-case behaviour is 2EXPTIME rather than EXPTIME. A prototype implementation in the TWB (http://twb.rsise.anu.edu.au) is available.",
        "published": "2007-11-07T06:11:13Z",
        "link": "http://arxiv.org/abs/0711.1016v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Considerations on P vs NP",
        "authors": [
            "Alfredo von Reckow"
        ],
        "summary": "In order to prove that the P of problems is different to the NP class, we consider the satisfability problem of propositional calculus formulae, which is an NP-complete problem. It is shown that, for every search algorithm A, there is a set E(A) containing propositional calculus formulae, each of which requires the algorithm A to take non-polynomial time to find the truth-values of its propositional letters satisfying it. Moreover, E(A)'s size is an exponential function of n, which makes it impossible to detect such formulae in a polynomial time. Hence, the satisfability problem does not have a polynomial complexity",
        "published": "2007-11-07T22:32:41Z",
        "link": "http://arxiv.org/abs/0711.1177v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3"
        ]
    },
    {
        "title": "Query Evaluation and Optimization in the Semantic Web",
        "authors": [
            "Edna Ruckhaus",
            "Eduardo Ruiz",
            "Maria-Esther Vidal"
        ],
        "summary": "We address the problem of answering Web ontology queries efficiently. An ontology is formalized as a Deductive Ontology Base (DOB), a deductive database that comprises the ontology's inference axioms and facts. A cost-based query optimization technique for DOB is presented. A hybrid cost model is proposed to estimate the cost and cardinality of basic and inferred facts. Cardinality and cost of inferred facts are estimated using an adaptive sampling technique, while techniques of traditional relational cost models are used for estimating the cost of basic facts and conjunctive ontology queries. Finally, we implement a dynamic-programming optimization algorithm to identify query evaluation plans that minimize the number of intermediate inferred facts. We modeled a subset of the Web ontology language OWL Lite as a DOB, and performed an experimental study to analyze the predictive capacity of our cost model and the benefits of the query optimization technique. Our study has been conducted over synthetic and real-world OWL ontologies, and shows that the techniques are accurate and improve query performance. To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2007-11-13T22:31:09Z",
        "link": "http://arxiv.org/abs/0711.2087v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "F.4.1; H.2.3; I.2.4"
        ]
    },
    {
        "title": "Guarded Hybrid Knowledge Bases",
        "authors": [
            "Stijn Heymans",
            "Jos de Bruijn",
            "Livia Predoiu",
            "Cristina Feier",
            "Davy Van Nieuwenborgh"
        ],
        "summary": "Recently, there has been a lot of interest in the integration of Description Logics and rules on the Semantic Web.We define guarded hybrid knowledge bases (or g-hybrid knowledge bases) as knowledge bases that consist of a Description Logic knowledge base and a guarded logic program, similar to the DL+log knowledge bases from (Rosati 2006). G-hybrid knowledge bases enable an integration of Description Logics and Logic Programming where, unlike in other approaches, variables in the rules of a guarded program do not need to appear in positive non-DL atoms of the body, i.e. DL atoms can act as guards as well. Decidability of satisfiability checking of g-hybrid knowledge bases is shown for the particular DL DLRO, which is close to OWL DL, by a reduction to guarded programs under the open answer set semantics. Moreover, we show 2-EXPTIME-completeness for satisfiability checking of such g-hybrid knowledge bases. Finally, we discuss advantages and disadvantages of our approach compared with DL+log knowledge bases.",
        "published": "2007-11-14T10:49:56Z",
        "link": "http://arxiv.org/abs/0711.2155v1",
        "categories": [
            "cs.LO",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Normalization of IZF with Replacement",
        "authors": [
            "Wojciech Moczydlowski"
        ],
        "summary": "ZF is a well investigated impredicative constructive version of Zermelo-Fraenkel set theory. Using set terms, we axiomatize IZF with Replacement, which we call \\izfr, along with its intensional counterpart \\iizfr. We define a typed lambda calculus $\\li$ corresponding to proofs in \\iizfr according to the Curry-Howard isomorphism principle. Using realizability for \\iizfr, we show weak normalization of $\\li$. We use normalization to prove the disjunction, numerical existence and term existence properties. An inner extensional model is used to show these properties, along with the set existence property, for full, extensional \\izfr.",
        "published": "2007-11-16T02:44:05Z",
        "link": "http://arxiv.org/abs/0711.2546v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "From vectors to mnesors",
        "authors": [
            "Gilles Champenois"
        ],
        "summary": "The mnesor theory is the adaptation of vectors to artificial intelligence. The scalar field is replaced by a lattice. Addition becomes idempotent and multiplication is interpreted as a selection operation. We also show that mnesors can be the foundation for a linear calculus.",
        "published": "2007-12-01T14:37:07Z",
        "link": "http://arxiv.org/abs/0712.0084v4",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.3"
        ]
    },
    {
        "title": "On Decidability Properties of Local Sentences",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "Local (first order) sentences, introduced by Ressayre, enjoy very nice decidability properties, following from some stretching theorems stating some remarkable links between the finite and the infinite model theory of these sentences. We prove here several additional results on local sentences. The first one is a new decidability result in the case of local sentences whose function symbols are at most unary: one can decide, for every regular cardinal k whether a local sentence phi has a model of order type k. Secondly we show that this result can not be extended to the general case. Assuming the consistency of an inaccessible cardinal we prove that the set of local sentences having a model of order type omega_2 is not determined by the axiomatic system ZFC + GCH, where GCH is the generalized continuum hypothesis",
        "published": "2007-12-02T18:34:00Z",
        "link": "http://arxiv.org/abs/0712.0164v1",
        "categories": [
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "On the Accepting Power of 2-Tape Büchi Automata",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We show that, from a topological point of view, 2-tape B\\\"uchi automata have the same accepting power than Turing machines equipped with a B\\\"uchi acceptance condition. In particular, we show that for every non null recursive ordinal alpha, there exist some Sigma^0_alpha-complete and some Pi^0_alpha-complete infinitary rational relations accepted by 2-tape B\\\"uchi automata. This very surprising result gives answers to questions of W. Thomas [Automata and Quantifier Hierarchies, in: Formal Properties of Finite automata and Applications, Ramatuelle, 1988, LNCS 386, Springer, 1989, p.104-119], of P. Simonnet [Automates et Th\\'eorie Descriptive, Ph. D. Thesis, Universit\\'e Paris 7, March 1992], and of H. Lescow and W. Thomas [Logical Specifications of Infinite Computations, In: \"A Decade of Concurrency\", LNCS 803, Springer, 1994, p. 583-621].",
        "published": "2007-12-02T18:36:34Z",
        "link": "http://arxiv.org/abs/0712.0165v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "A Common View on Strong, Uniform, and Other Notions of Equivalence in   Answer-Set Programming",
        "authors": [
            "Stefan Woltran"
        ],
        "summary": "Logic programming under the answer-set semantics nowadays deals with numerous different notions of program equivalence. This is due to the fact that equivalence for substitution (known as strong equivalence) and ordinary equivalence are different concepts. The former holds, given programs P and Q, iff P can be faithfully replaced by Q within any context R, while the latter holds iff P and Q provide the same output, that is, they have the same answer sets. Notions in between strong and ordinary equivalence have been introduced as theoretical tools to compare incomplete programs and are defined by either restricting the syntactic structure of the considered context programs R or by bounding the set A of atoms allowed to occur in R (relativized equivalence).For the latter approach, different A yield properly different equivalence notions, in general. For the former approach, however, it turned out that any ``reasonable'' syntactic restriction to R coincides with either ordinary, strong, or uniform equivalence. In this paper, we propose a parameterization for equivalence notions which takes care of both such kinds of restrictions simultaneously by bounding, on the one hand, the atoms which are allowed to occur in the rule heads of the context and, on the other hand, the atoms which are allowed to occur in the rule bodies of the context. We introduce a general semantical characterization which includes known ones as SE-models (for strong equivalence) or UE-models (for uniform equivalence) as special cases. Moreover,we provide complexity bounds for the problem in question and sketch a possible implementation method.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2007-12-06T14:26:42Z",
        "link": "http://arxiv.org/abs/0712.0948v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Cumulative and Averaging Fission of Beliefs",
        "authors": [
            "Audun Josang"
        ],
        "summary": "Belief fusion is the principle of combining separate beliefs or bodies of evidence originating from different sources. Depending on the situation to be modelled, different belief fusion methods can be applied. Cumulative and averaging belief fusion is defined for fusing opinions in subjective logic, and for fusing belief functions in general. The principle of fission is the opposite of fusion, namely to eliminate the contribution of a specific belief from an already fused belief, with the purpose of deriving the remaining belief. This paper describes fission of cumulative belief as well as fission of averaging belief in subjective logic. These operators can for example be applied to belief revision in Bayesian belief networks, where the belief contribution of a given evidence source can be determined as a function of a given fused belief and its other contributing beliefs.",
        "published": "2007-12-07T16:42:07Z",
        "link": "http://arxiv.org/abs/0712.1182v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Kleene, Rogers and Rice Theorems Revisited in C and in Bash",
        "authors": [
            "Salvatore Caporaso",
            "Nicola Corriero"
        ],
        "summary": "The recursion theorem in the weak form {e}(z)=x(e,z) (universal function not needed) and in Rogers form {n}(z)={{x}(n)}(z) and Rice theorem are proved a first time using programs in C, and a second time with scripts in Bash.",
        "published": "2007-12-08T12:05:40Z",
        "link": "http://arxiv.org/abs/0712.1279v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "About Algorithm for Transformation of Logic Functions (ATLF)",
        "authors": [
            "Lev Cherbanski"
        ],
        "summary": "In this article the algorithm for transformation of logic functions which are given by truth tables is considered. The suggested algorithm allows the transformation of many-valued logic functions with the required number of variables and can be looked in this sense as universal.",
        "published": "2007-12-08T22:36:44Z",
        "link": "http://arxiv.org/abs/0712.1310v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Axiomatizing rational power series",
        "authors": [
            "S. L. Bloom",
            "Z. Esik"
        ],
        "summary": "Iteration semirings are Conway semirings satisfying Conway's group identities. We show that the semirings $\\N^{\\rat}\\llangle \\Sigma^* \\rrangle$ of rational power series with coefficients in the semiring $\\N$ of natural numbers are the free partial iteration semirings. Moreover, we characterize the semirings $\\N_\\infty^{\\rat}\\llangle \\Sigma^* \\rrangle$ as the free semirings in the variety of iteration semirings defined by three additional simple identities, where $\\N_\\infty$ is the completion of $\\N$ obtained by adding a point of infinity. We also show that this latter variety coincides with the variety generated by the complete, or continuous semirings. As a consequence of these results, we obtain that the semirings $\\N_\\infty^{\\rat}\\llangle \\Sigma^* \\rrangle$, equipped with the sum order, are free in the class of symmetric inductive $^*$-semirings. This characterization corresponds to Kozen's axiomatization of regular languages.",
        "published": "2007-12-09T12:45:00Z",
        "link": "http://arxiv.org/abs/0712.1337v2",
        "categories": [
            "cs.LO",
            "cs.DM"
        ]
    },
    {
        "title": "Sequential operators in computability logic",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Computability logic (CL) (see http://www.cis.upenn.edu/~giorgi/cl.html) is a semantical platform and research program for redeveloping logic as a formal theory of computability, as opposed to the formal theory of truth which it has more traditionally been. Formulas in CL stand for (interactive) computational problems, understood as games between a machine and its environment; logical operators represent operations on such entities; and \"truth\" is understood as existence of an effective solution, i.e., of an algorithmic winning strategy.   The formalism of CL is open-ended, and may undergo series of extensions as the study of the subject advances. The main groups of operators on which CL has been focused so far are the parallel, choice, branching, and blind operators. The present paper introduces a new important group of operators, called sequential. The latter come in the form of sequential conjunction and disjunction, sequential quantifiers, and sequential recurrences. As the name may suggest, the algorithmic intuitions associated with this group are those of sequential computations, as opposed to the intuitions of parallel computations associated with the parallel group of operations: playing a sequential combination of games means playing its components in a sequential fashion, one after one.   The main technical result of the present paper is a sound and complete axiomatization of the propositional fragment of computability logic whose vocabulary, together with negation, includes all three -- parallel, choice and sequential -- sorts of conjunction and disjunction. An extension of this result to the first-order level is also outlined.",
        "published": "2007-12-09T16:59:35Z",
        "link": "http://arxiv.org/abs/0712.1345v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Borel Ranks and Wadge Degrees of Context Free Omega Languages",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We show that, from a topological point of view, considering the Borel and the Wadge hierarchies, 1-counter B\\\"uchi automata have the same accepting power than Turing machines equipped with a B\\\"uchi acceptance condition. In particular, for every non null recursive ordinal alpha, there exist some Sigma^0_alpha-complete and some Pi^0_alpha-complete omega context free languages accepted by 1-counter B\\\"uchi automata, and the supremum of the set of Borel ranks of context free omega languages is the ordinal gamma^1_2 which is strictly greater than the first non recursive ordinal. This very surprising result gives answers to questions of H. Lescow and W. Thomas [Logical Specifications of Infinite Computations, In:\"A Decade of Concurrency\", LNCS 803, Springer, 1994, p. 583-621].",
        "published": "2007-12-09T20:01:02Z",
        "link": "http://arxiv.org/abs/0712.1359v1",
        "categories": [
            "cs.LO",
            "cs.GT",
            "math.LO"
        ]
    },
    {
        "title": "Undecidable Problems About Timed Automata",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We solve some decision problems for timed automata which were recently raised by S. Tripakis in [ Folk Theorems on the Determinization and Minimization of Timed Automata, in the Proceedings of the International Workshop FORMATS'2003, LNCS, Volume 2791, p. 182-188, 2004 ] and by E. Asarin in [ Challenges in Timed Languages, From Applied Theory to Basic Theory, Bulletin of the EATCS, Volume 83, p. 106-120, 2004 ]. In particular, we show that one cannot decide whether a given timed automaton is determinizable or whether the complement of a timed regular language is timed regular. We show that the problem of the minimization of the number of clocks of a timed automaton is undecidable. It is also undecidable whether the shuffle of two timed regular languages is timed regular. We show that in the case of timed B\\\"uchi automata accepting infinite timed words some of these problems are Pi^1_1-hard, hence highly undecidable (located beyond the arithmetical hierarchy).",
        "published": "2007-12-09T20:11:42Z",
        "link": "http://arxiv.org/abs/0712.1363v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "On the computational complexity of cut-reduction",
        "authors": [
            "Klaus Aehlig",
            "Arnold Beckmann"
        ],
        "summary": "Using appropriate notation systems for proofs, cut-reduction can often be rendered feasible on these notations, and explicit bounds can be given. Developing a suitable notation system for Bounded Arithmetic, and applying these bounds, all the known results on definable functions of certain such theories can be reobtained in a uniform way.",
        "published": "2007-12-10T14:58:27Z",
        "link": "http://arxiv.org/abs/0712.1499v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "A case study of the difficulty of quantifier elimination in constraint   databases: the alibi query in moving object databases",
        "authors": [
            "Bart Kuijpers",
            "Walied Othman",
            "Rafael Grimson"
        ],
        "summary": "In the constraint database model, spatial and spatio-temporal data are stored by boolean combinations of polynomial equalities and inequalities over the real numbers. The relational calculus augmented with polynomial constraints is the standard first-order query language for constraint databases. Although the expressive power of this query language has been studied extensively, the difficulty of the efficient evaluation of queries, usually involving some form of quantifier elimination, has received considerably less attention. The inefficiency of existing quantifier-elimination software and the intrinsic difficulty of quantifier elimination have proven to be a bottle-neck for for real-world implementations of constraint database systems. In this paper, we focus on a particular query, called the \\emph{alibi query}, that asks whether two moving objects whose positions are known at certain moments in time, could have possibly met, given certain speed constraints. This query can be seen as a constraint database query and its evaluation relies on the elimination of a block of three existential quantifiers. Implementations of general purpose elimination algorithms are in the specific case, for practical purposes, too slow in answering the alibi query and fail completely in the parametric case. The main contribution of this paper is an analytical solution to the parametric alibi query, which can be used to answer this query in the specific case in constant time. We also give an analytic solution to the alibi query at a fixed moment in time. The solutions we propose are based on geometric argumentation and they illustrate the fact that some practical problems require creative solutions, where at least in theory, existing systems could provide a solution.",
        "published": "2007-12-12T18:05:41Z",
        "link": "http://arxiv.org/abs/0712.1996v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.DB"
        ]
    },
    {
        "title": "Is the injectivity of the global function of a cellular automaton in the   hyperbolic plane undecidable?",
        "authors": [
            "Margenstern Maurice"
        ],
        "summary": "In this paper, we look at the following question. We consider cellular automata in the hyperbolic plane and we consider the global function defined on all possible configurations. Is the injectivity of this function undecidable? The problem was answered positively in the case of the Euclidean plane by Jarkko Kari, in 1994. In the present paper, we give a partial answer: when the configurations are restricted to a certain condition, the problem is undecidable.",
        "published": "2007-12-16T15:12:45Z",
        "link": "http://arxiv.org/abs/0712.2577v2",
        "categories": [
            "cs.DM",
            "cs.LO",
            "F.2.2"
        ]
    },
    {
        "title": "Partial Conway and iteration semirings",
        "authors": [
            "S. L. Bloom",
            "Z. Esik",
            "W. Kuich"
        ],
        "summary": "A Conway semiring is a semiring $S$ equipped with a unary operation $^*:S \\to S$, always called 'star', satisfying the sum star and product star identities. It is known that these identities imply a Kleene type theorem. Some computationally important semirings, such as $N$ or $N^{\\rat}\\llangle \\Sigma^* \\rrangle$ of rational power series of words on $\\Sigma$ with coefficients in $N$, cannot have a total star operation satisfying the Conway identities. We introduce here partial Conway semirings, which are semirings $S$ which have a star operation defined only on an ideal of $S$; when the arguments are appropriate, the operation satisfies the above identities. We develop the general theory of partial Conway semirings and prove a Kleene theorem for this generalization.",
        "published": "2007-12-18T13:14:38Z",
        "link": "http://arxiv.org/abs/0712.2952v1",
        "categories": [
            "cs.DM",
            "cs.LO"
        ]
    },
    {
        "title": "Common knowledge logic in a higher order proof assistant?",
        "authors": [
            "Pierre Lescanne"
        ],
        "summary": "This paper presents experiments on common knowledge logic, conducted with the help of the proof assistant Coq. The main feature of common knowledge logic is the eponymous modality that says that a group of agents shares a knowledge about a certain proposition in a inductive way. This modality is specified by using a fixpoint approach. Furthermore, from these experiments, we discuss and compare the structure of theorems that can be proved in specific theories that use common knowledge logic. Those structures manifests the interplay between the theory (as implemented in the proof assistant Coq) and the metatheory.",
        "published": "2007-12-19T10:25:34Z",
        "link": "http://arxiv.org/abs/0712.3147v2",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Clones and Genoids in Lambda Calculus and First Order Logic",
        "authors": [
            "Zhaohua Luo"
        ],
        "summary": "A genoid is a category of two objects such that one is the product of itself with the other. A genoid may be viewed as an abstract substitution algebra. It is a remarkable fact that such a simple concept can be applied to present a unified algebraic approach to lambda calculus and first order logic.",
        "published": "2007-12-19T20:52:48Z",
        "link": "http://arxiv.org/abs/0712.3088v3",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Extending the Overlap Graph for Gene Assembly in Ciliates",
        "authors": [
            "Robert Brijder",
            "Hendrik Jan Hoogeboom"
        ],
        "summary": "Gene assembly is an intricate biological process that has been studied formally and modeled through string and graph rewriting systems. Recently, a restriction of the general (intramolecular) model, called simple gene assembly, has been introduced. This restriction has subsequently been defined as a string rewriting system. We show that by extending the notion of overlap graph it is possible to define a graph rewriting system for two of the three types of rules that make up simple gene assembly. It turns out that this graph rewriting system is less involved than its corresponding string rewriting system. Finally, we give characterizations of the `power' of both types of graph rewriting rules. Because of the equivalence of these string and graph rewriting systems, the given characterizations can be carried over to the string rewriting system.",
        "published": "2007-12-20T12:08:33Z",
        "link": "http://arxiv.org/abs/0712.3380v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Tuplix Calculus",
        "authors": [
            "J. A. Bergstra",
            "A. Ponse",
            "M. B. van der Zwaag"
        ],
        "summary": "We introduce a calculus for tuplices, which are expressions that generalize matrices and vectors. Tuplices have an underlying data type for quantities that are taken from a zero-totalized field. We start with the core tuplix calculus CTC for entries and tests, which are combined using conjunctive composition. We define a standard model and prove that CTC is relatively complete with respect to it. The core calculus is extended with operators for choice, information hiding, scalar multiplication, clearing and encapsulation. We provide two examples of applications; one on incremental financial budgeting, and one on modular financial budget design.",
        "published": "2007-12-20T13:58:14Z",
        "link": "http://arxiv.org/abs/0712.3423v1",
        "categories": [
            "cs.LO",
            "cs.CE"
        ]
    },
    {
        "title": "Weak Affine Light Typing: Polytime intensional expressivity, soundness   and completeness",
        "authors": [
            "Luca Roversi"
        ],
        "summary": "Weak affine light typing (WALT) assigns light affine linear formulae as types to a subset of lambda-terms in System F. WALT is poly-time sound: if a lambda-term M has type in WALT, M can be evaluated with a polynomial cost in the dimension of the derivation that gives it a type. In particular, the evaluation can proceed under any strategy of a rewriting relation, obtained as a mix of both call-by-name/call-by-value beta-reductions. WALT is poly-time complete since it can represent any poly-time Turing machine. WALT weakens, namely generalizes, the notion of stratification of deductions common to some Light Systems -- we call as such those logical systems, derived from Linear logic, to characterize FP, the set of Polynomial functions -- . A weaker stratification allows to define a compositional embedding of the Quasi-linear fragment QlSRN of Safe recursion on notation (SRN) into WALT. QlSRN is SRN, which is a recursive-theoretical system characterizing FP, where only the composition scheme is restricted to linear safe variables. So, the expressivity of WALT is stronger, as compared to the known Light Systems. In particular, using the types, the embedding puts in evidence the stratification of normal and safe arguments hidden in QlSRN: the less an argument is impredicative, the deeper, in a formal, proof-theoretical sense, gets its representation in WALT.",
        "published": "2007-12-27T14:35:16Z",
        "link": "http://arxiv.org/abs/0712.4222v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Finite Automata Based on Quantum Logic and Their Determinization",
        "authors": [
            "Yongming Li"
        ],
        "summary": "We give the quantum subset construction of orthomodular lattice-valued finite automata, then we show the equivalence between orthomodular lattice-valued finite automata, orthomodular lattice-valued deterministic finite automata and orthomodular lattice-valued finite automata with empty string-moves. Based on these equivalences, we study the algebraic operations on orthomodular lattice-valued regular languages, then we establish Kleene theorem in the frame of quantum logic.",
        "published": "2007-12-28T11:48:44Z",
        "link": "http://arxiv.org/abs/0712.4341v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Does intelligence imply contradiction?",
        "authors": [
            "Patrizio Frosini"
        ],
        "summary": "Contradiction is often seen as a defect of intelligent systems and a dangerous limitation on efficiency. In this paper we raise the question of whether, on the contrary, it could be considered a key tool in increasing intelligence in biological structures. A possible way of answering this question in a mathematical context is shown, formulating a proposition that suggests a link between intelligence and contradiction.   A concrete approach is presented in the well-defined setting of cellular automata. Here we define the models of ``observer'', ``entity'', ``environment'', ``intelligence'' and ``contradiction''. These definitions, which roughly correspond to the common meaning of these words, allow us to deduce a simple but strong result about these concepts in an unbiased, mathematical manner. Evidence for a real-world counterpart to the demonstrated formal link between intelligence and contradiction is provided by three computational experiments.",
        "published": "2007-12-31T19:07:22Z",
        "link": "http://arxiv.org/abs/0801.0232v2",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Asynchronous Distributed Searchlight Scheduling",
        "authors": [
            "Karl J. Obermeyer",
            "Anurag Ganguli",
            "Francesco Bullo"
        ],
        "summary": "This paper develops and compares two simple asynchronous distributed searchlight scheduling algorithms for multiple robotic agents in nonconvex polygonal environments. A searchlight is a ray emitted by an agent which cannot penetrate the boundary of the environment. A point is detected by a searchlight if and only if the point is on the ray at some instant. Targets are points which can move continuously with unbounded speed. The objective of the proposed algorithms is for the agents to coordinate the slewing (rotation about a point) of their searchlights in a distributed manner, i.e., using only local sensing and limited communication, such that any target will necessarily be detected in finite time. The first algorithm we develop, called the DOWSS (Distributed One Way Sweep Strategy), is a distributed version of a known algorithm described originally in 1990 by Sugihara et al \\cite{KS-IS-MY:90}, but it can be very slow in clearing the entire environment because only one searchlight may slew at a time. In an effort to reduce the time to clear the environment, we develop a second algorithm, called the PTSS (Parallel Tree Sweep Strategy), in which searchlights sweep in parallel if guards are placed according to an environment partition belonging to a class we call PTSS partitions. Finally, we discuss how DOWSS and PTSS could be combined with with deployment, or extended to environments with holes.",
        "published": "2007-01-11T20:55:17Z",
        "link": "http://arxiv.org/abs/cs/0701077v3",
        "categories": [
            "cs.MA",
            "cs.RO",
            "I.2.11"
        ]
    },
    {
        "title": "Artificiality in Social Sciences",
        "authors": [
            "Jean-Philippe Rennard"
        ],
        "summary": "This text provides with an introduction to the modern approach of artificiality and simulation in social sciences. It presents the relationship between complexity and artificiality, before introducing the field of artificial societies which greatly benefited from the computer power fast increase, gifting social sciences with formalization and experimentation tools previously owned by \"hard\" sciences alone. It shows that as \"a new way of doing social sciences\", artificial societies should undoubtedly contribute to a renewed approach in the study of sociality and should play a significant part in the elaboration of original theories of social phenomena.",
        "published": "2007-01-13T16:50:37Z",
        "link": "http://arxiv.org/abs/cs/0701087v2",
        "categories": [
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Distributed Decision Through Self-Synchronizing Sensor Networks in the   Presence of Propagation Delays and Nonreciprocal Channels",
        "authors": [
            "Gesualdo Scutari",
            "Sergio Barbarossa",
            "Loreto Pescosolido"
        ],
        "summary": "In this paper we propose and analyze a distributed algorithm for achieving globally optimal decisions, either estimation or detection, through a self-synchronization mechanism among linearly coupled integrators initialized with local measurements. We model the interaction among the nodes as a directed graph with weights dependent on the radio interface and we pose special attention to the effect of the propagation delays occurring in the exchange of data among sensors, as a function of the network geometry. We derive necessary and sufficient conditions for the proposed system to reach a consensus on globally optimal decision statistics. One of the major results proved in this work is that a consensus is achieved for any bounded delay condition if and only if the directed graph is quasi-strongly connected. We also provide a closed form expression for the global consensus, showing that the effect of delays is, in general, to introduce a bias in the final decision. The closed form expression is also useful to modify the consensus mechanism in order to get rid of the bias with minimum extra complexity.",
        "published": "2007-02-11T09:15:03Z",
        "link": "http://arxiv.org/abs/cs/0702068v2",
        "categories": [
            "cs.IT",
            "cs.MA",
            "math.IT"
        ]
    },
    {
        "title": "Observable Graphs",
        "authors": [
            "Raphael M. Jungers",
            "Vincent D. Blondel"
        ],
        "summary": "An edge-colored directed graph is \\emph{observable} if an agent that moves along its edges is able to determine his position in the graph after a sufficiently long observation of the edge colors. When the agent is able to determine his position only from time to time, the graph is said to be \\emph{partly observable}. Observability in graphs is desirable in situations where autonomous agents are moving on a network and one wants to localize them (or the agent wants to localize himself) with limited information. In this paper, we completely characterize observable and partly observable graphs and show how these concepts relate to observable discrete event systems and to local automata. Based on these characterizations, we provide polynomial time algorithms to decide observability, to decide partial observability, and to compute the minimal number of observations necessary for finding the position of an agent. In particular we prove that in the worst case this minimal number of observations increases quadratically with the number of nodes in the graph.   From this it follows that it may be necessary for an agent to pass through the same node several times before he is finally able to determine his position in the graph. We then consider the more difficult question of assigning colors to a graph so as to make it observable and we prove that two different versions of this problem are NP-complete.",
        "published": "2007-02-16T16:16:49Z",
        "link": "http://arxiv.org/abs/cs/0702091v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Avoiding bias in cards cryptography",
        "authors": [
            "M. D. Atkinson",
            "H. P. van Ditmarsch",
            "S. Roehling"
        ],
        "summary": "We outline the need for stricter requirements for unconditionally secure cryptographic protocols inspired by the Russian Cards problem. A new requirement CA4 is proposed that checks for bias in single card occurrence in announcements consisting of alternatives for players' holdings of cards. This requirement CA4 is shown to be equivalent to an alternative requirement CA5. All announcements found to satisfy CA4 are 2-designs. We also show that all binary designs are 3-designs. Instead of avoiding bias in announcements produced by such protocols, one may as well apply unbiased protocols such that patterns in announcements become meaningless. We gave two examples of such protocols for card deal parameters (3,3,1), i.e. two of the players hold three cards, and the remaining player, playing the role of eavesdropper, holds a single card.",
        "published": "2007-02-17T01:42:39Z",
        "link": "http://arxiv.org/abs/cs/0702097v1",
        "categories": [
            "cs.CR",
            "cs.MA"
        ]
    },
    {
        "title": "Cultural route to the emergence of linguistic categories",
        "authors": [
            "Andrea Puglisi",
            "Andrea Baronchelli",
            "Vittorio Loreto"
        ],
        "summary": "Categories provide a coarse grained description of the world. A fundamental question is whether categories simply mirror an underlying structure of nature, or instead come from the complex interactions of human beings among themselves and with the environment. Here we address this question by modelling a population of individuals who co-evolve their own system of symbols and meanings by playing elementary language games. The central result is the emergence of a hierarchical category structure made of two distinct levels: a basic layer, responsible for fine discrimination of the environment, and a shared linguistic layer that groups together perceptions to guarantee communicative success. Remarkably, the number of linguistic categories turns out to be finite and small, as observed in natural languages.",
        "published": "2007-03-15T18:46:44Z",
        "link": "http://arxiv.org/abs/physics/0703164v1",
        "categories": [
            "physics.soc-ph",
            "cond-mat.dis-nn",
            "cs.MA"
        ]
    },
    {
        "title": "Social Information Processing in Social News Aggregation",
        "authors": [
            "Kristina Lerman"
        ],
        "summary": "The rise of the social media sites, such as blogs, wikis, Digg and Flickr among others, underscores the transformation of the Web to a participatory medium in which users are collaboratively creating, evaluating and distributing information. The innovations introduced by social media has lead to a new paradigm for interacting with information, what we call 'social information processing'. In this paper, we study how social news aggregator Digg exploits social information processing to solve the problems of document recommendation and rating. First, we show, by tracking stories over time, that social networks play an important role in document recommendation. The second contribution of this paper consists of two mathematical models. The first model describes how collaborative rating and promotion of stories emerges from the independent decisions made by many users. The second model describes how a user's influence, the number of promoted stories and the user's social network, changes in time. We find qualitative agreement between predictions of the model and user data gathered from Digg.",
        "published": "2007-03-15T22:37:22Z",
        "link": "http://arxiv.org/abs/cs/0703087v2",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.HC",
            "cs.MA"
        ]
    },
    {
        "title": "Statistical User Model for the Internet Access",
        "authors": [
            "Carmen Pellicer-Lostao",
            "Daniel Morato",
            "Ricardo Lopez-Ruiz"
        ],
        "summary": "A new statistical based model approach to characterize a user's behavior in an Internet access link is presented. The real patterns of Internet traffic in a heterogeneous Campus Network are studied. We find three clearly different patterns of individual user's behavior, study their common features and group particular users behaving alike in three clusters. This allows us to build a probabilistic mixture model, that can explain the expected global behavior for the three different types of users. We discuss the implications of this emergent phenomenology in the field of multi-agent complex systems.",
        "published": "2007-03-20T09:36:25Z",
        "link": "http://arxiv.org/abs/nlin/0703036v1",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.MA",
            "cs.NI"
        ]
    },
    {
        "title": "On Approximating Optimal Weighted Lobbying, and Frequency of Correctness   versus Average-Case Polynomial Time",
        "authors": [
            "Gabor Erdelyi",
            "Lane A. Hemaspaandra",
            "Joerg Rothe",
            "Holger Spakowski"
        ],
        "summary": "We investigate issues related to two hard problems related to voting, the optimal weighted lobbying problem and the winner problem for Dodgson elections. Regarding the former, Christian et al. [CFRS06] showed that optimal lobbying is intractable in the sense of parameterized complexity. We provide an efficient greedy algorithm that achieves a logarithmic approximation ratio for this problem and even for a more general variant--optimal weighted lobbying. We prove that essentially no better approximation ratio than ours can be proven for this greedy algorithm.   The problem of determining Dodgson winners is known to be complete for parallel access to NP [HHR97]. Homan and Hemaspaandra [HH06] proposed an efficient greedy heuristic for finding Dodgson winners with a guaranteed frequency of success, and their heuristic is a ``frequently self-knowingly correct algorithm.'' We prove that every distributional problem solvable in polynomial time on the average with respect to the uniform distribution has a frequently self-knowingly correct polynomial-time algorithm. Furthermore, we study some features of probability weight of correctness with respect to Procaccia and Rosenschein's junta distributions [PR07].",
        "published": "2007-03-20T20:35:02Z",
        "link": "http://arxiv.org/abs/cs/0703097v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Computing Good Nash Equilibria in Graphical Games",
        "authors": [
            "Edith Elkind",
            "Leslie Ann Goldberg",
            "Paul W. Goldberg"
        ],
        "summary": "This paper addresses the problem of fair equilibrium selection in graphical games. Our approach is based on the data structure called the {\\em best response policy}, which was proposed by Kearns et al. \\cite{kls} as a way to represent all Nash equilibria of a graphical game. In \\cite{egg}, it was shown that the best response policy has polynomial size as long as the underlying graph is a path. In this paper, we show that if the underlying graph is a bounded-degree tree and the best response policy has polynomial size then there is an efficient algorithm which constructs a Nash equilibrium that guarantees certain payoffs to all participants. Another attractive solution concept is a Nash equilibrium that maximizes the social welfare. We show that, while exactly computing the latter is infeasible (we prove that solving this problem may involve algebraic numbers of an arbitrarily high degree), there exists an FPTAS for finding such an equilibrium as long as the best response policy has polynomial size. These two algorithms can be combined to produce Nash equilibria that satisfy various fairness criteria.",
        "published": "2007-03-27T16:15:54Z",
        "link": "http://arxiv.org/abs/cs/0703133v1",
        "categories": [
            "cs.GT",
            "cs.DS",
            "cs.MA"
        ]
    },
    {
        "title": "Extensive Games with Possibly Unaware Players",
        "authors": [
            "Joseph Y. Halpern",
            "Leandro C. Rêgo"
        ],
        "summary": "Standard game theory assumes that the structure of the game is common knowledge among players. We relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. In particular, they may not be aware of moves that they and other agents can make. We show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. We provide a generalization of Nash equilibrium and show that every game with awareness has a generalized Nash equilibrium. Finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.",
        "published": "2007-04-16T13:58:47Z",
        "link": "http://arxiv.org/abs/0704.2014v1",
        "categories": [
            "cs.GT",
            "cs.MA"
        ]
    },
    {
        "title": "Scalability and Optimisation of a Committee of Agents Using Genetic   Algorithm",
        "authors": [
            "T. Marwala",
            "P. De Wilde",
            "L. Correia",
            "P. Mariano",
            "R. Ribeiro",
            "V. Abramov",
            "N. Szirbik",
            "J. Goossenaerts"
        ],
        "summary": "A population of committees of agents that learn by using neural networks is implemented to simulate the stock market. Each committee of agents, which is regarded as a player in a game, is optimised by continually adapting the architecture of the agents using genetic algorithms. The committees of agents buy and sell stocks by following this procedure: (1) obtain the current price of stocks; (2) predict the future price of stocks; (3) and for a given price trade until all the players are mutually satisfied. The trading of stocks is conducted by following these rules: (1) if a player expects an increase in price then it tries to buy the stock; (2) else if it expects a drop in the price, it sells the stock; (3)and the order in which a player participates in the game is random. The proposed procedure is implemented to simulate trading of three stocks, namely, the Dow Jones, the Nasdaq and the S&P 500. A linear relationship between the number of players and agents versus the computational time to run the complete simulation is observed. It is also found that no player has a monopolistic advantage.",
        "published": "2007-05-12T10:11:57Z",
        "link": "http://arxiv.org/abs/0705.1757v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "A competitive multi-agent model of interbank payment systems",
        "authors": [
            "Marco Galbiati",
            "Kimmo Soramaki"
        ],
        "summary": "We develop a dynamic multi-agent model of an interbank payment system where banks choose their level of available funds on the basis of private payoff maximisation. The model consists of the repetition of a simultaneous move stage game with incomplete information, incomplete monitoring, and stochastic payoffs. Adaptation takes place with bayesian updating, with banks maximizing immediate payoffs. We carry out numerical simulations to solve the model and investigate two special scenarios: an operational incident and exogenous throughput guidelines for payment submission. We find that the demand for intraday credit is an S-shaped function of the cost ratio between intraday credit costs and the costs associated with delaying payments. We also find that the demand for liquidity is increased both under operational incidents and in the presence of effective throughput guidelines.",
        "published": "2007-05-22T14:26:05Z",
        "link": "http://arxiv.org/abs/0705.3050v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Modeling Epidemic Spread in Synthetic Populations - Virtual Plagues in   Massively Multiplayer Online Games",
        "authors": [
            "Magnus Boman",
            "Stefan J. Johansson"
        ],
        "summary": "A virtual plague is a process in which a behavior-affecting property spreads among characters in a Massively Multiplayer Online Game (MMOG). The MMOG individuals constitute a synthetic population, and the game can be seen as a form of interactive executable model for studying disease spread, albeit of a very special kind. To a game developer maintaining an MMOG, recognizing, monitoring, and ultimately controlling a virtual plague is important, regardless of how it was initiated. The prospect of using tools, methods and theory from the field of epidemiology to do this seems natural and appealing. We will address the feasibility of such a prospect, first by considering some basic measures used in epidemiology, then by pointing out the differences between real world epidemics and virtual plagues. We also suggest directions for MMOG developer control through epidemiological modeling. Our aim is understanding the properties of virtual plagues, rather than trying to eliminate them or mitigate their effects, as would be in the case of real infectious disease.",
        "published": "2007-05-31T12:15:05Z",
        "link": "http://arxiv.org/abs/0705.4584v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.MA",
            "I.2.1"
        ]
    },
    {
        "title": "Multi-Agent Modeling Using Intelligent Agents in the Game of Lerpa",
        "authors": [
            "Evan Hurwitz",
            "Tshilidzi Marwala"
        ],
        "summary": "Game theory has many limitations implicit in its application. By utilizing multiagent modeling, it is possible to solve a number of problems that are unsolvable using traditional game theory. In this paper reinforcement learning is applied to neural networks to create intelligent agents",
        "published": "2007-06-02T17:20:21Z",
        "link": "http://arxiv.org/abs/0706.0280v1",
        "categories": [
            "cs.MA",
            "cs.GT"
        ]
    },
    {
        "title": "FIPA-based Interoperable Agent Mobility Proposal",
        "authors": [
            "Jordi Cucurull",
            "Ramon Marti",
            "Sergi Robles",
            "Joan Borrell",
            "Guillermo Navarro"
        ],
        "summary": "This paper presents a proposal for a flexible agent mobility architecture based on IEEE-FIPA standards and intended to be one of them. This proposal is a first step towards interoperable mobility mechanisms, which are needed for future agent migration between different kinds of platforms. Our proposal is presented as a flexible and robust architecture that has been successfully implemented in the JADE and AgentScape platforms. It is based on an open set of protocols, allowing new protocols and future improvements to be accommodated in the architecture. With this proposal we demonstrate that a standard architecture for agent mobility capable of supporting several agent platforms can be defined and implemented.",
        "published": "2007-06-13T14:37:58Z",
        "link": "http://arxiv.org/abs/0706.1860v2",
        "categories": [
            "cs.MA",
            "cs.NI"
        ]
    },
    {
        "title": "Autonomy with regard to an Attribute",
        "authors": [
            "Eric Sanchis"
        ],
        "summary": "This paper presents a model of autonomy called autonomy with regard to an attribute applicable to cognitive and not cognitive artificial agents. Three criteria (global / partial, social / nonsocial, absolute / relative) are defined and used to describe the main characteristics of this type of autonomy. A software agent autonomous with regard to the mobility illustrates a possible implementation of this model.",
        "published": "2007-07-11T05:05:57Z",
        "link": "http://arxiv.org/abs/0707.1558v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Geometrical derivation of the Boltzmann factor",
        "authors": [
            "Ricardo Lopez-Ruiz",
            "Jaime Sanudo",
            "Xavier Calbet"
        ],
        "summary": "We show that the Boltzmann factor has a geometrical origin. Its derivation follows from the microcanonical picture. The Maxwell-Boltzmann distribution or the wealth distribution in human society are some direct applications of this new interpretation.",
        "published": "2007-07-27T10:54:41Z",
        "link": "http://arxiv.org/abs/0707.4081v1",
        "categories": [
            "nlin.CD",
            "cond-mat.stat-mech",
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Moderate Growth Time Series for Dynamic Combinatorics Modelisation",
        "authors": [
            "Luaï Jaff",
            "Gérard H. E. Duchamp",
            "Hatem Hadj Kacem",
            "Cyrille Bertelle"
        ],
        "summary": "Here, we present a family of time series with a simple growth constraint. This family can be the basis of a model to apply to emerging computation in business and micro-economy where global functions can be expressed from local rules. We explicit a double statistics on these series which allows to establish a one-to-one correspondence between three other ballot-like strunctures.",
        "published": "2007-08-16T14:58:35Z",
        "link": "http://arxiv.org/abs/0708.2213v1",
        "categories": [
            "cs.SC",
            "cs.MA",
            "math.CO"
        ]
    },
    {
        "title": "Optimal strategies in the average consensus problem",
        "authors": [
            "Jean-Charles Delvenne",
            "Ruggero Carli",
            "Sandro Zampieri"
        ],
        "summary": "We prove that for a set of communicating agents to compute the average of their initial positions (average consensus problem), the optimal topology of communication is given by a de Bruijn's graph. Consensus is then reached in a finitely many steps. A more general family of strategies, constructed by block Kronecker products, is investigated and compared to Cayley strategies.",
        "published": "2007-08-23T17:53:54Z",
        "link": "http://arxiv.org/abs/0708.3220v1",
        "categories": [
            "cs.MA",
            "cs.NI",
            "math.OC"
        ]
    },
    {
        "title": "Multi-agent systems, Equiprobability, Gamma distributions and other   Geometrical questions",
        "authors": [
            "Ricardo Lopez-Ruiz",
            "Jaime Sanudo",
            "Xavier Calbet"
        ],
        "summary": "A set of many identical interacting agents obeying a global additive constraint is considered. Under the hypothesis of equiprobability in the high-dimensional volume delimited in phase space by the constraint, the statistical behavior of a generic agent over the ensemble is worked out. The asymptotic distribution of that statistical behavior is derived from geometrical arguments. This distribution is related with the Gamma distributions found in several multi-agent economy models. The parallelism with all these systems is established. Also, as a collateral result, a formula for the volume of high-dimensional symmetrical bodies is proposed.",
        "published": "2007-08-28T11:38:30Z",
        "link": "http://arxiv.org/abs/0708.3761v2",
        "categories": [
            "nlin.CD",
            "cond-mat.stat-mech",
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Coexistence of Social Norms based on In- and Out-group Interactions",
        "authors": [
            "Thomas Fent",
            "Patrick Groeber",
            "Frank Schweitzer"
        ],
        "summary": "The question how social norms can emerge from microscopic interactions between individuals is a key problem in social sciences to explain collective behavior. In this paper we propose an agent-based model to show that randomly distributed social behavior by way of local interaction converges to a state with a multimodal distribution of behavior. This can be interpreted as a coexistence of different social norms, a result that goes beyond previous investigations. The model is discrete in time and space, behavior is characterized in a continuous state space. The adaptation of social behavior by each agent is based on attractive and repulsive forces caused by friendly and adversary relations among agents. The model is analyzed both analytically and by means of spatio-temporal computer simulations. It provides conditions under which we find convergence towards a single norm, coexistence of two opposing norms, and coexistence of a multitude of norms. For the latter case, we also show the evolution of the spatio-temporal distribution of behavior.",
        "published": "2007-08-30T13:27:52Z",
        "link": "http://arxiv.org/abs/0708.4155v1",
        "categories": [
            "nlin.AO",
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Distributed Decision Through Self-Synchronizing Sensor Networks in the   Presence of Propagation Delays and Asymmetric Channels",
        "authors": [
            "Gesualdo Scutari",
            "Sergio Barbarossa",
            "Loreto Pescosolido"
        ],
        "summary": "In this paper we propose and analyze a distributed algorithm for achieving globally optimal decisions, either estimation or detection, through a self-synchronization mechanism among linearly coupled integrators initialized with local measurements. We model the interaction among the nodes as a directed graph with weights (possibly) dependent on the radio channels and we pose special attention to the effect of the propagation delay occurring in the exchange of data among sensors, as a function of the network geometry. We derive necessary and sufficient conditions for the proposed system to reach a consensus on globally optimal decision statistics. One of the major results proved in this work is that a consensus is reached with exponential convergence speed for any bounded delay condition if and only if the directed graph is quasi-strongly connected. We provide a closed form expression for the global consensus, showing that the effect of delays is, in general, the introduction of a bias in the final decision. Finally, we exploit our closed form expression to devise a double-step consensus mechanism able to provide an unbiased estimate with minimum extra complexity, without the need to know or estimate the channel parameters.",
        "published": "2007-09-15T08:40:18Z",
        "link": "http://arxiv.org/abs/0709.2410v2",
        "categories": [
            "cs.MA",
            "cs.DC"
        ]
    },
    {
        "title": "On Real-Time Communication Systems with Noisy Feedback",
        "authors": [
            "Aditya Mahajan",
            "Demosthenis Teneketzis"
        ],
        "summary": "We consider a real-time communication system with noisy feedback consisting of a Markov source, a forward and a backward discrete memoryless channels, and a receiver with finite memory. The objective is to design an optimal communication strategy (that is, encoding, decoding, and memory update strategies) to minimize the total expected distortion over a finite horizon. We present a sequential decomposition for the problem, which results in a set of nested optimality equations to determine optimal communication strategies. This provides a systematic methodology to determine globally optimal joint source-channel encoding and decoding strategies for real-time communication systems with noisy feedback.",
        "published": "2007-09-23T04:20:38Z",
        "link": "http://arxiv.org/abs/0709.3753v1",
        "categories": [
            "cs.IT",
            "cs.MA",
            "math.IT",
            "math.OC",
            "math.PR"
        ]
    },
    {
        "title": "Power Efficient Scheduling under Delay Constraints over Multi-user   Wireless Channels",
        "authors": [
            "Nitin Salodkar",
            "Abhay Karandikar",
            "Vivek S. Borkar"
        ],
        "summary": "In this paper, we consider the problem of power efficient uplink scheduling in a Time Division Multiple Access (TDMA) system over a fading wireless channel. The objective is to minimize the power expenditure of each user subject to satisfying individual user delay. We make the practical assumption that the system statistics are unknown, i.e., the probability distributions of the user arrivals and channel states are unknown. The problem has the structure of a Constrained Markov Decision Problem (CMDP). Determining an optimal policy under for the CMDP faces the problems of state space explosion and unknown system statistics. To tackle the problem of state space explosion, we suggest determining the transmission rate of a particular user in each slot based on its channel condition and buffer occupancy only. The rate allocation algorithm for a particular user is a learning algorithm that learns about the buffer occupancy and channel states of that user during system execution and thus addresses the issue of unknown system statistics. Once the rate of each user is determined, the proposed algorithm schedules the user with the best rate. Our simulations within an IEEE 802.16 system demonstrate that the algorithm is indeed able to satisfy the user specified delay constraints. We compare the performance of our algorithm with the well known M-LWDF algorithm. Moreover, we demonstrate that the power expended by the users under our algorithm is quite low.",
        "published": "2007-10-05T12:51:46Z",
        "link": "http://arxiv.org/abs/0710.1190v1",
        "categories": [
            "cs.NI",
            "cs.MA"
        ]
    },
    {
        "title": "Rigidity and persistence for ensuring shape maintenance of multiagent   meta formations (ext'd version)",
        "authors": [
            "Julien M. Hendrickx",
            "Changbin Yu",
            "Baris Fidan",
            "Brian D. O. Anderson"
        ],
        "summary": "This paper treats the problem of the merging of formations, where the underlying model of a formation is graphical. We first analyze the rigidity and persistence of meta-formations, which are formations obtained by connecting several rigid or persistent formations. Persistence is a generalization to directed graphs of the undirected notion of rigidity. In the context of moving autonomous agent formations, persistence characterizes the efficacy of a directed structure of unilateral distance constraints seeking to preserve a formation shape. We derive then, for agents evolving in a two- or three-dimensional space, the conditions under which a set of persistent formations can be merged into a persistent meta-formation, and give the minimal number of interconnections needed for such a merging. We also give conditions for a meta-formation obtained by merging several persistent formations to be persistent.",
        "published": "2007-10-14T13:51:09Z",
        "link": "http://arxiv.org/abs/0710.2659v1",
        "categories": [
            "cs.MA",
            "cs.DM"
        ]
    },
    {
        "title": "Recognizing Members of the Tournament Equilibrium Set is NP-hard",
        "authors": [
            "Felix Brandt",
            "Felix Fischer",
            "Paul Harrenstein"
        ],
        "summary": "A recurring theme in the mathematical social sciences is how to select the \"most desirable\" elements given a binary dominance relation on a set of alternatives. Schwartz's tournament equilibrium set (TEQ) ranks among the most intriguing, but also among the most enigmatic, tournament solutions that have been proposed so far in this context. Due to its unwieldy recursive definition, little is known about TEQ. In particular, its monotonicity remains an open problem up to date. Yet, if TEQ were to satisfy monotonicity, it would be a very attractive tournament solution concept refining both the Banks set and Dutta's minimal covering set. We show that the problem of deciding whether a given alternative is contained in TEQ is NP-hard.",
        "published": "2007-11-19T15:48:46Z",
        "link": "http://arxiv.org/abs/0711.2961v2",
        "categories": [
            "cs.CC",
            "cs.GT",
            "cs.MA"
        ]
    },
    {
        "title": "Distributed Consensus Algorithms in Sensor Networks: Link Failures and   Channel Noise",
        "authors": [
            "Soummya Kar",
            "José M. F. Moura"
        ],
        "summary": "The paper studies average consensus with random topologies (intermittent links)   \\emph{and} noisy channels. Consensus with noise in the network links leads to the bias-variance dilemma--running consensus for long reduces the bias of the final average estimate but increases its variance. We present two different compromises to this tradeoff: the $\\mathcal{A-ND}$ algorithm modifies conventional consensus by forcing the weights to satisfy a \\emph{persistence} condition (slowly decaying to zero); and the $\\mathcal{A-NC}$ algorithm where the weights are constant but consensus is run for a fixed number of iterations $\\hat{\\imath}$, then it is restarted and rerun for a total of $\\hat{p}$ runs, and at the end averages the final states of the $\\hat{p}$ runs (Monte Carlo averaging). We use controlled Markov processes and stochastic approximation arguments to prove almost sure convergence of $\\mathcal{A-ND}$ to the desired average (asymptotic unbiasedness) and compute explicitly the m.s.e. (variance) of the consensus limit. We show that $\\mathcal{A-ND}$ represents the best of both worlds--low bias and low variance--at the cost of a slow convergence rate; rescaling the weights...",
        "published": "2007-11-25T18:19:42Z",
        "link": "http://arxiv.org/abs/0711.3915v2",
        "categories": [
            "cs.IT",
            "cs.MA",
            "math.IT",
            "math.OC"
        ]
    },
    {
        "title": "Copeland Voting Fully Resists Constructive Control",
        "authors": [
            "Piotr Faliszewski",
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Jörg Rothe"
        ],
        "summary": "Control and bribery are settings in which an external agent seeks to influence the outcome of an election. Faliszewski et al. [FHHR07] proved that Llull voting (which is here denoted by Copeland^1) and a variant (here denoted by Copeland^0) of Copeland voting are computationally resistant to many, yet not all, types of constructive control and that they also provide broad resistance to bribery. We study a parameterized version of Copeland voting, denoted by Copeland^alpha where the parameter alpha is a rational number between 0 and 1 that specifies how ties are valued in the pairwise comparisons of candidates in Copeland elections. We establish resistance or vulnerability results, in every previously studied control scenario, for Copeland^alpha, for each rational alpha, 0 <alpha < 1. In particular, we prove that Copeland^0.5, the system commonly referred to as ``Copeland voting,'' provides full resistance to constructive control. Among the systems with a polynomial-time winner problem, this is the first natural election system proven to have full resistance to constructive control. Results on bribery and fixed-parameter tractability of bounded-case control proven for Copeland^0 and Copeland^1 in [FHHR07] are extended to Copeland^alpha for each rational alpha, 0 < alpha < 1; we also give results in more flexible models such as microbribery and extended control.",
        "published": "2007-11-29T16:12:25Z",
        "link": "http://arxiv.org/abs/0711.4759v2",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Nonuniform Bribery",
        "authors": [
            "Piotr Faliszewski"
        ],
        "summary": "We study the concept of bribery in the situation where voters are willing to change their votes as we ask them, but where their prices depend on the nature of the change we request. Our model is an extension of the one of Faliszewski et al. [FHH06], where each voter has a single price for any change we may ask for. We show polynomial-time algorithms for our version of bribery for a broad range of voting protocols, including plurality, veto, approval, and utility based voting. In addition to our polynomial-time algorithms we provide NP-completeness results for a couple of our nonuniform bribery problems for weighted voters, and a couple of approximation algorithms for NP-complete bribery problems defined in [FHH06] (in particular, an FPTAS for plurality-weighted-$bribery problem).",
        "published": "2007-11-30T12:47:35Z",
        "link": "http://arxiv.org/abs/0711.4924v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Computational Chemotaxis in Ants and Bacteria over Dynamic Environments",
        "authors": [
            "Vitorino Ramos",
            "C. M. Fernandes",
            "A. C. Rosa",
            "A. Abraham"
        ],
        "summary": "Chemotaxis can be defined as an innate behavioural response by an organism to a directional stimulus, in which bacteria, and other single-cell or multicellular organisms direct their movements according to certain chemicals in their environment. This is important for bacteria to find food (e.g., glucose) by swimming towards the highest concentration of food molecules, or to flee from poisons. Based on self-organized computational approaches and similar stigmergic concepts we derive a novel swarm intelligent algorithm. What strikes from these observations is that both eusocial insects as ant colonies and bacteria have similar natural mechanisms based on stigmergy in order to emerge coherent and sophisticated patterns of global collective behaviour. Keeping in mind the above characteristics we will present a simple model to tackle the collective adaptation of a social swarm based on real ant colony behaviors (SSA algorithm) for tracking extrema in dynamic environments and highly multimodal complex functions described in the well-know De Jong test suite. Later, for the purpose of comparison, a recent model of artificial bacterial foraging (BFOA algorithm) based on similar stigmergic features is described and analyzed. Final results indicate that the SSA collective intelligence is able to cope and quickly adapt to unforeseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes, while outperforming BFOA in adaptive speed. Results indicate that the present approach deals well in severe Dynamic Optimization problems.",
        "published": "2007-12-05T15:02:19Z",
        "link": "http://arxiv.org/abs/0712.0744v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "q-bio.PE",
            "q-bio.QM",
            "I.2; I.2.11; G.1.6"
        ]
    },
    {
        "title": "Distributed Consensus Algorithms in Sensor Networks: Quantized Data and   Random Link Failures",
        "authors": [
            "Soummya Kar",
            "Jose M. F. Moura"
        ],
        "summary": "The paper studies the problem of distributed average consensus in sensor networks with quantized data and random link failures. To achieve consensus, dither (small noise) is added to the sensor states before quantization. When the quantizer range is unbounded (countable number of quantizer levels), stochastic approximation shows that consensus is asymptotically achieved with probability one and in mean square to a finite random variable. We show that the meansquared error (m.s.e.) can be made arbitrarily small by tuning the link weight sequence, at a cost of the convergence rate of the algorithm. To study dithered consensus with random links when the range of the quantizer is bounded, we establish uniform boundedness of the sample paths of the unbounded quantizer. This requires characterization of the statistical properties of the supremum taken over the sample paths of the state of the quantizer. This is accomplished by splitting the state vector of the quantizer in two components: one along the consensus subspace and the other along the subspace orthogonal to the consensus subspace. The proofs use maximal inequalities for submartingale and supermartingale sequences. From these, we derive probability bounds on the excursions of the two subsequences, from which probability bounds on the excursions of the quantizer state vector follow. The paper shows how to use these probability bounds to design the quantizer parameters and to explore tradeoffs among the number of quantizer levels, the size of the quantization steps, the desired probability of saturation, and the desired level of accuracy $\\epsilon$ away from consensus. Finally, the paper illustrates the quantizer design with a numerical study.",
        "published": "2007-12-10T21:38:42Z",
        "link": "http://arxiv.org/abs/0712.1609v3",
        "categories": [
            "cs.MA",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "An Economic Model of Coupled Exponential Maps",
        "authors": [
            "R. Lopez-Ruiz",
            "J. Gonzalez-Estevez",
            "M. G. Cosenza",
            "J. R. Sanchez"
        ],
        "summary": "In this work, an ensemble of economic interacting agents is considered. The agents are arranged in a linear array where only local couplings are allowed. The deterministic dynamics of each agent is given by a map. This map is expressed by two factors. The first one is a linear term that models the expansion of the agent's economy and that is controlled by the {\\it growth capacity parameter}. The second one is an inhibition exponential term that is regulated by the {\\it local environmental pressure}. Depending on the parameter setting, the system can display Pareto or Boltzmann-Gibbs behavior in the asymptotic dynamical regime. The regions of parameter space where the system exhibits one of these two statistical behaviors are delimited. Other properties of the system, such as the mean wealth, the standard deviation and the Gini coefficient, are also calculated.",
        "published": "2007-12-17T11:02:58Z",
        "link": "http://arxiv.org/abs/0712.2684v1",
        "categories": [
            "q-fin.GN",
            "cs.MA",
            "nlin.AO",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Biology of Applied Digital Ecosystems",
        "authors": [
            "G. Briscoe",
            "S. Sadedin",
            "G. Paperin"
        ],
        "summary": "A primary motivation for our research in Digital Ecosystems is the desire to exploit the self-organising properties of biological ecosystems. Ecosystems are thought to be robust, scalable architectures that can automatically solve complex, dynamic problems. However, the biological processes that contribute to these properties have not been made explicit in Digital Ecosystems research. Here, we discuss how biological properties contribute to the self-organising features of biological ecosystems, including population dynamics, evolution, a complex dynamic environment, and spatial distributions for generating local interactions. The potential for exploiting these properties in artificial systems is then considered. We suggest that several key features of biological ecosystems have not been fully explored in existing digital ecosystems, and discuss how mimicking these features may assist in developing robust, scalable self-organising architectures. An example architecture, the Digital Ecosystem, is considered in detail. The Digital Ecosystem is then measured experimentally through simulations, with measures originating from theoretical ecology, to confirm its likeness to a biological ecosystem. Including the responsiveness to requests for applications from the user base, as a measure of the 'ecological succession' (development).",
        "published": "2007-12-26T21:56:52Z",
        "link": "http://arxiv.org/abs/0712.4153v2",
        "categories": [
            "cs.NE",
            "cs.MA"
        ]
    },
    {
        "title": "A mathematical formalism for agent-based modeling",
        "authors": [
            "Reinhard Laubenbacher",
            "Abdul S. Jarrah",
            "Henning Mortveit",
            "S. S. Ravi"
        ],
        "summary": "Many complex systems can be modeled as multiagent systems in which the constituent entities (agents) interact with each other. The global dynamics of such a system is determined by the nature of the local interactions among the agents. Since it is difficult to formally analyze complex multiagent systems, they are often studied through computer simulations. While computer simulations can be very useful, results obtained through simulations do not formally validate the observed behavior. Thus, there is a need for a mathematical framework which one can use to represent multiagent systems and formally establish their properties. This work contains a brief exposition of some known mathematical frameworks that can model multiagent systems. The focus is on one such framework, namely that of finite dynamical systems. Both, deterministic and stochastic versions of this framework are discussed. The paper contains a sampling of the mathematical results from the literature to show how finite dynamical systems can be used to carry out a rigorous study of the properties of multiagent systems and it is shown how the framework can also serve as a universal model for computation.",
        "published": "2007-12-31T23:22:50Z",
        "link": "http://arxiv.org/abs/0801.0249v1",
        "categories": [
            "cs.MA",
            "cs.DM",
            "math.CO"
        ]
    },
    {
        "title": "Statistical keyword detection in literary corpora",
        "authors": [
            "Juan P. Herrera",
            "Pedro A. Pury"
        ],
        "summary": "Understanding the complexity of human language requires an appropriate analysis of the statistical distribution of words in texts. We consider the information retrieval problem of detecting and ranking the relevant words of a text by means of statistical information referring to the \"spatial\" use of the words. Shannon's entropy of information is used as a tool for automatic keyword extraction. By using The Origin of Species by Charles Darwin as a representative text sample, we show the performance of our detector and compare it with another proposals in the literature. The random shuffled text receives special attention as a tool for calibrating the ranking indices.",
        "published": "2007-01-05T06:11:16Z",
        "link": "http://arxiv.org/abs/cs/0701028v2",
        "categories": [
            "cs.CL",
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "On vocabulary size of grammar-based codes",
        "authors": [
            "Lukasz Debowski"
        ],
        "summary": "We discuss inequalities holding between the vocabulary size, i.e., the number of distinct nonterminal symbols in a grammar-based compression for a string, and the excess length of the respective universal code, i.e., the code-based analog of algorithmic mutual information. The aim is to strengthen inequalities which were discussed in a weaker form in linguistics but shed some light on redundancy of efficiently computable codes. The main contribution of the paper is a construction of universal grammar-based codes for which the excess lengths can be bounded easily.",
        "published": "2007-01-08T13:00:23Z",
        "link": "http://arxiv.org/abs/cs/0701047v2",
        "categories": [
            "cs.IT",
            "cs.CL",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Complex networks and human language",
        "authors": [
            "Jinyun KE"
        ],
        "summary": "This paper introduces how human languages can be studied in light of recent development of network theories. There are two directions of exploration. One is to study networks existing in the language system. Various lexical networks can be built based on different relationships between words, being semantic or syntactic. Recent studies have shown that these lexical networks exhibit small-world and scale-free features. The other direction of exploration is to study networks of language users (i.e. social networks of people in the linguistic community), and their role in language evolution. Social networks also show small-world and scale-free features, which cannot be captured by random or regular network models. In the past, computational models of language change and language emergence often assume a population to have a random or regular structure, and there has been little discussion how network structures may affect the dynamics. In the second part of the paper, a series of simulation models of diffusion of linguistic innovation are used to illustrate the importance of choosing realistic conditions of population structure for modeling language change. Four types of social networks are compared, which exhibit two categories of diffusion dynamics. While the questions about which type of networks are more appropriate for modeling still remains, we give some preliminary suggestions for choosing the type of social networks for modeling.",
        "published": "2007-01-22T00:45:31Z",
        "link": "http://arxiv.org/abs/cs/0701135v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "A Note on Local Ultrametricity in Text",
        "authors": [
            "Fionn Murtagh"
        ],
        "summary": "High dimensional, sparsely populated data spaces have been characterized in terms of ultrametric topology. This implies that there are natural, not necessarily unique, tree or hierarchy structures defined by the ultrametric topology. In this note we study the extent of local ultrametric topology in texts, with the aim of finding unique ``fingerprints'' for a text or corpus, discriminating between texts from different domains, and opening up the possibility of exploiting hierarchical structures in the data. We use coherent and meaningful collections of over 1000 texts, comprising over 1.3 million words.",
        "published": "2007-01-27T19:09:53Z",
        "link": "http://arxiv.org/abs/cs/0701181v1",
        "categories": [
            "cs.CL",
            "I.5.3; I.7.2; H.3"
        ]
    },
    {
        "title": "Menzerath-Altmann Law for Syntactic Structures in Ukrainian",
        "authors": [
            "Solomija Buk",
            "Andrij Rovenchak"
        ],
        "summary": "In the paper, the definition of clause suitable for an automated processing of a Ukrainian text is proposed. The Menzerath-Altmann law is verified on the sentence level and the parameters for the dependences of the clause length counted in words and syllables on the sentence length counted in clauses are calculated for \"Perekhresni Stezhky\" (\"The Cross-Paths\"), a novel by Ivan Franko.",
        "published": "2007-01-30T16:58:07Z",
        "link": "http://arxiv.org/abs/cs/0701194v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Random Sentences from a Generalized Phrase-Structure Grammar Interpreter",
        "authors": [
            "Rick Dale"
        ],
        "summary": "In numerous domains in cognitive science it is often useful to have a source for randomly generated corpora. These corpora may serve as a foundation for artificial stimuli in a learning experiment (e.g., Ellefson & Christiansen, 2000), or as input into computational models (e.g., Christiansen & Dale, 2001). The following compact and general C program interprets a phrase-structure grammar specified in a text file. It follows parameters set at a Unix or Unix-based command-line and generates a corpus of random sentences from that grammar.",
        "published": "2007-02-14T06:05:20Z",
        "link": "http://arxiv.org/abs/cs/0702081v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Interroger un corpus par le sens",
        "authors": [
            "Bernard Jacquemin"
        ],
        "summary": "In textual knowledge management, statistical methods prevail. Nonetheless, some difficulties cannot be overcome by these methodologies. I propose a symbolic approach using a complete textual analysis to identify which analysis level can improve the the answers provided by a system. The approach identifies word senses and relation between words and generates as many rephrasings as possible. Using synonyms and derivative, the system provides new utterances without changing the original meaning of the sentences. Such a way, an information can be retrieved whatever the question or answer's wording may be.",
        "published": "2007-03-06T19:50:37Z",
        "link": "http://arxiv.org/abs/cs/0703027v2",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "Algorithm of Segment-Syllabic Synthesis in Speech Recognition Problem",
        "authors": [
            "Oleg N. Karpov",
            "Olga A. Savenkova"
        ],
        "summary": "Speech recognition based on the syllable segment is discussed in this paper. The principal search methods in space of states for the speech recognition problem by segment-syllabic parameters trajectory synthesis are investigated. Recognition as comparison the parameters trajectories in chosen speech units on the sections of the segmented speech is realized. Some experimental results are given and discussed.",
        "published": "2007-03-10T23:59:55Z",
        "link": "http://arxiv.org/abs/cs/0703049v1",
        "categories": [
            "cs.SD",
            "cs.CL"
        ]
    },
    {
        "title": "Dependency Parsing with Dynamic Bayesian Network",
        "authors": [
            "Virginia Savova",
            "Leonid Peshkin"
        ],
        "summary": "Exact parsing with finite state automata is deemed inappropriate because of the unbounded non-locality languages overwhelmingly exhibit. We propose a way to structure the parsing task in order to make it amenable to local classification methods. This allows us to build a Dynamic Bayesian Network which uncovers the syntactic dependency structure of English sentences. Experiments with the Wall Street Journal demonstrate that the model successfully learns from labeled data.",
        "published": "2007-03-27T22:12:25Z",
        "link": "http://arxiv.org/abs/cs/0703135v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7; I.2.1; G.3; H.3.1"
        ]
    },
    {
        "title": "Introduction to Arabic Speech Recognition Using CMUSphinx System",
        "authors": [
            "H. Satori",
            "M. Harti",
            "N. Chenfour"
        ],
        "summary": "In this paper Arabic was investigated from the speech recognition problem point of view. We propose a novel approach to build an Arabic Automated Speech Recognition System (ASR). This system is based on the open source CMU Sphinx-4, from the Carnegie Mellon University. CMU Sphinx is a large-vocabulary; speaker-independent, continuous speech recognition system based on discrete Hidden Markov Models (HMMs). We build a model using utilities from the OpenSource CMU Sphinx. We will demonstrate the possible adaptability of this system to Arabic voice recognition.",
        "published": "2007-04-17T01:04:01Z",
        "link": "http://arxiv.org/abs/0704.2083v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Arabic Speech Recognition System using CMU-Sphinx4",
        "authors": [
            "H. Satori",
            "M. Harti",
            "N. Chenfour"
        ],
        "summary": "In this paper we present the creation of an Arabic version of Automated Speech Recognition System (ASR). This system is based on the open source Sphinx-4, from the Carnegie Mellon University. Which is a speech recognition system based on discrete hidden Markov models (HMMs). We investigate the changes that must be made to the model to adapt Arabic voice recognition.   Keywords: Speech recognition, Acoustic model, Arabic language, HMMs, CMUSphinx-4, Artificial intelligence.",
        "published": "2007-04-17T17:04:26Z",
        "link": "http://arxiv.org/abs/0704.2201v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "An Automated Evaluation Metric for Chinese Text Entry",
        "authors": [
            "Mike Tian-Jian Jiang",
            "James Zhan",
            "Jaimie Lin",
            "Jerry Lin",
            "Wen-Lien Hsu"
        ],
        "summary": "In this paper, we propose an automated evaluation metric for text entry. We also consider possible improvements to existing text entry evaluation metrics, such as the minimum string distance error rate, keystrokes per character, cost per correction, and a unified approach proposed by MacKenzie, so they can accommodate the special characteristics of Chinese text. Current methods lack an integrated concern about both typing speed and accuracy for Chinese text entry evaluation. Our goal is to remove the bias that arises due to human factors. First, we propose a new metric, called the correction penalty (P), based on Fitts' law and Hick's law. Next, we transform it into the approximate amortized cost (AAC) of information theory. An analysis of the AAC of Chinese text input methods with different context lengths is also presented.",
        "published": "2007-04-27T05:34:10Z",
        "link": "http://arxiv.org/abs/0704.3662v1",
        "categories": [
            "cs.HC",
            "cs.CL"
        ]
    },
    {
        "title": "On the Development of Text Input Method - Lessons Learned",
        "authors": [
            "Mike Tian-Jian Jiang",
            "Deng Liu",
            "Meng-Juei Hsieh",
            "Wen-Lien Hsu"
        ],
        "summary": "Intelligent Input Methods (IM) are essential for making text entries in many East Asian scripts, but their application to other languages has not been fully explored. This paper discusses how such tools can contribute to the development of computer processing of other oriental languages. We propose a design philosophy that regards IM as a text service platform, and treats the study of IM as a cross disciplinary subject from the perspectives of software engineering, human-computer interaction (HCI), and natural language processing (NLP). We discuss these three perspectives and indicate a number of possible future research directions.",
        "published": "2007-04-27T05:58:32Z",
        "link": "http://arxiv.org/abs/0704.3665v1",
        "categories": [
            "cs.CL",
            "cs.HC"
        ]
    },
    {
        "title": "Network statistics on early English Syntax: Structural criteria",
        "authors": [
            "Bernat Corominas-Murtra"
        ],
        "summary": "This paper includes a reflection on the role of networks in the study of English language acquisition, as well as a collection of practical criteria to annotate free-speech corpora from children utterances. At the theoretical level, the main claim of this paper is that syntactic networks should be interpreted as the outcome of the use of the syntactic machinery. Thus, the intrinsic features of such machinery are not accessible directly from (known) network properties. Rather, what one can see are the global patterns of its use and, thus, a global view of the power and organization of the underlying grammar. Taking a look into more practical issues, the paper examines how to build a net from the projection of syntactic relations. Recall that, as opposed to adult grammars, early-child language has not a well-defined concept of structure. To overcome such difficulty, we develop a set of systematic criteria assuming constituency hierarchy and a grammar based on lexico-thematic relations. At the end, what we obtain is a well defined corpora annotation that enables us i) to perform statistics on the size of structures and ii) to build a network from syntactic relations over which we can perform the standard measures of complexity. We also provide a detailed example.",
        "published": "2007-04-27T17:13:37Z",
        "link": "http://arxiv.org/abs/0704.3708v2",
        "categories": [
            "cs.CL",
            "D.2.2; H.1.2; I.1.3"
        ]
    },
    {
        "title": "A Note on Ontology and Ordinary Language",
        "authors": [
            "Walid S. Saba"
        ],
        "summary": "We argue for a compositional semantics grounded in a strongly typed ontology that reflects our commonsense view of the world and the way we talk about it. Assuming such a structure we show that the semantics of various natural language phenomena may become nearly trivial.",
        "published": "2007-04-30T17:55:39Z",
        "link": "http://arxiv.org/abs/0704.3886v6",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Resource modalities in game semantics",
        "authors": [
            "Paul-André Melliès",
            "Nicolas Tabareau"
        ],
        "summary": "The description of resources in game semantics has never achieved the simplicity and precision of linear logic, because of a misleading conception: the belief that linear logic is more primitive than game semantics. We advocate instead the contrary: that game semantics is conceptually more primitive than linear logic. Starting from this revised point of view, we design a categorical model of resources in game semantics, and construct an arena game model where the usual notion of bracketing is extended to multi- bracketing in order to capture various resource policies: linear, af&#64257;ne and exponential.",
        "published": "2007-05-03T13:44:54Z",
        "link": "http://arxiv.org/abs/0705.0462v1",
        "categories": [
            "math.CT",
            "cs.CL"
        ]
    },
    {
        "title": "IDF revisited: A simple new derivation within the Robertson-Spärck   Jones probabilistic model",
        "authors": [
            "Lillian Lee"
        ],
        "summary": "There have been a number of prior attempts to theoretically justify the effectiveness of the inverse document frequency (IDF). Those that take as their starting point Robertson and Sparck Jones's probabilistic model are based on strong or complex assumptions. We show that a more intuitively plausible assumption suffices. Moreover, the new assumption, while conceptually very simple, provides a solution to an estimation problem that had been deemed intractable by Robertson and Walker (1997).",
        "published": "2007-05-08T20:08:13Z",
        "link": "http://arxiv.org/abs/0705.1161v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3"
        ]
    },
    {
        "title": "Recursive n-gram hashing is pairwise independent, at best",
        "authors": [
            "Daniel Lemire",
            "Owen Kaser"
        ],
        "summary": "Many applications use sequences of n consecutive symbols (n-grams). Hashing these n-grams can be a performance bottleneck. For more speed, recursive hash families compute hash values by updating previous values. We prove that recursive hash families cannot be more than pairwise independent. While hashing by irreducible polynomials is pairwise independent, our implementations either run in time O(n) or use an exponential amount of memory. As a more scalable alternative, we make hashing by cyclic polynomials pairwise independent by ignoring n-1 bits. Experimentally, we show that hashing by cyclic polynomials is is twice as fast as hashing by irreducible polynomials. We also show that randomized Karp-Rabin hash families are not pairwise independent.",
        "published": "2007-05-31T18:41:28Z",
        "link": "http://arxiv.org/abs/0705.4676v8",
        "categories": [
            "cs.DB",
            "cs.CL",
            "H.3.3, H.2.7"
        ]
    },
    {
        "title": "Segmentation and Context of Literary and Musical Sequences",
        "authors": [
            "Damian H. Zanette"
        ],
        "summary": "We test a segmentation algorithm, based on the calculation of the Jensen-Shannon divergence between probability distributions, to two symbolic sequences of literary and musical origin. The first sequence represents the successive appearance of characters in a theatrical play, and the second represents the succession of tones from the twelve-tone scale in a keyboard sonata. The algorithm divides the sequences into segments of maximal compositional divergence between them. For the play, these segments are related to changes in the frequency of appearance of different characters and in the geographical setting of the action. For the sonata, the segments correspond to tonal domains and reveal in detail the characteristic tonal progression of such kind of musical composition.",
        "published": "2007-07-06T01:45:05Z",
        "link": "http://arxiv.org/abs/0707.0895v1",
        "categories": [
            "cs.CL",
            "physics.data-an"
        ]
    },
    {
        "title": "Removing Manually-Generated Boilerplate from Electronic Texts:   Experiments with Project Gutenberg e-Books",
        "authors": [
            "Owen Kaser",
            "Daniel Lemire"
        ],
        "summary": "Collaborative work on unstructured or semi-structured documents, such as in literature corpora or source code, often involves agreed upon templates containing metadata. These templates are not consistent across users and over time. Rule-based parsing of these templates is expensive to maintain and tends to fail as new documents are added. Statistical techniques based on frequent occurrences have the potential to identify automatically a large fraction of the templates, thus reducing the burden on the programmers. We investigate the case of the Project Gutenberg corpus, where most documents are in ASCII format with preambles and epilogues that are often copied and pasted or manually typed. We show that a statistical approach can solve most cases though some documents require knowledge of English. We also survey various technical solutions that make our approach applicable to large data sets.",
        "published": "2007-07-13T02:30:10Z",
        "link": "http://arxiv.org/abs/0707.1913v3",
        "categories": [
            "cs.DL",
            "cs.CL"
        ]
    },
    {
        "title": "International Standard for a Linguistic Annotation Framework",
        "authors": [
            "Laurent Romary",
            "Nancy Ide"
        ],
        "summary": "This paper describes the Linguistic Annotation Framework under development within ISO TC37 SC4 WG1. The Linguistic Annotation Framework is intended to serve as a basis for harmonizing existing language resources as well as developing new ones.",
        "published": "2007-07-22T15:24:48Z",
        "link": "http://arxiv.org/abs/0707.3269v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "A Formal Model of Dictionary Structure and Content",
        "authors": [
            "Laurent Romary",
            "Nancy Ide",
            "Adam Kilgarriff"
        ],
        "summary": "We show that a general model of lexical information conforms to an abstract model that reflects the hierarchy of information found in a typical dictionary entry. We show that this model can be mapped into a well-formed XML document, and how the XSL transformation language can be used to implement a semantics defined over the abstract model to enable extraction and manipulation of the information in any format.",
        "published": "2007-07-22T15:25:27Z",
        "link": "http://arxiv.org/abs/0707.3270v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Practical Approach to Knowledge-based Question Answering with Natural   Language Understanding and Advanced Reasoning",
        "authors": [
            "Wilson Wong"
        ],
        "summary": "This research hypothesized that a practical approach in the form of a solution framework known as Natural Language Understanding and Reasoning for Intelligence (NaLURI), which combines full-discourse natural language understanding, powerful representation formalism capable of exploiting ontological information and reasoning approach with advanced features, will solve the following problems without compromising practicality factors: 1) restriction on the nature of question and response, and 2) limitation to scale across domains and to real-life natural language text.",
        "published": "2007-07-24T14:30:27Z",
        "link": "http://arxiv.org/abs/0707.3559v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.IR",
            "I.2.7; H.5.2; H.3.4; H.3.3"
        ]
    },
    {
        "title": "Learning Probabilistic Models of Word Sense Disambiguation",
        "authors": [
            "Ted Pedersen"
        ],
        "summary": "This dissertation presents several new methods of supervised and unsupervised learning of word sense disambiguation models. The supervised methods focus on performing model searches through a space of probabilistic models, and the unsupervised methods rely on the use of Gibbs Sampling and the Expectation Maximization (EM) algorithm. In both the supervised and unsupervised case, the Naive Bayesian model is found to perform well. An explanation for this success is presented in terms of learning rates and bias-variance decompositions.",
        "published": "2007-07-26T17:02:40Z",
        "link": "http://arxiv.org/abs/0707.3972v1",
        "categories": [
            "cs.CL",
            "cs.AI"
        ]
    },
    {
        "title": "Reconstruction of Protein-Protein Interaction Pathways by Mining   Subject-Verb-Objects Intermediates",
        "authors": [
            "Maurice HT Ling",
            "Christophe Lefevre",
            "Kevin R. Nicholas",
            "Feng Lin"
        ],
        "summary": "The exponential increase in publication rate of new articles is limiting access of researchers to relevant literature. This has prompted the use of text mining tools to extract key biological information. Previous studies have reported extensive modification of existing generic text processors to process biological text. However, this requirement for modification had not been examined. In this study, we have constructed Muscorian, using MontyLingua, a generic text processor. It uses a two-layered generalization-specialization paradigm previously proposed where text was generically processed to a suitable intermediate format before domain-specific data extraction techniques are applied at the specialization layer. Evaluation using a corpus and experts indicated 86-90% precision and approximately 30% recall in extracting protein-protein interactions, which was comparable to previous studies using either specialized biological text processing tools or modified existing tools. Our study had also demonstrated the flexibility of the two-layered generalization-specialization paradigm by using the same generalization layer for two specialized information extraction tasks.",
        "published": "2007-08-06T01:22:46Z",
        "link": "http://arxiv.org/abs/0708.0694v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "cs.DL"
        ]
    },
    {
        "title": "Learning Phonotactics Using ILP",
        "authors": [
            "Stasinos Konstantopoulos"
        ],
        "summary": "This paper describes experiments on learning Dutch phonotactic rules using Inductive Logic Programming, a machine learning discipline based on inductive logical operators. Two different ways of approaching the problem are experimented with, and compared against each other as well as with related work on the task. The results show a direct correspondence between the quality and informedness of the background knowledge and the constructed theory, demonstrating the ability of ILP to take good advantage of the prior domain knowledge available. Further research is outlined.",
        "published": "2007-08-11T13:09:27Z",
        "link": "http://arxiv.org/abs/0708.1564v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Compositional Semantics Grounded in Commonsense Metaphysics",
        "authors": [
            "Walid S. Saba"
        ],
        "summary": "We argue for a compositional semantics grounded in a strongly typed ontology that reflects our commonsense view of the world and the way we talk about it in ordinary language. Assuming the existence of such a structure, we show that the semantics of various natural language phenomena may become nearly trivial.",
        "published": "2007-08-17T01:15:11Z",
        "link": "http://arxiv.org/abs/0708.2303v2",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "On Ultrametric Algorithmic Information",
        "authors": [
            "Fionn Murtagh"
        ],
        "summary": "How best to quantify the information of an object, whether natural or artifact, is a problem of wide interest. A related problem is the computability of an object. We present practical examples of a new way to address this problem. By giving an appropriate representation to our objects, based on a hierarchical coding of information, we exemplify how it is remarkably easy to compute complex objects. Our algorithmic complexity is related to the length of the class of objects, rather than to the length of the object.",
        "published": "2007-09-02T17:00:40Z",
        "link": "http://arxiv.org/abs/0709.0116v2",
        "categories": [
            "cs.AI",
            "cs.CL",
            "I.2.0"
        ]
    },
    {
        "title": "Bootstrapping Deep Lexical Resources: Resources for Courses",
        "authors": [
            "Timothy Baldwin"
        ],
        "summary": "We propose a range of deep lexical acquisition methods which make use of morphological, syntactic and ontological language resources to model word similarity and bootstrap from a seed lexicon. The different methods are deployed in learning lexical items for a precision grammar, and shown to each have strengths and weaknesses over different word classes. A particular focus of this paper is the relative accessibility of different language resource types, and predicted ``bang for the buck'' associated with each in deep lexical acquisition applications.",
        "published": "2007-09-15T01:37:21Z",
        "link": "http://arxiv.org/abs/0709.2401v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Zipf's Law and Avoidance of Excessive Synonymy",
        "authors": [
            "Dmitrii Manin"
        ],
        "summary": "Zipf's law states that if words of language are ranked in the order of decreasing frequency in texts, the frequency of a word is inversely proportional to its rank. It is very robust as an experimental observation, but to date it escaped satisfactory theoretical explanation. We suggest that Zipf's law may arise from the evolution of word semantics dominated by expansion of meanings and competition of synonyms.",
        "published": "2007-09-30T03:21:54Z",
        "link": "http://arxiv.org/abs/0710.0105v2",
        "categories": [
            "cs.CL",
            "physics.soc-ph"
        ]
    },
    {
        "title": "On the role of autocorrelations in texts",
        "authors": [
            "D. V. Lande",
            "A. A. Snarskii"
        ],
        "summary": "The task of finding a criterion allowing to distinguish a text from an arbitrary set of words is rather relevant in itself, for instance, in the aspect of development of means for internet-content indexing or separating signals and noise in communication channels. The Zipf law is currently considered to be the most reliable criterion of this kind [3]. At any rate, conventional stochastic word sets do not meet this law. The present paper deals with one of possible criteria based on the determination of the degree of data compression.",
        "published": "2007-10-01T08:23:24Z",
        "link": "http://arxiv.org/abs/0710.0225v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "On the fractal nature of mutual relevance sequences in the Internet news   message flows",
        "authors": [
            "S. Braichevsky",
            "D. Lande",
            "A. Snarskii"
        ],
        "summary": "In the task of information retrieval the term relevance is taken to mean formal conformity of a document given by the retrieval system to user's information query. As a rule, the documents found by the retrieval system should be submitted to the user in a certain order. Therefore, a retrieval perceived as a selection of documents formally solving the user's query, should be supplemented with a certain procedure of processing a relevant set. It would be natural to introduce a quantitative measure of document conformity to query, i.e. the relevance measure. Since no single rule exists for the determination of the relevance measure, we shall consider two of them which are the simplest in our opinion. The proposed approach does not suppose any restrictions and can be applied to other relevance measures.",
        "published": "2007-10-01T08:31:56Z",
        "link": "http://arxiv.org/abs/0710.0228v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Bio-linguistic transition and Baldwin effect in an evolutionary   naming-game model",
        "authors": [
            "Adam Lipowski",
            "Dorota Lipowska"
        ],
        "summary": "We examine an evolutionary naming-game model where communicating agents are equipped with an evolutionarily selected learning ability. Such a coupling of biological and linguistic ingredients results in an abrupt transition: upon a small change of a model control parameter a poorly communicating group of linguistically unskilled agents transforms into almost perfectly communicating group with large learning abilities. When learning ability is kept fixed, the transition appears to be continuous. Genetic imprinting of the learning abilities proceeds via Baldwin effect: initially unskilled communicating agents learn a language and that creates a niche in which there is an evolutionary pressure for the increase of learning ability.Our model suggests that when linguistic (or cultural) processes became intensive enough, a transition took place where both linguistic performance and biological endowment of our species experienced an abrupt change that perhaps triggered the rapid expansion of human civilization.",
        "published": "2007-10-01T09:39:49Z",
        "link": "http://arxiv.org/abs/0710.0009v2",
        "categories": [
            "cs.CL",
            "cond-mat.stat-mech",
            "cs.AI",
            "physics.soc-ph",
            "q-bio.PE"
        ]
    },
    {
        "title": "Evaluation experiments on related terms search in Wikipedia: Information   Content and Adapted HITS (In Russian)",
        "authors": [
            "A. A. Krizhanovsky"
        ],
        "summary": "The classification of metrics and algorithms search for related terms via WordNet, Roget's Thesaurus, and Wikipedia was extended to include adapted HITS algorithm. Evaluation experiments on Information Content and adapted HITS algorithm are described. The test collection of Russian word pairs with human-assigned similarity judgments is proposed.   -----   Klassifikacija metrik i algoritmov poiska semanticheski blizkih slov v tezaurusah WordNet, Rozhe i jenciklopedii Vikipedija rasshirena adaptirovannym HITS algoritmom. S pomow'ju jeksperimentov v Vikipedii oceneny metrika Information Content i adaptirovannyj algoritm HITS. Predlozhen resurs dlja ocenki semanticheskoj blizosti russkih slov.",
        "published": "2007-10-01T16:04:52Z",
        "link": "http://arxiv.org/abs/0710.0169v6",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.1; H.3.3; H.4.3; G.2.2"
        ]
    },
    {
        "title": "What's in a Name?",
        "authors": [
            "Stasinos Konstantopoulos"
        ],
        "summary": "This paper describes experiments on identifying the language of a single name in isolation or in a document written in a different language. A new corpus has been compiled and made available, matching names against languages. This corpus is used in a series of experiments measuring the performance of general language models and names-only language models on the language identification task. Conclusions are drawn from the comparison between using general language models and names-only language models and between identifying the language of isolated names and the language of very short document fragments. Future research directions are outlined.",
        "published": "2007-10-08T08:36:32Z",
        "link": "http://arxiv.org/abs/0710.1481v1",
        "categories": [
            "cs.CL",
            "cs.AI"
        ]
    },
    {
        "title": "Demographic growth and the distribution of language sizes",
        "authors": [
            "Damian H. Zanette"
        ],
        "summary": "It is argued that the present log-normal distribution of language sizes is, to a large extent, a consequence of demographic dynamics within the population of speakers of each language. A two-parameter stochastic multiplicative process is proposed as a model for the population dynamics of individual languages, and applied over a period spanning the last ten centuries. The model disregards language birth and death. A straightforward fitting of the two parameters, which statistically characterize the population growth rate, predicts a distribution of language sizes in excellent agreement with empirical data. Numerical simulations, and the study of the size distribution within language families, validate the assumptions at the basis of the model.",
        "published": "2007-10-08T11:16:51Z",
        "link": "http://arxiv.org/abs/0710.1511v1",
        "categories": [
            "physics.data-an",
            "cs.CL",
            "physics.soc-ph"
        ]
    },
    {
        "title": "The structure of verbal sequences analyzed with unsupervised learning   techniques",
        "authors": [
            "Catherine Recanati",
            "Nicoleta Rogovschi",
            "Younès Bennani"
        ],
        "summary": "Data mining allows the exploration of sequences of phenomena, whereas one usually tends to focus on isolated phenomena or on the relation between two phenomena. It offers invaluable tools for theoretical analyses and exploration of the structure of sentences, texts, dialogues, and speech. We report here the results of an attempt at using it for inspecting sequences of verbs from French accounts of road accidents. This analysis comes from an original approach of unsupervised training allowing the discovery of the structure of sequential data. The entries of the analyzer were only made of the verbs appearing in the sentences. It provided a classification of the links between two successive verbs into four distinct clusters, allowing thus text segmentation. We give here an interpretation of these clusters by applying a statistical analysis to independent semantic annotations.",
        "published": "2007-10-12T12:44:11Z",
        "link": "http://arxiv.org/abs/0710.2446v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Linguistic Information Energy",
        "authors": [
            "James Ford"
        ],
        "summary": "In this treatment a text is considered to be a series of word impulses which are read at a constant rate. The brain then assembles these units of information into higher units of meaning. A classical systems approach is used to model an initial part of this assembly process. The concepts of linguistic system response, information energy, and ordering energy are defined and analyzed. Finally, as a demonstration, information energy is used to estimate the publication dates of a series of texts and the similarity of a set of texts.",
        "published": "2007-10-14T16:09:53Z",
        "link": "http://arxiv.org/abs/0710.2674v1",
        "categories": [
            "cs.CL",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Generating models for temporal representations",
        "authors": [
            "Patrick Blackburn",
            "Sébastien Hinderer"
        ],
        "summary": "We discuss the use of model building for temporal representations. We chose Polish to illustrate our discussion because it has an interesting aspectual system, but the points we wish to make are not language specific. Rather, our goal is to develop theoretical and computational tools for temporal model building tasks in computational semantics. To this end, we present a first-order theory of time and events which is rich enough to capture interesting semantic distinctions, and an algorithm which takes minimal models for first-order theories and systematically attempts to ``perturb'' their temporal component to provide non-minimal, but semantically significant, models.",
        "published": "2007-10-15T15:45:13Z",
        "link": "http://arxiv.org/abs/0710.2852v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Using Description Logics for Recognising Textual Entailment",
        "authors": [
            "Paul Bedaride"
        ],
        "summary": "The aim of this paper is to show how we can handle the Recognising Textual Entailment (RTE) task by using Description Logics (DLs). To do this, we propose a representation of natural language semantics in DLs inspired by existing representations in first-order logic. But our most significant contribution is the definition of two novel inference tasks: A-Box saturation and subgraph detection which are crucial for our approach to RTE.",
        "published": "2007-10-16T09:16:24Z",
        "link": "http://arxiv.org/abs/0710.2988v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Nontraditional Scoring of C-tests",
        "authors": [
            "Tretjakova Tamara"
        ],
        "summary": "In C-tests the hypothesis of items local independence is violated, which doesn't permit to consider them as real tests. It is suggested to determine the distances between separate C-test items (blanks) and to combine items into clusters. Weights, inversely proportional to the number of items in corresponding clusters, are assigned to items. As a result, the C-test structure becomes similar to the structure of classical tests, without violation of local independence hypothesis.",
        "published": "2007-10-17T12:42:39Z",
        "link": "http://arxiv.org/abs/0710.3285v2",
        "categories": [
            "cs.CY",
            "cs.CL",
            "I.2.6; K.3.2"
        ]
    },
    {
        "title": "Using Synchronic and Diachronic Relations for Summarizing Multiple   Documents Describing Evolving Events",
        "authors": [
            "Stergos D. Afantenos",
            "V. Karkaletsis",
            "P. Stamatopoulos",
            "C. Halatsis"
        ],
        "summary": "In this paper we present a fresh look at the problem of summarizing evolving events from multiple sources. After a discussion concerning the nature of evolving events we introduce a distinction between linearly and non-linearly evolving events. We present then a general methodology for the automatic creation of summaries from evolving events. At its heart lie the notions of Synchronic and Diachronic cross-document Relations (SDRs), whose aim is the identification of similarities and differences between sources, from a synchronical and diachronical perspective. SDRs do not connect documents or textual elements found therein, but structures one might call messages. Applying this methodology will yield a set of messages and relations, SDRs, connecting them, that is a graph which we call grid. We will show how such a grid can be considered as the starting point of a Natural Language Generation System. The methodology is evaluated in two case-studies, one for linearly evolving events (descriptions of football matches) and another one for non-linearly evolving events (terrorist incidents involving hostages). In both cases we evaluate the results produced by our computational systems.",
        "published": "2007-10-18T13:24:26Z",
        "link": "http://arxiv.org/abs/0710.3502v1",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "The predictability of letters in written english",
        "authors": [
            "Thomas Schürmann",
            "Peter Grassberger"
        ],
        "summary": "We show that the predictability of letters in written English texts depends strongly on their position in the word. The first letters are usually the least easy to predict. This agrees with the intuitive notion that words are well defined subunits in written languages, with much weaker correlations across these units than within them. It implies that the average entropy of a letter deep inside a word is roughly 4 times smaller than the entropy of the first letter.",
        "published": "2007-10-24T17:23:13Z",
        "link": "http://arxiv.org/abs/0710.4516v2",
        "categories": [
            "physics.soc-ph",
            "cs.CL",
            "stat.ML"
        ]
    },
    {
        "title": "Some Reflections on the Task of Content Determination in the Context of   Multi-Document Summarization of Evolving Events",
        "authors": [
            "Stergos D. Afantenos"
        ],
        "summary": "Despite its importance, the task of summarizing evolving events has received small attention by researchers in the field of multi-document summariztion. In a previous paper (Afantenos et al. 2007) we have presented a methodology for the automatic summarization of documents, emitted by multiple sources, which describe the evolution of an event. At the heart of this methodology lies the identification of similarities and differences between the various documents, in two axes: the synchronic and the diachronic. This is achieved by the introduction of the notion of Synchronic and Diachronic Relations. Those relations connect the messages that are found in the documents, resulting thus in a graph which we call grid. Although the creation of the grid completes the Document Planning phase of a typical NLG architecture, it can be the case that the number of messages contained in a grid is very large, exceeding thus the required compression rate. In this paper we provide some initial thoughts on a probabilistic model which can be applied at the Content Determination stage, and which tries to alleviate this problem.",
        "published": "2007-10-29T10:48:48Z",
        "link": "http://arxiv.org/abs/0710.5382v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Discriminative Phoneme Sequences Extraction for Non-Native Speaker's   Origin Classification",
        "authors": [
            "Ghazi Bouselmi",
            "Dominique Fohr",
            "Irina Illina",
            "Jean-Paul Haton"
        ],
        "summary": "In this paper we present an automated method for the classification of the origin of non-native speakers. The origin of non-native speakers could be identified by a human listener based on the detection of typical pronunciations for each nationality. Thus we suppose the existence of several phoneme sequences that might allow the classification of the origin of non-native speakers. Our new method is based on the extraction of discriminative sequences of phonemes from a non-native English speech database. These sequences are used to construct a probabilistic classifier for the speakers' origin. The existence of discriminative phone sequences in non-native speech is a significant result of this work. The system that we have developed achieved a significant correct classification rate of 96.3% and a significant error reduction compared to some other tested techniques.",
        "published": "2007-11-05T15:20:47Z",
        "link": "http://arxiv.org/abs/0711.0666v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Combined Acoustic and Pronunciation Modelling for Non-Native Speech   Recognition",
        "authors": [
            "Ghazi Bouselmi",
            "Dominique Fohr",
            "Irina Illina"
        ],
        "summary": "In this paper, we present several adaptation methods for non-native speech recognition. We have tested pronunciation modelling, MLLR and MAP non-native pronunciation adaptation and HMM models retraining on the HIWIRE foreign accented English speech database. The ``phonetic confusion'' scheme we have developed consists in associating to each spoken phone several sequences of confused phones. In our experiments, we have used different combinations of acoustic models representing the canonical and the foreign pronunciations: spoken and native models, models adapted to the non-native accent with MAP and MLLR. The joint use of pronunciation modelling and acoustic adaptation led to further improvements in recognition accuracy. The best combination of the above mentioned techniques resulted in a relative word error reduction ranging from 46% to 71%.",
        "published": "2007-11-06T08:23:49Z",
        "link": "http://arxiv.org/abs/0711.0811v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Amélioration des Performances des Systèmes Automatiques de   Reconnaissance de la Parole pour la Parole Non Native",
        "authors": [
            "Ghazi Bouselmi",
            "Dominique Fohr",
            "Irina Illina",
            "Jean-Paul Haton"
        ],
        "summary": "In this article, we present an approach for non native automatic speech recognition (ASR). We propose two methods to adapt existing ASR systems to the non-native accents. The first method is based on the modification of acoustic models through integration of acoustic models from the mother tong. The phonemes of the target language are pronounced in a similar manner to the native language of speakers. We propose to combine the models of confused phonemes so that the ASR system could recognize both concurrent pronounciations. The second method we propose is a refinment of the pronounciation error detection through the introduction of graphemic constraints. Indeed, non native speakers may rely on the writing of words in their uttering. Thus, the pronounctiation errors might depend on the characters composing the words. The average error rate reduction that we observed is (22.5%) relative for the sentence error rate, and 34.5% (relative) in word error rate.",
        "published": "2007-11-07T08:51:09Z",
        "link": "http://arxiv.org/abs/0711.1038v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Analytical approach to bit-string models of language evolution",
        "authors": [
            "Damian H. Zanette"
        ],
        "summary": "A formulation of bit-string models of language evolution, based on differential equations for the population speaking each language, is introduced and preliminarily studied. Connections with replicator dynamics and diffusion processes are pointed out. The stability of the dominance state, where most of the population speaks a single language, is analyzed within a mean-field-like approximation, while the homogeneous state, where the population is evenly distributed among languages, can be exactly studied. This analysis discloses the existence of a bistability region, where dominance coexists with homogeneity as possible asymptotic states. Numerical resolution of the differential system validates these findings.",
        "published": "2007-11-08T21:05:38Z",
        "link": "http://arxiv.org/abs/0711.1360v1",
        "categories": [
            "physics.soc-ph",
            "cs.CL"
        ]
    },
    {
        "title": "Empirical Evaluation of Four Tensor Decomposition Algorithms",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Higher-order tensor decompositions are analogous to the familiar Singular Value Decomposition (SVD), but they transcend the limitations of matrices (second-order tensors). SVD is a powerful tool that has achieved impressive results in information retrieval, collaborative filtering, computational linguistics, computational vision, and other fields. However, SVD is limited to two-dimensional arrays of data (two modes), and many potential applications have three or more modes, which require higher-order tensor decompositions. This paper evaluates four algorithms for higher-order tensor decomposition: Higher-Order Singular Value Decomposition (HO-SVD), Higher-Order Orthogonal Iteration (HOOI), Slice Projection (SP), and Multislice Projection (MP). We measure the time (elapsed run time), space (RAM and disk space requirements), and fit (tensor reconstruction accuracy) of the four algorithms, under a variety of conditions. We find that standard implementations of HO-SVD and HOOI do not scale up to larger tensors, due to increasing RAM requirements. We recommend HOOI for tensors that are small enough for the available RAM and MP for larger tensors.",
        "published": "2007-11-13T16:28:47Z",
        "link": "http://arxiv.org/abs/0711.2023v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; I.2.6; I.2.7; E.1; G.1.3"
        ]
    },
    {
        "title": "Can a Computer Laugh ?",
        "authors": [
            "I. M. Suslov"
        ],
        "summary": "A computer model of \"a sense of humour\" suggested previously [arXiv:0711.2058,0711.2061], relating the humorous effect with a specific malfunction in information processing, is given in somewhat different exposition. Psychological aspects of humour are elaborated more thoroughly. The mechanism of laughter is formulated on the more general level. Detailed discussion is presented for the higher levels of information processing, which are responsible for a perception of complex samples of humour. Development of a sense of humour in the process of evolution is discussed.",
        "published": "2007-11-14T18:32:09Z",
        "link": "http://arxiv.org/abs/0711.2270v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "q-bio.NC"
        ]
    },
    {
        "title": "Proof nets for display logic",
        "authors": [
            "Richard Moot"
        ],
        "summary": "This paper explores several extensions of proof nets for the Lambek calculus in order to handle the different connectives of display logic in a natural way. The new proof net calculus handles some recent additions to the Lambek vocabulary such as Galois connections and Grishin interactions. It concludes with an exploration of the generative capacity of the Lambek-Grishin calculus, presenting an embedding of lexicalized tree adjoining grammars into the Lambek-Grishin calculus.",
        "published": "2007-11-15T15:39:48Z",
        "link": "http://arxiv.org/abs/0711.2444v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "How to realize \"a sense of humour\" in computers ?",
        "authors": [
            "I. M. Suslov"
        ],
        "summary": "Computer model of a \"sense of humour\" suggested previously [arXiv:0711.2058, 0711.2061, 0711.2270] is raised to the level of a realistic algorithm.",
        "published": "2007-11-20T19:57:23Z",
        "link": "http://arxiv.org/abs/0711.3197v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "q-bio.NC"
        ]
    },
    {
        "title": "Morphological annotation of Korean with Directly Maintainable Resources",
        "authors": [
            "Ivan Berlocher",
            "Hyun-Gue Huh",
            "Eric Laporte",
            "Jee-Sun Nam"
        ],
        "summary": "This article describes an exclusively resource-based method of morphological annotation of written Korean text. Korean is an agglutinative language. Our annotator is designed to process text before the operation of a syntactic parser. In its present state, it annotates one-stem words only. The output is a graph of morphemes annotated with accurate linguistic information. The granularity of the tagset is 3 to 5 times higher than usual tagsets. A comparison with a reference annotated corpus showed that it achieves 89% recall without any corpus training. The language resources used by the system are lexicons of stems, transducers of suffixes and transducers of generation of allomorphs. All can be easily updated, which allows users to control the evolution of the performances of the system. It has been claimed that morphological annotation of Korean text could only be performed by a morphological analysis module accessing a lexicon of morphemes. We show that it can also be performed directly with a lexicon of words and without applying morphological rules at annotation time, which speeds up annotation to 1,210 word/s. The lexicon of words is obtained from the maintainable language resources through a fully automated compilation process.",
        "published": "2007-11-21T16:47:57Z",
        "link": "http://arxiv.org/abs/0711.3412v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Lexicon management and standard formats",
        "authors": [
            "Eric Laporte"
        ],
        "summary": "International standards for lexicon formats are in preparation. To a certain extent, the proposed formats converge with prior results of standardization projects. However, their adequacy for (i) lexicon management and (ii) lexicon-driven applications have been little debated in the past, nor are they as a part of the present standardization effort. We examine these issues. IGM has developed XML formats compatible with the emerging international standards, and we report experimental results on large-coverage lexica.",
        "published": "2007-11-21T20:34:08Z",
        "link": "http://arxiv.org/abs/0711.3449v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "In memoriam Maurice Gross",
        "authors": [
            "Eric Laporte"
        ],
        "summary": "Maurice Gross (1934-2001) was both a great linguist and a pioneer in natural language processing. This article is written in homage to his memory",
        "published": "2007-11-21T20:38:29Z",
        "link": "http://arxiv.org/abs/0711.3452v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "A resource-based Korean morphological annotation system",
        "authors": [
            "Hyun-Gue Huh",
            "Eric Laporte"
        ],
        "summary": "We describe a resource-based method of morphological annotation of written Korean text. Korean is an agglutinative language. The output of our system is a graph of morphemes annotated with accurate linguistic information. The language resources used by the system can be easily updated, which allows us-ers to control the evolution of the per-formances of the system. We show that morphological annotation of Korean text can be performed directly with a lexicon of words and without morpho-logical rules.",
        "published": "2007-11-21T20:41:59Z",
        "link": "http://arxiv.org/abs/0711.3453v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Graphes paramétrés et outils de lexicalisation",
        "authors": [
            "Eric Laporte",
            "Sébastien Paumier"
        ],
        "summary": "Shifting to a lexicalized grammar reduces the number of parsing errors and improves application results. However, such an operation affects a syntactic parser in all its aspects. One of our research objectives is to design a realistic model for grammar lexicalization. We carried out experiments for which we used a grammar with a very simple content and formalism, and a very informative syntactic lexicon, the lexicon-grammar of French elaborated by the LADL. Lexicalization was performed by applying the parameterized-graph approach. Our results tend to show that most information in the lexicon-grammar can be transferred into a grammar and exploited successfully for the syntactic parsing of sentences.",
        "published": "2007-11-21T20:44:04Z",
        "link": "http://arxiv.org/abs/0711.3454v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Evaluation of a Grammar of French Determiners",
        "authors": [
            "Eric Laporte"
        ],
        "summary": "Existing syntactic grammars of natural languages, even with a far from complete coverage, are complex objects. Assessments of the quality of parts of such grammars are useful for the validation of their construction. We evaluated the quality of a grammar of French determiners that takes the form of a recursive transition network. The result of the application of this local grammar gives deeper syntactic information than chunking or information available in treebanks. We performed the evaluation by comparison with a corpus independently annotated with information on determiners. We obtained 86% precision and 92% recall on text not tagged for parts of speech.",
        "published": "2007-11-21T20:49:21Z",
        "link": "http://arxiv.org/abs/0711.3457v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Very strict selectional restrictions",
        "authors": [
            "Eric Laporte",
            "Christian Leclère",
            "Maria Carmelita P. Dias"
        ],
        "summary": "We discuss the characteristics and behaviour of two parallel classes of verbs in two Romance languages, French and Portuguese. Examples of these verbs are Port. abater [gado] and Fr. abattre [b\\'etail], both meaning \"slaughter [cattle]\". In both languages, the definition of the class of verbs includes several features: - They have only one essential complement, which is a direct object. - The nominal distribution of the complement is very limited, i.e., few nouns can be selected as head nouns of the complement. However, this selection is not restricted to a single noun, as would be the case for verbal idioms such as Fr. monter la garde \"mount guard\". - We excluded from the class constructions which are reductions of more complex constructions, e.g. Port. afinar [instrumento] com \"tune [instrument] with\".",
        "published": "2007-11-22T15:54:31Z",
        "link": "http://arxiv.org/abs/0711.3605v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Outilex, plate-forme logicielle de traitement de textes écrits",
        "authors": [
            "Olivier Blanc",
            "Matthieu Constant",
            "Eric Laporte"
        ],
        "summary": "The Outilex software platform, which will be made available to research, development and industry, comprises software components implementing all the fundamental operations of written text processing: processing without lexicons, exploitation of lexicons and grammars, language resource management. All data are structured in XML formats, and also in more compact formats, either readable or binary, whenever necessary; the required format converters are included in the platform; the grammar formats allow for combining statistical approaches with resource-based approaches. Manually constructed lexicons for French and English, originating from the LADL, and of substantial coverage, will be distributed with the platform under LGPL-LR license.",
        "published": "2007-11-23T09:45:13Z",
        "link": "http://arxiv.org/abs/0711.3691v2",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Let's get the student into the driver's seat",
        "authors": [
            "Michael Zock",
            "Stergos D. Afantenos"
        ],
        "summary": "Speaking a language and achieving proficiency in another one is a highly complex process which requires the acquisition of various kinds of knowledge and skills, like the learning of words, rules and patterns and their connection to communicative goals (intentions), the usual starting point. To help the learner to acquire these skills we propose an enhanced, electronic version of an age old method: pattern drills (henceforth PDs). While being highly regarded in the fifties, PDs have become unpopular since then, partially because of their lack of grounding (natural context) and rigidity. Despite these shortcomings we do believe in the virtues of this approach, at least with regard to the acquisition of basic linguistic reflexes or skills (automatisms), necessary to survive in the new language. Of course, the method needs improvement, and we will show here how this can be achieved. Unlike tapes or books, computers are open media, allowing for dynamic changes, taking users' performances and preferences into account. Building an electronic version of PDs amounts to building an open resource, accomodatable to the users' ever changing needs.",
        "published": "2007-11-23T13:44:55Z",
        "link": "http://arxiv.org/abs/0711.3726v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Valence extraction using EM selection and co-occurrence matrices",
        "authors": [
            "Łukasz Dębowski"
        ],
        "summary": "This paper discusses two new procedures for extracting verb valences from raw texts, with an application to the Polish language. The first novel technique, the EM selection algorithm, performs unsupervised disambiguation of valence frame forests, obtained by applying a non-probabilistic deep grammar parser and some post-processing to the text. The second new idea concerns filtering of incorrect frames detected in the parsed text and is motivated by an observation that verbs which take similar arguments tend to have similar frames. This phenomenon is described in terms of newly introduced co-occurrence matrices. Using co-occurrence matrices, we split filtering into two steps. The list of valid arguments is first determined for each verb, whereas the pattern according to which the arguments are combined into frames is computed in the following stage. Our best extracted dictionary reaches an $F$-score of 45%, compared to an $F$-score of 39% for the standard frame-based BHT filtering.",
        "published": "2007-11-28T12:16:08Z",
        "link": "http://arxiv.org/abs/0711.4475v6",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Ontology and Formal Semantics - Integration Overdue",
        "authors": [
            "Walid S. Saba"
        ],
        "summary": "In this note we suggest that difficulties encountered in natural language semantics are, for the most part, due to the use of mere symbol manipulation systems that are devoid of any content. In such systems, where there is hardly any link with our common-sense view of the world, and it is quite difficult to envision how one can formally account for the considerable amount of content that is often implicit, but almost never explicitly stated in our everyday discourse. The solution, in our opinion, is a compositional semantics grounded in an ontology that reflects our commonsense view of the world and the way we talk about it in ordinary language. In the compositional logic we envision there are ontological (or first-intension) concepts, and logical (or second-intension) concepts, and where the ontological concepts include not only Davidsonian events, but other abstract objects as well (e.g., states, processes, properties, activities, attributes, etc.) It will be demonstrated here that in such a framework, a number of challenges in the semantics of natural language (e.g., metonymy, intensionality, metaphor, etc.) can be properly and uniformly addressed.",
        "published": "2007-12-01T14:27:12Z",
        "link": "http://arxiv.org/abs/0712.1529v2",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "CLAIRLIB Documentation v1.03",
        "authors": [
            "Dragomir Radev",
            "Mark Hodges",
            "Anthony Fader",
            "Mark Joseph",
            "Joshua Gerrish",
            "Mark Schaller",
            "Jonathan dePeri",
            "Bryan Gibson"
        ],
        "summary": "The Clair library is intended to simplify a number of generic tasks in Natural Language Processing (NLP), Information Retrieval (IR), and Network Analysis. Its architecture also allows for external software to be plugged in with very little effort. Functionality native to Clairlib includes Tokenization, Summarization, LexRank, Biased LexRank, Document Clustering, Document Indexing, PageRank, Biased PageRank, Web Graph Analysis, Network Generation, Power Law Distribution Analysis, Network Analysis (clustering coefficient, degree distribution plotting, average shortest path, diameter, triangles, shortest path matrices, connected components), Cosine Similarity, Random Walks on Graphs, Statistics (distributions, tests), Tf, Idf, Community Finding.",
        "published": "2007-12-19T22:20:40Z",
        "link": "http://arxiv.org/abs/0712.3298v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Framework and Resources for Natural Language Parser Evaluation",
        "authors": [
            "Tuomo Kakkonen"
        ],
        "summary": "Because of the wide variety of contemporary practices used in the automatic syntactic parsing of natural languages, it has become necessary to analyze and evaluate the strengths and weaknesses of different approaches. This research is all the more necessary because there are currently no genre- and domain-independent parsers that are able to analyze unrestricted text with 100% preciseness (I use this term to refer to the correctness of analyses assigned by a parser). All these factors create a need for methods and resources that can be used to evaluate and compare parsing systems. This research describes: (1) A theoretical analysis of current achievements in parsing and parser evaluation. (2) A framework (called FEPa) that can be used to carry out practical parser evaluations and comparisons. (3) A set of new evaluation resources: FiEval is a Finnish treebank under construction, and MGTS and RobSet are parser evaluation resources in English. (4) The results of experiments in which the developed evaluation framework and the two resources for English were used for evaluating a set of selected parsers.",
        "published": "2007-12-21T08:55:17Z",
        "link": "http://arxiv.org/abs/0712.3705v1",
        "categories": [
            "cs.CL",
            "I.2.7; F.4.2"
        ]
    },
    {
        "title": "Toward a statistical mechanics of four letter words",
        "authors": [
            "Greg J. Stephens",
            "William Bialek"
        ],
        "summary": "We consider words as a network of interacting letters, and approximate the probability distribution of states taken on by this network. Despite the intuition that the rules of English spelling are highly combinatorial (and arbitrary), we find that maximum entropy models consistent with pairwise correlations among letters provide a surprisingly good approximation to the full statistics of four letter words, capturing ~92% of the multi-information among letters and even \"discovering\" real words that were not represented in the data from which the pairwise correlations were estimated. The maximum entropy model defines an energy landscape on the space of possible words, and local minima in this landscape account for nearly two-thirds of words used in written English.",
        "published": "2007-12-31T23:51:51Z",
        "link": "http://arxiv.org/abs/0801.0253v1",
        "categories": [
            "q-bio.NC",
            "cs.CL",
            "physics.data-an",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Statistical keyword detection in literary corpora",
        "authors": [
            "Juan P. Herrera",
            "Pedro A. Pury"
        ],
        "summary": "Understanding the complexity of human language requires an appropriate analysis of the statistical distribution of words in texts. We consider the information retrieval problem of detecting and ranking the relevant words of a text by means of statistical information referring to the \"spatial\" use of the words. Shannon's entropy of information is used as a tool for automatic keyword extraction. By using The Origin of Species by Charles Darwin as a representative text sample, we show the performance of our detector and compare it with another proposals in the literature. The random shuffled text receives special attention as a tool for calibrating the ranking indices.",
        "published": "2007-01-05T06:11:16Z",
        "link": "http://arxiv.org/abs/cs/0701028v2",
        "categories": [
            "cs.CL",
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Tagging, Folksonomy & Co - Renaissance of Manual Indexing?",
        "authors": [
            "Jakob Voss"
        ],
        "summary": "This paper gives an overview of current trends in manual indexing on the Web. Along with a general rise of user generated content there are more and more tagging systems that allow users to annotate digital resources with tags (keywords) and share their annotations with other users. Tagging is frequently seen in contrast to traditional knowledge organization systems or as something completely new. This paper shows that tagging should better be seen as a popular form of manual indexing on the Web. Difference between controlled and free indexing blurs with sufficient feedback mechanisms. A revised typology of tagging systems is presented that includes different user roles and knowledge organization systems with hierarchical relationships and vocabulary control. A detailed bibliography of current research in collaborative tagging is included.",
        "published": "2007-01-10T18:18:28Z",
        "link": "http://arxiv.org/abs/cs/0701072v2",
        "categories": [
            "cs.IR",
            "H.3.1"
        ]
    },
    {
        "title": "Citation Advantage For OA Self-Archiving Is Independent of Journal   Impact Factor, Article Age, and Number of Co-Authors",
        "authors": [
            "Chawki Hajjem",
            "Stevan Harnad"
        ],
        "summary": "Eysenbach has suggested that the OA (Green) self-archiving advantage might just be an artifact of potential uncontrolled confounding factors such as article age (older articles may be both more cited and more likely to be self-archived), number of authors (articles with more authors might be more cited and more self-archived), subject matter (the subjects that are cited more, self-archive more), country (same thing), number of authors, citation counts of authors, etc. Chawki Hajjem (doctoral candidate, UQaM) had already shown that the OA advantage was present in all cases when articles were analysed separately by age, subject matter or country. He has now done a multiple regression analysis jointly testing (1) article age, (2) journal impact factor, (3) number of authors, and (4) OA self-archiving as separate factors for 442,750 articles in 576 (biomedical) journals across 11 years, and has shown that each of the four factors contributes an independent, statistically significant increment to the citation counts. The OA-self-archiving advantage remains a robust, independent factor. Having successfully responded to his challenge, we now challenge Eysenbach to demonstrate -- by testing a sufficiently broad and representative sample of journals at all levels of the journal quality, visibility and prestige hierarchy -- that his finding of a citation advantage for Gold OA (articles published OA on the high-profile website of the only journal he tested (PNAS) over Green OA articles in the same journal (self-archived on the author's website) was not just an artifact of having tested only one very high-profile journal.",
        "published": "2007-01-22T02:14:10Z",
        "link": "http://arxiv.org/abs/cs/0701136v1",
        "categories": [
            "cs.IR",
            "cs.DL"
        ]
    },
    {
        "title": "The Open Access Citation Advantage: Quality Advantage Or Quality Bias?",
        "authors": [
            "Chawki Hajjem",
            "Stevan Harnad"
        ],
        "summary": "Many studies have now reported the positive correlation between Open Access (OA) self-archiving and citation counts (\"OA Advantage,\" OAA). But does this OAA occur because (QB) authors are more likely to self-selectively self-archive articles that are more likely to be cited (self-selection \"Quality Bias\": QB)? or because (QA) articles that are self-archived are more likely to be cited (\"Quality Advantage\": QA)? The probable answer is both. Three studies [by (i) Kurtz and co-workers in astrophysics, (ii) Moed in condensed matter physics, and (iii) Davis & Fromerth in mathematics] had reported the OAA to be due to QB [plus Early Advantage, EA, from self-archiving the preprint before publication, in (i) and (ii)] rather than QA. These three fields, however, (1) have less of a postprint access problem than most other fields and (i) and (ii) also happen to be among the minority of fields that (2) make heavy use of prepublication preprints. Chawki Hajjem has now analyzed preliminary evidence based on over 100,000 articles from multiple fields, comparing self-selected self-archiving with mandated self-archiving to estimate the contributions of QB and QA to the OAA. Both factors contribute, and the contribution of QA is greater.",
        "published": "2007-01-22T02:19:16Z",
        "link": "http://arxiv.org/abs/cs/0701137v1",
        "categories": [
            "cs.IR",
            "cs.DL"
        ]
    },
    {
        "title": "Dirac Notation, Fock Space and Riemann Metric Tensor in Information   Retrieval Models",
        "authors": [
            "Xing M. Wang"
        ],
        "summary": "Using Dirac Notation as a powerful tool, we investigate the three classical Information Retrieval (IR) models and some their extensions. We show that almost all such models can be described by vectors in Occupation Number Representations (ONR) of Fock spaces with various specifications on, e.g., occupation number, inner product or term-term interactions. As important cases of study, Concept Fock Space (CFS) is introduced for Boolean model; the basic formulas for Singular Value Decomposition (SVD) of Latent Semantic Indexing (LSI) Model are manipulated in terms of Dirac notation. And, based on SVD, a Riemannian metric tensor is introduced, which not only can be used to calculate the relevance of documents to a query, but also may be used to measure the closeness of documents in data clustering.",
        "published": "2007-01-23T01:22:59Z",
        "link": "http://arxiv.org/abs/cs/0701143v4",
        "categories": [
            "cs.IR",
            "math-ph",
            "math.MP",
            "H.3.3"
        ]
    },
    {
        "title": "Ontology from Local Hierarchical Structure in Text",
        "authors": [
            "F. Murtagh",
            "J. Mothe",
            "K. Englmeier"
        ],
        "summary": "We study the notion of hierarchy in the context of visualizing textual data and navigating text collections. A formal framework for ``hierarchy'' is given by an ultrametric topology. This provides us with a theoretical foundation for concept hierarchy creation. A major objective is {\\em scalable} annotation or labeling of concept maps. Serendipitously we pursue other objectives such as deriving common word pair (and triplet) phrases, i.e., word 2- and 3-grams. We evaluate our approach using (i) a collection of texts, (ii) a single text subdivided into successive parts (for which we provide an interactive demonstrator), and (iii) a text subdivided at the sentence or line level. While detailing a generic framework, a distinguishing feature of our work is that we focus on {\\em locality} of hierarchic structure in order to extract semantic information.",
        "published": "2007-01-27T18:31:22Z",
        "link": "http://arxiv.org/abs/cs/0701180v1",
        "categories": [
            "cs.IR",
            "H.5; I.5.3; H.5.2; I.7.2; H.3"
        ]
    },
    {
        "title": "Plagiarism Detection in arXiv",
        "authors": [
            "Daria Sorokina",
            "Johannes Gehrke",
            "Simeon Warner",
            "Paul Ginsparg"
        ],
        "summary": "We describe a large-scale application of methods for finding plagiarism in research document collections. The methods are applied to a collection of 284,834 documents collected by arXiv.org over a 14 year period, covering a few different research disciplines. The methodology efficiently detects a variety of problematic author behaviors, and heuristics are developed to reduce the number of false positives. The methods are also efficient enough to implement as a real-time submission screen for a collection many times larger.",
        "published": "2007-02-01T20:52:13Z",
        "link": "http://arxiv.org/abs/cs/0702012v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "The Haar Wavelet Transform of a Dendrogram: Additional Notes",
        "authors": [
            "Fionn Murtagh"
        ],
        "summary": "We consider the wavelet transform of a finite, rooted, node-ranked, $p$-way tree, focusing on the case of binary ($p = 2$) trees. We study a Haar wavelet transform on this tree. Wavelet transforms allow for multiresolution analysis through translation and dilation of a wavelet function. We explore how this works in our tree context.",
        "published": "2007-02-10T21:26:05Z",
        "link": "http://arxiv.org/abs/cs/0702067v1",
        "categories": [
            "cs.IR",
            "I.5.3; H.3.1; I.1.m; I.7.m"
        ]
    },
    {
        "title": "Social Behaviours Applied to P2P Systems: An efficient Algorithm for   Resource Organisation",
        "authors": [
            "V. Carchiolo",
            "M. Malgeri",
            "G. Mangioni",
            "V. Nicosia"
        ],
        "summary": "P2P systems are a great solution to the problem of distributing resources. The main issue of P2P networks is that searching and retrieving resources shared by peers is usually expensive and does not take into account similarities among peers. In this paper we present preliminary simulations of PROSA, a novel algorithm for P2P network structuring, inspired by social behaviours. Peers in PROSA self--organise in social groups of similar peers, called ``semantic--groups'', depending on the resources they are sharing. Such a network smoothly evolves to a small--world graph, where queries for resources are efficiently and effectively routed.",
        "published": "2007-02-14T11:53:14Z",
        "link": "http://arxiv.org/abs/cs/0702085v1",
        "categories": [
            "cs.DC",
            "cs.IR"
        ]
    },
    {
        "title": "Wild, Wild Wikis: A way forward",
        "authors": [
            "Charles Robert",
            "Ranmi Adigun"
        ],
        "summary": "Wikis can be considered as public domain knowledge sharing system. They provide opportunity for those who may not have the privilege to publish their thoughts through the traditional methods. They are one of the fastest growing systems of online encyclopaedia. In this study, we consider the importance of wikis as a way of creating, sharing and improving public knowledge. We identify some of the problems associated with wikis to include, (a) identification of the identities of information and its creator (b) accuracy of information (c) justification of the credibility of authors (d) vandalism of quality of information (e) weak control over the contents. A solution to some of these problems is sought through the use of an annotation model. The model assumes that contributions in wikis can be seen as annotation to the initial document. It proposed a systematic control of contributors and contributions to the initiative and the keeping of records of what existed and what was done to initial documents. We believe that with this model, analysis can be done on the progress of wiki initiatives. We assumed that using this model, wikis can be better used for creation and sharing of knowledge for public use.",
        "published": "2007-02-19T15:12:43Z",
        "link": "http://arxiv.org/abs/cs/0702106v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "AMIEDoT: An annotation model for document tracking and recommendation   service",
        "authors": [
            "Charles A. Robert"
        ],
        "summary": "The primary objective of document annotation in whatever form, manual or electronic is to allow those who may not have control to original document to provide personal view on information source. Beyond providing personal assessment to original information sources, we are looking at a situation where annotation made can be used as additional source of information for document tracking and recommendation service. Most of the annotation tools existing today were conceived for their independent use with no reference to the creator of the annotation. We propose AMIEDoT (Annotation Model for Information Exchange and Document Tracking) an annotation model that can assist in document tracking and recommendation service. The model is based on three parameters in the acts of annotation. We believe that introducing document parameters, time and the parameters of the creator of annotation into an annotation process can be a dependable source to know, who used a document, when a document was used and for what a document was used for. Beyond document tracking, our model can be used in not only for selective dissemination of information but for recommendation services. AMIEDoT can also be used for information sharing and information reuse.",
        "published": "2007-02-19T15:18:26Z",
        "link": "http://arxiv.org/abs/cs/0702107v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "AMIE: An annotation model for information research",
        "authors": [
            "Charles A. Robert",
            "David Amos"
        ],
        "summary": "The objective of most users for consulting any information database, information warehouse or the internet is to resolve one problem or the other. Available online or offline annotation tools were not conceived with the objective of assisting users in their bid to resolve a decisional problem. Apart from the objective and usage of annotation tools, how these tools are conceived and classified has implication on their usage. Several criteria have been used to categorize annotation concepts. Typically annotation are conceived based on how it affect the organization of document been considered for annotation or the organization of the resulting annotation. Our approach is annotation that will assist in information research for decision making. Annotation model for information exchange (AMIE) was conceived with the objective of information sharing and reuse.",
        "published": "2007-02-19T15:26:05Z",
        "link": "http://arxiv.org/abs/cs/0702109v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Efficient Searching and Retrieval of Documents in PROSA",
        "authors": [
            "V. Nicosia",
            "G. Mangioni",
            "V. Carchiolo",
            "M. Malgeri"
        ],
        "summary": "Retrieving resources in a distributed environment is more difficult than finding data in centralised databases. In the last decade P2P system arise as new and effective distributed architectures for resource sharing, but searching in such environments could be difficult and time-consuming. In this paper we discuss efficiency of resource discovery in PROSA, a self-organising P2P system heavily inspired by social networks. All routing choices in PROSA are made locally, looking only at the relevance of the next peer to each query. We show that PROSA is able to effectively answer queries for rare documents, forwarding them through the most convenient path to nodes that much probably share matching resources. This result is heavily related to the small-world structure that naturally emerges in PROSA.",
        "published": "2007-02-22T10:22:39Z",
        "link": "http://arxiv.org/abs/cs/0702126v1",
        "categories": [
            "cs.DC",
            "cs.IR"
        ]
    },
    {
        "title": "Exploiting social networks dynamics for P2P resource organisation",
        "authors": [
            "V. Nicosia",
            "G. Mangioni",
            "V. Carchiolo",
            "M. Malgeri"
        ],
        "summary": "In this paper we present a formal description of PROSA, a P2P resource management system heavily inspired by social networks. Social networks have been deeply studied in the last two decades in order to understand how communities of people arise and grow. It is a widely known result that networks of social relationships usually evolves to small-worlds, i.e. networks where nodes are strongly connected to neighbours and separated from all other nodes by a small amount of hops. This work shows that algorithms implemented into PROSA allow to obtain an efficient small-world P2P network.",
        "published": "2007-02-22T10:35:50Z",
        "link": "http://arxiv.org/abs/cs/0702127v1",
        "categories": [
            "cs.DC",
            "cs.IR"
        ]
    },
    {
        "title": "Interroger un corpus par le sens",
        "authors": [
            "Bernard Jacquemin"
        ],
        "summary": "In textual knowledge management, statistical methods prevail. Nonetheless, some difficulties cannot be overcome by these methodologies. I propose a symbolic approach using a complete textual analysis to identify which analysis level can improve the the answers provided by a system. The approach identifies word senses and relation between words and generates as many rephrasings as possible. Using synonyms and derivative, the system provides new utterances without changing the original meaning of the sentences. Such a way, an information can be retrieved whatever the question or answer's wording may be.",
        "published": "2007-03-06T19:50:37Z",
        "link": "http://arxiv.org/abs/cs/0703027v2",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "Time Warp Edit Distance with Stiffness Adjustment for Time Series   Matching",
        "authors": [
            "Pierre-François Marteau"
        ],
        "summary": "In a way similar to the string-to-string correction problem we address time series similarity in the light of a time-series-to-time-series-correction problem for which the similarity between two time series is measured as the minimum cost sequence of \"edit operations\" needed to transform one time series into another. To define the \"edit operations\" we use the paradigm of a graphical editing process and end up with a dynamic programming algorithm that we call Time Warp Edit Distance (TWED). TWED is slightly different in form from Dynamic Time Warping, Longest Common Subsequence or Edit Distance with Real Penalty algorithms. In particular, it highlights a parameter which drives a kind of stiffness of the elastic measure along the time axis. We show that the similarity provided by TWED is a metric potentially useful in time series retrieval applications since it could benefit from the triangular inequality property to speed up the retrieval process while tuning the parameters of the elastic measure. In that context, a lower bound is derived to relate the matching of time series into down sampled representation spaces to the matching into the original space. Empiric quality of the TWED distance is evaluated on a simple classification task. Compared to Edit Distance, Dynamic Time Warping, Longest Common Subsequnce and Edit Distance with Real Penalty, TWED has proven to be quite effective on the considered experimental task.",
        "published": "2007-03-07T19:54:33Z",
        "link": "http://arxiv.org/abs/cs/0703033v5",
        "categories": [
            "cs.IR",
            "I.5"
        ]
    },
    {
        "title": "Recommender System for Online Dating Service",
        "authors": [
            "Lukas Brozovsky",
            "Vaclav Petricek"
        ],
        "summary": "Users of online dating sites are facing information overload that requires them to manually construct queries and browse huge amount of matching user profiles. This becomes even more problematic for multimedia profiles. Although matchmaking is frequently cited as a typical application for recommender systems, there is a surprising lack of work published in this area. In this paper we describe a recommender system we implemented and perform a quantitative comparison of two collaborative filtering (CF) and two global algorithms. Results show that collaborative filtering recommenders significantly outperform global algorithms that are currently used by dating sites. A blind experiment with real users also confirmed that users prefer CF based recommendations to global popularity recommendations. Recommender systems show a great potential for online dating where they could improve the value of the service to users and improve monetization of the service.",
        "published": "2007-03-09T15:38:27Z",
        "link": "http://arxiv.org/abs/cs/0703042v1",
        "categories": [
            "cs.IR",
            "cs.SE",
            "H.3.3"
        ]
    },
    {
        "title": "Copula Component Analysis",
        "authors": [
            "Jian Ma",
            "Zengqi Sun"
        ],
        "summary": "A framework named Copula Component Analysis (CCA) for blind source separation is proposed as a generalization of Independent Component Analysis (ICA). It differs from ICA which assumes independence of sources that the underlying components may be dependent with certain structure which is represented by Copula. By incorporating dependency structure, much accurate estimation can be made in principle in the case that the assumption of independence is invalidated. A two phrase inference method is introduced for CCA which is based on the notion of multidimensional ICA.",
        "published": "2007-03-20T14:52:52Z",
        "link": "http://arxiv.org/abs/cs/0703095v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "A Note on Approximate Nearest Neighbor Methods",
        "authors": [
            "Thomas M. Breuel"
        ],
        "summary": "A number of authors have described randomized algorithms for solving the epsilon-approximate nearest neighbor problem. In this note I point out that the epsilon-approximate nearest neighbor property often fails to be a useful approximation property, since epsilon-approximate solutions fail to satisfy the necessary preconditions for using nearest neighbors for classification and related tasks.",
        "published": "2007-03-21T20:47:33Z",
        "link": "http://arxiv.org/abs/cs/0703101v1",
        "categories": [
            "cs.IR",
            "cs.CC",
            "cs.CV"
        ]
    },
    {
        "title": "Open Access Scientometrics and the UK Research Assessment Exercise",
        "authors": [
            "Stevan Harnad"
        ],
        "summary": "Scientometric predictors of research performance need to be validated by showing that they have a high correlation with the external criterion they are trying to predict. The UK Research Assessment Exercise (RAE), together with the growing movement toward making the full-texts of research articles freely available on the web -- offer a unique opportunity to test and validate a wealth of old and new scientometric predictors, through multiple regression analysis: Publications, journal impact factors, citations, co-citations, citation chronometrics (age, growth, latency to peak, decay rate), hub/authority scores, h-index, prior funding, student counts, co-authorship scores, endogamy/exogamy, textual proximity, download/co-downloads and their chronometrics, etc. can all be tested and validated jointly, discipline by discipline, against their RAE panel rankings in the forthcoming parallel panel-based and metric RAE in 2008. The weights of each predictor can be calibrated to maximize the joint correlation with the rankings. Open Access Scientometrics will provide powerful new means of navigating, evaluating, predicting and analyzing the growing Open Access database, as well as powerful incentives for making it grow faster. ~",
        "published": "2007-03-26T15:40:08Z",
        "link": "http://arxiv.org/abs/cs/0703131v1",
        "categories": [
            "cs.IR",
            "cs.DL"
        ]
    },
    {
        "title": "Novelty and Collective Attention",
        "authors": [
            "Fang Wu",
            "Bernardo A. Huberman"
        ],
        "summary": "The subject of collective attention is central to an information age where millions of people are inundated with daily messages. It is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. We have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. The observations can be described by a dynamical model characterized by a single novelty factor. Our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.",
        "published": "2007-04-09T22:02:29Z",
        "link": "http://arxiv.org/abs/0704.1158v1",
        "categories": [
            "cs.CY",
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Personalizing Image Search Results on Flickr",
        "authors": [
            "Kristina Lerman",
            "Anon Plangprasopchok",
            "Chio Wong"
        ],
        "summary": "The social media site Flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. Flickr offers multiple ways of browsing or searching it. One option is tag search, which returns all images tagged with a specific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. We claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. We show how to exploit this metadata to personalize search results for the user, thereby improving search performance. First, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. Secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. The users' interests can similarly be described by the tags they used for annotating their images. The latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.",
        "published": "2007-04-12T23:31:04Z",
        "link": "http://arxiv.org/abs/0704.1676v1",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CY",
            "cs.DL",
            "cs.HC"
        ]
    },
    {
        "title": "Using Access Data for Paper Recommendations on ArXiv.org",
        "authors": [
            "Stefan Pohl"
        ],
        "summary": "This thesis investigates in the use of access log data as a source of information for identifying related scientific papers. This is done for arXiv.org, the authority for publication of e-prints in several fields of physics.   Compared to citation information, access logs have the advantage of being immediately available, without manual or automatic extraction of the citation graph. Because of that, a main focus is on the question, how far user behavior can serve as a replacement for explicit meta-data, which potentially might be expensive or completely unavailable. Therefore, we compare access, content, and citation-based measures of relatedness on different recommendation tasks. As a final result, an online recommendation system has been built that can help scientists to find further relevant literature, without having to search for them actively.",
        "published": "2007-04-23T15:52:47Z",
        "link": "http://arxiv.org/abs/0704.2963v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Recommending Related Papers Based on Digital Library Access Records",
        "authors": [
            "Stefan Pohl",
            "Filip Radlinski",
            "Thorsten Joachims"
        ],
        "summary": "An important goal for digital libraries is to enable researchers to more easily explore related work. While citation data is often used as an indicator of relatedness, in this paper we demonstrate that digital access records (e.g. http-server logs) can be used as indicators as well. In particular, we show that measures based on co-access provide better coverage than co-citation, that they are available much sooner, and that they are more accurate for recent papers.",
        "published": "2007-04-23T16:51:40Z",
        "link": "http://arxiv.org/abs/0704.2902v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.7; H.3.3"
        ]
    },
    {
        "title": "Vocabulary growth in collaborative tagging systems",
        "authors": [
            "Ciro Cattuto",
            "Andrea Baldassarri",
            "Vito D. P. Servedio",
            "Vittorio Loreto"
        ],
        "summary": "We analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. We study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. In both cases, we find power-law behaviors with exponents smaller than one. Surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. Similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.",
        "published": "2007-04-25T07:47:40Z",
        "link": "http://arxiv.org/abs/0704.3316v1",
        "categories": [
            "cs.IR",
            "cond-mat.stat-mech",
            "cs.CY",
            "physics.data-an",
            "H.3.4; H.3.1"
        ]
    },
    {
        "title": "Direct Optimization of Ranking Measures",
        "authors": [
            "Quoc Le",
            "Alexander Smola"
        ],
        "summary": "Web page ranking and collaborative filtering require the optimization of sophisticated performance measures. Current Support Vector approaches are unable to optimize them directly and focus on pairwise comparisons instead. We present a new approach which allows direct optimization of the relevant loss functions. This is achieved via structured estimation in Hilbert spaces. It is most related to Max-Margin-Markov networks optimization of multivariate performance measures. Key to our approach is that during training the ranking problem can be viewed as a linear assignment problem, which can be solved by the Hungarian Marriage algorithm. At test time, a sort operation is sufficient, as our algorithm assigns a relevance score to every (document, query) pair. Experiments show that the our algorithm is fast and that it works very well.",
        "published": "2007-04-25T12:36:55Z",
        "link": "http://arxiv.org/abs/0704.3359v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "Rough Sets Computations to Impute Missing Data",
        "authors": [
            "Fulufhelo Vincent Nelwamondo",
            "Tshilidzi Marwala"
        ],
        "summary": "Many techniques for handling missing data have been proposed in the literature. Most of these techniques are overly complex. This paper explores an imputation technique based on rough set computations. In this paper, characteristic relations are introduced to describe incompletely specified decision tables.It is shown that the basic rough set idea of lower and upper approximations for incompletely specified decision tables may be defined in a variety of different ways. Empirical results obtained using real data are given and they provide a valuable and promising insight to the problem of missing data. Missing data were predicted with an accuracy of up to 99%.",
        "published": "2007-04-26T22:22:45Z",
        "link": "http://arxiv.org/abs/0704.3635v1",
        "categories": [
            "cs.CV",
            "cs.IR"
        ]
    },
    {
        "title": "Approximate textual retrieval",
        "authors": [
            "Pere Constans"
        ],
        "summary": "An approximate textual retrieval algorithm for searching sources with high levels of defects is presented. It considers splitting the words in a query into two overlapping segments and subsequently building composite regular expressions from interlacing subsets of the segments. This procedure reduces the probability of missed occurrences due to source defects, yet diminishes the retrieval of irrelevant, non-contextual occurrences.",
        "published": "2007-05-05T17:27:42Z",
        "link": "http://arxiv.org/abs/0705.0751v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.3; I.2.8"
        ]
    },
    {
        "title": "IDF revisited: A simple new derivation within the Robertson-Spärck   Jones probabilistic model",
        "authors": [
            "Lillian Lee"
        ],
        "summary": "There have been a number of prior attempts to theoretically justify the effectiveness of the inverse document frequency (IDF). Those that take as their starting point Robertson and Sparck Jones's probabilistic model are based on strong or complex assumptions. We show that a more intuitively plausible assumption suffices. Moreover, the new assumption, while conceptually very simple, provides a solution to an estimation problem that had been deemed intractable by Robertson and Walker (1997).",
        "published": "2007-05-08T20:08:13Z",
        "link": "http://arxiv.org/abs/0705.1161v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3"
        ]
    },
    {
        "title": "Ontology-Supported and Ontology-Driven Conceptual Navigation on the   World Wide Web",
        "authors": [
            "Michel Crampes",
            "Sylvie Ranwez"
        ],
        "summary": "This paper presents the principles of ontology-supported and ontology-driven conceptual navigation. Conceptual navigation realizes the independence between resources and links to facilitate interoperability and reusability. An engine builds dynamic links, assembles resources under an argumentative scheme and allows optimization with a possible constraint, such as the user's available time. Among several strategies, two are discussed in detail with examples of applications. On the one hand, conceptual specifications for linking and assembling are embedded in the resource meta-description with the support of the ontology of the domain to facilitate meta-communication. Resources are like agents looking for conceptual acquaintances with intention. On the other hand, the domain ontology and an argumentative ontology drive the linking and assembling strategies.",
        "published": "2007-05-14T08:19:28Z",
        "link": "http://arxiv.org/abs/0705.1886v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Scientific citations in Wikipedia",
        "authors": [
            "Finn Aarup Nielsen"
        ],
        "summary": "The Internet-based encyclopaedia Wikipedia has grown to become one of the most visited web-sites on the Internet. However, critics have questioned the quality of entries, and an empirical study has shown Wikipedia to contain errors in a 2005 sample of science entries. Biased coverage and lack of sources are among the \"Wikipedia risks\". The present work describes a simple assessment of these aspects by examining the outbound links from Wikipedia articles to articles in scientific journals with a comparison against journal statistics from Journal Citation Reports such as impact factors. The results show an increasing use of structured citation markup and good agreement with the citation pattern seen in the scientific literature though with a slight tendency to cite articles in high-impact journals such as Nature and Science. These results increase confidence in Wikipedia as an good information organizer for science in general.",
        "published": "2007-05-15T09:42:30Z",
        "link": "http://arxiv.org/abs/0705.2106v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.7; H.3.5; H.3.1"
        ]
    },
    {
        "title": "Dynamic User-Defined Similarity Searching in Semi-Structured Text   Retrieval",
        "authors": [
            "Filippo Geraci",
            "Marco Pellegrini"
        ],
        "summary": "Modern text retrieval systems often provide a similarity search utility, that allows the user to find efficiently a fixed number k of documents in the data set that are most similar to a given query (here a query is either a simple sequence of keywords or the identifier of a full document found in previous searches that is considered of interest). We consider the case of a textual database made of semi-structured documents. Each field, in turns, is modelled with a specific vector space. The problem is more complex when we also allow each such vector space to have an associated user-defined dynamic weight that influences its contribution to the overall dynamic aggregated and weighted similarity. This dynamic problem has been tackled in a recent paper by Singitham et al. in in VLDB 2004. Their proposed solution, which we take as baseline, is a variant of the cluster-pruning technique that has the potential for scaling to very large corpora of documents, and is far more efficient than the naive exhaustive search. We devise an alternative way of embedding weights in the data structure, coupled with a non-trivial application of a clustering algorithm based on the furthest point first heuristic for the metric k-center problem. The validity of our approach is demonstrated experimentally by showing significant performance improvements over the scheme proposed in Singitham et al. in VLDB 2004. We improve significantly tradeoffs between query time and output quality with respect to the baseline method in Singitham et al. in in VLDB 2004, and also with respect to a novel method by Chierichetti et al. to appear in ACM PODS 2007. We also speed up the pre-processing time by a factor at least thirty.",
        "published": "2007-05-31T13:46:39Z",
        "link": "http://arxiv.org/abs/0705.4606v1",
        "categories": [
            "cs.IR",
            "cs.DS",
            "H.3.3"
        ]
    },
    {
        "title": "Collaborative product and process model: Multiple Viewpoints approach",
        "authors": [
            "Hichem Geryville",
            "Abdelaziz Bouras",
            "Yacine Ouzrout",
            "Nikolaos Sapidis"
        ],
        "summary": "The design and development of complex products invariably involves many actors who have different points of view on the problem they are addressing, the product being developed, and the process by which it is being developed. The actors' viewpoints approach was designed to provide an organisational framework in which these different perspectives or points of views, and their relationships, could be explicitly gathered and formatted (by actor activity's focus). The approach acknowledges the inevitability of multiple interpretation of product information as different views, promotes gathering of actors' interests, and encourages retrieved adequate information while providing support for integration through PLM and/or SCM collaboration. In this paper, we present our multiple viewpoints approach, and we illustrate it by an industrial example on cyclone vessel product.",
        "published": "2007-06-08T13:09:13Z",
        "link": "http://arxiv.org/abs/0706.1179v1",
        "categories": [
            "cs.OH",
            "cs.IR"
        ]
    },
    {
        "title": "Extraction d'entités dans des collections évolutives",
        "authors": [
            "Thierry Despeyroux",
            "Eduardo Fraschini",
            "Anne-Marie Vercoustre"
        ],
        "summary": "The goal of our work is to use a set of reports and extract named entities, in our case the names of Industrial or Academic partners. Starting with an initial list of entities, we use a first set of documents to identify syntactic patterns that are then validated in a supervised learning phase on a set of annotated documents. The complete collection is then explored. This approach is similar to the ones used in data extraction from semi-structured documents (wrappers) and do not need any linguistic resources neither a large set for training. As our collection of documents would evolve over years, we hope that the performance of the extraction would improve with the increased size of the training set.",
        "published": "2007-06-19T14:16:48Z",
        "link": "http://arxiv.org/abs/0706.2797v3",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Delayed Correlations in Inter-Domain Network Traffic",
        "authors": [
            "Viktoria Rojkova",
            "Mehmed Kantardzic"
        ],
        "summary": "To observe the evolution of network traffic correlations we analyze the eigenvalue spectra and eigenvectors statistics of delayed correlation matrices of network traffic counts time series. Delayed correlation matrix D is composed of the correlations between one variable in the multivariable time series and another at a time delay \\tau . Inverse participation ratio (IPR) of eigenvectors of D deviates substantially from the IPR of eigenvectors of the equal time correlation matrix C. We relate this finding to the localization and discuss its importance for network congestion control. The time-lagged correlation pattern between network time series is preserved over a long time, up to 100\\tau, where \\tau=300 sec. The largest eigenvalue \\lambda_{max} of D and the corresponding IPR oscillate with two characteristic periods of 3\\tau and 6\\tau . The existence of delayed correlations between network time series fits well into the long range dependence (LRD) property of the network traffic.   The ability to monitor and control the long memory processes is crucial since they impact the network performance. Injecting the random traffic counts between non-randomly correlated time series, we were able to break the picture of periodicity of \\lambda_{max}. In addition, we investigated influence of the periodic injections on both largest eigenvalue and the IPR, and addressed relevance of these indicators for the LRD and self-similarity of the network traffic.",
        "published": "2007-07-07T19:01:51Z",
        "link": "http://arxiv.org/abs/0707.1083v1",
        "categories": [
            "cs.NI",
            "cs.IR"
        ]
    },
    {
        "title": "Practical Approach to Knowledge-based Question Answering with Natural   Language Understanding and Advanced Reasoning",
        "authors": [
            "Wilson Wong"
        ],
        "summary": "This research hypothesized that a practical approach in the form of a solution framework known as Natural Language Understanding and Reasoning for Intelligence (NaLURI), which combines full-discourse natural language understanding, powerful representation formalism capable of exploiting ontological information and reasoning approach with advanced features, will solve the following problems without compromising practicality factors: 1) restriction on the nature of question and response, and 2) limitation to scale across domains and to real-life natural language text.",
        "published": "2007-07-24T14:30:27Z",
        "link": "http://arxiv.org/abs/0707.3559v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.IR",
            "I.2.7; H.5.2; H.3.4; H.3.3"
        ]
    },
    {
        "title": "An exploratory study of Google Scholar",
        "authors": [
            "Philipp Mayr",
            "Anne-Kathrin Walter"
        ],
        "summary": "The paper discusses and analyzes the scientific search service Google Scholar (GS). The focus is on an exploratory study which investigates the coverage of scientific serials in GS. The study shows deficiencies in the coverage and up-to-dateness of the GS index. Furthermore, the study points up which Web servers are the most important data providers for this search service and which information sources are highly represented. We can show that there is a relatively large gap in Google Scholars coverage of German literature as well as weaknesses in the accessibility of Open Access content.   Keywords: Search engines, Digital libraries, Worldwide Web, Serials, Electronic journals",
        "published": "2007-07-24T19:19:36Z",
        "link": "http://arxiv.org/abs/0707.3575v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Reconstruction of Protein-Protein Interaction Pathways by Mining   Subject-Verb-Objects Intermediates",
        "authors": [
            "Maurice HT Ling",
            "Christophe Lefevre",
            "Kevin R. Nicholas",
            "Feng Lin"
        ],
        "summary": "The exponential increase in publication rate of new articles is limiting access of researchers to relevant literature. This has prompted the use of text mining tools to extract key biological information. Previous studies have reported extensive modification of existing generic text processors to process biological text. However, this requirement for modification had not been examined. In this study, we have constructed Muscorian, using MontyLingua, a generic text processor. It uses a two-layered generalization-specialization paradigm previously proposed where text was generically processed to a suitable intermediate format before domain-specific data extraction techniques are applied at the specialization layer. Evaluation using a corpus and experts indicated 86-90% precision and approximately 30% recall in extracting protein-protein interactions, which was comparable to previous studies using either specialized biological text processing tools or modified existing tools. Our study had also demonstrated the flexibility of the two-layered generalization-specialization paradigm by using the same generalization layer for two specialized information extraction tasks.",
        "published": "2007-08-06T01:22:46Z",
        "link": "http://arxiv.org/abs/0708.0694v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "cs.DL"
        ]
    },
    {
        "title": "Characterising Web Site Link Structure",
        "authors": [
            "Shi Zhou",
            "Ingemar Cox",
            "Vaclav Petricek"
        ],
        "summary": "The topological structures of the Internet and the Web have received considerable attention. However, there has been little research on the topological properties of individual web sites. In this paper, we consider whether web sites (as opposed to the entire Web) exhibit structural similarities. To do so, we exhaustively crawled 18 web sites as diverse as governmental departments, commercial companies and university departments in different countries. These web sites consisted of as little as a few thousand pages to millions of pages. Statistical analysis of these 18 sites revealed that the internal link structure of the web sites are significantly different when measured with first and second-order topological properties, i.e. properties based on the connectivity of an individual or a pairs of nodes. However, examination of a third-order topological property that consider the connectivity between three nodes that form a triangle, revealed a strong correspondence across web sites, suggestive of an invariant. Comparison with the Web, the AS Internet, and a citation network, showed that this third-order property is not shared across other types of networks. Nor is the property exhibited in generative network models such as that of Barabasi and Albert.",
        "published": "2007-08-06T11:00:58Z",
        "link": "http://arxiv.org/abs/0708.0741v1",
        "categories": [
            "cs.IR",
            "H.5.4; C.2.1; D.2.8"
        ]
    },
    {
        "title": "Integrating users' needs into multimedia information retrieval system",
        "authors": [
            "Hanène Maghrebi",
            "Amos David"
        ],
        "summary": "The exponential growth of multimedia information and the development of various communication media generated new problems at various levels including the rate of flow of information, problems of storage and management. The difficulty which arises is no longer the existence of information but rather the access to this information. When designing multimedia information retrieval system, it is appropriate to bear in mind the potential users and their information needs. We assumed that multimedia information representation which takes into account explicitly the users' needs and the cases of use could contribute to the adaptation potentials of the system for the end-users. We believe also that responses of multimedia information system would be more relevant to the users' needs if the types of results to be used from the system were identified before the design and development of the system. We propose the integration of the users' information needs. More precisely integrating usage contexts of resulting information in an information system (during creation and feedback) should enhance more pertinent users' need. The first section of this study is dedicated to traditional multimedia information systems and specifically the approaches of representing multimedia information. Taking into account the dynamism of users, these approaches do not permit the explicit integration of the users' information needs. In this paper, we will present our proposals based on economic intelligence approach. This approach emphasizes the importance of starting any process of information retrieval witch the user information need.",
        "published": "2007-08-21T09:04:49Z",
        "link": "http://arxiv.org/abs/0708.2788v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Fast evaluation of union-intersection expressions",
        "authors": [
            "Philip Bille",
            "Anna Pagh",
            "Rasmus Pagh"
        ],
        "summary": "We show how to represent sets in a linear space data structure such that expressions involving unions and intersections of sets can be computed in a worst-case efficient way. This problem has applications in e.g. information retrieval and database systems. We mainly consider the RAM model of computation, and sets of machine words, but also state our results in the I/O model. On a RAM with word size $w$, a special case of our result is that the intersection of $m$ (preprocessed) sets, containing $n$ elements in total, can be computed in expected time $O(n (\\log w)^2 / w + km)$, where $k$ is the number of elements in the intersection. If the first of the two terms dominates, this is a factor $w^{1-o(1)}$ faster than the standard solution of merging sorted lists. We show a cell probe lower bound of time $\\Omega(n/(w m \\log m)+ (1-\\tfrac{\\log k}{w}) k)$, meaning that our upper bound is nearly optimal for small $m$. Our algorithm uses a novel combination of approximate set representations and word-level parallelism.",
        "published": "2007-08-23T22:23:04Z",
        "link": "http://arxiv.org/abs/0708.3259v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "On the complexity of nonnegative matrix factorization",
        "authors": [
            "Stephen A. Vavasis"
        ],
        "summary": "Nonnegative matrix factorization (NMF) has become a prominent technique for the analysis of image databases, text databases and other information retrieval and clustering applications. In this report, we define an exact version of NMF. Then we establish several results about exact NMF: (1) that it is equivalent to a problem in polyhedral combinatorics; (2) that it is NP-hard; and (3) that a polynomial-time local search heuristic exists.",
        "published": "2007-08-30T13:01:33Z",
        "link": "http://arxiv.org/abs/0708.4149v2",
        "categories": [
            "cs.NA",
            "cs.IR",
            "G.1.3; H.3.3"
        ]
    },
    {
        "title": "When are recommender systems useful?",
        "authors": [
            "Marcel Blattner",
            "Alexander Hunziker",
            "Paolo Laureti"
        ],
        "summary": "Recommender systems are crucial tools to overcome the information overload brought about by the Internet. Rigorous tests are needed to establish to what extent sophisticated methods can improve the quality of the predictions. Here we analyse a refined correlation-based collaborative filtering algorithm and compare it with a novel spectral method for recommending. We test them on two databases that bear different statistical properties (MovieLens and Jester) without filtering out the less active users and ordering the opinions in time, whenever possible. We find that, when the distribution of user-user correlations is narrow, simple averages work nearly as well as advanced methods. Recommender systems can, on the other hand, exploit a great deal of additional information in systems where external influence is negligible and peoples' tastes emerge entirely. These findings are validated by simulations with artificially generated data.",
        "published": "2007-09-17T09:27:07Z",
        "link": "http://arxiv.org/abs/0709.2562v1",
        "categories": [
            "cs.IR",
            "cs.CY",
            "cs.DL",
            "cs.DS",
            "physics.data-an",
            "physics.soc-ph"
        ]
    },
    {
        "title": "The Extended Edit Distance Metric",
        "authors": [
            "Muhammad Marwan Muhammad Fuad",
            "Pierre-François Marteau"
        ],
        "summary": "Similarity search is an important problem in information retrieval. This similarity is based on a distance. Symbolic representation of time series has attracted many researchers recently, since it reduces the dimensionality of these high dimensional data objects. We propose a new distance metric that is applied to symbolic data objects and we test it on time series data bases in a classification task. We compare it to other distances that are well known in the literature for symbolic data objects. We also prove, mathematically, that our distance is metric.",
        "published": "2007-09-28T18:45:48Z",
        "link": "http://arxiv.org/abs/0709.4669v1",
        "categories": [
            "cs.IR",
            "H.3"
        ]
    },
    {
        "title": "Evaluation experiments on related terms search in Wikipedia: Information   Content and Adapted HITS (In Russian)",
        "authors": [
            "A. A. Krizhanovsky"
        ],
        "summary": "The classification of metrics and algorithms search for related terms via WordNet, Roget's Thesaurus, and Wikipedia was extended to include adapted HITS algorithm. Evaluation experiments on Information Content and adapted HITS algorithm are described. The test collection of Russian word pairs with human-assigned similarity judgments is proposed.   -----   Klassifikacija metrik i algoritmov poiska semanticheski blizkih slov v tezaurusah WordNet, Rozhe i jenciklopedii Vikipedija rasshirena adaptirovannym HITS algoritmom. S pomow'ju jeksperimentov v Vikipedii oceneny metrika Information Content i adaptirovannyj algoritm HITS. Predlozhen resurs dlja ocenki semanticheskoj blizosti russkih slov.",
        "published": "2007-10-01T16:04:52Z",
        "link": "http://arxiv.org/abs/0710.0169v6",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.1; H.3.3; H.4.3; G.2.2"
        ]
    },
    {
        "title": "Performance Comparison of Persistence Frameworks",
        "authors": [
            "Sabu M. Thampi",
            "Ashwin a K"
        ],
        "summary": "One of the essential and most complex components in the software development process is the database. The complexity increases when the \"orientation\" of the interacting components differs. A persistence framework moves the program data in its most natural form to and from a permanent data store, the database. Thus a persistence framework manages the database and the mapping between the database and the objects. This paper compares the performance of two persistence frameworks ? Hibernate and iBatis?s SQLMaps using a banking database. The performance of both of these tools in single and multi-user environments are evaluated.",
        "published": "2007-10-07T08:22:53Z",
        "link": "http://arxiv.org/abs/0710.1404v1",
        "categories": [
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "Efficient Optimally Lazy Algorithms for Minimal-Interval Semantics",
        "authors": [
            "Sebastiano Vigna",
            "Paolo Boldi"
        ],
        "summary": "Minimal-interval semantics associates with each query over a document a set of intervals, called witnesses, that are incomparable with respect to inclusion (i.e., they form an antichain): witnesses define the minimal regions of the document satisfying the query. Minimal-interval semantics makes it easy to define and compute several sophisticated proximity operators, provides snippets for user presentation, and can be used to rank documents. In this paper we provide algorithms for computing conjunction and disjunction that are linear in the number of intervals and logarithmic in the number of operands; for additional operators, such as ordered conjunction and Brouwerian difference, we provide linear algorithms. In all cases, space is linear in the number of operands. More importantly, we define a formal notion of optimal laziness, and either prove it, or prove its impossibility, for each algorithm. We cast our results in a general framework of antichains of intervals on total orders, making our algorithms directly applicable to other domains.",
        "published": "2007-10-08T12:15:48Z",
        "link": "http://arxiv.org/abs/0710.1525v2",
        "categories": [
            "cs.DS",
            "cs.IR",
            "F.2.m; H.3.3"
        ]
    },
    {
        "title": "Stanford Matrix Considered Harmful",
        "authors": [
            "Sebastiano Vigna"
        ],
        "summary": "This note argues about the validity of web-graph data used in the literature.",
        "published": "2007-10-10T10:03:03Z",
        "link": "http://arxiv.org/abs/0710.1962v1",
        "categories": [
            "cs.IR",
            "H.3.0"
        ]
    },
    {
        "title": "Recommendation model based on opinion diffusion",
        "authors": [
            "Yi-Cheng Zhang",
            "Matus Medo",
            "Jie Ren",
            "Tao Zhou",
            "Tao Li",
            "Fan Yang"
        ],
        "summary": "Information overload in the modern society calls for highly efficient recommendation algorithms. In this letter we present a novel diffusion based recommendation model, with users' ratings built into a transition matrix. To speed up computation we introduce a Green function method. The numerical tests on a benchmark database show that our prediction is superior to the standard recommendation methods.",
        "published": "2007-10-11T12:54:07Z",
        "link": "http://arxiv.org/abs/0710.2228v2",
        "categories": [
            "physics.soc-ph",
            "cs.CY",
            "cs.IR",
            "physics.data-an"
        ]
    },
    {
        "title": "An efficient reduction of ranking to classification",
        "authors": [
            "Nir Ailon",
            "Mehryar Mohri"
        ],
        "summary": "This paper describes an efficient reduction of the learning problem of ranking to binary classification. The reduction guarantees an average pairwise misranking regret of at most that of the binary classifier regret, improving a recent result of Balcan et al which only guarantees a factor of 2. Moreover, our reduction applies to a broader class of ranking loss functions, admits a simpler proof, and the expected running time complexity of our algorithm in terms of number of calls to a classifier or preference function is improved from $\\Omega(n^2)$ to $O(n \\log n)$. In addition, when the top $k$ ranked elements only are required ($k \\ll n$), as in many applications in information extraction or search engines, the time complexity of our algorithm can be further reduced to $O(k \\log k + n)$. Our reduction and algorithm are thus practical for realistic applications where the number of points to rank exceeds several thousands. Much of our results also extend beyond the bipartite case previously studied.   Our rediction is a randomized one. To complement our result, we also derive lower bounds on any deterministic reduction from binary (preference) classification to ranking, implying that our use of a randomized reduction is essentially necessary for the guarantees we provide.",
        "published": "2007-10-15T18:25:15Z",
        "link": "http://arxiv.org/abs/0710.2889v2",
        "categories": [
            "cs.LG",
            "cs.IR",
            "K.3.2"
        ]
    },
    {
        "title": "Using Synchronic and Diachronic Relations for Summarizing Multiple   Documents Describing Evolving Events",
        "authors": [
            "Stergos D. Afantenos",
            "V. Karkaletsis",
            "P. Stamatopoulos",
            "C. Halatsis"
        ],
        "summary": "In this paper we present a fresh look at the problem of summarizing evolving events from multiple sources. After a discussion concerning the nature of evolving events we introduce a distinction between linearly and non-linearly evolving events. We present then a general methodology for the automatic creation of summaries from evolving events. At its heart lie the notions of Synchronic and Diachronic cross-document Relations (SDRs), whose aim is the identification of similarities and differences between sources, from a synchronical and diachronical perspective. SDRs do not connect documents or textual elements found therein, but structures one might call messages. Applying this methodology will yield a set of messages and relations, SDRs, connecting them, that is a graph which we call grid. We will show how such a grid can be considered as the starting point of a Natural Language Generation System. The methodology is evaluated in two case-studies, one for linearly evolving events (descriptions of football matches) and another one for non-linearly evolving events (terrorist incidents involving hostages). In both cases we evaluate the results produced by our computational systems.",
        "published": "2007-10-18T13:24:26Z",
        "link": "http://arxiv.org/abs/0710.3502v1",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "Empirical Evaluation of Four Tensor Decomposition Algorithms",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Higher-order tensor decompositions are analogous to the familiar Singular Value Decomposition (SVD), but they transcend the limitations of matrices (second-order tensors). SVD is a powerful tool that has achieved impressive results in information retrieval, collaborative filtering, computational linguistics, computational vision, and other fields. However, SVD is limited to two-dimensional arrays of data (two modes), and many potential applications have three or more modes, which require higher-order tensor decompositions. This paper evaluates four algorithms for higher-order tensor decomposition: Higher-Order Singular Value Decomposition (HO-SVD), Higher-Order Orthogonal Iteration (HOOI), Slice Projection (SP), and Multislice Projection (MP). We measure the time (elapsed run time), space (RAM and disk space requirements), and fit (tensor reconstruction accuracy) of the four algorithms, under a variety of conditions. We find that standard implementations of HO-SVD and HOOI do not scale up to larger tensors, due to increasing RAM requirements. We recommend HOOI for tensors that are small enough for the available RAM and MP for larger tensors.",
        "published": "2007-11-13T16:28:47Z",
        "link": "http://arxiv.org/abs/0711.2023v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; I.2.6; I.2.7; E.1; G.1.3"
        ]
    },
    {
        "title": "A Biologically Inspired Classifier",
        "authors": [
            "Franco Bagnoli",
            "Francesca Di Patti"
        ],
        "summary": "We present a method for measuring the distance among records based on the correlations of data stored in the corresponding database entries. The original method (F. Bagnoli, A. Berrones and F. Franci. Physica A 332 (2004) 509-518) was formulated in the context of opinion formation. The opinions expressed over a set of topic originate a ``knowledge network'' among individuals, where two individuals are nearer the more similar their expressed opinions are. Assuming that individuals' opinions are stored in a database, the authors show that it is possible to anticipate an opinion using the correlations in the database. This corresponds to approximating the overlap between the tastes of two individuals with the correlations of their expressed opinions.   In this paper we extend this model to nonlinear matching functions, inspired by biological problems such as microarray (probe-sample pairing). We investigate numerically the error between the correlation and the overlap matrix for eight sequences of reference with random probes. Results show that this method is particularly robust for detecting similarities in the presence of translocations.",
        "published": "2007-11-16T13:38:15Z",
        "link": "http://arxiv.org/abs/0711.2615v1",
        "categories": [
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "Maximizing PageRank via outlinks",
        "authors": [
            "Cristobald de Kerchove",
            "Laure Ninove",
            "Paul Van Dooren"
        ],
        "summary": "We analyze linkage strategies for a set I of webpages for which the webmaster wants to maximize the sum of Google's PageRank scores. The webmaster can only choose the hyperlinks starting from the webpages of I and has no control on the hyperlinks from other webpages. We provide an optimal linkage strategy under some reasonable assumptions.",
        "published": "2007-11-19T09:43:22Z",
        "link": "http://arxiv.org/abs/0711.2867v1",
        "categories": [
            "cs.IR",
            "math.RA"
        ]
    },
    {
        "title": "Use of Wikipedia Categories in Entity Ranking",
        "authors": [
            "James A. Thom",
            "Jovan Pehcevski",
            "Anne-Marie Vercoustre"
        ],
        "summary": "Wikipedia is a useful source of knowledge that has many applications in language processing and knowledge representation. The Wikipedia category graph can be compared with the class hierarchy in an ontology; it has some characteristics in common as well as some differences. In this paper, we present our approach for answering entity ranking queries from the Wikipedia. In particular, we explore how to make use of Wikipedia categories to improve entity ranking effectiveness. Our experiments show that using categories of example entities works significantly better than using loosely defined target categories.",
        "published": "2007-11-19T12:35:48Z",
        "link": "http://arxiv.org/abs/0711.2917v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Première étape vers une navigation référentielle par l'image   pour l'assistance à la conception des ambiances lumineuses",
        "authors": [
            "Salma Chaabouni",
            "Jc Bignon",
            "Gilles Halin"
        ],
        "summary": "In the first design stage, image reference plays a double role of means of formulation and resolution of problems. In our approach, we consider image reference as a support of creation activity to generate ideas and we propose a tool for navigation in references by image in order to assist daylight ambience design. Within this paper, we present, in a first part, the semantic indexation method to be used for the indexation of our image database. In a second part we propose a synthetic analysis of various modes of referential navigation in order to propose a tool implementing all or a part of these modes.",
        "published": "2007-11-19T16:10:35Z",
        "link": "http://arxiv.org/abs/0711.2832v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Entity Ranking in Wikipedia",
        "authors": [
            "Anne-Marie Vercoustre",
            "James A. Thom",
            "Jovan Pehcevski"
        ],
        "summary": "The traditional entity extraction problem lies in the ability of extracting named entities from plain text using natural language processing techniques and intensive training from large document collections. Examples of named entities include organisations, people, locations, or dates. There are many research activities involving named entities; we are interested in entity ranking in the field of information retrieval. In this paper, we describe our approach to identifying and ranking entities from the INEX Wikipedia document collection. Wikipedia offers a number of interesting features for entity identification and ranking that we first introduce. We then describe the principles and the architecture of our entity ranking system, and introduce our methodology for evaluation. Our preliminary results show that the use of categories and the link structure of Wikipedia, together with entity examples, can significantly improve retrieval effectiveness.",
        "published": "2007-11-20T12:40:23Z",
        "link": "http://arxiv.org/abs/0711.3128v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Iterative Filtering for a Dynamical Reputation System",
        "authors": [
            "Cristobald de Kerchove",
            "Paul Van Dooren"
        ],
        "summary": "The paper introduces a novel iterative method that assigns a reputation to n + m items: n raters and m objects. Each rater evaluates a subset of objects leading to a n x m rating matrix with a certain sparsity pattern. From this rating matrix we give a nonlinear formula to define the reputation of raters and objects. We also provide an iterative algorithm that superlinearly converges to the unique vector of reputations and this for any rating matrix. In contrast to classical outliers detection, no evaluation is discarded in this method but each one is taken into account with different weights for the reputation of the objects. The complexity of one iteration step is linear in the number of evaluations, making our algorithm efficient for large data set. Experiments show good robustness of the reputation of the objects against cheaters and spammers and good detection properties of cheaters and spammers.",
        "published": "2007-11-26T08:12:51Z",
        "link": "http://arxiv.org/abs/0711.3964v1",
        "categories": [
            "cs.IR",
            "F.2.2"
        ]
    },
    {
        "title": "Content Reuse and Interest Sharing in Tagging Communities",
        "authors": [
            "Elizeu Santos-Neto",
            "Matei Ripeanu",
            "Adriana Iamnitchi"
        ],
        "summary": "Tagging communities represent a subclass of a broader class of user-generated content-sharing online communities. In such communities users introduce and tag content for later use. Although recent studies advocate and attempt to harness social knowledge in this context by exploiting collaboration among users, little research has been done to quantify the current level of user collaboration in these communities. This paper introduces two metrics to quantify the level of collaboration: content reuse and shared interest. Using these two metrics, this paper shows that the current level of collaboration in CiteULike and Connotea is consistently low, which significantly limits the potential of harnessing the social knowledge in communities. This study also discusses implications of these findings in the context of recommendation and reputation systems.",
        "published": "2007-11-26T23:05:02Z",
        "link": "http://arxiv.org/abs/0711.4142v2",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Contextual Information Retrieval based on Algorithmic Information Theory   and Statistical Outlier Detection",
        "authors": [
            "Rafael Martinez",
            "Manuel Cebrian",
            "Francisco de Borja Rodriguez",
            "David Camacho"
        ],
        "summary": "The main contribution of this paper is to design an Information Retrieval (IR) technique based on Algorithmic Information Theory (using the Normalized Compression Distance- NCD), statistical techniques (outliers), and novel organization of data base structure. The paper shows how they can be integrated to retrieve information from generic databases using long (text-based) queries. Two important problems are analyzed in the paper. On the one hand, how to detect \"false positives\" when the distance among the documents is very low and there is actual similarity. On the other hand, we propose a way to structure a document database which similarities distance estimation depends on the length of the selected text. Finally, the experimental evaluations that have been carried out to study previous problems are shown.",
        "published": "2007-11-27T23:58:49Z",
        "link": "http://arxiv.org/abs/0711.4388v1",
        "categories": [
            "cs.IR",
            "cs.IT",
            "math.IT",
            "E.5; H.1.1; H.3"
        ]
    },
    {
        "title": "Simrank++: Query rewriting through link analysis of the click graph",
        "authors": [
            "Ioannis Antonellis",
            "Hector Garcia-Molina",
            "Chi-Chao Chang"
        ],
        "summary": "We focus on the problem of query rewriting for sponsored search. We base rewrites on a historical click graph that records the ads that have been clicked on in response to past user queries. Given a query q, we first consider Simrank as a way to identify queries similar to q, i.e., queries whose ads a user may be interested in. We argue that Simrank fails to properly identify query similarities in our application, and we present two enhanced version of Simrank: one that exploits weights on click graph edges and another that exploits ``evidence.'' We experimentally evaluate our new schemes against Simrank, using actual click graphs and queries form Yahoo!, and using a variety of metrics. Our results show that the enhanced methods can yield more and better query rewrites.",
        "published": "2007-12-04T12:43:17Z",
        "link": "http://arxiv.org/abs/0712.0499v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "An axiomatic approach to intrinsic dimension of a dataset",
        "authors": [
            "Vladimir Pestov"
        ],
        "summary": "We perform a deeper analysis of an axiomatic approach to the concept of intrinsic dimension of a dataset proposed by us in the IJCNN'07 paper (arXiv:cs/0703125). The main features of our approach are that a high intrinsic dimension of a dataset reflects the presence of the curse of dimensionality (in a certain mathematically precise sense), and that dimension of a discrete i.i.d. sample of a low-dimensional manifold is, with high probability, close to that of the manifold. At the same time, the intrinsic dimension of a sample is easily corrupted by moderate high-dimensional noise (of the same amplitude as the size of the manifold) and suffers from prohibitevely high computational complexity (computing it is an $NP$-complete problem). We outline a possible way to overcome these difficulties.",
        "published": "2007-12-12T23:39:21Z",
        "link": "http://arxiv.org/abs/0712.2063v1",
        "categories": [
            "cs.IR",
            "H.1.1; H.2.1; H.3.1; I.5.3"
        ]
    },
    {
        "title": "CLAIRLIB Documentation v1.03",
        "authors": [
            "Dragomir Radev",
            "Mark Hodges",
            "Anthony Fader",
            "Mark Joseph",
            "Joshua Gerrish",
            "Mark Schaller",
            "Jonathan dePeri",
            "Bryan Gibson"
        ],
        "summary": "The Clair library is intended to simplify a number of generic tasks in Natural Language Processing (NLP), Information Retrieval (IR), and Network Analysis. Its architecture also allows for external software to be plugged in with very little effort. Functionality native to Clairlib includes Tokenization, Summarization, LexRank, Biased LexRank, Document Clustering, Document Indexing, PageRank, Biased PageRank, Web Graph Analysis, Network Generation, Power Law Distribution Analysis, Network Analysis (clustering coefficient, degree distribution plotting, average shortest path, diameter, triangles, shortest path matrices, connected components), Cosine Similarity, Random Walks on Graphs, Statistics (distributions, tests), Tf, Idf, Community Finding.",
        "published": "2007-12-19T22:20:40Z",
        "link": "http://arxiv.org/abs/0712.3298v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "On the implementation of construction functions for non-free concrete   data types",
        "authors": [
            "Frédéric Blanqui",
            "Thérèse Hardin",
            "Pierre Weis"
        ],
        "summary": "Many algorithms use concrete data types with some additional invariants. The set of values satisfying the invariants is often a set of representatives for the equivalence classes of some equational theory. For instance, a sorted list is a particular representative wrt commutativity. Theories like associativity, neutral element, idempotence, etc. are also very common. Now, when one wants to combine various invariants, it may be difficult to find the suitable representatives and to efficiently implement the invariants. The preservation of invariants throughout the whole program is even more difficult and error prone. Classically, the programmer solves this problem using a combination of two techniques: the definition of appropriate construction functions for the representatives and the consistent usage of these functions ensured via compiler verifications. The common way of ensuring consistency is to use an abstract data type for the representatives; unfortunately, pattern matching on representatives is lost. A more appealing alternative is to define a concrete data type with private constructors so that both compiler verification and pattern matching on representatives are granted. In this paper, we detail the notion of private data type and study the existence of construction functions. We also describe a prototype, called Moca, that addresses the entire problem of...",
        "published": "2007-01-05T16:54:35Z",
        "link": "http://arxiv.org/abs/cs/0701031v1",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Fingerprinting Logic Programs",
        "authors": [
            "Alexander Serebrenik",
            "Wim Vanhoof"
        ],
        "summary": "In this work we present work in progress on functionality duplication detection in logic programs. Eliminating duplicated functionality recently became prominent in context of refactoring. We describe a quantitative approach that allows to measure the ``similarity'' between two predicate definitions. Moreover, we show how to compute a so-called ``fingerprint'' for every predicate. Fingerprints capture those characteristics of the predicate that are significant when searching for duplicated functionality. Since reasoning on fingerprints is much easier than reasoning on predicate definitions, comparing the fingerprints is a promising direction in automated code duplication in logic programs.",
        "published": "2007-01-12T15:39:29Z",
        "link": "http://arxiv.org/abs/cs/0701081v1",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Recurrence with affine level mappings is P-time decidable for CLP(R)",
        "authors": [
            "Fred Mesnard",
            "Alexander Serebrenik"
        ],
        "summary": "In this paper we introduce a class of constraint logic programs such that their termination can be proved by using affine level mappings. We show that membership to this class is decidable in polynomial time.",
        "published": "2007-01-12T18:43:48Z",
        "link": "http://arxiv.org/abs/cs/0701082v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; F.3.2"
        ]
    },
    {
        "title": "A Delta Debugger for ILP Query Execution",
        "authors": [
            "Remko Troncon",
            "Gerda Janssens"
        ],
        "summary": "Because query execution is the most crucial part of Inductive Logic Programming (ILP) algorithms, a lot of effort is invested in developing faster execution mechanisms. These execution mechanisms typically have a low-level implementation, making them hard to debug. Moreover, other factors such as the complexity of the problems handled by ILP algorithms and size of the code base of ILP data mining systems make debugging at this level a very difficult job. In this work, we present the trace-based debugging approach currently used in the development of new execution mechanisms in hipP, the engine underlying the ACE Data Mining system. This debugger uses the delta debugging algorithm to automatically reduce the total time needed to expose bugs in ILP execution, thus making manual debugging step much lighter.",
        "published": "2007-01-17T13:35:35Z",
        "link": "http://arxiv.org/abs/cs/0701105v1",
        "categories": [
            "cs.PL",
            "cs.LG"
        ]
    },
    {
        "title": "On using Tracer Driver for External Dynamic Process Observation",
        "authors": [
            "Pierre Deransart"
        ],
        "summary": "One is interested here in the observation of dynamic processes starting from the traces which they leave or those that one makes them produce. It is considered here that it should be possible to make several observations simultaneously, using a large variety of independently developed analyzers. For this purpose, we introduce the original notion of ``full trace'' to capture the idea that a process can be instrumented in such a way that it may broadcast all information which could ever be requested by any kind of observer. Each analyzer can then find in the full trace the data elements which it needs. This approach uses what has been called a \"tracer driver\" which completes the tracer and drives it to answer the requests of the analyzers. A tracer driver allows to restrict the flow of information and makes this approach tractable. On the other side, the potential size of a full trace seems to make the idea of full trace unrealistic. In this work we explore the consequences of this notion in term of potential efficiency, by analyzing the respective workloads between the (full) tracer and many different analyzers, all being likely run in true parallel environments. To illustrate this study, we use the example of the observation of the resolution of constraints systems (proof-tree, search-tree and propagation) using sophisticated visualization tools, as developed in the project OADymPPaC (2001-2004). The processes considered here are computer programs, but we believe the approach can be extended to many other kinds of processes.",
        "published": "2007-01-17T13:43:50Z",
        "link": "http://arxiv.org/abs/cs/0701106v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "JavaTA: A Logic-based Debugger for Java",
        "authors": [
            "Hani Girgis",
            "Bharat Jayaraman"
        ],
        "summary": "This paper presents a logic based approach to debugging Java programs. In contrast with traditional debugging we propose a debugging methodology for Java programs using logical queries on individual execution states and also over the history of execution. These queries were arrived at by a systematic study of errors in object-oriented programs in our earlier research. We represent the salient events during the execution of a Java program by a logic database, and implement the queries as logic programs. Such an approach allows us to answer a number of useful and interesting queries about a Java program, such as the calling sequence that results in a certain outcome, the state of an object at a particular execution point, etc. Our system also provides the ability to compose new queries during a debugging session. We believe that logic programming offers a significant contribution to the art of object-oriented programs debugging.",
        "published": "2007-01-17T13:48:49Z",
        "link": "http://arxiv.org/abs/cs/0701107v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Towards Execution Time Estimation for Logic Programs via Static Analysis   and Profiling",
        "authors": [
            "Edison Mera",
            "Pedro Lopez-Garcia",
            "German Puebla",
            "Manuel Carro",
            "Manuel Hermenegildo"
        ],
        "summary": "Effective static analyses have been proposed which infer bounds on the number of resolutions or reductions. These have the advantage of being independent from the platform on which the programs are executed and have been shown to be useful in a number of applications, such as granularity control in parallel execution. On the other hand, in distributed computation scenarios where platforms with different capabilities come into play, it is necessary to express costs in metrics that include the characteristics of the platform. In particular, it is specially interesting to be able to infer upper and lower bounds on actual execution times. With this objective in mind, we propose an approach which combines compile-time analysis for cost bounds with a one-time profiling of the platform in order to determine the values of certain parameters for a given platform. These parameters calibrate a cost model which, from then on, is able to compute statically time bound functions for procedures and to predict with a significant degree of accuracy the execution times of such procedures in the given platform. The approach has been implemented and integrated in the CiaoPP system.",
        "published": "2007-01-17T14:22:17Z",
        "link": "http://arxiv.org/abs/cs/0701108v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "ExSched: Solving Constraint Satisfaction Problems with the Spreadsheet   Paradigm",
        "authors": [
            "Siddharth Chitnis",
            "Madhu Yennamani",
            "Gopal Gupta"
        ],
        "summary": "We report on the development of a general tool called ExSched, implemented as a plug-in for Microsoft Excel, for solving a class of constraint satisfaction problems. The traditional spreadsheet paradigm is based on attaching arithmetic expressions to individual cells and then evaluating them. The ExSched interface generalizes the spreadsheet paradigm to allow finite domain constraints to be attached to the individual cells that are then solved to get a solution. This extension provides a user-friendly interface for solving constraint satisfaction problems that can be modeled as 2D tables, such as scheduling problems, timetabling problems, product configuration, etc. ExSched can be regarded as a spreadsheet interface to CLP(FD) that hides the syntactic and semantic complexity of CLP(FD) and enables novice users to solve many scheduling and timetabling problems interactively.",
        "published": "2007-01-17T14:26:40Z",
        "link": "http://arxiv.org/abs/cs/0701109v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "A Web-based Tool Combining Different Type Analyses",
        "authors": [
            "Kim Henriksen",
            "John Gallagher"
        ],
        "summary": "There are various kinds of type analysis of logic programs. These include for example inference of types that describe an over-approximation of the success set of a program, inference of well-typings, and abstractions based on given types. Analyses can be descriptive or prescriptive or a mixture of both, and they can be goal-dependent or goal-independent. We describe a prototype tool that can be accessed from a web browser, allowing various type analyses to be run. The first goal of the tool is to allow the analysis results to be examined conveniently by clicking on points in the original program clauses, and to highlight ill-typed program constructs, empty types or other type anomalies. Secondly the tool allows combination of the various styles of analysis. For example, a descriptive regular type can be automatically inferred for a given program, and then that type can be used to generate the minimal \"domain model\" of the program with respect to the corresponding pre-interpretation, which can give more precise information than the original descriptive type.",
        "published": "2007-01-17T14:35:56Z",
        "link": "http://arxiv.org/abs/cs/0701110v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Some Issues on Incremental Abstraction-Carrying Code",
        "authors": [
            "Elvira Albert",
            "Puri Arenas",
            "German Puebla"
        ],
        "summary": "Abstraction-Carrying Code (ACC) has recently been proposed as a framework for proof-carrying code (PCC) in which the code supplier provides a program together with an abstraction (or abstract model of the program) whose validity entails compliance with a predefined safety policy. The abstraction thus plays the role of safety certificate and its generation (and validation) is carried out automatically by a fixed-point analyzer. Existing approaches for PCC are developed under the assumption that the consumer reads and validates the entire program w.r.t. the full certificate at once, in a non incremental way. In this abstract, we overview the main issues on incremental ACC. In particular, in the context of logic programming, we discuss both the generation of incremental certificates and the design of an incremental checking algorithm for untrusted updates of a (trusted) program, i.e., when a producer provides a modified version of a previously validated program. By update, we refer to any arbitrary change on a program, i.e., the extension of the program with new predicates, the deletion of existing predicates and the replacement of existing predicates by new versions for them. We also discuss how each kind of update affects the incremental extension in terms of accuracy and correctness.",
        "published": "2007-01-17T14:44:30Z",
        "link": "http://arxiv.org/abs/cs/0701111v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "A Generic Analysis Environment for Curry Programs",
        "authors": [
            "Michael Hanus"
        ],
        "summary": "We present CurryBrowser, a generic analysis environment for the declarative multi-paradigm language Curry. CurryBrowser supports browsing through the program code of an application written in Curry, i.e., the main module and all directly or indirectly imported modules. Each module can be shown in different formats (e.g., source code, interface, intermediate code) and, inside each module, various properties of functions defined in this module can be analyzed. In order to support the integration of various program analyses, CurryBrowser has a generic interface to connect local and global analyses implemented in Curry. CurryBrowser is completely implemented in Curry using libraries for GUI programming and meta-programming.",
        "published": "2007-01-24T06:55:34Z",
        "link": "http://arxiv.org/abs/cs/0701147v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Proceedings of the 16th Workshop in Logic-based Methods in Programming   Environments (WLPE2006)",
        "authors": [
            "Wim Vanhoof",
            "Susana Munoz-Hernandez"
        ],
        "summary": "This volume contains the papers presented at WLPE'06: the 16th Workshop on Logic-based Methods in Programming Environments held on August 16, 2006 in the Seattle Sheraton Hotel and Towers, Seattle, Washington (USA). It was organised as a satellite workshop of ICLP'06, the 22th International Conference on Logic Programming.",
        "published": "2007-01-24T07:03:17Z",
        "link": "http://arxiv.org/abs/cs/0701148v1",
        "categories": [
            "cs.PL",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Towards Practical Typechecking for Macro Tree Transducers",
        "authors": [
            "Alain Frisch",
            "Haruo Hosoya"
        ],
        "summary": "Macro tree transducers (mtt) are an important model that both covers many useful XML transformations and allows decidable exact typechecking. This paper reports our first step toward an implementation of mtt typechecker that has a practical efficiency. Our approach is to represent an input type obtained from a backward inference as an alternating tree automaton, in a style similar to Tozawa's XSLT0 typechecking. In this approach, typechecking reduces to checking emptiness of an alternating tree automaton. We propose several optimizations (Cartesian factorization, state partitioning) on the backward inference process in order to produce much smaller alternating tree automata than the naive algorithm, and we present our efficient algorithm for checking emptiness of alternating tree automata, where we exploit the explicit representation of alternation for local optimizations. Our preliminary experiments confirm that our algorithm has a practical performance that can typecheck simple transformations with respect to the full XHTML in a reasonable time.",
        "published": "2007-01-26T15:16:31Z",
        "link": "http://arxiv.org/abs/cs/0701176v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "The parallel implementation of the Astrée static analyzer",
        "authors": [
            "David Monniaux"
        ],
        "summary": "The Astr\\'{e}e static analyzer is a specialized tool that can prove the absence of runtime errors, including arithmetic overflows, in large critical programs. Keeping analysis times reasonable for industrial use is one of the design objectives. In this paper, we discuss the parallel implementation of the analysis.",
        "published": "2007-01-30T15:20:07Z",
        "link": "http://arxiv.org/abs/cs/0701191v1",
        "categories": [
            "cs.PL",
            "cs.PF",
            "D.2.4"
        ]
    },
    {
        "title": "The pitfalls of verifying floating-point computations",
        "authors": [
            "David Monniaux"
        ],
        "summary": "Current critical systems commonly use a lot of floating-point computations, and thus the testing or static analysis of programs containing floating-point operators has become a priority. However, correctly defining the semantics of common implementations of floating-point is tricky, because semantics may change with many factors beyond source-code level, such as choices made by compilers. We here give concrete examples of problems that can appear and solutions to implement in analysis software.",
        "published": "2007-01-30T16:26:50Z",
        "link": "http://arxiv.org/abs/cs/0701192v5",
        "categories": [
            "cs.PL",
            "cs.NA",
            "D.2.4; D.3.1; F.3.1; G.1.0; G.4"
        ]
    },
    {
        "title": "A Static Analyzer for Large Safety-Critical Software",
        "authors": [
            "Bruno Blanchet",
            "Patrick Cousot",
            "Radhia Cousot",
            "Jerôme Feret",
            "Laurent Mauborgne",
            "Antoine Miné",
            "David Monniaux",
            "Xavier Rival"
        ],
        "summary": "We show that abstract interpretation-based static program analysis can be made efficient and precise enough to formally verify a class of properties for a family of large programs with few or no false alarms. This is achieved by refinement of a general purpose static analyzer and later adaptation to particular programs of the family by the end-user through parametrization. This is applied to the proof of soundness of data manipulation operations at the machine level for periodic synchronous safety critical embedded software. The main novelties are the design principle of static analyzers by refinement and adaptation through parametrization, the symbolic manipulation of expressions to improve the precision of abstract transfer functions, the octagon, ellipsoid, and decision tree abstract domains, all with sound handling of rounding errors in floating point computations, widening strategies (with thresholds, delayed) and the automatic determination of the parameters (parametrized packing).",
        "published": "2007-01-30T16:57:14Z",
        "link": "http://arxiv.org/abs/cs/0701193v1",
        "categories": [
            "cs.PL",
            "cs.PF",
            "D.2.4; D.3.1; F.3.1; F.3.2"
        ]
    },
    {
        "title": "An Abstract Monte-Carlo Method for the Analysis of Probabilistic   Programs",
        "authors": [
            "David Monniaux"
        ],
        "summary": "We introduce a new method, combination of random testing and abstract interpretation, for the analysis of programs featuring both probabilistic and non-probabilistic nondeterminism. After introducing \"ordinary\" testing, we show how to combine testing and abstract interpretation and give formulas linking the precision of the results to the number of iterations. We then discuss complexity and optimization issues and end with some experimental results.",
        "published": "2007-01-30T17:14:52Z",
        "link": "http://arxiv.org/abs/cs/0701195v1",
        "categories": [
            "cs.PL",
            "cs.PF"
        ]
    },
    {
        "title": "The Suspension Calculus and its Relationship to Other Explicit   Treatments of Substitution in Lambda Calculi",
        "authors": [
            "Andrew Gacek"
        ],
        "summary": "The intrinsic treatment of binding in the lambda calculus makes it an ideal data structure for representing syntactic objects with binding such as formulas, proofs, types, and programs. Supporting such a data structure in an implementation is made difficult by the complexity of the substitution operation relative to lambda terms. In this paper we present the suspension calculus, an explicit treatment of meta level binding in the lambda calculus. We prove properties of this calculus which make it a suitable replacement for the lambda calculus in implementation. Finally, we compare the suspension calculus with other explicit treatments of substitution.",
        "published": "2007-02-05T14:35:01Z",
        "link": "http://arxiv.org/abs/cs/0702027v1",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "A Formal Model for Programming Wireless Sensor Networks",
        "authors": [
            "Luis Lopes",
            "Francisco Martins",
            "Miguel S. Silva",
            "Joao Barros"
        ],
        "summary": "In this paper we present new developments in the expressiveness and in the theory of a Calculus for Sensor Networks (CSN). We combine a network layer of sensor devices with a local object model to describe sensor devices with state. The resulting calculus is quite small and yet very expressive. We also present a type system and a type invariance result for the calculus. These results provide the fundamental framework for the development of programming languages and run-time environments.",
        "published": "2007-02-07T14:17:29Z",
        "link": "http://arxiv.org/abs/cs/0702042v1",
        "categories": [
            "cs.DC",
            "cs.PL"
        ]
    },
    {
        "title": "Logic Programming with Satisfiability",
        "authors": [
            "Michael Codish",
            "Vitaly Lagoon",
            "Peter J. Stuckey"
        ],
        "summary": "This paper presents a Prolog interface to the MiniSat satisfiability solver. Logic program- ming with satisfiability combines the strengths of the two paradigms: logic programming for encoding search problems into satisfiability on the one hand and efficient SAT solving on the other. This synergy between these two exposes a programming paradigm which we propose here as a logic programming pearl. To illustrate logic programming with SAT solving we give an example Prolog program which solves instances of Partial MAXSAT.",
        "published": "2007-02-13T03:10:02Z",
        "link": "http://arxiv.org/abs/cs/0702072v1",
        "categories": [
            "cs.PL",
            "cs.AI"
        ]
    },
    {
        "title": "Bistable Biorders: A Sequential Domain Theory",
        "authors": [
            "James Laird"
        ],
        "summary": "We give a simple order-theoretic construction of a Cartesian closed category of sequential functions. It is based on bistable biorders, which are sets with a partial order -- the extensional order -- and a bistable coherence, which captures equivalence of program behaviour, up to permutation of top (error) and bottom (divergence). We show that monotone and bistable functions (which are required to preserve bistably bounded meets and joins) are strongly sequential, and use this fact to prove universality results for the bistable biorder semantics of the simply-typed lambda-calculus (with atomic constants), and an extension with arithmetic and recursion.   We also construct a bistable model of SPCF, a higher-order functional programming language with non-local control. We use our universality result for the lambda-calculus to show that the semantics of SPCF is fully abstract. We then establish a direct correspondence between bistable functions and sequential algorithms by showing that sequential data structures give rise to bistable biorders, and that each bistable function between such biorders is computed by a sequential algorithm.",
        "published": "2007-02-28T14:57:27Z",
        "link": "http://arxiv.org/abs/cs/0702169v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "A New Numerical Abstract Domain Based on Difference-Bound Matrices",
        "authors": [
            "Antoine Miné"
        ],
        "summary": "This paper presents a new numerical abstract domain for static analysis by abstract interpretation. This domain allows us to represent invariants of the form (x-y<=c) and (+/-x<=c), where x and y are variables values and c is an integer or real constant. Abstract elements are represented by Difference-Bound Matrices, widely used by model-checkers, but we had to design new operators to meet the needs of abstract interpretation. The result is a complete lattice of infinite height featuring widening, narrowing and common transfer functions. We focus on giving an efficient O(n2) representation and graph-based O(n3) algorithms - where n is the number of variables|and claim that this domain always performs more precisely than the well-known interval domain. To illustrate the precision/cost tradeoff of this domain, we have implemented simple abstract interpreters for toy imperative and parallel languages which allowed us to prove some non-trivial algorithms correct.",
        "published": "2007-03-15T05:44:24Z",
        "link": "http://arxiv.org/abs/cs/0703073v2",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Field-Sensitive Value Analysis of Embedded C Programs with Union Types   and Pointer Arithmetics",
        "authors": [
            "Antoine Miné"
        ],
        "summary": "We propose a memory abstraction able to lift existing numerical static analyses to C programs containing union types, pointer casts, and arbitrary pointer arithmetics. Our framework is that of a combined points-to and data-value analysis. We abstract the contents of compound variables in a field-sensitive way, whether these fields contain numeric or pointer values, and use stock numerical abstract domains to find an overapproximation of all possible memory states--with the ability to discover relationships between variables. A main novelty of our approach is the dynamic mapping scheme we use to associate a flat collection of abstract cells of scalar type to the set of accessed memory locations, while taking care of byte-level aliases - i.e., C variables with incompatible types allocated in overlapping memory locations. We do not rely on static type information which can be misleading in C programs as it does not account for all the uses a memory zone may be put to. Our work was incorporated within the Astr\\'{e}e static analyzer that checks for the absence of run-time-errors in embedded, safety-critical, numerical-intensive software. It replaces the former memory domain limited to well-typed, union-free, pointer-cast free data-structures. Early results demonstrate that this abstraction allows analyzing a larger class of C programs, without much cost overhead.",
        "published": "2007-03-15T05:46:39Z",
        "link": "http://arxiv.org/abs/cs/0703074v2",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "A Few Graph-Based Relational Numerical Abstract Domains",
        "authors": [
            "Antoine Miné"
        ],
        "summary": "This article presents the systematic design of a class of relational numerical abstract domains from non-relational ones. Constructed domains represent sets of invariants of the form (vj - vi in C), where vj and vi are two variables, and C lives in an abstraction of P(Z), P(Q), or P(R). We will call this family of domains weakly relational domains. The underlying concept allowing this construction is an extension of potential graphs and shortest-path closure algorithms in exotic-like algebras. Example constructions are given in order to retrieve well-known domains as well as new ones. Such domains can then be used in the Abstract Interpretation framework in order to design various static analyses. Amajor benfit of this construction is its modularity, allowing to quickly implement new abstract domains from existing ones.",
        "published": "2007-03-15T05:56:51Z",
        "link": "http://arxiv.org/abs/cs/0703075v2",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Symbolic Methods to Enhance the Precision of Numerical Abstract Domains",
        "authors": [
            "Antoine Miné"
        ],
        "summary": "We present lightweight and generic symbolic methods to improve the precison of numerical static analyses based on Abstract Interpretation. The main idea is to simplify numerical expressions before they are fed to abstract transfer functions. An important novelty is that these simplifications are performed on-the-fly, using information gathered dynamically by the analyzer. A first method, called \"linearization,\" allows abstracting arbitrary expressions into affine forms with interval coefficients while simplifying them. A second method, called \"symbolic constant propagation,\" enhances the simplification feature of the linearization by propagating assigned expressions in a symbolic way. Combined together, these methods increase the relationality level of numerical abstract domains and make them more robust against program transformations. We show how they can be integrated within the classical interval, octagon and polyhedron domains. These methods have been incorporated within the Astr\\'{e}e static analyzer that checks for the absence of run-time errors in embedded critical avionics software. We present an experimental proof of their usefulness.",
        "published": "2007-03-15T06:05:40Z",
        "link": "http://arxiv.org/abs/cs/0703076v2",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Relational Abstract Domains for the Detection of Floating-Point Run-Time   Errors",
        "authors": [
            "Antoine Miné"
        ],
        "summary": "We present a new idea to adapt relational abstract domains to the analysis of IEEE 754-compliant floating-point numbers in order to statically detect, through abstract Interpretation-based static analyses, potential floating-point run-time exceptions such as overflows or invalid operations. In order to take the non-linearity of rounding into account, expressions are modeled as linear forms with interval coefficients. We show how to extend already existing numerical abstract domains, such as the octagon abstract domain, to efficiently abstract transfer functions based on interval linear forms. We discuss specific fixpoint stabilization techniques and give some experimental results.",
        "published": "2007-03-15T06:07:02Z",
        "link": "http://arxiv.org/abs/cs/0703077v2",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "The Octagon Abstract Domain",
        "authors": [
            "Antoine Miné"
        ],
        "summary": "This article presents a new numerical abstract domain for static analysis by abstract interpretation. It extends a former numerical abstract domain based on Difference-Bound Matrices and allows us to represent invariants of the form (+/-x+/-y<=c), where x and y are program variables and c is a real constant. We focus on giving an efficient representation based on Difference-Bound Matrices - O(n2) memory cost, where n is the number of variables - and graph-based algorithms for all common abstract operators - O(n3) time cost. This includes a normal form algorithm to test equivalence of representation and a widening operator to compute least fixpoint approximations.",
        "published": "2007-03-15T18:16:32Z",
        "link": "http://arxiv.org/abs/cs/0703084v2",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "On the Design of Generic Static Analyzers for Modern Imperative   Languages",
        "authors": [
            "Roberto Bagnara",
            "Patricia M. Hill",
            "Andrea Pescetti",
            "Enea Zaffanella"
        ],
        "summary": "The design and implementation of precise static analyzers for significant fragments of modern imperative languages like C, C++, Java and Python is a challenging problem. In this paper, we consider a core imperative language that has several features found in mainstream languages such as those including recursive functions, run-time system and user-defined exceptions, and a realistic data and memory model. For this language we provide a concrete semantics --characterizing both finite and infinite computations-- and a generic abstract semantics that we prove sound with respect to the concrete one. We say the abstract semantics is generic since it is designed to be completely parametric on the analysis domains: in particular, it provides support for \\emph{relational} domains (i.e., abstract domains that can capture the relationships between different data objects). We also sketch how the proposed methodology can be extended to accommodate a larger language that includes pointers, compound data objects and non-structured control flow mechanisms. The approach, which is based on structured, big-step $\\mathrm{G}^\\infty\\mathrm{SOS}$ operational semantics and on abstract interpretation, is modular in that the overall static analyzer is naturally partitioned into components with clearly identified responsibilities and interfaces, something that greatly simplifies both the proof of correctness and the implementation.",
        "published": "2007-03-23T09:47:15Z",
        "link": "http://arxiv.org/abs/cs/0703116v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.1; F.3.2"
        ]
    },
    {
        "title": "Liveness of Heap Data for Functional Programs",
        "authors": [
            "Amey Karkare",
            "Uday Khedker",
            "Amitabha Sanyal"
        ],
        "summary": "Functional programming languages use garbage collection for heap memory management. Ideally, garbage collectors should reclaim all objects that are dead at the time of garbage collection. An object is dead at an execution instant if it is not used in future. Garbage collectors collect only those dead objects that are not reachable from any program variable. This is because they are not able to distinguish between reachable objects that are dead and reachable objects that are live.   In this paper, we describe a static analysis to discover reachable dead objects in programs written in first-order, eager functional programming languages. The results of this technique can be used to make reachable dead objects unreachable, thereby allowing garbage collectors to reclaim more dead objects.",
        "published": "2007-03-30T16:06:27Z",
        "link": "http://arxiv.org/abs/cs/0703155v1",
        "categories": [
            "cs.PL",
            "D.3.2; D.3.4"
        ]
    },
    {
        "title": "A Language-Based Approach for Improving the Robustness of Network   Application Protocol Implementations",
        "authors": [
            "Burgy Laurent",
            "Laurent Réveillère",
            "Julia Lawall",
            "Gilles Muller"
        ],
        "summary": "The secure and robust functioning of a network relies on the defect-free implementation of network applications. As network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. In this paper, we present a domain-specific language, Zebu, for describing protocol message formats and related processing constraints. From a Zebu specification, a compiler automatically generates stubs to be used by an application to parse network messages. Zebu is easy to use, as it builds on notations used in RFCs to describe protocol grammars. Zebu is also efficient, as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand. Finally, Zebu-based applications are robust, as the Zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. Using a mutation analysis in the context of SIP and RTSP, we show that Zebu significantly improves application robustness.",
        "published": "2007-04-11T08:35:32Z",
        "link": "http://arxiv.org/abs/0704.1373v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Light Logics and Optimal Reduction: Completeness and Complexity",
        "authors": [
            "Patrick Baillot",
            "Paolo Coppola",
            "Ugo Dal Lago"
        ],
        "summary": "Typing of lambda-terms in Elementary and Light Affine Logic (EAL, LAL, resp.) has been studied for two different reasons: on the one hand the evaluation of typed terms using LAL (EAL, resp.) proof-nets admits a guaranteed polynomial (elementary, resp.) bound; on the other hand these terms can also be evaluated by optimal reduction using the abstract version of Lamping's algorithm. The first reduction is global while the second one is local and asynchronous. We prove that for LAL (EAL, resp.) typed terms, Lamping's abstract algorithm also admits a polynomial (elementary, resp.) bound. We also show its soundness and completeness (for EAL and LAL with type fixpoints), by using a simple geometry of interaction model (context semantics).",
        "published": "2007-04-19T01:17:29Z",
        "link": "http://arxiv.org/abs/0704.2448v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.4.1"
        ]
    },
    {
        "title": "General-Purpose Computing on a Semantic Network Substrate",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "This article presents a model of general-purpose computing on a semantic network substrate. The concepts presented are applicable to any semantic network representation. However, due to the standards and technological infrastructure devoted to the Semantic Web effort, this article is presented from this point of view. In the proposed model of computing, the application programming interface, the run-time program, and the state of the computing virtual machine are all represented in the Resource Description Framework (RDF). The implementation of the concepts presented provides a practical computing paradigm that leverages the highly-distributed and standardized representational-layer of the Semantic Web.",
        "published": "2007-04-25T15:37:52Z",
        "link": "http://arxiv.org/abs/0704.3395v4",
        "categories": [
            "cs.AI",
            "cs.PL",
            "I.2.4; I.2.5; H.3.7; H.3.4"
        ]
    },
    {
        "title": "Typer la dé-sérialisation sans sérialiser les types",
        "authors": [
            "Grégoire Henry",
            "Michel Mauny",
            "Emmanuel Chailloux"
        ],
        "summary": "In this paper, we propose a way of assigning static type information to unmarshalling functions and we describe a verification technique for unmarshalled data that preserves the execution safety provided by static type checking. This technique, whose correctness is proven, relies on singleton types whose values are transmitted to unmarshalling routines at runtime, and on an efficient checking algorithm able to deal with sharing and cycles.",
        "published": "2007-05-10T12:19:51Z",
        "link": "http://arxiv.org/abs/0705.1452v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Mixing the Objective Caml and C# Programming Models in the .Net   Framework",
        "authors": [
            "Emmanuel Chailloux",
            "Grégoire Henry",
            "Raphaël Montelatici"
        ],
        "summary": "We present a new code generator, called O'Jacare.net, to inter-operate between C# and Objective Caml through their object models. O'Jacare.net defines a basic IDL (Interface Definition Language) that describes classes and interfaces in order to communicate between Objective Caml and C#. O'Jacare.net generates all needed wrapper classes and takes advantage of static type checking in both worlds. Although the IDL intersects these two object models, O'Jacare.net allows to combine features from both.",
        "published": "2007-05-10T12:31:17Z",
        "link": "http://arxiv.org/abs/0705.1458v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Improvements to the Psi-SSA representation",
        "authors": [
            "Francois De Ferriere"
        ],
        "summary": "Modern compiler implementations use the Static Single Assignment representation as a way to efficiently implement optimizing algorithms. However this representation is not well adapted to architectures with a predicated instruction set. The Psi-SSA representation extends the SSA representation such that standard SSA algorithms can be easily adapted to an architecture with a fully predicated instruction set. A new pseudo operation, the Psi operation, is introduced to merge several conditional definitions into a unique definition.",
        "published": "2007-05-15T12:06:32Z",
        "link": "http://arxiv.org/abs/0705.2126v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Elementary transformation analysis for Array-OL",
        "authors": [
            "Paul Feautrier"
        ],
        "summary": "Array-OL is a high-level specification language dedicated to the definition of intensive signal processing applications. Several tools exist for implementing an Array-OL specification as a data parallel program. While Array-OL can be used directly, it is often convenient to be able to deduce part of the specification from a sequential version of the application. This paper proposes such an analysis and examines its feasibility and its limits.",
        "published": "2007-05-15T13:44:35Z",
        "link": "http://arxiv.org/abs/0705.2145v2",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Linear Tabling Strategies and Optimizations",
        "authors": [
            "Neng-Fa Zhou",
            "Taisuke Sato",
            "Yi-Dong Shen"
        ],
        "summary": "Recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. Linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. One decision concerns when answers are consumed and returned. This paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. The results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. Linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. Each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. Naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. In this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. We give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. Benchmarking in B-Prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of SLG.",
        "published": "2007-05-23T20:52:42Z",
        "link": "http://arxiv.org/abs/0705.3468v1",
        "categories": [
            "cs.PL",
            "D.1.6"
        ]
    },
    {
        "title": "Applying the Z-transform for the static analysis of floating-point   numerical filters",
        "authors": [
            "David Monniaux"
        ],
        "summary": "Digital linear filters are used in a variety of applications (sound treatment, control/command, etc.), implemented in software, in hardware, or a combination thereof. For safety-critical applications, it is necessary to bound all variables and outputs of all filters. We give a compositional, effective abstraction for digital linear filters expressed as block diagrams, yielding sound, precise bounds for fixed-point or floating-point implementations of the filters.",
        "published": "2007-06-02T06:18:48Z",
        "link": "http://arxiv.org/abs/0706.0252v1",
        "categories": [
            "cs.PL",
            "cs.NA"
        ]
    },
    {
        "title": "Interpolant-Based Transition Relation Approximation",
        "authors": [
            "Ranjit Jhala",
            "Kenneth L. McMillan"
        ],
        "summary": "In predicate abstraction, exact image computation is problematic, requiring in the worst case an exponential number of calls to a decision procedure. For this reason, software model checkers typically use a weak approximation of the image. This can result in a failure to prove a property, even given an adequate set of predicates. We present an interpolant-based method for strengthening the abstract transition relation in case of such failures. This approach guarantees convergence given an adequate set of predicates, without requiring an exact image computation. We show empirically that the method converges more rapidly than an earlier method based on counterexample analysis.",
        "published": "2007-06-04T20:07:54Z",
        "link": "http://arxiv.org/abs/0706.0523v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SE",
            "D.2.4; F.3.1"
        ]
    },
    {
        "title": "An Efficient OpenMP Runtime System for Hierarchical Arch",
        "authors": [
            "Samuel Thibault",
            "François Broquedis",
            "Brice Goglin",
            "Raymond Namyst",
            "Pierre-André Wacrenier"
        ],
        "summary": "Exploiting the full computational power of always deeper hierarchical multiprocessor machines requires a very careful distribution of threads and data among the underlying non-uniform architecture. The emergence of multi-core chips and NUMA machines makes it important to minimize the number of remote memory accesses, to favor cache affinities, and to guarantee fast completion of synchronization steps. By using the BubbleSched platform as a threading backend for the GOMP OpenMP compiler, we are able to easily transpose affinities of thread teams into scheduling hints using abstractions called bubbles. We then propose a scheduling strategy suited to nested OpenMP parallelism. The resulting preliminary performance evaluations show an important improvement of the speedup on a typical NAS OpenMP benchmark application.",
        "published": "2007-06-14T09:43:23Z",
        "link": "http://arxiv.org/abs/0706.2073v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Resource control of object-oriented programs",
        "authors": [
            "Jean-Yves Marion",
            "Romain Pechoux"
        ],
        "summary": "A sup-interpretation is a tool which provides an upper bound on the size of a value computed by some symbol of a program. Sup-interpretations have shown their interest to deal with the complexity of first order functional programs. For instance, they allow to characterize all the functions bitwise computable in Alogtime. This paper is an attempt to adapt the framework of sup-interpretations to a fragment of oriented-object programs, including distinct encodings of numbers through the use of constructor symbols, loop and while constructs and non recursive methods with side effects. We give a criterion, called brotherly criterion, which ensures that each brotherly program computes objects whose size is polynomially bounded by the inputs sizes.",
        "published": "2007-06-15T13:39:12Z",
        "link": "http://arxiv.org/abs/0706.2293v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.2"
        ]
    },
    {
        "title": "Une sémantique observationnelle du modèle des boîtes pour la   résolution de programmes logiques (version étendue)",
        "authors": [
            "Pierre Deransart",
            "Mireille Ducassé",
            "Gérard Ferrand"
        ],
        "summary": "This report specifies an observational semantics and gives an original presentation of the Byrd's box model. The approach accounts for the semantics of Prolog tracers independently of a particular implementation. Traces are, in general, considered as rather obscure and difficult to use. The proposed formal presentation of a trace constitutes a simple and pedagogical approach for teaching Prolog or for implementing Prolog tracers. It constitutes a form of declarative specification for the tracers. Our approach highlights qualities of the box model which made its success, but also its drawbacks and limits. As a matter of fact, the presented semantics is only one example to illustrate general problems relating to tracers and observing processes. Observing processes know, from observed processes, only their traces. The issue is then to be able to reconstitute by the sole analysis of the trace the main part of the observed process, and if possible, without any loss of information.",
        "published": "2007-06-21T14:20:30Z",
        "link": "http://arxiv.org/abs/0706.3159v2",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Theorem proving support in programming language semantics",
        "authors": [
            "Yves Bertot"
        ],
        "summary": "We describe several views of the semantics of a simple programming language as formal documents in the calculus of inductive constructions that can be verified by the Coq proof system. Covered aspects are natural semantics, denotational semantics, axiomatic semantics, and abstract interpretation. Descriptions as recursive functions are also provided whenever suitable, thus yielding a a verification condition generator and a static analyser that can be run inside the theorem prover for use in reflective proofs. Extraction of an interpreter from the denotational semantics is also described. All different aspects are formally proved sound with respect to the natural semantics specification.",
        "published": "2007-07-06T08:55:26Z",
        "link": "http://arxiv.org/abs/0707.0926v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Projection semantics for rigid loops",
        "authors": [
            "Jan A. Bergstra",
            "Alban Ponse"
        ],
        "summary": "A rigid loop is a for-loop with a counter not accessible to the loop body or any other part of a program. Special instructions for rigid loops are introduced on top of the syntax of the program algebra PGA. Two different semantic projections are provided and proven equivalent. One of these is taken to have definitional status on the basis of two criteria: `normative semantic adequacy' and `indicative algorithmic adequacy'.",
        "published": "2007-07-06T23:58:45Z",
        "link": "http://arxiv.org/abs/0707.1059v1",
        "categories": [
            "cs.PL",
            "D.2.4; D.3.1; F.3.2"
        ]
    },
    {
        "title": "Sequential products in effect categories",
        "authors": [
            "Jean-Guillaume Dumas",
            "Dominique Duval",
            "Jean-Claude Reynaud"
        ],
        "summary": "A new categorical framework is provided for dealing with multiple arguments in a programming language with effects, for example in a language with imperative features. Like related frameworks (Monads, Arrows, Freyd categories), we distinguish two kinds of functions. In addition, we also distinguish two kinds of equations. Then, we are able to define a kind of product, that generalizes the usual categorical product. This yields a powerful tool for deriving many results about languages with effects.",
        "published": "2007-07-10T12:50:35Z",
        "link": "http://arxiv.org/abs/0707.1432v1",
        "categories": [
            "math.CT",
            "cs.PL",
            "F.3.2; F.3.3"
        ]
    },
    {
        "title": "Programming Telepathy: Implementing Quantum Non-Locality Games",
        "authors": [
            "Anya Tafliovich",
            "Eric C. R. Hehner"
        ],
        "summary": "Quantum pseudo-telepathy is an intriguing phenomenon which results from the application of quantum information theory to communication complexity. To demonstrate this phenomenon researchers in the field of quantum communication complexity devised a number of quantum non-locality games. The setting of these games is as follows: the players are separated so that no communication between them is possible and are given a certain computational task. When the players have access to a quantum resource called entanglement, they can accomplish the task: something that is impossible in a classical setting. To an observer who is unfamiliar with the laws of quantum mechanics it seems that the players employ some sort of telepathy; that is, they somehow exchange information without sharing a communication channel. This paper provides a formal framework for specifying, implementing, and analysing quantum non-locality games.",
        "published": "2007-07-11T19:50:25Z",
        "link": "http://arxiv.org/abs/0707.1527v2",
        "categories": [
            "quant-ph",
            "cs.PL"
        ]
    },
    {
        "title": "How to be correct, lazy and efficient ?",
        "authors": [
            "Catherine Recanati"
        ],
        "summary": "This paper is an introduction to Lambdix, a lazy Lisp interpreter implemented at the Research Laboratory of Paris XI University (Laboratoire de Recherche en Informatique, Orsay). Lambdix was devised in the course of an investigation into the relationship between the semantics of programming languages and their implementation; it was used to demonstrate that in the Lisp domain, semantic correctness is consistent with efficiency, contrary to what has often been claimed. The first part of the paper is an overview of well-known semantic difficulties encountered by Lisp as well as an informal presentation of Lambdix; it is shown that the difficulties which Lisp encouters do not arise in Lambdix. The second part is about efficiency in implementation models. It explains why Lambdix is better suited for lazy evaluation than previous models. The section ends by giving comparative execution time tables.",
        "published": "2007-07-25T19:33:07Z",
        "link": "http://arxiv.org/abs/0707.3807v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Separation Logic for Small-step Cminor",
        "authors": [
            "Andrew W. Appel",
            "Sandrine Blazy"
        ],
        "summary": "Cminor is a mid-level imperative programming language; there are proved-correct optimizing compilers from C to Cminor and from Cminor to machine language. We have redesigned Cminor so that it is suitable for Hoare Logic reasoning and we have designed a Separation Logic for Cminor. In this paper, we give a small-step semantics (instead of the big-step of the proved-correct compiler) that is motivated by the need to support future concurrent extensions. We detail a machine-checked proof of soundness of our Separation Logic. This is the first large-scale machine-checked proof of a Separation Logic w.r.t. a small-step semantics. The work presented in this paper has been carried out in the Coq proof assistant. It is a first step towards an environment in which concurrent Cminor programs can be verified using Separation Logic and also compiled by a proved-correct compiler with formal end-to-end correctness guarantees.",
        "published": "2007-07-30T12:09:16Z",
        "link": "http://arxiv.org/abs/0707.4389v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Provenance as Dependency Analysis",
        "authors": [
            "James Cheney",
            "Amal Ahmed",
            "Umut Acar"
        ],
        "summary": "Provenance is information recording the source, derivation, or history of some information. Provenance tracking has been studied in a variety of settings; however, although many design points have been explored, the mathematical or semantic foundations of data provenance have received comparatively little attention. In this paper, we argue that dependency analysis techniques familiar from program analysis and program slicing provide a formal foundation for forms of provenance that are intended to show how (part of) the output of a query depends on (parts of) its input. We introduce a semantic characterization of such dependency provenance, show that this form of provenance is not computable, and provide dynamic and static approximation techniques.",
        "published": "2007-08-16T11:11:43Z",
        "link": "http://arxiv.org/abs/0708.2173v2",
        "categories": [
            "cs.DB",
            "cs.PL",
            "H.2.3; F.3.2"
        ]
    },
    {
        "title": "A Language for Generic Programming in the Large",
        "authors": [
            "Jeremy G. Siek",
            "Andrew Lumsdaine"
        ],
        "summary": "Generic programming is an effective methodology for developing reusable software libraries. Many programming languages provide generics and have features for describing interfaces, but none completely support the idioms used in generic programming. To address this need we developed the language G. The central feature of G is the concept, a mechanism for organizing constraints on generics that is inspired by the needs of modern C++ libraries. G provides modular type checking and separate compilation (even of generics). These characteristics support modular software development, especially the smooth integration of independently developed components. In this article we present the rationale for the design of G and demonstrate the expressiveness of G with two case studies: porting the Standard Template Library and the Boost Graph Library from C++ to G. The design of G shares much in common with the concept extension proposed for the next C++ Standard (the authors participated in its design) but there are important differences described in this article.",
        "published": "2007-08-16T18:06:18Z",
        "link": "http://arxiv.org/abs/0708.2255v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.3"
        ]
    },
    {
        "title": "A proof of strong normalisation using domain theory",
        "authors": [
            "Thierry Coquand",
            "Arnaud Spiwack"
        ],
        "summary": "Ulrich Berger presented a powerful proof of strong normalisation using domains, in particular it simplifies significantly Tait's proof of strong normalisation of Spector's bar recursion. The main contribution of this paper is to show that, using ideas from intersection types and Martin-Lof's domain interpretation of type theory one can in turn simplify further U. Berger's argument. We build a domain model for an untyped programming language where U. Berger has an interpretation only for typed terms or alternatively has an interpretation for untyped terms but need an extra condition to deduce strong normalisation. As a main application, we show that Martin-L\\\"{o}f dependent type theory extended with a program for Spector double negation shift.",
        "published": "2007-09-10T14:08:26Z",
        "link": "http://arxiv.org/abs/0709.1401v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.4.1"
        ]
    },
    {
        "title": "On the interaction between sharing and linearity",
        "authors": [
            "Gianluca Amato",
            "Francesca Scozzari"
        ],
        "summary": "In the analysis of logic programs, abstract domains for detecting sharing and linearity information are widely used. Devising abstract unification algorithms for such domains has proved to be rather hard. At the moment, the available algorithms are correct but not optimal, i.e., they cannot fully exploit the information conveyed by the abstract domains. In this paper, we define a new (infinite) domain ShLin-w which can be thought of as a general framework from which other domains can be easily derived by abstraction. ShLin-w makes the interaction between sharing and linearity explicit. We provide a constructive characterization of the optimal abstract unification operator on ShLin-w and we lift it to two well-known abstractions of ShLin-w. Namely, to the classical Sharing X Lin abstract domain and to the more precise ShLin-2 abstract domain by Andy King. In the case of single binding substitutions, we obtain optimal abstract unification algorithms for such domains.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2007-10-02T13:29:28Z",
        "link": "http://arxiv.org/abs/0710.0528v2",
        "categories": [
            "cs.PL",
            "cs.LO"
        ]
    },
    {
        "title": "Two algorithms in search of a type system",
        "authors": [
            "Norman Danner",
            "James S. Royer"
        ],
        "summary": "The authors' ATR programming formalism is a version of call-by-value PCF under a complexity-theoretically motivated type system. ATR programs run in type-2 polynomial-time and all standard type-2 basic feasible functionals are ATR-definable (ATR types are confined to levels 0, 1, and 2). A limitation of the original version of ATR is that the only directly expressible recursions are tail-recursions. Here we extend ATR so that a broad range of affine recursions are directly expressible. In particular, the revised ATR can fairly naturally express the classic insertion- and selection-sort algorithms, thus overcoming a sticking point of most prior implicit-complexity-based formalisms. The paper's main work is in refining the original time-complexity semantics for ATR to show that these new recursion schemes do not lead out of the realm of feasibility.",
        "published": "2007-10-03T16:04:59Z",
        "link": "http://arxiv.org/abs/0710.0824v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.3; F.1.3"
        ]
    },
    {
        "title": "Heap Reference Analysis for Functional Programs",
        "authors": [
            "Amey Karkare",
            "Amitabha Sanyal",
            "Uday Khedker"
        ],
        "summary": "Current garbage collectors leave a lot of garbage uncollected because they conservatively approximate liveness by reachability from program variables. In this paper, we describe a sequence of static analyses that takes as input a program written in a first-order, eager functional programming language, and finds at each program point the references to objects that are guaranteed not to be used in the future. Such references are made null by a transformation pass. If this makes the object unreachable, it can be collected by the garbage collector. This causes more garbage to be collected, resulting in fewer collections. Additionally, for those garbage collectors which scavenge live objects, it makes each collection faster.   The interesting aspects of our method are both in the identification of the analyses required to solve the problem and the way they are carried out. We identify three different analyses -- liveness, sharing and accessibility. In liveness and sharing analyses, the function definitions are analyzed independently of the calling context. This is achieved by using a variable to represent the unknown context of the function being analyzed and setting up constraints expressing the effect of the function with respect to the variable. The solution of the constraints is a summary of the function that is parameterized with respect to a calling context and is used to analyze function calls. As a result we achieve context sensitivity at call sites without analyzing the function multiple number of times.",
        "published": "2007-10-08T08:43:58Z",
        "link": "http://arxiv.org/abs/0710.1482v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.2; D.3.4; F.3.2"
        ]
    },
    {
        "title": "Success and failure of programming environments - report on the design   and use of a graphic abstract syntax tree editor",
        "authors": [
            "C. Recanati"
        ],
        "summary": "The STAPLE project investigated (at the end of the eighties), a persistent architecture for functional programming. Work has been done in two directions: the development of a programming environment for a functional language within a persistent system and an experiment on transferring the expertise of functional prototyping into industry. This paper is a report on the first activity. The first section gives a general description of Absynte - the abstract syntax tree editor developed within the Project. Following sections make an attempt at measuring the effectiveness of such an editor and discuss the problems raised by structured syntax editing - specially environments based on abstract syntax trees.",
        "published": "2007-10-11T23:40:47Z",
        "link": "http://arxiv.org/abs/0710.2358v1",
        "categories": [
            "cs.PL",
            "cs.HC"
        ]
    },
    {
        "title": "Implementation, Compilation, Optimization of Object-Oriented Languages,   Programs and Systems - Report on the Workshop ICOOOLPS'2006 at ECOOP'06",
        "authors": [
            "Roland Ducournau",
            "Etienne Gagnon",
            "Chandra Krintz",
            "Philippe Mulet",
            "Jan Vitek",
            "Olivier Zendra"
        ],
        "summary": "ICOOOLPS'2006 was the first edition of ECOOP-ICOOOLPS workshop. It intended to bring researchers and practitioners both from academia and industry together, with a spirit of openness, to try and identify and begin to address the numerous and very varied issues of optimization. This succeeded, as can be seen from the papers, the attendance and the liveliness of the discussions that took place during and after the workshop, not to mention a few new cooperations or postdoctoral contracts. The 22 talented people from different groups who participated were unanimous to appreciate this first edition and recommend that ICOOOLPS be continued next year. A community is thus beginning to form, and should be reinforced by a second edition next year, with all the improvements this first edition made emerge.",
        "published": "2007-10-15T17:53:49Z",
        "link": "http://arxiv.org/abs/0710.2887v1",
        "categories": [
            "cs.PF",
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "FORAY-GEN: Automatic Generation of Affine Functions for Memory   Optimizations",
        "authors": [
            "Ilya Issenin",
            "Nikil Dutt"
        ],
        "summary": "In today's embedded applications a significant portion of energy is spent in the memory subsystem. Several approaches have been proposed to minimize this energy, including the use of scratch pad memories, with many based on static analysis of a program. However, often it is not possible to perform static analysis and optimization of a program's memory access behavior unless the program is specifically written for this purpose. In this paper we introduce the FORAY model of a program that permits aggressive analysis of the application's memory behavior that further enables such optimizations since it consists of 'for' loops and array accesses which are easily analyzable. We present FORAY-GEN: an automated profile-based approach for extraction of the FORAY model from the original program. We also demonstrate how FORAY-GEN enhances applicability of other memory subsystem optimization approaches, resulting in an average of two times increase in the number of memory references that can be analyzed by existing static approaches.",
        "published": "2007-10-25T08:11:20Z",
        "link": "http://arxiv.org/abs/0710.4640v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "The Challenges of Hardware Synthesis from C-Like Languages",
        "authors": [
            "Stephen A. Edwards"
        ],
        "summary": "MANY TECHNIQUES for synthesizing digital hardware from C-like languages have been proposed, but none have emerged as successful as Verilog or VHDL for register-transfer-level design. This paper looks at two of the fundamental challenges: concurrency and timing control.",
        "published": "2007-10-25T09:07:39Z",
        "link": "http://arxiv.org/abs/0710.4683v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "A Register Allocation Algorithm in the Presence of Scalar Replacement   for Fine-Grain Configurable Architectures",
        "authors": [
            "Nastaran Baradaran",
            "Pedro C. Diniz"
        ],
        "summary": "The aggressive application of scalar replacement to array references substantially reduces the number of memory operations at the expense of a possibly very large number of registers. In this paper we describe a register allocation algorithm that assigns registers to scalar replaced array references along the critical paths of a computation, in many cases exploiting the opportunity for concurrent memory accesses. Experimental results, for a set of image/signal processing code kernels, reveal that the proposed algorithm leads to a substantial reduction of the number of execution cycles for the corresponding hardware implementation on a contemporary Field-Programmable-Gate-Array (FPGA) when compared to other greedy allocation algorithms, in some cases, using even fewer number of registers.",
        "published": "2007-10-25T09:27:20Z",
        "link": "http://arxiv.org/abs/0710.4702v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Querying XML Documents in Logic Programming",
        "authors": [
            "J. M. Almendros-Jiménez",
            "A. Becerra-Terón",
            "F. J. Enciso-Baños"
        ],
        "summary": "Extensible Markup Language (XML) is a simple, very flexible text format derived from SGML. Originally designed to meet the challenges of large-scale electronic publishing, XML is also playing an increasingly important role in the exchange of a wide variety of data on the Web and elsewhere. XPath language is the result of an effort to provide address parts of an XML document. In support of this primary purpose, it becomes in a query language against an XML document. In this paper we present a proposal for the implementation of the XPath language in logic programming. With this aim we will describe the representation of XML documents by means of a logic program. Rules and facts can be used for representing the document schema and the XML document itself. In particular, we will present how to index XML documents in logic programs: rules are supposed to be stored in main memory, however facts are stored in secondary memory by using two kind of indexes: one for each XML tag, and other for each group of terminal items. In addition, we will study how to query by means of the XPath language against a logic program representing an XML document. It evolves the specialization of the logic program with regard to the XPath expression. Finally, we will also explain how to combine the indexing and the top-down evaluation of the logic program. To appear in Theory and Practice of Logic Programming (TPLP)\"",
        "published": "2007-10-25T10:45:08Z",
        "link": "http://arxiv.org/abs/0710.4780v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "H.2.3; I.2.3"
        ]
    },
    {
        "title": "A Constraint Network Based Approach to Memory Layout Optimization",
        "authors": [
            "G. Chen",
            "M. Kandemir",
            "M. Karakoy"
        ],
        "summary": "While loop restructuring based code optimization for array intensive applications has been successful in the past, it has several problems such as the requirement of checking dependences (legality issues) and transformation of all of the array references within the loop body indiscriminately (while some of the references can benefit from the transformation, others may not). As a result, data transformations, i.e., transformations that modify memory layout of array data instead of loop structure have been proposed. One of the problems associated with data transformations is the difficulty of selecting a memory layout for an array that is acceptable to the entire program (not just to a single loop). In this paper, we formulate the problem of determining the memory layouts of arrays as a constraint network, and explore several methods of solution in a systematic way. Our experiments provide strong support in favor of employing constraint processing, and point out future research directions.",
        "published": "2007-10-25T11:59:01Z",
        "link": "http://arxiv.org/abs/0710.4807v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Source-to-source optimizing transformations of Prolog programs based on   abstract interpretation",
        "authors": [
            "Francois Gobert",
            "Baudouin Le Charlier"
        ],
        "summary": "Making a Prolog program more efficient by transforming its source code, without changing its operational semantics, is not an obvious task. It requires the user to have a clear understanding of how the Prolog compiler works, and in particular, of the effects of impure features like the cut. The way a Prolog code is written - e.g., the order of clauses, the order of literals in a clause, the use of cuts or negations - influences its efficiency. Furthermore, different optimization techniques may be redundant or conflicting when they are applied together, depending on the way a procedure is called - e.g., inserting cuts and enabling indexing. We present an optimiser, based on abstract interpretation, that automatically performs safe code transformations of Prolog procedures in the context of some class of input calls. The method is more effective if procedures are annotated with additional information about modes, types, sharing, number of solutions and the like. Thus the approach is similar to Mercury. It applies to any Prolog program, however.",
        "published": "2007-10-31T15:59:50Z",
        "link": "http://arxiv.org/abs/0710.5895v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Declarative Diagnosis of Floundering",
        "authors": [
            "Lee Naish"
        ],
        "summary": "Many logic programming languages have delay primitives which allow coroutining. This introduces a class of bug symptoms -- computations can flounder when they are intended to succeed or finitely fail. For concurrent logic programs this is normally called deadlock. Similarly, constraint logic programs can fail to invoke certain constraint solvers because variables are insufficiently instantiated or constrained. Diagnosing such faults has received relatively little attention to date. Since delay primitives affect the procedural but not the declarative view of programs, it may be expected that debugging would have to consider the often complex details of interleaved execution. However, recent work on semantics has suggested an alternative approach. In this paper we show how the declarative debugging paradigm can be used to diagnose unexpected floundering, insulating the user from the complexities of the execution.   Keywords: logic programming, coroutining, delay, debugging, floundering, deadlock, constraints",
        "published": "2007-11-01T01:40:50Z",
        "link": "http://arxiv.org/abs/0711.0048v1",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "A Prolog-based Environment for Reasoning about Programming Languages   (Extended abstract)",
        "authors": [
            "Roberto Bagnara",
            "Patricia Hill",
            "Enea Zaffanella"
        ],
        "summary": "ECLAIR is a Prolog-based prototype system aiming to provide a functionally complete environment for the study, development and evaluation of programming language analysis and implementation tools. In this paper, we sketch the overall structure of the system, outlining the main methodologies and technologies underlying its components. We also discuss the appropriateness of Prolog as the implementation language for the system: besides highlighting its strengths, we also point out a few potential weaknesses, hinting at possible solutions.",
        "published": "2007-11-02T16:40:10Z",
        "link": "http://arxiv.org/abs/0711.0345v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Compiling ER Specifications into Declarative Programs",
        "authors": [
            "Bernd Braßel",
            "Michael Hanus",
            "Marion Muller"
        ],
        "summary": "This paper proposes an environment to support high-level database programming in a declarative programming language. In order to ensure safe database updates, all access and update operations related to the database are generated from high-level descriptions in the entity- relationship (ER) model. We propose a representation of ER diagrams in the declarative language Curry so that they can be constructed by various tools and then translated into this representation. Furthermore, we have implemented a compiler from this representation into a Curry program that provides access and update operations based on a high-level API for database programming.",
        "published": "2007-11-02T16:49:30Z",
        "link": "http://arxiv.org/abs/0711.0348v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Automatic Coding Rule Conformance Checking Using Logic Programs",
        "authors": [
            "Guillem Marpons-Ucero",
            "Julio Mariño",
            "Ángel Herranz",
            "Lars-Åke Fredlund",
            "Manuel Carro",
            "Juan José Moreno-Navarro"
        ],
        "summary": "Some approaches to increasing program reliability involve a disciplined use of programming languages so as to minimise the hazards introduced by error-prone features. This is realised by writing code that is constrained to a subset of the a priori admissible programs, and that, moreover, may use only a subset of the language. These subsets are determined by a collection of so-called coding rules.",
        "published": "2007-11-02T16:53:34Z",
        "link": "http://arxiv.org/abs/0711.0344v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "PIDoc: Wiki style Literate Programming for Prolog",
        "authors": [
            "Jan Wielemaker",
            "Anjo Anjewierden"
        ],
        "summary": "This document introduces PlDoc, a literate programming system for Prolog. Starting point for PlDoc was minimal distraction from the programming task and maximal immediate reward, attempting to seduce the programmer to use the system. Minimal distraction is achieved using structured comments that are as closely as possible related to common Prolog documentation practices. Immediate reward is provided by a web interface powered from the Prolog development environment that integrates searching and browsing application and system documentation. When accessed from localhost, it is possible to go from documentation shown in a browser to the source code displayed in the user's editor of choice.",
        "published": "2007-11-05T12:13:12Z",
        "link": "http://arxiv.org/abs/0711.0618v1",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Instruction sequences with indirect jumps",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "We study sequential programs that are instruction sequences with direct and indirect jump instructions. The intuition is that indirect jump instructions are jump instructions where the position of the instruction to jump to is the content of some memory cell. We consider several kinds of indirect jump instructions. For each kind, we define the meaning of programs with indirect jump instructions of that kind by means of a translation into programs without indirect jump instructions. For each kind, the intended behaviour of a program with indirect jump instructions of that kind under execution is the behaviour of the translated program under execution on interaction with some memory device.",
        "published": "2007-11-06T10:26:03Z",
        "link": "http://arxiv.org/abs/0711.0829v2",
        "categories": [
            "cs.PL",
            "D.3.1; D.3.3; F.1.1; F.3.2; F.3.3"
        ]
    },
    {
        "title": "SWI-Prolog and the Web",
        "authors": [
            "Jan Wielemaker",
            "Zhisheng Huang",
            "Lourens van der Meij"
        ],
        "summary": "Where Prolog is commonly seen as a component in a Web application that is either embedded or communicates using a proprietary protocol, we propose an architecture where Prolog communicates to other components in a Web application using the standard HTTP protocol. By avoiding embedding in external Web servers development and deployment become much easier. To support this architecture, in addition to the transfer protocol, we must also support parsing, representing and generating the key Web document types such as HTML, XML and RDF.   This paper motivates the design decisions in the libraries and extensions to Prolog for handling Web documents and protocols. The design has been guided by the requirement to handle large documents efficiently. The described libraries support a wide range of Web applications ranging from HTML and XML documents to Semantic Web RDF processing.   To appear in Theory and Practice of Logic Programming (TPLP)",
        "published": "2007-11-06T16:22:39Z",
        "link": "http://arxiv.org/abs/0711.0917v1",
        "categories": [
            "cs.PL",
            "cs.SC"
        ]
    },
    {
        "title": "Observational semantics of the Prolog Resolution Box Model",
        "authors": [
            "Pierre Deransart",
            "Mireille Ducassé",
            "Gérard Ferrand"
        ],
        "summary": "This paper specifies an observational semantics and gives an original presentation of the Byrd box model. The approach accounts for the semantics of Prolog tracers independently of a particular Prolog implementation. Prolog traces are, in general, considered as rather obscure and difficult to use. The proposed formal presentation of its trace constitutes a simple and pedagogical approach for teaching Prolog or for implementing Prolog tracers. It is a form of declarative specification for the tracers. The trace model introduced here is only one example to illustrate general problems relating to tracers and observing processes. Observing processes know, from observed processes, only their traces. The issue is then to be able to reconstitute, by the sole analysis of the trace, part of the behaviour of the observed process, and if possible, without any loss of information. As a matter of fact, our approach highlights qualities of the Prolog resolution box model which made its success, but also its insufficiencies.",
        "published": "2007-11-26T18:03:07Z",
        "link": "http://arxiv.org/abs/0711.4071v1",
        "categories": [
            "cs.PL",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Instruction sequences with dynamically instantiated instructions",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "We study sequential programs that are instruction sequences with dynamically instantiated instructions. We define the meaning of such programs in two different ways. In either case, we give a translation by which each program with dynamically instantiated instructions is turned into a program without them that exhibits on execution the same behaviour by interaction with some service. The complexity of the translations differ considerably, whereas the services concerned are equally simple. However, the service concerned in the case of the simpler translation is far more powerful than the service concerned in the other case.",
        "published": "2007-11-27T10:24:34Z",
        "link": "http://arxiv.org/abs/0711.4217v4",
        "categories": [
            "cs.PL",
            "D.3.1; D.3.3; F.1.1; F.3.2; F.3.3"
        ]
    },
    {
        "title": "Implementation, Compilation, Optimization of Object-Oriented Languages,   Programs and Systems - Report on the Workshop ICOOOLPS'2007 at ECOOP'07",
        "authors": [
            "Olivier Zendra",
            "Eric Jul",
            "Roland Ducournau",
            "Etienne Gagnon",
            "Richard E. Jones",
            "Chandra Krintz",
            "Philippe Mulet",
            "Jan Vitek"
        ],
        "summary": "ICOOOLPS'2007 was the second edition of the ECOOP-ICOOOLPS workshop. ICOOOLPS intends to bring researchers and practitioners both from academia and industry together, with a spirit of openness, to try and identify and begin to address the numerous and very varied issues of optimization. After a first successful edition, this second one put a stronger emphasis on exchanges and discussions amongst the participants, progressing on the bases set last year in Nantes. The workshop attendance was a success, since the 30-people limit we had set was reached about 2 weeks before the workshop itself. Some of the discussions (e.g. annotations) were so successful that they would required even more time than we were able to dedicate to them. That's one area we plan to further improve for the next edition.",
        "published": "2007-12-07T17:01:52Z",
        "link": "http://arxiv.org/abs/0712.1189v1",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Lambda-RBAC: Programming with Role-Based Access Control",
        "authors": [
            "Radha Jagadeesan",
            "Alan Jeffrey",
            "Corin Pitcher",
            "James Riely"
        ],
        "summary": "We study mechanisms that permit program components to express role constraints on clients, focusing on programmatic security mechanisms, which permit access controls to be expressed, in situ, as part of the code realizing basic functionality. In this setting, two questions immediately arise: (1) The user of a component faces the issue of safety: is a particular role sufficient to use the component? (2) The component designer faces the dual issue of protection: is a particular role demanded in all execution paths of the component? We provide a formal calculus and static analysis to answer both questions.",
        "published": "2007-12-07T18:58:35Z",
        "link": "http://arxiv.org/abs/0712.1205v2",
        "categories": [
            "cs.PL",
            "cs.CR",
            "F.4.1; F.3.2"
        ]
    },
    {
        "title": "Program algebra with a jump-shift instruction",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "We study sequential programs that are instruction sequences with jump-shift instructions in the setting of PGA (ProGram Algebra). Jump-shift instructions preceding a jump instruction increase the position to jump to. The jump-shift instruction is not found in programming practice. Its merit is that the expressive power of PGA extended with the jump-shift instruction, is not reduced if the reach of jump instructions is bounded. This is used to show that there exists a finite-state execution mechanism that by making use of a counter can produce each finite-state thread from some program that is a finite or periodic infinite sequence of instructions from a finite set.",
        "published": "2007-12-11T08:53:49Z",
        "link": "http://arxiv.org/abs/0712.1658v1",
        "categories": [
            "cs.PL",
            "D.3.1; D.3.3; F.1.1; F.3.2; F.3.3"
        ]
    },
    {
        "title": "Experiments with a Convex Polyhedral Analysis Tool for Logic Programs",
        "authors": [
            "Kim Henriksen",
            "Gourinath Banda",
            "John Gallagher"
        ],
        "summary": "Convex polyhedral abstractions of logic programs have been found very useful in deriving numeric relationships between program arguments in order to prove program properties and in other areas such as termination and complexity analysis. We present a tool for constructing polyhedral analyses of (constraint) logic programs. The aim of the tool is to make available, with a convenient interface, state-of-the-art techniques for polyhedral analysis such as delayed widening, narrowing, \"widening up-to\", and enhanced automatic selection of widening points. The tool is accessible on the web, permits user programs to be uploaded and analysed, and is integrated with related program transformations such as size abstractions and query-answer transformation. We then report some experiments using the tool, showing how it can be conveniently used to analyse transition systems arising from models of embedded systems, and an emulator for a PIC microcontroller which is used for example in wearable computing systems. We discuss issues including scalability, tradeoffs of precision and computation time, and other program transformations that can enhance the results of analysis.",
        "published": "2007-12-17T15:11:36Z",
        "link": "http://arxiv.org/abs/0712.2737v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Evolving XSLT stylesheets",
        "authors": [
            "Nestor Zorzano",
            "Daniel Merino",
            "J. L. J. Laredo",
            "J. P. Sevilla",
            "Pablo Garcia",
            "J. J. Merelo"
        ],
        "summary": "This paper introduces a procedure based on genetic programming to evolve XSLT programs (usually called stylesheets or logicsheets). XSLT is a general purpose, document-oriented functional language, generally used to transform XML documents (or, in general, solve any problem that can be coded as an XML document). The proposed solution uses a tree representation for the stylesheets as well as diverse specific operators in order to obtain, in the studied cases and a reasonable time, a XSLT stylesheet that performs the transformation. Several types of representation have been compared, resulting in different performance and degree of success.",
        "published": "2007-12-17T19:59:42Z",
        "link": "http://arxiv.org/abs/0712.2630v1",
        "categories": [
            "cs.NE",
            "cs.PL"
        ]
    },
    {
        "title": "Optimizing Queries in a Logic-based Information Integration System",
        "authors": [
            "András Gyorgy Békés",
            "Péter Szeredi"
        ],
        "summary": "The SINTAGMA information integration system is an infrastructure for accessing several different information sources together. Besides providing a uniform interface to the information sources (databases, web services, web sites, RDF resources, XML files), semantic integration is also needed. Semantic integration is carried out by providing a high-level model and the mappings to the models of the sources. When executing a query of the high level model, a query is transformed to a low-level query plan, which is a piece of Prolog code that answers the high-level query. This transformation is done in two phases. First, the Query Planner produces a plan as a logic formula expressing the low-level query. Next, the Query Optimizer transforms this formula to executable Prolog code and optimizes it according to structural and statistical information about the information sources.   This article discusses the main ideas of the optimization algorithm and its implementation.",
        "published": "2007-12-19T08:07:30Z",
        "link": "http://arxiv.org/abs/0712.3113v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Proceedings of the 17th Workshop on Logic-based methods in Programming   Environments (WLPE 2007)",
        "authors": [
            "Patricia Hill",
            "Wim Vanhoof"
        ],
        "summary": "This volume contains the papers presented at WLPE 2007: the 17th Workshop on Logic-based Methods in Programming Environments on 13th September, 2007 in Porto, Portugal. It was held as a satellite workshop of ICLP 2007, the 23th International Conference on Logic Programming.",
        "published": "2007-12-19T08:28:12Z",
        "link": "http://arxiv.org/abs/0712.3116v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Clones and Genoids in Lambda Calculus and First Order Logic",
        "authors": [
            "Zhaohua Luo"
        ],
        "summary": "A genoid is a category of two objects such that one is the product of itself with the other. A genoid may be viewed as an abstract substitution algebra. It is a remarkable fact that such a simple concept can be applied to present a unified algebraic approach to lambda calculus and first order logic.",
        "published": "2007-12-19T20:52:48Z",
        "link": "http://arxiv.org/abs/0712.3088v3",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "TCHR: a framework for tabled CLP",
        "authors": [
            "Tom Schrijvers",
            "Bart Demoen",
            "David S. Warren"
        ],
        "summary": "Tabled Constraint Logic Programming is a powerful execution mechanism for dealing with Constraint Logic Programming without worrying about fixpoint computation. Various applications, e.g in the fields of program analysis and model checking, have been proposed. Unfortunately, a high-level system for developing new applications is lacking, and programmers are forced to resort to complicated ad hoc solutions.   This papers presents TCHR, a high-level framework for tabled Constraint Logic Programming. It integrates in a light-weight manner Constraint Handling Rules (CHR), a high-level language for constraint solvers, with tabled Logic Programming. The framework is easily instantiated with new application-specific constraint domains. Various high-level operations can be instantiated to control performance. In particular, we propose a novel, generalized technique for compacting answer sets.",
        "published": "2007-12-26T15:28:16Z",
        "link": "http://arxiv.org/abs/0712.3830v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "An Approach to Programming Based on Concepts",
        "authors": [
            "Alexandr Savinov"
        ],
        "summary": "In this paper we describe a new approach to programming which generalizes object-oriented programming. It is based on using a new programming construct, called concept, which generalizes classes. Concept is defined as a pair of two classes: one reference class and one object class. Each concept has a parent concept which is specified using inclusion relation generalizing inheritance. We describe several important mechanisms such as reference resolution, context stack, dual methods and life-cycle management, inheritance and polymorphism. This approach to programming is positioned as a new programming paradigm and therefore we formulate its main principles and rules.",
        "published": "2007-12-30T14:43:27Z",
        "link": "http://arxiv.org/abs/0801.0133v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Concepts and their Use for Modelling Objects and References in   Programming Languages",
        "authors": [
            "Alexandr Savinov"
        ],
        "summary": "In the paper a new programming construct, called concept, is introduced. Concept is pair of two classes: a reference class and an object class. Instances of the reference classes are passed-by-value and are intended to represent objects. Instances of the object class are passed-by-reference. An approach to programming where concepts are used instead of classes is called concept-oriented programming (CoP). In CoP objects are represented and accessed indirectly by means of references. The structure of concepts describes a hierarchical space with a virtual address system. The paper describes this new approach to programming including such mechanisms as reference resolution, complex references, method interception, dual methods, life-cycle management inheritance and polymorphism.",
        "published": "2007-12-30T14:50:01Z",
        "link": "http://arxiv.org/abs/0801.0135v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Indirect Object Representation and Access by Means of Concepts",
        "authors": [
            "Alexandr Savinov"
        ],
        "summary": "The paper describes a mechanism for indirect object representation and access (ORA) in programming languages. The mechanism is based on using a new programming construct which is referred to as concept. Concept consists of one object class and one reference class both having their fields and methods. The object class is the conventional class as defined in OOP with instances passed by reference. Instances of the reference class are passed by value and are intended to represent objects. The reference classes are used to describe how objects have to be represented and accessed by providing custom format for their identifiers and custom access procedures. Such an approach to programming where concepts are used instead of classes is referred to as concept-oriented programming. It generalizes OOP and its main advantage is that it allows the programmer to describe not only the functionality of target objects but also intermediate functions which are executed behind the scenes as an object is being accessed.",
        "published": "2007-12-30T14:56:05Z",
        "link": "http://arxiv.org/abs/0801.0136v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Polygon Convexity: Another O(n) Test",
        "authors": [
            "Iosif Pinelis"
        ],
        "summary": "An n-gon is defined as a sequence \\P=(V_0,...,V_{n-1}) of n points on the plane. An n-gon \\P is said to be convex if the boundary of the convex hull of the set {V_0,...,V_{n-1}} of the vertices of \\P coincides with the union of the edges [V_0,V_1],...,[V_{n-1},V_0]; if at that no three vertices of \\P are collinear then \\P is called strictly convex. We prove that an n-gon \\P with n\\ge3 is strictly convex if and only if a cyclic shift of the sequence (\\al_0,...,\\al_{n-1})\\in[0,2\\pi)^n of the angles between the x-axis and the vectors V_1-V_0,...,V_0-V_{n-1} is strictly monotone. A ``non-strict'' version of this result is also proved.",
        "published": "2007-01-08T18:51:37Z",
        "link": "http://arxiv.org/abs/cs/0701045v2",
        "categories": [
            "cs.CG",
            "cs.DS",
            "I.3.5; F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "About the domino problem in the hyperbolic plane, a new solution",
        "authors": [
            "Margenstern Maurice"
        ],
        "summary": "In this paper we improve the approach of a previous paper about the domino problem in the hyperbolic plane, see arXiv.cs.CG/0603093. This time, we prove that the general problem of the hyperbolic plane with \\`a la Wang tiles is undecidable.",
        "published": "2007-01-16T18:19:22Z",
        "link": "http://arxiv.org/abs/cs/0701096v2",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Signature Sequence of Intersection Curve of Two Quadrics for Exact   Morphological Classification",
        "authors": [
            "Changhe Tu",
            "Wenping Wang",
            "Bernard Mourrain",
            "Jiaye Wang"
        ],
        "summary": "We present an efficient method for classifying the morphology of the intersection curve of two quadrics (QSIC) in PR3, 3D real projective space; here, the term morphology is used in a broad sense to mean the shape, topological, and algebraic properties of a QSIC, including singularity, reducibility, the number of connected components, and the degree of each irreducible component, etc. There are in total 35 different QSIC morphologies with non-degenerate quadric pencils. For each of these 35 QSIC morphologies, through a detailed study of the eigenvalue curve and the index function jump we establish a characterizing algebraic condition expressed in terms of the Segre characteristics and the signature sequence of a quadric pencil. We show how to compute a signature sequence with rational arithmetic so as to determine the morphology of the intersection curve of any two given quadrics. Two immediate applications of our results are the robust topological classification of QSIC in computing B-rep surface representation in solid modeling and the derivation of algebraic conditions for collision detection of quadric primitives.",
        "published": "2007-01-19T08:08:24Z",
        "link": "http://arxiv.org/abs/cs/0701121v1",
        "categories": [
            "cs.CG",
            "cs.SC"
        ]
    },
    {
        "title": "Applications of Polyhedral Computations to the Analysis and Verification   of Hardware and Software Systems",
        "authors": [
            "Roberto Bagnara",
            "Patricia M. Hill",
            "Enea Zaffanella"
        ],
        "summary": "Convex polyhedra are the basis for several abstractions used in static analysis and computer-aided verification of complex and sometimes mission critical systems. For such applications, the identification of an appropriate complexity-precision trade-off is a particularly acute problem, so that the availability of a wide spectrum of alternative solutions is mandatory. We survey the range of applications of polyhedral computations in this area; give an overview of the different classes of polyhedra that may be adopted; outline the main polyhedral operations required by automatic analyzers and verifiers; and look at some possible combinations of polyhedra with other numerical abstractions that have the potential to improve the precision of the analysis. Areas where further theoretical investigations can result in important contributions are highlighted.",
        "published": "2007-01-19T08:39:34Z",
        "link": "http://arxiv.org/abs/cs/0701122v2",
        "categories": [
            "cs.CG",
            "cs.MS",
            "D.2.4; F.3.1"
        ]
    },
    {
        "title": "A polynomial time algorithm to approximate the mixed volume within a   simply exponential factor",
        "authors": [
            "Leonid Gurvits"
        ],
        "summary": "Let ${\\bf K} = (K_1, ..., K_n)$ be an $n$-tuple of convex compact subsets in the Euclidean space $\\R^n$, and let $V(\\cdot)$ be the Euclidean volume in $\\R^n$. The Minkowski polynomial $V_{{\\bf K}}$ is defined as $V_{{\\bf K}}(\\lambda_1, ... ,\\lambda_n) = V(\\lambda_1 K_1 +, ..., + \\lambda_n K_n)$ and the mixed volume $V(K_1, ..., K_n)$ as $$ V(K_1, ..., K_n) = \\frac{\\partial^n}{\\partial \\lambda_1...\\partial \\lambda_n} V_{{\\bf K}}(\\lambda_1 K_1 +, ..., + \\lambda_n K_n). $$ Our main result is a poly-time algorithm which approximates $V(K_1, ..., K_n)$ with multiplicative error $e^n$ and with better rates if the affine dimensions of most of the sets $K_i$ are small. Our approach is based on a particular approximation of $\\log(V(K_1, ..., K_n))$ by a solution of some convex minimization problem. We prove the mixed volume analogues of the Van der Waerden and Schrijver-Valiant conjectures on the permanent. These results, interesting on their own, allow us to justify the abovementioned approximation by a convex minimization, which is solved using the ellipsoid method and a randomized poly-time time algorithm for the approximation of the volume of a convex set.",
        "published": "2007-02-02T01:09:36Z",
        "link": "http://arxiv.org/abs/cs/0702013v4",
        "categories": [
            "cs.CG",
            "cs.CC",
            "math.CO"
        ]
    },
    {
        "title": "Pebble Game Algorithms and Sparse Graphs",
        "authors": [
            "Audrey Lee",
            "Ileana Streinu"
        ],
        "summary": "A multi-graph $G$ on $n$ vertices is $(k,\\ell)$-sparse if every subset of $n'\\leq n$ vertices spans at most $kn'- \\ell$ edges. $G$ is {\\em tight} if, in addition, it has exactly $kn - \\ell$ edges. For integer values $k$ and $\\ell \\in [0, 2k)$, we characterize the $(k,\\ell)$-sparse graphs via a family of simple, elegant and efficient algorithms called the $(k,\\ell)$-pebble games.",
        "published": "2007-02-06T01:18:00Z",
        "link": "http://arxiv.org/abs/math/0702129v1",
        "categories": [
            "math.CO",
            "cs.CG",
            "52C25;68R10;05C85"
        ]
    },
    {
        "title": "Hadwiger and Helly-type theorems for disjoint unit spheres",
        "authors": [
            "Otfried Cheong",
            "Xavier Goaoc",
            "Andreas Holmsen",
            "Sylvain Petitjean"
        ],
        "summary": "We prove Helly-type theorems for line transversals to disjoint unit balls in $\\R^{d}$. In particular, we show that a family of $n \\geq 2d$ disjoint unit balls in $\\R^d$ has a line transversal if, for some ordering $\\prec$ of the balls, any subfamily of 2d balls admits a line transversal consistent with $\\prec$. We also prove that a family of $n \\geq 4d-1$ disjoint unit balls in $\\R^d$ admits a line transversal if any subfamily of size $4d-1$ admits a transversal.",
        "published": "2007-02-07T08:29:03Z",
        "link": "http://arxiv.org/abs/cs/0702039v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Polyhedral representation conversion up to symmetries",
        "authors": [
            "David Bremner",
            "Mathieu Dutour Sikiric",
            "Achill Schuermann"
        ],
        "summary": "We give a short survey on computational techniques which can be used to solve the representation conversion problem for polyhedra up to symmetries. We in particular discuss decomposition methods, which reduce the problem to a number of lower dimensional subproblems. These methods have been successfully used by different authors in special contexts. Moreover, we sketch an incremental method, which is a generalization of Fourier-Motzkin elimination, and we give some ideas how symmetry can be exploited using pivots.",
        "published": "2007-02-09T01:33:42Z",
        "link": "http://arxiv.org/abs/math/0702239v2",
        "categories": [
            "math.MG",
            "cs.CG",
            "68W05"
        ]
    },
    {
        "title": "Sparse geometric graphs with small dilation",
        "authors": [
            "Boris Aronov",
            "Mark de Berg",
            "Otfried Cheong",
            "Joachim Gudmundsson",
            "Herman Haverkort",
            "Michiel Smid",
            "Antoine Vigneron"
        ],
        "summary": "Given a set S of n points in R^D, and an integer k such that 0 <= k < n, we show that a geometric graph with vertex set S, at most n - 1 + k edges, maximum degree five, and dilation O(n / (k+1)) can be computed in time O(n log n). For any k, we also construct planar n-point sets for which any geometric graph with n-1+k edges has dilation Omega(n/(k+1)); a slightly weaker statement holds if the points of S are required to be in convex position.",
        "published": "2007-02-14T03:46:54Z",
        "link": "http://arxiv.org/abs/cs/0702080v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "An Upper Bound on the Average Size of Silhouettes",
        "authors": [
            "Marc Glisse",
            "Sylvain Lazard"
        ],
        "summary": "It is a widely observed phenomenon in computer graphics that the size of the silhouette of a polyhedron is much smaller than the size of the whole polyhedron. This paper provides, for the first time, theoretical evidence supporting this for a large class of objects, namely for polyhedra that approximate surfaces in some reasonable way; the surfaces may be non-convex and non-differentiable and they may have boundaries. We prove that such polyhedra have silhouettes of expected size $O(\\sqrt{n})$ where the average is taken over all points of view and n is the complexity of the polyhedron.",
        "published": "2007-02-14T13:52:35Z",
        "link": "http://arxiv.org/abs/cs/0702087v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "The Hadwiger Number of Jordan Regions is Unbounded",
        "authors": [
            "Otfried Cheong",
            "Mira Lee"
        ],
        "summary": "We show that for every n > 0 there is a planar topological disk A_0 and n translates A_1, A_2, ..., A_n of A_0 such that the interiors of A_0, ... A_n are pairwise disjoint, but with each A_i touching A_0 for 1 <= i <= n.",
        "published": "2007-02-14T15:31:30Z",
        "link": "http://arxiv.org/abs/cs/0702079v1",
        "categories": [
            "cs.CG",
            "math.MG"
        ]
    },
    {
        "title": "Aperture-Angle and Hausdorff-Approximation of Convex Figures",
        "authors": [
            "Hee-Kap Ahn",
            "Sang Won Bae",
            "Otfried Cheong",
            "Joachim Gudmundsson"
        ],
        "summary": "The aperture angle alpha(x, Q) of a point x not in Q in the plane with respect to a convex polygon Q is the angle of the smallest cone with apex x that contains Q. The aperture angle approximation error of a compact convex set C in the plane with respect to an inscribed convex polygon Q of C is the minimum aperture angle of any x in C Q with respect to Q. We show that for any compact convex set C in the plane and any k > 2, there is an inscribed convex k-gon Q of C with aperture angle approximation error (1 - 2/(k+1)) pi. This bound is optimal, and settles a conjecture by Fekete from the early 1990s. The same proof technique can be used to prove a conjecture by Brass: If a polygon P admits no approximation by a sub-k-gon (the convex hull of k vertices of P) with Hausdorff distance sigma, but all subpolygons of P (the convex hull of some vertices of P) admit such an approximation, then P is a (k+1)-gon. This implies the following result: For any k > 2 and any convex polygon P of perimeter at most 1 there is a sub-k-gon Q of P such that the Hausdorff-distance of P and Q is at most 1/(k+1) * sin(pi/(k+1)).",
        "published": "2007-02-16T06:00:08Z",
        "link": "http://arxiv.org/abs/cs/0702090v2",
        "categories": [
            "cs.CG",
            "math.MG"
        ]
    },
    {
        "title": "On a family of strong geometric spanners that admit local routing   strategies",
        "authors": [
            "Prosenjit Bose",
            "Paz Carmi",
            "Mathieu Couture",
            "Michiel Smid",
            "Daming Xu"
        ],
        "summary": "We introduce a family of directed geometric graphs, denoted $\\paz$, that depend on two parameters $\\lambda$ and $\\theta$. For $0\\leq \\theta<\\frac{\\pi}{2}$ and ${1/2} < \\lambda < 1$, the $\\paz$ graph is a strong $t$-spanner, with $t=\\frac{1}{(1-\\lambda)\\cos\\theta}$. The out-degree of a node in the $\\paz$ graph is at most $\\lfloor2\\pi/\\min(\\theta, \\arccos\\frac{1}{2\\lambda})\\rfloor$. Moreover, we show that routing can be achieved locally on $\\paz$. Next, we show that all strong $t$-spanners are also $t$-spanners of the unit disk graph. Simulations for various values of the parameters $\\lambda$ and $\\theta$ indicate that for random point sets, the spanning ratio of $\\paz$ is better than the proven theoretical bounds.",
        "published": "2007-02-20T20:54:16Z",
        "link": "http://arxiv.org/abs/cs/0702117v3",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "On a characterization of cellular automata in tilings of the hyperbolic   plane",
        "authors": [
            "Maurice Margenstern"
        ],
        "summary": "In this paper, we look at the extention of Hedlund's characterization of cellular automata to the case of cellular automata in the hyperbolic plane. This requires an additionnal condition. The new theorem is proved with full details in the case of the pentagrid and in the case of the ternary heptagrid and enough indications to show that it holds also on the grids $\\{p,q\\}$ of the hyperbolic plane.",
        "published": "2007-02-27T09:17:05Z",
        "link": "http://arxiv.org/abs/cs/0702155v1",
        "categories": [
            "cs.DM",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Can we Compute the Similarity Between Surfaces?",
        "authors": [
            "Helmut Alt",
            "Maike Buchin"
        ],
        "summary": "A suitable measure for the similarity of shapes represented by parameterized curves or surfaces is the Fr\\'echet distance. Whereas efficient algorithms are known for computing the Fr\\'echet distance of polygonal curves, the same problem for triangulated surfaces is NP-hard. Furthermore, it remained open whether it is computable at all. Here, using a discrete approximation we show that it is {\\em upper semi-computable}, i.e., there is a non-halting Turing machine which produces a monotone decreasing sequence of rationals converging to the result. It follows that the decision problem, whether the Fr\\'echet distance of two given surfaces lies below some specified value, is recursively enumerable.   Furthermore, we show that a relaxed version of the problem, the computation of the {\\em weak Fr\\'echet distance} can be solved in polynomial time. For this, we give a computable characterization of the weak Fr\\'echet distance in a geometric data structure called the {\\em free space diagram}.",
        "published": "2007-03-02T17:00:18Z",
        "link": "http://arxiv.org/abs/cs/0703011v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "F.2.2; F.1.1"
        ]
    },
    {
        "title": "Computing a Minimum-Dilation Spanning Tree is NP-hard",
        "authors": [
            "Otfried Cheong",
            "Herman Haverkort",
            "Mira Lee"
        ],
        "summary": "In a geometric network G = (S, E), the graph distance between two vertices u, v in S is the length of the shortest path in G connecting u to v. The dilation of G is the maximum factor by which the graph distance of a pair of vertices differs from their Euclidean distance. We show that given a set S of n points with integer coordinates in the plane and a rational dilation delta > 1, it is NP-hard to determine whether a spanning tree of S with dilation at most delta exists.",
        "published": "2007-03-06T01:53:36Z",
        "link": "http://arxiv.org/abs/cs/0703023v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "An Efficient Local Approach to Convexity Testing of Piecewise-Linear   Hypersurfaces",
        "authors": [
            "Konstantin Rybnikov"
        ],
        "summary": "We show that a closed piecewise-linear hypersurface immersed in $R^n$ ($n\\ge 3$) is the boundary of a convex body if and only if every point in the interior of each $(n-3)$-face has a neighborhood that lies on the boundary of some convex body; no assumptions about the hypersurface's topology are needed. We derive this criterion from our generalization of Van Heijenoort's (1952) theorem on locally convex hypersurfaces in $R^n$ to spherical spaces. We also give an easy-to-implement convexity testing algorithm, which is based on our criterion. For $R^3$ the number of arithmetic operations used by the algorithm is at most linear in the number of vertices, while in general it is at most linear in the number of incidences between the $(n-2)$-faces and $(n-3)$-faces. When the dimension $n$ is not fixed and only ring arithmetic is allowed, the algorithm still remains polynomial. Our method works in more general situations than the convexity verification algorithms developed by Mehlhorn et al. (1996) and Devillers et al. (1998) -- for example, our method does not require the input surface to be orientable, nor it requires the input data to include normal vectors to the facets that are oriented \"in a coherent way\". For $R^3$ the complexity of our algorithm is the same as that of previous algorithms; for higher dimensions there seems to be no clear winner, but our approach is the only one that easily handles inputs in which the facet normals are not known to be coherently oriented or are not given at all. Furthermore, our method can be extended to piecewise-polynomial surfaces of small degree.",
        "published": "2007-03-07T07:33:02Z",
        "link": "http://arxiv.org/abs/cs/0703030v1",
        "categories": [
            "cs.CG",
            "D.2.4; I.3.5"
        ]
    },
    {
        "title": "Constructing Optimal Highways",
        "authors": [
            "Hee-Kap Ahn",
            "Helmut Alt",
            "Tetsuo Asano",
            "Sang Won Bae",
            "Peter Brass",
            "Otfried Cheong",
            "Christian Knauer",
            "Hyeon-Suk Na",
            "Chan-Su Shin",
            "Alexander Wolff"
        ],
        "summary": "For two points $p$ and $q$ in the plane, a straight line $h$, called a highway, and a real $v>1$, we define the \\emph{travel time} (also known as the \\emph{city distance}) from $p$ and $q$ to be the time needed to traverse a quickest path from $p$ to $q$, where the distance is measured with speed $v$ on $h$ and with speed 1 in the underlying metric elsewhere.   Given a set $S$ of $n$ points in the plane and a highway speed $v$, we consider the problem of finding a \\emph{highway} that minimizes the maximum travel time over all pairs of points in $S$. If the orientation of the highway is fixed, the optimal highway can be computed in linear time, both for the $L_1$- and the Euclidean metric as the underlying metric. If arbitrary orientations are allowed, then the optimal highway can be computed in $O(n^{2} \\log n)$ time. We also consider the problem of computing an optimal pair of highways, one being horizontal, one vertical.",
        "published": "2007-03-08T17:22:21Z",
        "link": "http://arxiv.org/abs/cs/0703037v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Space Program Language (SPL/SQL) for the Relational Approach of the   Spatial Databases",
        "authors": [
            "Ignacio Vega-Paez",
            "Feliu D. Sagols T"
        ],
        "summary": "In this project we are presenting a grammar which unify the design and development of spatial databases. In order to make it, we combine nominal and spatial information, the former is represented by the relational model and latter by a modification of the same model. The modification lets to represent spatial data structures (as Quadtrees, Octrees, etc.) in a integrated way. This grammar is important because with it we can create tools to build systems that combine spatial-nominal characteristics such as Geographical Information Systems (GIS), Hypermedia Systems, Computed Aided Design Systems (CAD), and so on",
        "published": "2007-03-16T00:39:57Z",
        "link": "http://arxiv.org/abs/cs/0703089v1",
        "categories": [
            "cs.DB",
            "cs.CG"
        ]
    },
    {
        "title": "Some problems in asymptotic convex geometry and random matrices   motivated by numerical algorithms",
        "authors": [
            "Roman Vershynin"
        ],
        "summary": "The simplex method in Linear Programming motivates several problems of asymptotic convex geometry. We discuss some conjectures and known results in two related directions -- computing the size of projections of high dimensional polytopes and estimating the norms of random matrices and their inverses.",
        "published": "2007-03-19T21:51:50Z",
        "link": "http://arxiv.org/abs/cs/0703093v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.NA",
            "G.1.6; G.1.3"
        ]
    },
    {
        "title": "The finite tiling problem is undecidable in the hyperbolic plane",
        "authors": [
            "Maurice Margenstern"
        ],
        "summary": "In this paper, we consider the finite tiling problem which was proved undecidable in the Euclidean plane by Jarkko Kari in 1994. Here, we prove that the same problem for the hyperbolic plane is also undecidable.",
        "published": "2007-03-29T14:23:59Z",
        "link": "http://arxiv.org/abs/cs/0703147v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "The periodic domino problem is undecidable in the hyperbolic plane",
        "authors": [
            "Maurice Margenstern"
        ],
        "summary": "In this paper, we consider the periodic tiling problem which was proved undecidable in the Euclidean plane by Yu. Gurevich and I. Koriakov in 1972. Here, we prove that the same problem for the hyperbolic plane is also undecidable.",
        "published": "2007-03-30T09:31:40Z",
        "link": "http://arxiv.org/abs/cs/0703153v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Sparsity-certifying Graph Decompositions",
        "authors": [
            "Ileana Streinu",
            "Louis Theran"
        ],
        "summary": "We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. Special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. In particular, our colored pebbles generalize and strengthen the previous results of Lee and Streinu and give a new proof of the Tutte-Nash-Williams characterization of arboricity. We also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. Our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and Westermann and Hendrickson.",
        "published": "2007-03-31T02:26:18Z",
        "link": "http://arxiv.org/abs/0704.0002v2",
        "categories": [
            "math.CO",
            "cs.CG",
            "05C85; 05C70; 68R10; 05B35"
        ]
    },
    {
        "title": "Novel algorithm to calculate hypervolume indicator of Pareto   approximation set",
        "authors": [
            "Qing Yang",
            "Shengchao Ding"
        ],
        "summary": "Hypervolume indicator is a commonly accepted quality measure for comparing Pareto approximation set generated by multi-objective optimizers. The best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $O(n^{d/2})$ with special data structures. This paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. It splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. In special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. The complexity analysis shows that the proposed algorithm achieves $O((\\frac{d}{2})^n)$ time and $O(dn^2)$ space complexity in the worst case.",
        "published": "2007-04-10T07:21:02Z",
        "link": "http://arxiv.org/abs/0704.1196v1",
        "categories": [
            "cs.CG",
            "cs.NE"
        ]
    },
    {
        "title": "Euclidean Shortest Paths in Simple Cube Curves at a Glance",
        "authors": [
            "Fajie Li",
            "Reinhard Klette"
        ],
        "summary": "This paper reports about the development of two provably correct approximate algorithms which calculate the Euclidean shortest path (ESP) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal O}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. A run-time diagram also illustrates this linear-time behavior of the implemented ESP algorithm.",
        "published": "2007-04-24T03:54:51Z",
        "link": "http://arxiv.org/abs/0704.3197v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "About the domino problem in the hyperbolic plane, a new solution:   complement",
        "authors": [
            "Maurice Margenstern"
        ],
        "summary": "In this paper, we complete the construction of paper arXiv:cs.CG/0701096v2. Together with the proof contained in arXiv:cs.CG/0701096v2, this paper definitely proves that the general problem of tiling the hyperbolic plane with {\\it \\`a la} Wang tiles is undecidable.",
        "published": "2007-05-01T09:29:59Z",
        "link": "http://arxiv.org/abs/0705.0086v4",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Algorithms for laying points optimally on a plane and a circle",
        "authors": [
            "Ruslan Sharipov"
        ],
        "summary": "Two averaging algorithms are considered which are intended for choosing an optimal plane and an optimal circle approximating a group of points in three-dimensional Euclidean space.",
        "published": "2007-05-02T19:41:44Z",
        "link": "http://arxiv.org/abs/0705.0350v1",
        "categories": [
            "cs.CG",
            "math.OC",
            "F.2.2"
        ]
    },
    {
        "title": "Edges and Switches, Tunnels and Bridges",
        "authors": [
            "David Eppstein",
            "Marc van Kreveld",
            "Elena Mumford",
            "Bettina Speckmann"
        ],
        "summary": "Edge casing is a well-known method to improve the readability of drawings of non-planar graphs. A cased drawing orders the edges of each edge crossing and interrupts the lower edge in an appropriate neighborhood of the crossing. Certain orders will lead to a more readable drawing than others. We formulate several optimization criteria that try to capture the concept of a \"good\" cased drawing. Further, we address the algorithmic question of how to turn a given drawing into an optimal cased drawing. For many of the resulting optimization problems, we either find polynomial time algorithms or NP-hardness results.",
        "published": "2007-05-03T06:33:04Z",
        "link": "http://arxiv.org/abs/0705.0413v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Moving Walkways, Escalators, and Elevators",
        "authors": [
            "J. Cardinal",
            "S. Collette",
            "F. Hurtado",
            "S. Langerman",
            "B. Palop"
        ],
        "summary": "We study a simple geometric model of transportation facility that consists of two points between which the travel speed is high. This elementary definition can model shuttle services, tunnels, bridges, teleportation devices, escalators or moving walkways. The travel time between a pair of points is defined as a time distance, in such a way that a customer uses the transportation facility only if it is helpful.   We give algorithms for finding the optimal location of such a transportation facility, where optimality is defined with respect to the maximum travel time between two points in a given set.",
        "published": "2007-05-04T14:52:06Z",
        "link": "http://arxiv.org/abs/0705.0635v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "An Approximation Algorithm for Shortest Descending Paths",
        "authors": [
            "Mustaq Ahmed",
            "Anna Lubiw"
        ],
        "summary": "A path from s to t on a polyhedral terrain is descending if the height of a point p never increases while we move p along the path from s to t. No efficient algorithm is known to find a shortest descending path (SDP) from s to t in a polyhedral terrain. We give a simple approximation algorithm that solves the SDP problem on general terrains. Our algorithm discretizes the terrain with O(n^2 X / e) Steiner points so that after an O(n^2 X / e * log(n X /e))-time preprocessing phase for a given vertex s, we can determine a (1+e)-approximate SDP from s to any point v in O(n) time if v is either a vertex of the terrain or a Steiner point, and in O(n X /e) time otherwise. Here n is the size of the terrain, and X is a parameter of the geometry of the terrain.",
        "published": "2007-05-09T22:02:28Z",
        "link": "http://arxiv.org/abs/0705.1364v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Unfolding Manhattan Towers",
        "authors": [
            "Mirela Damian",
            "Robin Flatland",
            "Joseph O'Rourke"
        ],
        "summary": "We provide an algorithm for unfolding the surface of any orthogonal polyhedron that falls into a particular shape class we call Manhattan Towers, to a nonoverlapping planar orthogonal polygon. The algorithm cuts along edges of a 4x5x1 refinement of the vertex grid.",
        "published": "2007-05-10T19:50:48Z",
        "link": "http://arxiv.org/abs/0705.1541v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "A Branch and Cut Algorithm for the Halfspace Depth Problem",
        "authors": [
            "Dan Chen"
        ],
        "summary": "The concept of data depth in non-parametric multivariate descriptive statistics is the generalization of the univariate rank method to multivariate data. Halfspace depth is a measure of data depth. Given a set S of points and a point p, the halfspace depth (or rank) k of p is defined as the minimum number of points of S contained in any closed halfspace with p on its boundary. Computing halfspace depth is NP-hard, and it is equivalent to the Maximum Feasible Subsystem problem. In this thesis a mixed integer program is formulated with the big-M method for the halfspace depth problem. We suggest a branch and cut algorithm. In this algorithm, Chinneck's heuristic algorithm is used to find an upper bound and a related technique based on sensitivity analysis is used for branching. Irreducible Infeasible Subsystem (IIS) hitting set cuts are applied. We also suggest a binary search algorithm which may be more stable numerically. The algorithms are implemented with the BCP framework from the COIN-OR project.",
        "published": "2007-05-14T15:31:22Z",
        "link": "http://arxiv.org/abs/0705.1956v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Voronoi Diagram of Polygonal Chains under the Discrete Fréchet   Distance",
        "authors": [
            "Sergey Bereg",
            "Marina Gavrilova",
            "Binhai Zhu"
        ],
        "summary": "Polygonal chains are fundamental objects in many applications like pattern recognition and protein structure alignment. A well-known measure to characterize the similarity of two polygonal chains is the famous Fr\\`{e}chet distance. In this paper, for the first time, we consider the Voronoi diagram of polygonal chains in $d$-dimension ($d=2,3$) under the discrete Fr\\`{e}chet distance. Given $n$ polygonal chains ${\\cal C}$ in $d$-dimension ($d=2,3$), each with at most $k$ vertices, we prove fundamental properties of such a Voronoi diagram {\\em VD}$_F({\\cal C})$ by presenting the first known upper and lower bounds for {\\em VD}$_F({\\cal C})$.",
        "published": "2007-05-19T18:35:18Z",
        "link": "http://arxiv.org/abs/0705.2835v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Measuring and Localing Homology Classes",
        "authors": [
            "Daniel Freedman",
            "Chao Chen"
        ],
        "summary": "We develop a method for measuring and localizing homology classes. This involves two problems. First, we define relevant notions of size for both a homology class and a homology group basis, using ideas from relative homology. Second, we propose an algorithm to compute the optimal homology basis, using techniques from persistent homology and finite field algebra. Classes of the computed optimal basis are localized with cycles conveying their sizes. The algorithm runs in $O(\\beta^4 n^3 \\log^2 n)$ time, where $n$ is the size of the simplicial complex and $\\beta$ is the Betti number of the homology group.",
        "published": "2007-05-21T22:16:20Z",
        "link": "http://arxiv.org/abs/0705.3061v2",
        "categories": [
            "cs.CG",
            "math.AT"
        ]
    },
    {
        "title": "Optimal Separable Algorithms to Compute the Reverse Euclidean Distance   Transformation and Discrete Medial Axis in Arbitrary Dimension",
        "authors": [
            "David Coeurjolly",
            "Annick Montanvert"
        ],
        "summary": "In binary images, the distance transformation (DT) and the geometrical skeleton extraction are classic tools for shape analysis. In this paper, we present time optimal algorithms to solve the reverse Euclidean distance transformation and the reversible medial axis extraction problems for $d$-dimensional images. We also present a $d$-dimensional medial axis filtering process that allows us to control the quality of the reconstructed shape.",
        "published": "2007-05-23T11:29:52Z",
        "link": "http://arxiv.org/abs/0705.3343v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Maximizing Maximal Angles for Plane Straight-Line Graphs",
        "authors": [
            "Oswin Aichholzer",
            "Thomas Hackl",
            "Michael Hoffmann",
            "Clemens Huemer",
            "Attila Por",
            "Francisco Santos",
            "Bettina Speckmann",
            "Birgit Vogtenhuber"
        ],
        "summary": "Let $G=(S, E)$ be a plane straight-line graph on a finite point set $S\\subset\\R^2$ in general position. The incident angles of a vertex $p \\in S$ of $G$ are the angles between any two edges of $G$ that appear consecutively in the circular order of the edges incident to $p$.   A plane straight-line graph is called $\\phi$-open if each vertex has an incident angle of size at least $\\phi$. In this paper we study the following type of question: What is the maximum angle $\\phi$ such that for any finite set $S\\subset\\R^2$ of points in general position we can find a graph from a certain class of graphs on $S$ that is $\\phi$-open? In particular, we consider the classes of triangulations, spanning trees, and paths on $S$ and give tight bounds in most cases.",
        "published": "2007-05-25T18:10:45Z",
        "link": "http://arxiv.org/abs/0705.3820v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "math.CO",
            "G.2.2; I.3.5"
        ]
    },
    {
        "title": "The Distance Geometry of Music",
        "authors": [
            "Erik D. Demaine",
            "Francisco Gomez-Martin",
            "Henk Meijer",
            "David Rappaport",
            "Perouz Taslakian",
            "Godfried T. Toussaint",
            "Terry Winograd",
            "David R. Wood"
        ],
        "summary": "We demonstrate relationships between the classic Euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. Specifically, we show how the structure of the Euclidean algorithm defines a family of rhythms which encompass over forty timelines (\\emph{ostinatos}) from traditional world music. We prove that these \\emph{Euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the Euclidean distances between all pairs of onsets, viewing onsets as points on a circle. Indeed, Euclidean rhythms are the unique rhythms that maximize this notion of \\emph{evenness}. We also show that essentially all Euclidean rhythms are \\emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. Finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. All of our results for musical rhythms apply equally well to musical scales. In addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by Erd\\H{o}s in the plane.",
        "published": "2007-05-28T18:36:19Z",
        "link": "http://arxiv.org/abs/0705.4085v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "An Improved Tight Closure Algorithm for Integer Octagonal Constraints",
        "authors": [
            "Roberto Bagnara",
            "Patricia M. Hill",
            "Enea Zaffanella"
        ],
        "summary": "Integer octagonal constraints (a.k.a. ``Unit Two Variables Per Inequality'' or ``UTVPI integer constraints'') constitute an interesting class of constraints for the representation and solution of integer problems in the fields of constraint programming and formal analysis and verification of software and hardware systems, since they couple algorithms having polynomial complexity with a relatively good expressive power. The main algorithms required for the manipulation of such constraints are the satisfiability check and the computation of the inferential closure of a set of constraints. The latter is called `tight' closure to mark the difference with the (incomplete) closure algorithm that does not exploit the integrality of the variables. In this paper we present and fully justify an O(n^3) algorithm to compute the tight closure of a set of UTVPI integer constraints.",
        "published": "2007-05-31T14:32:46Z",
        "link": "http://arxiv.org/abs/0705.4618v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "cs.LO"
        ]
    },
    {
        "title": "Moving Vertices to Make Drawings Plane",
        "authors": [
            "Xavier Goaoc",
            "Jan Kratochvil",
            "Yoshio Okamoto",
            "Chan-Su Shin",
            "Alexander Wolff"
        ],
        "summary": "A straight-line drawing $\\delta$ of a planar graph $G$ need not be plane, but can be made so by moving some of the vertices. Let shift$(G,\\delta)$ denote the minimum number of vertices that need to be moved to turn $\\delta$ into a plane drawing of $G$. We show that shift$(G,\\delta)$ is NP-hard to compute and to approximate, and we give explicit bounds on shift$(G,\\delta)$ when $G$ is a tree or a general planar graph. Our hardness results extend to 1BendPointSetEmbeddability, a well-known graph-drawing problem.",
        "published": "2007-06-07T13:57:52Z",
        "link": "http://arxiv.org/abs/0706.1002v3",
        "categories": [
            "cs.CG",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Stability of boundary measures",
        "authors": [
            "Frédéric Chazal",
            "David Cohen-Steiner",
            "Quentin Mérigot"
        ],
        "summary": "We introduce the boundary measure at scale r of a compact subset of the n-dimensional Euclidean space. We show how it can be computed for point clouds and suggest these measures can be used for feature detection. The main contribution of this work is the proof a quantitative stability theorem for boundary measures using tools of convex analysis and geometric measure theory. As a corollary we obtain a stability result for Federer's curvature measures of a compact, allowing to compute them from point-cloud approximations of the compact.",
        "published": "2007-06-14T16:03:28Z",
        "link": "http://arxiv.org/abs/0706.2153v2",
        "categories": [
            "cs.CG",
            "math.CA",
            "math.MG"
        ]
    },
    {
        "title": "The Domino Problem of the Hyperbolic Plane Is Undecidable",
        "authors": [
            "Maurice Margenstern"
        ],
        "summary": "In this paper, we prove that the general tiling problem of the hyperbolic plane is undecidable by proving a slightly stronger version using only a regular polygon as the basic shape of the tiles. The problem was raised by a paper of Raphael Robinson in 1971, in his famous simplified proof that the general tiling problem is undecidable for the Euclidean plane, initially proved by Robert Berger in 1966.",
        "published": "2007-06-28T09:16:19Z",
        "link": "http://arxiv.org/abs/0706.4161v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Unfolding Orthogonal Terrains",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "It is shown that every orthogonal terrain, i.e., an orthogonal (right-angled) polyhedron based on a rectangle that meets every vertical line in a segment, has a grid unfolding: its surface may be unfolded to a single non-overlapping piece by cutting along grid edges defined by coordinate planes through every vertex.",
        "published": "2007-07-04T15:04:33Z",
        "link": "http://arxiv.org/abs/0707.0610v4",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Properties of polynomial bases used in a line-surface intersection   algorithm",
        "authors": [
            "Gun Srijuntongsiri",
            "Stephen A. Vavasis"
        ],
        "summary": "In [5], Srijuntongsiri and Vavasis propose the \"Kantorovich-Test Subdivision algorithm\", or KTS, which is an algorithm for finding all zeros of a polynomial system in a bounded region of the plane. This algorithm can be used to find the intersections between a line and a surface. The main features of KTS are that it can operate on polynomials represented in any basis that satisfies certain conditions and that its efficiency has an upper bound that depends only on the conditioning of the problem and the choice of the basis representing the polynomial system.   This article explores in detail the dependence of the efficiency of the KTS algorithm on the choice of basis. Three bases are considered: the power, the Bernstein, and the Chebyshev bases. These three bases satisfy the basis properties required by KTS. Theoretically, Chebyshev case has the smallest upper bound on its running time. The computational results, however, do not show that Chebyshev case performs better than the other two.",
        "published": "2007-07-10T18:56:05Z",
        "link": "http://arxiv.org/abs/0707.1515v5",
        "categories": [
            "cs.NA",
            "cs.CG"
        ]
    },
    {
        "title": "Clifford Algebra of the Vector Space of Conics for decision boundary   Hyperplanes in m-Euclidean Space",
        "authors": [
            "Isidro B. Nieto",
            "J. Refugio Vallejo"
        ],
        "summary": "In this paper we embed $m$-dimensional Euclidean space in the geometric algebra $Cl_m $ to extend the operators of incidence in ${R^m}$ to operators of incidence in the geometric algebra to generalize the notion of separator to a decision boundary hyperconic in the Clifford algebra of hyperconic sections denoted as ${Cl}({Co}_{2})$. This allows us to extend the concept of a linear perceptron or the spherical perceptron in conformal geometry and introduce the more general conic perceptron, namely the {elliptical perceptron}. Using Clifford duality a vector orthogonal to the decision boundary hyperplane is determined. Experimental results are shown in 2-dimensional Euclidean space where we separate data that are naturally separated by some typical plane conic separators by this procedure. This procedure is more general in the sense that it is independent of the dimension of the input data and hence we can speak of the hyperconic elliptic perceptron.",
        "published": "2007-07-26T18:03:23Z",
        "link": "http://arxiv.org/abs/0707.3979v1",
        "categories": [
            "cs.NE",
            "cs.CG",
            "I.5.1; I.5.2; I.2.6; I.3.5"
        ]
    },
    {
        "title": "Star Unfolding Convex Polyhedra via Quasigeodesic Loops",
        "authors": [
            "Jin-ichi Itoh",
            "Joseph O'Rourke",
            "Costin Vîlcu"
        ],
        "summary": "We extend the notion of star unfolding to be based on a quasigeodesic loop Q rather than on a point. This gives a new general method to unfold the surface of any convex polyhedron P to a simple (non-overlapping), planar polygon: cut along one shortest path from each vertex of P to Q, and cut all but one segment of Q.",
        "published": "2007-07-28T20:02:05Z",
        "link": "http://arxiv.org/abs/0707.4258v4",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "A note on equipartition",
        "authors": [
            "M. A. Lopez",
            "S. Reisner"
        ],
        "summary": "The problem of the existence of an equi-partition of a curve in $\\R^n$ has recently been raised in the context of computational geometry. The problem is to show that for a (continuous) curve $\\Gamma : [0,1] \\to \\R^n$ and for any positive integer N, there exist points $t_0=0<t_1<...<t_{N-1}<1=t_N$, such that $d(\\Gamma(t_{i-1}),\\Gamma(t_i))=d(\\Gamma(t_{i}),\\Gamma(t_{i+1}))$ for all $i=1,...,N$, where d is a metric or even a semi-metric (a weaker notion) on $\\R^n$. We show here that the existence of such points, in a broader context, is a consequence of Brower's fixed point theorem.",
        "published": "2007-07-29T15:46:53Z",
        "link": "http://arxiv.org/abs/0707.4298v3",
        "categories": [
            "cs.CG",
            "math.FA",
            "I.3.5"
        ]
    },
    {
        "title": "Nodally 3-connected planar graphs and convex combination mappings",
        "authors": [
            "Colm O Dunlaing"
        ],
        "summary": "A convex combination mapping of a planar graph is a plane mapping in which the external vertices are mapped to the corners of a convex polygon and every internal vertex is a proper weighted average of its neighbours. If a planar graph is nodally 3-connected or triangulated then every such mapping is an embedding (Tutte, Floater).   We give a simple characterisation of nodally 3-connected planar graphs, and generalise the above result to any planar graph which admits any convex embedding.",
        "published": "2007-08-07T14:32:38Z",
        "link": "http://arxiv.org/abs/0708.0964v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "From symmetry break to Poisson point process in 2D Voronoi   tessellations: the generic nature of hexagons",
        "authors": [
            "Valerio Lucarini"
        ],
        "summary": "We bridge the properties of the regular square and honeycomb Voronoi tessellations of the plane to those of the Poisson-Voronoi case, thus analyzing in a common framework symmetry-break processes and the approach to uniformly random distributions of tessellation-generating points. We consider ensemble simulations of tessellations generated by points whose regular positions are perturbed through a Gaussian noise controlled by the parameter alpha. We study the number of sides, the area, and the perimeter of the Voronoi cells. For alpha>0, hexagons are the most common class of cells, and 2-parameter gamma distributions describe well the statistics of the geometrical characteristics. The symmetry break due to noise destroys the square tessellation, whereas the honeycomb hexagonal tessellation is very stable and all Voronoi cells are hexagon for small but finite noise with alpha<0.1. For a moderate amount of Gaussian noise, memory of the specific unperturbed tessellation is lost, because the statistics of the two perturbed tessellations is indistinguishable. When alpha>2, results converge to those of Poisson-Voronoi tessellations. The properties of n-sided cells change with alpha until the Poisson-Voronoi limit is reached for alpha>2. The Desch law for perimeters is confirmed to be not valid and a square root dependence on n is established. The ensemble mean of the cells area and perimeter restricted to the hexagonal cells coincides with the full ensemble mean; this might imply that the number of sides acts as a thermodynamic state variable fluctuating about n=6; this reinforces the idea that hexagons, beyond their ubiquitous numerical prominence, can be taken as generic polygons in 2D Voronoi tessellations.",
        "published": "2007-08-07T15:23:31Z",
        "link": "http://arxiv.org/abs/0708.0977v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CG",
            "math-ph",
            "math.MP",
            "physics.data-an"
        ]
    },
    {
        "title": "Lower Bounds for the Complexity of the Voronoi Diagram of Polygonal   Curves under the Discrete Frechet Distance",
        "authors": [
            "Kevin Buchin",
            "Maike Buchin"
        ],
        "summary": "We give lower bounds for the combinatorial complexity of the Voronoi diagram of polygonal curves under the discrete Frechet distance. We show that the Voronoi diagram of n curves in R^d with k vertices each, has complexity Omega(n^{dk}) for dimension d=1,2 and Omega(n^{d(k-1)+2}) for d>2.",
        "published": "2007-08-14T14:53:44Z",
        "link": "http://arxiv.org/abs/0708.1909v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Zone and double zone diagrams in abstract spaces",
        "authors": [
            "Daniel Reem",
            "Simeon Reich"
        ],
        "summary": "A zone diagram is a relatively new concept which was first defined and studied by T. Asano, J. Matousek and T. Tokuyama. It can be interpreted as a state of equilibrium between several mutually hostile kingdoms. Formally, it is a fixed point of a certain mapping. These authors considered the Euclidean plane and proved the existence and uniqueness of zone diagrams there. In the present paper we generalize this concept in various ways. We consider general sites in m-spaces (a simple generalization of metric spaces) and prove several existence and (non)uniqueness results in this setting. In contrast to previous works, our (rather simple) proofs are based on purely order theoretic arguments. Many explicit examples are given, and some of them illustrate new phenomena which occur in the general case. We also re-interpret zone diagrams as a stable configuration in a certain combinatorial game, and provide an algorithm for finding this configuration in a particular case.",
        "published": "2007-08-19T15:35:25Z",
        "link": "http://arxiv.org/abs/0708.2668v2",
        "categories": [
            "math.MG",
            "cs.CG",
            "math.CO",
            "06B23, 47H10, 51K99, 54E35"
        ]
    },
    {
        "title": "Algorithmic Semi-algebraic Geometry and Topology -- Recent Progress and   Open Problems",
        "authors": [
            "Saugata Basu"
        ],
        "summary": "We give a survey of algorithms for computing topological invariants of semi-algebraic sets with special emphasis on the more recent developments in designing algorithms for computing the Betti numbers of semi-algebraic sets. Aside from describing these results, we discuss briefly the background as well as the importance of these problems, and also describe the main tools from algorithmic semi-algebraic geometry, as well as algebraic topology, which make these advances possible. We end with a list of open problems.",
        "published": "2007-08-21T14:55:56Z",
        "link": "http://arxiv.org/abs/0708.2854v2",
        "categories": [
            "math.GT",
            "cs.CC",
            "cs.CG",
            "math.AG",
            "math.AT",
            "14P10, 14P25 (Primary) 68W30 (Secondary)"
        ]
    },
    {
        "title": "Untangling a Planar Graph",
        "authors": [
            "Xavier Goaoc",
            "Jan Kratochvil",
            "Yoshio Okamoto",
            "Chan-Su Shin",
            "Andreas Spillner",
            "Alexander Wolff"
        ],
        "summary": "A straight-line drawing $\\delta$ of a planar graph $G$ need not be plane, but can be made so by \\emph{untangling} it, that is, by moving some of the vertices of $G$. Let shift$(G,\\delta)$ denote the minimum number of vertices that need to be moved to untangle $\\delta$. We show that shift$(G,\\delta)$ is NP-hard to compute and to approximate. Our hardness results extend to a version of \\textsc{1BendPointSetEmbeddability}, a well-known graph-drawing problem.   Further we define fix$(G,\\delta)=n-shift(G,\\delta)$ to be the maximum number of vertices of a planar $n$-vertex graph $G$ that can be fixed when untangling $\\delta$. We give an algorithm that fixes at least $\\sqrt{((\\log n)-1)/\\log \\log n}$ vertices when untangling a drawing of an $n$-vertex graph $G$. If $G$ is outerplanar, the same algorithm fixes at least $\\sqrt{n/2}$ vertices. On the other hand we construct, for arbitrarily large $n$, an $n$-vertex planar graph $G$ and a drawing $\\delta_G$ of $G$ with fix$(G,\\delta_G) \\le \\sqrt{n-2}+1$ and an $n$-vertex outerplanar graph $H$ and a drawing $\\delta_H$ of $H$ with fix$(H,\\delta_H) \\le 2 \\sqrt{n-1}+1$. Thus our algorithm is asymptotically worst-case optimal for outerplanar graphs.",
        "published": "2007-09-03T08:59:34Z",
        "link": "http://arxiv.org/abs/0709.0170v5",
        "categories": [
            "cs.CG",
            "cs.DM",
            "G.2.2; I.3.5"
        ]
    },
    {
        "title": "Unfolding Restricted Convex Caps",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "This paper details an algorithm for unfolding a class of convex polyhedra, where each polyhedron in the class consists of a convex cap over a rectangular base, with several restrictions: the cap's faces are quadrilaterals, with vertices over an underlying integer lattice, and such that the cap convexity is ``radially monotone,'' a type of smoothness constraint. Extensions of Cauchy's arm lemma are used in the proof of non-overlap.",
        "published": "2007-09-11T15:01:31Z",
        "link": "http://arxiv.org/abs/0709.1647v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Speeding up Simplification of Polygonal Curves using Nested   Approximations",
        "authors": [
            "Pierre-François Marteau",
            "Gildas G. Ménier"
        ],
        "summary": "We develop a multiresolution approach to the problem of polygonal curve approximation. We show theoretically and experimentally that, if the simplification algorithm A used between any two successive levels of resolution satisfies some conditions, the multiresolution algorithm MR will have a complexity lower than the complexity of A. In particular, we show that if A has a O(N2/K) complexity (the complexity of a reduced search dynamic solution approach), where N and K are respectively the initial and the final number of segments, the complexity of MR is in O(N).We experimentally compare the outcomes of MR with those of the optimal \"full search\" dynamic programming solution and of classical merge and split approaches. The experimental evaluations confirm the theoretical derivations and show that the proposed approach evaluated on 2D coastal maps either shows a lower complexity or provides polygonal approximations closer to the initial curves.",
        "published": "2007-09-12T18:27:53Z",
        "link": "http://arxiv.org/abs/0709.1941v3",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Connecting Polygonizations via Stretches and Twangs",
        "authors": [
            "Mirela Damian",
            "Robin Flatland",
            "Joseph O'Rourke",
            "Suneeta Ramaswami"
        ],
        "summary": "We show that the space of polygonizations of a fixed planar point set S of n points is connected by O(n^2) ``moves'' between simple polygons. Each move is composed of a sequence of atomic moves called ``stretches'' and ``twangs''. These atomic moves walk between weakly simple ``polygonal wraps'' of S. These moves show promise to serve as a basis for generating random polygons.",
        "published": "2007-09-12T18:38:54Z",
        "link": "http://arxiv.org/abs/0709.1942v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2"
        ]
    },
    {
        "title": "Bregman Voronoi Diagrams: Properties, Algorithms and Applications",
        "authors": [
            "Frank Nielsen",
            "Jean-Daniel Boissonnat",
            "Richard Nock"
        ],
        "summary": "The Voronoi diagram of a finite set of objects is a fundamental geometric structure that subdivides the embedding space into regions, each region consisting of the points that are closer to a given object than to the others. We may define many variants of Voronoi diagrams depending on the class of objects, the distance functions and the embedding space. In this paper, we investigate a framework for defining and building Voronoi diagrams for a broad class of distance functions called Bregman divergences. Bregman divergences include not only the traditional (squared) Euclidean distance but also various divergence measures based on entropic functions. Accordingly, Bregman Voronoi diagrams allow to define information-theoretic Voronoi diagrams in statistical parametric spaces based on the relative entropy of distributions. We define several types of Bregman diagrams, establish correspondences between those diagrams (using the Legendre transformation), and show how to compute them efficiently. We also introduce extensions of these diagrams, e.g. k-order and k-bag Bregman Voronoi diagrams, and introduce Bregman triangulations of a set of points and their connexion with Bregman Voronoi diagrams. We show that these triangulations capture many of the properties of the celebrated Delaunay triangulation. Finally, we give some applications of Bregman Voronoi diagrams which are of interest in the context of computational geometry and machine learning.",
        "published": "2007-09-14T01:31:17Z",
        "link": "http://arxiv.org/abs/0709.2196v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "New Complexity Bounds for Certain Real Fewnomial Zero Sets",
        "authors": [
            "Joel Gomez",
            "Andrew Niles",
            "J. Maurice Rojas"
        ],
        "summary": "Consider real bivariate polynomials f and g, respectively having 3 and m monomial terms. We prove that for all m>=3, there are systems of the form (f,g) having exactly 2m-1 roots in the positive quadrant. Even examples with m=4 having 7 positive roots were unknown before this paper, so we detail an explicit example of this form. We also present an O(n^{11}) upper bound for the number of diffeotopy types of the real zero set of an n-variate polynomial with n+4 monomial terms.",
        "published": "2007-09-15T06:28:46Z",
        "link": "http://arxiv.org/abs/0709.2405v1",
        "categories": [
            "math.AG",
            "cs.CG"
        ]
    },
    {
        "title": "Quantifying Homology Classes II: Localization and Stability",
        "authors": [
            "Chao Chen",
            "Daniel Freedman"
        ],
        "summary": "In the companion paper, we measured homology classes and computed the optimal homology basis. This paper addresses two related problems, namely, localization and stability. We localize a class with the cycle minimizing a certain objective function. We explore three different objective functions, namely, volume, diameter and radius. We show that it is NP-hard to compute the smallest cycle using the former two. We also prove that the measurement defined in the companion paper is stable with regard to small changes of the geometry of the concerned space.",
        "published": "2007-09-16T20:48:49Z",
        "link": "http://arxiv.org/abs/0709.2512v2",
        "categories": [
            "cs.CG",
            "cs.CC",
            "math.AT",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Triangulating the Real Projective Plane",
        "authors": [
            "Mridul Aanjaneya",
            "Monique Teillaud"
        ],
        "summary": "We consider the problem of computing a triangulation of the real projective plane P2, given a finite point set S={p1, p2,..., pn} as input. We prove that a triangulation of P2 always exists if at least six points in S are in general position, i.e., no three of them are collinear. We also design an algorithm for triangulating P2 if this necessary condition holds. As far as we know, this is the first computational result on the real projective plane.",
        "published": "2007-09-18T15:36:38Z",
        "link": "http://arxiv.org/abs/0709.2831v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Incremental Satisfiability and Implication for UTVPI Constraints",
        "authors": [
            "Andreas Schutt",
            "Peter J. Stuckey"
        ],
        "summary": "Unit two-variable-per-inequality (UTVPI) constraints form one of the largest class of integer constraints which are polynomial time solvable (unless P=NP). There is considerable interest in their use for constraint solving, abstract interpretation, spatial databases, and theorem proving. In this paper we develop a new incremental algorithm for UTVPI constraint satisfaction and implication checking that requires O(m + n log n + p) time and O(n+m+p) space to incrementally check satisfiability of m UTVPI constraints on n variables and check implication of p UTVPI constraints.",
        "published": "2007-09-19T06:58:05Z",
        "link": "http://arxiv.org/abs/0709.2961v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "cs.LO",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Algorithmic and topological aspects of semi-algebraic sets defined by   quadratic polynomial",
        "authors": [
            "Michael Kettner"
        ],
        "summary": "In this thesis, we consider semi-algebraic sets over a real closed field $R$ defined by quadratic polynomials. Semi-algebraic sets of $R^k$ are defined as the smallest family of sets in $R^k$ that contains the algebraic sets as well as the sets defined by polynomial inequalities, and which is also closed under the boolean operations (complementation, finite unions and finite intersections). We prove new bounds on the Betti numbers as well as on the number of different stable homotopy types of certain fibers of semi-algebraic sets over a real closed field $R$ defined by quadratic polynomials, in terms of the parameters of the system of polynomials defining them, which improve the known results. We conclude the thesis with presenting two new algorithms along with their implementations. The first algorithm computes the number of connected components and the first Betti number of a semi-algebraic set defined by compact objects in $\\mathbb{R}^k$ which are simply connected. This algorithm improves the well-know method using a triangulation of the semi-algebraic set. Moreover, the algorithm has been efficiently implemented which was not possible before. The second algorithm computes efficiently the real intersection of three quadratic surfaces in $\\mathbb{R}^3$ using a semi-numerical approach.",
        "published": "2007-09-20T19:28:10Z",
        "link": "http://arxiv.org/abs/0709.3283v1",
        "categories": [
            "math.AG",
            "cs.CG",
            "math.AT",
            "math.GT"
        ]
    },
    {
        "title": "A New Lower Bound on Guard Placement for Wireless Localization",
        "authors": [
            "Mirela Damian",
            "Robin Flatland",
            "Joseph O'Rourke",
            "Suneeta Ramaswami"
        ],
        "summary": "The problem of wireless localization asks to place and orient stations in the plane, each of which broadcasts a unique key within a fixed angular range, so that each point in the plane can determine whether it is inside or outside a given polygonal region. The primary goal is to minimize the number of stations. In this paper we establish a lower bound of 2n/3 - 1 stations for polygons in general position, for the case in which the placement of stations is restricted to polygon vertices, improving upon the existing n/2 lower bound.",
        "published": "2007-09-22T00:35:28Z",
        "link": "http://arxiv.org/abs/0709.3554v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "The Topology of Bendless Three-Dimensional Orthogonal Graph Drawing",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We consider embeddings of 3-regular graphs into 3-dimensional Cartesian coordinates, in such a way that two vertices are adjacent if and only if two of their three coordinates are equal (that is, if they lie on an axis-parallel line) and such that no three points lie on the same axis-parallel line; we call a graph with such an embedding an xyz graph}. We describe a correspondence between xyz graphs and face-colored embeddings of the graph onto two-dimensional manifolds, and we relate bipartiteness of the xyz graph to orientability of the underlying topological surface. Using this correspondence, we show that planar graphs are xyz graphs if and only if they are bipartite, cubic, and three-connected, and that it is NP-complete to determine whether an arbitrary graph is an xyz graph. We also describe an algorithm with running time O(n 2^{n/2}) for testing whether a given graph is an xyz graph.",
        "published": "2007-09-26T06:49:33Z",
        "link": "http://arxiv.org/abs/0709.4087v3",
        "categories": [
            "cs.CG",
            "math.CO",
            "F.2.2"
        ]
    },
    {
        "title": "Constructing a uniform plane-filling path in the ternary heptagrid of   the hyperbolic plane",
        "authors": [
            "Maurice Margenstern"
        ],
        "summary": "In this paper, we distinguish two levels for the plane-filling property. We consider a simple and a strong one. In this paper, we give the construction which proves that the simple plane-filling property also holds for the hyperbolic plane. The plane-filling property was established for the Euclidean plane by J. Kari, in 1994, in the strong version.",
        "published": "2007-10-01T08:42:03Z",
        "link": "http://arxiv.org/abs/0710.0232v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Band Unfoldings and Prismatoids: A Counterexample",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "This note shows that the hope expressed in [ADL+07]--that the new algorithm for edge-unfolding any polyhedral band without overlap might lead to an algorithm for unfolding any prismatoid without overlap--cannot be realized. A prismatoid is constructed whose sides constitute a nested polyhedral band, with the property that every placement of the prismatoid top face overlaps with the band unfolding.",
        "published": "2007-10-03T15:18:35Z",
        "link": "http://arxiv.org/abs/0710.0811v2",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Degeneracy of Angular Voronoi Diagram",
        "authors": [
            "Hidetoshi Muta",
            "Kimikazu Kato"
        ],
        "summary": "Angular Voronoi diagram was introduced by Asano et al. as fundamental research for a mesh generation. In an angular Voronoi diagram, the edges are curves of degree three. From view of computational robustness we need to treat the curves carefully, because they might have a singularity.   We enumerate all the possible types of curves that appear as an edge of an angular Voronoi diagram, which tells us what kind of degeneracy is possible and tells us necessity of considering a singularity for computational robustness.",
        "published": "2007-10-04T05:05:06Z",
        "link": "http://arxiv.org/abs/0710.0925v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "A polynomial bound for untangling geometric planar graphs",
        "authors": [
            "Prosenjit Bose",
            "Vida Dujmovic",
            "Ferran Hurtado",
            "Stefan Langerman",
            "Pat Morin",
            "David R. Wood"
        ],
        "summary": "To untangle a geometric graph means to move some of the vertices so that the resulting geometric graph has no crossings. Pach and Tardos [Discrete Comput. Geom., 2002] asked if every n-vertex geometric planar graph can be untangled while keeping at least n^\\epsilon vertices fixed. We answer this question in the affirmative with \\epsilon=1/4. The previous best known bound was \\Omega((\\log n / \\log\\log n)^{1/2}). We also consider untangling geometric trees. It is known that every n-vertex geometric tree can be untangled while keeping at least (n/3)^{1/2} vertices fixed, while the best upper bound was O(n\\log n)^{2/3}. We answer a question of Spillner and Wolff [arXiv:0709.0170 2007] by closing this gap for untangling trees. In particular, we show that for infinitely many values of n, there is an n-vertex geometric tree that cannot be untangled while keeping more than 3(n^{1/2}-1) vertices fixed. Moreover, we improve the lower bound to (n/2)^{1/2}.",
        "published": "2007-10-09T13:13:24Z",
        "link": "http://arxiv.org/abs/0710.1641v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "math.CO"
        ]
    },
    {
        "title": "Discrete differential geometry of tetrahedrons and encoding of local   protein structure",
        "authors": [
            "Naoto Morikawa"
        ],
        "summary": "Local protein structure analysis is informative to protein structure analysis and has been used successfully in protein structure prediction and others. Proteins have recurring structural features, such as helix caps and beta turns, which often have strong amino acid sequence preferences. And the challenges for local structure analysis have been identification and assignment of such common short structural motifs.   This paper proposes a new mathematical framework that can be applied to analysis of the local structure of proteins, where local conformations of protein backbones are described using differential geometry of folded tetrahedron sequences. Using the framework, we could capture the recurring structural features without any structural templates, which makes local structure analysis not only simpler, but also more objective.   Programs and examples are available from http://www.genocript.com .",
        "published": "2007-10-25T01:16:39Z",
        "link": "http://arxiv.org/abs/0710.4596v1",
        "categories": [
            "math.CO",
            "cs.CG",
            "math.MG",
            "q-bio.BM",
            "52C99, 92B99"
        ]
    },
    {
        "title": "Geometric Spanners With Small Chromatic Number",
        "authors": [
            "Prosenjit Bose",
            "Paz Carmi",
            "Mathieu Couture",
            "Anil Maheshwari",
            "Michiel Smid",
            "Norbert Zeh"
        ],
        "summary": "Given an integer $k \\geq 2$, we consider the problem of computing the smallest real number $t(k)$ such that for each set $P$ of points in the plane, there exists a $t(k)$-spanner for $P$ that has chromatic number at most $k$. We prove that $t(2) = 3$, $t(3) = 2$, $t(4) = \\sqrt{2}$, and give upper and lower bounds on $t(k)$ for $k>4$. We also show that for any $\\epsilon >0$, there exists a $(1+\\epsilon)t(k)$-spanner for $P$ that has $O(|P|)$ edges and chromatic number at most $k$. Finally, we consider an on-line variant of the problem where the points of $P$ are given one after another, and the color of a point must be assigned at the moment the point is given. In this setting, we prove that $t(2) = 3$, $t(3) = 1+ \\sqrt{3}$, $t(4) = 1+ \\sqrt{2}$, and give upper and lower bounds on $t(k)$ for $k>4$.",
        "published": "2007-11-01T13:46:14Z",
        "link": "http://arxiv.org/abs/0711.0114v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Generalized D-Forms Have No Spurious Creases",
        "authors": [
            "Gregory N. Price",
            "Erik D. Demaine"
        ],
        "summary": "A convex surface that is flat everywhere but on finitely many smooth curves (or \"seams\") and points is a seam form. We show that the only creases through the flat components of a seam form are either between vertices or tangent to the seams. As corollaries we resolve open problems about certain special seam forms: the flat components of a D-form have no creases at all, and the flat component of a pita-form has at most one crease, between the seam's endpoints.",
        "published": "2007-11-16T20:58:57Z",
        "link": "http://arxiv.org/abs/0711.2605v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Faster Algorithms for Rigidity in the Plane",
        "authors": [
            "Sergey Bereg"
        ],
        "summary": "In [1], a new construction called red-black hierarchy characterizing Laman graphs and an algorithm for computing it were presented. For a Laman graph G=(V,E) with n vertices it runs in O(n^2) time assuming that a partition of (V,E+e) into two spanning trees is given. We show that a simple modification reduces the running time to O(n\\log n). The total running time can be reduced O(n\\sqrt{n\\log n}) using the algorithm by Gabow and Westermann [2] for partitioning a graph into two forests. The existence of a red-black hierarchy is a necessary and sufficient condition for a graph to be a Laman graph. The algorithm for constructing a red-black hierarchy can be then modified to recognize Laman graphs in the same time.",
        "published": "2007-11-19T18:54:32Z",
        "link": "http://arxiv.org/abs/0711.2835v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Natural realizations of sparsity matroids",
        "authors": [
            "Ileana Streinu",
            "Louis Theran"
        ],
        "summary": "A hypergraph G with n vertices and m hyperedges with d endpoints each is (k,l)-sparse if for all sub-hypergraphs G' on n' vertices and m' edges, m'\\le kn'-l. For integers k and l satisfying 0\\le l\\le dk-1, this is known to be a linearly representable matroidal family.   Motivated by problems in rigidity theory, we give a new linear representation theorem for the (k,l)-sparse hypergraphs that is natural; i.e., the representing matrix captures the vertex-edge incidence structure of the underlying hypergraph G.",
        "published": "2007-11-19T21:02:34Z",
        "link": "http://arxiv.org/abs/0711.3013v5",
        "categories": [
            "math.CO",
            "cs.CG",
            "math.AG",
            "math.MG",
            "05B35"
        ]
    },
    {
        "title": "On the Centroids of Symmetrized Bregman Divergences",
        "authors": [
            "Frank Nielsen",
            "Richard Nock"
        ],
        "summary": "In this paper, we generalize the notions of centroids and barycenters to the broad class of information-theoretic distortion measures called Bregman divergences. Bregman divergences are versatile, and unify quadratic geometric distances with various statistical entropic measures. Because Bregman divergences are typically asymmetric, we consider both the left-sided and right-sided centroids and the symmetrized centroids, and prove that all three are unique. We give closed-form solutions for the sided centroids that are generalized means, and design a provably fast and efficient approximation algorithm for the symmetrized centroid based on its exact geometric characterization that requires solely to walk on the geodesic linking the two sided centroids. We report on our generic implementation for computing entropic centers of image clusters and entropic centers of multivariate normals, and compare our results with former ad-hoc methods.",
        "published": "2007-11-21T01:15:19Z",
        "link": "http://arxiv.org/abs/0711.3242v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "A condition number analysis of an algorithm for solving a system of   polynomial equations with one degree of freedom",
        "authors": [
            "Gun Srijuntongsiri",
            "Stephen A. Vavasis"
        ],
        "summary": "This article considers the problem of solving a system of $n$ real polynomial equations in $n+1$ variables. We propose an algorithm based on Newton's method and subdivision for this problem. Our algorithm is intended only for nondegenerate cases, in which case the solution is a 1-dimensional curve. Our first main contribution is a definition of a condition number measuring reciprocal distance to degeneracy that can distinguish poor and well conditioned instances of this problem. (Degenerate problems would be infinitely ill conditioned in our framework.) Our second contribution, which is the main novelty of our algorithm, is an analysis showing that its running time is bounded in terms of the condition number of the problem instance as well as $n$ and the polynomial degrees.",
        "published": "2007-11-29T06:02:16Z",
        "link": "http://arxiv.org/abs/0711.4656v2",
        "categories": [
            "cs.CG",
            "cs.NA"
        ]
    },
    {
        "title": "Spanners of Complete $k$-Partite Geometric Graphs",
        "authors": [
            "Prosenjit Bose",
            "Paz Carmi",
            "Mathieu Couture",
            "Anil Maheshwari",
            "Pat Morin",
            "Michiel Smid"
        ],
        "summary": "We address the following problem: Given a complete $k$-partite geometric graph $K$ whose vertex set is a set of $n$ points in $\\mathbb{R}^d$, compute a spanner of $K$ that has a ``small'' stretch factor and ``few'' edges. We present two algorithms for this problem. The first algorithm computes a $(5+\\epsilon)$-spanner of $K$ with O(n) edges in $O(n \\log n)$ time. The second algorithm computes a $(3+\\epsilon)$-spanner of $K$ with $O(n \\log n)$ edges in $O(n \\log n)$ time. The latter result is optimal: We show that for any $2 \\leq k \\leq n - \\Theta(\\sqrt{n \\log n})$, spanners with $O(n \\log n)$ edges and stretch factor less than 3 do not exist for all complete $k$-partite geometric graphs.",
        "published": "2007-12-04T16:14:07Z",
        "link": "http://arxiv.org/abs/0712.0554v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Delaunay Edge Flips in Dense Surface Triangulations",
        "authors": [
            "Siu-Wing Cheng",
            "Tamal K. Dey"
        ],
        "summary": "Delaunay flip is an elegant, simple tool to convert a triangulation of a point set to its Delaunay triangulation. The technique has been researched extensively for full dimensional triangulations of point sets. However, an important case of triangulations which are not full dimensional is surface triangulations in three dimensions. In this paper we address the question of converting a surface triangulation to a subcomplex of the Delaunay triangulation with edge flips. We show that the surface triangulations which closely approximate a smooth surface with uniform density can be transformed to a Delaunay triangulation with a simple edge flip algorithm. The condition on uniformity becomes less stringent with increasing density of the triangulation. If the condition is dropped completely, the flip algorithm still terminates although the output surface triangulation becomes \"almost Delaunay\" instead of exactly Delaunay.",
        "published": "2007-12-12T15:45:53Z",
        "link": "http://arxiv.org/abs/0712.1959v1",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Hinged Dissections Exist",
        "authors": [
            "Timothy G. Abbott",
            "Zachary Abel",
            "David Charlton",
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Scott D. Kominers"
        ],
        "summary": "We prove that any finite collection of polygons of equal area has a common hinged dissection. That is, for any such collection of polygons there exists a chain of polygons hinged at vertices that can be folded in the plane continuously without self-intersection to form any polygon in the collection. This result settles the open problem about the existence of hinged dissections between pairs of polygons that goes back implicitly to 1864 and has been studied extensively in the past ten years. Our result generalizes and indeed builds upon the result from 1814 that polygons have common dissections (without hinges). We also extend our common dissection result to edge-hinged dissections of solid 3D polyhedra that have a common (unhinged) dissection, as determined by Dehn's 1900 solution to Hilbert's Third Problem. Our proofs are constructive, giving explicit algorithms in all cases. For a constant number of planar polygons, both the number of pieces and running time required by our construction are pseudopolynomial. This bound is the best possible, even for unhinged dissections. Hinged dissections have possible applications to reconfigurable robotics, programmable matter, and nanomanufacturing.",
        "published": "2007-12-13T04:43:10Z",
        "link": "http://arxiv.org/abs/0712.2094v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Towards Persistence-Based Reconstruction in Euclidean Spaces",
        "authors": [
            "Frédéric Chazal",
            "Steve Oudot"
        ],
        "summary": "Manifold reconstruction has been extensively studied for the last decade or so, especially in two and three dimensions. Recently, significant improvements were made in higher dimensions, leading to new methods to reconstruct large classes of compact subsets of Euclidean space $\\R^d$. However, the complexities of these methods scale up exponentially with d, which makes them impractical in medium or high dimensions, even for handling low-dimensional submanifolds. In this paper, we introduce a novel approach that stands in-between classical reconstruction and topological estimation, and whose complexity scales up with the intrinsic dimension of the data. Specifically, when the data points are sufficiently densely sampled from a smooth $m$-submanifold of $\\R^d$, our method retrieves the homology of the submanifold in time at most $c(m)n^5$, where $n$ is the size of the input and $c(m)$ is a constant depending solely on $m$. It can also provably well handle a wide range of compact subsets of $\\R^d$, though with worse complexities. Along the way to proving the correctness of our algorithm, we obtain new results on \\v{C}ech, Rips, and witness complex filtrations in Euclidean spaces.",
        "published": "2007-12-17T06:30:08Z",
        "link": "http://arxiv.org/abs/0712.2638v2",
        "categories": [
            "cs.CG",
            "math.AT"
        ]
    },
    {
        "title": "How to Complete a Doubling Metric",
        "authors": [
            "Anupam Gupta",
            "Kunal Talwar"
        ],
        "summary": "In recent years, considerable advances have been made in the study of properties of metric spaces in terms of their doubling dimension. This line of research has not only enhanced our understanding of finite metrics, but has also resulted in many algorithmic applications. However, we still do not understand the interaction between various graph-theoretic (topological) properties of graphs, and the doubling (geometric) properties of the shortest-path metrics induced by them. For instance, the following natural question suggests itself: \\emph{given a finite doubling metric $(V,d)$, is there always an \\underline{unweighted} graph $(V',E')$ with $V\\subseteq V'$ such that the shortest path metric $d'$ on $V'$ is still doubling, and which agrees with $d$ on $V$.} This is often useful, given that unweighted graphs are often easier to reason about.   We show that for any metric space $(V,d)$, there is an \\emph{unweighted} graph $(V',E')$ with shortest-path metric $d':V'\\times V' \\to \\R_{\\geq 0}$ such that   -- for all $x,y \\in V$, the distances $d(x,y) \\leq d'(x,y) \\leq (1+\\eps) \\cdot d(x,y)$, and   -- the doubling dimension for $d'$ is not much more than that of $d$, where this change depends only on $\\e$ and not on the size of the graph.   We show a similar result when both $(V,d)$ and $(V',E')$ are restricted to be trees: this gives a simpler proof that doubling trees embed into constant dimensional Euclidean space with constant distortion. We also show that our results are tight in terms of the tradeoff between distortion and dimension blowup.",
        "published": "2007-12-20T06:12:15Z",
        "link": "http://arxiv.org/abs/0712.3331v2",
        "categories": [
            "cs.DM",
            "cs.CG"
        ]
    },
    {
        "title": "Shape preservation behavior of spline curves",
        "authors": [
            "Ravi Shankar Gautam"
        ],
        "summary": "Shape preservation behavior of a spline consists of criterial conditions for preserving convexity, inflection, collinearity, torsion and coplanarity shapes of data polgonal arc. We present our results which acts as an improvement in the definitions of and provide geometrical insight into each of the above shape preservation criteria. We also investigate the effect of various results from the literature on various shape preservation criteria. These results have not been earlier refered in the context of shape preservation behaviour of splines. We point out that each curve segment need to satisfy more than one shape preservation criteria. We investigate the conflict between different shape preservation criteria 1)on each curve segment and 2)of adjacent curve segments. We derive simplified formula for shape preservation criteria for cubic curve segments. We study the shape preservation behavior of cubic Catmull-Rom splines and see that, though being very simple spline curve, it indeed satisfy all the shape preservation criteria.",
        "published": "2007-02-05T10:11:58Z",
        "link": "http://arxiv.org/abs/cs/0702026v1",
        "categories": [
            "cs.GR",
            "I.3.5"
        ]
    },
    {
        "title": "Plot 94 in ambiance X-Window",
        "authors": [
            "Ignacio Vega-Paez",
            "Carlos Alberto Hernandez-Hernandez"
        ],
        "summary": "<PLOT > is a collection of routines to draw surfaces, contours and so on. In this work we are presenting a version, that functions over work stations with the operative system UNIX, that count with the graphic ambiance X-WINDOW with the tools XLIB and OSF/MOTIF. This implant was realized for the work stations DEC 5000-200, DEC IPX, and DEC ALFA of the CINVESTAV (Center of Investigation and Advanced Studies). Also implanted in SILICON GRAPHICS of the CENAC (National Center of Calculation of the Polytechnic National Institute",
        "published": "2007-03-16T00:18:11Z",
        "link": "http://arxiv.org/abs/cs/0703088v1",
        "categories": [
            "cs.CV",
            "cs.GR"
        ]
    },
    {
        "title": "User driven applications - new design paradigm",
        "authors": [
            "Sergey Andreyev"
        ],
        "summary": "Programs for complicated engineering and scientific tasks always have to deal with a problem of showing numerous graphical results. The limits of the screen space and often opposite requirements from different users are the cause of the infinite discussions between designers and users, but the source of this ongoing conflict is not in the level of interface design, but in the basic principle of current graphical output: user may change some views and details, but in general the output view is absolutely defined and fixed by the developer. Author was working for several years on the algorithm that will allow eliminating this problem thus allowing stepping from designer-driven applications to user-driven. Such type of applications in which user is deciding what, when and how to show on the screen, is the dream of scientists and engineers working on the analysis of the most complicated tasks. The new paradigm is based on movable and resizable graphics, and such type of graphics can be widely used not only for scientific and engineering applications.",
        "published": "2007-06-28T13:19:04Z",
        "link": "http://arxiv.org/abs/0706.4224v1",
        "categories": [
            "cs.GR",
            "cs.HC"
        ]
    },
    {
        "title": "The Trade-offs with Space Time Cube Representation of Spatiotemporal   Patterns",
        "authors": [
            "Per Ola Kristensson",
            "Nils Dahlback",
            "Daniel Anundi",
            "Marius Bjornstad",
            "Hanna Gillberg",
            "Jonas Haraldsson",
            "Ingrid Martensson",
            "Matttias Nordvall",
            "Josefin Stahl"
        ],
        "summary": "Space time cube representation is an information visualization technique where spatiotemporal data points are mapped into a cube. Fast and correct analysis of such information is important in for instance geospatial and social visualization applications. Information visualization researchers have previously argued that space time cube representation is beneficial in revealing complex spatiotemporal patterns in a dataset to users. The argument is based on the fact that both time and spatial information are displayed simultaneously to users, an effect difficult to achieve in other representations. However, to our knowledge the actual usefulness of space time cube representation in conveying complex spatiotemporal patterns to users has not been empirically validated. To fill this gap we report on a between-subjects experiment comparing novice users error rates and response times when answering a set of questions using either space time cube or a baseline 2D representation. For some simple questions the error rates were lower when using the baseline representation. For complex questions where the participants needed an overall understanding of the spatiotemporal structure of the dataset, the space time cube representation resulted in on average twice as fast response times with no difference in error rates compared to the baseline. These results provide an empirical foundation for the hypothesis that space time cube representation benefits users when analyzing complex spatiotemporal patterns.",
        "published": "2007-07-11T13:39:34Z",
        "link": "http://arxiv.org/abs/0707.1618v1",
        "categories": [
            "cs.HC",
            "cs.GR"
        ]
    },
    {
        "title": "Network synchronizability analysis: the theory of subgraphs and   complementary graphs",
        "authors": [
            "Zhisheng Duan",
            "Chao Liu",
            "Guanrong Chen"
        ],
        "summary": "In this paper, subgraphs and complementary graphs are used to analyze the network synchronizability. Some sharp and attainable bounds are provided for the eigenratio of the network structural matrix, which characterizes the network synchronizability, especially when the network's corresponding graph has cycles, chains, bipartite graphs or product graphs as its subgraphs.",
        "published": "2007-08-05T05:25:45Z",
        "link": "http://arxiv.org/abs/0708.0660v1",
        "categories": [
            "cs.NI",
            "cs.GR"
        ]
    },
    {
        "title": "Virtual Environments for Training: From Individual Learning to   Collaboration with Humanoids",
        "authors": [
            "Stéphanie Gerbaud",
            "Nicolas Mollet",
            "Bruno Arnaldi"
        ],
        "summary": "The next generation of virtual environments for training is oriented towards collaborative aspects. Therefore, we have decided to enhance our platform for virtual training environments, adding collaboration opportunities and integrating humanoids. In this paper we put forward a model of humanoid that suits both virtual humans and representations of real users, according to collaborative training activities. We suggest adaptations to the scenario model of our platform making it possible to write collaborative procedures. We introduce a mechanism of action selection made up of a global repartition and an individual choice. These models are currently being integrated and validated in GVT, a virtual training tool for maintenance of military equipments, developed in collaboration with the French company NEXTER-Group.",
        "published": "2007-08-06T07:42:56Z",
        "link": "http://arxiv.org/abs/0708.0712v1",
        "categories": [
            "cs.GR"
        ]
    },
    {
        "title": "Simple Algorithmic Principles of Discovery, Subjective Beauty, Selective   Attention, Curiosity & Creativity",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "I postulate that human or other intelligent agents function or should function as follows. They store all sensory observations as they come - the data is holy. At any time, given some agent's current coding capabilities, part of the data is compressible by a short and hopefully fast program / description / explanation / world model. In the agent's subjective eyes, such data is more regular and more \"beautiful\" than other data. It is well-known that knowledge of regularity and repeatability may improve the agent's ability to plan actions leading to external rewards. In absence of such rewards, however, known beauty is boring. Then \"interestingness\" becomes the first derivative of subjective beauty: as the learning agent improves its compression algorithm, formerly apparently random data parts become subjectively more regular and beautiful. Such progress in compressibility is measured and maximized by the curiosity drive: create action sequences that extend the observation history and yield previously unknown / unpredictable but quickly learnable algorithmic regularity. We discuss how all of the above can be naturally implemented on computers, through an extension of passive unsupervised learning to the case of active data selection: we reward a general reinforcement learner (with access to the adaptive compressor) for actions that improve the subjective compressibility of the growing data. An unusually large breakthrough in compressibility deserves the name \"discovery\". The \"creativity\" of artists, dancers, musicians, pure mathematicians can be viewed as a by-product of this principle. Several qualitative examples support this hypothesis.",
        "published": "2007-09-05T15:20:59Z",
        "link": "http://arxiv.org/abs/0709.0674v1",
        "categories": [
            "cs.AI",
            "cs.GR",
            "I.2.0"
        ]
    },
    {
        "title": "Design of moveable and resizable graphics",
        "authors": [
            "Sergey Andreyev"
        ],
        "summary": "We are communicating with computers on two different levels. On upper level we have a very flexible system of windows: we can move them, resize, overlap or put side by side. At any moment we decide what would be the best view and reorganize the whole view easily. Then we start any application, go to the inner level, and everything changes. Here we are stripped of all the flexibility and can work only inside the scenario, developed by the designer of the program. Interface will allow us to change some tiny details, but in general everything is fixed: graphics is neither moveable, nor resizable, and the same with controls. Author designed an extremely powerful mechanism of turning graphical objects and controls into moveable and resizable. This can not only significantly improve the existing applications, but this will bring the applications to another level. (To estimate the possible difference, try to imagine the Windows system without its flexibility and compare it with the current one.) This article explains in details the construction and use of moveable and resizable graphical objects.",
        "published": "2007-09-22T00:21:08Z",
        "link": "http://arxiv.org/abs/0709.3553v1",
        "categories": [
            "cs.GR",
            "cs.HC"
        ]
    },
    {
        "title": "Efficient Binary and Run Length Morphology and its Application to   Document Image Processing",
        "authors": [
            "Thomas M. Breuel"
        ],
        "summary": "This paper describes the implementation and evaluation of an open source library for mathematical morphology based on packed binary and run-length compressed images for document imaging applications. Abstractions and patterns useful in the implementation of the interval operations are described. A number of benchmarks and comparisons to bit-blit based implementations on standard document images are provided.",
        "published": "2007-12-02T07:25:59Z",
        "link": "http://arxiv.org/abs/0712.0121v1",
        "categories": [
            "cs.GR",
            "I.4; I.4.10; I.7.4; I.7.5"
        ]
    },
    {
        "title": "Dynamic Multilevel Graph Visualization",
        "authors": [
            "Todd L. Veldhuizen"
        ],
        "summary": "We adapt multilevel, force-directed graph layout techniques to visualizing dynamic graphs in which vertices and edges are added and removed in an online fashion (i.e., unpredictably). We maintain multiple levels of coarseness using a dynamic, randomized coarsening algorithm. To ensure the vertices follow smooth trajectories, we employ dynamics simulation techniques, treating the vertices as point particles. We simulate fine and coarse levels of the graph simultaneously, coupling the dynamics of adjacent levels. Projection from coarser to finer levels is adaptive, with the projection determined by an affine transformation that evolves alongside the graph layouts. The result is a dynamic graph visualizer that quickly and smoothly adapts to changes in a graph.",
        "published": "2007-12-10T17:42:48Z",
        "link": "http://arxiv.org/abs/0712.1549v1",
        "categories": [
            "cs.GR",
            "cs.DM",
            "H.5.0; G.2.2"
        ]
    },
    {
        "title": "Curve Tracking Control for Legged Locomotion in Horizontal Plane",
        "authors": [
            "F. Zhang"
        ],
        "summary": "We derive a hybrid feedback control law for the lateral leg spring (LLS) model so that the center of mass of a legged runner follows a curved path in horizontal plane. The control law enables the runner to change the placement and the elasticity of its legs to move in a desired direction. Stable motion along a curved path is achieved using curvature, bearing and relative distance between the runner and the curve as feedback. Constraints on leg parameters determine the class of curves that can be followed. We also derive an optimal control law that stabilizes the orientation of the runner's body relative to the velocity of the runner's center of mass.",
        "published": "2007-01-06T20:55:15Z",
        "link": "http://arxiv.org/abs/cs/0701040v1",
        "categories": [
            "cs.RO",
            "I.2.9"
        ]
    },
    {
        "title": "Asynchronous Distributed Searchlight Scheduling",
        "authors": [
            "Karl J. Obermeyer",
            "Anurag Ganguli",
            "Francesco Bullo"
        ],
        "summary": "This paper develops and compares two simple asynchronous distributed searchlight scheduling algorithms for multiple robotic agents in nonconvex polygonal environments. A searchlight is a ray emitted by an agent which cannot penetrate the boundary of the environment. A point is detected by a searchlight if and only if the point is on the ray at some instant. Targets are points which can move continuously with unbounded speed. The objective of the proposed algorithms is for the agents to coordinate the slewing (rotation about a point) of their searchlights in a distributed manner, i.e., using only local sensing and limited communication, such that any target will necessarily be detected in finite time. The first algorithm we develop, called the DOWSS (Distributed One Way Sweep Strategy), is a distributed version of a known algorithm described originally in 1990 by Sugihara et al \\cite{KS-IS-MY:90}, but it can be very slow in clearing the entire environment because only one searchlight may slew at a time. In an effort to reduce the time to clear the environment, we develop a second algorithm, called the PTSS (Parallel Tree Sweep Strategy), in which searchlights sweep in parallel if guards are placed according to an environment partition belonging to a class we call PTSS partitions. Finally, we discuss how DOWSS and PTSS could be combined with with deployment, or extended to environments with holes.",
        "published": "2007-01-11T20:55:17Z",
        "link": "http://arxiv.org/abs/cs/0701077v3",
        "categories": [
            "cs.MA",
            "cs.RO",
            "I.2.11"
        ]
    },
    {
        "title": "Target assignment for robotic networks: asymptotic performance under   limited communication",
        "authors": [
            "Stephen L. Smith",
            "Francesco Bullo"
        ],
        "summary": "We are given an equal number of mobile robotic agents, and distinct target locations. Each agent has simple integrator dynamics, a limited communication range, and knowledge of the position of every target. We address the problem of designing a distributed algorithm that allows the group of agents to divide the targets among themselves and, simultaneously, leads each agent to reach its unique target. We do not require connectivity of the communication graph at any time. We introduce a novel assignment-based algorithm with the following features: initial assignments and robot motions follow a greedy rule, and distributed refinements of the assignment exploit an implicit circular ordering of the targets. We prove correctness of the algorithm, and give worst-case asymptotic bounds on the time to complete the assignment as the environment grows with the number of agents. We show that among a certain class of distributed algorithms, our algorithm is asymptotically optimal. The analysis utilizes results on the Euclidean traveling salesperson problem.",
        "published": "2007-03-14T19:20:45Z",
        "link": "http://arxiv.org/abs/cs/0703067v2",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "2D Path Solutions from a Single Layer Excitable CNN Model",
        "authors": [
            "Koray Karahaliloglu"
        ],
        "summary": "An easily implementable path solution algorithm for 2D spatial problems, based on excitable/programmable characteristics of a specific cellular nonlinear network (CNN) model is presented and numerically investigated. The network is a single layer bioinspired model which was also implemented in CMOS technology. It exhibits excitable characteristics with regionally bistable cells. The related response realizes propagations of trigger autowaves, where the excitable mode can be globally preset and reset. It is shown that, obstacle distributions in 2D space can also be directly mapped onto the coupled cell array in the network. Combining these two features, the network model can serve as the main block in a 2D path computing processor. The related algorithm and configurations are numerically experimented with circuit level parameters and performance estimations are also presented. The simplicity of the model also allows alternative technology and device level implementation, which may become critical in autonomous processor design of related micro or nanoscale robotic applications.",
        "published": "2007-04-24T20:20:46Z",
        "link": "http://arxiv.org/abs/0704.3268v1",
        "categories": [
            "cs.RO",
            "cs.NE"
        ]
    },
    {
        "title": "The Optimization of a Novel Prismatic Drive",
        "authors": [
            "Damien Chablat",
            "Stéphane Caro",
            "Emilie Bouyer"
        ],
        "summary": "The design of a mechanical transmission taking into account the transmitted forces is reported in this paper. This transmission is based on Slide-o-Cam, a cam mechanism with multiple rollers mounted on a common translating follower. The design of Slide-o-Cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. This transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. The pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. To assess the transmission capability of the mechanism, the Hertz formula is introduced to calculate the stresses on the rollers and on the cams. The final transmission is intended to replace the current ball-screws in the Orthoglide, a three-DOF parallel robot for the production of translational motions, currently under development for machining applications at Ecole Centrale de Nantes.",
        "published": "2007-05-05T10:28:22Z",
        "link": "http://arxiv.org/abs/0705.0738v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Multiobjective Optimization of a Prismatic Drive",
        "authors": [
            "Emilie Bouyer",
            "Stéphane Caro",
            "Damien Chablat",
            "Jorge Angeles"
        ],
        "summary": "The multiobjective optimization of Slide-o-Cam is reported in this paper. Slide-o-Cam is a cam mechanism with multiple rollers mounted on a common translating follower. This transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. A Pareto frontier is obtained by means of multiobjective optimization. This optimization is based on three objective functions: (i) the pressure angle, which is a suitable performance index for the transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame; (ii) the Hertz pressure used to evaluate the stresses produced on the contact surface between cam and roller; and (iii) the size of the mechanism, characterized by the number of cams and their width.",
        "published": "2007-05-07T06:39:17Z",
        "link": "http://arxiv.org/abs/0705.0856v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "On Isotropic Sets of Points in the Plane. Application to the Design of   Robot Archirectures",
        "authors": [
            "Jorge Angeles",
            "Damien Chablat"
        ],
        "summary": "Various performance indices are used for the design of serial manipulators. One method of optimization relies on the condition number of the Jacobian matrix. The minimization of the condition number leads, under certain conditions, to isotropic configurations, for which the roundoff-error amplification is lowest. In this paper, the isotropy conditions, introduced elsewhere, are the motivation behind the introduction of isotropic sets of points. By connecting together these points, we define families of isotropic manipulators. This paper is devoted to planar manipulators, the concepts being currently extended to their spatial counterparts. Furthermore, only manipulators with revolute joints are considered here.",
        "published": "2007-05-07T18:19:06Z",
        "link": "http://arxiv.org/abs/0705.0956v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Kinematic Analysis of a Symmetrical Three-Degree-of-Freedom Planar   Parallel Manipulator",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "Presented in this paper is the kinematic analysis of a symmetrical three-degree-of-freedom planar parallel manipulator. In opposite to serial manipulators, parallel manipulators can admit not only multiple inverse kinematic solutions, but also multiple direct kinematic solutions. This property produces more complicated kinematic models but allows more flexibility in trajectory planning. To take into account this property, the notion of aspects, i.e. the maximal singularity-free domains, was introduced, based on the notion of working modes, which makes it possible to separate the inverse kinematic solutions. The aim of this paper is to show that a non-singular assembly-mode changing trajectory exist for a symmetrical planar parallel manipulator, with equilateral base and platform triangle.",
        "published": "2007-05-07T18:28:17Z",
        "link": "http://arxiv.org/abs/0705.0959v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Uniqueness Domains in the Workspace of Parallel Manipulators",
        "authors": [
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This work investigates new kinematic features of parallel manipulators. It is well known that parallel manipulators admit generally several direct kinematic solutions for a given set of input joint values. The aim of this paper is to characterize the uniqueness domains in the workspace of parallel manipulators, as well as their image in the joint space. The study focuses on the most usual case of parallel manipulators with only one inverse kinematic solution. The notion of aspect introduced for serial manipulators in [Borrel 86] is redefined for such parallel manipulators. Then, it is shown that it is possible to link several solutions to the forward kinematic problem without meeting a singularity, thus meaning that the aspects are not uniqueness domains. An additional set of surfaces, namely the characteristic surfaces, are characterized which divide the workspace into basic regions and yield new uniqueness domains. This study is illustrated all along the paper with a 3-RPR planar parallel manipulator. An octree model of spaces is used to compute the joint space, the workspace and all other newly defined sets.",
        "published": "2007-05-07T18:33:34Z",
        "link": "http://arxiv.org/abs/0705.0960v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Kinematic design of a 3-dof Hybrid Manipulator",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger",
            "Jorge Angeles"
        ],
        "summary": "This paper focuses on the kinematic properties of a new three-degree-of-freedom hybrid manipulator. This manipulator is obtained by adding in series to a five-bar planar mechanism (similar to the one studied by Bajpai and Roth) a third revolute passing through the line of centers of the two actuated revolute joints of the above linkage. The resulting architecture is hybrid in that it has both serial and parallel links. Fully-parallel manipulators are known for the existence of particularly undesirable singularities (referred to as parallel singularities) where control is lost [4] and [6]. On the other hand, due to their cantilever type of kinematic arrangement, fully serial manipulators suffer from a lack of stiffness and from relatively large positioning errors. The hybrid manipulator studied is intrinsically stiffer and more accurate. Furthermore, since all actuators are located on the first axis, the inertial effects are considerably reduced. In addition, it is shown that the special kinematic structure of our manipulator has the potential of avoiding parallel singularities by a suitable choice of the \"working mode\", thus leading to larger workspaces. The influence of the different structural dimensions (e.g. the link lengths) on the kinematic and mechanical properties are analysed in view of the optimal design of such hybrid manipulators.",
        "published": "2007-05-07T18:36:02Z",
        "link": "http://arxiv.org/abs/0705.0961v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Definition sets for the Direct Kinematics of Parallel Manipulators",
        "authors": [
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "The aim of this paper is to characterize the uniqueness domains in the workspace of parallel manipulators, as well as their image in the joint space. The notion of aspect introduced for serial manipulators in [Borrel 86] is redefined for such parallel manipulators. Then, it is shown that it is possible to link several solutions to the direct kinematic problem without meeting a singularity, thus meaning that the aspects are not uniqueness domains. Additional surfaces are characterized in the workspace which yield new uniqueness domains. An octree model of spaces is used to compute the joint space, the workspace and all other newly defined sets. This study is illustrated all along the paper with a 3-RPR planar parallel manipulator.",
        "published": "2007-05-07T18:39:19Z",
        "link": "http://arxiv.org/abs/0705.0962v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A New Three-DOF Parallel Mechanism: Milling Machine Applications",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "This paper describes a new parallel kinematic architecture for machining applications, namely, the orthoglide. This machine features three fixed parallel linear joints which are mounted orthogonally and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The main interest of the orthoglide is that it takes benefit from the advantages of the popular PPP serial machines (regular Cartesian workspace shape and uniform performances) as well as from the parallel kinematic arrangement of the links (less inertia and better dynamic performances), which makes the orthoglide well suited to high-speed machining applications. Possible extension of the orthoglide to 5-axis machining is also investigated.",
        "published": "2007-05-07T20:01:20Z",
        "link": "http://arxiv.org/abs/0705.0982v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Strategies for the Design of a Slide-o-Cam Transmission",
        "authors": [
            "Damien Chablat",
            "Jorge Angeles"
        ],
        "summary": "The optimization of the pressure angle in a cam-follower transmission is reported in this paper. This transmission is based on Slide-o-Cam, a cam mechanism with multiple rollers mounted on a common translating follower. The design of Slide-o-Cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. This transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. The pressure angle is a suitable performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. Two alternative design strategies are studied, namely, (i) increase the number of lobes on each cam or (ii) increase the number of cams. This device is intended to replace the current ball-screws in Orthoglide, a three-DOF parallel robot for the production of translational motions, currently under development at Ecole Centrale de Nantes for machining applications.",
        "published": "2007-05-08T07:09:57Z",
        "link": "http://arxiv.org/abs/0705.1036v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Regions of Feasible Point-to-Point Trajectories in the Cartesian   Workspace of Fully-Parallel Manipulators",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The goal of this paper is to define the n-connected regions in the Cartesian workspace of fully-parallel manipulators, i.e. the maximal regions where it is possible to execute point-to-point motions. The manipulators considered in this study may have multiple direct and inverse kinematic solutions. The N-connected regions are characterized by projection, onto the Cartesian workspace, of the connected components of the reachable configuration space defined in the Cartesian product of the Cartesian space by the joint space. Generalized octree models are used for the construction of all spaces. This study is illustrated with a simple planar fully-parallel manipulator.",
        "published": "2007-05-08T07:11:02Z",
        "link": "http://arxiv.org/abs/0705.1037v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Design of Parallel Kinematic Machine Tools Using Kinetostatic   Performance Criteria",
        "authors": [
            "Félix Majou",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "Most industrial machine tools have a serial kinematic architecture, which means that each axis has to carry the following one, including its actuators and joints. High Speed Machining highlights some drawbacks of such architectures: heavy moving parts require from the machine structure high stiffness to limit bending problems that lower the machine accuracy, and limit the dynamic performances of the feed axes. That is why PKMs attract more and more researchers and companies, because they are claimed to offer several advantages over their serial counterparts, like high structural rigidity and high dynamic capacities. Indeed, the parallel kinematic arrangement of the links provides higher stiffness and lower moving masses that reduce inertia effects. Thus, PKMs have better dynamic performances. However, the design of a parallel kinematic machine tool (PKMT) is a hard task that requires further research studies before wide industrial use can be expected. Many criteria need to be taken into account in the design of a PKMT. We pay special attention to the description of kinetostatic criteria that rely on the conditioning of the Jacobian matrix of the mechanism. The organisation of this paper is as follows: next section introduces general remarks about PKMs, then is explained why PKMs can be interesting alternative machine tool designs. Then are presented existing PKMTs. An application to the design of a small-scale machine tool prototype developed at IRCCyN is presented at the end of this paper.",
        "published": "2007-05-08T07:13:25Z",
        "link": "http://arxiv.org/abs/0705.1038v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Séparation des Solutions aux Modèles Géométriques Direct et   Inverse pour les Manipulateurs Pleinement Parallèles",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "This article provides a formalism making it possible to manage the solutions of the direct and inverse kinematic models of the fully parallel manipulators. We introduce the concept of working modes to separate the solutions from the opposite geometrical model. Then, we define, for each working mode, the aspects of these manipulators. To separate the solutions from the direct kinematics model, we introduce the concept of characteristic surfaces. Then, we define the uniqueness domains, as being the greatest domains of the workspace in which there is unicity of solutions. The principal applications of this work are the design, the trajectory planning.",
        "published": "2007-05-08T19:10:18Z",
        "link": "http://arxiv.org/abs/0705.1148v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "On the Kinetostatic Optimization of Revolute-Coupled Planar Manipulators",
        "authors": [
            "Damien Chablat",
            "Jorge Angeles"
        ],
        "summary": "Proposed in this paper is a kinetostatic performance index for the optimum dimensioning of planar manipulators of the serial type. The index is based on the concept of distance of the underlying Jacobian matrix to a given isotropic matrix that is used as a reference model for purposes of performance evaluation. Applications of the index fall in the realm of design, but control applications are outlined. The paper focuses on planar manipulators, the basic concepts being currently extended to their three-dimensional counterparts.",
        "published": "2007-05-08T19:11:12Z",
        "link": "http://arxiv.org/abs/0705.1150v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Kinematic Calibration of the Orthoglide-Type Mechanisms",
        "authors": [
            "Anatoly Pashkevich",
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The paper proposes a novel calibration approach for the Orthoglide-type mechanisms based on observations of the manipulator leg parallelism during motions between the prespecified test postures. It employs a low-cost measuring system composed of standard comparator indicators attached to the universal magnetic stands. They are sequentially used for measuring the deviation of the relevant leg location while the manipulator moves the TCP along the Cartesian axes. Using the measured differences, the developed algorithm estimates the joint offsets that are treated as the most essential parameters to be adjusted. The sensitivity of the measurement methods and the calibration accuracy are also studied. Experimental results are presented that demonstrate validity of the proposed calibration technique.",
        "published": "2007-05-09T07:21:35Z",
        "link": "http://arxiv.org/abs/0705.1215v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Design of a Novel Prismatic Drive for a Three-DOF   Parallel-Kinematics Machine",
        "authors": [
            "Jérome Renotte",
            "Damien Chablat",
            "Jorge Angeles"
        ],
        "summary": "The design of a novel prismatic drive is reported in this paper. This transmission is based on Slide-O-Cam, a cam mechanism with multiple rollers mounted on a common translating follower. The design of Slide-O-Cam was reported elsewhere. This drive thus provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. Such properties can be used to design new transmissions for parallel-kinematics machines. In this paper, this transmission is optimized to replace ball-screws in Orthoglide, a three-DOF parallel robot optimized for machining applications.",
        "published": "2007-05-09T07:27:45Z",
        "link": "http://arxiv.org/abs/0705.1217v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Calibration of quasi-isotropic parallel kinematic Machines: Orthoglide",
        "authors": [
            "Anatoly Pashkevich",
            "Roman Gomolitsky",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "The paper proposes a novel approach for the geometrical model calibration of quasi-isotropic parallel kinematic mechanisms of the Orthoglide family. It is based on the observations of the manipulator leg parallelism during motions between the specific test postures and employs a low-cost measuring system composed of standard comparator indicators attached to the universal magnetic stands. They are sequentially used for measuring the deviation of the relevant leg location while the manipulator moves the TCP along the Cartesian axes. Using the measured differences, the developed algorithm estimates the joint offsets and the leg lengths that are treated as the most essential parameters. Validity of the proposed calibration technique is confirmed by the experimental results.",
        "published": "2007-05-09T07:30:48Z",
        "link": "http://arxiv.org/abs/0705.1218v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Design of a 3 Axis Parallel Machine Tool for High Speed Machining: The   Orthoglide",
        "authors": [
            "Félix Majou",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "The Orthoglide project aims at designing a new 3-axis machine tool for High Speed Machining. Basis kinematics is a 3 degree-of-freedom translational parallel mechanism. This basis was submitted to isotropic and manipulability constraints that allowed the optmization of its kinematic architecture and legs architecture. Thus, several leg morphologies are convenient for the chosen mechanism. We explain the process that led us to the choice we made for the Orthoglide. A static study is presented to show how singular configurations of the legs can cause stiffness problems.",
        "published": "2007-05-09T12:23:53Z",
        "link": "http://arxiv.org/abs/0705.1271v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Isoconditioning Loci of Planar Three-DOF Parallel Manipulators",
        "authors": [
            "Damien Chablat",
            "Stéphane Caro",
            "Philippe Wenger",
            "Jorge Angeles"
        ],
        "summary": "The subject of this paper is a special class of parallel manipulators. First, we analyze a family of three-degree-of-freedom manipulators. Two Jacobian matrices appear in the kinematic relations between the joint-rate and the Cartesian-velocity vectors, which are called the \"inverse kinematics\" and the \"direct kinematics\" matrices. The singular configurations of these matrices are studied. The isotropic configurations are then studied based on the characteristic length of this manipulator. The isoconditioning loci of all Jacobian matrices are computed to define a global performance index to compare the different working modes.",
        "published": "2007-05-09T12:27:08Z",
        "link": "http://arxiv.org/abs/0705.1272v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A Novel method for the design of 2-DOF Parallel mechanisms for machining   applications",
        "authors": [
            "Félix Majou",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "Parallel Kinematic Mechanisms (PKM) are interesting alternative designs for machine tools. A design method based on velocity amplification factors analysis is presented in this paper. The comparative study of two simple two-degree-of-freedom PKM dedicated to machining applications is led through this method: the common desired properties are the largest square Cartesian workspace for given kinetostatic performances. The orientation and position of the Cartesian workspace are chosen to avoid singularities and to produce the best ratio between Cartesian workspace size and mechanism size. The machine size of each resulting design is used as a comparative criterion.",
        "published": "2007-05-09T13:18:59Z",
        "link": "http://arxiv.org/abs/0705.1280v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Design of a Three-Axis Isotropic Parallel Manipulator for Machining   Applications: The Orthoglide",
        "authors": [
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "The orthoglide is a 3-DOF parallel mechanism designed at IRCCyN for machining applications. It features three fixed parallel linear joints which are mounted orthogonally and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The orthoglide has been designed as function of a prescribed Cartesian workspace with prescribed kinetostatic performances. The interesting features of the orthoglide are a regular Cartesian workspace shape, uniform performances in all directions and good compactness. A small-scale prototype of the orthoglide under development is presented at the end of this paper.",
        "published": "2007-05-09T13:23:37Z",
        "link": "http://arxiv.org/abs/0705.1282v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Workspace Analysis of the Orthoglide using Interval Analysis",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger",
            "Jean-Pierre Merlet"
        ],
        "summary": "This paper addresses the workspace analysis of the orthoglide, a 3-DOF parallel mechanism designed for machining applications. This machine features three fixed parallel linear joints which are mounted orthogonally and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The workspace analysis is conducted on the bases of prescribed kinetostatic performances. The interesting features of the orthoglide are a regular Cartesian workspace shape, uniform performances in all directions and good compactness. Interval analysis based methods for computing the dextrous workspace and the largest cube enclosed in this workspace are presented.",
        "published": "2007-05-09T13:24:39Z",
        "link": "http://arxiv.org/abs/0705.1284v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Périphériques haptiques et simulation d'objets, de robots et de   mannequins dans un environnement de CAO-Robotique : eM-Virtual Desktop",
        "authors": [
            "Damien Chablat",
            "Fouad Bennis",
            "Bernard Hoessler",
            "Matthieu Guibert"
        ],
        "summary": "This paper presents the development of a new software in order to manage objects, robots and mannequins in using the possibilities given by the haptic feedback of the Phantom desktop devices. The haptic device provides 6 positional degree of freedom sensing but three degrees force feedback. This software called eM-Virtual Desktop is integrated in the Tecnomatix's solution called eM-Workplace. The eM-Workplace provides powerful solutions for planning and designing of complex assembly facilities, lines and workplaces. In the digital mockup context, the haptic interfaces can be used to reduce the development cycle of products. Three different loops are used to manage the graphic, the collision detection and the haptic feedback according to theirs own frequencies. The developed software is currently tested in industrial context by a European automotive constructor.",
        "published": "2007-05-09T13:26:46Z",
        "link": "http://arxiv.org/abs/0705.1285v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Optimal Design of Three Degree-of-Freedom Parallel Mechanisms for   Machining Applications",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger",
            "Félix Majou"
        ],
        "summary": "The subject of this paper is the optimal design of a parallel mechanism intended for three-axis machining applications. Parallel mechanisms are interesting alternative designs in this context but most of them are designed for three- or six-axis machining applications. In the last case, the position and the orientation of the tool are coupled and the shape of the workspace is complex. The aim of this paper is to use a simple parallel mechanism with two-degree-of-freedom (dof) for translational motions and to add one leg to have one-dof rotational motion. The kinematics and singular configurations are studied as well as an optimization method. The three-degree-of-freedom mechanisms analyzed in this paper can be extended to four-axis machines by adding a fourth axis in series with the first two.",
        "published": "2007-05-09T19:13:46Z",
        "link": "http://arxiv.org/abs/0705.1343v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Classification of one family of 3R positioning Manipulators",
        "authors": [
            "Maher Baili",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "The aim of this paper is to classify one family of 3R serial positioning manipulators. This categorization is based on the number of cusp points of quaternary, binary, generic and non-generic manipulators. It was found three subsets of manipulators with 0, 2 or 4 cusp points and one homotopy class for generic quaternary manipulators. This classification allows us to define the design parameters for which the manipulator is cuspidal or not, i.e., for which the manipulator can or cannot change posture without meeting a singularity, respectively.",
        "published": "2007-05-09T19:16:05Z",
        "link": "http://arxiv.org/abs/0705.1344v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Orthoglide: Kinematics and Workspace Analysis",
        "authors": [
            "Anatoly Pashkevich",
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The paper addresses kinematic and geometrical aspects of the Orthoglide, a three-DOF parallel mechanism. This machine consists of three fixed linear joints, which are mounted orthogonally, three identical legs and a mobile platform, which moves in the Cartesian x-y-z space with fixed orientation. New solutions to solve inverse/direct kinematics are proposed and a detailed workspace analysis is performed taking into account specific joint limit constraints.",
        "published": "2007-05-10T06:53:24Z",
        "link": "http://arxiv.org/abs/0705.1394v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Subjective Evaluation of Forms in an Immersive Environment",
        "authors": [
            "Jean-François Petiot",
            "Damien Chablat"
        ],
        "summary": "User's perception of product, by essence subjective, is a major topic in marketing and industrial design. Many methods, based on users' tests, are used so as to characterise this perception. We are interested in three main methods: multidimensional scaling, semantic differential method, and preference mapping. These methods are used to built a perceptual space, in order to position the new product, to specify requirements by the study of user's preferences, to evaluate some product attributes, related in particular to style (aesthetic). These early stages of the design are primordial for a good orientation of the project. In parallel, virtual reality tools and interfaces are more and more efficient for suggesting to the user complex feelings, and creating in this way various levels of perceptions. In this article, we present on an example the use of multidimensional scaling, semantic differential method and preference mapping for the subjective assessment of virtual products. These products, which geometrical form is variable, are defined with a CAD model and are proposed to the user with a spacemouse and stereoscopic glasses. Advantages and limitations of such evaluation is next discussed..",
        "published": "2007-05-10T06:54:11Z",
        "link": "http://arxiv.org/abs/0705.1395v1",
        "categories": [
            "cs.HC",
            "cs.RO"
        ]
    },
    {
        "title": "Realistic Rendering of Kinetostatic Indices of Mechanisms",
        "authors": [
            "Damien Chablat",
            "Fouad Bennis"
        ],
        "summary": "The work presented in this paper is related to the use of a haptic device in an environment of robotic simulation. Such device introduces a new approach to feel and to understand the boundaries of the workspace of mechanisms as well as its kinetostatic properties. Indeed, these concepts are abstract and thus often difficult to understand for the end-users. To catch his attention, we propose to amplify the problems of the mechanisms in order to help him to take the good decisions.",
        "published": "2007-05-10T06:56:25Z",
        "link": "http://arxiv.org/abs/0705.1397v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A New Concept of Modular Parallel Mechanism for Machining Applications",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The subject of this paper is the design of a new concept of modular parallel mechanisms for three, four or five-axis machining applications. Most parallel mechanisms are designed for three- or six-axis machining applications. In the last case, the position and the orientation of the tool are coupled and the shape of the workspace is complex. The aim of this paper is to use a simple parallel mechanism with two-degree-of-freedom (dof) for translation motions and to add one or two legs to add one or two-dofs for rotation motions. The kinematics and singular configurations are studied for each mechanism.",
        "published": "2007-05-10T07:03:29Z",
        "link": "http://arxiv.org/abs/0705.1399v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A Workspace based Classification of 3R Orthogonal Manipulators",
        "authors": [
            "Philippe Wenger",
            "Maher Baili",
            "Damien Chablat"
        ],
        "summary": "A classification of a family of 3-revolute (3R) positioning manipulators is established. This classification is based on the topology of their workspace. The workspace is characterized in a half-cross section by the singular curves of the manipulator. The workspace topology is defined by the number of cusps and nodes that appear on these singular curves. The design parameters space is shown to be partitioned into nine subspaces of distinct workspace topologies. Each separating surface is given as an explicit expression in the DH-parameters.",
        "published": "2007-05-10T07:05:37Z",
        "link": "http://arxiv.org/abs/0705.1400v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Singularity Surfaces and Maximal Singularity-Free Boxes in the Joint   Space of Planar 3-RPR Parallel Manipulators",
        "authors": [
            "Mazen Zein",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "In this paper, a method to compute joint space singularity surfaces of 3-RPR planar parallel manipulators is first presented. Then, a procedure to determine maximal joint space singularity-free boxes is introduced. Numerical examples are given in order to illustrate graphically the results. This study is of high interest for planning trajectories in the joint space of 3-RPR parallel manipulators and for manipulators design as it may constitute a tool for choosing appropriate joint limits and thus for sizing the link lengths of the manipulator.",
        "published": "2007-05-10T08:32:06Z",
        "link": "http://arxiv.org/abs/0705.1409v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Kinematics analysis of the parallel module of the VERNE machine",
        "authors": [
            "Daniel Kanaan",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "The paper derives the inverse and forward kinematic equations of a spatial three-degree-of-freedom parallel mechanism, which is the parallel module of a hybrid serial-parallel 5-axis machine tool. This parallel mechanism consists of a moving platform that is connected to a fixed base by three non-identical legs. Each leg is made up of one prismatic and two pair spherical joint, which are connected in a way that the combined effects of the three legs lead to an over-constrained mechanism with complex motion. This motion is defined as a simultaneous combination of rotation and translation.",
        "published": "2007-05-10T08:32:53Z",
        "link": "http://arxiv.org/abs/0705.1410v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "An Algorithm for Computing Cusp Points in the Joint Space of 3-RPR   Parallel Manipulators",
        "authors": [
            "Mazen Zein",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This paper presents an algorithm for detecting and computing the cusp points in the joint space of 3-RPR planar parallel manipulators. In manipulator kinematics, cusp points are special points, which appear on the singular curves of the manipulators. The nonsingular change of assembly mode of 3-RPR parallel manipulators was shown to be associated with the existence of cusp points. At each of these points, three direct kinematic solutions coincide. In the literature, a condition for the existence of three coincident direct kinematic solutions was established, but has never been exploited, because the algebra involved was too complicated to be solved. The algorithm presented in this paper solves this equation and detects all the cusp points in the joint space of these manipulators.",
        "published": "2007-05-10T12:10:12Z",
        "link": "http://arxiv.org/abs/0705.1450v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Challenges and Opportunities of Evolutionary Robotics",
        "authors": [
            "D. A. Sofge",
            "M. A. Potter",
            "M. D. Bugajska",
            "A. C. Schultz"
        ],
        "summary": "Robotic hardware designs are becoming more complex as the variety and number of on-board sensors increase and as greater computational power is provided in ever-smaller packages on-board robots. These advances in hardware, however, do not automatically translate into better software for controlling complex robots. Evolutionary techniques hold the potential to solve many difficult problems in robotics which defy simple conventional approaches, but present many challenges as well. Numerous disciplines including artificial life, cognitive science and neural networks, rule-based systems, behavior-based control, genetic algorithms and other forms of evolutionary computation have contributed to shaping the current state of evolutionary robotics. This paper provides an overview of developments in the emerging field of evolutionary robotics, and discusses some of the opportunities and challenges which currently face practitioners in the field.",
        "published": "2007-06-04T16:08:22Z",
        "link": "http://arxiv.org/abs/0706.0457v1",
        "categories": [
            "cs.NE",
            "cs.RO"
        ]
    },
    {
        "title": "Design, Implementation, and Cooperative Coevolution of an Autonomous/   Teleoperated Control System for a Serpentine Robotic Manipulator",
        "authors": [
            "Donald Sofge",
            "Gerald Chiang"
        ],
        "summary": "Design, implementation, and machine learning issues associated with developing a control system for a serpentine robotic manipulator are explored. The controller developed provides autonomous control of the serpentine robotic manipulatorduring operation of the manipulator within an enclosed environment such as an underground storage tank. The controller algorithms make use of both low-level joint angle control employing force/position feedback constraints, and high-level coordinated control of end-effector positioning. This approach has resulted in both high-level full robotic control and low-level telerobotic control modes, and provides a high level of dexterity for the operator.",
        "published": "2007-06-07T19:27:12Z",
        "link": "http://arxiv.org/abs/0706.1061v1",
        "categories": [
            "cs.NE",
            "cs.RO"
        ]
    },
    {
        "title": "Workspace Analysis of the Parallel Module of the VERNE Machine",
        "authors": [
            "Daniel Kanaan",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "The paper addresses geometric aspects of a spatial three-degree-of-freedom parallel module, which is the parallel module of a hybrid serial-parallel 5-axis machine tool. This parallel module consists of a moving platform that is connected to a fixed base by three non-identical legs. Each leg is made up of one prismatic and two pairs of spherical joint, which are connected in a way that the combined effects of the three legs lead to an over-constrained mechanism with complex motion. This motion is defined as a simultaneous combination of rotation and translation. A method for computing the complete workspace of the VERNE parallel module for various tool lengths is presented. An algorithm describing this method is also introduced.",
        "published": "2007-07-05T06:43:49Z",
        "link": "http://arxiv.org/abs/0707.0724v1",
        "categories": [
            "cs.RO",
            "physics.class-ph"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the   Astrobiology Phone-cam",
        "authors": [
            "Alexandra Bartolo",
            "Patrick C. McGuire",
            "Kenneth P. Camilleri",
            "Christopher Spiteri",
            "Jonathan C. Borg",
            "Philip J. Farrugia",
            "Jens Ormo",
            "Javier Gomez-Elvira",
            "Jose Antonio Rodriguez-Manfredi",
            "Enrique Diaz-Martinez",
            "Helge Ritter",
            "Robert Haschke",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "We have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. This camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. We envision that the `Astrobiology Phone-cam' exploration system can be fruitfully used in other problem domains as well.",
        "published": "2007-07-05T15:19:37Z",
        "link": "http://arxiv.org/abs/0707.0808v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.NI",
            "cs.RO",
            "cs.SE"
        ]
    },
    {
        "title": "Singular curves and cusp points in the joint space of 3-RPR parallel   manipulators",
        "authors": [
            "Mazen Zein",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This paper investigates the singular curves in two-dimensional slices of the joint space of a family of planar parallel manipulators. It focuses on special points, referred to as cusp points, which may appear on these curves. Cusp points play an important role in the kinematic behavior of parallel manipulators since they make possible a nonsingular change of assembly mode. The purpose of this study is twofold. First, it reviews an important previous work, which, to the authors' knowledge, has never been exploited yet. Second, it determines the cusp points in any two-dimensional slice of the joint space. First results show that the number of cusp points may vary from zero to eight. This work finds applications in both design and trajectory planning.",
        "published": "2007-07-09T07:38:47Z",
        "link": "http://arxiv.org/abs/0707.1193v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Kinematics of Manipulators Built From Closed Planar Mechanisms",
        "authors": [
            "Leonid Slutski",
            "Damien Chablat",
            "Jorge Angeles"
        ],
        "summary": "The paper discusses the kinematics of manipulators builts of planar closed kinematic chains. A special kinematic scheme is extracted from the array of these mechanisms that looks the most promising for the creation of different types of robotic manipulators. The structural features of this manipulator determine a number of its original properties that essentially simplify its control. These features allow the main control problems to be effectively overcome by application of the simple kinematic problems. The workspace and singular configurations of a basic planar manipulator are studied. By using a graphic simulation method, motions of the designed mechanism are examined. A prototype of this mechanism was implemented to verify the proposed approach.",
        "published": "2007-07-12T15:42:46Z",
        "link": "http://arxiv.org/abs/0707.1824v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Moveability and Collision Analysis for Fully-Parallel Manipulators",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The aim of this paper is to characterize the moveability of fully-parallel manipulators in the presence of obstacles. Fully parallel manipulators are used in applications where accuracy, stiffness or high speeds and accelerations are required \\cite{Merlet:97}. However, one of its main drawbacks is a relatively small workspace compared to the one of serial manipulators. This is due mainly to the existence of potential internal collisions, and the existence of singularities. In this paper, the notion of free aspect is defined which permits to exhibit domains of the workspace and the joint space free of singularity and collision. The main application of this study is the moveability analysis in the workspace of the manipulator as well as path-planning, control and design.",
        "published": "2007-07-13T09:59:25Z",
        "link": "http://arxiv.org/abs/0707.1957v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Working Modes and Aspects in Fully-Parallel Manipulator",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The aim of this paper is to characterize the notion of aspect in the workspace and in the joint space for parallel manipulators. In opposite to the serial manipulators, the parallel manipulators can admit not only multiple inverse kinematic solutions, but also multiple direct kinematic solutions. The notion of aspect introduced for serial manipulators in [Borrel 86], and redefined for parallel manipulators with only one inverse kinematic solution in [Wenger 1997], is redefined for general fully parallel manipulators. Two Jacobian matrices appear in the kinematic relations between the joint-rate and the Cartesian-velocity vectors, which are called the \"inverse kinematics\" and the \"direct kinematics\" matrices. The study of these matrices allow to respectively define the parallel and the serial singularities. The notion of working modes is introduced to separate inverse kinematic solutions. Thus, we can find out domains of the workspace and the joint space exempt of singularity. Application of this study is the moveability analysis in the workspace of the manipulator as well as path-planing and control. This study is illustrated in this paper with a RR-RRR planar parallel manipulator.",
        "published": "2007-07-13T13:40:56Z",
        "link": "http://arxiv.org/abs/0707.2006v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Isoconditioning Loci of A Class of Closed-Chain Manipulators",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger",
            "Jorge Angeles"
        ],
        "summary": "The subject of this paper is a special class of closed-chain manipulators. First, we analyze a family of two-degree-of-freedom (dof) five-bar planar linkages. Two Jacobian matrices appear in the kinematic relations between the joint-rate and the Cartesian-velocity vectors, which are called the ``inverse kinematics\" and the \"direct kinematics\" matrices. It is shown that the loci of points of the workspace where the condition number of the direct-kinematics matrix remains constant, i.e., the isoconditioning loci, are the coupler points of the four-bar linkage obtained upon locking the middle joint of the linkage. Furthermore, if the line of centers of the two actuated revolutes is used as the axis of a third actuated revolute, then a three-dof hybrid manipulator is obtained. The isoconditioning loci of this manipulator are surfaces of revolution generated by the isoconditioning curves of the two-dof manipulator, whose axis of symmetry is that of the third actuated revolute.",
        "published": "2007-07-13T14:18:45Z",
        "link": "http://arxiv.org/abs/0707.2017v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Workspace and Assembly modes in Fully-Parallel Manipulators : A   Descriptive Study",
        "authors": [
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "The goal of this paper is to explain, using a typical example, the distribution of the different assembly modes in the workspace and their effective role in the execution of trajectories. The singular and non-singular changes of assembly mode are described and compared to each other. The non-singular change of assembly mode is more deeply analysed and discussed in the context of trajectory planning. In particular, it is shown that, according to the location of the initial and final configurations with respect to the uniqueness domains in the workspace, there are three different cases to consider before planning a linking trajectory.",
        "published": "2007-07-13T15:04:13Z",
        "link": "http://arxiv.org/abs/0707.2027v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Conception Isotropique D'Une Morphologie ParallÈle : Application à   L'Usinage",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger",
            "Jorge Angeles"
        ],
        "summary": "The aim of this paper is the isotropic design of a hybrid morphology dedicated to 3-axis machining applications. It is necessary to ensure the feasibility of continuous, singularity-free trajectories, as well as a good manipulability in position and velocity. We want to propose an alternative design to conventional serial machine-tools. We compare a serial PPP machine-tool (three prismatic orthogonal axes) with a hybrid architecture which we optimize only the first two axes. The critrerion used for the optimization is the conditioning of the Jacobian matrices. The optimum, namely isotropy, can be obtained which provides our architecture with excellent manipulability properties.",
        "published": "2007-07-13T15:24:29Z",
        "link": "http://arxiv.org/abs/0707.2034v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A distributed Approach for Access and Visibility Task under Ergonomic   Constraints with a Manikin in a Virtual Reality Environment",
        "authors": [
            "Florence Bidault",
            "Damien Chablat",
            "Patrick Chedmail",
            "Laurent Pino"
        ],
        "summary": "This paper presents a new method, based on a multi-agent system and on digital mock-up technology, to assess an efficient path planner for a manikin for access and visibility task under ergonomic constraints. In order to solve this problem, the human operator is integrated in the process optimization to contribute to a global perception of the environment. This operator cooperates, in real-time, with several automatic local elementary agents. The result of this work validates solutions brought by digital mock-up and that can be applied to simulate maintenance task.",
        "published": "2007-07-13T15:34:22Z",
        "link": "http://arxiv.org/abs/0707.2042v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Modélisation Dynamique d'un Robot Parallèle à 3-DDL : l'Orthoglide",
        "authors": [
            "Sylvain Guegan",
            "Wisama Khalil",
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "In this article, we propose a method for calculation of the inverse and direct dynamic models of the Orthoglide, a parallel robot with threedegrees of freedom in translation. These models are calculated starting from the elements of the dynamic model of the kinematic chain structure and equations of Newton-Euler applied to the platform. These models are obtained in explicit form having an interesting physical interpretation.",
        "published": "2007-07-15T07:14:51Z",
        "link": "http://arxiv.org/abs/0707.2185v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Degeneracy study of the forward kinematics of planar 3-RPR parallel   manipulators",
        "authors": [
            "Philippe Wenger",
            "Damien Chablat",
            "Mazen Zein"
        ],
        "summary": "This paper investigates two situations in which the forward kinematics of planar 3-RPR parallel manipulators degenerates. These situations have not been addressed before. The first degeneracy arises when the three input joint variables r1, r2 and r3 satisfy a certain relationship. This degeneracy yields a double root of the characteristic polynomial in t, which could be erroneously interpreted as two coalesce assembly modes. But, unlike what arises in non-degenerate cases, this double root yields two sets of solutions for the position coordinates (x, y) of the platform. In the second situation, we show that the forward kinematics degenerates over the whole joint space if the base and platform triangles are congruent and the platform triangle is rotated by 180 deg about one of its sides. For these \"degenerate\" manipulators, which are defined here for the first time, the forward kinematics is reduced to the solution of a 3rd-degree polynomial and a quadratics in sequence. Such manipulators constitute, in turn, a new family of analytic planar manipulators that would be more suitable for industrial applications.",
        "published": "2007-07-15T17:57:21Z",
        "link": "http://arxiv.org/abs/0707.2227v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Kinematic Analysis of a Family of 3R Manipulators",
        "authors": [
            "Maher Baili",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "The workspace topologies of a family of 3-revolute (3R) positioning manipulators are enumerated. The workspace is characterized in a half-cross section by the singular curves. The workspace topology is defined by the number of cusps that appear on these singular curves. The design parameters space is shown to be divided into five domains where all manipulators have the same number of cusps. Each separating surface is given as an explicit expression in the DH-parameters. As an application of this work, we provide a necessary and sufficient condition for a 3R orthogonal manipulator to be cuspidal, i.e. to change posture without meeting a singularity. This condition is set as an explicit expression in the DH parameters.",
        "published": "2007-07-15T18:00:53Z",
        "link": "http://arxiv.org/abs/0707.2228v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Computation of All 4R Serial Spherical Wrists With an Isotropic   Architecture",
        "authors": [
            "Damien Chablat",
            "Jorge Angeles"
        ],
        "summary": "A spherical wrist of the serial type with n revolute (R) joints is said to be isotropic if it can attain a posture whereby the singular values of its Jacobian matrix are all equal to sqrt(n/3). What isotropy brings about is robustness to manufacturing, assembly, and measurement errors, thereby guaranteeing a maximum orientation accuracy. In this paper we investigate the existence of redundant isotropic architectures, which should add to the dexterity of the wrist under design by virtue of its extra degree of freedom. The problem formulation, for, leads to a system of eight quadratic equations with eight unknowns. The Bezout number of this system is thus 2^8=256, its BKK bound being 192. However, the actual number of solutions is shown to be 32. We list all solutions of the foregoing algebraic problem. All these solutions are real, but distinct solutions do not necessarily lead to distinct manipulators. Upon discarding those algebraic solutions that yield no new wrists, we end up with exactly eight distinct architectures, the eight corresponding manipulators being displayed at their isotropic postures.",
        "published": "2007-07-15T18:04:09Z",
        "link": "http://arxiv.org/abs/0707.2229v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A design oriented study for 3R Orthogonal Manipulators With Geometric   Simplifications",
        "authors": [
            "Mazen Zein",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This paper proposes a method to calculate the largest Regular Dextrous Workspace (RDW) of some types of three-revolute orthogonal manipulators that have at least one of their DH parameters equal to zero. Then a new performance index based on the RDW is introduced, the isocontours of this index are plotted in the parameter space of the interesting types of manipulators and finally an inspection of the domains of the parameter spaces is conducted in order to identify the better manipulator architectures. The RDW is a part of the workspace whose shape is regular (cube, cylinder) and the performances (conditioning index) are bounded inside. The groups of 3R orthogonal manipulators studied have interesting kinematic properties such as, a well-connected workspace that is fully reachable with four inverse kinematic solutions and that does not contain any void. This study is of high interest for the design of alternative manipulator geometries.",
        "published": "2007-07-15T20:25:24Z",
        "link": "http://arxiv.org/abs/0707.2238v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Design of a Spherical Wrist with Parallel Architecture: Application to   Vertebrae of an Eel Robot",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The design of a spherical wrist with parallel architecture is the object of this article. This study is part of a larger project, which aims to design and to build an eel robot for inspection of immersed piping. The kinematic analysis of the mechanism is presented first to characterize the singular configurations as well as the isotropic configurations. We add the design constraints related to the application, such as (i) the compactness of the mechanism, (ii) the symmetry of the elements in order to ensure static and dynamic balance and (iii) the possibility of the mechanism to fill the elliptic form of the ell sections.",
        "published": "2007-07-16T07:41:16Z",
        "link": "http://arxiv.org/abs/0707.2270v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Passive Control Architecture for Virtual Humans",
        "authors": [
            "Antoine Rennuit",
            "Alain Micaelli",
            "Xavier Merlhiot",
            "Claude Andriot",
            "François Guillaume",
            "Nicolas Chevassus",
            "Damien Chablat",
            "Patrick Chedmail"
        ],
        "summary": "In the present paper, we introduce a new control architecture aimed at driving virtual humans in interaction with virtual environments, by motion capture. It brings decoupling of functionalities, and also of stability thanks to passivity. We show projections can break passivity, and thus must be used carefully. Our control scheme enables task space and internal control, contact, and joint limits management. Thanks to passivity, it can be easily extended. Besides, we introduce a new tool as for manikin's control, which makes it able to build passive projections, so as to guide the virtual manikin when sharp movements are needed.",
        "published": "2007-07-16T08:33:43Z",
        "link": "http://arxiv.org/abs/0707.2275v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Animation of virtual mannequins, robot-like simulation or motion   captures",
        "authors": [
            "Damien Chablat"
        ],
        "summary": "In order to optimize the costs and time of design of the new products while improving their quality, concurrent engineering is based on the digital model of these products, the numerical model. However, in order to be able to avoid definitively physical model, old support of the design, without loss of information, new tools must be available. Especially, a tool making it possible to check simply and quickly the maintainability of complex mechanical sets using the numerical model is necessary. Since one decade, our team works on the creation of tool for the generation and the analysis of trajectories of virtual mannequins. The simulation of human tasks can be carried out either by robot-like simulation or by simulation by motion capture. This paper presents some results on the both two methods. The first method is based on a multi-agent system and on a digital mock-up technology, to assess an efficient path planner for a manikin or a robot for access and visibility task taking into account ergonomic constraints or joint and mechanical limits. In order to solve this problem, the human operator is integrated in the process optimization to contribute to a global perception of the environment. This operator cooperates, in real-time, with several automatic local elementary agents. In the case of the second approach, we worked with the CEA and EADS/CCR to solve the constraints related to the evolution of human virtual in its environment on the basis of data resulting from motion capture system. An approach using of the virtual guides was developed to allow to the user the realization of precise trajectory in absence of force feedback. The result of this work validates solutions through the digital mock-up; it can be applied to simulate maintenability and mountability tasks.",
        "published": "2007-07-18T12:59:15Z",
        "link": "http://arxiv.org/abs/0707.2718v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A Framework to Illustrate Kinematic Behavior of Mechanisms by Haptic   Feedback",
        "authors": [
            "Qinqin Zhang",
            "Damien Chablat",
            "Fouad Bennis",
            "Wei Zhang"
        ],
        "summary": "The kinematic properties of mechanisms are well known by the researchers and teachers. The theory based on the study of Jacobian matrices allows us to explain, for example, the singular configuration. However, in many cases, the physical sense of such properties is difficult to explain to students. The aim of this article is to use haptic feedback to render to the user the signification of different kinematic indices. The framework uses a Phantom Omni and a serial and parallel mechanism with two degrees of freedom. The end-effector of both mechanisms can be moved either by classical mouse, or Phantom Omni with or without feedback.",
        "published": "2007-07-18T13:02:29Z",
        "link": "http://arxiv.org/abs/0707.2721v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A Comparative Study between Two Three-DOF Parallel Kinematic Machines   using Kinetostatic Criteria and Interval Analysis",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger",
            "Jean-Pierre Merlet"
        ],
        "summary": "This paper addresses the workspace analysis of two 3-DOF translational parallel mechanisms designed for machining applications. The two machines features three fixed linear joints. The joint axes of the first machine are orthogonal whereas these of the second are parallel. In both cases, the mobile platform moves in the Cartesian $x-y-z$ space with fixed orientation. The workspace analysis is conducted on the basis of prescribed kinetostatic performances. Interval analysis based methods are used to compute the dextrous workspace and the largest cube enclosed in this workspace.",
        "published": "2007-07-19T04:44:06Z",
        "link": "http://arxiv.org/abs/0707.2833v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Virtual Manufacturing concept: Scope, Socio-Economic Aspects and   Future Trends",
        "authors": [
            "Damien Chablat",
            "Philippe Dépincé",
            "Eric Noël",
            "Peer-Oliver Woelk"
        ],
        "summary": "The research area \"Virtual Manufacturing (VM)'' is the use of information technology and computer simulation to model real world manufacturing processes for the purpose of analysing and understanding them. As automation technologies such as CAD/CAM have substantially shortened the time required to design products, Virtual Manufacturing will have a similar effect on the manufacturing phase thanks to the modelling, simulation and optimisation of the product and the processes involved in its fabrication. After a description of Virtual Manufacturing (definitions and scope), we present some socio-economic factors of VM and finaly some \"hot topics'' for the future are proposed.",
        "published": "2007-07-19T07:15:53Z",
        "link": "http://arxiv.org/abs/0707.2841v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A Classification of 3R Orthogonal Manipulators by the Topology of their   Workspace",
        "authors": [
            "Maher Baili",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "A classification of a family of 3-revolute (3R) positining manipulators is established. This classification is based on the topology of their workspace. The workspace is characterized in a half-cross section by the singular curves. The workspace topology is defined by the number of cusps and nodes that appear on these singular curves. The design parameters space is shown to be divided into nine domains of distinct workspace topologies, in which all manipulators have similar global kinematic properties. Each separating surface is given as an explicit expression in the DH-parameters.",
        "published": "2007-07-19T07:20:26Z",
        "link": "http://arxiv.org/abs/0707.2842v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Kinematic and stiffness analysis of the Orthoglide, a PKM with simple,   regular workspace and homogeneous performances",
        "authors": [
            "Anatoly Pashkevich",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "The Orthoglide is a Delta-type PKM dedicated to 3-axis rapid machining applications that was originally developed at IRCCyN in 2000-2001 to meet the advantages of both serial 3-axis machines (regular workspace and homogeneous performances) and parallel kinematic architectures (good dynamic performances and stiffness). This machine has three fixed parallel linear joints that are mounted orthogonally. The geometric parameters of the Orthoglide were defined as function of the size of a prescribed cubic Cartesian workspace that is free of singularities and internal collision. The interesting features of the Orthoglide are a regular Cartesian workspace shape, uniform performances in all directions and good compactness. In this paper, a new method is proposed to analyze the stiffness of overconstrained Delta-type manipulators, such as the Orthoglide. The Orthoglide is then benchmarked according to geometric, kinematic and stiffness criteria: workspace to footprint ratio, velocity and force transmission factors, sensitivity to geometric errors, torsional stiffness and translational stiffness.",
        "published": "2007-07-21T05:24:17Z",
        "link": "http://arxiv.org/abs/0707.3186v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Workspace and Kinematic Analysis of the VERNE machine",
        "authors": [
            "Daniel Kanaan",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This paper describes the workspace and the inverse and direct kinematic analysis of the VERNE machine, a serial/parallel 5-axis machine tool designed by Fatronik for IRCCyN. This machine is composed of a three-degree-of-freedom (DOF) parallel module and a two-DOF serial tilting table. The parallel module consists of a moving platform that is connected to a fixed base by three non-identical legs. This feature involves (i) a simultaneous combination of rotation and translation for the moving platform, which is balanced by the tilting table and (ii) workspace whose shape and volume vary as a function of the tool length. This paper summarizes results obtained in the context of the European projects NEXT (\"Next Generation of Productions Systems\").",
        "published": "2007-07-24T09:13:22Z",
        "link": "http://arxiv.org/abs/0707.3507v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Kinetostatic Optimization of a Novel Prismatic Drive",
        "authors": [
            "Damien Chablat",
            "Stéphane Caro"
        ],
        "summary": "The design of a mechanical transmission taking into account the transmitted forces is reported in this paper. This transmission is based on Slide-o-Cam, a cam mechanism with multiple rollers mounted on a common translating follower. The design of Slide-o-Cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. This transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. The pressure angle is a suitable performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. To assess the transmission capability of the mechanism, the Hertz formula is introduced to calculate the stresses on the rollers and on the cams. The final transmission is intended to replace the current ball-screws in the Orthoglide, a three-DOF parallel robot for the production of translational motions, currently under development for machining applications at Ecole Centrale de Nantes.",
        "published": "2007-07-24T12:18:05Z",
        "link": "http://arxiv.org/abs/0707.3534v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A Six Degree-Of-Freedom Haptic Device Based On The Orthoglide And A   Hybrid Agile Eye",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "This paper is devoted to the kinematic design of a new six degree-of-freedom haptic device using two parallel mechanisms. The first one, called orthoglide, provides the translation motions and the second one, called agile eye, produces the rotational motions. These two motions are decoupled to simplify the direct and inverse kinematics, as it is needed for real-time control. To reduce the inertial load, the motors are fixed on the base and a transmission with two universal joints is used to transmit the rotational motions from the base to the end-effector. Two alternative wrists are proposed (i), the agile eye with three degrees of freedom or (ii) a hybrid wrist made by the assembly of a two-dof agile eye with a rotary motor. The last one is optimized to increase its stiffness and to decrease the number of moving parts.",
        "published": "2007-07-24T13:42:08Z",
        "link": "http://arxiv.org/abs/0707.3550v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Analyse Comparative des Manipulateurs 3R à Axes Orthogonaux",
        "authors": [
            "Maher Baili",
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "A family of 3R orthogonal manipulators without offset on the third body can be divided into exactly nine workspace topologies. The workspace is characterized in a half-cross section by the singular curves. The workspace topology is defined by the number of cusps and nodes that appear on these singular curves. Based on this classification, we evaluate theses manipulators by the condition number related to the joint space and the proportion of the region with four inverse kinematic solutions compared to a sphere containing all the workspace. This second performance number is in relation with the workspace. We determine finally le topology of workspace to which belong manipulators having the best performance number values.",
        "published": "2007-07-24T13:46:22Z",
        "link": "http://arxiv.org/abs/0707.3552v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "An Exhaustive Study of the Workspace Topologies of all 3R Orthogonal   Manipulators with Geometric Simplifications",
        "authors": [
            "Mazen Zein",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This paper analyses the workspace of the three-revolute orthogonal manipulators that have at least one of their DH parameters equal to zero. These manipulators are classified into different groups with similar kinematic properties. The classification criteria are based on the topology of the workspace. Each group is evaluated according to interesting kinematic properties such as the size of the workspace subregion reachable with four inverse kinematic solutions, the existence and the size of voids, and the size of the regions of feasible paths in the workspace.",
        "published": "2007-07-24T13:49:51Z",
        "link": "http://arxiv.org/abs/0707.3553v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Integration of a Balanced Virtual Manikin in a Virtual Reality Platform   aimed at Virtual Prototyping",
        "authors": [
            "Antoine Rennuit",
            "Alain Micaelli",
            "Xavier Merlhiot",
            "Claude Andriot",
            "François Guillaume",
            "Nicolas Chevassus",
            "Damien Chablat",
            "Patrick Chedmail"
        ],
        "summary": "The work presented here is aimed at introducing a virtual human controller in a virtual prototyping framework. After a brief introduction describing the problem solved in the paper, we describe the interest as for digital humans in the context of concurrent engineering. This leads us to draw a control architecture enabling to drive virtual humans in a real-time immersed way, and to interact with the product, through motion capture. Unfortunately, we show this control scheme can lead to unfeasible movements because of the lack of balance control. Introducing such a controller is a problem that was never addressed in the context of real-time. We propose an implementation of a balance controller, that we insert into the previously described control scheme. Next section is dedicated to show the results we obtained. Finally, we propose a virtual reality platform into which the digital character controller is integrated.",
        "published": "2007-07-24T14:23:37Z",
        "link": "http://arxiv.org/abs/0707.3560v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Balanced Virtual Humans Interacting with their Environment",
        "authors": [
            "Antoine Rennuit",
            "Alain Micaelli",
            "Xavier Merlhiot",
            "Claude Andriot",
            "François Guillaume",
            "Nicolas Chevassus",
            "Damien Chablat",
            "Patrick Chedmail"
        ],
        "summary": "The animation of human avatars seems very successful; the computer graphics industry shows outstanding results in films everyday, the game industry achieves exploits... Nevertheless, the animation and control processes of such manikins are very painful. It takes days to a specialist to build such animated sequences, and it is not adaptive to any type of modifications. Our main purpose is the virtual human for engineering, especially virtual prototyping. As for this domain of activity, such amounts of time are prohibitive.",
        "published": "2007-07-24T14:26:28Z",
        "link": "http://arxiv.org/abs/0707.3562v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Virtual reality: A human centered tool for improving Manufacturing",
        "authors": [
            "Fouad Bennis",
            "Damien Chablat",
            "Philippe Dépincé"
        ],
        "summary": "Manufacturing is using Virtual Reality tools to enhance the product life cycle. Their definitions are still in flux and it is necessary to define their connections. Thus, firstly, we will introduce more closely some definitions where we will find that, if the Virtual manufacturing concepts originate from machining operations and evolve in this manufacturing area, there exist a lot of applications in different fields such as casting, forging, sheet metalworking and robotics (mechanisms). From the recent projects in Europe or in USA, we notice that the human perception or the simulation of mannequin is more and more needed in both fields. In this context, we have isolated some applications as ergonomic studies, assembly and maintenance simulation, design or training where the virtual reality tools can be applied. Thus, we find out a family of applications where the virtual reality tools give the engineers the main role in the optimization process. We will illustrate our paper by several examples where virtual reality interfaces are used and combined with optimization tools as multi-agent systems.",
        "published": "2007-07-24T14:28:01Z",
        "link": "http://arxiv.org/abs/0707.3563v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A New Six Degree-of-Freedom Haptic Device based on the Orthoglide and   the Agile Eye",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The aim of this paper is to present a new six degree-of-freedom (dof) haptic device using two parallel mechanisms. The first one, called orthoglide, provides the translation motions and the second one produces the rotational motions. These two motions are decoupled to simplify the direct and inverse kinematics, as it is needed for real-times control. To reduce the inertial load, the motors are fixed on the base and a transmission with two universal joints is used to transmit the rotational motions from the base to the end-effector. The main feature of the orthoglide and of the agile eye mechanism is the existence of an isotropic configuration. The length of the legs and the range limits of the orthoglide are optimized to have homogeneous performance throughout the Cartesian workspace, which has a nearly cubic workspace. These properties permit to have a high stiffness throughout the workspace and workspace limits that are easily understandable by the user.",
        "published": "2007-07-24T14:29:41Z",
        "link": "http://arxiv.org/abs/0707.3564v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "L'orthoglide : une machine-outil rapide d'architecture parallèle   isotrope",
        "authors": [
            "Philippe Wenger",
            "Damien Chablat",
            "Félix Majou"
        ],
        "summary": "This article presents the Orthoglide project. The purpose of this project is the realization of a prototype of machine tool to three degrees of translation. The characteristic of this machine is a parallel kinematic architecture optimized to obtain a compact workspace with homogeneous performance. For that, the principal criterion of design which was used is the isotropy.",
        "published": "2007-07-24T15:14:38Z",
        "link": "http://arxiv.org/abs/0707.3574v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A Comparative Study of Parallel Kinematic Architectures for Machining   Applications",
        "authors": [
            "Philippe Wenger",
            "Clément Gosselin",
            "Damien Chablat"
        ],
        "summary": "Parallel kinematic mechanisms are interesting alternative designs for machining applications. Three 2-DOF parallel mechanism architectures dedicated to machining applications are studied in this paper. The three mechanisms have two constant length struts gliding along fixed linear actuated joints with different relative orientation. The comparative study is conducted on the basis of a same prescribed Cartesian workspace for the three mechanisms. The common desired workspace properties are a rectangular shape and given kinetostatic performances. The machine size of each resulting design is used as a comparative criterion. The 2-DOF machine mechanisms analyzed in this paper can be extended to 3-axis machines by adding a third joint.",
        "published": "2007-07-25T04:40:01Z",
        "link": "http://arxiv.org/abs/0707.3665v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Kinematic Analysis of a New Parallel Machine Tool: the Orthoglide",
        "authors": [
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This paper describes a new parallel kinematic architecture for machining applications: the orthoglide. This machine features three fixed parallel linear joints which are mounted orthogonally and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The main interest of the orthoglide is that it takes benefit from the advantages of the popular PPP serial machines (regular Cartesian workspace shape and uniform performances) as well as from the parallel kinematic arrangement of the links (less inertia and better dynamic performances), which makes the orthoglide well suited to high-speed machining applications. Possible extension of the orthoglide to 5-axis machining is also investigated.",
        "published": "2007-07-25T04:42:19Z",
        "link": "http://arxiv.org/abs/0707.3666v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Computation of All 4R Serial Spherical Wrists With an Isotropic   Architecture",
        "authors": [
            "Damien Chablat",
            "Jorge Angeles"
        ],
        "summary": "A spherical wrist of the serial type is said to be isotropic if it can attain a posture whereby the singular values of its Jacobian matrix are all identical and nonzero. What isotropy brings about is robustness to manufacturing, assembly, and measurement errors, thereby guaranteeing a maximum orientation accuracy. In this paper we investigate the existence of redundant isotropic architectures, which should add to the dexterity of the wrist under design by virtue of its extra degree of freedom. The problem formulation leads to a system of eight quadratic equations with eight unknowns. The Bezout number of this system is thus 2^8 = 256, its BKK bound being 192. However, the actual number of solutions is shown to be 32. We list all solutions of the foregoing algebraic problem. All these solutions are real, but distinct solutions do not necessarily lead to distinct manipulators. Upon discarding those algebraic solutions that yield no new wrists, we end up with exactly eight distinct architectures, the eight corresponding manipulators being displayed at their isotropic posture.",
        "published": "2007-07-25T06:51:53Z",
        "link": "http://arxiv.org/abs/0707.3673v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Virtual Manufacturing : Tools for improving Design and Production",
        "authors": [
            "Philippe Dépincé",
            "Damien Chablat",
            "Peer-Oliver Woelk"
        ],
        "summary": "The research area \"Virtual Manufacturing\" can be defined as an integrated manufacturing environment which can enhance one or several levels of decision and control in manufacturing process. Several domains can be addressed: Product and Process Design, Process and Production Planning, Machine Tool, Robot and Manufacturing System. As automation technologies such as CAD/CAM have substantially shortened the time required to design products, Virtual Manufacturing will have a similar effect on the manufacturing phase thanks to the modelling, simulation and optimisation of the product and the processes involved in its fabrication.",
        "published": "2007-08-03T11:15:21Z",
        "link": "http://arxiv.org/abs/0708.0495v1",
        "categories": [
            "cs.RO",
            "physics.class-ph"
        ]
    },
    {
        "title": "Real-time control and monitoring system for LIPI's Public Cluster",
        "authors": [
            "I. Firmansyah",
            "B. Hermanto",
            "Hadiyanto",
            "L. T. Handoko"
        ],
        "summary": "We have developed a monitoring and control system for LIPI's Public Cluster. The system consists of microcontrollers and full web-based user interfaces for daily operation. It is argued that, due to its special natures, the cluster requires fully dedicated and self developed control and monitoring system. We discuss the implementation of using parallel port and dedicated micro-controller for this purpose. We also show that integrating such systems enables an autonomous control system based on the real time monitoring, for instance an autonomous power supply control based on the actual temperature, etc.",
        "published": "2007-08-04T05:16:43Z",
        "link": "http://arxiv.org/abs/0708.0607v1",
        "categories": [
            "cs.DC",
            "cs.RO"
        ]
    },
    {
        "title": "An Interval Analysis Based Study for the Design and the Comparison of   3-DOF Parallel Kinematic Machines",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger",
            "Félix Majou",
            "Jean-Pierre Merlet"
        ],
        "summary": "This paper addresses an interval analysis based study that is applied to the design and the comparison of 3-DOF parallel kinematic machines. Two design criteria are used, (i) a regular workspace shape and, (ii) a kinetostatic performance index that needs to be as homogeneous as possible throughout the workspace. The interval analysis based method takes these two criteria into account: on the basis of prescribed kinetostatic performances, the workspace is analysed to find out the largest regular dextrous workspace enclosed in the Cartesian workspace. An algorithm describing this method is introduced. Two 3-DOF translational parallel mechanisms designed for machining applications are compared using this method. The first machine features three fixed linear joints which are mounted orthogonally and the second one features three linear joints which are mounted in parallel. In both cases, the mobile platform moves in the Cartesian x-y-z space with fixed orientation.",
        "published": "2007-08-08T07:06:48Z",
        "link": "http://arxiv.org/abs/0708.1049v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Architecture Optimization of a 3-DOF Translational Parallel Mechanism   for Machining Applications, the Orthoglide",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "This paper addresses the architecture optimization of a 3-DOF translational parallel mechanism designed for machining applications. The design optimization is conducted on the basis of a prescribed Cartesian workspace with prescribed kinetostatic performances. The resulting machine, the Orthoglide, features three fixed parallel linear joints which are mounted orthogonally and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The interesting features of the Orthoglide are a regular Cartesian workspace shape, uniform performances in all directions and good compactness. A small-scale prototype of the Orthoglide under development is presented at the end of this paper.",
        "published": "2007-08-24T18:25:36Z",
        "link": "http://arxiv.org/abs/0708.3381v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Parametric stiffness analysis of the Orthoglide",
        "authors": [
            "Félix Majou",
            "Clément Gosselin",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This paper presents a parametric stiffness analysis of the Orthoglide. A compliant modeling and a symbolic expression of the stiffness matrix are conducted. This allows a simple systematic analysis of the influence of the geometric design parameters and to quickly identify the critical link parameters. Our symbolic model is used to display the stiffest areas of the workspace for a specific machining task. Our approach can be applied to any parallel manipulator for which stiffness is a critical issue.",
        "published": "2007-08-27T13:59:35Z",
        "link": "http://arxiv.org/abs/0708.3607v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Kinematics and Workspace Analysis of a Three-Axis Parallel Manipulator:   the Orthoglide",
        "authors": [
            "Anatoly Pashkevich",
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The paper addresses kinematic and geometrical aspects of the Orthoglide, a three-DOF parallel mechanism. This machine consists of three fixed linear joints, which are mounted orthogonally, three identical legs and a mobile platform, which moves in the Cartesian x-y-z space with fixed orientation. New solutions to solve inverse/direct kinematics are proposed and we perform a detailed workspace and singularity analysis, taking into account specific joint limit constraints.",
        "published": "2007-08-27T14:05:34Z",
        "link": "http://arxiv.org/abs/0708.3613v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Parametric Stiffness Analysis of the Orthoglide",
        "authors": [
            "Félix Majou",
            "Clément Gosselin",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This paper presents a parametric stiffness analysis of the Orthoglide, a 3-DOF translational Parallel Kinematic Machine. First, a compliant modeling of the Orthoglide is conducted based on an existing method. Then stiffness matrix is symbolically computed. This allows one to easily study the influence of the geometric design parameters on the matrix elements. Critical links are displayed. Cutting forces are then modeled so that static displacements of the Orthoglide tool during slot milling are symbolically computed. Influence of the geometric design parameters on the static displacements is checked as well. Other machining operations can be modeled. This parametric stiffness analysis can be applied to any parallel manipulator for which stiffness is a critical issue.",
        "published": "2007-08-28T07:17:46Z",
        "link": "http://arxiv.org/abs/0708.3723v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Design Strategies for the Geometric Synthesis of Orthoglide-type   Mechanisms",
        "authors": [
            "Anatoly Pashkevich",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "The paper addresses the geometric synthesis of Orthoglide-type mechanism, a family of 3-DOF parallel manipulators for rapid machining applications, which combine advantages of both serial mechanisms and parallel kinematic architectures. These manipulators possess quasi-isotropic kinematic performances and are made up of three actuated fixed prismatic joints, which are mutually orthogonal and connected to a mobile platform via three parallelogram chains. The platform moves in the Cartesian space with fixed orientation, similar to conventional XYZ-machine. Three strategies have been proposed to define the Orthoglide geometric parameters (manipulator link lengths and actuated joint limits) as functions of a cubic workspace size and dextrous properties expressed by bounds on the velocity transmission factors, manipulability or the Jacobian condition number. Low inertia and intrinsic stiffness have been set as additional design goals expressed by the minimal link length requirement. For each design strategy, analytical expressions for computing the Orthoglide parameters are proposed. It is showed that the proposed strategies yield Pareto-optimal solutions, which differ by the kinematic performances outside the prescribed Cartesian cube (but within the workspace bounded by the actuated joint limits). The proposed technique is illustrated with numerical examples for the Orthoglide prototype design.",
        "published": "2007-08-28T15:40:39Z",
        "link": "http://arxiv.org/abs/0708.3809v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "An Exhaustive Study of the Workspaces Tolopogies of all 3R Orthogonal   Manipulators with Geometric Simplifications",
        "authors": [
            "Mazen Zein",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This paper proposes a classification of three-revolute orthogonal manipulators that have at least one of their DH parameters equal to zero. This classification is based on the topology of their workspace. The workspace is characterized in a half-cross section by the singular curves. The workspace topology is defined by the number of cusps and nodes that appear on these singular curves. The manipulators are classified into different types with similar kinematic properties. Each type is evaluated according to interesting kinematic properties such as, whether the workspace is fully reachable with four inverse kinematic solutions or not, the existence of voids, and the feasibility of continuous trajectories in the workspace. It is found that several orthogonal manipulators have a \"well-connected\" workspace, that is, their workspace is fully accessible with four inverse kinematic solutions and any continuous trajectory is feasible. This result is of interest for the design of alternative manipulator geometries.",
        "published": "2007-08-28T15:51:50Z",
        "link": "http://arxiv.org/abs/0708.3811v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The Isoconditioning Loci of Planar Three-DOF Parallel Manipulators",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger",
            "Stéphane Caro",
            "Jorge Angeles"
        ],
        "summary": "The subject of this paper is a special class of three-degree-of-freedom parallel manipulators. The singular configurations of the two Jacobian matrices are first studied. The isotropic configurations are then found based on the characteristic length of this manipulator. The isoconditioning loci for the Jacobian matrices are plotted to define a global performance index allowing the comparison of the different working modes. The index thus resulting is compared with the Cartesian workspace surface and the average of the condition number.",
        "published": "2007-08-29T07:08:21Z",
        "link": "http://arxiv.org/abs/0708.3896v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Kinematic analysis of the 3-RPR parallel manipulator",
        "authors": [
            "Damien Chablat",
            "Philippe Wenger",
            "Ilian Bonev"
        ],
        "summary": "The aim of this paper is the kinematic study of a 3-RPR planar parallel manipulator where the three fixed revolute joints are actuated. The direct and inverse kinematic problem as well as the singular configuration is characterized. On parallel singular configurations, the motion produce by the mobile platform can be compared to the Reuleaux straight-line mechanism.",
        "published": "2007-08-29T09:28:52Z",
        "link": "http://arxiv.org/abs/0708.3920v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Working and Assembly Modes of the Agile Eye",
        "authors": [
            "Ilian Bonev",
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "This paper deals with the in-depth kinematic analysis of a special spherical parallel wrist, called the Agile Eye. The Agile Eye is a three-legged spherical parallel robot with revolute joints in which all pairs of adjacent joint axes are orthogonal. Its most peculiar feature, demonstrated in this paper for the first time, is that its (orientation) workspace is unlimited and flawed only by six singularity curves (rather than surfaces). Furthermore, these curves correspond to self-motions of the mobile platform. This paper also demonstrates that, unlike for any other such complex spatial robots, the four solutions to the direct kinematics of the Agile Eye (assembly modes) have a simple geometric relationship with the eight solutions to the inverse kinematics (working modes).",
        "published": "2007-08-29T11:24:41Z",
        "link": "http://arxiv.org/abs/0708.3936v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Sensitivity Analysis of the Orthoglide, a 3-DOF Translational Parallel   Kinematic Machine",
        "authors": [
            "Stéphane Caro",
            "Philippe Wenger",
            "Fouad Bennis",
            "Damien Chablat"
        ],
        "summary": "This paper presents a sensitivity analysis of the Orthoglide, a 3-DOF translational Parallel Kinematic Machine. Two complementary methods are developed to analyze its sensitivity to its dimensional and angular variations. First, a linkage kinematic analysis method is used to have a rough idea of the influence of the dimensional variations on the location of the end-effector. Besides, this method shows that variations in the design parameters of the same type from one leg to the other have the same influence on the end-effector. However, this method does not take into account the variations in the parallelograms. Thus, a differential vector method is used to study the influence of the dimensional and angular variations in the parts of the manipulator on the position and orientation of the end-effector, and particularly the influence of the variations in the parallelograms. It turns out that the kinematic isotropic configuration of the manipulator is the least sensitive one to its dimensional and angular variations. On the contrary, the closest configurations to its kinematic singular configurations are the most sensitive ones to geometrical variations.",
        "published": "2007-08-31T11:53:35Z",
        "link": "http://arxiv.org/abs/0708.4324v2",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A DH-parameter based condition for 3R orthogonal manipulators to have 4   distinct inverse kinematic solutions",
        "authors": [
            "Philippe Wenger",
            "Damien Chablat",
            "Maher Baili"
        ],
        "summary": "Positioning 3R manipulators may have two or four inverse kinematic solutions (IKS). This paper derives a necessary and sufficient condition for 3R positioning manipulators with orthogonal joint axes to have four distinct IKS. We show that the transition between manipulators with 2 and 4 IKS is defined by the set of manipulators with a quadruple root of their inverse kinematics. The resulting condition is explicit and states that the last link length of the manipulator must be greater than a quantity that depends on three of its remaining DH-parameters. This result is of interest for the design of new manipulators.",
        "published": "2007-09-04T12:27:14Z",
        "link": "http://arxiv.org/abs/0709.0409v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Designing a Virtual Manikin Animation Framework Aimed at Virtual   Prototyping",
        "authors": [
            "Antoine Rennuit",
            "Alain Micaelli",
            "Claude Andriot",
            "François Guillaume",
            "Nicolas Chevassus",
            "Damien Chablat",
            "Patrick Chedmail"
        ],
        "summary": "In the industry, numerous commercial packages provide tools to introduce, and analyse human behaviour in the product's environment (for maintenance, ergonomics...), thanks to Virtual Humans. We will focus on control. Thanks to algorithms newly introduced in recent research papers, we think we can provide an implementation, which even widens, and simplifies the animation capacities of virtual manikins. In order to do so, we are going to express the industrial expectations as for Virtual Humans, without considering feasibility (not to bias the issue). The second part will show that no commercial application provides the tools that perfectly meet the needs. Thus we propose a new animation framework that better answers the problem. Our contribution is the integration - driven by need ~ of available new scientific techniques to animate Virtual Humans, in a new control scheme that better answers industrial expectations.",
        "published": "2007-09-05T15:52:53Z",
        "link": "http://arxiv.org/abs/0709.0680v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Multi-Sensor Fusion Method using Dynamic Bayesian Network for Precise   Vehicle Localization and Road Matching",
        "authors": [
            "Cherif Smaili",
            "Maan El Badaoui El Najjar",
            "François Charpillet"
        ],
        "summary": "This paper presents a multi-sensor fusion strategy for a novel road-matching method designed to support real-time navigational features within advanced driving-assistance systems. Managing multihypotheses is a useful strategy for the road-matching problem. The multi-sensor fusion and multi-modal estimation are realized using Dynamical Bayesian Network. Experimental results, using data from Antilock Braking System (ABS) sensors, a differential Global Positioning System (GPS) receiver and an accurate digital roadmap, illustrate the performances of this approach, especially in ambiguous situations.",
        "published": "2007-09-07T15:03:37Z",
        "link": "http://arxiv.org/abs/0709.1099v1",
        "categories": [
            "cs.AI",
            "cs.RO"
        ]
    },
    {
        "title": "Experiments with small helicopter automated landings at unusual   attitudes",
        "authors": [
            "S. Bayraktar",
            "E. Feron"
        ],
        "summary": "This paper describes a set of experiments involving small helicopters landing automated landing at unusual attitudes. By leveraging the increased agility of small air vehicles, we show that it is possible to automatically land a small helicopter on surfaces pitched at angles up to 60 degrees. Such maneuvers require considerable agility from the vehicle and its avionics system, and they pose significant technical and safety challenges. Our work builds upon previous activities in human-inspired, high-agility flight for small rotorcraft. However, it was not possible to leverage manual flight test data to extract landing maneuvers due to stringent attitude and position control requirements. Availability of low-cost, local navigation systems requiring no on-board instrumentation has proven particularly important for these experiments to be successful.",
        "published": "2007-09-12T02:16:44Z",
        "link": "http://arxiv.org/abs/0709.1744v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Control and Monitoring System for Modular Wireless Robot",
        "authors": [
            "I. Firmansyah",
            "B. Hermanto",
            "L. T. Handoko"
        ],
        "summary": "We introduce our concept on the modular wireless robot consisting of three main modules : main unit, data acquisition and data processing modules. We have developed a generic prototype with an integrated control and monitoring system to enhance its flexibility, and to enable simple operation through a web-based interface accessible wirelessly. In present paper, we focus on the microcontroller based hardware to enable data acquisition and remote mechanical control.",
        "published": "2007-10-03T23:05:29Z",
        "link": "http://arxiv.org/abs/0710.0903v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Singular Curves in the Joint Space and Cusp Points of 3-RPR parallel   manipulators",
        "authors": [
            "Mazen Zein",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This paper investigates the singular curves in the joint space of a family of planar parallel manipulators. It focuses on special points, referred to as cusp points, which may appear on these curves. Cusp points play an important role in the kinematic behavior of parallel manipulators since they make possible a nonsingular change of assembly mode. The purpose of this study is twofold. First, it exposes a method to compute joint space singular curves of 3-RPR planar parallel manipulators. Second, it presents an algorithm for detecting and computing all cusp points in the joint space of these same manipulators.",
        "published": "2007-11-05T08:14:07Z",
        "link": "http://arxiv.org/abs/0711.0574v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Performance Bounds for Lambda Policy Iteration and Application to the   Game of Tetris",
        "authors": [
            "Bruno Scherrer"
        ],
        "summary": "We consider the discrete-time infinite-horizon optimal control problem formalized by Markov Decision Processes. We revisit the work of Bertsekas and Ioffe, that introduced $\\lambda$ Policy Iteration, a family of algorithms parameterized by $\\lambda$ that generalizes the standard algorithms Value Iteration and Policy Iteration, and has some deep connections with the Temporal Differences algorithm TD($\\lambda$) described by Sutton and Barto. We deepen the original theory developped by the authors by providing convergence rate bounds which generalize standard bounds for Value Iteration described for instance by Puterman. Then, the main contribution of this paper is to develop the theory of this algorithm when it is used in an approximate form and show that this is sound. Doing so, we extend and unify the separate analyses developped by Munos for Approximate Value Iteration and Approximate Policy Iteration. Eventually, we revisit the use of this algorithm in the training of a Tetris playing controller as originally done by Bertsekas and Ioffe. We provide an original performance bound that can be applied to such an undiscounted control problem. Our empirical results are different from those of Bertsekas and Ioffe (which were originally qualified as \"paradoxical\" and \"intriguing\"), and much more conform to what one would expect from a learning experiment. We discuss the possible reason for such a difference.",
        "published": "2007-11-05T17:07:22Z",
        "link": "http://arxiv.org/abs/0711.0694v5",
        "categories": [
            "cs.AI",
            "cs.RO"
        ]
    },
    {
        "title": "Kinematic calibration of orthoglide-type mechanisms",
        "authors": [
            "Anatoly Pashkevich",
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The paper proposes a novel calibration approach for the Orthoglide-type mechanisms based on observations of the manipulator leg parallelism during mo-tions between the prespecified test postures. It employs a low-cost measuring system composed of standard comparator indicators attached to the universal magnetic stands. They are sequentially used for measuring the deviation of the relevant leg location while the manipulator moves the TCP along the Cartesian axes. Using the measured differences, the developed algorithm estimates the joint offsets that are treated as the most essential parameters to be adjusted. The sensitivity of the meas-urement methods and the calibration accuracy are also studied. Experimental re-sults are presented that demonstrate validity of the proposed calibration technique",
        "published": "2007-11-12T12:57:46Z",
        "link": "http://arxiv.org/abs/0711.1765v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Medical image computing and computer-aided medical interventions applied   to soft tissues. Work in progress in urology",
        "authors": [
            "Jocelyne Troccaz",
            "Michael Baumann",
            "Peter Berkelman",
            "Philippe Cinquin",
            "Vincent Daanen",
            "Antoine Leroy",
            "Maud Marchal",
            "Yohan Payan",
            "Emmanuel Promayon",
            "Sandrine Voros",
            "Stéphane Bart",
            "Michel Bolla",
            "Emmanuel Chartier-Kastler",
            "Jean-Luc Descotes",
            "Andrée Dusserre",
            "Jean-Yves Giraud",
            "Jean-Alexandre Long",
            "Ronan Moalic",
            "Pierre Mozer"
        ],
        "summary": "Until recently, Computer-Aided Medical Interventions (CAMI) and Medical Robotics have focused on rigid and non deformable anatomical structures. Nowadays, special attention is paid to soft tissues, raising complex issues due to their mobility and deformation. Mini-invasive digestive surgery was probably one of the first fields where soft tissues were handled through the development of simulators, tracking of anatomical structures and specific assistance robots. However, other clinical domains, for instance urology, are concerned. Indeed, laparoscopic surgery, new tumour destruction techniques (e.g. HIFU, radiofrequency, or cryoablation), increasingly early detection of cancer, and use of interventional and diagnostic imaging modalities, recently opened new challenges to the urologist and scientists involved in CAMI. This resulted in the last five years in a very significant increase of research and developments of computer-aided urology systems. In this paper, we propose a description of the main problems related to computer-aided diagnostic and therapy of soft tissues and give a survey of the different types of assistance offered to the urologist: robotization, image fusion, surgical navigation. Both research projects and operational industrial systems are discussed.",
        "published": "2007-12-13T06:45:28Z",
        "link": "http://arxiv.org/abs/0712.2100v1",
        "categories": [
            "cs.OH",
            "cs.RO"
        ]
    },
    {
        "title": "Computer- and robot-assisted urological surgery",
        "authors": [
            "Jocelyne Troccaz"
        ],
        "summary": "The author reviews the computer and robotic tools available to urologists to help in diagnosis and technical procedures. The first part concerns the contribution of robotics and presents several systems at various stages of development (laboratory prototypes, systems under validation or marketed systems). The second part describes image fusion tools and navigation systems currently under development or evaluation. Several studies on computerized simulation of urological procedures are also presented.",
        "published": "2007-12-19T22:09:18Z",
        "link": "http://arxiv.org/abs/0712.3299v1",
        "categories": [
            "cs.OH",
            "cs.RO"
        ]
    },
    {
        "title": "Why is a new Journal of Informetrics needed?",
        "authors": [
            "Philipp Mayr",
            "Walther Umstaetter"
        ],
        "summary": "In our study we analysed 3.889 records which were indexed in the Library and Information Science Abstracts (LISA) database in the research field of informetrics. We can show the core journals of the field via a Bradford (power law) distribution and corroborate on the basis of the restricted LISA data set that it was the appropriate time to found a new specialized journal dedicated to informetrics. According to Bradford's Law of scattering (pure quantitative calculation), Egghe's Journal of Informetrics (JOI) first issue to appear in 2007, comes most probable at the right time.",
        "published": "2007-01-17T12:17:53Z",
        "link": "http://arxiv.org/abs/cs/0701104v1",
        "categories": [
            "cs.DL",
            "cs.DB"
        ]
    },
    {
        "title": "The problem determination of Functional Dependencies between attributes   Relation Scheme in the Relational Data Model. El problema de determinar   Dependencias Funcionales entre atributos en los esquemas en el Modelo   Relacional",
        "authors": [
            "Ignacio Vega-Paez",
            "Georgina G. Pulido",
            "Jose Angel Ortega"
        ],
        "summary": "An alternative definition of the concept is given of functional dependence among the attributes of the relational schema in the Relational Model, this definition is obtained in terms of the set theory. For that which a theorem is demonstrated that establishes equivalence and on the basis theorem an algorithm is built for the search of the functional dependences among the attributes. The algorithm is illustrated by a concrete example",
        "published": "2007-01-17T20:08:53Z",
        "link": "http://arxiv.org/abs/cs/0701114v2",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Data Cube: A Relational Aggregation Operator Generalizing Group-By,   Cross-Tab, and Sub-Totals",
        "authors": [
            "Jim Gray",
            "Surajit Chaudhuri",
            "Adam Bosworth",
            "Andrew Layman",
            "Don Reichart",
            "Murali Venkatrao",
            "Frank Pellow",
            "Hamid Pirahesh"
        ],
        "summary": "Data analysis applications typically aggregate data across many dimensions looking for anomalies or unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional or one-dimensional aggregates. Applications need the N-dimensional generalization of these operators. This paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram, cross-tabulation, roll-up, drill-down, and sub-total constructs found in most report writers. The novelty is that cubes are relations. Consequently, the cube operator can be imbedded in more complex non-procedural data analysis programs. The cube operator treats each of the N aggregation attributes as a dimension of N-space. The aggregate of a particular set of attribute values is a point in this space. The set of points forms an N-dimensional cube. Super-aggregates are computed by aggregating the N-cube to lower dimensional spaces. This paper (1) explains the cube and roll-up operators, (2) shows how they fit in SQL, (3) explains how users can define new aggregate functions for cubes, and (4) discusses efficient techniques to compute the cube. Many of these features are being added to the SQL Standard.",
        "published": "2007-01-25T22:39:37Z",
        "link": "http://arxiv.org/abs/cs/0701155v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Data Management: Past, Present, and Future",
        "authors": [
            "Jim Gray"
        ],
        "summary": "Soon most information will be available at your fingertips, anytime, anywhere. Rapid advances in storage, communications, and processing allow us move all information into Cyberspace. Software to define, search, and visualize online information is also a key to creating and accessing online information. This article traces the evolution of data management systems and outlines current trends. Data management systems began by automating traditional tasks: recording transactions in business, science, and commerce. This data consisted primarily of numbers and character strings. Today these systems provide the infrastructure for much of our society, allowing fast, reliable, secure, and automatic access to data distributed throughout the world. Increasingly these systems automatically design and manage access to the data. The next steps are to automate access to richer forms of data: images, sound, video, maps, and other media. A second major challenge is automatically summarizing and abstracting data in anticipation of user requests. These multi-media databases and tools to access them will be a cornerstone of our move to Cyberspace.",
        "published": "2007-01-25T22:42:28Z",
        "link": "http://arxiv.org/abs/cs/0701156v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "A Critique of ANSI SQL Isolation Levels",
        "authors": [
            "Hal Berenson",
            "Phil Bernstein",
            "Jim Gray",
            "Jim Melton",
            "Elizabeth O'Neil",
            "Patrick O'Neil"
        ],
        "summary": "ANSI SQL-92 defines Isolation Levels in terms of phenomena: Dirty Reads, Non-Repeatable Reads, and Phantoms. This paper shows that these phenomena and the ANSI SQL definitions fail to characterize several popular isolation levels, including the standard locking implementations of the levels. Investigating the ambiguities of the phenomena leads to clearer definitions; in addition new phenomena that better characterize isolation types are introduced. An important multiversion isolation type, Snapshot Isolation, is defined.",
        "published": "2007-01-25T22:53:48Z",
        "link": "http://arxiv.org/abs/cs/0701157v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Queues Are Databases",
        "authors": [
            "Jim Gray"
        ],
        "summary": "Message-oriented-middleware (MOM) has become an small industry. MOM offers queued transaction processing as an advance over pure client-server transaction processing. This note makes four points: Queued transaction processing is less general than direct transaction processing. Queued systems are built on top of direct systems. You cannot build a direct system atop a queued system. It is difficult to build direct, conversational, or distributed transactions atop a queued system. Queues are interesting databases with interesting concurrency control. It is best to build these mechanisms into a standard database system so other applications can use these interesting features. Queue systems need DBMS functionality. Queues need security, configuration, performance monitoring, recovery, and reorganization utilities. Database systems already have these features. A full-function MOM system duplicates these database features. Queue managers are simple TP-monitors managing server pools driven by queues. Database systems are encompassing many server pool features as they evolve to TP-lite systems.",
        "published": "2007-01-25T22:56:45Z",
        "link": "http://arxiv.org/abs/cs/0701158v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Supporting Finite Element Analysis with a Relational Database Backend,   Part I: There is Life beyond Files",
        "authors": [
            "Gerd Heber",
            "Jim Gray"
        ],
        "summary": "In this paper, we show how to use a Relational Database Management System in support of Finite Element Analysis. We believe it is a new way of thinking about data management in well-understood applications to prepare them for two major challenges, - size and integration (globalization). Neither extreme size nor integration (with other applications over the Web) was a design concern 30 years ago when the paradigm for FEA implementation first was formed. On the other hand, database technology has come a long way since its inception and it is past time to highlight its usefulness to the field of scientific computing and computer based engineering. This series aims to widen the list of applications for database designers and for FEA users and application developers to reap some of the benefits of database development.",
        "published": "2007-01-25T23:02:32Z",
        "link": "http://arxiv.org/abs/cs/0701159v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Supporting Finite Element Analysis with a Relational Database Backend,   Part II: Database Design and Access",
        "authors": [
            "Gerd Heber",
            "Jim Gray"
        ],
        "summary": "This is Part II of a three article series on using databases for Finite Element Analysis (FEA). It discusses (1) db design, (2) data loading, (3) typical use cases during grid building, (4) typical use cases during simulation (get and put), (5) typical use cases during analysis (also done in Part III) and some performance measures of these cases. It argues that using a database is simpler to implement than custom data schemas, has better performance because it can use data parallelism, and better supports FEA modularity and tool evolution because database schema evolution, data independence, and self-defining data.",
        "published": "2007-01-25T23:05:40Z",
        "link": "http://arxiv.org/abs/cs/0701160v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Thousands of DebitCredit Transactions-Per-Second: Easy and Inexpensive",
        "authors": [
            "Jim Gray",
            "Charles Levine"
        ],
        "summary": "A $2k computer can execute about 8k transactions per second. This is 80x more than one of the largest US bank's 1970's traffic - it approximates the total US 1970's financial transaction volume. Very modest modern computers can easily solve yesterday's problems.",
        "published": "2007-01-25T23:51:22Z",
        "link": "http://arxiv.org/abs/cs/0701161v1",
        "categories": [
            "cs.DB",
            "cs.PF"
        ]
    },
    {
        "title": "A Measure of Transaction Processing 20 Years Later",
        "authors": [
            "Jim Gray"
        ],
        "summary": "This provides a retrospective of the paper \"A Measure of Transaction Processing\" published in 1985. It shows that transaction processing peak performance and price-peformance have improved about 100,000x respectively and that sort/sequential performance has approximately doubled each year (so a million fold improvement) even though processor performance plateaued in 1995.",
        "published": "2007-01-25T23:57:15Z",
        "link": "http://arxiv.org/abs/cs/0701162v1",
        "categories": [
            "cs.DB",
            "cs.PF"
        ]
    },
    {
        "title": "Using Table Valued Functions in SQL Server 2005 To Implement a Spatial   Data Library",
        "authors": [
            "Jim Gray",
            "Alex Szalay",
            "Gyorgy Fekete"
        ],
        "summary": "This article explains how to add spatial search functions (point-near-point and point in polygon) to Microsoft SQL Server 2005 using C# and table-valued functions. It is possible to use this library to add spatial search to your application without writing any special code. The library implements the public-domain C# Hierarchical Triangular Mesh (HTM) algorithms from Johns Hopkins University. That C# library is connected to SQL Server 2005 via a set of scalar-valued and table-valued functions. These functions act as a spatial index.",
        "published": "2007-01-26T00:00:37Z",
        "link": "http://arxiv.org/abs/cs/0701163v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Indexing the Sphere with the Hierarchical Triangular Mesh",
        "authors": [
            "Alexander S. Szalay",
            "Jim Gray",
            "George Fekete",
            "Peter Z. Kunszt",
            "Peter Kukol",
            "Ani Thakar"
        ],
        "summary": "We describe a method to subdivide the surface of a sphere into spherical triangles of similar, but not identical, shapes and sizes. The Hierarchical Triangular Mesh (HTM) is a quad-tree that is particularly good at supporting searches at different resolutions, from arc seconds to hemispheres. The subdivision scheme is universal, providing the basis for addressing and for fast lookups. The HTM provides the basis for an efficient geospatial indexing scheme in relational databases where the data have an inherent location on either the celestial sphere or the Earth. The HTM index is superior to cartographical methods using coordinates with singularities at the poles. We also describe a way to specify surface regions that efficiently represent spherical query areas. This article presents the algorithms used to identify the HTM triangles covering such regions.",
        "published": "2007-01-26T00:04:12Z",
        "link": "http://arxiv.org/abs/cs/0701164v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Petascale Computational Systems",
        "authors": [
            "Gordon Bell",
            "Jim Gray",
            "Alex Szalay"
        ],
        "summary": "Computational science is changing to be data intensive. Super-Computers must be balanced systems; not just CPU farms but also petascale IO and networking arrays. Anyone building CyberInfrastructure should allocate resources to support a balanced Tier-1 through Tier-3 design.",
        "published": "2007-01-26T00:23:07Z",
        "link": "http://arxiv.org/abs/cs/0701165v1",
        "categories": [
            "cs.DB",
            "cs.AR"
        ]
    },
    {
        "title": "Empirical Measurements of Disk Failure Rates and Error Rates",
        "authors": [
            "Jim Gray",
            "Catharine van Ingen"
        ],
        "summary": "The SATA advertised bit error rate of one error in 10 terabytes is frightening. We moved 2 PB through low-cost hardware and saw five disk read error events, several controller failures, and many system reboots caused by security patches. We conclude that SATA uncorrectable read errors are not yet a dominant system-fault source - they happen, but are rare compared to other problems. We also conclude that UER (uncorrectable error rate) is not the relevant metric for our needs. When an uncorrectable read error happens, there are typically several damaged storage blocks (and many uncorrectable read errors.) Also, some uncorrectable read errors may be masked by the operating system. The more meaningful metric for data architects is Mean Time To Data Loss (MTTDL.)",
        "published": "2007-01-26T00:29:02Z",
        "link": "http://arxiv.org/abs/cs/0701166v1",
        "categories": [
            "cs.DB",
            "cs.AR"
        ]
    },
    {
        "title": "Large-Scale Query and XMatch, Entering the Parallel Zone",
        "authors": [
            "Maria A. Nieto-Santisteban",
            "Aniruddha R. Thakar",
            "Alexander S. Szalay",
            "Jim Gray"
        ],
        "summary": "Current and future astronomical surveys are producing catalogs with millions and billions of objects. On-line access to such big datasets for data mining and cross-correlation is usually as highly desired as unfeasible. Providing these capabilities is becoming critical for the Virtual Observatory framework. In this paper we present various performance tests that show how using Relational Database Management Systems (RDBMS) and a Zoning algorithm to partition and parallelize the computation, we can facilitate large-scale query and cross-match.",
        "published": "2007-01-26T00:33:26Z",
        "link": "http://arxiv.org/abs/cs/0701167v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "To BLOB or Not To BLOB: Large Object Storage in a Database or a   Filesystem?",
        "authors": [
            "Russell Sears",
            "Catharine Van Ingen",
            "Jim Gray"
        ],
        "summary": "Application designers often face the question of whether to store large objects in a filesystem or in a database. Often this decision is made for application design simplicity. Sometimes, performance measurements are also used. This paper looks at the question of fragmentation - one of the operational issues that can affect the performance and/or manageability of the system as deployed long term. As expected from the common wisdom, objects smaller than 256KB are best stored in a database while objects larger than 1M are best stored in the filesystem. Between 256KB and 1MB, the read:write ratio and rate of object overwrite or replacement are important factors. We used the notion of \"storage age\" or number of object overwrites as way of normalizing wall clock time. Storage age allows our results or similar such results to be applied across a number of read:write ratios and object replacement rates.",
        "published": "2007-01-26T00:54:04Z",
        "link": "http://arxiv.org/abs/cs/0701168v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Life Under Your Feet: An End-to-End Soil Ecology Sensor Network,   Database, Web Server, and Analysis Service",
        "authors": [
            "Katalin Szlavecz",
            "Andreas Terzis",
            "Stuart Ozer",
            "Razvan Musaloiu-E",
            "Joshua Cogan",
            "Sam Small",
            "Randal Burns",
            "Jim Gray",
            "Alex Szalay"
        ],
        "summary": "Wireless sensor networks can revolutionize soil ecology by providing measurements at temporal and spatial granularities previously impossible. This paper presents a soil monitoring system we developed and deployed at an urban forest in Baltimore as a first step towards realizing this vision. Motes in this network measure and save soil moisture and temperature in situ every minute. Raw measurements are periodically retrieved by a sensor gateway and stored in a central database where calibrated versions are derived and stored. The measurement database is published through Web Services interfaces. In addition, analysis tools let scientists analyze current and historical data and help manage the sensor network. The article describes the system design, what we learned from the deployment, and initial results obtained from the sensors. The system measures soil factors with unprecedented temporal precision. However, the deployment required device-level programming, sensor calibration across space and time, and cross-referencing measurements with external sources. The database, web server, and data analysis design required considerable innovation and expertise. So, the ratio of computer-scientists to ecologists was 3:1. Before sensor networks can fulfill their potential as instruments that can be easily deployed by scientists, these technical problems must be addressed so that the ratio is one nerd per ten ecologists.",
        "published": "2007-01-26T05:08:06Z",
        "link": "http://arxiv.org/abs/cs/0701170v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "The Zones Algorithm for Finding Points-Near-a-Point or Cross-Matching   Spatial Datasets",
        "authors": [
            "Jim Gray",
            "Maria A. Nieto-Santisteban",
            "Alexander S. Szalay"
        ],
        "summary": "Zones index an N-dimensional Euclidian or metric space to efficiently support points-near-a-point queries either within a dataset or between two datasets. The approach uses relational algebra and the B-Tree mechanism found in almost all relational database systems. Hence, the Zones Algorithm gives a portable-relational implementation of points-near-point, spatial cross-match, and self-match queries. This article corrects some mistakes in an earlier article we wrote on the Zones Algorithm and describes some algorithmic improvements. The Appendix includes an implementation of point-near-point, self-match, and cross-match using the USGS city and stream gauge database.",
        "published": "2007-01-26T05:11:20Z",
        "link": "http://arxiv.org/abs/cs/0701171v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Cross-Matching Multiple Spatial Observations and Dealing with Missing   Data",
        "authors": [
            "Jim Gray",
            "Alex Szalay",
            "Tamas Budavari",
            "Robert Lupton",
            "Maria Nieto-Santisteban",
            "Ani Thakar"
        ],
        "summary": "Cross-match spatially clusters and organizes several astronomical point-source measurements from one or more surveys. Ideally, each object would be found in each survey. Unfortunately, the observation conditions and the objects themselves change continually. Even some stationary objects are missing in some observations; sometimes objects have a variable light flux and sometimes the seeing is worse. In most cases we are faced with a substantial number of differences in object detections between surveys and between observations taken at different times within the same survey or instrument. Dealing with such missing observations is a difficult problem. The first step is to classify misses as ephemeral - when the object moved or simply disappeared, masked - when noise hid or corrupted the object observation, or edge - when the object was near the edge of the observational field. This classification and a spatial library to represent and manipulate observational footprints help construct a Match table recording both hits and misses. Transitive closure clusters friends-of-friends into object bundles. The bundle summary statistics are recorded in a Bundle table. This design is an evolution of the Sloan Digital Sky Survey cross-match design that compared overlapping observations taken at different times. Cross-Matching Multiple Spatial Observations and Dealing with Missing Data.",
        "published": "2007-01-26T05:18:45Z",
        "link": "http://arxiv.org/abs/cs/0701172v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "SkyServer Traffic Report - The First Five Years",
        "authors": [
            "Vik Singh",
            "Jim Gray",
            "Ani Thakar",
            "Alexander S. Szalay",
            "Jordan Raddick",
            "Bill Boroski",
            "Svetlana Lebedeva",
            "Brian Yanny"
        ],
        "summary": "The SkyServer is an Internet portal to the Sloan Digital Sky Survey Catalog Archive Server. From 2001 to 2006, there were a million visitors in 3 million sessions generating 170 million Web hits, 16 million ad-hoc SQL queries, and 62 million page views. The site currently averages 35 thousand visitors and 400 thousand sessions per month. The Web and SQL logs are public. We analyzed traffic and sessions by duration, usage pattern, data product, and client type (mortal or bot) over time. The analysis shows (1) the site's popularity, (2) the educational website that delivered nearly fifty thousand hours of interactive instruction, (3) the relative use of interactive, programmatic, and batch-local access, (4) the success of offering ad-hoc SQL, personal database, and batch job access to scientists as part of the data publication, (5) the continuing interest in \"old\" datasets, (6) the usage of SQL constructs, and (7) a novel approach of using the corpus of correct SQL queries to suggest similar but correct statements when a user presents an incorrect SQL statement.",
        "published": "2007-01-26T05:22:15Z",
        "link": "http://arxiv.org/abs/cs/0701173v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Plagiarism Detection in arXiv",
        "authors": [
            "Daria Sorokina",
            "Johannes Gehrke",
            "Simeon Warner",
            "Paul Ginsparg"
        ],
        "summary": "We describe a large-scale application of methods for finding plagiarism in research document collections. The methods are applied to a collection of 284,834 documents collected by arXiv.org over a 14 year period, covering a few different research disciplines. The methodology efficiently detects a variety of problematic author behaviors, and heuristics are developed to reduce the number of false positives. The methods are also efficient enough to implement as a real-time submission screen for a collection many times larger.",
        "published": "2007-02-01T20:52:13Z",
        "link": "http://arxiv.org/abs/cs/0702012v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Firebird Database Backup by Serialized Database Table Dump",
        "authors": [
            "Maurice HT Ling"
        ],
        "summary": "This paper presents a simple data dump and load utility for Firebird databases which mimics mysqldump in MySQL. This utility, fb_dump and fb_load, for dumping and loading respectively, retrieves each database table using kinterbasdb and serializes the data using marshal module. This utility has two advantages over the standard Firebird database backup utility, gbak. Firstly, it is able to backup and restore single database tables which might help to recover corrupted databases. Secondly, the output is in text-coded format (from marshal module) making it more resilient than a compressed text backup, as in the case of using gbak.",
        "published": "2007-02-13T11:13:29Z",
        "link": "http://arxiv.org/abs/cs/0702075v1",
        "categories": [
            "cs.DB",
            "H.2.7; E.5"
        ]
    },
    {
        "title": "An Optimal Linear Time Algorithm for Quasi-Monotonic Segmentation",
        "authors": [
            "Daniel Lemire",
            "Martin Brooks",
            "Yuhong Yan"
        ],
        "summary": "Monotonicity is a simple yet significant qualitative characteristic. We consider the problem of segmenting an array in up to K segments. We want segments to be as monotonic as possible and to alternate signs. We propose a quality metric for this problem, present an optimal linear time algorithm based on novel formalism, and compare experimentally its performance to a linear time top-down regression algorithm. We show that our algorithm is faster and more accurate. Applications include pattern recognition and qualitative modeling.",
        "published": "2007-02-24T02:29:36Z",
        "link": "http://arxiv.org/abs/cs/0702142v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "Attribute Value Reordering For Efficient Hybrid OLAP",
        "authors": [
            "Owen Kaser",
            "Daniel Lemire"
        ],
        "summary": "The normalization of a data cube is the ordering of the attribute values. For large multidimensional arrays where dense and sparse chunks are stored differently, proper normalization can lead to improved storage efficiency. We show that it is NP-hard to compute an optimal normalization even for 1x3 chunks, although we find an exact algorithm for 1x2 chunks. When dimensions are nearly statistically independent, we show that dimension-wise attribute frequency sorting is an optimal normalization and takes time O(d n log(n)) for data cubes of size n^d. When dimensions are not independent, we propose and evaluate several heuristics. The hybrid OLAP (HOLAP) storage mechanism is already 19%-30% more efficient than ROLAP, but normalization can improve it further by 9%-13% for a total gain of 29%-44% over ROLAP.",
        "published": "2007-02-24T03:11:09Z",
        "link": "http://arxiv.org/abs/cs/0702143v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Slope One Predictors for Online Rating-Based Collaborative Filtering",
        "authors": [
            "Daniel Lemire",
            "Anna Maclachlan"
        ],
        "summary": "Rating-based collaborative filtering is the process of predicting how a user would rate a given item from other user ratings. We propose three related slope one schemes with predictors of the form f(x) = x + b, which precompute the average difference between the ratings of one item and another for users who rated both. Slope one algorithms are easy to implement, efficient to query, reasonably accurate, and they support both online queries and dynamic updates, which makes them good candidates for real-world systems. The basic slope one scheme is suggested as a new reference scheme for collaborative filtering. By factoring in items that a user liked separately from items that a user disliked, we achieve results competitive with slower memory-based schemes over the standard benchmark EachMovie and Movielens data sets while better fulfilling the desiderata of CF applications.",
        "published": "2007-02-24T03:16:27Z",
        "link": "http://arxiv.org/abs/cs/0702144v2",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Perfect Hashing for Data Management Applications",
        "authors": [
            "Fabiano C. Botelho",
            "Rasmus Pagh",
            "Nivio Ziviani"
        ],
        "summary": "Perfect hash functions can potentially be used to compress data in connection with a variety of data management tasks. Though there has been considerable work on how to construct good perfect hash functions, there is a gap between theory and practice among all previous methods on minimal perfect hashing. On one side, there are good theoretical results without experimentally proven practicality for large key sets. On the other side, there are the theoretically analyzed time and space usage algorithms that assume that truly random hash functions are available for free, which is an unrealistic assumption. In this paper we attempt to bridge this gap between theory and practice, using a number of techniques from the literature to obtain a novel scheme that is theoretically well-understood and at the same time achieves an order-of-magnitude increase in performance compared to previous ``practical'' methods. This improvement comes from a combination of a novel, theoretically optimal perfect hashing scheme that greatly simplifies previous methods, and the fact that our algorithm is designed to make good use of the memory hierarchy. We demonstrate the scalability of our algorithm by considering a set of over one billion URLs from the World Wide Web of average length 64, for which we construct a minimal perfect hash function on a commodity PC in a little more than 1 hour. Our scheme produces minimal perfect hash functions using slightly more than 3 bits per key. For perfect hash functions in the range $\\{0,...,2n-1\\}$ the space usage drops to just over 2 bits per key (i.e., one bit more than optimal for representing the key). This is significantly below of what has been achieved previously for very large values of $n$.",
        "published": "2007-02-27T20:56:41Z",
        "link": "http://arxiv.org/abs/cs/0702159v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "E.1"
        ]
    },
    {
        "title": "Reading policies for joins: An asymptotic analysis",
        "authors": [
            "Ralph P. Russo",
            "Nariankadu D. Shyamalkumar"
        ],
        "summary": "Suppose that $m_n$ observations are made from the distribution $\\mathbf {R}$ and $n-m_n$ from the distribution $\\mathbf {S}$. Associate with each pair, $x$ from $\\mathbf {R}$ and $y$ from $\\mathbf {S}$, a nonnegative score $\\phi(x,y)$. An optimal reading policy is one that yields a sequence $m_n$ that maximizes $\\mathbb{E}(M(n))$, the expected sum of the $(n-m_n)m_n$ observed scores, uniformly in $n$. The alternating policy, which switches between the two sources, is the optimal nonadaptive policy. In contrast, the greedy policy, which chooses its source to maximize the expected gain on the next step, is shown to be the optimal policy. Asymptotics are provided for the case where the $\\mathbf {R}$ and $\\mathbf {S}$ distributions are discrete and $\\phi(x,y)=1 or 0$ according as $x=y$ or not (i.e., the observations match). Specifically, an invariance result is proved which guarantees that for a wide class of policies, including the alternating and the greedy, the variable M(n) obeys the same CLT and LIL. A more delicate analysis of the sequence $\\mathbb{E}(M(n))$ and the sample paths of M(n), for both alternating and greedy, reveals the slender sense in which the latter policy is asymptotically superior to the former, as well as a sense of equivalence of the two and robustness of the former.",
        "published": "2007-03-01T09:02:55Z",
        "link": "http://arxiv.org/abs/math/0703019v1",
        "categories": [
            "math.PR",
            "cs.DB",
            "90C40 (Primary) 60G40, 60F05, 60F15 (Secondary)"
        ]
    },
    {
        "title": "Unasssuming View-Size Estimation Techniques in OLAP",
        "authors": [
            "Kamel Aouiche",
            "Daniel Lemire"
        ],
        "summary": "Even if storage was infinite, a data warehouse could not materialize all possible views due to the running time and update requirements. Therefore, it is necessary to estimate quickly, accurately, and reliably the size of views. Many available techniques make particular statistical assumptions and their error can be quite large. Unassuming techniques exist, but typically assume we have independent hashing for which there is no known practical implementation. We adapt an unassuming estimator due to Gibbons and Tirthapura: its theoretical bounds do not make unpractical assumptions. We compare this technique experimentally with stochastic probabilistic counting, LogLog probabilistic counting, and multifractal statistical models. Our experiments show that we can reliably and accurately (within 10%, 19 times out 20) estimate view sizes over large data sets (1.5 GB) within minutes, using almost no memory. However, only Gibbons-Tirthapura provides universally tight estimates irrespective of the size of the view. For large views, probabilistic counting has a small edge in accuracy, whereas the competitive sampling-based method (multifractal) we tested is an order of magnitude faster but can sometimes provide poor estimates (relative error of 100%). In our tests, LogLog probabilistic counting is not competitive. Experimental validation on the US Census 1990 data set and on the Transaction Processing Performance (TPC H) data set is provided.",
        "published": "2007-03-12T21:42:59Z",
        "link": "http://arxiv.org/abs/cs/0703056v3",
        "categories": [
            "cs.DB",
            "cs.PF"
        ]
    },
    {
        "title": "A Comparison of Five Probabilistic View-Size Estimation Techniques in   OLAP",
        "authors": [
            "Kamel Aouiche",
            "Daniel Lemire"
        ],
        "summary": "A data warehouse cannot materialize all possible views, hence we must estimate quickly, accurately, and reliably the size of views to determine the best candidates for materialization. Many available techniques for view-size estimation make particular statistical assumptions and their error can be large. Comparatively, unassuming probabilistic techniques are slower, but they estimate accurately and reliability very large view sizes using little memory. We compare five unassuming hashing-based view-size estimation techniques including Stochastic Probabilistic Counting and LogLog Probabilistic Counting. Our experiments show that only Generalized Counting, Gibbons-Tirthapura, and Adaptive Counting provide universally tight estimates irrespective of the size of the view; of those, only Adaptive Counting remains constantly fast as we increase the memory budget.",
        "published": "2007-03-13T17:46:11Z",
        "link": "http://arxiv.org/abs/cs/0703058v3",
        "categories": [
            "cs.DB",
            "cs.PF"
        ]
    },
    {
        "title": "Randomized Computations on Large Data Sets: Tight Lower Bounds",
        "authors": [
            "Martin Grohe",
            "Andre Hernich",
            "Nicole Schweikardt"
        ],
        "summary": "We study the randomized version of a computation model (introduced by Grohe, Koch, and Schweikardt (ICALP'05); Grohe and Schweikardt (PODS'05)) that restricts random access to external memory and internal memory space. Essentially, this model can be viewed as a powerful version of a data stream model that puts no cost on sequential scans of external memory (as other models for data streams) and, in addition, (like other external memory models, but unlike streaming models), admits several large external memory devices that can be read and written to in parallel.   We obtain tight lower bounds for the decision problems set equality, multiset equality, and checksort. More precisely, we show that any randomized one-sided-error bounded Monte Carlo algorithm for these problems must perform Omega(log N) random accesses to external memory devices, provided that the internal memory size is at most O(N^(1/4)/log N), where N denotes the size of the input data.   From the lower bound on the set equality problem we can infer lower bounds on the worst case data complexity of query evaluation for the languages XQuery, XPath, and relational algebra on streaming data. More precisely, we show that there exist queries in XQuery, XPath, and relational algebra, such that any (randomized) Las Vegas algorithm that evaluates these queries must perform Omega(log N) random accesses to external memory devices, provided that the internal memory size is at most O(N^(1/4)/log N).",
        "published": "2007-03-15T12:58:36Z",
        "link": "http://arxiv.org/abs/cs/0703081v1",
        "categories": [
            "cs.DB",
            "cs.CC",
            "F.1.3; F.1.1"
        ]
    },
    {
        "title": "Space Program Language (SPL/SQL) for the Relational Approach of the   Spatial Databases",
        "authors": [
            "Ignacio Vega-Paez",
            "Feliu D. Sagols T"
        ],
        "summary": "In this project we are presenting a grammar which unify the design and development of spatial databases. In order to make it, we combine nominal and spatial information, the former is represented by the relational model and latter by a modification of the same model. The modification lets to represent spatial data structures (as Quadtrees, Octrees, etc.) in a integrated way. This grammar is important because with it we can create tools to build systems that combine spatial-nominal characteristics such as Geographical Information Systems (GIS), Hypermedia Systems, Computed Aided Design Systems (CAD), and so on",
        "published": "2007-03-16T00:39:57Z",
        "link": "http://arxiv.org/abs/cs/0703089v1",
        "categories": [
            "cs.DB",
            "cs.CG"
        ]
    },
    {
        "title": "Concept of a Value in Multilevel Security Databases",
        "authors": [
            "Jia Tao",
            "Shashi Gadia",
            "Tsz Shing Cheng"
        ],
        "summary": "This paper has been withdrawn.",
        "published": "2007-03-22T03:31:27Z",
        "link": "http://arxiv.org/abs/cs/0703103v3",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Automatic Selection of Bitmap Join Indexes in Data Warehouses",
        "authors": [
            "Kamel Aouiche",
            "Jerome Darmont",
            "Omar Boussaid",
            "Fadila Bentayeb"
        ],
        "summary": "The queries defined on data warehouses are complex and use several join operations that induce an expensive computational cost. This cost becomes even more prohibitive when queries access very large volumes of data. To improve response time, data warehouse administrators generally use indexing techniques such as star join indexes or bitmap join indexes. This task is nevertheless complex and fastidious. Our solution lies in the field of data warehouse auto-administration. In this framework, we propose an automatic index selection strategy. We exploit a data mining technique ; more precisely frequent itemset mining, in order to determine a set of candidate indexes from a given workload. Then, we propose several cost models allowing to create an index configuration composed by the indexes providing the best profit. These models evaluate the cost of accessing data using bitmap join indexes, and the cost of updating and storing these indexes.",
        "published": "2007-03-23T04:25:36Z",
        "link": "http://arxiv.org/abs/cs/0703113v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Clustering-Based Materialized View Selection in Data Warehouses",
        "authors": [
            "Kamel Aouiche",
            "Pierre-Emmanuel Jouve",
            "Jerome Darmont"
        ],
        "summary": "Materialized view selection is a non-trivial task. Hence, its complexity must be reduced. A judicious choice of views must be cost-driven and influenced by the workload experienced by the system. In this paper, we propose a framework for materialized view selection that exploits a data mining technique (clustering), in order to determine clusters of similar queries. We also propose a view merging algorithm that builds a set of candidate views, as well as a greedy process for selecting a set of views to materialize. This selection is based on cost models that evaluate the cost of accessing data using views and the cost of storing these views. To validate our strategy, we executed a workload of decision-support queries on a test data warehouse, with and without using our strategy. Our experimental results demonstrate its efficiency, even when storage space is limited.",
        "published": "2007-03-23T04:31:35Z",
        "link": "http://arxiv.org/abs/cs/0703114v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Supporting Knowledge and Expertise Finding within Australia's Defence   Science and Technology Organisation",
        "authors": [
            "Paul Prekop"
        ],
        "summary": "This paper reports on work aimed at supporting knowledge and expertise finding within a large Research and Development (R&D) organisation. The paper first discusses the nature of knowledge important to R&D organisations and presents a prototype information system developed to support knowledge and expertise finding. The paper then discusses a trial of the system within an R&D organisation, the implications and limitations of the trial, and discusses future research questions.",
        "published": "2007-04-11T06:49:06Z",
        "link": "http://arxiv.org/abs/0704.1353v1",
        "categories": [
            "cs.OH",
            "cs.DB",
            "cs.DL",
            "cs.HC"
        ]
    },
    {
        "title": "Experimenting with recursive queries in database and logic programming   systems",
        "authors": [
            "Giorgio Terracina",
            "Nicola Leone",
            "Vincenzino Lio",
            "Claudio Panetta"
        ],
        "summary": "This paper considers the problem of reasoning on massive amounts of (possibly distributed) data. Presently, existing proposals show some limitations: {\\em (i)} the quantity of data that can be handled contemporarily is limited, due to the fact that reasoning is generally carried out in main-memory; {\\em (ii)} the interaction with external (and independent) DBMSs is not trivial and, in several cases, not allowed at all; {\\em (iii)} the efficiency of present implementations is still not sufficient for their utilization in complex reasoning tasks involving massive amounts of data. This paper provides a contribution in this setting; it presents a new system, called DLV$^{DB}$, which aims to solve these problems. Moreover, the paper reports the results of a thorough experimental analysis we have carried out for comparing our system with several state-of-the-art systems (both logic and databases) on some classical deductive problems; the other tested systems are: LDL++, XSB, Smodels and three top-level commercial DBMSs. DLV$^{DB}$ significantly outperforms even the commercial Database Systems on recursive queries. To appear in Theory and Practice of Logic Programming (TPLP)",
        "published": "2007-04-24T10:58:40Z",
        "link": "http://arxiv.org/abs/0704.3157v1",
        "categories": [
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "Une plate-forme dynamique pour l'évaluation des performances des bases   de données à objets",
        "authors": [
            "Zhen He",
            "Jérôme Darmont"
        ],
        "summary": "In object-oriented or object-relational databases such as multimedia databases or most XML databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. However, this has been the way these databases and associated optimisation techniques such as clustering have been evaluated up to now. This paper opens up research regarding this issue by proposing a dynamic object evaluation framework (DOEF). DOEF accomplishes access pattern change by defining configurable styles of change. It is a preliminary prototype that has been designed to be open and fully extensible. Though originally designed for the object-oriented model, it can also be used within the object-relational model with few adaptations. Furthermore, new access pattern change models can be added too. To illustrate the capabilities of DOEF, we conducted two different sets of experiments. In the first set of experiments, we used DOEF to compare the performances of four state of the art dynamic clustering algorithms. The results show that DOEF is effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern. They also led us to conclude that dynamic clustering algorithms can cope with moderate levels of access pattern change, but that performance rapidly degrades to be worse than no clustering when vigorous styles of access pattern change are applied. In the second set of experiments, we used DOEF to compare the performance of two different object stores: Platypus and SHORE. The use of DOEF exposed the poor swapping performance of Platypus.",
        "published": "2007-04-26T09:10:41Z",
        "link": "http://arxiv.org/abs/0704.3500v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Conception d'un banc d'essais décisionnel",
        "authors": [
            "Jérôme Darmont",
            "Fadila Bentayeb",
            "Omar Boussaïd"
        ],
        "summary": "We present in this paper a new benchmark for evaluating the performances of data warehouses. Benchmarking is useful either to system users for comparing the performances of different systems, or to system engineers for testing the effect of various design choices. While the TPC (Transaction Processing Performance Council) standard benchmarks address the first point, they are not tuneable enough to address the second one. Our Data Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc synthetic data warehouses and workloads. DWEB is fully parameterized. However, two levels of parameterization keep it easy to tune. Since DWEB mainly meets engineering benchmarking needs, it is complimentary to the TPC standard benchmarks, and not a competitor. Finally, DWEB is implemented as a Java free software that can be interfaced with most existing relational database management systems.",
        "published": "2007-04-26T09:13:04Z",
        "link": "http://arxiv.org/abs/0704.3501v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Vers l'auto-administration des entrepôts de données",
        "authors": [
            "Kamel Aouiche",
            "Jérôme Darmont"
        ],
        "summary": "With the wide development of databases in general and data warehouses in particular, it is important to reduce the tasks that a database administrator must perform manually. The idea of using data mining techniques to extract useful knowledge for administration from the data themselves has existed for some years. However, little research has been achieved. The aim of this study is to search for a way of extracting useful knowledge from stored data to automatically apply performance optimization techniques, and more particularly indexing techniques. We have designed a tool that extracts frequent itemsets from a given workload to compute an index configuration that helps optimizing data access time. The experiments we performed showed that the index configurations generated by our tool allowed performance gains of 15% to 25% on a test database and a test data warehouse.",
        "published": "2007-04-26T11:47:35Z",
        "link": "http://arxiv.org/abs/0704.3520v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Dynamic Clustering in Object-Oriented Databases: An Advocacy for   Simplicity",
        "authors": [
            "Jérôme Darmont",
            "Christophe Fromantin",
            "Stéphane Régnier",
            "Le Gruenwald",
            "Michel Schneider"
        ],
        "summary": "We present in this paper three dynamic clustering techniques for Object-Oriented Databases (OODBs). The first two, Dynamic, Statistical & Tunable Clustering (DSTC) and StatClust, exploit both comprehensive usage statistics and the inter-object reference graph. They are quite elaborate. However, they are also complex to implement and induce a high overhead. The third clustering technique, called Detection & Reclustering of Objects (DRO), is based on the same principles, but is much simpler to implement. These three clustering algorithm have been implemented in the Texas persistent object store and compared in terms of clustering efficiency (i.e., overall performance increase) and overhead using the Object Clustering Benchmark (OCB). The results obtained showed that DRO induced a lighter overhead while still achieving better overall performance.",
        "published": "2007-05-02T12:50:39Z",
        "link": "http://arxiv.org/abs/0705.0281v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "VOODB: A Generic Discrete-Event Random Simulation Model to Evaluate the   Performances of OODBs",
        "authors": [
            "Jérôme Darmont",
            "Michel Schneider"
        ],
        "summary": "Performance of object-oriented database systems (OODBs) is still an issue to both designers and users nowadays. The aim of this paper is to propose a generic discrete-event random simulation model, called VOODB, in order to evaluate the performances of OODBs in general, and the performances of optimization methods like clustering in particular. Such optimization methods undoubtedly improve the performances of OODBs. Yet, they also always induce some kind of overhead for the system. Therefore, it is important to evaluate their exact impact on the overall performances. VOODB has been designed as a generic discrete-event random simulation model by putting to use a modelling approach, and has been validated by simulating the behavior of the O2 OODB and the Texas persistent object store. Since our final objective is to compare object clustering algorithms, some experiments have also been conducted on the DSTC clustering technique, which is implemented in Texas. To validate VOODB, performance results obtained by simulation for a given experiment have been compared to the results obtained by benchmarking the real systems in the same conditions. Benchmarking and simulation performance evaluations have been observed to be consistent, so it appears that simulation can be a reliable approach to evaluate the performances of OODBs.",
        "published": "2007-05-03T12:50:04Z",
        "link": "http://arxiv.org/abs/0705.0450v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "OCB: A Generic Benchmark to Evaluate the Performances of Object-Oriented   Database Systems",
        "authors": [
            "Jérôme Darmont",
            "Bertrand Petit",
            "Michel Schneider"
        ],
        "summary": "We present in this paper a generic object-oriented benchmark (the Object Clustering Benchmark) that has been designed to evaluate the performances of clustering policies in object-oriented databases. OCB is generic because its sample database may be customized to fit the databases introduced by the main existing benchmarks (e.g., OO1). OCB's current form is clustering-oriented because of its clustering-oriented workload, but it can be easily adapted to other purposes. Lastly, OCB's code is compact and easily portable. OCB has been implemented in a real system (Texas, running on a Sun workstation), in order to test a specific clustering policy called DSTC. A few results concerning this test are presented.",
        "published": "2007-05-03T12:54:30Z",
        "link": "http://arxiv.org/abs/0705.0453v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Performance Evaluation for Clustering Algorithms in Object-Oriented   Database Systems",
        "authors": [
            "Jérôme Darmont",
            "Amar Attoui",
            "Michel Gourgand"
        ],
        "summary": "It is widely acknowledged that good object clustering is critical to the performance of object-oriented databases. However, object clustering always involves some kind of overhead for the system. The aim of this paper is to propose a modelling methodology in order to evaluate the performances of different clustering policies. This methodology has been used to compare the performances of three clustering algorithms found in the literature (Cactis, CK and ORION) that we considered representative of the current research in the field of object clustering. The actual performance evaluation was performed using simulation. Simulation experiments we performed showed that the Cactis algorithm is better than the ORION algorithm and that the CK algorithm totally outperforms both other algorithms in terms of response time and clustering overhead.",
        "published": "2007-05-03T13:02:06Z",
        "link": "http://arxiv.org/abs/0705.0454v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Mining Patterns with a Balanced Interval",
        "authors": [
            "Edgar de Graaf Joost Kok Walter Kosters"
        ],
        "summary": "In many applications it will be useful to know those patterns that occur with a balanced interval, e.g., a certain combination of phone numbers are called almost every Friday or a group of products are sold a lot on Tuesday and Thursday.   In previous work we proposed a new measure of support (the number of occurrences of a pattern in a dataset), where we count the number of times a pattern occurs (nearly) in the middle between two other occurrences. If the number of non-occurrences between two occurrences of a pattern stays almost the same then we call the pattern balanced.   It was noticed that some very frequent patterns obviously also occur with a balanced interval, meaning in every transaction. However more interesting patterns might occur, e.g., every three transactions. Here we discuss a solution using standard deviation and average. Furthermore we propose a simpler approach for pruning patterns with a balanced interval, making estimating the pruning threshold more intuitive.",
        "published": "2007-05-08T15:22:38Z",
        "link": "http://arxiv.org/abs/0705.1110v1",
        "categories": [
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "DWEB: A Data Warehouse Engineering Benchmark",
        "authors": [
            "Jérôme Darmont",
            "Fadila Bentayeb",
            "Omar Boussaïd"
        ],
        "summary": "Data warehouse architectural choices and optimization techniques are critical to decision support query performance. To facilitate these choices, the performance of the designed data warehouse must be assessed. This is usually done with the help of benchmarks, which can either help system users comparing the performances of different systems, or help system engineers testing the effect of various design choices. While the TPC standard decision support benchmarks address the first point, they are not tuneable enough to address the second one and fail to model different data warehouse schemas. By contrast, our Data Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc synthetic data warehouses and workloads. DWEB is fully parameterized to fulfill data warehouse design needs. However, two levels of parameterization keep it relatively easy to tune. Finally, DWEB is implemented as a Java free software that can be interfaced with most existing relational database management systems. A sample usage of DWEB is also provided in this paper.",
        "published": "2007-05-10T12:23:35Z",
        "link": "http://arxiv.org/abs/0705.1453v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "DOEF: A Dynamic Object Evaluation Framework",
        "authors": [
            "Zhen He",
            "Jérôme Darmont"
        ],
        "summary": "In object-oriented or object-relational databases such as multimedia databases or most XML databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. However, this has been the way these databases and associated optimisation techniques like clustering have been evaluated up to now. This paper opens up research regarding this issue by proposing a dynamic object evaluation framework (DOEF) that accomplishes access pattern change by defining configurable styles of change. This preliminary prototype has been designed to be open and fully extensible. To illustrate the capabilities of DOEF, we used it to compare the performances of four state of the art dynamic clustering algorithms. The results show that DOEF is indeed effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern.",
        "published": "2007-05-10T12:24:27Z",
        "link": "http://arxiv.org/abs/0705.1454v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Decision tree modeling with relational views",
        "authors": [
            "Fadila Bentayeb",
            "Jérôme Darmont"
        ],
        "summary": "Data mining is a useful decision support technique that can be used to discover production rules in warehouses or corporate data. Data mining research has made much effort to apply various mining algorithms efficiently on large databases. However, a serious problem in their practical application is the long processing time of such algorithms. Nowadays, one of the key challenges is to integrate data mining methods within the framework of traditional database systems. Indeed, such implementations can take advantage of the efficiency provided by SQL engines. In this paper, we propose an integrating approach for decision trees within a classical database system. In other words, we try to discover knowledge from relational databases, in the form of production rules, via a procedure embedding SQL queries. The obtained decision tree is defined by successive, related relational views. Each view corresponds to a given population in the underlying decision tree. We selected the classical Induction Decision Tree (ID3) algorithm to build the decision tree. To prove that our implementation of ID3 works properly, we successfully compared the output of our procedure with the output of an existing and validated data mining software, SIPINA. Furthermore, since our approach is tuneable, it can be generalized to any other similar decision tree-based method.",
        "published": "2007-05-10T12:25:57Z",
        "link": "http://arxiv.org/abs/0705.1455v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Warehousing Web Data",
        "authors": [
            "Jérôme Darmont",
            "Omar Boussaïd",
            "Fadila Bentayeb"
        ],
        "summary": "In a data warehousing process, mastering the data preparation phase allows substantial gains in terms of time and performance when performing multidimensional analysis or using data mining algorithms. Furthermore, a data warehouse can require external data. The web is a prevalent data source in this context. In this paper, we propose a modeling process for integrating diverse and heterogeneous (so-called multiform) data into a unified format. Furthermore, the very schema definition provides first-rate metadata in our data warehousing context. At the conceptual level, a complex object is represented in UML. Our logical model is an XML schema that can be described with a DTD or the XML-Schema language. Eventually, we have designed a Java prototype that transforms our multiform input data into XML documents representing our physical model. Then, the XML documents we obtain are mapped into a relational database we view as an ODS (Operational Data Storage), whose content will have to be re-modeled in a multidimensional way to allow its storage in a star schema-based warehouse and, later, its analysis.",
        "published": "2007-05-10T12:28:52Z",
        "link": "http://arxiv.org/abs/0705.1456v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Web data modeling for integration in data warehouses",
        "authors": [
            "Sami Miniaoui",
            "Jérôme Darmont",
            "Omar Boussaïd"
        ],
        "summary": "In a data warehousing process, the data preparation phase is crucial. Mastering this phase allows substantial gains in terms of time and performance when performing a multidimensional analysis or using data mining algorithms. Furthermore, a data warehouse can require external data. The web is a prevalent data source in this context, but the data broadcasted on this medium are very heterogeneous. We propose in this paper a UML conceptual model for a complex object representing a superclass of any useful data source (databases, plain texts, HTML and XML documents, images, sounds, video clips...). The translation into a logical model is achieved with XML, which helps integrating all these diverse, heterogeneous data into a unified format, and whose schema definition provides first-rate metadata in our data warehousing context. Moreover, we benefit from XML's flexibility, extensibility and from the richness of the semi-structured data model, but we are still able to later map XML documents into a database if more structuring is needed.",
        "published": "2007-05-10T12:30:19Z",
        "link": "http://arxiv.org/abs/0705.1457v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Worst-Case Background Knowledge for Privacy-Preserving Data Publishing",
        "authors": [
            "David J. Martin",
            "Daniel Kifer",
            "Ashwin Machanavajjhala",
            "Johannes Gehrke",
            "Joseph Y. Halpern"
        ],
        "summary": "Recent work has shown the necessity of considering an attacker's background knowledge when reasoning about privacy in data publishing. However, in practice, the data publisher does not know what background knowledge the attacker possesses. Thus, it is important to consider the worst-case. In this paper, we initiate a formal study of worst-case background knowledge. We propose a language that can express any background knowledge about the data. We provide a polynomial time algorithm to measure the amount of disclosure of sensitive information in the worst case, given that the attacker has at most a specified number of pieces of information in this language. We also provide a method to efficiently sanitize the data so that the amount of disclosure in the worst case is less than a specified threshold.",
        "published": "2007-05-19T00:12:24Z",
        "link": "http://arxiv.org/abs/0705.2787v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Translating a first-order modal language to relational algebra",
        "authors": [
            "Yeb Havinga"
        ],
        "summary": "This paper is about Kripke structures that are inside a relational database and queried with a modal language. At first the modal language that is used is introduced, followed by a definition of the database and relational algebra. Based on these definitions two things are presented: a mapping from components of the modal structure to a relational database schema and instance, and a translation from queries in the modal language to relational algebra queries.",
        "published": "2007-05-27T12:36:58Z",
        "link": "http://arxiv.org/abs/0705.3949v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "H.2.3; I.2.4"
        ]
    },
    {
        "title": "World-set Decompositions: Expressiveness and Efficient Algorithms",
        "authors": [
            "Dan Olteanu",
            "Christoph Koch",
            "Lyublena Antova"
        ],
        "summary": "Uncertain information is commonplace in real-world data management scenarios. The ability to represent large sets of possible instances (worlds) while supporting efficient storage and processing is an important challenge in this context. The recent formalism of world-set decompositions (WSDs) provides a space-efficient representation for uncertain data that also supports scalable processing. WSDs are complete for finite world-sets in that they can represent any finite set of possible worlds. For possibly infinite world-sets, we show that a natural generalization of WSDs precisely captures the expressive power of c-tables. We then show that several important decision problems are efficiently solvable on WSDs while they are NP-hard on c-tables. Finally, we give a polynomial-time algorithm for factorizing WSDs, i.e. an efficient algorithm for minimizing such representations.",
        "published": "2007-05-30T17:56:06Z",
        "link": "http://arxiv.org/abs/0705.4442v2",
        "categories": [
            "cs.DB",
            "H.2.1; H.2.4"
        ]
    },
    {
        "title": "Recursive n-gram hashing is pairwise independent, at best",
        "authors": [
            "Daniel Lemire",
            "Owen Kaser"
        ],
        "summary": "Many applications use sequences of n consecutive symbols (n-grams). Hashing these n-grams can be a performance bottleneck. For more speed, recursive hash families compute hash values by updating previous values. We prove that recursive hash families cannot be more than pairwise independent. While hashing by irreducible polynomials is pairwise independent, our implementations either run in time O(n) or use an exponential amount of memory. As a more scalable alternative, we make hashing by cyclic polynomials pairwise independent by ignoring n-1 bits. Experimentally, we show that hashing by cyclic polynomials is is twice as fast as hashing by irreducible polynomials. We also show that randomized Karp-Rabin hash families are not pairwise independent.",
        "published": "2007-05-31T18:41:28Z",
        "link": "http://arxiv.org/abs/0705.4676v8",
        "categories": [
            "cs.DB",
            "cs.CL",
            "H.3.3, H.2.7"
        ]
    },
    {
        "title": "The $k$-anonymity Problem is Hard",
        "authors": [
            "Paola Bonizzoni",
            "Gianluca Della Vedova",
            "Riccardo Dondi"
        ],
        "summary": "The problem of publishing personal data without giving up privacy is becoming increasingly important. An interesting formalization recently proposed is the k-anonymity. This approach requires that the rows in a table are clustered in sets of size at least k and that all the rows in a cluster become the same tuple, after the suppression of some records. The natural optimization problem, where the goal is to minimize the number of suppressed entries, is known to be NP-hard when the values are over a ternary alphabet, k = 3 and the rows length is unbounded. In this paper we give a lower bound on the approximation factor that any polynomial-time algorithm can achive on two restrictions of the problem,namely (i) when the records values are over a binary alphabet and k = 3, and (ii) when the records have length at most 8 and k = 4, showing that these restrictions of the problem are APX-hard.",
        "published": "2007-07-03T14:17:49Z",
        "link": "http://arxiv.org/abs/0707.0421v2",
        "categories": [
            "cs.DB",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Semantic Information Retrieval from Distributed Heterogeneous Data   Sources",
        "authors": [
            "K. Munir",
            "M. Odeh",
            "R. McClatchey",
            "S. Khan",
            "I. Habib"
        ],
        "summary": "Information retrieval from distributed heterogeneous data sources remains a challenging issue. As the number of data sources increases more intelligent retrieval techniques, focusing on information content and semantics, are required. Currently ontologies are being widely used for managing semantic knowledge, especially in the field of bioinformatics. In this paper we describe an ontology assisted system that allows users to query distributed heterogeneous data sources by hiding details like location, information structure, access pattern and semantic structure of the data. Our goal is to provide an integrated view on biomedical information sources for the Health-e-Child project with the aim to overcome the lack of sufficient semantic-based reformulation techniques for querying distributed data sources. In particular, this paper examines the problem of query reformulation across biomedical data sources, based on merged ontologies and the underlying heterogeneous descriptions of the respective data sources.",
        "published": "2007-07-05T09:40:19Z",
        "link": "http://arxiv.org/abs/0707.0745v1",
        "categories": [
            "cs.DB",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "The Requirements for Ontologies in Medical Data Integration: A Case   Study",
        "authors": [
            "Ashiq Anjum",
            "Peter Bloodsworth",
            "Andrew Branson",
            "Tamas Hauer",
            "Richard McClatchey",
            "Kamran Munir",
            "Dmitry Rogulin",
            "Jetendr Shamdasani"
        ],
        "summary": "Evidence-based medicine is critically dependent on three sources of information: a medical knowledge base, the patients medical record and knowledge of available resources, including where appropriate, clinical protocols. Patient data is often scattered in a variety of databases and may, in a distributed model, be held across several disparate repositories. Consequently addressing the needs of an evidence-based medicine community presents issues of biomedical data integration, clinical interpretation and knowledge management. This paper outlines how the Health-e-Child project has approached the challenge of requirements specification for (bio-) medical data integration, from the level of cellular data, through disease to that of patient and population. The approach is illuminated through the requirements elicitation and analysis of Juvenile Idiopathic Arthritis (JIA), one of three diseases being studied in the EC-funded Health-e-Child project.",
        "published": "2007-07-05T11:21:39Z",
        "link": "http://arxiv.org/abs/0707.0763v1",
        "categories": [
            "cs.DB",
            "D.2.11"
        ]
    },
    {
        "title": "Espaces de représentation multidimensionnels dédiés à la   visualisation",
        "authors": [
            "Riadh Ben Messaoud",
            "Kamel Aouiche",
            "Cécile Favre"
        ],
        "summary": "In decision-support systems, the visual component is important for On Line Analysis Processing (OLAP). In this paper, we propose a new approach that faces the visualization problem due to data sparsity. We use the results of a Multiple Correspondence Analysis (MCA) to reduce the negative effect of sparsity by organizing differently data cube cells. Our approach does not reduce sparsity, however it tries to build relevant representation spaces where facts are efficiently gathered. In order to evaluate our approach, we propose an homogeneity criterion based on geometric neighborhood of cells. The obtained experimental results have shown the efficiency of our method.",
        "published": "2007-07-09T15:52:02Z",
        "link": "http://arxiv.org/abs/0707.1288v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Un index de jointure pour les entrepôts de données XML",
        "authors": [
            "Hadj Mahboubi",
            "Kamel Aouiche",
            "Jérôme Darmont"
        ],
        "summary": "XML data warehouses form an interesting basis for decision-support applications that exploit heterogeneous data from multiple sources. However, XML-native database systems currently bear limited performances and it is necessary to research ways to optimize them. In this paper, we propose a new index that is specifically adapted to the multidimensional architecture of XML warehouses and eliminates join operations, while preserving the information contained in the original warehouse. A theoretical study and experimental results demonstrate the efficiency of our index, even when queries are complex.",
        "published": "2007-07-09T16:58:14Z",
        "link": "http://arxiv.org/abs/0707.1304v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Sélection simultanée d'index et de vues matérialisées",
        "authors": [
            "Nora Maiz",
            "Kamel Aouiche",
            "Jérôme Darmont"
        ],
        "summary": "Indices and materialized views are physical structures that accelerate data access in data warehouses. However, these data structures generate some maintenance overhead. They also share the same storage space. The existing studies about index and materialized view selection consider these structures separately. In this paper, we adopt the opposite stance and couple index and materialized view selection to take into account the interactions between them and achieve an efficient storage space sharing. We develop cost models that evaluate the respective benefit of indexing and view materialization. These cost models are then exploited by a greedy algorithm to select a relevant configuration of indices and materialized views. Experimental results show that our strategy performs better than the independent selection of indices and materialized views.",
        "published": "2007-07-09T17:23:31Z",
        "link": "http://arxiv.org/abs/0707.1306v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "An Architecture Framework for Complex Data Warehouses",
        "authors": [
            "Jérôme Darmont",
            "Omar Boussaid",
            "Jean-Christian Ralaivao",
            "Kamel Aouiche"
        ],
        "summary": "Nowadays, many decision support applications need to exploit data that are not only numerical or symbolic, but also multimedia, multistructure, multisource, multimodal, and/or multiversion. We term such data complex data. Managing and analyzing complex data involves a lot of different issues regarding their structure, storage and processing, and metadata are a key element in all these processes. Such problems have been addressed by classical data warehousing (i.e., applied to \"simple\" data). However, data warehousing approaches need to be adapted for complex data. In this paper, we first propose a precise, though open, definition of complex data. Then we present a general architecture framework for warehousing complex data. This architecture heavily relies on metadata and domain-related knowledge, and rests on the XML language, which helps storing data, metadata and domain-specific knowledge altogether, and facilitates communication between the various warehousing processes.",
        "published": "2007-07-10T22:01:40Z",
        "link": "http://arxiv.org/abs/0707.1534v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Data Mining-based Materialized View and Index Selection in Data   Warehouses",
        "authors": [
            "Kamel Aouiche",
            "Jérôme Darmont"
        ],
        "summary": "Materialized views and indexes are physical structures for accelerating data access that are casually used in data warehouses. However, these data structures generate some maintenance overhead. They also share the same storage space. Most existing studies about materialized view and index selection consider these structures separately. In this paper, we adopt the opposite stance and couple materialized view and index selection to take view-index interactions into account and achieve efficient storage space sharing. Candidate materialized views and indexes are selected through a data mining process. We also exploit cost models that evaluate the respective benefit of indexing and view materialization, and help select a relevant configuration of indexes and materialized views among the candidates. Experimental results show that our strategy performs better than an independent selection of materialized views and indexes.",
        "published": "2007-07-11T02:45:10Z",
        "link": "http://arxiv.org/abs/0707.1548v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Fast and Simple Relational Processing of Uncertain Data",
        "authors": [
            "Lyublena Antova",
            "Thomas Jansen",
            "Christoph Koch",
            "Dan Olteanu"
        ],
        "summary": "This paper introduces U-relations, a succinct and purely relational representation system for uncertain databases. U-relations support attribute-level uncertainty using vertical partitioning. If we consider positive relational algebra extended by an operation for computing possible answers, a query on the logical level can be translated into, and evaluated as, a single relational algebra query on the U-relation representation. The translation scheme essentially preserves the size of the query in terms of number of operations and, in particular, number of joins. Standard techniques employed in off-the-shelf relational database management systems are effective for optimizing and processing queries on U-relations. In our experiments we show that query evaluation on U-relations scales to large amounts of data with high degrees of uncertainty.",
        "published": "2007-07-11T15:13:39Z",
        "link": "http://arxiv.org/abs/0707.1644v1",
        "categories": [
            "cs.DB",
            "cs.PF",
            "H.2.1; H.2.4"
        ]
    },
    {
        "title": "Spatial Aggregation: Data Model and Implementation",
        "authors": [
            "Leticia Gomez",
            "Sofie Haesevoets",
            "Bart Kuijpers",
            "Alejandro Vaisman"
        ],
        "summary": "Data aggregation in Geographic Information Systems (GIS) is only marginally present in commercial systems nowadays, mostly through ad-hoc solutions. In this paper, we first present a formal model for representing spatial data. This model integrates geographic data and information contained in data warehouses external to the GIS. We define the notion of geometric aggregation, a general framework for aggregate queries in a GIS setting. We also identify the class of summable queries, which can be efficiently evaluated by precomputing the overlay of two or more of the thematic layers involved in the query. We also sketch a language, denoted GISOLAP-QL, for expressing queries that involve GIS and OLAP features. In addition, we introduce Piet, an implementation of our proposal, that makes use of overlay precomputation for answering spatial queries (aggregate or not). Our experimental evaluation showed that for a certain class of geometric queries with or without aggregation, overlay precomputation outperforms R-tree-based techniques. Finally, as a particular application of our proposal, we study topological queries.",
        "published": "2007-07-29T21:46:50Z",
        "link": "http://arxiv.org/abs/0707.4304v1",
        "categories": [
            "cs.DB",
            "H.2.8"
        ]
    },
    {
        "title": "Why the relational data model can be considered as a formal basis for   group operations in object-oriented systems",
        "authors": [
            "Evgeniy Grigoriev"
        ],
        "summary": "Relational data model defines a specification of a type \"relation\". However, its simplicity does not mean that the system implementing this model must operate with structures having the same simplicity. We consider two principles allowing create a system which combines object-oriented paradigm (OOP) and relational data model (RDM) in one framework. The first principle -- \"complex data in encapsulated domains\" -- is well known from The Third Manifesto by Date and Darwen. The second principle --\"data complexity in names\"-- is the basis for a system where data are described as complex objects and uniquely represented as a set of relations. Names of these relations and names of their attributes are combinations of names entered in specifications of the complex objects. Below, we consider the main properties of such a system.",
        "published": "2007-08-02T15:24:29Z",
        "link": "http://arxiv.org/abs/0708.0361v7",
        "categories": [
            "cs.DB",
            "E.2; D.3.3; F.3.3; F.4.1; H.2.1; H.2.3; H.2.4; H.3.3"
        ]
    },
    {
        "title": "Repairing Inconsistent XML Write-Access Control Policies",
        "authors": [
            "Loreto Bravo",
            "James Cheney",
            "Irini Fundulaki"
        ],
        "summary": "XML access control policies involving updates may contain security flaws, here called inconsistencies, in which a forbidden operation may be simulated by performing a sequence of allowed operations. This paper investigates the problem of deciding whether a policy is consistent, and if not, how its inconsistencies can be repaired. We consider policies expressed in terms of annotated DTDs defining which operations are allowed or denied for the XML trees that are instances of the DTD. We show that consistency is decidable in PTIME for such policies and that consistent partial policies can be extended to unique \"least-privilege\" consistent total policies. We also consider repair problems based on deleting privileges to restore consistency, show that finding minimal repairs is NP-complete, and give heuristics for finding repairs.",
        "published": "2007-08-15T18:31:48Z",
        "link": "http://arxiv.org/abs/0708.2076v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Provenance as Dependency Analysis",
        "authors": [
            "James Cheney",
            "Amal Ahmed",
            "Umut Acar"
        ],
        "summary": "Provenance is information recording the source, derivation, or history of some information. Provenance tracking has been studied in a variety of settings; however, although many design points have been explored, the mathematical or semantic foundations of data provenance have received comparatively little attention. In this paper, we argue that dependency analysis techniques familiar from program analysis and program slicing provide a formal foundation for forms of provenance that are intended to show how (part of) the output of a query depends on (parts of) its input. We introduce a semantic characterization of such dependency provenance, show that this form of provenance is not computable, and provide dynamic and static approximation techniques.",
        "published": "2007-08-16T11:11:43Z",
        "link": "http://arxiv.org/abs/0708.2173v2",
        "categories": [
            "cs.DB",
            "cs.PL",
            "H.2.3; F.3.2"
        ]
    },
    {
        "title": "Aggregation Languages for Moving Object and Places of Interest Data",
        "authors": [
            "Leticia Gomez",
            "Bart Kuijpers",
            "Alejandro Vaisman"
        ],
        "summary": "We address aggregate queries over GIS data and moving object data, where non-spatial data are stored in a data warehouse. We propose a formal data model and query language to express complex aggregate queries. Next, we study the compression of trajectory data, produced by moving objects, using the notions of stops and moves. We show that stops and moves are expressible in our query language and we consider a fragment of this language, consisting of regular expressions to talk about temporally ordered sequences of stops and moves. This fragment can be used to efficiently express data mining and pattern matching tasks over trajectory data.",
        "published": "2007-08-20T20:08:53Z",
        "link": "http://arxiv.org/abs/0708.2717v1",
        "categories": [
            "cs.DB",
            "H.2.8"
        ]
    },
    {
        "title": "Fast evaluation of union-intersection expressions",
        "authors": [
            "Philip Bille",
            "Anna Pagh",
            "Rasmus Pagh"
        ],
        "summary": "We show how to represent sets in a linear space data structure such that expressions involving unions and intersections of sets can be computed in a worst-case efficient way. This problem has applications in e.g. information retrieval and database systems. We mainly consider the RAM model of computation, and sets of machine words, but also state our results in the I/O model. On a RAM with word size $w$, a special case of our result is that the intersection of $m$ (preprocessed) sets, containing $n$ elements in total, can be computed in expected time $O(n (\\log w)^2 / w + km)$, where $k$ is the number of elements in the intersection. If the first of the two terms dominates, this is a factor $w^{1-o(1)}$ faster than the standard solution of merging sorted lists. We show a cell probe lower bound of time $\\Omega(n/(w m \\log m)+ (1-\\tfrac{\\log k}{w}) k)$, meaning that our upper bound is nearly optimal for small $m$. Our algorithm uses a novel combination of approximate set representations and word-level parallelism.",
        "published": "2007-08-23T22:23:04Z",
        "link": "http://arxiv.org/abs/0708.3259v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "Inferring Neuronal Network Connectivity using Time-constrained Episodes",
        "authors": [
            "Debprakash Patnaik",
            "P. S. Sastry",
            "K. P. Unnikrishnan"
        ],
        "summary": "Discovering frequent episodes in event sequences is an interesting data mining task. In this paper, we argue that this framework is very effective for analyzing multi-neuronal spike train data. Analyzing spike train data is an important problem in neuroscience though there are no data mining approaches reported for this. Motivated by this application, we introduce different temporal constraints on the occurrences of episodes. We present algorithms for discovering frequent episodes under temporal constraints. Through simulations, we show that our method is very effective for analyzing spike train data for unearthing underlying connectivity patterns.",
        "published": "2007-09-03T13:23:33Z",
        "link": "http://arxiv.org/abs/0709.0218v2",
        "categories": [
            "cs.DB",
            "q-bio.NC"
        ]
    },
    {
        "title": "Discovering Patterns in Multi-neuronal Spike Trains using the Frequent   Episode Method",
        "authors": [
            "K. P. Unnikrishnan",
            "Debprakash Patnaik",
            "P. S. Sastry"
        ],
        "summary": "Discovering the 'Neural Code' from multi-neuronal spike trains is an important task in neuroscience. For such an analysis, it is important to unearth interesting regularities in the spiking patterns. In this report, we present an efficient method for automatically discovering synchrony, synfire chains, and more general sequences of neuronal firings. We use the Frequent Episode Discovery framework of Laxman, Sastry, and Unnikrishnan (2005), in which the episodes are represented and recognized using finite-state automata. Many aspects of functional connectivity between neuronal populations can be inferred from the episodes. We demonstrate these using simulated multi-neuronal data from a Poisson model. We also present a method to assess the statistical significance of the discovered episodes. Since the Temporal Data Mining (TDM) methods used in this report can analyze data from hundreds and potentially thousands of neurons, we argue that this framework is appropriate for discovering the `Neural Code'.",
        "published": "2007-09-05T14:46:59Z",
        "link": "http://arxiv.org/abs/0709.0566v2",
        "categories": [
            "cs.DB",
            "q-bio.NC"
        ]
    },
    {
        "title": "An Optimal Linear Time Algorithm for Quasi-Monotonic Segmentation",
        "authors": [
            "Daniel Lemire",
            "Martin Brooks",
            "Yuhong Yan"
        ],
        "summary": "Monotonicity is a simple yet significant qualitative characteristic. We consider the problem of segmenting a sequence in up to K segments. We want segments to be as monotonic as possible and to alternate signs. We propose a quality metric for this problem using the l_inf norm, and we present an optimal linear time algorithm based on novel formalism. Moreover, given a precomputation in time O(n log n) consisting of a labeling of all extrema, we compute any optimal segmentation in constant time. We compare experimentally its performance to two piecewise linear segmentation heuristics (top-down and bottom-up). We show that our algorithm is faster and more accurate. Applications include pattern recognition and qualitative modeling.",
        "published": "2007-09-07T21:18:57Z",
        "link": "http://arxiv.org/abs/0709.1166v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Efficient Algorithms for Node Disjoint Subgraph Homeomorphism   Determination",
        "authors": [
            "Yanghua Xiao",
            "Wentao Wu",
            "Wei Wang",
            "Zhengying He"
        ],
        "summary": "Recently, great efforts have been dedicated to researches on the management of large scale graph based data such as WWW, social networks, biological networks. In the study of graph based data management, node disjoint subgraph homeomorphism relation between graphs is more suitable than (sub)graph isomorphism in many cases, especially in those cases that node skipping and node mismatching are allowed. However, no efficient node disjoint subgraph homeomorphism determination (ndSHD) algorithms have been available. In this paper, we propose two computationally efficient ndSHD algorithms based on state spaces searching with backtracking, which employ many heuristics to prune the search spaces. Experimental results on synthetic data sets show that the proposed algorithms are efficient, require relative little time in most of the testing cases, can scale to large or dense graphs, and can accommodate to more complex fuzzy matching cases.",
        "published": "2007-09-08T18:14:47Z",
        "link": "http://arxiv.org/abs/0709.1227v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Autoencoder, Principal Component Analysis and Support Vector Regression   for Data Imputation",
        "authors": [
            "Vukosi N. Marivate",
            "Fulufhelo V. Nelwamodo",
            "Tshilidzi Marwala"
        ],
        "summary": "Data collection often results in records that have missing values or variables. This investigation compares 3 different data imputation models and identifies their merits by using accuracy measures. Autoencoder Neural Networks, Principal components and Support Vector regression are used for prediction and combined with a genetic algorithm to then impute missing variables. The use of PCA improves the overall performance of the autoencoder network while the use of support vector regression shows promising potential for future investigation. Accuracies of up to 97.4 % on imputation of some of the variables were achieved.",
        "published": "2007-09-16T18:15:01Z",
        "link": "http://arxiv.org/abs/0709.2506v1",
        "categories": [
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "Query Evaluation in P2P Systems of Taxonomy-based Sources: Algorithms,   Complexity, and Optimizations",
        "authors": [
            "Carlo Meghini",
            "Yannis Tzitzikas",
            "Anastasia Analyti"
        ],
        "summary": "In this study, we address the problem of answering queries over a peer-to-peer system of taxonomy-based sources. A taxonomy states subsumption relationships between negation-free DNF formulas on terms and negation-free conjunctions of terms. To the end of laying the foundations of our study, we first consider the centralized case, deriving the complexity of the decision problem and of query evaluation. We conclude by presenting an algorithm that is efficient in data complexity and is based on hypergraphs. More expressive forms of taxonomies are also investigated, which however lead to intractability. We then move to the distributed case, and introduce a logical model of a network of taxonomy-based sources. On such network, a distributed version of the centralized algorithm is then presented, based on a message passing paradigm, and its correctness is proved. We finally discuss optimization issues, and relate our work to the literature.",
        "published": "2007-09-19T15:10:05Z",
        "link": "http://arxiv.org/abs/0709.3034v1",
        "categories": [
            "cs.DB",
            "cs.DC",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "Mining for trees in a graph is NP-complete",
        "authors": [
            "Jan Van den Bussche"
        ],
        "summary": "Mining for trees in a graph is shown to be NP-complete.",
        "published": "2007-09-28T17:08:39Z",
        "link": "http://arxiv.org/abs/0709.4655v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "Semantic distillation: a method for clustering objects by their   contextual specificity",
        "authors": [
            "Thomas Sierocinski",
            "Anthony Le Béchec",
            "Nathalie Théret",
            "Dimitri Petritis"
        ],
        "summary": "Techniques for data-mining, latent semantic analysis, contextual search of databases, etc. have long ago been developed by computer scientists working on information retrieval (IR). Experimental scientists, from all disciplines, having to analyse large collections of raw experimental data (astronomical, physical, biological, etc.) have developed powerful methods for their statistical analysis and for clustering, categorising, and classifying objects. Finally, physicists have developed a theory of quantum measurement, unifying the logical, algebraic, and probabilistic aspects of queries into a single formalism. The purpose of this paper is twofold: first to show that when formulated at an abstract level, problems from IR, from statistical data analysis, and from physical measurement theories are very similar and hence can profitably be cross-fertilised, and, secondly, to propose a novel method of fuzzy hierarchical clustering, termed \\textit{semantic distillation} -- strongly inspired from the theory of quantum measurement --, we developed to analyse raw data coming from various types of experiments on DNA arrays. We illustrate the method by analysing DNA arrays experiments and clustering the genes of the array according to their specificity.",
        "published": "2007-10-05T12:30:43Z",
        "link": "http://arxiv.org/abs/0710.1203v2",
        "categories": [
            "math.PR",
            "cs.DB",
            "math.ST",
            "q-bio.QM",
            "stat.ML",
            "stat.TH"
        ]
    },
    {
        "title": "Performance Comparison of Persistence Frameworks",
        "authors": [
            "Sabu M. Thampi",
            "Ashwin a K"
        ],
        "summary": "One of the essential and most complex components in the software development process is the database. The complexity increases when the \"orientation\" of the interacting components differs. A persistence framework moves the program data in its most natural form to and from a permanent data store, the database. Thus a persistence framework manages the database and the mapping between the database and the objects. This paper compares the performance of two persistence frameworks ? Hibernate and iBatis?s SQLMaps using a banking database. The performance of both of these tools in single and multi-user environments are evaluated.",
        "published": "2007-10-07T08:22:53Z",
        "link": "http://arxiv.org/abs/0710.1404v1",
        "categories": [
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "Association Rules in the Relational Calculus",
        "authors": [
            "Oliver Schulte",
            "Flavia Moser",
            "Martin Ester",
            "Zhiyong Lu"
        ],
        "summary": "One of the most utilized data mining tasks is the search for association rules. Association rules represent significant relationships between items in transactions. We extend the concept of association rule to represent a much broader class of associations, which we refer to as \\emph{entity-relationship rules.} Semantically, entity-relationship rules express associations between properties of related objects. Syntactically, these rules are based on a broad subclass of safe domain relational calculus queries. We propose a new definition of support and confidence for entity-relationship rules and for the frequency of entity-relationship queries. We prove that the definition of frequency satisfies standard probability axioms and the Apriori property.",
        "published": "2007-10-10T18:00:44Z",
        "link": "http://arxiv.org/abs/0710.2083v1",
        "categories": [
            "cs.DB",
            "cs.LG",
            "cs.LO"
        ]
    },
    {
        "title": "Collaborative OLAP with Tag Clouds: Web 2.0 OLAP Formalism and   Experimental Evaluation",
        "authors": [
            "Kamel Aouiche",
            "Daniel Lemire",
            "Robert Godin"
        ],
        "summary": "Increasingly, business projects are ephemeral. New Business Intelligence tools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds are a popular community-driven visualization technique. Hence, we investigate tag-cloud views with support for OLAP operations such as roll-ups, slices, dices, clustering, and drill-downs. As a case study, we implemented an application where users can upload data and immediately navigate through its ad hoc dimensions. To support social networking, views can be easily shared and embedded in other Web sites. Algorithmically, our tag-cloud views are approximate range top-k queries over spontaneous data cubes. We present experimental evidence that iceberg cuboids provide adequate online approximations. We benchmark several browser-oblivious tag-cloud layout optimizations.",
        "published": "2007-10-11T19:48:10Z",
        "link": "http://arxiv.org/abs/0710.2156v3",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Efficient Skyline Querying with Variable User Preferences on Nominal   Attributes",
        "authors": [
            "Raymond Chi-Wing Wong",
            "Ada Wai-chee Fu",
            "Jian Pei",
            "Yip Sing Ho",
            "Tai Wong",
            "Yubao Liu"
        ],
        "summary": "Current skyline evaluation techniques assume a fixed ordering on the attributes. However, dynamic preferences on nominal attributes are more realistic in known applications. In order to generate online response for any such preference issued by a user, we propose two methods of different characteristics. The first one is a semi-materialization method and the second is an adaptive SFS method. Finally, we conduct experiments to show the efficiency of our proposed algorithms.",
        "published": "2007-10-13T11:47:14Z",
        "link": "http://arxiv.org/abs/0710.2604v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "A quick search method for audio signals based on a piecewise linear   representation of feature trajectories",
        "authors": [
            "Akisato Kimura",
            "Kunio Kashino",
            "Takayuki Kurozumi",
            "Hiroshi Murase"
        ],
        "summary": "This paper presents a new method for a quick similarity-based search through long unlabeled audio streams to detect and locate audio clips provided by users. The method involves feature-dimension reduction based on a piecewise linear representation of a sequential feature trajectory extracted from a long audio stream. Two techniques enable us to obtain a piecewise linear representation: the dynamic segmentation of feature trajectories and the segment-based Karhunen-L\\'{o}eve (KL) transform. The proposed search method guarantees the same search results as the search method without the proposed feature-dimension reduction method in principle. Experiment results indicate significant improvements in search speed. For example the proposed method reduced the total search time to approximately 1/12 that of previous methods and detected queries in approximately 0.3 seconds from a 200-hour audio database.",
        "published": "2007-10-23T03:06:53Z",
        "link": "http://arxiv.org/abs/0710.4180v1",
        "categories": [
            "cs.MM",
            "cs.DB"
        ]
    },
    {
        "title": "Querying XML Documents in Logic Programming",
        "authors": [
            "J. M. Almendros-Jiménez",
            "A. Becerra-Terón",
            "F. J. Enciso-Baños"
        ],
        "summary": "Extensible Markup Language (XML) is a simple, very flexible text format derived from SGML. Originally designed to meet the challenges of large-scale electronic publishing, XML is also playing an increasingly important role in the exchange of a wide variety of data on the Web and elsewhere. XPath language is the result of an effort to provide address parts of an XML document. In support of this primary purpose, it becomes in a query language against an XML document. In this paper we present a proposal for the implementation of the XPath language in logic programming. With this aim we will describe the representation of XML documents by means of a logic program. Rules and facts can be used for representing the document schema and the XML document itself. In particular, we will present how to index XML documents in logic programs: rules are supposed to be stored in main memory, however facts are stored in secondary memory by using two kind of indexes: one for each XML tag, and other for each group of terminal items. In addition, we will study how to query by means of the XPath language against a logic program representing an XML document. It evolves the specialization of the logic program with regard to the XPath expression. Finally, we will also explain how to combine the indexing and the top-down evaluation of the logic program. To appear in Theory and Practice of Logic Programming (TPLP)\"",
        "published": "2007-10-25T10:45:08Z",
        "link": "http://arxiv.org/abs/0710.4780v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "H.2.3; I.2.3"
        ]
    },
    {
        "title": "Neutrosophic Relational Data Model",
        "authors": [
            "Haibin Wang",
            "Rajshekhar Sunderraman",
            "Florentin Smarandache",
            "Andre Rogatko"
        ],
        "summary": "In this paper, we present a generalization of the relational data model based on interval neutrosophic set. Our data model is capable of manipulating incomplete as well as inconsistent information. Fuzzy relation or intuitionistic fuzzy relation can only handle incomplete information. Associated with each relation are two membership functions one is called truth-membership function T which keeps track of the extent to which we believe the tuple is in the relation, another is called falsity-membership function F which keeps track of the extent to which we believe that it is not in the relation. A neutrosophic relation is inconsistent if there exists one tuple a such that T(a) + F(a) > 1 . In order to handle inconsistent situation, we propose an operator called \"split\" to transform inconsistent neutrosophic relations into pseudo-consistent neutrosophic relations and do the set-theoretic and relation-theoretic operations on them and finally use another operator called \"combine\" to transform the result back to neutrosophic relation. For this data model, we define algebraic operators that are generalizations of the usual operators such as intersection, union, selection, join on fuzzy relations. Our data model can underlie any database and knowledge-base management system that deals with incomplete and inconsistent information.",
        "published": "2007-10-29T19:46:58Z",
        "link": "http://arxiv.org/abs/0710.5333v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Query Evaluation and Optimization in the Semantic Web",
        "authors": [
            "Edna Ruckhaus",
            "Eduardo Ruiz",
            "Maria-Esther Vidal"
        ],
        "summary": "We address the problem of answering Web ontology queries efficiently. An ontology is formalized as a Deductive Ontology Base (DOB), a deductive database that comprises the ontology's inference axioms and facts. A cost-based query optimization technique for DOB is presented. A hybrid cost model is proposed to estimate the cost and cardinality of basic and inferred facts. Cardinality and cost of inferred facts are estimated using an adaptive sampling technique, while techniques of traditional relational cost models are used for estimating the cost of basic facts and conjunctive ontology queries. Finally, we implement a dynamic-programming optimization algorithm to identify query evaluation plans that minimize the number of intermediate inferred facts. We modeled a subset of the Web ontology language OWL Lite as a DOB, and performed an experimental study to analyze the predictive capacity of our cost model and the benefits of the query optimization technique. Our study has been conducted over synthetic and real-world OWL ontologies, and shows that the techniques are accurate and improve query performance. To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2007-11-13T22:31:09Z",
        "link": "http://arxiv.org/abs/0711.2087v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "F.4.1; H.2.3; I.2.4"
        ]
    },
    {
        "title": "A Biologically Inspired Classifier",
        "authors": [
            "Franco Bagnoli",
            "Francesca Di Patti"
        ],
        "summary": "We present a method for measuring the distance among records based on the correlations of data stored in the corresponding database entries. The original method (F. Bagnoli, A. Berrones and F. Franci. Physica A 332 (2004) 509-518) was formulated in the context of opinion formation. The opinions expressed over a set of topic originate a ``knowledge network'' among individuals, where two individuals are nearer the more similar their expressed opinions are. Assuming that individuals' opinions are stored in a database, the authors show that it is possible to anticipate an opinion using the correlations in the database. This corresponds to approximating the overlap between the tastes of two individuals with the correlations of their expressed opinions.   In this paper we extend this model to nonlinear matching functions, inspired by biological problems such as microarray (probe-sample pairing). We investigate numerically the error between the correlation and the overlap matrix for eight sequences of reference with random probes. Results show that this method is particularly robust for detecting similarities in the presence of translocations.",
        "published": "2007-11-16T13:38:15Z",
        "link": "http://arxiv.org/abs/0711.2615v1",
        "categories": [
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "An Inflationary Fixed Point Operator in XQuery",
        "authors": [
            "Loredana Afanasiev",
            "Torsten Grust",
            "Maarten Marx",
            "Jan Rittinger",
            "Jens Teubner"
        ],
        "summary": "We introduce a controlled form of recursion in XQuery, inflationary fixed points, familiar in the context of relational databases. This imposes restrictions on the expressible types of recursion, but we show that inflationary fixed points nevertheless are sufficiently versatile to capture a wide range of interesting use cases, including the semantics of Regular XPath and its core transitive closure construct.   While the optimization of general user-defined recursive functions in XQuery appears elusive, we will describe how inflationary fixed points can be efficiently evaluated, provided that the recursive XQuery expressions exhibit a distributivity property. We show how distributivity can be assessed both, syntactically and algebraically, and provide experimental evidence that XQuery processors can substantially benefit during inflationary fixed point evaluation.",
        "published": "2007-11-21T13:22:15Z",
        "link": "http://arxiv.org/abs/0711.3375v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Simrank++: Query rewriting through link analysis of the click graph",
        "authors": [
            "Ioannis Antonellis",
            "Hector Garcia-Molina",
            "Chi-Chao Chang"
        ],
        "summary": "We focus on the problem of query rewriting for sponsored search. We base rewrites on a historical click graph that records the ads that have been clicked on in response to past user queries. Given a query q, we first consider Simrank as a way to identify queries similar to q, i.e., queries whose ads a user may be interested in. We argue that Simrank fails to properly identify query similarities in our application, and we present two enhanced version of Simrank: one that exploits weights on click graph edges and another that exploits ``evidence.'' We experimentally evaluate our new schemes against Simrank, using actual click graphs and queries form Yahoo!, and using a variety of metrics. Our results show that the enhanced methods can yield more and better query rewrites.",
        "published": "2007-12-04T12:43:17Z",
        "link": "http://arxiv.org/abs/0712.0499v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "Constructing Bio-molecular Databases on a DNA-based Computer",
        "authors": [
            "Weng-Long Chang",
            "Michael",
            "Ho",
            "Minyi Guo"
        ],
        "summary": "Codd [Codd 1970] wrote the first paper in which the model of a relational database was proposed. Adleman [Adleman 1994] wrote the first paper in which DNA strands in a test tube were used to solve an instance of the Hamiltonian path problem. From [Adleman 1994], it is obviously indicated that for storing information in molecules of DNA allows for an information density of approximately 1 bit per cubic nm (nanometer) and a dramatic improvement over existing storage media such as video tape which store information at a density of approximately 1 bit per 1012 cubic nanometers. This paper demonstrates that biological operations can be applied to construct bio-molecular databases where data records in relational tables are encoded as DNA strands. In order to achieve the goal, DNA algorithms are proposed to perform eight operations of relational algebra (calculus) on bio-molecular relational databases, which include Cartesian product, union, set difference, selection, projection, intersection, join and division. Furthermore, this work presents clear evidence of the ability of molecular computing to perform data retrieval operations on bio-molecular relational databases.",
        "published": "2007-12-12T03:58:01Z",
        "link": "http://arxiv.org/abs/0712.1863v1",
        "categories": [
            "cs.NE",
            "cs.DB",
            "q-bio.OT",
            "H.3.0; H.3.3; D.3.0; D.3.1; D.3.m"
        ]
    },
    {
        "title": "A case study of the difficulty of quantifier elimination in constraint   databases: the alibi query in moving object databases",
        "authors": [
            "Bart Kuijpers",
            "Walied Othman",
            "Rafael Grimson"
        ],
        "summary": "In the constraint database model, spatial and spatio-temporal data are stored by boolean combinations of polynomial equalities and inequalities over the real numbers. The relational calculus augmented with polynomial constraints is the standard first-order query language for constraint databases. Although the expressive power of this query language has been studied extensively, the difficulty of the efficient evaluation of queries, usually involving some form of quantifier elimination, has received considerably less attention. The inefficiency of existing quantifier-elimination software and the intrinsic difficulty of quantifier elimination have proven to be a bottle-neck for for real-world implementations of constraint database systems. In this paper, we focus on a particular query, called the \\emph{alibi query}, that asks whether two moving objects whose positions are known at certain moments in time, could have possibly met, given certain speed constraints. This query can be seen as a constraint database query and its evaluation relies on the elimination of a block of three existential quantifiers. Implementations of general purpose elimination algorithms are in the specific case, for practical purposes, too slow in answering the alibi query and fail completely in the parametric case. The main contribution of this paper is an analytical solution to the parametric alibi query, which can be used to answer this query in the specific case in constant time. We also give an analytic solution to the alibi query at a fixed moment in time. The solutions we propose are based on geometric argumentation and they illustrate the fact that some practical problems require creative solutions, where at least in theory, existing systems could provide a solution.",
        "published": "2007-12-12T18:05:41Z",
        "link": "http://arxiv.org/abs/0712.1996v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.DB"
        ]
    },
    {
        "title": "Middleware-based Database Replication: The Gaps between Theory and   Practice",
        "authors": [
            "Emmanuel Cecchet",
            "George Candea",
            "Anastasia Ailamaki"
        ],
        "summary": "The need for high availability and performance in data management systems has been fueling a long running interest in database replication from both academia and industry. However, academic groups often attack replication problems in isolation, overlooking the need for completeness in their solutions, while commercial teams take a holistic approach that often misses opportunities for fundamental innovation. This has created over time a gap between academic research and industrial practice.   This paper aims to characterize the gap along three axes: performance, availability, and administration. We build on our own experience developing and deploying replication systems in commercial and academic settings, as well as on a large body of prior related work. We sift through representative examples from the last decade of open-source, academic, and commercial database replication systems and combine this material with case studies from real systems deployed at Fortune 500 customers. We propose two agendas, one for academic research and one for industrial R&D, which we believe can bridge the gap within 5-10 years. This way, we hope to both motivate and help researchers in making the theory and practice of middleware-based database replication more relevant to each other.",
        "published": "2007-12-17T18:42:15Z",
        "link": "http://arxiv.org/abs/0712.2773v2",
        "categories": [
            "cs.DB",
            "cs.DC",
            "cs.PF",
            "H.2; C.2.4; C.4; D.4.5"
        ]
    },
    {
        "title": "QIS-XML: A metadata specification for Quantum Information Science",
        "authors": [
            "Pascal Heus",
            "Richard Gomez"
        ],
        "summary": "While Quantum Information Science (QIS) is still in its infancy, the ability for quantum based hardware or computers to communicate and integrate with their classical counterparts will be a major requirement towards their success. Little attention however has been paid to this aspect of QIS. To manage and exchange information between systems, today's classic Information Technology (IT) commonly uses the eXtensible Markup Language (XML) and its related tools. XML is composed of numerous specifications related to various fields of expertise. No such global specification however has been defined for quantum computers. QIS-XML is a proposed XML metadata specification for the description of fundamental components of QIS (gates & circuits) and a platform for the development of a hardware independent low level pseudo-code for quantum algorithms. This paper lays out the general characteristics of the QIS-XML specification and outlines practical applications through prototype use cases.",
        "published": "2007-12-23T15:24:35Z",
        "link": "http://arxiv.org/abs/0712.3925v1",
        "categories": [
            "cs.SE",
            "cs.DB",
            "quant-ph"
        ]
    },
    {
        "title": "Two-Level Concept-Oriented Data Model",
        "authors": [
            "Alexandr Savinov"
        ],
        "summary": "In this paper we describe a new approach to data modelling called the concept-oriented model (CoM). This model is based on the formalism of nested ordered sets which uses inclusion relation to produce hierarchical structure of sets and ordering relation to produce multi-dimensional structure among its elements. Nested ordered set is defined as an ordered set where an each element can be itself an ordered set. Ordering relation in CoM is used to define data semantics and operations with data such as projection and de-projection. This data model can be applied to very different problems and the paper describes some its uses such grouping with aggregation and multi-dimensional analysis.",
        "published": "2007-12-30T14:29:17Z",
        "link": "http://arxiv.org/abs/0801.0131v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Principles of the Concept-Oriented Data Model",
        "authors": [
            "Alexandr Savinov"
        ],
        "summary": "In the paper a new approach to data representation and manipulation is described, which is called the concept-oriented data model (CODM). It is supposed that items represent data units, which are stored in concepts. A concept is a combination of superconcepts, which determine the concept's dimensionality or properties. An item is a combination of superitems taken by one from all the superconcepts. An item stores a combination of references to its superitems. The references implement inclusion relation or attribute-value relation among items. A concept-oriented database is defined by its concept structure called syntax or schema and its item structure called semantics. The model defines formal transformations of syntax and semantics including the canonical semantics where all concepts are merged and the data semantics is represented by one set of items. The concept-oriented data model treats relations as subconcepts where items are instances of the relations. Multi-valued attributes are defined via subconcepts as a view on the database semantics rather than as a built-in mechanism. The model includes concept-oriented query language, which is based on collection manipulations. It also has such mechanisms as aggregation and inference based on semantics propagation through the database schema.",
        "published": "2007-12-30T15:04:25Z",
        "link": "http://arxiv.org/abs/0801.0139v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "On the use of self-organizing maps to accelerate vector quantization",
        "authors": [
            "Eric De Bodt",
            "Marie Cottrell",
            "Patrick Letrémy",
            "Michel Verleysen"
        ],
        "summary": "Self-organizing maps (SOM) are widely used for their topology preservation property: neighboring input vectors are quantified (or classified) either on the same location or on neighbor ones on a predefined grid. SOM are also widely used for their more classical vector quantization property. We show in this paper that using SOM instead of the more classical Simple Competitive Learning (SCL) algorithm drastically increases the speed of convergence of the vector quantization process. This fact is demonstrated through extensive simulations on artificial and real examples, with specific SOM (fixed and decreasing neighborhoods) and SCL algorithms.",
        "published": "2007-01-04T15:42:43Z",
        "link": "http://arxiv.org/abs/math/0701142v1",
        "categories": [
            "math.ST",
            "cs.NE",
            "stat.TH"
        ]
    },
    {
        "title": "Statistical tools to assess the reliability of self-organizing maps",
        "authors": [
            "Eric De Bodt",
            "Marie Cottrell",
            "Michel Verleysen"
        ],
        "summary": "Results of neural network learning are always subject to some variability, due to the sensitivity to initial conditions, to convergence to local minima, and, sometimes more dramatically, to sampling variability. This paper presents a set of tools designed to assess the reliability of the results of Self-Organizing Maps (SOM), i.e. to test on a statistical basis the confidence we can have on the result of a specific SOM. The tools concern the quantization error in a SOM, and the neighborhood relations (both at the level of a specific pair of observations and globally on the map). As a by-product, these measures also allow to assess the adequacy of the number of units chosen in a map. The tools may also be used to measure objectively how the SOM are less sensitive to non-linear optimization problems (local minima, convergence, etc.) than other neural network models.",
        "published": "2007-01-04T16:04:09Z",
        "link": "http://arxiv.org/abs/math/0701144v1",
        "categories": [
            "math.ST",
            "cs.NE",
            "stat.TH"
        ]
    },
    {
        "title": "Bootstrap for neural model selection",
        "authors": [
            "Riadh Kallel",
            "Marie Cottrell",
            "Vincent Vigneron"
        ],
        "summary": "Bootstrap techniques (also called resampling computation techniques) have introduced new advances in modeling and model evaluation. Using resampling methods to construct a series of new samples which are based on the original data set, allows to estimate the stability of the parameters. Properties such as convergence and asymptotic normality can be checked for any particular observed data set. In most cases, the statistics computed on the generated data sets give a good idea of the confidence regions of the estimates. In this paper, we debate on the contribution of such methods for model selection, in the case of feedforward neural networks. The method is described and compared with the leave-one-out resampling method. The effectiveness of the bootstrap method, versus the leave-one-out methode, is checked through a number of examples.",
        "published": "2007-01-04T16:18:33Z",
        "link": "http://arxiv.org/abs/math/0701145v1",
        "categories": [
            "math.ST",
            "cs.NE",
            "stat.TH"
        ]
    },
    {
        "title": "Missing values : processing with the Kohonen algorithm",
        "authors": [
            "Marie Cottrell",
            "Patrick Letrémy"
        ],
        "summary": "The processing of data which contain missing values is a complicated and always awkward problem, when the data come from real-world contexts. In applications, we are very often in front of observations for which all the values are not available, and this can occur for many reasons: typing errors, fields left unanswered in surveys, etc. Most of the statistical software (as SAS for example) simply suppresses incomplete observations. It has no practical consequence when the data are very numerous. But if the number of remaining data is too small, it can remove all significance to the results. To avoid suppressing data in that way, it is possible to replace a missing value with the mean value of the corresponding variable, but this approximation can be very bad when the variable has a large variance. So it is very worthwhile seeing that the Kohonen algorithm (as well as the Forgy algorithm) perfectly deals with data with missing values, without having to estimate them beforehand. We are particularly interested in the Kohonen algorithm for its visualization properties.",
        "published": "2007-01-05T06:06:02Z",
        "link": "http://arxiv.org/abs/math/0701152v1",
        "categories": [
            "math.ST",
            "cs.NE",
            "stat.TH"
        ]
    },
    {
        "title": "Browser-based distributed evolutionary computation: performance and   scaling behavior",
        "authors": [
            "J. J. Merelo",
            "Antonio Mora-Garcia",
            "J. L. J. Laredo",
            "Juan Lupion",
            "Fernando Tricas"
        ],
        "summary": "The challenge of ad-hoc computing is to find the way of taking advantage of spare cycles in an efficient way that takes into account all capabilities of the devices and interconnections available to them. In this paper we explore distributed evolutionary computation based on the Ruby on Rails framework, which overlays a Model-View-Controller on evolutionary computation. It allows anybody with a web browser (that is, mostly everybody connected to the Internet) to participate in an evolutionary computation experiment. Using a straightforward farming model, we consider different factors, such as the size of the population used. We are mostly interested in how they impact on performance, but also the scaling behavior when a non-trivial number of computers is applied to the problem. Experiments show the impact of different packet sizes on performance, as well as a quite limited scaling behavior, due to the characteristics of the server. Several solutions for that problem are proposed.",
        "published": "2007-01-18T09:23:29Z",
        "link": "http://arxiv.org/abs/cs/0701115v1",
        "categories": [
            "cs.DC",
            "cs.NE"
        ]
    },
    {
        "title": "Bistability: a common feature in some \"aggregates\" of logistic maps",
        "authors": [
            "Ricardo Lopez-Ruiz",
            "Daniele Fournier-Prunaret"
        ],
        "summary": "As it was argued by Anderson [Science 177, 393 (1972)], the \"reductionist\" hypothesis does not by any means imply a \"constructionist\" one. Hence, in general, the behavior of large and complex aggregates of elementary components can not be understood nor extrapolated from the properties of a few components. Following this insight, we have simulated different \"aggregates\" of logistic maps according to a particular coupling scheme. All these aggregates show a similar pattern of dynamical properties, concretely a bistable behavior, that is also found in a network of many units of the same type, independently of the number of components and of the interconnection topology. A qualitative relationship with brain-like systems is suggested.",
        "published": "2007-02-01T13:56:58Z",
        "link": "http://arxiv.org/abs/nlin/0702001v1",
        "categories": [
            "nlin.AO",
            "cs.NE"
        ]
    },
    {
        "title": "Genetic Representations for Evolutionary Minimization of Network Coding   Resources",
        "authors": [
            "Minkyu Kim",
            "Varun Aggarwal",
            "Una-May O'Reilly",
            "Muriel Medard",
            "Wonsik Kim"
        ],
        "summary": "We demonstrate how a genetic algorithm solves the problem of minimizing the resources used for network coding, subject to a throughput constraint, in a multicast scenario. A genetic algorithm avoids the computational complexity that makes the problem NP-hard and, for our experiments, greatly improves on sub-optimal solutions of established methods. We compare two different genotype encodings, which tradeoff search space size with fitness landscape, as well as the associated genetic operators. Our finding favors a smaller encoding despite its fewer intermediate solutions and demonstrates the impact of the modularity enforced by genetic operators on the performance of the algorithm.",
        "published": "2007-02-07T05:54:00Z",
        "link": "http://arxiv.org/abs/cs/0702038v1",
        "categories": [
            "cs.NE",
            "cs.NI"
        ]
    },
    {
        "title": "On the possibility of making the complete computer model of a human   brain",
        "authors": [
            "A. V. Paraskevov"
        ],
        "summary": "The development of the algorithm of a neural network building by the corresponding parts of a DNA code is discussed.",
        "published": "2007-02-09T13:16:14Z",
        "link": "http://arxiv.org/abs/cs/0702055v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Overcoming Hierarchical Difficulty by Hill-Climbing the Building Block   Structure",
        "authors": [
            "David Iclanzan",
            "Dan Dumitrescu"
        ],
        "summary": "The Building Block Hypothesis suggests that Genetic Algorithms (GAs) are well-suited for hierarchical problems, where efficient solving requires proper problem decomposition and assembly of solution from sub-solution with strong non-linear interdependencies. The paper proposes a hill-climber operating over the building block (BB) space that can efficiently address hierarchical problems. The new Building Block Hill-Climber (BBHC) uses past hill-climb experience to extract BB information and adapts its neighborhood structure accordingly. The perpetual adaptation of the neighborhood structure allows the method to climb the hierarchical structure solving successively the hierarchical levels. It is expected that for fully non deceptive hierarchical BB structures the BBHC can solve hierarchical problems in linearithmic time. Empirical results confirm that the proposed method scales almost linearly with the problem size thus clearly outperforms population based recombinative methods.",
        "published": "2007-02-16T21:47:04Z",
        "link": "http://arxiv.org/abs/cs/0702096v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "G.1.6; I.2.8"
        ]
    },
    {
        "title": "Linking Microscopic and Macroscopic Models for Evolution: Markov Chain   Network Training and Conservation Law Approximations",
        "authors": [
            "Roderick V. N. Melnik"
        ],
        "summary": "In this paper, a general framework for the analysis of a connection between the training of artificial neural networks via the dynamics of Markov chains and the approximation of conservation law equations is proposed. This framework allows us to demonstrate an intrinsic link between microscopic and macroscopic models for evolution via the concept of perturbed generalized dynamic systems. The main result is exemplified with a number of illustrative examples where efficient numerical approximations follow directly from network-based computational models, viewed here as Markov chain approximations. Finally, stability and consistency conditions of such computational models are discussed.",
        "published": "2007-02-25T11:19:25Z",
        "link": "http://arxiv.org/abs/cs/0702148v1",
        "categories": [
            "cs.CE",
            "cs.IT",
            "cs.NA",
            "cs.NE",
            "math.IT"
        ]
    },
    {
        "title": "Consumer Profile Identification and Allocation",
        "authors": [
            "Patrick Letrémy",
            "Marie Cottrell",
            "Eric Esposito",
            "Valérie Laffite",
            "Sally Showk"
        ],
        "summary": "We propose an easy-to-use methodology to allocate one of the groups which have been previously built from a complete learning data base, to new individuals. The learning data base contains continuous and categorical variables for each individual. The groups (clusters) are built by using only the continuous variables and described with the help of the categorical ones. For the new individuals, only the categorical variables are available, and it is necessary to define a model which computes the probabilities to belong to each of the clusters, by using only the categorical variables. Then this model provides a decision rule to assign the new individuals and gives an efficient tool to decision-makers. This tool is shown to be very efficient for customers allocation in consumer clusters for marketing purposes, for example.",
        "published": "2007-02-28T08:02:35Z",
        "link": "http://arxiv.org/abs/math/0702866v2",
        "categories": [
            "math.ST",
            "cs.NE",
            "stat.TH"
        ]
    },
    {
        "title": "Integral Biomathics: A Post-Newtonian View into the Logos of Bios (On   the New Meaning, Relations and Principles of Life in Science)",
        "authors": [
            "Plamen L. Simeonov"
        ],
        "summary": "This work is an attempt for a state-of-the-art survey of natural and life sciences with the goal to define the scope and address the central questions of an original research program. It is focused on the phenomena of emergence, adaptive dynamics and evolution of self-assembling, self-organizing, self-maintaining and self-replicating biosynthetic systems viewed from a newly-arranged perspective and understanding of computation and communication in the living nature.",
        "published": "2007-02-28T22:52:43Z",
        "link": "http://arxiv.org/abs/cs/0703002v9",
        "categories": [
            "cs.NE",
            "cs.CC",
            "F.1.1; F.4.0; H.1.1; I.2.0; J.3"
        ]
    },
    {
        "title": "Competition of Self-Organized Rotating Spiral Autowaves in a   Nonequilibrium Dissipative System of Three-Level Phaser",
        "authors": [
            "D. N. Makovetskii"
        ],
        "summary": "We present results of cellular automata based investigations of rotating spiral autowaves in a nonequilibrium excitable medium which models three-level paramagnetic microwave phonon laser (phaser). The computational model is described in arXiv:cond-mat/0410460v2 and arXiv:cond-mat/0602345v1 . We have observed several new scenarios of self-organization, competition and dynamical stabilization of rotating spiral autowaves under conditions of cross-relaxation between three-level active centers. In particular, phenomena of inversion of topological charge, as well as processes of regeneration and replication of rotating spiral autowaves in various excitable media were revealed and visualized for mesoscopic-scale areas of phaser-type active systems, which model real phaser devices.",
        "published": "2007-03-26T07:03:21Z",
        "link": "http://arxiv.org/abs/nlin/0703050v2",
        "categories": [
            "nlin.CG",
            "cs.NE",
            "nlin.AO"
        ]
    },
    {
        "title": "Automatic Generation of Benchmarks for Plagiarism Detection Tools using   Grammatical Evolution",
        "authors": [
            "Manuel Cebrian",
            "Manuel Alfonseca",
            "Alfonso Ortega"
        ],
        "summary": "This paper has been withdrawn by the authors due to a major rewriting.",
        "published": "2007-03-27T16:57:39Z",
        "link": "http://arxiv.org/abs/cs/0703134v5",
        "categories": [
            "cs.NE",
            "cs.IT",
            "math.IT",
            "J.1; I.2.2; D.2.8"
        ]
    },
    {
        "title": "Exploring Logic Artificial Chemistries: An Illogical Attempt?",
        "authors": [
            "Christof Teuscher"
        ],
        "summary": "Robustness to a wide variety of negative factors and the ability to self-repair is an inherent and natural characteristic of all life forms on earth. As opposed to nature, man-made systems are in most cases not inherently robust and a significant effort has to be made in order to make them resistant against failures. This can be done in a wide variety of ways and on various system levels. In the field of digital systems, for example, techniques such as triple modular redundancy (TMR) are frequently used, which results in a considerable hardware overhead. Biologically-inspired computing by means of bio-chemical metaphors offers alternative paradigms, which need to be explored and evaluated.   Here, we are interested to evaluate the potential of nature-inspired artificial chemistries and membrane systems as an alternative information representing and processing paradigm in order to obtain robust and spatially extended Boolean computing systems in a distributed environment. We investigate conceptual approaches inspired by artificial chemistries and membrane systems and compare proof-of-concepts. First, we show, that elementary logical functions can be implemented. Second, we illustrate how they can be made more robust and how they can be assembled to larger-scale systems. Finally, we discuss the implications for and paths to possible genuine implementations. Compared to the main body of work in artificial chemistries, we take a very pragmatic and implementation-oriented approach and are interested in realizing Boolean computations only. The results emphasize that artificial chemistries can be used to implement Boolean logic in a spatially extended and distributed environment and can also be made highly robust, but at a significant price.",
        "published": "2007-03-29T20:57:00Z",
        "link": "http://arxiv.org/abs/cs/0703149v1",
        "categories": [
            "cs.NE",
            "nlin.AO"
        ]
    },
    {
        "title": "Intelligent location of simultaneously active acoustic emission sources:   Part I",
        "authors": [
            "T. Kosel",
            "I. Grabec"
        ],
        "summary": "The intelligent acoustic emission locator is described in Part I, while Part II discusses blind source separation, time delay estimation and location of two simultaneously active continuous acoustic emission sources.   The location of acoustic emission on complicated aircraft frame structures is a difficult problem of non-destructive testing. This article describes an intelligent acoustic emission source locator. The intelligent locator comprises a sensor antenna and a general regression neural network, which solves the location problem based on learning from examples. Locator performance was tested on different test specimens. Tests have shown that the accuracy of location depends on sound velocity and attenuation in the specimen, the dimensions of the tested area, and the properties of stored data. The location accuracy achieved by the intelligent locator is comparable to that obtained by the conventional triangulation method, while the applicability of the intelligent locator is more general since analysis of sonic ray paths is avoided. This is a promising method for non-destructive testing of aircraft frame structures by the acoustic emission method.",
        "published": "2007-04-01T13:06:50Z",
        "link": "http://arxiv.org/abs/0704.0047v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Intelligent location of simultaneously active acoustic emission sources:   Part II",
        "authors": [
            "T. Kosel",
            "I. Grabec"
        ],
        "summary": "Part I describes an intelligent acoustic emission locator, while Part II discusses blind source separation, time delay estimation and location of two continuous acoustic emission sources.   Acoustic emission (AE) analysis is used for characterization and location of developing defects in materials. AE sources often generate a mixture of various statistically independent signals. A difficult problem of AE analysis is separation and characterization of signal components when the signals from various sources and the mode of mixing are unknown. Recently, blind source separation (BSS) by independent component analysis (ICA) has been used to solve these problems. The purpose of this paper is to demonstrate the applicability of ICA to locate two independent simultaneously active acoustic emission sources on an aluminum band specimen. The method is promising for non-destructive testing of aircraft frame structures by acoustic emission analysis.",
        "published": "2007-04-01T18:53:13Z",
        "link": "http://arxiv.org/abs/0704.0050v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Architecture for Pseudo Acausal Evolvable Embedded Systems",
        "authors": [
            "Mohd Abubakr",
            "R. M. Vinay"
        ],
        "summary": "Advances in semiconductor technology are contributing to the increasing complexity in the design of embedded systems. Architectures with novel techniques such as evolvable nature and autonomous behavior have engrossed lot of attention. This paper demonstrates conceptually evolvable embedded systems can be characterized basing on acausal nature. It is noted that in acausal systems, future input needs to be known, here we make a mechanism such that the system predicts the future inputs and exhibits pseudo acausal nature. An embedded system that uses theoretical framework of acausality is proposed. Our method aims at a novel architecture that features the hardware evolability and autonomous behavior alongside pseudo acausality. Various aspects of this architecture are discussed in detail along with the limitations.",
        "published": "2007-04-07T13:40:49Z",
        "link": "http://arxiv.org/abs/0704.0985v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A neural network approach to ordinal regression",
        "authors": [
            "Jianlin Cheng"
        ],
        "summary": "Ordinal regression is an important type of learning, which has properties of both classification and regression. Here we describe a simple and effective approach to adapt a traditional neural network to learn ordinal categories. Our approach is a generalization of the perceptron method for ordinal regression. On several benchmark datasets, our method (NNRank) outperforms a neural network classification method. Compared with the ordinal regression methods using Gaussian processes and support vector machines, NNRank achieves comparable performance. Moreover, NNRank has the advantages of traditional neural networks: learning in both online and batch modes, handling very large training datasets, and making rapid predictions. These features make NNRank a useful and complementary tool for large-scale data processing tasks such as information retrieval, web page ranking, collaborative filtering, and protein ranking in Bioinformatics.",
        "published": "2007-04-08T17:36:00Z",
        "link": "http://arxiv.org/abs/0704.1028v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Novel algorithm to calculate hypervolume indicator of Pareto   approximation set",
        "authors": [
            "Qing Yang",
            "Shengchao Ding"
        ],
        "summary": "Hypervolume indicator is a commonly accepted quality measure for comparing Pareto approximation set generated by multi-objective optimizers. The best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $O(n^{d/2})$ with special data structures. This paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. It splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. In special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. The complexity analysis shows that the proposed algorithm achieves $O((\\frac{d}{2})^n)$ time and $O(dn^2)$ space complexity in the worst case.",
        "published": "2007-04-10T07:21:02Z",
        "link": "http://arxiv.org/abs/0704.1196v1",
        "categories": [
            "cs.CG",
            "cs.NE"
        ]
    },
    {
        "title": "A Doubly Distributed Genetic Algorithm for Network Coding",
        "authors": [
            "Minkyu Kim",
            "Varun Aggarwal",
            "Una-May O'Reilly",
            "Muriel Medard"
        ],
        "summary": "We present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. Our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. This genotype distribution is shown to offer a significant gain in running time. Then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. This temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.",
        "published": "2007-04-10T13:36:44Z",
        "link": "http://arxiv.org/abs/0704.1198v1",
        "categories": [
            "cs.NE",
            "cs.NI"
        ]
    },
    {
        "title": "Traitement Des Donnees Manquantes Au Moyen De L'Algorithme De Kohonen",
        "authors": [
            "Marie Cottrell",
            "Smail Ibbou",
            "Patrick Letrémy"
        ],
        "summary": "Nous montrons comment il est possible d'utiliser l'algorithme d'auto organisation de Kohonen pour traiter des donn\\'ees avec valeurs manquantes et estimer ces derni\\`eres. Apr\\`es un rappel m\\'ethodologique, nous illustrons notre propos \\`a partir de trois applications \\`a des donn\\'ees r\\'eelles.   -----   We show how it is possible to use the Kohonen self-organizing algorithm to deal with data which contain missing values and to estimate them. After a methodological recall, we illustrate our purpose from three real databases applications.",
        "published": "2007-04-13T07:33:15Z",
        "link": "http://arxiv.org/abs/0704.1709v1",
        "categories": [
            "stat.AP",
            "cs.NE"
        ]
    },
    {
        "title": "Exploiting Heavy Tails in Training Times of Multilayer Perceptrons: A   Case Study with the UCI Thyroid Disease Database",
        "authors": [
            "Manuel Cebrian",
            "Ivan Cantador"
        ],
        "summary": "The random initialization of weights of a multilayer perceptron makes it possible to model its training process as a Las Vegas algorithm, i.e. a randomized algorithm which stops when some required training error is obtained, and whose execution time is a random variable. This modeling is used to perform a case study on a well-known pattern recognition benchmark: the UCI Thyroid Disease Database. Empirical evidence is presented of the training time probability distribution exhibiting a heavy tail behavior, meaning a big probability mass of long executions. This fact is exploited to reduce the training time cost by applying two simple restart strategies. The first assumes full knowledge of the distribution yielding a 40% cut down in expected time with respect to the training without restarts. The second, assumes null knowledge, yielding a reduction ranging from 9% to 23%.",
        "published": "2007-04-20T15:58:04Z",
        "link": "http://arxiv.org/abs/0704.2725v2",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "2D Path Solutions from a Single Layer Excitable CNN Model",
        "authors": [
            "Koray Karahaliloglu"
        ],
        "summary": "An easily implementable path solution algorithm for 2D spatial problems, based on excitable/programmable characteristics of a specific cellular nonlinear network (CNN) model is presented and numerically investigated. The network is a single layer bioinspired model which was also implemented in CMOS technology. It exhibits excitable characteristics with regionally bistable cells. The related response realizes propagations of trigger autowaves, where the excitable mode can be globally preset and reset. It is shown that, obstacle distributions in 2D space can also be directly mapped onto the coupled cell array in the network. Combining these two features, the network model can serve as the main block in a 2D path computing processor. The related algorithm and configurations are numerically experimented with circuit level parameters and performance estimations are also presented. The simplicity of the model also allows alternative technology and device level implementation, which may become critical in autonomous processor design of related micro or nanoscale robotic applications.",
        "published": "2007-04-24T20:20:46Z",
        "link": "http://arxiv.org/abs/0704.3268v1",
        "categories": [
            "cs.RO",
            "cs.NE"
        ]
    },
    {
        "title": "Stochastic Optimization Algorithms",
        "authors": [
            "Pierre Collet",
            "Jean-Philippe Rennard"
        ],
        "summary": "When looking for a solution, deterministic methods have the enormous advantage that they do find global optima. Unfortunately, they are very CPU-intensive, and are useless on untractable NP-hard problems that would require thousands of years for cutting-edge computers to explore. In order to get a result, one needs to revert to stochastic algorithms, that sample the search space without exploring it thoroughly. Such algorithms can find very good results, without any guarantee that the global optimum has been reached; but there is often no other choice than using them. This chapter is a short introduction to the main methods used in stochastic optimization.",
        "published": "2007-04-28T06:52:19Z",
        "link": "http://arxiv.org/abs/0704.3780v1",
        "categories": [
            "cs.NE",
            "G.1.6"
        ]
    },
    {
        "title": "The Parameter-Less Self-Organizing Map algorithm",
        "authors": [
            "Erik Berglund",
            "Joaquin Sitte"
        ],
        "summary": "The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network algorithm based on the Self-Organizing Map (SOM). It eliminates the need for a learning rate and annealing schemes for learning rate and neighbourhood size. We discuss the relative performance of the PLSOM and the SOM and demonstrate some tasks in which the SOM fails but the PLSOM performs satisfactory. Finally we discuss some example applications of the PLSOM and present a proof of ordering under certain limited conditions.",
        "published": "2007-05-02T04:04:51Z",
        "link": "http://arxiv.org/abs/0705.0199v2",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CV"
        ]
    },
    {
        "title": "Risk Assessment Algorithms Based On Recursive Neural Networks",
        "authors": [
            "Alejandro Chinea Manrique De Lara",
            "Michel Parent"
        ],
        "summary": "The assessment of highly-risky situations at road intersections have been recently revealed as an important research topic within the context of the automotive industry. In this paper we shall introduce a novel approach to compute risk functions by using a combination of a highly non-linear processing model in conjunction with a powerful information encoding procedure. Specifically, the elements of information either static or dynamic that appear in a road intersection scene are encoded by using directed positional acyclic labeled graphs. The risk assessment problem is then reformulated in terms of an inductive learning task carried out by a recursive neural network. Recursive neural networks are connectionist models capable of solving supervised and non-supervised learning problems represented by directed ordered acyclic graphs. The potential of this novel approach is demonstrated through well predefined scenarios. The major difference of our approach compared to others is expressed by the fact of learning the structure of the risk. Furthermore, the combination of a rich information encoding procedure with a generalized model of dynamical recurrent networks permit us, as we shall demonstrate, a sophisticated processing of information that we believe as being a first step for building future advanced intersection safety systems",
        "published": "2007-05-04T11:53:35Z",
        "link": "http://arxiv.org/abs/0705.0602v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Control of Complex Systems Using Bayesian Networks and Genetic Algorithm",
        "authors": [
            "Tshilidzi Marwala"
        ],
        "summary": "A method based on Bayesian neural networks and genetic algorithm is proposed to control the fermentation process. The relationship between input and output variables is modelled using Bayesian neural network that is trained using hybrid Monte Carlo method. A feedback loop based on genetic algorithm is used to change input variables so that the output variables are as close to the desired target as possible without the loss of confidence level on the prediction that the neural network gives. The proposed procedure is found to reduce the distance between the desired target and measured outputs significantly.",
        "published": "2007-05-09T07:08:58Z",
        "link": "http://arxiv.org/abs/0705.1214v1",
        "categories": [
            "cs.CE",
            "cs.NE"
        ]
    },
    {
        "title": "Actin - Technical Report",
        "authors": [
            "Raihan H. Kibria"
        ],
        "summary": "The Boolean satisfiability problem (SAT) can be solved efficiently with variants of the DPLL algorithm. For industrial SAT problems, DPLL with conflict analysis dependent dynamic decision heuristics has proved to be particularly efficient, e.g. in Chaff. In this work, algorithms that initialize the variable activity values in the solver MiniSAT v1.14 by analyzing the CNF are evolved using genetic programming (GP), with the goal to reduce the total number of conflicts of the search and the solving time. The effect of using initial activities other than zero is examined by initializing with random numbers. The possibility of countering the detrimental effects of reordering the CNF with improved initialization is investigated. The best result found (with validation testing on further problems) was used in the solver Actin, which was submitted to SAT-Race 2006.",
        "published": "2007-05-10T14:10:08Z",
        "link": "http://arxiv.org/abs/0705.1481v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Using artificial intelligence for data reduction in mechanical   engineering",
        "authors": [
            "L. Mdlazi",
            "C. J. Stander",
            "P. S. Heyns",
            "T. Marwala"
        ],
        "summary": "In this paper artificial neural networks and support vector machines are used to reduce the amount of vibration data that is required to estimate the Time Domain Average of a gear vibration signal. Two models for estimating the time domain average of a gear vibration signal are proposed. The models are tested on data from an accelerated gear life test rig. Experimental results indicate that the required data for calculating the Time Domain Average of a gear vibration signal can be reduced by up to 75% when the proposed models are implemented.",
        "published": "2007-05-11T15:49:40Z",
        "link": "http://arxiv.org/abs/0705.1673v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Option Pricing Using Bayesian Neural Networks",
        "authors": [
            "Michael Maio Pires",
            "Tshilidzi Marwala"
        ],
        "summary": "Options have provided a field of much study because of the complexity involved in pricing them. The Black-Scholes equations were developed to price options but they are only valid for European styled options. There is added complexity when trying to price American styled options and this is why the use of neural networks has been proposed. Neural Networks are able to predict outcomes based on past data. The inputs to the networks here are stock volatility, strike price and time to maturity with the output of the network being the call option price. There are two techniques for Bayesian neural networks used. One is Automatic Relevance Determination (for Gaussian Approximation) and one is a Hybrid Monte Carlo method, both used with Multi-Layer Perceptrons.",
        "published": "2007-05-11T15:55:31Z",
        "link": "http://arxiv.org/abs/0705.1680v1",
        "categories": [
            "cs.CE",
            "cs.NE"
        ]
    },
    {
        "title": "Dynamic Model Updating Using Particle Swarm Optimization Method",
        "authors": [
            "Tshilidzi Marwala"
        ],
        "summary": "This paper proposes the use of particle swarm optimization method (PSO) for finite element (FE) model updating. The PSO method is compared to the existing methods that use simulated annealing (SA) or genetic algorithms (GA) for FE model for model updating. The proposed method is tested on an unsymmetrical H-shaped structure. It is observed that the proposed method gives updated natural frequencies the most accurate and followed by those given by an updated model that was obtained using the GA and a full FE model. It is also observed that the proposed method gives updated mode shapes that are best correlated to the measured ones, followed by those given by an updated model that was obtained using the SA and a full FE model. Furthermore, it is observed that the PSO achieves this accuracy at a computational speed that is faster than that by the GA and a full FE model which is faster than the SA and a full FE model.",
        "published": "2007-05-12T10:27:07Z",
        "link": "http://arxiv.org/abs/0705.1760v1",
        "categories": [
            "cs.CE",
            "cs.NE"
        ]
    },
    {
        "title": "Fuzzy and Multilayer Perceptron for Evaluation of HV Bushings",
        "authors": [
            "Sizwe M. Dhlamini",
            "Tshilidzi Marwala",
            "Thokozani Majozi"
        ],
        "summary": "The work proposes the application of fuzzy set theory (FST) to diagnose the condition of high voltage bushings. The diagnosis uses dissolved gas analysis (DGA) data from bushings based on IEC60599 and IEEE C57-104 criteria for oil impregnated paper (OIP) bushings. FST and neural networks are compared in terms of accuracy and computational efficiency. Both FST and NN simulations were able to diagnose the bushings condition with 10% error. By using fuzzy theory, the maintenance department can classify bushings and know the extent of degradation in the component.",
        "published": "2007-05-16T09:06:19Z",
        "link": "http://arxiv.org/abs/0705.2305v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "A Study in a Hybrid Centralised-Swarm Agent Community",
        "authors": [
            "Bradley van Aardt",
            "Tshilidzi Marwala"
        ],
        "summary": "This paper describes a systems architecture for a hybrid Centralised/Swarm based multi-agent system. The issue of local goal assignment for agents is investigated through the use of a global agent which teaches the agents responses to given situations. We implement a test problem in the form of a Pursuit game, where the Multi-Agent system is a set of captor agents. The agents learn solutions to certain board positions from the global agent if they are unable to find a solution. The captor agents learn through the use of multi-layer perceptron neural networks. The global agent is able to solve board positions through the use of a Genetic Algorithm. The cooperation between agents and the results of the simulation are discussed here. .",
        "published": "2007-05-16T09:12:09Z",
        "link": "http://arxiv.org/abs/0705.2307v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Using Genetic Algorithms to Optimise Rough Set Partition Sizes for HIV   Data Analysis",
        "authors": [
            "Bodie Crossingham",
            "Tshilidzi Marwala"
        ],
        "summary": "In this paper, we present a method to optimise rough set partition sizes, to which rule extraction is performed on HIV data. The genetic algorithm optimisation technique is used to determine the partition sizes of a rough set in order to maximise the rough sets prediction accuracy. The proposed method is tested on a set of demographic properties of individuals obtained from the South African antenatal survey. Six demographic variables were used in the analysis, these variables are; race, age of mother, education, gravidity, parity, and age of father, with the outcome or decision being either HIV positive or negative. Rough set theory is chosen based on the fact that it is easy to interpret the extracted rules. The prediction accuracy of equal width bin partitioning is 57.7% while the accuracy achieved after optimising the partitions is 72.8%. Several other methods have been used to analyse the HIV data and their results are stated and compared to that of rough set theory (RST).",
        "published": "2007-05-17T07:02:23Z",
        "link": "http://arxiv.org/abs/0705.2485v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "q-bio.QM"
        ]
    },
    {
        "title": "Condition Monitoring of HV Bushings in the Presence of Missing Data   Using Evolutionary Computing",
        "authors": [
            "Sizwe M. Dhlamini*",
            "Fulufhelo V. Nelwamondo**",
            "Tshilidzi Marwala**"
        ],
        "summary": "The work proposes the application of neural networks with particle swarm optimisation (PSO) and genetic algorithms (GA) to compensate for missing data in classifying high voltage bushings. The classification is done using DGA data from 60966 bushings based on IEEEc57.104, IEC599 and IEEE production rates methods for oil impregnated paper (OIP) bushings. PSO and GA were compared in terms of accuracy and computational efficiency. Both GA and PSO simulations were able to estimate missing data values to an average 95% accuracy when only one variable was missing. However PSO rapidly deteriorated to 66% accuracy with two variables missing simultaneously, compared to 84% for GA. The data estimated using GA was found to classify the conditions of bushings than the PSO.",
        "published": "2007-05-17T11:33:34Z",
        "link": "http://arxiv.org/abs/0705.2516v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Structural Health Monitoring Using Neural Network Based Vibrational   System Identification",
        "authors": [
            "Donald A. Sofge"
        ],
        "summary": "Composite fabrication technologies now provide the means for producing high-strength, low-weight panels, plates, spars and other structural components which use embedded fiber optic sensors and piezoelectric transducers. These materials, often referred to as smart structures, make it possible to sense internal characteristics, such as delaminations or structural degradation. In this effort we use neural network based techniques for modeling and analyzing dynamic structural information for recognizing structural defects. This yields an adaptable system which gives a measure of structural integrity for composite structures.",
        "published": "2007-05-24T21:48:18Z",
        "link": "http://arxiv.org/abs/0705.3669v1",
        "categories": [
            "cs.NE",
            "cs.CV",
            "cs.SD"
        ]
    },
    {
        "title": "On complexity of optimized crossover for binary representations",
        "authors": [
            "Anton Eremeev"
        ],
        "summary": "We consider the computational complexity of producing the best possible offspring in a crossover, given two solutions of the parents. The crossover operators are studied on the class of Boolean linear programming problems, where the Boolean vector of variables is used as the solution representation. By means of efficient reductions of the optimized gene transmitting crossover problems (OGTC) we show the polynomial solvability of the OGTC for the maximum weight set packing problem, the minimum weight set partition problem and for one of the versions of the simple plant location problem. We study a connection between the OGTC for linear Boolean programming problem and the maximum weight independent set problem on 2-colorable hypergraph and prove the NP-hardness of several special cases of the OGTC problem in Boolean linear programming.",
        "published": "2007-05-25T13:07:18Z",
        "link": "http://arxiv.org/abs/0705.3766v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "Challenges and Opportunities of Evolutionary Robotics",
        "authors": [
            "D. A. Sofge",
            "M. A. Potter",
            "M. D. Bugajska",
            "A. C. Schultz"
        ],
        "summary": "Robotic hardware designs are becoming more complex as the variety and number of on-board sensors increase and as greater computational power is provided in ever-smaller packages on-board robots. These advances in hardware, however, do not automatically translate into better software for controlling complex robots. Evolutionary techniques hold the potential to solve many difficult problems in robotics which defy simple conventional approaches, but present many challenges as well. Numerous disciplines including artificial life, cognitive science and neural networks, rule-based systems, behavior-based control, genetic algorithms and other forms of evolutionary computation have contributed to shaping the current state of evolutionary robotics. This paper provides an overview of developments in the emerging field of evolutionary robotics, and discusses some of the opportunities and challenges which currently face practitioners in the field.",
        "published": "2007-06-04T16:08:22Z",
        "link": "http://arxiv.org/abs/0706.0457v1",
        "categories": [
            "cs.NE",
            "cs.RO"
        ]
    },
    {
        "title": "Improved Neural Modeling of Real-World Systems Using Genetic Algorithm   Based Variable Selection",
        "authors": [
            "Donald A. Sofge",
            "David L. Elliott"
        ],
        "summary": "Neural network models of real-world systems, such as industrial processes, made from sensor data must often rely on incomplete data. System states may not all be known, sensor data may be biased or noisy, and it is not often known which sensor data may be useful for predictive modelling. Genetic algorithms may be used to help to address this problem by determining the near optimal subset of sensor variables most appropriate to produce good models. This paper describes the use of genetic search to optimize variable selection to determine inputs into the neural network model. We discuss genetic algorithm implementation issues including data representation types and genetic operators such as crossover and mutation. We present the use of this technique for neural network modelling of a typical industrial application, a liquid fed ceramic melter, and detail the results of the genetic search to optimize the neural network model for this application.",
        "published": "2007-06-07T18:13:59Z",
        "link": "http://arxiv.org/abs/0706.1051v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Design, Implementation, and Cooperative Coevolution of an Autonomous/   Teleoperated Control System for a Serpentine Robotic Manipulator",
        "authors": [
            "Donald Sofge",
            "Gerald Chiang"
        ],
        "summary": "Design, implementation, and machine learning issues associated with developing a control system for a serpentine robotic manipulator are explored. The controller developed provides autonomous control of the serpentine robotic manipulatorduring operation of the manipulator within an enclosed environment such as an underground storage tank. The controller algorithms make use of both low-level joint angle control employing force/position feedback constraints, and high-level coordinated control of end-effector positioning. This approach has resulted in both high-level full robotic control and low-level telerobotic control modes, and provides a high level of dexterity for the operator.",
        "published": "2007-06-07T19:27:12Z",
        "link": "http://arxiv.org/abs/0706.1061v1",
        "categories": [
            "cs.NE",
            "cs.RO"
        ]
    },
    {
        "title": "Evolutionary Mesh Numbering: Preliminary Results",
        "authors": [
            "Francis Sourd",
            "Marc Schoenauer"
        ],
        "summary": "Mesh numbering is a critical issue in Finite Element Methods, as the computational cost of one analysis is highly dependent on the order of the nodes of the mesh. This paper presents some preliminary investigations on the problem of mesh numbering using Evolutionary Algorithms. Three conclusions can be drawn from these experiments. First, the results of the up-to-date method used in all FEM softwares (Gibb's method) can be consistently improved; second, none of the crossover operators tried so far (either general or problem specific) proved useful; third, though the general tendency in Evolutionary Computation seems to be the hybridization with other methods (deterministic or heuristic), none of the presented attempt did encounter any success yet. The good news, however, is that this algorithm allows an improvement over the standard heuristic method between 12% and 20% for both the 1545 and 5453-nodes meshes used as test-bed. Finally, some strange interaction between the selection scheme and the use of problem specific mutation operator was observed, which appeals for further investigation.",
        "published": "2007-06-11T12:17:19Z",
        "link": "http://arxiv.org/abs/0706.1410v1",
        "categories": [
            "cs.NA",
            "cs.NE",
            "math.NA",
            "math.OC"
        ]
    },
    {
        "title": "From Royal Road to Epistatic Road for Variable Length Evolution   Algorithm",
        "authors": [
            "Michael Defoin Platel",
            "Sebastien Verel",
            "Manuel Clergue",
            "Philippe Collard"
        ],
        "summary": "Although there are some real world applications where the use of variable length representation (VLR) in Evolutionary Algorithm is natural and suitable, an academic framework is lacking for such representations. In this work we propose a family of tunable fitness landscapes based on VLR of genotypes. The fitness landscapes we propose possess a tunable degree of both neutrality and epistasis; they are inspired, on the one hand by the Royal Road fitness landscapes, and the other hand by the NK fitness landscapes. So these landscapes offer a scale of continuity from Royal Road functions, with neutrality and no epistasis, to landscapes with a large amount of epistasis and no redundancy. To gain insight into these fitness landscapes, we first use standard tools such as adaptive walks and correlation length. Second, we evaluate the performances of evolutionary algorithms on these landscapes for various values of the neutral and the epistatic parameters; the results allow us to correlate the performances with the expected degrees of neutrality and epistasis.",
        "published": "2007-07-04T06:57:52Z",
        "link": "http://arxiv.org/abs/0707.0548v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Where are Bottlenecks in NK Fitness Landscapes?",
        "authors": [
            "Sébastien Verel",
            "Philippe Collard",
            "Manuel Clergue"
        ],
        "summary": "Usually the offspring-parent fitness correlation is used to visualize and analyze some caracteristics of fitness landscapes such as evolvability. In this paper, we introduce a more general representation of this correlation, the Fitness Cloud (FC). We use the bottleneck metaphor to emphasise fitness levels in landscape that cause local search process to slow down. For a local search heuristic such as hill-climbing or simulated annealing, FC allows to visualize bottleneck and neutrality of landscapes. To confirm the relevance of the FC representation we show where the bottlenecks are in the well-know NK fitness landscape and also how to use neutrality information from the FC to combine some neutral operator with local search heuristic.",
        "published": "2007-07-04T15:30:54Z",
        "link": "http://arxiv.org/abs/0707.0641v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Scuba Search : when selection meets innovation",
        "authors": [
            "Sébastien Verel",
            "Philippe Collard",
            "Manuel Clergue"
        ],
        "summary": "We proposed a new search heuristic using the scuba diving metaphor. This approach is based on the concept of evolvability and tends to exploit neutrality in fitness landscape. Despite the fact that natural evolution does not directly select for evolvability, the basic idea behind the scuba search heuristic is to explicitly push the evolvability to increase. The search process switches between two phases: Conquest-of-the-Waters and Invasion-of-the-Land. A comparative study of the new algorithm and standard local search heuristics on the NKq-landscapes has shown advantage and limit of the scuba search. To enlighten qualitative differences between neutral search processes, the space is changed into a connected graph to visualize the pathways that the search is likely to follow.",
        "published": "2007-07-04T15:36:35Z",
        "link": "http://arxiv.org/abs/0707.0643v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "How to use the Scuba Diving metaphor to solve problem with neutrality ?",
        "authors": [
            "Philippe Collard",
            "Sébastien Verel",
            "Manuel Clergue"
        ],
        "summary": "We proposed a new search heuristic using the scuba diving metaphor. This approach is based on the concept of evolvability and tends to exploit neutrality which exists in many real-world problems. Despite the fact that natural evolution does not directly select for evolvability, the basic idea behind the scuba search heuristic is to explicitly push evolvability to increase. A comparative study of the scuba algorithm and standard local search heuristics has shown the advantage and the limitation of the scuba search. In order to tune neutrality, we use the NKq fitness landscapes and a family of travelling salesman problems (TSP) where cities are randomly placed on a lattice and where travel distance between cities is computed with the Manhattan metric. In this last problem the amount of neutrality varies with the city concentration on the grid ; assuming the concentration below one, this TSP reasonably remains a NP-hard problem.",
        "published": "2007-07-04T16:12:17Z",
        "link": "http://arxiv.org/abs/0707.0652v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Efficient supervised learning in networks with binary synapses",
        "authors": [
            "Carlo Baldassi",
            "Alfredo Braunstein",
            "Nicolas Brunel",
            "Riccardo Zecchina"
        ],
        "summary": "Recent experimental studies indicate that synaptic changes induced by neuronal activity are discrete jumps between a small number of stable states. Learning in systems with discrete synapses is known to be a computationally hard problem. Here, we study a neurobiologically plausible on-line learning algorithm that derives from Belief Propagation algorithms. We show that it performs remarkably well in a model neuron with binary synapses, and a finite number of `hidden' states per synapse, that has to learn a random classification task. Such system is able to learn a number of associations close to the theoretical limit, in time which is sublinear in system size. This is to our knowledge the first on-line algorithm that is able to achieve efficiently a finite number of patterns learned per binary synapse. Furthermore, we show that performance is optimal for a finite number of hidden states which becomes very small for sparse coding. The algorithm is similar to the standard `perceptron' learning algorithm, with an additional rule for synaptic transitions which occur only if a currently presented pattern is `barely correct'. In this case, the synaptic changes are meta-plastic only (change in hidden states and not in actual synaptic state), stabilizing the synapse in its current state. Finally, we show that a system with two visible states and K hidden states is much more robust to noise than a system with K visible states. We suggest this rule is sufficiently simple to be easily implemented by neurobiological systems or in hardware.",
        "published": "2007-07-09T16:23:55Z",
        "link": "http://arxiv.org/abs/0707.1295v1",
        "categories": [
            "q-bio.NC",
            "cond-mat.stat-mech",
            "cs.NE",
            "q-bio.QM"
        ]
    },
    {
        "title": "Optimal Design of Ad Hoc Injection Networks by Using Genetic Algorithms",
        "authors": [
            "Gregoire Danoy",
            "Pascal Bouvry",
            "Matthias R. Brust",
            "Enrique Alba"
        ],
        "summary": "This work aims at optimizing injection networks, which consist in adding a set of long-range links (called bypass links) in mobile multi-hop ad hoc networks so as to improve connectivity and overcome network partitioning. To this end, we rely on small-world network properties, that comprise a high clustering coefficient and a low characteristic path length. We investigate the use of two genetic algorithms (generational and steady-state) to optimize three instances of this topology control problem and present results that show initial evidence of their capacity to solve it.",
        "published": "2007-07-20T10:07:27Z",
        "link": "http://arxiv.org/abs/0707.3030v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.NI",
            "I.2.8"
        ]
    },
    {
        "title": "The universal evolutionary computer based on super-recursive algorithms   of evolvability",
        "authors": [
            "D. Roglic"
        ],
        "summary": "This work exposes which mechanisms and procesess in the Nature of evolution compute a function not computable by Turing machine. The computer with intelligence that is not higher than one bacteria population could have, but with efficency to solve the problems that are non-computable by Turing machine is represented. This theoretical construction is called Universal Evolutinary Computer and it is based on the superecursive algorithms of evolvability.",
        "published": "2007-07-24T18:50:22Z",
        "link": "http://arxiv.org/abs/0708.2686v1",
        "categories": [
            "cs.NE",
            "F.1.1; H.1.1"
        ]
    },
    {
        "title": "Clifford Algebra of the Vector Space of Conics for decision boundary   Hyperplanes in m-Euclidean Space",
        "authors": [
            "Isidro B. Nieto",
            "J. Refugio Vallejo"
        ],
        "summary": "In this paper we embed $m$-dimensional Euclidean space in the geometric algebra $Cl_m $ to extend the operators of incidence in ${R^m}$ to operators of incidence in the geometric algebra to generalize the notion of separator to a decision boundary hyperconic in the Clifford algebra of hyperconic sections denoted as ${Cl}({Co}_{2})$. This allows us to extend the concept of a linear perceptron or the spherical perceptron in conformal geometry and introduce the more general conic perceptron, namely the {elliptical perceptron}. Using Clifford duality a vector orthogonal to the decision boundary hyperplane is determined. Experimental results are shown in 2-dimensional Euclidean space where we separate data that are naturally separated by some typical plane conic separators by this procedure. This procedure is more general in the sense that it is independent of the dimension of the input data and hence we can speak of the hyperconic elliptic perceptron.",
        "published": "2007-07-26T18:03:23Z",
        "link": "http://arxiv.org/abs/0707.3979v1",
        "categories": [
            "cs.NE",
            "cs.CG",
            "I.5.1; I.5.2; I.2.6; I.3.5"
        ]
    },
    {
        "title": "One-way Hash Function Based on Neural Network",
        "authors": [
            "Shiguo Lian",
            "Jinsheng Sun",
            "Zhiquan Wang"
        ],
        "summary": "A hash function is constructed based on a three-layer neural network. The three neuron-layers are used to realize data confusion, diffusion and compression respectively, and the multi-block hash mode is presented to support the plaintext with variable length. Theoretical analysis and experimental results show that this hash function is one-way, with high key sensitivity and plaintext sensitivity, and secure against birthday attacks or meet-in-the-middle attacks. Additionally, the neural network's property makes it practical to realize in a parallel way. These properties make it a suitable choice for data signature or authentication.",
        "published": "2007-07-27T02:38:56Z",
        "link": "http://arxiv.org/abs/0707.4032v1",
        "categories": [
            "cs.CR",
            "cs.NE",
            "C.1.3; C.2.0; E.3.x; F.1.1"
        ]
    },
    {
        "title": "Image Authentication Based on Neural Networks",
        "authors": [
            "Shiguo Lian"
        ],
        "summary": "Neural network has been attracting more and more researchers since the past decades. The properties, such as parameter sensitivity, random similarity, learning ability, etc., make it suitable for information protection, such as data encryption, data authentication, intrusion detection, etc. In this paper, by investigating neural networks' properties, the low-cost authentication method based on neural networks is proposed and used to authenticate images or videos. The authentication method can detect whether the images or videos are modified maliciously. Firstly, this chapter introduces neural networks' properties, such as parameter sensitivity, random similarity, diffusion property, confusion property, one-way property, etc. Secondly, the chapter gives an introduction to neural network based protection methods. Thirdly, an image or video authentication scheme based on neural networks is presented, and its performances, including security, robustness and efficiency, are analyzed. Finally, conclusions are drawn, and some open issues in this field are presented.",
        "published": "2007-07-31T02:27:10Z",
        "link": "http://arxiv.org/abs/0707.4524v1",
        "categories": [
            "cs.MM",
            "cs.NE",
            "C.1.3; E.3.x; F.1.1"
        ]
    },
    {
        "title": "Who is the best connected EC researcher? Centrality analysis of the   complex network of authors in evolutionary computation",
        "authors": [
            "Juan J. Merelo",
            "Carlos Cotta"
        ],
        "summary": "Co-authorship graphs (that is, the graph of authors linked by co-authorship of papers) are complex networks, which expresses the dynamics of a complex system. Only recently its study has started to draw interest from the EC community, the first paper dealing with it having been published two years ago. In this paper we will study the co-authorship network of EC at a microscopic level. Our objective is ascertaining which are the most relevant nodes (i.e. authors) in it. For this purpose, we examine several metrics defined in the complex-network literature, and analyze them both in isolation and combined within a Pareto-dominance approach. The result of our analysis indicates that there are some well-known researchers that appear systematically in top rankings. This also provides some hints on the social behavior of our community.",
        "published": "2007-08-15T10:13:03Z",
        "link": "http://arxiv.org/abs/0708.2021v1",
        "categories": [
            "cs.CY",
            "cs.NE"
        ]
    },
    {
        "title": "A Neural Networks Model of the Venezuelan Economy",
        "authors": [
            "Sabatino Costanzo",
            "Loren Trigo",
            "Luis Jimenez",
            "Juan Gonzalez"
        ],
        "summary": "Besides an indicator of the GDP, the Central Bank of Venezuela generates the so called Monthly Economic Activity General Indicator. The a priori knowledge of this indicator, which represents and sometimes even anticipates the economy's fluctuations, could be helpful in developing public policies and in investment decision making. The purpose of this study is forecasting the IGAEM through non parametric methods, an approach that has proven effective in a wide variety of problems in economics and finance.",
        "published": "2007-08-26T05:10:29Z",
        "link": "http://arxiv.org/abs/0708.3463v1",
        "categories": [
            "cs.CE",
            "cs.NE"
        ]
    },
    {
        "title": "A Non Parametric Study of the Volatility of the Economy as a Country   Risk Predictor",
        "authors": [
            "Sabatino Costanzo",
            "Loren Trigo",
            "Ramses Dominguez",
            "William Moreno"
        ],
        "summary": "This paper intends to explain Venezuela's country spread behavior through the Neural Networks analysis of a monthly economic activity general index of economic indicators constructed by the Central Bank of Venezuela, a measure of the shocks affecting country risk of emerging markets and the U.S. short term interest rate. The use of non parametric methods allowed the finding of non linear relationship between these inputs and the country risk. The networks performance was evaluated using the method of excess predictability.",
        "published": "2007-08-26T05:30:18Z",
        "link": "http://arxiv.org/abs/0708.3464v1",
        "categories": [
            "cs.CE",
            "cs.NE"
        ]
    },
    {
        "title": "A Non Parametric Model for the Forecasting of the Venezuelan Oil Prices",
        "authors": [
            "Sabatino Costanzo",
            "Loren Trigo",
            "Wafaa Dehne",
            "Hender Prato"
        ],
        "summary": "A neural net model for forecasting the prices of Venezuelan crude oil is proposed. The inputs of the neural net are selected by reference to a dynamic system model of oil prices by Mashayekhi (1995, 2001) and its performance is evaluated using two criteria: the Excess Profitability test by Anatoliev and Gerko (2005) and the characteristics of the equity curve generated by a trading strategy based on the neural net predictions.   -----   Se introduce aqui un modelo no parametrico para pronosticar los precios del petroleo Venezolano cuyos insumos son seleccionados en base a un sistema dinamico que explica los precios en terminos de dichos insumos. Se describe el proceso de recoleccion y pre-procesamiento de datos y la corrida de la red y se evaluan sus pronosticos a traves de un test estadistico de predictibilidad y de las caracteristicas del Equity Curve inducido por la estrategia de compraventa bursatil generada por dichos pronosticos.",
        "published": "2007-08-28T18:29:55Z",
        "link": "http://arxiv.org/abs/0708.3829v1",
        "categories": [
            "cs.CE",
            "cs.NE"
        ]
    },
    {
        "title": "Liquid State Machines in Adbiatic Quantum Computers for General   Computation",
        "authors": [
            "Joshua Jay Herman"
        ],
        "summary": "Major mistakes do not read",
        "published": "2007-09-06T16:04:42Z",
        "link": "http://arxiv.org/abs/0709.0883v5",
        "categories": [
            "cs.CC",
            "cs.NE",
            "C.1.3; F.1.3"
        ]
    },
    {
        "title": "Mutual information for the selection of relevant variables in   spectrometric nonlinear modelling",
        "authors": [
            "Fabrice Rossi",
            "Amaury Lendasse",
            "Damien François",
            "Vincent Wertz",
            "Michel Verleysen"
        ],
        "summary": "Data from spectrophotometers form vectors of a large number of exploitable variables. Building quantitative models using these variables most often requires using a smaller set of variables than the initial one. Indeed, a too large number of input variables to a model results in a too large number of parameters, leading to overfitting and poor generalization abilities. In this paper, we suggest the use of the mutual information measure to select variables from the initial set. The mutual information measures the information content in input variables with respect to the model output, without making any assumption on the model that will be used; it is thus suitable for nonlinear modelling. In addition, it leads to the selection of variables among the initial set, and not to linear or nonlinear combinations of them. Without decreasing the model performances compared to other variable projection methods, it allows therefore a greater interpretability of the results.",
        "published": "2007-09-21T12:49:47Z",
        "link": "http://arxiv.org/abs/0709.3427v1",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.AP"
        ]
    },
    {
        "title": "Fast Algorithm and Implementation of Dissimilarity Self-Organizing Maps",
        "authors": [
            "Brieuc Conan-Guez",
            "Fabrice Rossi",
            "Aïcha El Golli"
        ],
        "summary": "In many real world applications, data cannot be accurately represented by vectors. In those situations, one possible solution is to rely on dissimilarity measures that enable sensible comparison between observations. Kohonen's Self-Organizing Map (SOM) has been adapted to data described only through their dissimilarity matrix. This algorithm provides both non linear projection and clustering of non vector data. Unfortunately, the algorithm suffers from a high cost that makes it quite difficult to use with voluminous data sets. In this paper, we propose a new algorithm that provides an important reduction of the theoretical cost of the dissimilarity SOM without changing its outcome (the results are exactly the same as the ones obtained with the original algorithm). Moreover, we introduce implementation methods that result in very short running times. Improvements deduced from the theoretical cost model are validated on simulated and real world data (a word list clustering problem). We also demonstrate that the proposed implementation methods reduce by a factor up to 3 the running time of the fast algorithm over a standard implementation.",
        "published": "2007-09-21T15:20:07Z",
        "link": "http://arxiv.org/abs/0709.3461v1",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Une adaptation des cartes auto-organisatrices pour des données   décrites par un tableau de dissimilarités",
        "authors": [
            "Aïcha El Golli",
            "Fabrice Rossi",
            "Brieuc Conan-Guez",
            "Yves Lechevallier"
        ],
        "summary": "Many data analysis methods cannot be applied to data that are not represented by a fixed number of real values, whereas most of real world observations are not readily available in such a format. Vector based data analysis methods have therefore to be adapted in order to be used with non standard complex data. A flexible and general solution for this adaptation is to use a (dis)similarity measure. Indeed, thanks to expert knowledge on the studied data, it is generally possible to define a measure that can be used to make pairwise comparison between observations. General data analysis methods are then obtained by adapting existing methods to (dis)similarity matrices. In this article, we propose an adaptation of Kohonen's Self Organizing Map (SOM) to (dis)similarity data. The proposed algorithm is an adapted version of the vector based batch SOM. The method is validated on real world data: we provide an analysis of the usage patterns of the web site of the Institut National de Recherche en Informatique et Automatique, constructed thanks to web log mining method.",
        "published": "2007-09-22T15:53:54Z",
        "link": "http://arxiv.org/abs/0709.3586v1",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Self-organizing maps and symbolic data",
        "authors": [
            "Aïcha El Golli",
            "Brieuc Conan-Guez",
            "Fabrice Rossi"
        ],
        "summary": "In data analysis new forms of complex data have to be considered like for example (symbolic data, functional data, web data, trees, SQL query and multimedia data, ...). In this context classical data analysis for knowledge discovery based on calculating the center of gravity can not be used because input are not $\\mathbb{R}^p$ vectors. In this paper, we present an application on real world symbolic data using the self-organizing map. To this end, we propose an extension of the self-organizing map that can handle symbolic data.",
        "published": "2007-09-22T15:54:37Z",
        "link": "http://arxiv.org/abs/0709.3587v1",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Representation of Functional Data in Neural Networks",
        "authors": [
            "Fabrice Rossi",
            "Nicolas Delannay",
            "Brieuc Conan-Guez",
            "Michel Verleysen"
        ],
        "summary": "Functional Data Analysis (FDA) is an extension of traditional data analysis to functional data, for example spectra, temporal series, spatio-temporal images, gesture recognition data, etc. Functional data are rarely known in practice; usually a regular or irregular sampling is known. For this reason, some processing is needed in order to benefit from the smooth character of functional data in the analysis methods. This paper shows how to extend the Radial-Basis Function Networks (RBFN) and Multi-Layer Perceptron (MLP) models to functional data inputs, in particular when the latter are known through lists of input-output pairs. Various possibilities for functional processing are discussed, including the projection on smooth bases, Functional Principal Component Analysis, functional centering and reduction, and the use of differential operators. It is shown how to incorporate these functional processing into the RBFN and MLP models. The functional approach is illustrated on a benchmark of spectrometric data analysis.",
        "published": "2007-09-23T14:10:08Z",
        "link": "http://arxiv.org/abs/0709.3641v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Functional Multi-Layer Perceptron: a Nonlinear Tool for Functional Data   Analysis",
        "authors": [
            "Fabrice Rossi",
            "Brieuc Conan-Guez"
        ],
        "summary": "In this paper, we study a natural extension of Multi-Layer Perceptrons (MLP) to functional inputs. We show that fundamental results for classical MLP can be extended to functional MLP. We obtain universal approximation results that show the expressive power of functional MLP is comparable to that of numerical MLP. We obtain consistency results which imply that the estimation of optimal parameters for functional MLP is statistically well defined. We finally show on simulated and real world data that the proposed model performs in a very satisfactory way.",
        "published": "2007-09-23T14:10:48Z",
        "link": "http://arxiv.org/abs/0709.3642v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Evolving Classifiers: Methods for Incremental Learning",
        "authors": [
            "Greg Hulley",
            "Tshilidzi Marwala"
        ],
        "summary": "The ability of a classifier to take on new information and classes by evolving the classifier without it having to be fully retrained is known as incremental learning. Incremental learning has been successfully applied to many classification problems, where the data is changing and is not all available at once. In this paper there is a comparison between Learn++, which is one of the most recent incremental learning algorithms, and the new proposed method of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has shown good incremental learning capabilities on benchmark datasets on which the new ILUGA method has been tested. ILUGA has also shown good incremental learning ability using only a few classifiers and does not suffer from catastrophic forgetting. The results obtained for ILUGA on the Optical Character Recognition (OCR) and Wine datasets are good, with an overall accuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT for the difficult multi-class OCR dataset.",
        "published": "2007-09-25T14:28:32Z",
        "link": "http://arxiv.org/abs/0709.3965v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Adaptive Investment Strategies For Periodic Environments",
        "authors": [
            "J. -Emeterio Navarro"
        ],
        "summary": "In this paper, we present an adaptive investment strategy for environments with periodic returns on investment. In our approach, we consider an investment model where the agent decides at every time step the proportion of wealth to invest in a risky asset, keeping the rest of the budget in a risk-free asset. Every investment is evaluated in the market via a stylized return on investment function (RoI), which is modeled by a stochastic process with unknown periodicities and levels of noise. For comparison reasons, we present two reference strategies which represent the case of agents with zero-knowledge and complete-knowledge of the dynamics of the returns. We consider also an investment strategy based on technical analysis to forecast the next return by fitting a trend line to previous received returns. To account for the performance of the different strategies, we perform some computer experiments to calculate the average budget that can be obtained with them over a certain number of time steps. To assure for fair comparisons, we first tune the parameters of each strategy. Afterwards, we compare the performance of these strategies for RoIs with different periodicities and levels of noise.",
        "published": "2007-09-27T19:04:00Z",
        "link": "http://arxiv.org/abs/0709.4464v2",
        "categories": [
            "cs.CE",
            "cs.NE",
            "I.2.8"
        ]
    },
    {
        "title": "Optimising the topology of complex neural networks",
        "authors": [
            "Fei Jiang",
            "Hugues Berry",
            "Marc Schoenauer"
        ],
        "summary": "In this paper, we study instances of complex neural networks, i.e. neural netwo rks with complex topologies. We use Self-Organizing Map neural networks whose n eighbourhood relationships are defined by a complex network, to classify handwr itten digits. We show that topology has a small impact on performance and robus tness to neuron failures, at least at long learning times. Performance may howe ver be increased (by almost 10%) by artificial evolution of the network topo logy. In our experimental conditions, the evolved networks are more random than their parents, but display a more heterogeneous degree distribution.",
        "published": "2007-10-01T06:51:42Z",
        "link": "http://arxiv.org/abs/0710.0213v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Optimization of supply diversity for the self-assembly of simple objects   in two and three dimensions",
        "authors": [
            "Fabio R. J. Vieira",
            "Valmir C. Barbosa"
        ],
        "summary": "The field of algorithmic self-assembly is concerned with the design and analysis of self-assembly systems from a computational perspective, that is, from the perspective of mathematical problems whose study may give insight into the natural processes through which elementary objects self-assemble into more complex ones. One of the main problems of algorithmic self-assembly is the minimum tile set problem (MTSP), which asks for a collection of types of elementary objects (called tiles) to be found for the self-assembly of an object having a pre-established shape. Such a collection is to be as concise as possible, thus minimizing supply diversity, while satisfying a set of stringent constraints having to do with the termination and other properties of the self-assembly process from its tile types. We present a study of what we think is the first practical approach to MTSP. Our study starts with the introduction of an evolutionary heuristic to tackle MTSP and includes results from extensive experimentation with the heuristic on the self-assembly of simple objects in two and three dimensions. The heuristic we introduce combines classic elements from the field of evolutionary computation with a problem-specific variant of Pareto dominance into a multi-objective approach to MTSP.",
        "published": "2007-10-03T18:29:12Z",
        "link": "http://arxiv.org/abs/0710.0672v2",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "A System for Predicting Subcellular Localization of Yeast Genome Using   Neural Network",
        "authors": [
            "Sabu M. Thampi",
            "K. Chandra Sekaran"
        ],
        "summary": "The subcellular location of a protein can provide valuable information about its function. With the rapid increase of sequenced genomic data, the need for an automated and accurate tool to predict subcellular localization becomes increasingly important. Many efforts have been made to predict protein subcellular localization. This paper aims to merge the artificial neural networks and bioinformatics to predict the location of protein in yeast genome. We introduce a new subcellular prediction method based on a backpropagation neural network. The results show that the prediction within an error limit of 5 to 10 percentage can be achieved with the system.",
        "published": "2007-10-11T12:09:33Z",
        "link": "http://arxiv.org/abs/0710.2227v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Effective linkage learning using low-order statistics and clustering",
        "authors": [
            "Leonardo Emmendorfer",
            "Aurora Pozo"
        ],
        "summary": "The adoption of probabilistic models for the best individuals found so far is a powerful approach for evolutionary computation. Increasingly more complex models have been used by estimation of distribution algorithms (EDAs), which often result better effectiveness on finding the global optima for hard optimization problems. Supervised and unsupervised learning of Bayesian networks are very effective options, since those models are able to capture interactions of high order among the variables of a problem. Diversity preservation, through niching techniques, has also shown to be very important to allow the identification of the problem structure as much as for keeping several global optima. Recently, clustering was evaluated as an effective niching technique for EDAs, but the performance of simpler low-order EDAs was not shown to be much improved by clustering, except for some simple multimodal problems. This work proposes and evaluates a combination operator guided by a measure from information theory which allows a clustered low-order EDA to effectively solve a comprehensive range of benchmark optimization problems.",
        "published": "2007-10-15T15:28:47Z",
        "link": "http://arxiv.org/abs/0710.2782v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Stationary probability density of stochastic search processes in global   optimization",
        "authors": [
            "Arturo Berrones"
        ],
        "summary": "A method for the construction of approximate analytical expressions for the stationary marginal densities of general stochastic search processes is proposed. By the marginal densities, regions of the search space that with high probability contain the global optima can be readily defined. The density estimation procedure involves a controlled number of linear operations, with a computational cost per iteration that grows linearly with problem size.",
        "published": "2007-10-18T18:04:42Z",
        "link": "http://arxiv.org/abs/0710.3561v1",
        "categories": [
            "cs.AI",
            "cond-mat.stat-mech",
            "cs.NE"
        ]
    },
    {
        "title": "Beyond Feedforward Models Trained by Backpropagation: a Practical   Training Tool for a More Efficient Universal Approximator",
        "authors": [
            "Roman Ilin",
            "Robert Kozma",
            "Paul J. Werbos"
        ],
        "summary": "Cellular Simultaneous Recurrent Neural Network (SRN) has been shown to be a function approximator more powerful than the MLP. This means that the complexity of MLP would be prohibitively large for some problems while SRN could realize the desired mapping with acceptable computational constraints. The speed of training of complex recurrent networks is crucial to their successful application. Present work improves the previous results by training the network with extended Kalman filter (EKF). We implemented a generic Cellular SRN and applied it for solving two challenging problems: 2D maze navigation and a subset of the connectedness problem. The speed of convergence has been improved by several orders of magnitude in comparison with the earlier results in the case of maze navigation, and superior generalization has been demonstrated in the case of connectedness. The implications of this improvements are discussed.",
        "published": "2007-10-23T03:43:57Z",
        "link": "http://arxiv.org/abs/0710.4182v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Fault-Trajectory Approach for Fault Diagnosis on Analog Circuits",
        "authors": [
            "Carlos Eduardo Savioli",
            "Claudio C. Czendrodi",
            "Jose Vicente Calvano",
            "Antonio Carneiro De Mesquita Filho"
        ],
        "summary": "This issue discusses the fault-trajectory approach suitability for fault diagnosis on analog networks. Recent works have shown promising results concerning a method based on this concept for ATPG for diagnosing faults on analog networks. Such method relies on evolutionary techniques, where a generic algorithm (GA) is coded to generate a set of optimum frequencies capable to disclose faults.",
        "published": "2007-10-25T09:37:48Z",
        "link": "http://arxiv.org/abs/0710.4725v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Computational Intelligence Characterization Method of Semiconductor   Device",
        "authors": [
            "Eric Liau",
            "Doris Schmitt-Landsiedel"
        ],
        "summary": "Characterization of semiconductor devices is used to gather as much data about the device as possible to determine weaknesses in design or trends in the manufacturing process. In this paper, we propose a novel multiple trip point characterization concept to overcome the constraint of single trip point concept in device characterization phase. In addition, we use computational intelligence techniques (e.g. neural network, fuzzy and genetic algorithm) to further manipulate these sets of multiple trip point values and tests based on semiconductor test equipments, Our experimental results demonstrate an excellent design parameter variation analysis in device characterization phase, as well as detection of a set of worst case tests that can provoke the worst case variation, while traditional approach was not capable of detecting them.",
        "published": "2007-10-25T09:41:43Z",
        "link": "http://arxiv.org/abs/0710.4734v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Towards a Sound Theory of Adaptation for the Simple Genetic Algorithm",
        "authors": [
            "Keki Burjorjee"
        ],
        "summary": "The pace of progress in the fields of Evolutionary Computation and Machine Learning is currently limited -- in the former field, by the improbability of making advantageous extensions to evolutionary algorithms when their capacity for adaptation is poorly understood, and in the latter by the difficulty of finding effective semi-principled reductions of hard real-world problems to relatively simple optimization problems. In this paper we explain why a theory which can accurately explain the simple genetic algorithm's remarkable capacity for adaptation has the potential to address both these limitations. We describe what we believe to be the impediments -- historic and analytic -- to the discovery of such a theory and highlight the negative role that the building block hypothesis (BBH) has played. We argue based on experimental results that a fundamental limitation which is widely believed to constrain the SGA's adaptive ability (and is strongly implied by the BBH) is in fact illusionary and does not exist. The SGA therefore turns out to be more powerful than it is currently thought to be. We give conditions under which it becomes feasible to numerically approximate and study the multivariate marginals of the search distribution of an infinite population SGA over multiple generations even when its genomes are long, and explain why this analysis is relevant to the riddle of the SGA's remarkable adaptive abilities.",
        "published": "2007-11-09T02:28:12Z",
        "link": "http://arxiv.org/abs/0711.1401v2",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.8; F.2"
        ]
    },
    {
        "title": "A Compact Self-organizing Cellular Automata-based Genetic Algorithm",
        "authors": [
            "Vasileios Barmpoutis",
            "Gary F. Dargush"
        ],
        "summary": "A Genetic Algorithm (GA) is proposed in which each member of the population can change schemata only with its neighbors according to a rule. The rule methodology and the neighborhood structure employ elements from the Cellular Automata (CA) strategies. Each member of the GA population is assigned to a cell and crossover takes place only between adjacent cells, according to the predefined rule. Although combinations of CA and GA approaches have appeared previously, here we rely on the inherent self-organizing features of CA, rather than on parallelism. This conceptual shift directs us toward the evolution of compact populations containing only a handful of members. We find that the resulting algorithm can search the design space more efficiently than traditional GA strategies due to its ability to exploit mutations within this compact self-organizing population. Consequently, premature convergence is avoided and the final results often are more accurate. In order to reinforce the superior mutation capability, a re-initialization strategy also is implemented. Ten test functions and two benchmark structural engineering truss design problems are examined in order to demonstrate the performance of the method.",
        "published": "2007-11-15T18:19:39Z",
        "link": "http://arxiv.org/abs/0711.2478v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Estimation of fuzzy anomalies in Water Distribution Systems",
        "authors": [
            "J. Izquierdo",
            "M. M. Tung",
            "R. Perez",
            "F. J. Martinez"
        ],
        "summary": "State estimation is necessary in diagnosing anomalies in Water Demand Systems (WDS). In this paper we present a neural network performing such a task. State estimation is performed by using optimization, which tries to reconcile all the available information. Quantification of the uncertainty of the input data (telemetry measures and demand predictions) can be achieved by means of robust estate estimation. Using a mathematical model of the network, fuzzy estimated states for anomalous states of the network can be obtained. They are used to train a neural network capable of assessing WDS anomalies associated with particular sets of measurements.",
        "published": "2007-11-19T11:24:47Z",
        "link": "http://arxiv.org/abs/0711.2897v1",
        "categories": [
            "cs.NE",
            "I.5.1"
        ]
    },
    {
        "title": "An evolutionary model with Turing machines",
        "authors": [
            "Giovanni Feverati",
            "Fabio Musso"
        ],
        "summary": "The development of a large non-coding fraction in eukaryotic DNA and the phenomenon of the code-bloat in the field of evolutionary computations show a striking similarity. This seems to suggest that (in the presence of mechanisms of code growth) the evolution of a complex code can't be attained without maintaining a large inactive fraction. To test this hypothesis we performed computer simulations of an evolutionary toy model for Turing machines, studying the relations among fitness and coding/non-coding ratio while varying mutation and code growth rates. The results suggest that, in our model, having a large reservoir of non-coding states constitutes a great (long term) evolutionary advantage.",
        "published": "2007-11-22T14:47:06Z",
        "link": "http://arxiv.org/abs/0711.3580v1",
        "categories": [
            "q-bio.QM",
            "cs.NE",
            "q-bio.GN"
        ]
    },
    {
        "title": "An Estimation of Distribution Algorithm with Intelligent Local Search   for Rule-based Nurse Rostering",
        "authors": [
            "Uwe Aickelin",
            "Edmund Burke",
            "Jingpeng Li"
        ],
        "summary": "This paper proposes a new memetic evolutionary algorithm to achieve explicit learning in rule-based nurse rostering, which involves applying a set of heuristic rules for each nurse's assignment. The main framework of the algorithm is an estimation of distribution algorithm, in which an ant-miner methodology improves the individual solutions produced in each generation. Unlike our previous work (where learning is implicit), the learning in the memetic estimation of distribution algorithm is explicit, i.e. we are able to identify building blocks directly. The overall approach learns by building a probabilistic model, i.e. an estimation of the probability distribution of individual nurse-rule pairs that are used to construct schedules. The local search processor (i.e. the ant-miner) reinforces nurse-rule pairs that receive higher rewards. A challenging real world nurse rostering problem is used as the test problem. Computational results show that the proposed approach outperforms most existing approaches. It is suggested that the learning methodologies suggested in this paper may be applied to other scheduling problems where schedules are built systematically according to specific rules",
        "published": "2007-11-22T15:16:21Z",
        "link": "http://arxiv.org/abs/0711.3591v2",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Automatic Pattern Classification by Unsupervised Learning Using   Dimensionality Reduction of Data with Mirroring Neural Networks",
        "authors": [
            "Dasika Ratna Deepthi",
            "G. R. Aditya Krishna",
            "K. Eswaran"
        ],
        "summary": "This paper proposes an unsupervised learning technique by using Multi-layer Mirroring Neural Network and Forgy's clustering algorithm. Multi-layer Mirroring Neural Network is a neural network that can be trained with generalized data inputs (different categories of image patterns) to perform non-linear dimensionality reduction and the resultant low-dimensional code is used for unsupervised pattern classification using Forgy's algorithm. By adapting the non-linear activation function (modified sigmoidal function) and initializing the weights and bias terms to small random values, mirroring of the input pattern is initiated. In training, the weights and bias terms are changed in such a way that the input presented is reproduced at the output by back propagating the error. The mirroring neural network is capable of reducing the input vector to a great degree (approximately 1/30th the original size) and also able to reconstruct the input pattern at the output layer from this reduced code units. The feature set (output of central hidden layer) extracted from this network is fed to Forgy's algorithm, which classify input data patterns into distinguishable classes. In the implementation of Forgy's algorithm, initial seed points are selected in such a way that they are distant enough to be perfectly grouped into different categories. Thus a new method of unsupervised learning is formulated and demonstrated in this paper. This method gave impressive results when applied to classification of different image patterns.",
        "published": "2007-12-06T13:52:04Z",
        "link": "http://arxiv.org/abs/0712.0938v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Dimensionality Reduction and Reconstruction using Mirroring Neural   Networks and Object Recognition based on Reduced Dimension Characteristic   Vector",
        "authors": [
            "Dasika Ratna Deepthi",
            "Sujeet Kuchibhotla",
            "K. Eswaran"
        ],
        "summary": "In this paper, we present a Mirroring Neural Network architecture to perform non-linear dimensionality reduction and Object Recognition using a reduced lowdimensional characteristic vector. In addition to dimensionality reduction, the network also reconstructs (mirrors) the original high-dimensional input vector from the reduced low-dimensional data. The Mirroring Neural Network architecture has more number of processing elements (adalines) in the outer layers and the least number of elements in the central layer to form a converging-diverging shape in its configuration. Since this network is able to reconstruct the original image from the output of the innermost layer (which contains all the information about the input pattern), these outputs can be used as object signature to classify patterns. The network is trained to minimize the discrepancy between actual output and the input by back propagating the mean squared error from the output layer to the input layer. After successfully training the network, it can reduce the dimension of input vectors and mirror the patterns fed to it. The Mirroring Neural Network architecture gave very good results on various test patterns.",
        "published": "2007-12-06T14:11:07Z",
        "link": "http://arxiv.org/abs/0712.0932v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Constructing Bio-molecular Databases on a DNA-based Computer",
        "authors": [
            "Weng-Long Chang",
            "Michael",
            "Ho",
            "Minyi Guo"
        ],
        "summary": "Codd [Codd 1970] wrote the first paper in which the model of a relational database was proposed. Adleman [Adleman 1994] wrote the first paper in which DNA strands in a test tube were used to solve an instance of the Hamiltonian path problem. From [Adleman 1994], it is obviously indicated that for storing information in molecules of DNA allows for an information density of approximately 1 bit per cubic nm (nanometer) and a dramatic improvement over existing storage media such as video tape which store information at a density of approximately 1 bit per 1012 cubic nanometers. This paper demonstrates that biological operations can be applied to construct bio-molecular databases where data records in relational tables are encoded as DNA strands. In order to achieve the goal, DNA algorithms are proposed to perform eight operations of relational algebra (calculus) on bio-molecular relational databases, which include Cartesian product, union, set difference, selection, projection, intersection, join and division. Furthermore, this work presents clear evidence of the ability of molecular computing to perform data retrieval operations on bio-molecular relational databases.",
        "published": "2007-12-12T03:58:01Z",
        "link": "http://arxiv.org/abs/0712.1863v1",
        "categories": [
            "cs.NE",
            "cs.DB",
            "q-bio.OT",
            "H.3.0; H.3.3; D.3.0; D.3.1; D.3.m"
        ]
    },
    {
        "title": "Evolving XSLT stylesheets",
        "authors": [
            "Nestor Zorzano",
            "Daniel Merino",
            "J. L. J. Laredo",
            "J. P. Sevilla",
            "Pablo Garcia",
            "J. J. Merelo"
        ],
        "summary": "This paper introduces a procedure based on genetic programming to evolve XSLT programs (usually called stylesheets or logicsheets). XSLT is a general purpose, document-oriented functional language, generally used to transform XML documents (or, in general, solve any problem that can be coded as an XML document). The proposed solution uses a tree representation for the stylesheets as well as diverse specific operators in order to obtain, in the studied cases and a reasonable time, a XSLT stylesheet that performs the transformation. Several types of representation have been compared, resulting in different performance and degree of success.",
        "published": "2007-12-17T19:59:42Z",
        "link": "http://arxiv.org/abs/0712.2630v1",
        "categories": [
            "cs.NE",
            "cs.PL"
        ]
    },
    {
        "title": "Improving the Performance of PieceWise Linear Separation Incremental   Algorithms for Practical Hardware Implementations",
        "authors": [
            "Alejandro Chinea Manrique De Lara",
            "Juan Manuel Moreno",
            "Arostegui Jordi Madrenas",
            "Joan Cabestany"
        ],
        "summary": "In this paper we shall review the common problems associated with Piecewise Linear Separation incremental algorithms. This kind of neural models yield poor performances when dealing with some classification problems, due to the evolving schemes used to construct the resulting networks. So as to avoid this undesirable behavior we shall propose a modification criterion. It is based upon the definition of a function which will provide information about the quality of the network growth process during the learning phase. This function is evaluated periodically as the network structure evolves, and will permit, as we shall show through exhaustive benchmarks, to considerably improve the performance(measured in terms of network complexity and generalization capabilities) offered by the networks generated by these incremental models.",
        "published": "2007-12-21T10:05:52Z",
        "link": "http://arxiv.org/abs/0712.3654v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "GUIDE: Unifying Evolutionary Engines through a Graphical User Interface",
        "authors": [
            "Pierre Collet",
            "Marc Schoenauer"
        ],
        "summary": "Many kinds of Evolutionary Algorithms (EAs) have been described in the literature since the last 30 years. However, though most of them share a common structure, no existing software package allows the user to actually shift from one model to another by simply changing a few parameters, e.g. in a single window of a Graphical User Interface. This paper presents GUIDE, a Graphical User Interface for DREAM Experiments that, among other user-friendly features, unifies all kinds of EAs into a single panel, as far as evolution parameters are concerned. Such a window can be used either to ask for one of the well known ready-to-use algorithms, or to very easily explore new combinations that have not yet been studied. Another advantage of grouping all necessary elements to describe virtually all kinds of EAs is that it creates a fantastic pedagogic tool to teach EAs to students and newcomers to the field.",
        "published": "2007-12-24T07:31:58Z",
        "link": "http://arxiv.org/abs/0712.3973v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "TRUST-TECH based Methods for Optimization and Learning",
        "authors": [
            "Chandan K. Reddy"
        ],
        "summary": "Many problems that arise in machine learning domain deal with nonlinearity and quite often demand users to obtain global optimal solutions rather than local optimal ones. Optimization problems are inherent in machine learning algorithms and hence many methods in machine learning were inherited from the optimization literature. Popularly known as the initialization problem, the ideal set of parameters required will significantly depend on the given initialization values. The recently developed TRUST-TECH (TRansformation Under STability-reTaining Equilibria CHaracterization) methodology systematically explores the subspace of the parameters to obtain a complete set of local optimal solutions. In this thesis work, we propose TRUST-TECH based methods for solving several optimization and machine learning problems. Two stages namely, the local stage and the neighborhood-search stage, are repeated alternatively in the solution space to achieve improvements in the quality of the solutions. Our methods were tested on both synthetic and real datasets and the advantages of using this novel framework are clearly manifested. This framework not only reduces the sensitivity to initialization, but also allows the flexibility for the practitioners to use various global and local methods that work well for a particular problem of interest. Other hierarchical stochastic algorithms like evolutionary algorithms and smoothing algorithms are also studied and frameworks for combining these methods with TRUST-TECH have been proposed and evaluated on several test systems.",
        "published": "2007-12-25T03:14:32Z",
        "link": "http://arxiv.org/abs/0712.4126v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.MS",
            "cs.NA",
            "cs.NE",
            "G.1.6; I.5.3; I.5.1"
        ]
    },
    {
        "title": "Digital Ecosystems: Optimisation by a Distributed Intelligence",
        "authors": [
            "G. Briscoe",
            "P. De Wilde"
        ],
        "summary": "Can intelligence optimise Digital Ecosystems? How could a distributed intelligence interact with the ecosystem dynamics? Can the software components that are part of genetic selection be intelligent in themselves, as in an adaptive technology? We consider the effect of a distributed intelligence mechanism on the evolutionary and ecological dynamics of our Digital Ecosystem, which is the digital counterpart of a biological ecosystem for evolving software services in a distributed network. We investigate Neural Networks and Support Vector Machine for the learning based pattern recognition functionality of our distributed intelligence. Simulation results imply that the Digital Ecosystem performs better with the application of a distributed intelligence, marginally more effectively when powered by Support Vector Machine than Neural Networks, and suggest that it can contribute to optimising the operation of our Digital Ecosystem.",
        "published": "2007-12-26T04:13:20Z",
        "link": "http://arxiv.org/abs/0712.4099v3",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Digital Ecosystems: Stability of Evolving Agent Populations",
        "authors": [
            "P. De Wilde",
            "G. Briscoe"
        ],
        "summary": "Stability is perhaps one of the most desirable features of any engineered system, given the importance of being able to predict its response to various environmental conditions prior to actual deployment. Engineered systems are becoming ever more complex, approaching the same levels of biological ecosystems, and so their stability becomes ever more important, but taking on more and more differential dynamics can make stability an ever more elusive property. The Chli-DeWilde definition of stability views a Multi-Agent System as a discrete time Markov chain with potentially unknown transition probabilities. With a Multi-Agent System being considered stable when its state, a stochastic process, has converged to an equilibrium distribution, because stability of a system can be understood intuitively as exhibiting bounded behaviour. We investigate an extension to include Multi-Agent Systems with evolutionary dynamics, focusing on the evolving agent populations of our Digital Ecosystem. We then built upon this to construct an entropy-based definition for the degree of instability (entropy of the limit probabilities), which was later used to perform a stability analysis. The Digital Ecosystem is considered to investigate the stability of an evolving agent population through simulations, for which the results were consistent with the original Chli-DeWilde definition.",
        "published": "2007-12-26T04:40:16Z",
        "link": "http://arxiv.org/abs/0712.4101v5",
        "categories": [
            "cs.NE",
            "C.2.4; D.2.11; H.1.0"
        ]
    },
    {
        "title": "Digital Ecosystems: Evolving Service-Oriented Architectures",
        "authors": [
            "G. Briscoe",
            "P. De Wilde"
        ],
        "summary": "We view Digital Ecosystems to be the digital counterparts of biological ecosystems, exploiting the self-organising properties of biological ecosystems, which are considered to be robust, self-organising and scalable architectures that can automatically solve complex, dynamic problems. Digital Ecosystems are a novel optimisation technique where the optimisation works at two levels: a first optimisation, migration of agents (representing services) which are distributed in a decentralised peer-to-peer network, operating continuously in time; this process feeds a second optimisation based on evolutionary computing that operates locally on single peers and is aimed at finding solutions to satisfy locally relevant constraints. We created an Ecosystem-Oriented Architecture of Digital Ecosystems by extending Service-Oriented Architectures with distributed evolutionary computing, allowing services to recombine and evolve over time, constantly seeking to improve their effectiveness for the user base. Individuals within our Digital Ecosystem will be applications (groups of services), created in response to user requests by using evolutionary optimisation to aggregate the services. These individuals will migrate through the Digital Ecosystem and adapt to find niches where they are useful in fulfilling other user requests for applications. Simulation results imply that the Digital Ecosystem performs better at large scales than a comparable Service-Oriented Architecture, suggesting that incorporating ideas from theoretical ecology can contribute to useful self-organising properties in digital ecosystems.",
        "published": "2007-12-26T05:44:31Z",
        "link": "http://arxiv.org/abs/0712.4102v6",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Biology of Applied Digital Ecosystems",
        "authors": [
            "G. Briscoe",
            "S. Sadedin",
            "G. Paperin"
        ],
        "summary": "A primary motivation for our research in Digital Ecosystems is the desire to exploit the self-organising properties of biological ecosystems. Ecosystems are thought to be robust, scalable architectures that can automatically solve complex, dynamic problems. However, the biological processes that contribute to these properties have not been made explicit in Digital Ecosystems research. Here, we discuss how biological properties contribute to the self-organising features of biological ecosystems, including population dynamics, evolution, a complex dynamic environment, and spatial distributions for generating local interactions. The potential for exploiting these properties in artificial systems is then considered. We suggest that several key features of biological ecosystems have not been fully explored in existing digital ecosystems, and discuss how mimicking these features may assist in developing robust, scalable self-organising architectures. An example architecture, the Digital Ecosystem, is considered in detail. The Digital Ecosystem is then measured experimentally through simulations, with measures originating from theoretical ecology, to confirm its likeness to a biological ecosystem. Including the responsiveness to requests for applications from the user base, as a measure of the 'ecological succession' (development).",
        "published": "2007-12-26T21:56:52Z",
        "link": "http://arxiv.org/abs/0712.4153v2",
        "categories": [
            "cs.NE",
            "cs.MA"
        ]
    },
    {
        "title": "Creating a Digital Ecosystem: Service-Oriented Architectures with   Distributed Evolutionary Computing",
        "authors": [
            "G Briscoe"
        ],
        "summary": "We start with a discussion of the relevant literature, including Nature Inspired Computing as a framework in which to understand this work, and the process of biomimicry to be used in mimicking the necessary biological processes to create Digital Ecosystems. We then consider the relevant theoretical ecology in creating the digital counterpart of a biological ecosystem, including the topological structure of ecosystems, and evolutionary processes within distributed environments. This leads to a discussion of the relevant fields from computer science for the creation of Digital Ecosystems, including evolutionary computing, Multi-Agent Systems, and Service-Oriented Architectures. We then define Ecosystem-Oriented Architectures for the creation of Digital Ecosystems, imbibed with the properties of self-organisation and scalability from biological ecosystems, including a novel form of distributed evolutionary computing.",
        "published": "2007-12-26T23:32:10Z",
        "link": "http://arxiv.org/abs/0712.4159v5",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Infinite-Alphabet Prefix Codes Optimal for $β$-Exponential Penalties",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "Let $P = \\{p(i)\\}$ be a measure of strictly positive probabilities on the set of nonnegative integers. Although the countable number of inputs prevents usage of the Huffman algorithm, there are nontrivial $P$ for which known methods find a source code that is optimal in the sense of minimizing expected codeword length. For some applications, however, a source code should instead minimize one of a family of nonlinear objective functions, $\\beta$-exponential means, those of the form $\\log_a \\sum_i p(i) a^{n(i)}$, where $n(i)$ is the length of the $i$th codeword and $a$ is a positive constant. Applications of such minimizations include a problem of maximizing the chance of message receipt in single-shot communications ($a<1$) and a problem of minimizing the chance of buffer overflow in a queueing system ($a>1$). This paper introduces methods for finding codes optimal for such exponential means. One method applies to geometric distributions, while another applies to distributions with lighter tails. The latter algorithm is applied to Poisson distributions. Both are extended to minimizing maximum pointwise redundancy.",
        "published": "2007-01-03T03:39:46Z",
        "link": "http://arxiv.org/abs/cs/0701011v2",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "$D$-ary Bounded-Length Huffman Coding",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "Efficient optimal prefix coding has long been accomplished via the Huffman algorithm. However, there is still room for improvement and exploration regarding variants of the Huffman problem. Length-limited Huffman coding, useful for many practical applications, is one such variant, in which codes are restricted to the set of codes in which none of the $n$ codewords is longer than a given length, $l_{\\max}$. Binary length-limited coding can be done in $O(n l_{\\max})$ time and O(n) space via the widely used Package-Merge algorithm. In this paper the Package-Merge approach is generalized without increasing complexity in order to introduce a minimum codeword length, $l_{\\min}$, to allow for objective functions other than the minimization of expected codeword length, and to be applicable to both binary and nonbinary codes; nonbinary codes were previously addressed using a slower dynamic programming approach. These extensions have various applications -- including faster decompression -- and can be used to solve the problem of finding an optimal code with limited fringe, that is, finding the best code among codes with a maximum difference between the longest and shortest codewords. The previously proposed method for solving this problem was nonpolynomial time, whereas solving this using the novel algorithm requires only $O(n (l_{\\max}- l_{\\min})^2)$ time and O(n) space.",
        "published": "2007-01-03T03:42:09Z",
        "link": "http://arxiv.org/abs/cs/0701012v2",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "G.2.2; F.2; E.4; H.1.1"
        ]
    },
    {
        "title": "A nearly optimal and deterministic summary structure for update data   streams",
        "authors": [
            "Sumit Ganguly"
        ],
        "summary": "The paper has been withdrawn due to an error in Lemma 1.",
        "published": "2007-01-04T09:03:08Z",
        "link": "http://arxiv.org/abs/cs/0701020v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A Polynomial Time Algorithm for 3-SAT",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Article describes a class of efficient algorithms for 3SAT and their generalizations on SAT.",
        "published": "2007-01-04T18:16:30Z",
        "link": "http://arxiv.org/abs/cs/0701023v4",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.LO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Polygon Convexity: Another O(n) Test",
        "authors": [
            "Iosif Pinelis"
        ],
        "summary": "An n-gon is defined as a sequence \\P=(V_0,...,V_{n-1}) of n points on the plane. An n-gon \\P is said to be convex if the boundary of the convex hull of the set {V_0,...,V_{n-1}} of the vertices of \\P coincides with the union of the edges [V_0,V_1],...,[V_{n-1},V_0]; if at that no three vertices of \\P are collinear then \\P is called strictly convex. We prove that an n-gon \\P with n\\ge3 is strictly convex if and only if a cyclic shift of the sequence (\\al_0,...,\\al_{n-1})\\in[0,2\\pi)^n of the angles between the x-axis and the vectors V_1-V_0,...,V_0-V_{n-1} is strictly monotone. A ``non-strict'' version of this result is also proved.",
        "published": "2007-01-08T18:51:37Z",
        "link": "http://arxiv.org/abs/cs/0701045v2",
        "categories": [
            "cs.CG",
            "cs.DS",
            "I.3.5; F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Practical Binary Adaptive Block Coder",
        "authors": [
            "Yuriy A. Reznik"
        ],
        "summary": "This paper describes design of a low-complexity algorithm for adaptive encoding/ decoding of binary sequences produced by memoryless sources. The algorithm implements universal block codes constructed for a set of contexts identified by the numbers of non-zero bits in previous bits in a sequence. We derive a precise formula for asymptotic redundancy of such codes, which refines previous well-known estimate by Krichevsky and Trofimov, and provide experimental verification of this result. In our experimental study we also compare our implementation with existing binary adaptive encoders, such as JBIG's Q-coder, and MPEG AVC (ITU-T H.264)'s CABAC algorithms.",
        "published": "2007-01-11T19:58:05Z",
        "link": "http://arxiv.org/abs/cs/0701079v1",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT"
        ]
    },
    {
        "title": "A Backtracking-Based Algorithm for Computing Hypertree-Decompositions",
        "authors": [
            "Georg Gottlob",
            "Marko Samer"
        ],
        "summary": "Hypertree decompositions of hypergraphs are a generalization of tree decompositions of graphs. The corresponding hypertree-width is a measure for the cyclicity and therefore tractability of the encoded computation problem. Many NP-hard decision and computation problems are known to be tractable on instances whose structure corresponds to hypergraphs of bounded hypertree-width. Intuitively, the smaller the hypertree-width, the faster the computation problem can be solved. In this paper, we present the new backtracking-based algorithm det-k-decomp for computing hypertree decompositions of small width. Our benchmark evaluations have shown that det-k-decomp significantly outperforms opt-k-decomp, the only exact hypertree decomposition algorithm so far. Even compared to the best heuristic algorithm, we obtained competitive results as long as the hypergraphs are not too large.",
        "published": "2007-01-14T01:14:25Z",
        "link": "http://arxiv.org/abs/cs/0701083v1",
        "categories": [
            "cs.DS",
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "Strong Spatial Mixing and Rapid Mixing with Five Colours for the Kagome   Lattice",
        "authors": [
            "Markus Jalsenius"
        ],
        "summary": "We consider proper 5-colourings of the kagome lattice. Proper q-colourings correspond to configurations in the zero-temperature q-state anti-ferromagnetic Potts model. Salas and Sokal have given a computer assisted proof of strong spatial mixing on the kagome lattice for q>=6 under any temperature, including zero temperature. It is believed that there is strong spatial mixing for q>=4. Here we give a computer assisted proof of strong spatial mixing for q=5 and zero temperature. It is commonly known that strong spatial mixing implies that there is a unique infinite-volume Gibbs measure and that the Glauber dynamics is rapidly mixing. We give a proof of rapid mixing of the Glauber dynamics on any finite subset of the vertices of the kagome lattice, provided that the boundary is free (not coloured). The Glauber dynamics is not necessarily irreducible if the boundary is chosen arbitrarily for q=5 colours. The Glauber dynamics can be used to uniformly sample proper 5-colourings. Thus, a consequence of rapidly mixing Glauber dynamics is that there is fully polynomial randomised approximation scheme for counting the number of proper 5-colourings.",
        "published": "2007-01-14T21:06:14Z",
        "link": "http://arxiv.org/abs/math-ph/0701043v2",
        "categories": [
            "math-ph",
            "cs.DM",
            "cs.DS",
            "math.MP"
        ]
    },
    {
        "title": "The problem determination of Functional Dependencies between attributes   Relation Scheme in the Relational Data Model. El problema de determinar   Dependencias Funcionales entre atributos en los esquemas en el Modelo   Relacional",
        "authors": [
            "Ignacio Vega-Paez",
            "Georgina G. Pulido",
            "Jose Angel Ortega"
        ],
        "summary": "An alternative definition of the concept is given of functional dependence among the attributes of the relational schema in the Relational Model, this definition is obtained in terms of the set theory. For that which a theorem is demonstrated that establishes equivalence and on the basis theorem an algorithm is built for the search of the functional dependences among the attributes. The algorithm is illustrated by a concrete example",
        "published": "2007-01-17T20:08:53Z",
        "link": "http://arxiv.org/abs/cs/0701114v2",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Knowledge State Algorithms: Randomization with Limited Information",
        "authors": [
            "Wolfgang Bein",
            "Lawrence L. Larmore",
            "Rüdiger Reischuk"
        ],
        "summary": "We introduce the concept of knowledge states; many well-known algorithms can be viewed as knowledge state algorithms. The knowledge state approach can be used to to construct competitive randomized online algorithms and study the tradeoff between competitiveness and memory. A knowledge state simply states conditional obligations of an adversary, by fixing a work function, and gives a distribution for the algorithm. When a knowledge state algorithm receives a request, it then calculates one or more \"subsequent\" knowledge states, together with a probability of transition to each. The algorithm then uses randomization to select one of those subsequents to be the new knowledge state. We apply the method to the paging problem. We present optimally competitive algorithm for paging for the cases where the cache sizes are k=2 and k=3. These algorithms use only a very limited number of bookmarks.",
        "published": "2007-01-23T00:54:27Z",
        "link": "http://arxiv.org/abs/cs/0701142v1",
        "categories": [
            "cs.DS",
            "F.1.2; F.2.3"
        ]
    },
    {
        "title": "Online Bandwidth Allocation",
        "authors": [
            "Michal Forišek",
            "Branislav Katreniak",
            "Jana Katreniaková",
            "Rastislav Královič",
            "Richard Královič",
            "Vladimír Koutný",
            "Dana Pardubská",
            "Tomáš Plachetka",
            "Branislav Rovan"
        ],
        "summary": "The paper investigates a version of the resource allocation problem arising in the wireless networking, namely in the OVSF code reallocation process. In this setting a complete binary tree of a given height $n$ is considered, together with a sequence of requests which have to be served in an online manner. The requests are of two types: an insertion request requires to allocate a complete subtree of a given height, and a deletion request frees a given allocated subtree. In order to serve an insertion request it might be necessary to move some already allocated subtrees to other locations in order to free a large enough subtree. We are interested in the worst case average number of such reallocations needed to serve a request.   It was proved in previous work that the competitive ratio of the optimal online algorithm solving this problem is between 1.5 and O(n). We partially answer the question about its exact value by giving an O(1)-competitive online algorithm.   Same model has been used in the context of memory management systems, and analyzed for the number of reallocations needed to serve a request in the worst case. In this setting, our result is a corresponding amortized analysis.",
        "published": "2007-01-25T11:52:29Z",
        "link": "http://arxiv.org/abs/cs/0701153v1",
        "categories": [
            "cs.DS",
            "cs.NI"
        ]
    },
    {
        "title": "Indexing the Sphere with the Hierarchical Triangular Mesh",
        "authors": [
            "Alexander S. Szalay",
            "Jim Gray",
            "George Fekete",
            "Peter Z. Kunszt",
            "Peter Kukol",
            "Ani Thakar"
        ],
        "summary": "We describe a method to subdivide the surface of a sphere into spherical triangles of similar, but not identical, shapes and sizes. The Hierarchical Triangular Mesh (HTM) is a quad-tree that is particularly good at supporting searches at different resolutions, from arc seconds to hemispheres. The subdivision scheme is universal, providing the basis for addressing and for fast lookups. The HTM provides the basis for an efficient geospatial indexing scheme in relational databases where the data have an inherent location on either the celestial sphere or the Earth. The HTM index is superior to cartographical methods using coordinates with singularities at the poles. We also describe a way to specify surface regions that efficiently represent spherical query areas. This article presents the algorithms used to identify the HTM triangles covering such regions.",
        "published": "2007-01-26T00:04:12Z",
        "link": "http://arxiv.org/abs/cs/0701164v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "The Zones Algorithm for Finding Points-Near-a-Point or Cross-Matching   Spatial Datasets",
        "authors": [
            "Jim Gray",
            "Maria A. Nieto-Santisteban",
            "Alexander S. Szalay"
        ],
        "summary": "Zones index an N-dimensional Euclidian or metric space to efficiently support points-near-a-point queries either within a dataset or between two datasets. The approach uses relational algebra and the B-Tree mechanism found in almost all relational database systems. Hence, the Zones Algorithm gives a portable-relational implementation of points-near-point, spatial cross-match, and self-match queries. This article corrects some mistakes in an earlier article we wrote on the Zones Algorithm and describes some algorithmic improvements. The Appendix includes an implementation of point-near-point, self-match, and cross-match using the USGS city and stream gauge database.",
        "published": "2007-01-26T05:11:20Z",
        "link": "http://arxiv.org/abs/cs/0701171v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "A Prototype for Educational Planning Using Course Constraints to   Simulate Student Populations",
        "authors": [
            "T. Hadzilacos",
            "D. Kalles",
            "D. Koumanakos",
            "V. Mitsionis"
        ],
        "summary": "Distance learning universities usually afford their students the flexibility to advance their studies at their own pace. This can lead to a considerable fluctuation of student populations within a program's courses, possibly affecting the academic viability of a program as well as the related required resources. Providing a method that estimates this population could be of substantial help to university management and academic personnel. We describe how to use course precedence constraints to calculate alternative tuition paths and then use Markov models to estimate future populations. In doing so, we identify key issues of a large scale potential deployment.",
        "published": "2007-01-26T08:32:10Z",
        "link": "http://arxiv.org/abs/cs/0701174v3",
        "categories": [
            "cs.AI",
            "cs.CY",
            "cs.DS",
            "cs.SC"
        ]
    },
    {
        "title": "Graph Operations on Clique-Width Bounded Graphs",
        "authors": [
            "Frank Gurski"
        ],
        "summary": "Clique-width is a well-known graph parameter. Many NP-hard graph problems admit polynomial-time solutions when restricted to graphs of bounded clique-width. The same holds for NLC-width. In this paper we study the behavior of clique-width and NLC-width under various graph operations and graph transformations. We give upper and lower bounds for the clique-width and NLC-width of the modified graphs in terms of the clique-width and NLC-width of the involved graphs.",
        "published": "2007-01-29T14:36:25Z",
        "link": "http://arxiv.org/abs/cs/0701185v3",
        "categories": [
            "cs.DS",
            "cs.DM",
            "E.1; G.2.2"
        ]
    },
    {
        "title": "A New Self-Stabilizing Maximal Matching Algorithm",
        "authors": [
            "Fredrik Manne",
            "Morten Mjelde",
            "Laurence Pilard",
            "Sébastien Tixeuil"
        ],
        "summary": "The maximal matching problem has received considerable attention in the self-stabilizing community. Previous work has given different self-stabilizing algorithms that solves the problem for both the adversarial and fair distributed daemon, the sequential adversarial daemon, as well as the synchronous daemon. In the following we present a single self-stabilizing algorithm for this problem that unites all of these algorithms in that it stabilizes in the same number of moves as the previous best algorithms for the sequential adversarial, the distributed fair, and the synchronous daemon. In addition, the algorithm improves the previous best moves complexities for the distributed adversarial daemon from O(n^2) and O(delta m) to O(m) where n is the number of processes, m is thenumber of edges, and delta is the maximum degree in the graph.",
        "published": "2007-01-30T14:52:37Z",
        "link": "http://arxiv.org/abs/cs/0701189v1",
        "categories": [
            "cs.DS",
            "cs.DC"
        ]
    },
    {
        "title": "Algebraic Signal Processing Theory: Cooley-Tukey Type Algorithms for   DCTs and DSTs",
        "authors": [
            "Markus Pueschel",
            "Jose M. F. Moura"
        ],
        "summary": "This paper presents a systematic methodology based on the algebraic theory of signal processing to classify and derive fast algorithms for linear transforms. Instead of manipulating the entries of transform matrices, our approach derives the algorithms by stepwise decomposition of the associated signal models, or polynomial algebras. This decomposition is based on two generic methods or algebraic principles that generalize the well-known Cooley-Tukey FFT and make the algorithms' derivations concise and transparent. Application to the 16 discrete cosine and sine transforms yields a large class of fast algorithms, many of which have not been found before.",
        "published": "2007-02-04T23:44:34Z",
        "link": "http://arxiv.org/abs/cs/0702025v1",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "F.2.1"
        ]
    },
    {
        "title": "On the variance of subset sum estimation",
        "authors": [
            "Mario Szegedy",
            "Mikkel Thorup"
        ],
        "summary": "For high volume data streams and large data warehouses, sampling is used for efficient approximate answers to aggregate queries over selected subsets. Mathematically, we are dealing with a set of weighted items and want to support queries to arbitrary subset sums. With unit weights, we can compute subset sizes which together with the previous sums provide the subset averages. The question addressed here is which sampling scheme we should use to get the most accurate subset sum estimates.   We present a simple theorem on the variance of subset sum estimation and use it to prove variance optimality and near-optimality of subset sum estimation with different known sampling schemes. This variance is measured as the average over all subsets of any given size. By optimal we mean there is no set of input weights for which any sampling scheme can have a better average variance. Such powerful results can never be established experimentally. The results of this paper are derived mathematically. For example, we show that appropriately weighted systematic sampling is simultaneously optimal for all subset sizes. More standard schemes such as uniform sampling and probability-proportional-to-size sampling with replacement can be arbitrarily bad.   Knowing the variance optimality of different sampling schemes can help deciding which sampling scheme to apply in a given context.",
        "published": "2007-02-05T15:55:41Z",
        "link": "http://arxiv.org/abs/cs/0702029v1",
        "categories": [
            "cs.DS",
            "C.2.3; E.1; F.2; G.3; H.3"
        ]
    },
    {
        "title": "Finding large and small dense subgraphs",
        "authors": [
            "Reid Andersen"
        ],
        "summary": "We consider two optimization problems related to finding dense subgraphs. The densest at-least-k-subgraph problem (DalkS) is to find an induced subgraph of highest average degree among all subgraphs with at least k vertices, and the densest at-most-k-subgraph problem (DamkS) is defined similarly. These problems are related to the well-known densest k-subgraph problem (DkS), which is to find the densest subgraph on exactly k vertices. We show that DalkS can be approximated efficiently, while DamkS is nearly as hard to approximate as the densest k-subgraph problem.",
        "published": "2007-02-05T19:29:38Z",
        "link": "http://arxiv.org/abs/cs/0702032v1",
        "categories": [
            "cs.DS",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Deciding k-colourability of $P_5$-free graphs in polynomial time",
        "authors": [
            "Chính T. Hoàng",
            "Marcin Kamiński",
            "Vadim Lozin",
            "J. Sawada",
            "X. Shu"
        ],
        "summary": "The problem of computing the chromatic number of a $P_5$-free graph is known to be NP-hard. In contrast to this negative result, we show that determining whether or not a $P_5$-free graph admits a $k$-colouring, for each fixed number of colours $k$, can be done in polynomial time. If such a colouring exists, our algorithm produces it.",
        "published": "2007-02-07T15:29:32Z",
        "link": "http://arxiv.org/abs/cs/0702043v1",
        "categories": [
            "cs.DS",
            "G.2.2"
        ]
    },
    {
        "title": "Parameterized Algorithms for Directed Maximum Leaf Problems",
        "authors": [
            "Noga Alon",
            "Fedor Fomin",
            "Gregory Gutin",
            "Michael Krivelevich",
            "Saket Saurabh"
        ],
        "summary": "We prove that finding a rooted subtree with at least $k$ leaves in a digraph is a fixed parameter tractable problem. A similar result holds for finding rooted spanning trees with many leaves in digraphs from a wide family $\\cal L$ that includes all strong and acyclic digraphs. This settles completely an open question of Fellows and solves another one for digraphs in $\\cal L$. Our algorithms are based on the following combinatorial result which can be viewed as a generalization of many results for a `spanning tree with many leaves' in the undirected case, and which is interesting on its own: If a digraph $D\\in \\cal L$ of order $n$ with minimum in-degree at least 3 contains a rooted spanning tree, then $D$ contains one with at least $(n/2)^{1/5}-1$ leaves.",
        "published": "2007-02-08T18:25:08Z",
        "link": "http://arxiv.org/abs/cs/0702049v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Nash equilibria in Voronoi games on graphs",
        "authors": [
            "Christoph Durr",
            "Nguyen Kim Thang"
        ],
        "summary": "In this paper we study a game where every player is to choose a vertex (facility) in a given undirected graph. All vertices (customers) are then assigned to closest facilities and a player's payoff is the number of customers assigned to it. We show that deciding the existence of a Nash equilibrium for a given graph is NP-hard which to our knowledge is the first result of this kind for a zero-sum game. We also introduce a new measure, the social cost discrepancy, defined as the ratio of the costs between the worst and the best Nash equilibria. We show that the social cost discrepancy in our game is Omega(sqrt(n/k)) and O(sqrt(kn)), where n is the number of vertices and k the number of players.",
        "published": "2007-02-09T12:12:17Z",
        "link": "http://arxiv.org/abs/cs/0702054v2",
        "categories": [
            "cs.GT",
            "cs.DS"
        ]
    },
    {
        "title": "A probabilistic analysis of a leader election algorithm",
        "authors": [
            "Hanene Mohamed"
        ],
        "summary": "A {\\em leader election} algorithm is an elimination process that divides recursively into tow subgroups an initial group of n items, eliminates one subgroup and continues the procedure until a subgroup is of size 1. In this paper the biased case is analyzed. We are interested in the {\\em cost} of the algorithm, i.e. the number of operations needed until the algorithm stops. Using a probabilistic approach, the asymptotic behavior of the algorithm is shown to be related to the behavior of a hitting time of two random sequences on [0,1].",
        "published": "2007-02-09T15:16:48Z",
        "link": "http://arxiv.org/abs/cs/0702056v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "An Efficient Algorithm to Recognize Locally Equivalent Graphs in   Non-Binary Case",
        "authors": [
            "Mohsen Bahramgiri",
            "Salman Beigi"
        ],
        "summary": "Let $v$ be a vertex of a graph $G$. By the local complementation of $G$ at $v$ we mean to complement the subgraph induced by the neighbors of $v$. This operator can be generalized as follows. Assume that, each edge of $G$ has a label in the finite field $\\mathbf{F}_q$. Let $(g_{ij})$ be set of labels ($g_{ij}$ is the label of edge $ij$). We define two types of operators. For the first one, let $v$ be a vertex of $G$ and $a\\in \\mathbf{F}_q$, and obtain the graph with labels $g'_{ij}=g_{ij}+ag_{vi}g_{vj}$. For the second, if $0\\neq b\\in \\mathbf{F}_q$ the resulted graph is a graph with labels $g''_{vi}=bg_{vi}$ and $g''_{ij}=g_{ij}$, for $i,j$ unequal to $v$. It is clear that if the field is binary, the operators are just local complementations that we described.   The problem of whether two graphs are equivalent under local complementations has been studied, \\cite{bouchalg}. Here we consider the general case and assuming that $q$ is odd, present the first known efficient algorithm to verify whether two graphs are locally equivalent or not.",
        "published": "2007-02-09T15:42:46Z",
        "link": "http://arxiv.org/abs/cs/0702057v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Neighbor selection and hitting probability in small-world graphs",
        "authors": [
            "Oskar Sandberg"
        ],
        "summary": "Small-world graphs, which combine randomized and structured elements, are seen as prevalent in nature. Jon Kleinberg showed that in some graphs of this type it is possible to route, or navigate, between vertices in few steps even with very little knowledge of the graph itself. In an attempt to understand how such graphs arise we introduce a different criterion for graphs to be navigable in this sense, relating the neighbor selection of a vertex to the hitting probability of routed walks. In several models starting from both discrete and continuous settings, this can be shown to lead to graphs with the desired properties. It also leads directly to an evolutionary model for the creation of similar graphs by the stepwise rewiring of the edges, and we conjecture, supported by simulations, that these too are navigable.",
        "published": "2007-02-12T12:16:35Z",
        "link": "http://arxiv.org/abs/math/0702325v2",
        "categories": [
            "math.PR",
            "cs.DS",
            "60C05 (Primary) 68R10, 68W20 (Secondary)"
        ]
    },
    {
        "title": "A Local Algorithm for Finding Dense Subgraphs",
        "authors": [
            "Reid Andersen"
        ],
        "summary": "We present a local algorithm for finding dense subgraphs of bipartite graphs, according to the definition of density proposed by Kannan and Vinay. Our algorithm takes as input a bipartite graph with a specified starting vertex, and attempts to find a dense subgraph near that vertex. We prove that for any subgraph S with k vertices and density theta, there are a significant number of starting vertices within S for which our algorithm produces a subgraph S' with density theta / O(log n) on at most O(D k^2) vertices, where D is the maximum degree. The running time of the algorithm is O(D k^2), independent of the number of vertices in the graph.",
        "published": "2007-02-13T23:41:46Z",
        "link": "http://arxiv.org/abs/cs/0702078v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Fast Computation of Small Cuts via Cycle Space Sampling",
        "authors": [
            "David Pritchard",
            "Ramakrishna Thurimella"
        ],
        "summary": "We describe a new sampling-based method to determine cuts in an undirected graph. For a graph (V, E), its cycle space is the family of all subsets of E that have even degree at each vertex. We prove that with high probability, sampling the cycle space identifies the cuts of a graph. This leads to simple new linear-time sequential algorithms for finding all cut edges and cut pairs (a set of 2 edges that form a cut) of a graph.   In the model of distributed computing in a graph G=(V, E) with O(log V)-bit messages, our approach yields faster algorithms for several problems. The diameter of G is denoted by Diam, and the maximum degree by Delta. We obtain simple O(Diam)-time distributed algorithms to find all cut edges, 2-edge-connected components, and cut pairs, matching or improving upon previous time bounds. Under natural conditions these new algorithms are universally optimal --- i.e. a Omega(Diam)-time lower bound holds on every graph. We obtain a O(Diam+Delta/log V)-time distributed algorithm for finding cut vertices; this is faster than the best previous algorithm when Delta, Diam = O(sqrt(V)). A simple extension of our work yields the first distributed algorithm with sub-linear time for 3-edge-connected components. The basic distributed algorithms are Monte Carlo, but they can be made Las Vegas without increasing the asymptotic complexity.   In the model of parallel computing on the EREW PRAM our approach yields a simple algorithm with optimal time complexity O(log V) for finding cut pairs and 3-edge-connected components.",
        "published": "2007-02-20T03:00:33Z",
        "link": "http://arxiv.org/abs/cs/0702113v5",
        "categories": [
            "cs.DC",
            "cs.DS",
            "F.1.2; F.2.2; G.2.2; G.3"
        ]
    },
    {
        "title": "An Optimal Linear Time Algorithm for Quasi-Monotonic Segmentation",
        "authors": [
            "Daniel Lemire",
            "Martin Brooks",
            "Yuhong Yan"
        ],
        "summary": "Monotonicity is a simple yet significant qualitative characteristic. We consider the problem of segmenting an array in up to K segments. We want segments to be as monotonic as possible and to alternate signs. We propose a quality metric for this problem, present an optimal linear time algorithm based on novel formalism, and compare experimentally its performance to a linear time top-down regression algorithm. We show that our algorithm is faster and more accurate. Applications include pattern recognition and qualitative modeling.",
        "published": "2007-02-24T02:29:36Z",
        "link": "http://arxiv.org/abs/cs/0702142v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "Matrix norms and rapid mixing for spin systems",
        "authors": [
            "Martin Dyer",
            "Leslie Ann Goldberg",
            "Mark Jerrum"
        ],
        "summary": "We give a systematic development of the application of matrix norms to rapid mixing in spin systems. We show that rapid mixing of both random update Glauber dynamics and systematic scan Glauber dynamics occurs if any matrix norm of the associated dependency matrix is less than 1. We give improved analysis for the case in which the diagonal of the dependency matrix is $\\mathbf{0}$ (as in heat bath dynamics). We apply the matrix norm methods to random update and systematic scan Glauber dynamics for coloring various classes of graphs. We give a general method for estimating a norm of a symmetric nonregular matrix. This leads to improved mixing times for any class of graphs which is hereditary and sufficiently sparse including several classes of degree-bounded graphs such as nonregular graphs, trees, planar graphs and graphs with given tree-width and genus.",
        "published": "2007-02-25T13:25:16Z",
        "link": "http://arxiv.org/abs/math/0702744v3",
        "categories": [
            "math.PR",
            "cs.DS",
            "15A60, 60J10, 68W20, 68W40, 82B20 (Primary)"
        ]
    },
    {
        "title": "Succinct Sampling on Streams",
        "authors": [
            "Vladimir Braverman",
            "Rafail Ostrovsky",
            "Carlo Zaniolo"
        ],
        "summary": "A streaming model is one where data items arrive over long period of time, either one item at a time or in bursts. Typical tasks include computing various statistics over a sliding window of some fixed time-horizon. What makes the streaming model interesting is that as the time progresses, old items expire and new ones arrive. One of the simplest and central tasks in this model is sampling. That is, the task of maintaining up to $k$ uniformly distributed items from a current time-window as old items expire and new ones arrive. We call sampling algorithms {\\bf succinct} if they use provably optimal (up to constant factors) {\\bf worst-case} memory to maintain $k$ items (either with or without replacement). We stress that in many applications structures that have {\\em expected} succinct representation as the time progresses are not sufficient, as small probability events eventually happen with probability 1. Thus, in this paper we ask the following question: are Succinct Sampling on Streams (or $S^3$-algorithms)possible, and if so for what models? Perhaps somewhat surprisingly, we show that $S^3$-algorithms are possible for {\\em all} variants of the problem mentioned above, i.e. both with and without replacement and both for one-at-a-time and bursty arrival models. Finally, we use $S^3$ algorithms to solve various problems in sliding windows model, including frequency moments, counting triangles, entropy and density estimations. For these problems we present \\emph{first} solutions with provable worst-case memory guarantees.",
        "published": "2007-02-25T17:20:48Z",
        "link": "http://arxiv.org/abs/cs/0702151v3",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Analysis of Steiner subtrees of Random Trees for Traceroute Algorithms",
        "authors": [
            "Fabrice Guillemin",
            "Philippe Robert"
        ],
        "summary": "We consider in this paper the problem of discovering, via a traceroute algorithm, the topology of a network, whose graph is spanned by an infinite branching process. A subset of nodes is selected according to some criterion. As a measure of efficiency of the algorithm, the Steiner distance of the selected nodes, i.e. the size of the spanning sub-tree of these nodes, is investigated. For the selection of nodes, two criteria are considered: A node is randomly selected with a probability, which is either independent of the depth of the node (uniform model) or else in the depth biased model, is exponentially decaying with respect to its depth. The limiting behavior the size of the discovered subtree is investigated for both models.",
        "published": "2007-02-27T13:42:28Z",
        "link": "http://arxiv.org/abs/cs/0702156v2",
        "categories": [
            "cs.NI",
            "cs.DS"
        ]
    },
    {
        "title": "Perfect Hashing for Data Management Applications",
        "authors": [
            "Fabiano C. Botelho",
            "Rasmus Pagh",
            "Nivio Ziviani"
        ],
        "summary": "Perfect hash functions can potentially be used to compress data in connection with a variety of data management tasks. Though there has been considerable work on how to construct good perfect hash functions, there is a gap between theory and practice among all previous methods on minimal perfect hashing. On one side, there are good theoretical results without experimentally proven practicality for large key sets. On the other side, there are the theoretically analyzed time and space usage algorithms that assume that truly random hash functions are available for free, which is an unrealistic assumption. In this paper we attempt to bridge this gap between theory and practice, using a number of techniques from the literature to obtain a novel scheme that is theoretically well-understood and at the same time achieves an order-of-magnitude increase in performance compared to previous ``practical'' methods. This improvement comes from a combination of a novel, theoretically optimal perfect hashing scheme that greatly simplifies previous methods, and the fact that our algorithm is designed to make good use of the memory hierarchy. We demonstrate the scalability of our algorithm by considering a set of over one billion URLs from the World Wide Web of average length 64, for which we construct a minimal perfect hash function on a commodity PC in a little more than 1 hour. Our scheme produces minimal perfect hash functions using slightly more than 3 bits per key. For perfect hash functions in the range $\\{0,...,2n-1\\}$ the space usage drops to just over 2 bits per key (i.e., one bit more than optimal for representing the key). This is significantly below of what has been achieved previously for very large values of $n$.",
        "published": "2007-02-27T20:56:41Z",
        "link": "http://arxiv.org/abs/cs/0702159v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "E.1"
        ]
    },
    {
        "title": "Embedding Graphs into the Extended Grid",
        "authors": [
            "Michael D. Coury"
        ],
        "summary": "Let $G=(V,E)$ be an arbitrary undirected source graph to be embedded in a target graph $EM$, the extended grid with vertices on integer grid points and edges to nearest and next-nearest neighbours. We present an algorithm showing how to embed $G$ into $EM$ in both time and space $O(|V|^2)$ using the new notions of islands and bridges. An island is a connected subgraph in the target graph which is mapped from exactly one vertex in the source graph while a bridge is an edge between two islands which is mapped from exactly one edge in the source graph. This work is motivated by real industrial applications in the field of quantum computing and a need to efficiently embed source graphs in the extended grid.",
        "published": "2007-02-28T22:37:52Z",
        "link": "http://arxiv.org/abs/cs/0703001v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "G.2.2"
        ]
    },
    {
        "title": "XORSAT: An Efficient Algorithm for the DIMACS 32-bit Parity Problem",
        "authors": [
            "Jing-Chao Chen"
        ],
        "summary": "The DIMACS 32-bit parity problem is a satisfiability (SAT) problem hard to solve. So far, EqSatz by Li is the only solver which can solve this problem. However, This solver is very slow. It is reported that it spent 11855 seconds to solve a par32-5 instance on a Maxintosh G3 300 MHz. The paper introduces a new solver, XORSAT, which splits the original problem into two parts: structured part and random part, and then solves separately them with WalkSAT and an XOR equation solver. Based our empirical observation, XORSAT is surprisingly fast, which is approximately 1000 times faster than EqSatz. For a par32-5 instance, XORSAT took 2.9 seconds, while EqSatz took 2844 seconds on Intel Pentium IV 2.66GHz CPU. We believe that this method significantly different from traditional methods is also useful beyond this domain.",
        "published": "2007-03-02T01:38:16Z",
        "link": "http://arxiv.org/abs/cs/0703006v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "An optimal bifactor approximation algorithm for the metric uncapacitated   facility location problem",
        "authors": [
            "Jaroslaw Byrka",
            "Karen Aardal"
        ],
        "summary": "We obtain a 1.5-approximation algorithm for the metric uncapacitated facility location problem (UFL), which improves on the previously best known 1.52-approximation algorithm by Mahdian, Ye and Zhang. Note, that the approximability lower bound by Guha and Khuller is 1.463.   An algorithm is a {\\em ($\\lambda_f$,$\\lambda_c$)-approximation algorithm} if the solution it produces has total cost at most $\\lambda_f \\cdot F^* + \\lambda_c \\cdot C^*$, where $F^*$ and $C^*$ are the facility and the connection cost of an optimal solution. Our new algorithm, which is a modification of the $(1+2/e)$-approximation algorithm of Chudak and Shmoys, is a (1.6774,1.3738)-approximation algorithm for the UFL problem and is the first one that touches the approximability limit curve $(\\gamma_f, 1+2e^{-\\gamma_f})$ established by Jain, Mahdian and Saberi. As a consequence, we obtain the first optimal approximation algorithm for instances dominated by connection costs. When combined with a (1.11,1.7764)-approximation algorithm proposed by Jain et al., and later analyzed by Mahdian et al., we obtain the overall approximation guarantee of 1.5 for the metric UFL problem. We also describe how to use our algorithm to improve the approximation ratio for the 3-level version of UFL.",
        "published": "2007-03-02T14:49:57Z",
        "link": "http://arxiv.org/abs/cs/0703010v2",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "NLC-2 graph recognition and isomorphism",
        "authors": [
            "Vincent Limouzy",
            "Fabien De Montgolfier",
            "Michaël Rao"
        ],
        "summary": "NLC-width is a variant of clique-width with many application in graph algorithmic. This paper is devoted to graphs of NLC-width two. After giving new structural properties of the class, we propose a $O(n^2 m)$-time algorithm, improving Johansson's algorithm \\cite{Johansson00}. Moreover, our alogrithm is simple to understand. The above properties and algorithm allow us to propose a robust $O(n^2 m)$-time isomorphism algorithm for NLC-2 graphs. As far as we know, it is the first polynomial-time algorithm.",
        "published": "2007-03-03T06:44:57Z",
        "link": "http://arxiv.org/abs/cs/0703013v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "The Stackelberg Minimum Spanning Tree Game",
        "authors": [
            "Jean Cardinal",
            "Erik D. Demaine",
            "Samuel Fiorini",
            "Gwenaël Joret",
            "Stefan Langerman",
            "Ilan Newman",
            "Oren Weimann"
        ],
        "summary": "We consider a one-round two-player network pricing game, the Stackelberg Minimum Spanning Tree game or StackMST.   The game is played on a graph (representing a network), whose edges are colored either red or blue, and where the red edges have a given fixed cost (representing the competitor's prices). The first player chooses an assignment of prices to the blue edges, and the second player then buys the cheapest possible minimum spanning tree, using any combination of red and blue edges. The goal of the first player is to maximize the total price of purchased blue edges. This game is the minimum spanning tree analog of the well-studied Stackelberg shortest-path game.   We analyze the complexity and approximability of the first player's best strategy in StackMST. In particular, we prove that the problem is APX-hard even if there are only two different red costs, and give an approximation algorithm whose approximation ratio is at most $\\min \\{k,1+\\ln b,1+\\ln W\\}$, where $k$ is the number of distinct red costs, $b$ is the number of blue edges, and $W$ is the maximum ratio between red costs. We also give a natural integer linear programming formulation of the problem, and show that the integrality gap of the fractional relaxation asymptotically matches the approximation guarantee of our algorithm.",
        "published": "2007-03-05T09:46:26Z",
        "link": "http://arxiv.org/abs/cs/0703019v3",
        "categories": [
            "cs.GT",
            "cs.DS"
        ]
    },
    {
        "title": "Counting preimages of TCP reordering patterns",
        "authors": [
            "Anders Hansson",
            "Gabriel Istrate"
        ],
        "summary": "Packet reordering is an important property of network traffic that should be captured by analytical models of the Transmission Control Protocol (TCP). We study a combinatorial problem motivated by RESTORED, a TCP modeling methodology that incorporates information about packet dynamics. A significant component of this model is a many-to-one mapping B that transforms sequences of packet IDs into buffer sequences, in a manner that is compatible with TCP semantics. We show that the following hold:   1. There exists a linear time algorithm that, given a buffer sequence W of length n, decides whether there exists a permutation A of 1,2,..., n such that $A\\in B^{-1}(W)$ (and constructs such a permutation, when it exists).   2. The problem of counting the number of permutations in $B^{-1}(W)$ has a polynomial time algorithm.   We also show how to extend these results to sequences of IDs that contain repeated packets.",
        "published": "2007-03-05T13:38:45Z",
        "link": "http://arxiv.org/abs/cs/0703020v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "math.CO"
        ]
    },
    {
        "title": "Sampling Eulerian orientations of triangular lattice graphs",
        "authors": [
            "Paidi Creed"
        ],
        "summary": "We consider the problem of sampling from the uniform distribution on the set of Eulerian orientations of subgraphs of the triangular lattice. Although it is known that this can be achieved in polynomial time for any graph, the algorithm studied here is more natural in the context of planar Eulerian graphs. We analyse the mixing time of a Markov chain on the Eulerian orientations of a planar graph which moves between orientations by reversing the edges of directed faces. Using path coupling and the comparison method we obtain a polynomial upper bound on the mixing time of this chain for any solid subgraph of the triangular lattice. By considering the conductance of the chain we show that there exist subgraphs with holes for which the chain will always take an exponential amount of time to converge. Finally, as an additional justification for studying a Markov chain on the set of Eulerian orientations of planar graphs, we show that the problem of counting Eulerian orientations remains #P-complete when restricted to planar graphs.   A preliminary version of this work appeared as an extended abstract in the 2nd Algorithms and Complexity in Durham workshop.",
        "published": "2007-03-07T12:34:03Z",
        "link": "http://arxiv.org/abs/cs/0703031v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Some problems in asymptotic convex geometry and random matrices   motivated by numerical algorithms",
        "authors": [
            "Roman Vershynin"
        ],
        "summary": "The simplex method in Linear Programming motivates several problems of asymptotic convex geometry. We discuss some conjectures and known results in two related directions -- computing the size of projections of high dimensional polytopes and estimating the norms of random matrices and their inverses.",
        "published": "2007-03-19T21:51:50Z",
        "link": "http://arxiv.org/abs/cs/0703093v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.NA",
            "G.1.6; G.1.3"
        ]
    },
    {
        "title": "Polynomial time algorithm for 3-SAT. Examples of use",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "The algorithm checks the propositional formulas for patterns of unsatisfiability.",
        "published": "2007-03-21T06:46:09Z",
        "link": "http://arxiv.org/abs/cs/0703098v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.LO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Approximation Algorithms for Multiprocessor Scheduling under Uncertainty",
        "authors": [
            "Guolong Lin",
            "Rajmohan Rajaraman"
        ],
        "summary": "Motivated by applications in grid computing and project management, we study multiprocessor scheduling in scenarios where there is uncertainty in the successful execution of jobs when assigned to processors. We consider the problem of multiprocessor scheduling under uncertainty, in which we are given n unit-time jobs and m machines, a directed acyclic graph C giving the dependencies among the jobs, and for every job j and machine i, the probability p_{ij} of the successful completion of job j when scheduled on machine i in any given particular step. The goal of the problem is to find a schedule that minimizes the expected makespan, that is, the expected completion time of all the jobs.   The problem of multiprocessor scheduling under uncertainty was introduced by Malewicz and was shown to be NP-hard even when all the jobs are independent. In this paper, we present polynomial-time approximation algorithms for the problem, for special cases of the dag C. We obtain an O(log(n))-approximation for the case of independent jobs, an O(log(m)log(n)log(n+m)/loglog(n+m))-approximation when C is a collection of disjoint chains, an O(log(m)log^2(n))-approximation when C is a collection of directed out- or in-trees, and an O(log(m)log^2(n)log(n+m)/loglog(n+m))-approximation when C is a directed forest.",
        "published": "2007-03-21T20:35:40Z",
        "link": "http://arxiv.org/abs/cs/0703100v1",
        "categories": [
            "cs.DC",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Tag-Cloud Drawing: Algorithms for Cloud Visualization",
        "authors": [
            "Owen Kaser",
            "Daniel Lemire"
        ],
        "summary": "Tag clouds provide an aggregate of tag-usage statistics. They are typically sent as in-line HTML to browsers. However, display mechanisms suited for ordinary text are not ideal for tags, because font sizes may vary widely on a line. As well, the typical layout does not account for relationships that may be known between tags. This paper presents models and algorithms to improve the display of tag clouds that consist of in-line HTML, as well as algorithms that use nested tables to achieve a more general 2-dimensional layout in which tag relationships are considered. The first algorithms leverage prior work in typesetting and rectangle packing, whereas the second group of algorithms leverage prior work in Electronic Design Automation. Experiments show our algorithms can be efficiently implemented and perform well.",
        "published": "2007-03-22T14:54:48Z",
        "link": "http://arxiv.org/abs/cs/0703109v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Structure induction by lossless graph compression",
        "authors": [
            "Leonid Peshkin"
        ],
        "summary": "This work is motivated by the necessity to automate the discovery of structure in vast and evergrowing collection of relational data commonly represented as graphs, for example genomic networks. A novel algorithm, dubbed Graphitour, for structure induction by lossless graph compression is presented and illustrated by a clear and broadly known case of nested structure in a DNA molecule. This work extends to graphs some well established approaches to grammatical inference previously applied only to strings. The bottom-up graph compression problem is related to the maximum cardinality (non-bipartite) maximum cardinality matching problem. The algorithm accepts a variety of graph types including directed graphs and graphs with labeled nodes and arcs. The resulting structure could be used for representation and classification of graphs.",
        "published": "2007-03-27T05:46:31Z",
        "link": "http://arxiv.org/abs/cs/0703132v1",
        "categories": [
            "cs.DS",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "I.2.6; G.2.2; E.1; E.4; F.4.2; G.2.3; I.3.5; I.4.2; I.5.3; J.3"
        ]
    },
    {
        "title": "Computing Good Nash Equilibria in Graphical Games",
        "authors": [
            "Edith Elkind",
            "Leslie Ann Goldberg",
            "Paul W. Goldberg"
        ],
        "summary": "This paper addresses the problem of fair equilibrium selection in graphical games. Our approach is based on the data structure called the {\\em best response policy}, which was proposed by Kearns et al. \\cite{kls} as a way to represent all Nash equilibria of a graphical game. In \\cite{egg}, it was shown that the best response policy has polynomial size as long as the underlying graph is a path. In this paper, we show that if the underlying graph is a bounded-degree tree and the best response policy has polynomial size then there is an efficient algorithm which constructs a Nash equilibrium that guarantees certain payoffs to all participants. Another attractive solution concept is a Nash equilibrium that maximizes the social welfare. We show that, while exactly computing the latter is infeasible (we prove that solving this problem may involve algebraic numbers of an arbitrarily high degree), there exists an FPTAS for finding such an equilibrium as long as the best response policy has polynomial size. These two algorithms can be combined to produce Nash equilibria that satisfy various fairness criteria.",
        "published": "2007-03-27T16:15:54Z",
        "link": "http://arxiv.org/abs/cs/0703133v1",
        "categories": [
            "cs.GT",
            "cs.DS",
            "cs.MA"
        ]
    },
    {
        "title": "The Simultaneous Triple Product Property and Group-theoretic Results for   the Exponent of Matrix Multiplication",
        "authors": [
            "Sandeep Murthy"
        ],
        "summary": "We describe certain special consequences of certain elementary methods from group theory for studying the algebraic complexity of matrix multiplication, as developed by H. Cohn, C. Umans et. al. in 2003 and 2005. The measure of complexity here is the exponent of matrix multiplication, a real parameter between 2 and 3, which has been conjectured to be 2. More specifically, a finite group may simultaneously \"realize\" several independent matrix multiplications via its regular algebra if it has a family of triples of \"index\" subsets which satisfy the so-called simultaneous triple product property (STPP), in which case the complexity of these several multiplications does not exceed the rank (complexity) of the algebra. This leads to bounds for the exponent in terms of the size of the group and the sizes of its STPP triples, as well as the dimensions of its distinct irreducible representations. Wreath products of Abelian with symmetric groups appear especially important, in this regard, and we give an example of such a group which shows that the exponent is less than 2.84, and could be possibly be as small as 2.02 depending on the number of simultaneous matrix multiplications it realizes.",
        "published": "2007-03-29T02:55:17Z",
        "link": "http://arxiv.org/abs/cs/0703145v4",
        "categories": [
            "cs.DS",
            "cs.CC",
            "math.GR",
            "F.2.1"
        ]
    },
    {
        "title": "A Polynomial Time Algorithm for SAT",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Article presents the compatibility matrix method and illustrates it with the application to P vs NP problem. The method is a generalization of descriptive geometry: in the method, we draft problems and solve them utilizing the image creation technique. The method reveals: P = NP = PSPACE",
        "published": "2007-03-29T07:36:30Z",
        "link": "http://arxiv.org/abs/cs/0703146v4",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.LO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Type-II/III DCT/DST algorithms with reduced number of arithmetic   operations",
        "authors": [
            "Xuancheng Shao",
            "Steven G. Johnson"
        ],
        "summary": "We present algorithms for the discrete cosine transform (DCT) and discrete sine transform (DST), of types II and III, that achieve a lower count of real multiplications and additions than previously published algorithms, without sacrificing numerical accuracy. Asymptotically, the operation count is reduced from ~ 2N log_2 N to ~ (17/9) N log_2 N for a power-of-two transform size N. Furthermore, we show that a further N multiplications may be saved by a certain rescaling of the inputs or outputs, generalizing a well-known technique for N=8 by Arai et al. These results are derived by considering the DCT to be a special case of a DFT of length 4N, with certain symmetries, and then pruning redundant operations from a recent improved fast Fourier transform algorithm (based on a recursive rescaling of the conjugate-pair split radix algorithm). The improved algorithms for DCT-III, DST-II, and DST-III follow immediately from the improved count for the DCT-II.",
        "published": "2007-03-30T00:53:48Z",
        "link": "http://arxiv.org/abs/cs/0703150v2",
        "categories": [
            "cs.NA",
            "cs.DS",
            "cs.MS",
            "F.2.1"
        ]
    },
    {
        "title": "Sparse Hypergraphs and Pebble Game Algorithms",
        "authors": [
            "Ileana Streinu",
            "Louis Theran"
        ],
        "summary": "A hypergraph $G=(V,E)$ is $(k,\\ell)$-sparse if no subset $V'\\subset V$ spans more than $k|V'|-\\ell$ hyperedges. We characterize $(k,\\ell)$-sparse hypergraphs in terms of graph theoretic, matroidal and algorithmic properties. We extend several well-known theorems of Haas, Lov{\\'{a}}sz, Nash-Williams, Tutte, and White and Whiteley, linking arboricity of graphs to certain counts on the number of edges. We also address the problem of finding lower-dimensional representations of sparse hypergraphs, and identify a critical behaviour in terms of the sparsity parameters $k$ and $\\ell$. Our constructions extend the pebble games of Lee and Streinu from graphs to hypergraphs.",
        "published": "2007-03-30T14:14:58Z",
        "link": "http://arxiv.org/abs/math/0703921v1",
        "categories": [
            "math.CO",
            "cs.DS",
            "05C65; 05C85; 68R10; 05B35"
        ]
    },
    {
        "title": "On Computing the Distinguishing Numbers of Planar Graphs and Beyond: a   Counting Approach",
        "authors": [
            "V. Arvind",
            "Christine T. Cheng",
            "Nikhil R. Devanur"
        ],
        "summary": "A vertex k-labeling of graph G is distinguishing if the only automorphism that preserves the labels of G is the identity map. The distinguishing number of G, D(G), is the smallest integer k for which G has a distinguishing k-labeling. In this paper, we apply the principle of inclusion-exclusion and develop recursive formulas to count the number of inequivalent distinguishing k-labelings of a graph. Along the way, we prove that the distinguishing number of a planar graph can be computed in time polynomial in the size of the graph.}",
        "published": "2007-03-30T16:50:09Z",
        "link": "http://arxiv.org/abs/math/0703927v1",
        "categories": [
            "math.CO",
            "cs.DS",
            "05C78, 05C85"
        ]
    },
    {
        "title": "On-line Viterbi Algorithm and Its Relationship to Random Walks",
        "authors": [
            "Rastislav Šrámek",
            "Broňa Brejová",
            "Tomáš Vinař"
        ],
        "summary": "In this paper, we introduce the on-line Viterbi algorithm for decoding hidden Markov models (HMMs) in much smaller than linear space. Our analysis on two-state HMMs suggests that the expected maximum memory used to decode sequence of length $n$ with $m$-state HMM can be as low as $\\Theta(m\\log n)$, without a significant slow-down compared to the classical Viterbi algorithm. Classical Viterbi algorithm requires $O(mn)$ space, which is impractical for analysis of long DNA sequences (such as complete human genome chromosomes) and for continuous data streams. We also experimentally demonstrate the performance of the on-line Viterbi algorithm on a simple HMM for gene finding on both simulated and real DNA sequences.",
        "published": "2007-03-31T23:52:33Z",
        "link": "http://arxiv.org/abs/0704.0062v1",
        "categories": [
            "cs.DS",
            "G.3; E.1; F.1.2; J.3"
        ]
    },
    {
        "title": "Inapproximability of Maximum Weighted Edge Biclique and Its Applications",
        "authors": [
            "Jinsong Tan"
        ],
        "summary": "Given a bipartite graph $G = (V_1,V_2,E)$ where edges take on {\\it both} positive and negative weights from set $\\mathcal{S}$, the {\\it maximum weighted edge biclique} problem, or $\\mathcal{S}$-MWEB for short, asks to find a bipartite subgraph whose sum of edge weights is maximized. This problem has various applications in bioinformatics, machine learning and databases and its (in)approximability remains open. In this paper, we show that for a wide range of choices of $\\mathcal{S}$, specifically when $| \\frac{\\min\\mathcal{S}} {\\max \\mathcal{S}} | \\in \\Omega(\\eta^{\\delta-1/2}) \\cap O(\\eta^{1/2-\\delta})$ (where $\\eta = \\max\\{|V_1|, |V_2|\\}$, and $\\delta \\in (0,1/2]$), no polynomial time algorithm can approximate $\\mathcal{S}$-MWEB within a factor of $n^{\\epsilon}$ for some $\\epsilon > 0$ unless $\\mathsf{RP = NP}$. This hardness result gives justification of the heuristic approaches adopted for various applied problems in the aforementioned areas, and indicates that good approximation algorithms are unlikely to exist. Specifically, we give two applications by showing that: 1) finding statistically significant biclusters in the SAMBA model, proposed in \\cite{Tan02} for the analysis of microarray data, is $n^{\\epsilon}$-inapproximable; and 2) no polynomial time algorithm exists for the Minimum Description Length with Holes problem \\cite{Bu05} unless $\\mathsf{RP=NP}$.",
        "published": "2007-04-03T21:39:11Z",
        "link": "http://arxiv.org/abs/0704.0468v2",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.2.0"
        ]
    },
    {
        "title": "Optimal Synthesis of Multiple Algorithms",
        "authors": [
            "Kerry M. Soileau"
        ],
        "summary": "In this paper we give a definition of \"algorithm,\" \"finite algorithm,\" \"equivalent algorithms,\" and what it means for a single algorithm to dominate a set of algorithms. We define a derived algorithm which may have a smaller mean execution time than any of its component algorithms. We give an explicit expression for the mean execution time (when it exists) of the derived algorithm. We give several illustrative examples of derived algorithms with two component algorithms. We include mean execution time solutions for two-algorithm processors whose joint density of execution times are of several general forms. For the case in which the joint density for a two-algorithm processor is a step function, we give a maximum-likelihood estimation scheme with which to analyze empirical processing time data.",
        "published": "2007-04-05T19:47:54Z",
        "link": "http://arxiv.org/abs/0704.0788v1",
        "categories": [
            "cs.DS",
            "cs.PF"
        ]
    },
    {
        "title": "P-adic arithmetic coding",
        "authors": [
            "Anatoly Rodionov",
            "Sergey Volkov"
        ],
        "summary": "A new incremental algorithm for data compression is presented. For a sequence of input symbols algorithm incrementally constructs a p-adic integer number as an output. Decoding process starts with less significant part of a p-adic integer and incrementally reconstructs a sequence of input symbols. Algorithm is based on certain features of p-adic numbers and p-adic norm. p-adic coding algorithm may be considered as of generalization a popular compression technique - arithmetic coding algorithms. It is shown that for p = 2 the algorithm works as integer variant of arithmetic coding; for a special class of models it gives exactly the same codes as Huffman's algorithm, for another special model and a specific alphabet it gives Golomb-Rice codes.",
        "published": "2007-04-06T02:30:42Z",
        "link": "http://arxiv.org/abs/0704.0834v1",
        "categories": [
            "cs.DS",
            "H.1.1"
        ]
    },
    {
        "title": "Fast paths in large-scale dynamic road networks",
        "authors": [
            "Giacomo Nannicini",
            "Philippe Baptiste",
            "Gilles Barbier",
            "Daniel Krob",
            "Leo Liberti"
        ],
        "summary": "Efficiently computing fast paths in large scale dynamic road networks (where dynamic traffic information is known over a part of the network) is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to GPS terminal enabled vehicles. The heuristic solution method we propose is based on a highway hierarchy-based shortest path algorithm for static large-scale networks; we maintain a static highway hierarchy and perform each query on the dynamically evaluated network.",
        "published": "2007-04-09T07:04:19Z",
        "link": "http://arxiv.org/abs/0704.1068v2",
        "categories": [
            "cs.NI",
            "cs.DS",
            "E.1; G.2.2"
        ]
    },
    {
        "title": "Self-Organization applied to Dynamic Network Layout",
        "authors": [
            "Markus M. Geipel"
        ],
        "summary": "As networks and their structure have become a major field of research, a strong demand for network visualization has emerged. We address this challenge by formalizing the well established spring layout in terms of dynamic equations. We thus open up the design space for new algorithms. Drawing from the knowledge of systems design, we derive a layout algorithm that remedies several drawbacks of the original spring layout. This new algorithm relies on the balancing of two antagonistic forces. We thus call it {\\em arf} for \"attractive and repulsive forces\". It is, as we claim, particularly suited for a dynamic layout of smaller networks ($n < 10^3$). We back this claim with several application examples from on going complex systems research.",
        "published": "2007-04-13T16:45:28Z",
        "link": "http://arxiv.org/abs/0704.1748v5",
        "categories": [
            "physics.comp-ph",
            "cs.DS",
            "nlin.AO"
        ]
    },
    {
        "title": "A Note on the Inapproximability of Correlation Clustering",
        "authors": [
            "Jinsong Tan"
        ],
        "summary": "We consider inapproximability of the correlation clustering problem defined as follows: Given a graph $G = (V,E)$ where each edge is labeled either \"+\" (similar) or \"-\" (dissimilar), correlation clustering seeks to partition the vertices into clusters so that the number of pairs correctly (resp. incorrectly) classified with respect to the labels is maximized (resp. minimized). The two complementary problems are called MaxAgree and MinDisagree, respectively, and have been studied on complete graphs, where every edge is labeled, and general graphs, where some edge might not have been labeled. Natural edge-weighted versions of both problems have been studied as well. Let S-MaxAgree denote the weighted problem where all weights are taken from set S, we show that S-MaxAgree with weights bounded by $O(|V|^{1/2-\\delta})$ essentially belongs to the same hardness class in the following sense: if there is a polynomial time algorithm that approximates S-MaxAgree within a factor of $\\lambda = O(\\log{|V|})$ with high probability, then for any choice of S', S'-MaxAgree can be approximated in polynomial time within a factor of $(\\lambda + \\epsilon)$, where $\\epsilon > 0$ can be arbitrarily small, with high probability. A similar statement also holds for $S-MinDisagree. This result implies it is hard (assuming $NP \\neq RP$) to approximate unweighted MaxAgree within a factor of $80/79-\\epsilon$, improving upon a previous known factor of $116/115-\\epsilon$ by Charikar et. al. \\cite{Chari05}.",
        "published": "2007-04-17T03:52:41Z",
        "link": "http://arxiv.org/abs/0704.2092v2",
        "categories": [
            "cs.LG",
            "cs.DS",
            "F.2.0"
        ]
    },
    {
        "title": "On Verifying and Engineering the Well-gradedness of a Union-closed   Family",
        "authors": [
            "David Eppstein",
            "Jean-Claude Falmagne",
            "Hasan Uzun"
        ],
        "summary": "Current techniques for generating a knowledge space, such as QUERY, guarantees that the resulting structure is closed under union, but not that it satisfies wellgradedness, which is one of the defining conditions for a learning space. We give necessary and sufficient conditions on the base of a union-closed set family that ensures that the family is well-graded. We consider two cases, depending on whether or not the family contains the empty set. We also provide algorithms for efficiently testing these conditions, and for augmenting a set family in a minimal way to one that satisfies these conditions.",
        "published": "2007-04-23T04:37:08Z",
        "link": "http://arxiv.org/abs/0704.2919v3",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.DS",
            "03E05; 03E75; 91E40"
        ]
    },
    {
        "title": "Straggler Identification in Round-Trip Data Streams via Newton's   Identities and Invertible Bloom Filters",
        "authors": [
            "David Eppstein",
            "Michael T. Goodrich"
        ],
        "summary": "We introduce the straggler identification problem, in which an algorithm must determine the identities of the remaining members of a set after it has had a large number of insertion and deletion operations performed on it, and now has relatively few remaining members. The goal is to do this in o(n) space, where n is the total number of identities. The straggler identification problem has applications, for example, in determining the set of unacknowledged packets in a high-bandwidth multicast data stream. We provide a deterministic solution to the straggler identification problem that uses only O(d log n) bits and is based on a novel application of Newton's identities for symmetric polynomials. This solution can identify any subset of d stragglers from a set of n O(log n)-bit identifiers, assuming that there are no false deletions of identities not already in the set. Indeed, we give a lower bound argument that shows that any small-space deterministic solution to the straggler identification problem cannot be guaranteed to handle false deletions. Nevertheless, we show that there is a simple randomized solution using O(d log n log(1/epsilon)) bits that can maintain a multiset and solve the straggler identification problem, tolerating false deletions, where epsilon>0 is a user-defined parameter bounding the probability of an incorrect response. This randomized solution is based on a new type of Bloom filter, which we call the invertible Bloom filter.",
        "published": "2007-04-25T06:59:43Z",
        "link": "http://arxiv.org/abs/0704.3313v3",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Polynomial algorithms for protein similarity search for restricted mRNA   structures",
        "authors": [
            "Frank Gurski"
        ],
        "summary": "In this paper we consider the problem of computing an mRNA sequence of maximal similarity for a given mRNA of secondary structure constraints, introduced by Backofen et al. in [BNS02] denoted as the MRSO problem. The problem is known to be NP-complete for planar associated implied structure graphs of vertex degree at most 3. In [BFHV05] a first polynomial dynamic programming algorithms for MRSO on implied structure graphs with maximum vertex degree 3 of bounded cut-width is shown. We give a simple but more general polynomial dynamic programming solution for the MRSO problem for associated implied structure graphs of bounded clique-width. Our result implies that MRSO is polynomial for graphs of bounded tree-width, co-graphs, $P_4$-sparse graphs, and distance hereditary graphs. Further we conclude that the problem of comparing two solutions for MRSO is hard for the class of problems which can be solved in polynomial time with a number of parallel queries to an oracle in NP.",
        "published": "2007-04-26T08:30:14Z",
        "link": "http://arxiv.org/abs/0704.3496v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Avoiding Rotated Bitboards with Direct Lookup",
        "authors": [
            "Sam Tannous"
        ],
        "summary": "This paper describes an approach for obtaining direct access to the attacked squares of sliding pieces without resorting to rotated bitboards. The technique involves creating four hash tables using the built in hash arrays from an interpreted, high level language. The rank, file, and diagonal occupancy are first isolated by masking the desired portion of the board. The attacked squares are then directly retrieved from the hash tables. Maintaining incrementally updated rotated bitboards becomes unnecessary as does all the updating, mapping and shifting required to access the attacked squares. Finally, rotated bitboard move generation speed is compared with that of the direct hash table lookup method.",
        "published": "2007-04-28T03:11:59Z",
        "link": "http://arxiv.org/abs/0704.3773v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Minimizing Unsatisfaction in Colourful Neighbourhoods",
        "authors": [
            "K. Y. Michael Wong",
            "David Saad"
        ],
        "summary": "Colouring sparse graphs under various restrictions is a theoretical problem of significant practical relevance. Here we consider the problem of maximizing the number of different colours available at the nodes and their neighbourhoods, given a predetermined number of colours. In the analytical framework of a tree approximation, carried out at both zero and finite temperatures, solutions obtained by population dynamics give rise to estimates of the threshold connectivity for the incomplete to complete transition, which are consistent with those of existing algorithms. The nature of the transition as well as the validity of the tree approximation are investigated.",
        "published": "2007-04-29T10:03:00Z",
        "link": "http://arxiv.org/abs/0704.3835v3",
        "categories": [
            "cs.DS",
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "Acyclic Preference Systems in P2P Networks",
        "authors": [
            "Anh-Tuan Gai",
            "Dmitry Lebedev",
            "Fabien Mathieu",
            "Fabien De Montgolfier",
            "Julien Reynier",
            "Laurent Viennot"
        ],
        "summary": "In this work we study preference systems natural for the Peer-to-Peer paradigm. Most of them fall in three categories: global, symmetric and complementary. All these systems share an acyclicity property. As a consequence, they admit a stable (or Pareto efficient) configuration, where no participant can collaborate with better partners than their current ones. We analyze the representation of the such preference systems and show that any acyclic system can be represented with a symmetric mark matrix. This gives a method to merge acyclic preference systems and retain the acyclicity. We also consider such properties of the corresponding collaboration graph, as clustering coefficient and diameter. In particular, studying the example of preferences based on real latency measurements, we observe that its stable configuration is a small-world graph.",
        "published": "2007-04-30T09:26:39Z",
        "link": "http://arxiv.org/abs/0704.3904v2",
        "categories": [
            "cs.DS",
            "cs.GT"
        ]
    },
    {
        "title": "Using Images to create a Hierarchical Grid Spatial Index",
        "authors": [
            "Lukasz A. Machowski",
            "Tshilidzi Marwala"
        ],
        "summary": "This paper presents a hybrid approach to spatial indexing of two dimensional data. It sheds new light on the age old problem by thinking of the traditional algorithms as working with images. Inspiration is drawn from an analogous situation that is found in machine and human vision. Image processing techniques are used to assist in the spatial indexing of the data. A fixed grid approach is used and bins with too many records are sub-divided hierarchically. Search queries are pre-computed for bins that do not contain any data records. This has the effect of dividing the search space up into non rectangular regions which are based on the spatial properties of the data. The bucketing quad tree can be considered as an image with a resolution of two by two for each layer. The results show that this method performs better than the quad tree if there are more divisions per layer. This confirms our suspicions that the algorithm works better if it gets to look at the data with higher resolution images. An elegant class structure is developed where the implementation of concrete spatial indexes for a particular data type merely relies on rendering the data onto an image.",
        "published": "2007-05-02T05:37:32Z",
        "link": "http://arxiv.org/abs/0705.0204v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "More Efficient Algorithms and Analyses for Unequal Letter Cost   Prefix-Free Coding",
        "authors": [
            "Mordecai Golin",
            "Li Jian"
        ],
        "summary": "There is a large literature devoted to the problem of finding an optimal (min-cost) prefix-free code with an unequal letter-cost encoding alphabet of size. While there is no known polynomial time algorithm for solving it optimally there are many good heuristics that all provide additive errors to optimal. The additive error in these algorithms usually depends linearly upon the largest encoding letter size.   This paper was motivated by the problem of finding optimal codes when the encoding alphabet is infinite. Because the largest letter cost is infinite, the previous analyses could give infinite error bounds. We provide a new algorithm that works with infinite encoding alphabets. When restricted to the finite alphabet case, our algorithm often provides better error bounds than the best previous ones known.",
        "published": "2007-05-02T11:23:52Z",
        "link": "http://arxiv.org/abs/0705.0253v2",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT"
        ]
    },
    {
        "title": "Edges and Switches, Tunnels and Bridges",
        "authors": [
            "David Eppstein",
            "Marc van Kreveld",
            "Elena Mumford",
            "Bettina Speckmann"
        ],
        "summary": "Edge casing is a well-known method to improve the readability of drawings of non-planar graphs. A cased drawing orders the edges of each edge crossing and interrupts the lower edge in an appropriate neighborhood of the crossing. Certain orders will lead to a more readable drawing than others. We formulate several optimization criteria that try to capture the concept of a \"good\" cased drawing. Further, we address the algorithmic question of how to turn a given drawing into an optimal cased drawing. For many of the resulting optimization problems, we either find polynomial time algorithms or NP-hardness results.",
        "published": "2007-05-03T06:33:04Z",
        "link": "http://arxiv.org/abs/0705.0413v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Iterative Rounding for the Closest String Problem",
        "authors": [
            "Jing-Chao Chen"
        ],
        "summary": "The closest string problem is an NP-hard problem, whose task is to find a string that minimizes maximum Hamming distance to a given set of strings. This can be reduced to an integer program (IP). However, to date, there exists no known polynomial-time algorithm for IP. In 2004, Meneses et al. introduced a branch-and-bound (B & B) method for solving the IP problem. Their algorithm is not always efficient and has the exponential time complexity. In the paper, we attempt to solve efficiently the IP problem by a greedy iterative rounding technique. The proposed algorithm is polynomial time and much faster than the existing B & B IP for the CSP. If the number of strings is limited to 3, the algorithm is provably at most 1 away from the optimum. The empirical results show that in many cases we can find an exact solution. Even though we fail to find an exact solution, the solution found is very close to exact solution.",
        "published": "2007-05-04T03:01:42Z",
        "link": "http://arxiv.org/abs/0705.0561v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Succinct Indexable Dictionaries with Applications to Encoding $k$-ary   Trees, Prefix Sums and Multisets",
        "authors": [
            "Rajeev Raman",
            "Venkatesh Raman",
            "Srinivasa Rao Satti"
        ],
        "summary": "We consider the {\\it indexable dictionary} problem, which consists of storing a set $S \\subseteq \\{0,...,m-1\\}$ for some integer $m$, while supporting the operations of $\\Rank(x)$, which returns the number of elements in $S$ that are less than $x$ if $x \\in S$, and -1 otherwise; and $\\Select(i)$ which returns the $i$-th smallest element in $S$. We give a data structure that supports both operations in O(1) time on the RAM model and requires ${\\cal B}(n,m) + o(n) + O(\\lg \\lg m)$ bits to store a set of size $n$, where ${\\cal B}(n,m) = \\ceil{\\lg {m \\choose n}}$ is the minimum number of bits required to store any $n$-element subset from a universe of size $m$. Previous dictionaries taking this space only supported (yes/no) membership queries in O(1) time. In the cell probe model we can remove the $O(\\lg \\lg m)$ additive term in the space bound, answering a question raised by Fich and Miltersen, and Pagh.   We present extensions and applications of our indexable dictionary data structure, including:   An information-theoretically optimal representation of a $k$-ary cardinal tree that supports standard operations in constant time,   A representation of a multiset of size $n$ from $\\{0,...,m-1\\}$ in ${\\cal B}(n,m+n) + o(n)$ bits that supports (appropriate generalizations of) $\\Rank$ and $\\Select$ operations in constant time, and   A representation of a sequence of $n$ non-negative integers summing up to $m$ in ${\\cal B}(n,m+n) + o(n)$ bits that supports prefix sum queries in constant time.",
        "published": "2007-05-04T07:47:05Z",
        "link": "http://arxiv.org/abs/0705.0552v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "cs.IT",
            "math.IT",
            "E.1; E.4; F.2.2"
        ]
    },
    {
        "title": "Clustering Co-occurrence of Maximal Frequent Patterns in Streams",
        "authors": [
            "Edgar H. de Graaf",
            "Joost N. Kok",
            "Walter A. Kosters"
        ],
        "summary": "One way of getting a better view of data is using frequent patterns. In this paper frequent patterns are subsets that occur a minimal number of times in a stream of itemsets. However, the discovery of frequent patterns in streams has always been problematic. Because streams are potentially endless it is in principle impossible to say if a pattern is often occurring or not. Furthermore the number of patterns can be huge and a good overview of the structure of the stream is lost quickly. The proposed approach will use clustering to facilitate the analysis of the structure of the stream.   A clustering on the co-occurrence of patterns will give the user an improved view on the structure of the stream. Some patterns might occur so much together that they should form a combined pattern. In this way the patterns in the clustering will be the largest frequent patterns: maximal frequent patterns.   Our approach to decide if patterns occur often together will be based on a method of clustering when only the distance between pairs is known. The number of maximal frequent patterns is much smaller and combined with clustering methods these patterns provide a good view on the structure of the stream.",
        "published": "2007-05-04T10:36:53Z",
        "link": "http://arxiv.org/abs/0705.0588v1",
        "categories": [
            "cs.AI",
            "cs.DS"
        ]
    },
    {
        "title": "Clustering with Lattices in the Analysis of Graph Patterns",
        "authors": [
            "Edgar H. de Graaf",
            "Joost N. Kok",
            "Walter A. Kosters"
        ],
        "summary": "Mining frequent subgraphs is an area of research where we have a given set of graphs (each graph can be seen as a transaction), and we search for (connected) subgraphs contained in many of these graphs. In this work we will discuss techniques used in our framework Lattice2SAR for mining and analysing frequent subgraph data and their corresponding lattice information. Lattice information is provided by the graph mining algorithm gSpan; it contains all supergraph-subgraph relations of the frequent subgraph patterns -- and their supports.   Lattice2SAR is in particular used in the analysis of frequent graph patterns where the graphs are molecules and the frequent subgraphs are fragments. In the analysis of fragments one is interested in the molecules where patterns occur. This data can be very extensive and in this paper we focus on a technique of making it better available by using the lattice information in our clustering. Now we can reduce the number of times the highly compressed occurrence data needs to be accessed by the user. The user does not have to browse all the occurrence data in search of patterns occurring in the same molecules. Instead one can directly see which frequent subgraphs are of interest.",
        "published": "2007-05-04T10:52:28Z",
        "link": "http://arxiv.org/abs/0705.0593v1",
        "categories": [
            "cs.AI",
            "cs.DS"
        ]
    },
    {
        "title": "Computing Minimal Polynomials of Matrices",
        "authors": [
            "Max Neunhoeffer",
            "Cheryl E. Praeger"
        ],
        "summary": "We present and analyse a Monte-Carlo algorithm to compute the minimal polynomial of an $n\\times n$ matrix over a finite field that requires $O(n^3)$ field operations and O(n) random vectors, and is well suited for successful practical implementation. The algorithm, and its complexity analysis, use standard algorithms for polynomial and matrix operations. We compare features of the algorithm with several other algorithms in the literature. In addition we present a deterministic verification procedure which is similarly efficient in most cases but has a worst-case complexity of $O(n^4)$. Finally, we report the results of practical experiments with an implementation of our algorithms in comparison with the current algorithms in the {\\sf GAP} library.",
        "published": "2007-05-07T15:48:12Z",
        "link": "http://arxiv.org/abs/0705.0933v2",
        "categories": [
            "math.RA",
            "cs.DS",
            "15A21;15A15"
        ]
    },
    {
        "title": "Optimal Cache-Oblivious Mesh Layouts",
        "authors": [
            "Michael A. Bender",
            "Bradley C. Kuszmaul",
            "Shang-Hua Teng",
            "Kebin Wang"
        ],
        "summary": "A mesh is a graph that divides physical space into regularly-shaped regions. Meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. In one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. The performance of a mesh update depends on the layout of the mesh in memory.   This paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. Such a memory layout is called cache-oblivious. Formally, for a $d$-dimensional mesh $G$, block size $B$, and cache size $M$ (where $M=\\Omega(B^d)$), the mesh update of $G$ uses $O(1+|G|/B)$ memory transfers. The paper also shows how the mesh-update performance degrades for smaller caches, where $M=o(B^d)$.   The paper then gives two algorithms for finding cache-oblivious mesh layouts. The first layout algorithm runs in time $O(|G|\\log^2|G|)$ both in expectation and with high probability on a RAM. It uses $O(1+|G|\\log^2(|G|/M)/B)$ memory transfers in expectation and $O(1+(|G|/B)(\\log^2(|G|/M) + \\log|G|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (DAM) models. The layout is obtained by finding a fully balanced decomposition tree of $G$ and then performing an in-order traversal of the leaves of the tree. The second algorithm runs faster by almost a $\\log|G|/\\log\\log|G|$ factor in all three memory models, both in expectation and with high probability. The layout obtained by finding a relax-balanced decomposition tree of $G$ and then performing an in-order traversal of the leaves of the tree.",
        "published": "2007-05-08T05:59:55Z",
        "link": "http://arxiv.org/abs/0705.1033v2",
        "categories": [
            "cs.DS",
            "cs.CE",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "Recognizing Partial Cubes in Quadratic Time",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We show how to test whether a graph with n vertices and m edges is a partial cube, and if so how to find a distance-preserving embedding of the graph into a hypercube, in the near-optimal time bound O(n^2), improving previous O(nm)-time solutions.",
        "published": "2007-05-08T17:59:08Z",
        "link": "http://arxiv.org/abs/0705.1025v2",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "An Approximation Algorithm for Shortest Descending Paths",
        "authors": [
            "Mustaq Ahmed",
            "Anna Lubiw"
        ],
        "summary": "A path from s to t on a polyhedral terrain is descending if the height of a point p never increases while we move p along the path from s to t. No efficient algorithm is known to find a shortest descending path (SDP) from s to t in a polyhedral terrain. We give a simple approximation algorithm that solves the SDP problem on general terrains. Our algorithm discretizes the terrain with O(n^2 X / e) Steiner points so that after an O(n^2 X / e * log(n X /e))-time preprocessing phase for a given vertex s, we can determine a (1+e)-approximate SDP from s to any point v in O(n) time if v is either a vertex of the terrain or a Steiner point, and in O(n X /e) time otherwise. Here n is the size of the terrain, and X is a parameter of the geometry of the terrain.",
        "published": "2007-05-09T22:02:28Z",
        "link": "http://arxiv.org/abs/0705.1364v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "A note on module-composed graphs",
        "authors": [
            "Frank Gurski"
        ],
        "summary": "In this paper we consider module-composed graphs, i.e. graphs which can be defined by a sequence of one-vertex insertions v_1,...,v_n, such that the neighbourhood of vertex v_i, 2<= i<= n, forms a module (a homogeneous set) of the graph defined by vertices v_1,..., v_{i-1}.   We show that module-composed graphs are HHDS-free and thus homogeneously orderable, weakly chordal, and perfect. Every bipartite distance hereditary graph, every (co-2C_4,P_4)-free graph and thus every trivially perfect graph is module-composed. We give an O(|V_G|(|V_G|+|E_G|)) time algorithm to decide whether a given graph G is module-composed and construct a corresponding module-sequence.   For the case of bipartite graphs, module-composed graphs are exactly distance hereditary graphs, which implies simple linear time algorithms for their recognition and construction of a corresponding module-sequence.",
        "published": "2007-05-10T18:08:22Z",
        "link": "http://arxiv.org/abs/0705.1521v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A Tighter Analysis of Setcover Greedy Algorithm for Test Set",
        "authors": [
            "Peng Cui"
        ],
        "summary": "Setcover greedy algorithm is a natural approximation algorithm for test set problem. This paper gives a precise and tighter analysis of performance guarantee of this algorithm. The author improves the performance guarantee $2\\ln n$ which derives from set cover problem to $1.1354\\ln n$ by applying the potential function technique. In addition, the author gives a nontrivial lower bound $1.0004609\\ln n$ of performance guarantee of this algorithm. This lower bound, together with the matching bound of information content heuristic, confirms the fact information content heuristic is slightly better than setcover greedy algorithm in worst case.",
        "published": "2007-05-12T04:18:36Z",
        "link": "http://arxiv.org/abs/0705.1750v6",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Scheduling Dags under Uncertainty",
        "authors": [
            "Grzegorz Malewicz"
        ],
        "summary": "This paper introduces a parallel scheduling problem where a directed acyclic graph modeling $t$ tasks and their dependencies needs to be executed on $n$ unreliable workers. Worker $i$ executes task $j$ correctly with probability $p_{i,j}$. The goal is to find a regimen $\\Sigma$, that dictates how workers get assigned to tasks (possibly in parallel and redundantly) throughout execution, so as to minimize the expected completion time. This fundamental parallel scheduling problem arises in grid computing and project management fields, and has several applications.   We show a polynomial time algorithm for the problem restricted to the case when dag width is at most a constant and the number of workers is also at most a constant. These two restrictions may appear to be too severe. However, they are fundamentally required. Specifically, we demonstrate that the problem is NP-hard with constant number of workers when dag width can grow, and is also NP-hard with constant dag width when the number of workers can grow. When both dag width and the number of workers are unconstrained, then the problem is inapproximable within factor less than 5/4, unless P=NP.",
        "published": "2007-05-14T06:54:42Z",
        "link": "http://arxiv.org/abs/0705.1876v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "A Closed-Form Method for LRU Replacement under Generalized Power-Law   Demand",
        "authors": [
            "Nikolaos Laoutaris"
        ],
        "summary": "We consider the well known \\emph{Least Recently Used} (LRU) replacement algorithm and analyze it under the independent reference model and generalized power-law demand. For this extensive family of demand distributions we derive a closed-form expression for the per object steady-state hit ratio. To the best of our knowledge, this is the first analytic derivation of the per object hit ratio of LRU that can be obtained in constant time without requiring laborious numeric computations or simulation. Since most applications of replacement algorithms include (at least) some scenarios under i.i.d. requests, our method has substantial practical value, especially when having to analyze multiple caches, where existing numeric methods and simulation become too time consuming.",
        "published": "2007-05-14T16:04:48Z",
        "link": "http://arxiv.org/abs/0705.1970v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "On the Hopcroft's minimization algorithm",
        "authors": [
            "Andrei Paun"
        ],
        "summary": "We show that the absolute worst case time complexity for Hopcroft's minimization algorithm applied to unary languages is reached only for de Bruijn words. A previous paper by Berstel and Carton gave the example of de Bruijn words as a language that requires O(n log n) steps by carefully choosing the splitting sets and processing these sets in a FIFO mode. We refine the previous result by showing that the Berstel/Carton example is actually the absolute worst case time complexity in the case of unary languages. We also show that a LIFO implementation will not achieve the same worst time complexity for the case of unary languages. Lastly, we show that the same result is valid also for the cover automata and a modification of the Hopcroft's algorithm, modification used in minimization of cover automata.",
        "published": "2007-05-14T17:15:53Z",
        "link": "http://arxiv.org/abs/0705.1986v1",
        "categories": [
            "cs.DS",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Parallelized approximation algorithms for minimum routing cost spanning   trees",
        "authors": [
            "Ching-Lueh Chang",
            "Yuh-Dauh Lyuu"
        ],
        "summary": "We parallelize several previously proposed algorithms for the minimum routing cost spanning tree problem and some related problems.",
        "published": "2007-05-15T17:48:42Z",
        "link": "http://arxiv.org/abs/0705.2125v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Improved Approximability Result for Test Set with Small Redundancy",
        "authors": [
            "Peng Cui"
        ],
        "summary": "Test set with redundancy is one of the focuses in recent bioinformatics research. Set cover greedy algorithm (SGA for short) is a commonly used algorithm for test set with redundancy. This paper proves that the approximation ratio of SGA can be $(2-\\frac{1}{2r})\\ln n+{3/2}\\ln r+O(\\ln\\ln n)$ by using the potential function technique. This result is better than the approximation ratio $2\\ln n$ which directly derives from set multicover, when $r=o(\\frac{\\ln n}{\\ln\\ln n})$, and is an extension of the approximability results for plain test set.",
        "published": "2007-05-17T09:53:20Z",
        "link": "http://arxiv.org/abs/0705.2503v4",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "An online algorithm for generating fractal hash chains applied to   digital chains of custody",
        "authors": [
            "Phillip G. Bradford",
            "Daniel A. Ray"
        ],
        "summary": "This paper gives an online algorithm for generating Jakobsson's fractal hash chains. Our new algorithm compliments Jakobsson's fractal hash chain algorithm for preimage traversal since his algorithm assumes the entire hash chain is precomputed and a particular list of Ceiling(log n) hash elements or pebbles are saved. Our online algorithm for hash chain traversal incrementally generates a hash chain of n hash elements without knowledge of n before it starts. For any n, our algorithm stores only the Ceiling(log n) pebbles which are precisely the inputs for Jakobsson's amortized hash chain preimage traversal algorithm. This compact representation is useful to generate, traverse, and store a number of large digital hash chains on a small and constrained device. We also give an application using both Jakobsson's and our new algorithm applied to digital chains of custody for validating dynamically changing forensics data.",
        "published": "2007-05-20T17:14:38Z",
        "link": "http://arxiv.org/abs/0705.2876v1",
        "categories": [
            "cs.CR",
            "cs.DS",
            "D.4.6; K.6.5"
        ]
    },
    {
        "title": "Grover search algorithm",
        "authors": [
            "Eva Borbely"
        ],
        "summary": "A quantum algorithm is a set of instructions for a quantum computer, however, unlike algorithms in classical computer science their results cannot be guaranteed. A quantum system can undergo two types of operation, measurement and quantum state transformation, operations themselves must be unitary (reversible). Most quantum algorithms involve a series of quantum state transformations followed by a measurement. Currently very few quantum algorithms are known and no general design methodology exists for their construction.",
        "published": "2007-05-29T09:42:46Z",
        "link": "http://arxiv.org/abs/0705.4171v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Defect-Tolerant CMOL Cell Assignment via Satisfiability",
        "authors": [
            "William N. N. Hung",
            "Changjian Gao",
            "Xiaoyu Song",
            "Dan Hammerstrom"
        ],
        "summary": "We present a CAD framework for CMOL, a hybrid CMOS/ molecular circuit architecture. Our framework first transforms any logically synthesized circuit based on AND/OR/NOT gates to a NOR gate circuit, and then maps the NOR gates to CMOL. We encode the CMOL cell assignment problem as boolean conditions. The boolean constraint is satisfiable if and only if there is a way to map all the NOR gates to the CMOL cells. We further investigate various types of static defects for the CMOL architecture, and propose a reconfiguration technique that can deal with these defects through our CAD framework. This is the first automated framework for CMOL cell assignment, and the first to model several different CMOL static defects. Empirical results show that our approach is efficient and scalable.",
        "published": "2007-05-29T23:46:38Z",
        "link": "http://arxiv.org/abs/0705.4320v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Dynamic User-Defined Similarity Searching in Semi-Structured Text   Retrieval",
        "authors": [
            "Filippo Geraci",
            "Marco Pellegrini"
        ],
        "summary": "Modern text retrieval systems often provide a similarity search utility, that allows the user to find efficiently a fixed number k of documents in the data set that are most similar to a given query (here a query is either a simple sequence of keywords or the identifier of a full document found in previous searches that is considered of interest). We consider the case of a textual database made of semi-structured documents. Each field, in turns, is modelled with a specific vector space. The problem is more complex when we also allow each such vector space to have an associated user-defined dynamic weight that influences its contribution to the overall dynamic aggregated and weighted similarity. This dynamic problem has been tackled in a recent paper by Singitham et al. in in VLDB 2004. Their proposed solution, which we take as baseline, is a variant of the cluster-pruning technique that has the potential for scaling to very large corpora of documents, and is far more efficient than the naive exhaustive search. We devise an alternative way of embedding weights in the data structure, coupled with a non-trivial application of a clustering algorithm based on the furthest point first heuristic for the metric k-center problem. The validity of our approach is demonstrated experimentally by showing significant performance improvements over the scheme proposed in Singitham et al. in VLDB 2004. We improve significantly tradeoffs between query time and output quality with respect to the baseline method in Singitham et al. in in VLDB 2004, and also with respect to a novel method by Chierichetti et al. to appear in ACM PODS 2007. We also speed up the pre-processing time by a factor at least thirty.",
        "published": "2007-05-31T13:46:39Z",
        "link": "http://arxiv.org/abs/0705.4606v1",
        "categories": [
            "cs.IR",
            "cs.DS",
            "H.3.3"
        ]
    },
    {
        "title": "An Improved Tight Closure Algorithm for Integer Octagonal Constraints",
        "authors": [
            "Roberto Bagnara",
            "Patricia M. Hill",
            "Enea Zaffanella"
        ],
        "summary": "Integer octagonal constraints (a.k.a. ``Unit Two Variables Per Inequality'' or ``UTVPI integer constraints'') constitute an interesting class of constraints for the representation and solution of integer problems in the fields of constraint programming and formal analysis and verification of software and hardware systems, since they couple algorithms having polynomial complexity with a relatively good expressive power. The main algorithms required for the manipulation of such constraints are the satisfiability check and the computation of the inferential closure of a set of constraints. The latter is called `tight' closure to mark the difference with the (incomplete) closure algorithm that does not exploit the integrality of the variables. In this paper we present and fully justify an O(n^3) algorithm to compute the tight closure of a set of UTVPI integer constraints.",
        "published": "2007-05-31T14:32:46Z",
        "link": "http://arxiv.org/abs/0705.4618v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "cs.LO"
        ]
    },
    {
        "title": "A randomized algorithm for the on-line weighted bipartite matching   problem",
        "authors": [
            "Béla Csaba",
            "András S. Pluhár"
        ],
        "summary": "We study the on-line minimum weighted bipartite matching problem in arbitrary metric spaces. Here, $n$ not necessary disjoint points of a metric space $M$ are given, and are to be matched on-line with $n$ points of $M$ revealed one by one. The cost of a matching is the sum of the distances of the matched points, and the goal is to find or approximate its minimum. The competitive ratio of the deterministic problem is known to be $\\Theta(n)$. It was conjectured that a randomized algorithm may perform better against an oblivious adversary, namely with an expected competitive ratio $\\Theta(\\log n)$. We prove a slightly weaker result by showing a $o(\\log^3 n)$ upper bound on the expected competitive ratio. As an application the same upper bound holds for the notoriously hard fire station problem, where $M$ is the real line.",
        "published": "2007-05-31T18:35:21Z",
        "link": "http://arxiv.org/abs/0705.4673v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; F.1.2"
        ]
    },
    {
        "title": "Symmetry Partition Sort",
        "authors": [
            "Jing-Chao Chen"
        ],
        "summary": "In this paper, we propose a useful replacement for quicksort-style utility functions. The replacement is called Symmetry Partition Sort, which has essentially the same principle as Proportion Extend Sort. The maximal difference between them is that the new algorithm always places already partially sorted inputs (used as a basis for the proportional extension) on both ends when entering the partition routine. This is advantageous to speeding up the partition routine. The library function based on the new algorithm is more attractive than Psort which is a library function introduced in 2004. Its implementation mechanism is simple. The source code is clearer. The speed is faster, with O(n log n) performance guarantee. Both the robustness and adaptivity are better. As a library function, it is competitive.",
        "published": "2007-06-01T01:47:06Z",
        "link": "http://arxiv.org/abs/0706.0046v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Sampling Colourings of the Triangular Lattice",
        "authors": [
            "Markus Jalsenius"
        ],
        "summary": "We show that the Glauber dynamics on proper 9-colourings of the triangular lattice is rapidly mixing, which allows for efficient sampling. Consequently, there is a fully polynomial randomised approximation scheme (FPRAS) for counting proper 9-colourings of the triangular lattice. Proper colourings correspond to configurations in the zero-temperature anti-ferromagnetic Potts model. We show that the spin system consisting of proper 9-colourings of the triangular lattice has strong spatial mixing. This implies that there is a unique infinite-volume Gibbs distribution, which is an important property studied in statistical physics. Our results build on previous work by Goldberg, Martin and Paterson, who showed similar results for 10 colours on the triangular lattice. Their work was preceded by Salas and Sokal's 11-colour result. Both proofs rely on computational assistance, and so does our 9-colour proof. We have used a randomised heuristic to guide us towards rigourous results.",
        "published": "2007-06-04T17:49:25Z",
        "link": "http://arxiv.org/abs/0706.0489v3",
        "categories": [
            "math-ph",
            "cs.DM",
            "cs.DS",
            "math.MP"
        ]
    },
    {
        "title": "Small Worlds: Strong Clustering in Wireless Networks",
        "authors": [
            "Matthias R. Brust",
            "Steffen Rothkugel"
        ],
        "summary": "Small-worlds represent efficient communication networks that obey two distinguishing characteristics: a high clustering coefficient together with a small characteristic path length. This paper focuses on an interesting paradox, that removing links in a network can increase the overall clustering coefficient. Reckful Roaming, as introduced in this paper, is a 2-localized algorithm that takes advantage of this paradox in order to selectively remove superfluous links, this way optimizing the clustering coefficient while still retaining a sufficiently small characteristic path length.",
        "published": "2007-06-07T19:42:51Z",
        "link": "http://arxiv.org/abs/0706.1063v2",
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Sublinear Algorithms for Approximating String Compressibility",
        "authors": [
            "Sofya Raskhodnikova",
            "Dana Ron",
            "Ronitt Rubinfeld",
            "Adam Smith"
        ],
        "summary": "We raise the question of approximating the compressibility of a string with respect to a fixed compression scheme, in sublinear time. We study this question in detail for two popular lossless compression schemes: run-length encoding (RLE) and Lempel-Ziv (LZ), and present sublinear algorithms for approximating compressibility with respect to both schemes. We also give several lower bounds that show that our algorithms for both schemes cannot be improved significantly.   Our investigation of LZ yields results whose interest goes beyond the initial questions we set out to study. In particular, we prove combinatorial structural lemmas that relate the compressibility of a string with respect to Lempel-Ziv to the number of distinct short substrings contained in it. In addition, we show that approximating the compressibility with respect to LZ is related to approximating the support size of a distribution.",
        "published": "2007-06-08T02:58:28Z",
        "link": "http://arxiv.org/abs/0706.1084v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Constructing a maximum utility slate of on-line advertisements",
        "authors": [
            "S. Sathiya Keerthi",
            "John A. Tomlin"
        ],
        "summary": "We present an algorithm for constructing an optimal slate of sponsored search advertisements which respects the ordering that is the outcome of a generalized second price auction, but which must also accommodate complicating factors such as overall budget constraints. The algorithm is easily fast enough to use on the fly for typical problem sizes, or as a subroutine in an overall optimization.",
        "published": "2007-06-09T16:18:45Z",
        "link": "http://arxiv.org/abs/0706.1318v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Dualheap Selection Algorithm: Efficient, Inherently Parallel and   Somewhat Mysterious",
        "authors": [
            "Greg Sepesi"
        ],
        "summary": "An inherently parallel algorithm is proposed that efficiently performs selection: finding the K-th largest member of a set of N members. Selection is a common component of many more complex algorithms and therefore is a widely studied problem.   Not much is new in the proposed dualheap selection algorithm: the heap data structure is from J.W.J.Williams, the bottom-up heap construction is from R.W. Floyd, and the concept of a two heap data structure is from J.W.J. Williams and D.E. Knuth. The algorithm's novelty is limited to a few relatively minor implementation twists: 1) the two heaps are oriented with their roots at the partition values rather than at the minimum and maximum values, 2)the coding of one of the heaps (the heap of smaller values) employs negative indexing, and 3) the exchange phase of the algorithm is similar to a bottom-up heap construction, but navigates the heap with a post-order tree traversal.   When run on a single processor, the dualheap selection algorithm's performance is competitive with quickselect with median estimation, a common variant of C.A.R. Hoare's quicksort algorithm. When run on parallel processors, the dualheap selection algorithm is superior due to its subtasks that are easily partitioned and innately balanced.",
        "published": "2007-06-14T16:11:24Z",
        "link": "http://arxiv.org/abs/0706.2155v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DC",
            "C.1.4; F.2.0; G.2.2"
        ]
    },
    {
        "title": "The Complexity of Determining Existence a Hamiltonian Cycle is $O(n^3)$",
        "authors": [
            "Guohun Zhu"
        ],
        "summary": "The Hamiltonian cycle problem in digraph is mapped into a matching cover bipartite graph. Based on this mapping, it is proved that determining existence a Hamiltonian cycle in graph is $O(n^3)$.",
        "published": "2007-06-19T07:57:51Z",
        "link": "http://arxiv.org/abs/0706.2725v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Cache Analysis of Non-uniform Distribution Sorting Algorithms",
        "authors": [
            "Naila Rahman",
            "Rajeev Raman"
        ],
        "summary": "We analyse the average-case cache performance of distribution sorting algorithms in the case when keys are independently but not necessarily uniformly distributed. The analysis is for both `in-place' and `out-of-place' distribution sorting algorithms and is more accurate than the analysis presented in \\cite{RRESA00}. In particular, this new analysis yields tighter upper and lower bounds when the keys are drawn from a uniform distribution.   We use this analysis to tune the performance of the integer sorting algorithm MSB radix sort when it is used to sort independent uniform floating-point numbers (floats). Our tuned MSB radix sort algorithm comfortably outperforms a cache-tuned implementations of bucketsort \\cite{RR99} and Quicksort when sorting uniform floats from $[0, 1)$.",
        "published": "2007-06-19T17:12:47Z",
        "link": "http://arxiv.org/abs/0706.2839v2",
        "categories": [
            "cs.DS",
            "cs.PF"
        ]
    },
    {
        "title": "Dualheap Sort Algorithm: An Inherently Parallel Generalization of   Heapsort",
        "authors": [
            "Greg Sepesi"
        ],
        "summary": "A generalization of the heapsort algorithm is proposed. At the expense of about 50% more comparison and move operations for typical cases, the dualheap sort algorithm offers several advantages over heapsort: improved cache performance, better performance if the input happens to be already sorted, and easier parallel implementations.",
        "published": "2007-06-20T14:42:45Z",
        "link": "http://arxiv.org/abs/0706.2893v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DC"
        ]
    },
    {
        "title": "Group Testing with Random Pools: optimal two-stage algorithms",
        "authors": [
            "Marc Mezard",
            "Cristina Toninelli"
        ],
        "summary": "We study Probabilistic Group Testing of a set of N items each of which is defective with probability p. We focus on the double limit of small defect probability, p<<1, and large number of variables, N>>1, taking either p->0 after $N\\to\\infty$ or $p=1/N^{\\beta}$ with $\\beta\\in(0,1/2)$. In both settings the optimal number of tests which are required to identify with certainty the defectives via a two-stage procedure, $\\bar T(N,p)$, is known to scale as $Np|\\log p|$. Here we determine the sharp asymptotic value of $\\bar T(N,p)/(Np|\\log p|)$ and construct a class of two-stage algorithms over which this optimal value is attained. This is done by choosing a proper bipartite regular graph (of tests and variable nodes) for the first stage of the detection. Furthermore we prove that this optimal value is also attained on average over a random bipartite graph where all variables have the same degree, while the tests have Poisson-distributed degrees. Finally, we improve the existing upper and lower bound for the optimal number of tests in the case $p=1/N^{\\beta}$ with $\\beta\\in[1/2,1)$.",
        "published": "2007-06-21T08:57:44Z",
        "link": "http://arxiv.org/abs/0706.3104v1",
        "categories": [
            "cs.DS",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Experimental Algorithm for the Maximum Independent Set Problem",
        "authors": [
            "Anatoly D. Plotnikov"
        ],
        "summary": "We develop an experimental algorithm for the exact solving of the maximum independent set problem. The algorithm consecutively finds the maximal independent sets of vertices in an arbitrary undirected graph such that the next such set contains more elements than the preceding one. For this purpose, we use a technique, developed by Ford and Fulkerson for the finite partially ordered sets, in particular, their method for partition of a poset into the minimum number of chains with finding the maximum antichain. In the process of solving, a special digraph is constructed, and a conjecture is formulated concerning properties of such digraph. This allows to offer of the solution algorithm. Its theoretical estimation of running time equals to is $O(n^{8})$, where $n$ is the number of graph vertices. The offered algorithm was tested by a program on random graphs. The testing the confirms correctness of the algorithm.",
        "published": "2007-06-25T06:45:49Z",
        "link": "http://arxiv.org/abs/0706.3565v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Radix Sorting With No Extra Space",
        "authors": [
            "Gianni Franceschini",
            "S. Muthukrishnan",
            "Mihai Patrascu"
        ],
        "summary": "It is well known that n integers in the range [1,n^c] can be sorted in O(n) time in the RAM model using radix sorting. More generally, integers in any range [1,U] can be sorted in O(n sqrt{loglog n}) time. However, these algorithms use O(n) words of extra memory. Is this necessary?   We present a simple, stable, integer sorting algorithm for words of size O(log n), which works in O(n) time and uses only O(1) words of extra memory on a RAM model. This is the integer sorting case most useful in practice. We extend this result with same bounds to the case when the keys are read-only, which is of theoretical interest. Another interesting question is the case of arbitrary c. Here we present a black-box transformation from any RAM sorting algorithm to a sorting algorithm which uses only O(1) extra space and has the same running time. This settles the complexity of in-place sorting in terms of the complexity of sorting.",
        "published": "2007-06-27T22:04:40Z",
        "link": "http://arxiv.org/abs/0706.4107v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Directed Feedback Vertex Set is Fixed-Parameter Tractable",
        "authors": [
            "Igor Razgon",
            "Barry O'Sullivan"
        ],
        "summary": "We resolve positively a long standing open question regarding the fixed-parameter tractability of the parameterized Directed Feedback Vertex Set problem. In particular, we propose an algorithm which solves this problem in $O(8^kk!*poly(n))$.",
        "published": "2007-07-02T17:56:53Z",
        "link": "http://arxiv.org/abs/0707.0282v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "The $k$-anonymity Problem is Hard",
        "authors": [
            "Paola Bonizzoni",
            "Gianluca Della Vedova",
            "Riccardo Dondi"
        ],
        "summary": "The problem of publishing personal data without giving up privacy is becoming increasingly important. An interesting formalization recently proposed is the k-anonymity. This approach requires that the rows in a table are clustered in sets of size at least k and that all the rows in a cluster become the same tuple, after the suppression of some records. The natural optimization problem, where the goal is to minimize the number of suppressed entries, is known to be NP-hard when the values are over a ternary alphabet, k = 3 and the rows length is unbounded. In this paper we give a lower bound on the approximation factor that any polynomial-time algorithm can achive on two restrictions of the problem,namely (i) when the records values are over a binary alphabet and k = 3, and (ii) when the records have length at most 8 and k = 4, showing that these restrictions of the problem are APX-hard.",
        "published": "2007-07-03T14:17:49Z",
        "link": "http://arxiv.org/abs/0707.0421v2",
        "categories": [
            "cs.DB",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Weighted Popular Matchings",
        "authors": [
            "Julián Mestre"
        ],
        "summary": "We study the problem of assigning jobs to applicants. Each applicant has a weight and provides a preference list ranking a subset of the jobs. A matching M is popular if there is no other matching M' such that the weight of the applicants who prefer M' over M exceeds the weight of those who prefer M over M'. This paper gives efficient algorithms to find a popular matching if one exists.",
        "published": "2007-07-04T06:55:43Z",
        "link": "http://arxiv.org/abs/0707.0546v1",
        "categories": [
            "cs.DS",
            "G.2.1"
        ]
    },
    {
        "title": "Another view of the Gaussian algorithm",
        "authors": [
            "Ali Akhavi",
            "Céline Moreira"
        ],
        "summary": "We introduce here a rewrite system in the group of unimodular matrices, \\emph{i.e.}, matrices with integer entries and with determinant equal to $\\pm 1$. We use this rewrite system to precisely characterize the mechanism of the Gaussian algorithm, that finds shortest vectors in a two--dimensional lattice given by any basis. Putting together the algorithmic of lattice reduction and the rewrite system theory, we propose a new worst--case analysis of the Gaussian algorithm. There is already an optimal worst--case bound for some variant of the Gaussian algorithm due to Vall\\'ee \\cite {ValGaussRevisit}. She used essentially geometric considerations. Our analysis generalizes her result to the case of the usual Gaussian algorithm. An interesting point in our work is its possible (but not easy) generalization to the same problem in higher dimensions, in order to exhibit a tight upper-bound for the number of iterations of LLL--like reduction algorithms in the worst case. Moreover, our method seems to work for analyzing other families of algorithms. As an illustration, the analysis of sorting algorithms are briefly developed in the last section of the paper.",
        "published": "2007-07-04T15:37:15Z",
        "link": "http://arxiv.org/abs/0707.0644v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Dial a Ride from k-forest",
        "authors": [
            "Anupam Gupta",
            "MohammadTaghi Hajiaghayi",
            "Viswanath Nagarajan",
            "R. Ravi"
        ],
        "summary": "The k-forest problem is a common generalization of both the k-MST and the dense-$k$-subgraph problems. Formally, given a metric space on $n$ vertices $V$, with $m$ demand pairs $\\subseteq V \\times V$ and a ``target'' $k\\le m$, the goal is to find a minimum cost subgraph that connects at least $k$ demand pairs. In this paper, we give an $O(\\min\\{\\sqrt{n},\\sqrt{k}\\})$-approximation algorithm for $k$-forest, improving on the previous best ratio of $O(n^{2/3}\\log n)$ by Segev & Segev.   We then apply our algorithm for k-forest to obtain approximation algorithms for several Dial-a-Ride problems. The basic Dial-a-Ride problem is the following: given an $n$ point metric space with $m$ objects each with its own source and destination, and a vehicle capable of carrying at most $k$ objects at any time, find the minimum length tour that uses this vehicle to move each object from its source to destination. We prove that an $\\alpha$-approximation algorithm for the $k$-forest problem implies an $O(\\alpha\\cdot\\log^2n)$-approximation algorithm for Dial-a-Ride. Using our results for $k$-forest, we get an $O(\\min\\{\\sqrt{n},\\sqrt{k}\\}\\cdot\\log^2 n)$- approximation algorithm for Dial-a-Ride. The only previous result known for Dial-a-Ride was an $O(\\sqrt{k}\\log n)$-approximation by Charikar & Raghavachari; our results give a different proof of a similar approximation guarantee--in fact, when the vehicle capacity $k$ is large, we give a slight improvement on their results.",
        "published": "2007-07-04T16:08:40Z",
        "link": "http://arxiv.org/abs/0707.0648v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Noisy Sorting Without Resampling",
        "authors": [
            "Mark Braverman",
            "Elchanan Mossel"
        ],
        "summary": "In this paper we study noisy sorting without re-sampling. In this problem there is an unknown order $a_{\\pi(1)} < ... < a_{\\pi(n)}$ where $\\pi$ is a permutation on $n$ elements. The input is the status of $n \\choose 2$ queries of the form $q(a_i,x_j)$, where $q(a_i,a_j) = +$ with probability at least $1/2+\\ga$ if $\\pi(i) > \\pi(j)$ for all pairs $i \\neq j$, where $\\ga > 0$ is a constant and $q(a_i,a_j) = -q(a_j,a_i)$ for all $i$ and $j$. It is assumed that the errors are independent. Given the status of the queries the goal is to find the maximum likelihood order. In other words, the goal is find a permutation $\\sigma$ that minimizes the number of pairs $\\sigma(i) > \\sigma(j)$ where $q(\\sigma(i),\\sigma(j)) = -$. The problem so defined is the feedback arc set problem on distributions of inputs, each of which is a tournament obtained as a noisy perturbations of a linear order. Note that when $\\ga < 1/2$ and $n$ is large, it is impossible to recover the original order $\\pi$.   It is known that the weighted feedback are set problem on tournaments is NP-hard in general. Here we present an algorithm of running time $n^{O(\\gamma^{-4})}$ and sampling complexity $O_{\\gamma}(n \\log n)$ that with high probability solves the noisy sorting without re-sampling problem. We also show that if $a_{\\sigma(1)},a_{\\sigma(2)},...,a_{\\sigma(n)}$ is an optimal solution of the problem then it is ``close'' to the original order. More formally, with high probability it holds that $\\sum_i |\\sigma(i) - \\pi(i)| = \\Theta(n)$ and $\\max_i |\\sigma(i) - \\pi(i)| = \\Theta(\\log n)$.   Our results are of interest in applications to ranking, such as ranking in sports, or ranking of search items based on comparisons by experts.",
        "published": "2007-07-06T21:30:24Z",
        "link": "http://arxiv.org/abs/0707.1051v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Better Algorithms and Bounds for Directed Maximum Leaf Problems",
        "authors": [
            "Noga Alon",
            "Fedor V. Fomin",
            "Gregory Gutin",
            "Michael Krivelevich",
            "Saket Saurabh"
        ],
        "summary": "The {\\sc Directed Maximum Leaf Out-Branching} problem is to find an out-branching (i.e. a rooted oriented spanning tree) in a given digraph with the maximum number of leaves. In this paper, we improve known parameterized algorithms and combinatorial bounds on the number of leaves in out-branchings. We show that   \\begin{itemize} \\item every strongly connected digraph $D$ of order $n$ with minimum in-degree at least 3 has an out-branching with at least $(n/4)^{1/3}-1$ leaves; \\item if a strongly connected digraph $D$ does not contain an out-branching with $k$ leaves, then the pathwidth of its underlying graph is $O(k\\log k)$; \\item it can be decided in time $2^{O(k\\log^2 k)}\\cdot n^{O(1)}$ whether a strongly connected digraph on $n$ vertices has an out-branching with at least $k$ leaves. \\end{itemize}   All improvements use properties of extremal structures obtained after applying local search and of some out-branching decompositions.",
        "published": "2007-07-07T15:52:29Z",
        "link": "http://arxiv.org/abs/0707.1095v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Sorting and Selection in Posets",
        "authors": [
            "Constantinos Daskalakis",
            "Richard M. Karp",
            "Elchanan Mossel",
            "Samantha Riesenfeld",
            "Elad Verbin"
        ],
        "summary": "Classical problems of sorting and searching assume an underlying linear ordering of the objects being compared. In this paper, we study a more general setting, in which some pairs of objects are incomparable. This generalization is relevant in applications related to rankings in sports, college admissions, or conference submissions. It also has potential applications in biology, such as comparing the evolutionary fitness of different strains of bacteria, or understanding input-output relations among a set of metabolic reactions or the causal influences among a set of interacting genes or proteins. Our results improve and extend results from two decades ago of Faigle and Tur\\'{a}n.   A measure of complexity of a partially ordered set (poset) is its width. Our algorithms obtain information about a poset by queries that compare two elements. We present an algorithm that sorts, i.e. completely identifies, a width w poset of size n and has query complexity O(wn + nlog(n)), which is within a constant factor of the information-theoretic lower bound. We also show that a variant of Mergesort has query complexity O(wn(log(n/w))) and total complexity O((w^2)nlog(n/w)). Faigle and Tur\\'{a}n have shown that the sorting problem has query complexity O(wn(log(n/w))) but did not address its total complexity.   For the related problem of determining the minimal elements of a poset, we give efficient deterministic and randomized algorithms with O(wn) query and total complexity, along with matching lower bounds for the query complexity up to a factor of 2. We generalize these results to the k-selection problem of determining the elements of height at most k. We also derive upper bounds on the total complexity of some other problems of a similar flavor.",
        "published": "2007-07-10T21:52:17Z",
        "link": "http://arxiv.org/abs/0707.1532v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Sampling Algorithms and Coresets for Lp Regression",
        "authors": [
            "Anirban Dasgupta",
            "Petros Drineas",
            "Boulos Harb",
            "Ravi Kumar",
            "Michael W. Mahoney"
        ],
        "summary": "The Lp regression problem takes as input a matrix $A \\in \\Real^{n \\times d}$, a vector $b \\in \\Real^n$, and a number $p \\in [1,\\infty)$, and it returns as output a number ${\\cal Z}$ and a vector $x_{opt} \\in \\Real^d$ such that ${\\cal Z} = \\min_{x \\in \\Real^d} ||Ax -b||_p = ||Ax_{opt}-b||_p$. In this paper, we construct coresets and obtain an efficient two-stage sampling-based approximation algorithm for the very overconstrained ($n \\gg d$) version of this classical problem, for all $p \\in [1, \\infty)$. The first stage of our algorithm non-uniformly samples $\\hat{r}_1 = O(36^p d^{\\max\\{p/2+1, p\\}+1})$ rows of $A$ and the corresponding elements of $b$, and then it solves the Lp regression problem on the sample; we prove this is an 8-approximation. The second stage of our algorithm uses the output of the first stage to resample $\\hat{r}_1/\\epsilon^2$ constraints, and then it solves the Lp regression problem on the new sample; we prove this is a $(1+\\epsilon)$-approximation. Our algorithm unifies, improves upon, and extends the existing algorithms for special cases of Lp regression, namely $p = 1,2$. In course of proving our result, we develop two concepts--well-conditioned bases and subspace-preserving sampling--that are of independent interest.",
        "published": "2007-07-11T22:04:18Z",
        "link": "http://arxiv.org/abs/0707.1714v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Splay Trees, Davenport-Schinzel Sequences, and the Deque Conjecture",
        "authors": [
            "Seth Pettie"
        ],
        "summary": "We introduce a new technique to bound the asymptotic performance of splay trees. The basic idea is to transcribe, in an indirect fashion, the rotations performed by the splay tree as a Davenport-Schinzel sequence S, none of whose subsequences are isomorphic to fixed forbidden subsequence. We direct this technique towards Tarjan's deque conjecture and prove that n deque operations require O(n alpha^*(n)) time, where alpha^*(n) is the minimum number of applications of the inverse-Ackermann function mapping n to a constant. We are optimistic that this approach could be directed towards other open conjectures on splay trees such as the traversal and split conjectures.",
        "published": "2007-07-14T16:38:08Z",
        "link": "http://arxiv.org/abs/0707.2160v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A fixed point iteration for computing the matrix logarithm",
        "authors": [
            "Gernot Schaller"
        ],
        "summary": "In various areas of applied numerics, the problem of calculating the logarithm of a matrix A emerges. Since series expansions of the logarithm usually do not converge well for matrices far away from the identity, the standard numerical method calculates successive square roots. In this article, a new algorithm is presented that relies on the computation of successive matrix exponentials. Convergence of the method is demonstrated for a large class of initial matrices and favorable choices of the initial matrix are discussed.",
        "published": "2007-07-18T11:04:27Z",
        "link": "http://arxiv.org/abs/0707.2701v1",
        "categories": [
            "cs.NA",
            "cs.DS"
        ]
    },
    {
        "title": "Faster subsequence recognition in compressed strings",
        "authors": [
            "Alexander Tiskin"
        ],
        "summary": "Computation on compressed strings is one of the key approaches to processing massive data sets. We consider local subsequence recognition problems on strings compressed by straight-line programs (SLP), which is closely related to Lempel--Ziv compression. For an SLP-compressed text of length $\\bar m$, and an uncompressed pattern of length $n$, C{\\'e}gielski et al. gave an algorithm for local subsequence recognition running in time $O(\\bar mn^2 \\log n)$. We improve the running time to $O(\\bar mn^{1.5})$. Our algorithm can also be used to compute the longest common subsequence between a compressed text and an uncompressed pattern in time $O(\\bar mn^{1.5})$; the same problem with a compressed pattern is known to be NP-hard.",
        "published": "2007-07-23T16:26:24Z",
        "link": "http://arxiv.org/abs/0707.3407v4",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Faster exon assembly by sparse spliced alignment",
        "authors": [
            "Alexander Tiskin"
        ],
        "summary": "Assembling a gene from candidate exons is an important problem in computational biology. Among the most successful approaches to this problem is \\emph{spliced alignment}, proposed by Gelfand et al., which scores different candidate exon chains within a DNA sequence of length $m$ by comparing them to a known related gene sequence of length n, $m = \\Theta(n)$. Gelfand et al.\\ gave an algorithm for spliced alignment running in time O(n^3). Kent et al.\\ considered sparse spliced alignment, where the number of candidate exons is O(n), and proposed an algorithm for this problem running in time O(n^{2.5}). We improve on this result, by proposing an algorithm for sparse spliced alignment running in time O(n^{2.25}). Our approach is based on a new framework of \\emph{quasi-local string comparison}.",
        "published": "2007-07-23T16:35:54Z",
        "link": "http://arxiv.org/abs/0707.3409v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.CE",
            "q-bio.QM"
        ]
    },
    {
        "title": "Semi-local string comparison: algorithmic techniques and applications",
        "authors": [
            "Alexander Tiskin"
        ],
        "summary": "A classical measure of string comparison is given by the longest common subsequence (LCS) problem on a pair of strings. We consider its generalisation, called the semi-local LCS problem, which arises naturally in many string-related problems. The semi-local LCS problem asks for the LCS scores for each of the input strings against every substring of the other input string, and for every prefix of each input string against every suffix of the other input string. Such a comparison pattern provides a much more detailed picture of string similarity than a single LCS score; it also arises naturally in many string-related problems. In fact, the semi-local LCS problem turns out to be fundamental for string comparison, providing a powerful and flexible alternative to classical dynamic programming. It is especially useful when the input to a string comparison problem may not be available all at once: for example, comparison of dynamically changing strings; comparison of compressed strings; parallel string comparison. The same approach can also be applied to permutation strings, providing efficient solutions for local versions of the longest increasing subsequence (LIS) problem, and for the problem of computing a maximum clique in a circle graph. Furthermore, the semi-local LCS problem turns out to have surprising connections in a few seemingly unrelated fields, such as computational geometry and algebra of semigroups. This work is devoted to exploring the structure of the semi-local LCS problem, its efficient solutions, and its applications in string comparison and other related areas, including computational molecular biology.",
        "published": "2007-07-24T19:12:23Z",
        "link": "http://arxiv.org/abs/0707.3619v21",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Constant-degree graph expansions that preserve the treewidth",
        "authors": [
            "Igor Markov",
            "Yaoyun Shi"
        ],
        "summary": "Many hard algorithmic problems dealing with graphs, circuits, formulas and constraints admit polynomial-time upper bounds if the underlying graph has small treewidth. The same problems often encourage reducing the maximal degree of vertices to simplify theoretical arguments or address practical concerns. Such degree reduction can be performed through a sequence of splittings of vertices, resulting in an _expansion_ of the original graph. We observe that the treewidth of a graph may increase dramatically if the splittings are not performed carefully. In this context we address the following natural question: is it possible to reduce the maximum degree to a constant without substantially increasing the treewidth?   Our work answers the above question affirmatively. We prove that any simple undirected graph G=(V, E) admits an expansion G'=(V', E') with the maximum degree <= 3 and treewidth(G') <= treewidth(G)+1. Furthermore, such an expansion will have no more than 2|E|+|V| vertices and 3|E| edges; it can be computed efficiently from a tree-decomposition of G. We also construct a family of examples for which the increase by 1 in treewidth cannot be avoided.",
        "published": "2007-07-24T19:56:27Z",
        "link": "http://arxiv.org/abs/0707.3622v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "math.CO",
            "quant-ph",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "On sparse representations of linear operators and the approximation of   matrix products",
        "authors": [
            "Mohamed-Ali Belabbas",
            "Patrick J. Wolfe"
        ],
        "summary": "Thus far, sparse representations have been exploited largely in the context of robustly estimating functions in a noisy environment from a few measurements. In this context, the existence of a basis in which the signal class under consideration is sparse is used to decrease the number of necessary measurements while controlling the approximation error. In this paper, we instead focus on applications in numerical analysis, by way of sparse representations of linear operators with the objective of minimizing the number of operations needed to perform basic operations (here, multiplication) on these operators. We represent a linear operator by a sum of rank-one operators, and show how a sparse representation that guarantees a low approximation error for the product can be obtained from analyzing an induced quadratic form. This construction in turn yields new algorithms for computing approximate matrix products.",
        "published": "2007-07-30T17:20:23Z",
        "link": "http://arxiv.org/abs/0707.4448v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Complementary algorithms for graphs and percolation",
        "authors": [
            "Michael J. Lee"
        ],
        "summary": "A pair of complementary algorithms are presented. One of the pair is a fast method for connecting graphs with an edge. The other is a fast method for removing edges from a graph. Both algorithms employ the same tree based graph representation and so, in concert, can arbitrarily modify any graph. Since the clusters of a percolation model may be described as simple connected graphs, an efficient Monte Carlo scheme can be constructed that uses the algorithms to sweep the occupation probability back and forth between two turning points. This approach concentrates computational sampling time within a region of interest. A high precision value of pc = 0.59274603(9) was thus obtained, by Mersenne twister, for the two dimensional square site percolation threshold.",
        "published": "2007-08-04T02:56:13Z",
        "link": "http://arxiv.org/abs/0708.0600v1",
        "categories": [
            "cs.DS",
            "J.2.x; I.6.8"
        ]
    },
    {
        "title": "On the Self-stabilization of Mobile Robots in Graphs",
        "authors": [
            "Lélia Blin",
            "Maria Gradinariu Potop-Butucaru",
            "Sébastien Tixeuil"
        ],
        "summary": "Self-stabilization is a versatile technique to withstand any transient fault in a distributed system. Mobile robots (or agents) are one of the emerging trends in distributed computing as they mimic autonomous biologic entities. The contribution of this paper is threefold. First, we present a new model for studying mobile entities in networks subject to transient faults. Our model differs from the classical robot model because robots have constraints about the paths they are allowed to follow, and from the classical agent model because the number of agents remains fixed throughout the execution of the protocol. Second, in this model, we study the possibility of designing self-stabilizing algorithms when those algorithms are run by mobile robots (or agents) evolving on a graph. We concentrate on the core building blocks of robot and agents problems: naming and leader election. Not surprisingly, when no constraints are given on the network graph topology and local execution model, both problems are impossible to solve. Finally, using minimal hypothesis with respect to impossibility results, we provide deterministic and probabilistic solutions to both problems, and show equivalence of these problems by an algorithmic reduction mechanism.",
        "published": "2007-08-07T09:34:14Z",
        "link": "http://arxiv.org/abs/0708.0909v2",
        "categories": [
            "cs.DS",
            "cs.DC"
        ]
    },
    {
        "title": "Randomized algorithm for the k-server problem on decomposable spaces",
        "authors": [
            "Judit Nagy-György"
        ],
        "summary": "We study the randomized k-server problem on metric spaces consisting of widely separated subspaces. We give a method which extends existing algorithms to larger spaces with the growth rate of the competitive quotients being at most O(log k). This method yields o(k)-competitive algorithms solving the randomized k-server problem, for some special underlying metric spaces, e.g. HSTs of \"small\" height (but unbounded degree). HSTs are important tools for probabilistic approximation of metric spaces.",
        "published": "2007-08-17T11:54:44Z",
        "link": "http://arxiv.org/abs/0708.2351v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "On the Complexity of the Minimum Cost Homomorphism Problem for Reflexive   Multipartite Tournaments",
        "authors": [
            "G. Gutin",
            "E. J. Kim"
        ],
        "summary": "For digraphs $D$ and $H$, a mapping $f: V(D)\\dom V(H)$ is a homomorphism of $D$ to $H$ if $uv\\in A(D)$ implies $f(u)f(v)\\in A(H).$ For a fixed digraph $H$, the homomorphism problem is to decide whether an input digraph $D$ admits a homomorphism to $H$ or not, and is denoted as HOMP($H$). Digraphs are allowed to have loops, but not allowed to have parallel arcs.   A natural optimization version of the homomorphism problem is defined as follows. If each vertex $u \\in V(D)$ is associated with costs $c_i(u), i \\in V(H)$, then the cost of the homomorphism $f$ is $\\sum_{u\\in V(D)}c_{f(u)}(u)$. For each fixed digraph $H$, we have the {\\em minimum cost homomorphism problem for} $H$ and denote it as MinHOMP($H$). The problem is to decide, for an input graph $D$ with costs $c_i(u),$ $u \\in V(D), i\\in V(H)$, whether there exists a homomorphism of $D$ to $H$ and, if one exists, to find one of minimum cost.   In a recent paper, we posed a problem of characterizing polynomial time solvable and NP-hard cases of the minimum cost homomorphism problem for acyclic multipartite tournaments with possible loops (w.p.l.). In this paper, we solve the problem for reflexive multipartite tournaments and demonstrate a considerate difficulty of the problem for the whole class of multipartite tournaments w.p.l. using, as an example, acyclic 3-partite tournaments of order 4 w.p.l.\\footnote{This paper was submitted to Discrete Mathematics on April 6, 2007}",
        "published": "2007-08-19T13:00:59Z",
        "link": "http://arxiv.org/abs/0708.2544v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Complexity of the Minimum Cost Homomorphism Problem for Semicomplete   Digraphs with Possible Loops",
        "authors": [
            "E. J. Kim",
            "G. Gutin"
        ],
        "summary": "For digraphs $D$ and $H$, a mapping $f: V(D)\\dom V(H)$ is a homomorphism of $D$ to $H$ if $uv\\in A(D)$ implies $f(u)f(v)\\in A(H).$ For a fixed digraph $H$, the homomorphism problem is to decide whether an input digraph $D$ admits a homomorphism to $H$ or not, and is denoted as HOM($H$).   An optimization version of the homomorphism problem was motivated by a real-world problem in defence logistics and was introduced in \\cite{gutinDAM154a}. If each vertex $u \\in V(D)$ is associated with costs $c_i(u), i \\in V(H)$, then the cost of the homomorphism $f$ is $\\sum_{u\\in V(D)}c_{f(u)}(u)$. For each fixed digraph $H$, we have the {\\em minimum cost homomorphism problem for} $H$ and denote it as MinHOM($H$). The problem is to decide, for an input graph $D$ with costs $c_i(u),$ $u \\in V(D), i\\in V(H)$, whether there exists a homomorphism of $D$ to $H$ and, if one exists, to find one of minimum cost.   Although a complete dichotomy classification of the complexity of MinHOM($H$) for a digraph $H$ remains an unsolved problem, complete dichotomy classifications for MinHOM($H$) were proved when $H$ is a semicomplete digraph \\cite{gutinDAM154b}, and a semicomplete multipartite digraph \\cite{gutinDAM}. In these studies, it is assumed that the digraph $H$ is loopless. In this paper, we present a full dichotomy classification for semicomplete digraphs with possible loops, which solves a problem in \\cite{gutinRMS}.\\footnote{This paper was submitted to SIAM J. Discrete Math. on October 27, 2006}",
        "published": "2007-08-19T13:47:00Z",
        "link": "http://arxiv.org/abs/0708.2545v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Priority Queue Based on Multilevel Prefix Tree",
        "authors": [
            "David S. Planeta"
        ],
        "summary": "Tree structures are very often used data structures. Among ordered types of trees there are many variants whose basic operations such as insert, delete, search, delete-min are characterized by logarithmic time complexity. In the article I am going to present the structure whose time complexity for each of the above operations is $O(\\frac{M}{K} + K)$, where M is the size of data type and K is constant properly matching the size of data type. Properly matched K will make the structure function as a very effective Priority Queue. The structure size linearly depends on the number and size of elements. PTrie is a clever combination of the idea of prefix tree -- Trie, structure of logarithmic time complexity for insert and remove operations, doubly linked list and queues.",
        "published": "2007-08-21T22:59:49Z",
        "link": "http://arxiv.org/abs/0708.2936v1",
        "categories": [
            "cs.DS",
            "F.2.2; E.1.x"
        ]
    },
    {
        "title": "Fast evaluation of union-intersection expressions",
        "authors": [
            "Philip Bille",
            "Anna Pagh",
            "Rasmus Pagh"
        ],
        "summary": "We show how to represent sets in a linear space data structure such that expressions involving unions and intersections of sets can be computed in a worst-case efficient way. This problem has applications in e.g. information retrieval and database systems. We mainly consider the RAM model of computation, and sets of machine words, but also state our results in the I/O model. On a RAM with word size $w$, a special case of our result is that the intersection of $m$ (preprocessed) sets, containing $n$ elements in total, can be computed in expected time $O(n (\\log w)^2 / w + km)$, where $k$ is the number of elements in the intersection. If the first of the two terms dominates, this is a factor $w^{1-o(1)}$ faster than the standard solution of merging sorted lists. We show a cell probe lower bound of time $\\Omega(n/(w m \\log m)+ (1-\\tfrac{\\log k}{w}) k)$, meaning that our upper bound is nearly optimal for small $m$. Our algorithm uses a novel combination of approximate set representations and word-level parallelism.",
        "published": "2007-08-23T22:23:04Z",
        "link": "http://arxiv.org/abs/0708.3259v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "Linear Time Algorithms Based on Multilevel Prefix Tree for Finding   Shortest Path with Positive Weights and Minimum Spanning Tree in a Networks",
        "authors": [
            "David S. Planeta"
        ],
        "summary": "In this paper I present general outlook on questions relevant to the basic graph algorithms; Finding the Shortest Path with Positive Weights and Minimum Spanning Tree. I will show so far known solution set of basic graph problems and present my own. My solutions to graph problems are characterized by their linear worst-case time complexity. It should be noticed that the algorithms which compute the Shortest Path and Minimum Spanning Tree problems not only analyze the weight of arcs (which is the main and often the only criterion of solution hitherto known algorithms) but also in case of identical path weights they select this path which walks through as few vertices as possible. I have presented algorithms which use priority queue based on multilevel prefix tree -- PTrie. PTrie is a clever combination of the idea of prefix tree -- Trie, the structure of logarithmic time complexity for insert and remove operations, doubly linked list and queues. In C++ I will implement linear worst-case time algorithm computing the Single-Destination Shortest-Paths problem and I will explain its usage.",
        "published": "2007-08-24T21:58:29Z",
        "link": "http://arxiv.org/abs/0708.3408v1",
        "categories": [
            "cs.DS",
            "F.2.2; G.2.2; E.1; C.2.1"
        ]
    },
    {
        "title": "Relative-Error CUR Matrix Decompositions",
        "authors": [
            "Petros Drineas",
            "Michael W. Mahoney",
            "S. Muthukrishnan"
        ],
        "summary": "Many data analysis applications deal with large matrices and involve approximating the matrix using a small number of ``components.'' Typically, these components are linear combinations of the rows and columns of the matrix, and are thus difficult to interpret in terms of the original features of the input data. In this paper, we propose and study matrix approximations that are explicitly expressed in terms of a small number of columns and/or rows of the data matrix, and thereby more amenable to interpretation in terms of the original data. Our main algorithmic results are two randomized algorithms which take as input an $m \\times n$ matrix $A$ and a rank parameter $k$. In our first algorithm, $C$ is chosen, and we let $A'=CC^+A$, where $C^+$ is the Moore-Penrose generalized inverse of $C$. In our second algorithm $C$, $U$, $R$ are chosen, and we let $A'=CUR$. ($C$ and $R$ are matrices that consist of actual columns and rows, respectively, of $A$, and $U$ is a generalized inverse of their intersection.) For each algorithm, we show that with probability at least $1-\\delta$: $$ ||A-A'||_F \\leq (1+\\epsilon) ||A-A_k||_F, $$ where $A_k$ is the ``best'' rank-$k$ approximation provided by truncating the singular value decomposition (SVD) of $A$. The number of columns of $C$ and rows of $R$ is a low-degree polynomial in $k$, $1/\\epsilon$, and $\\log(1/\\delta)$. Our two algorithms are the first polynomial time algorithms for such low-rank matrix approximations that come with relative-error guarantees; previously, in some cases, it was not even known whether such matrix decompositions exist. Both of our algorithms are simple, they take time of the order needed to approximately compute the top $k$ singular vectors of $A$, and they use a novel, intuitive sampling method called ``subspace sampling.''",
        "published": "2007-08-27T23:34:50Z",
        "link": "http://arxiv.org/abs/0708.3696v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Optimal Per-Edge Processing Times in the Semi-Streaming Model",
        "authors": [
            "Mariano Zelke"
        ],
        "summary": "We present semi-streaming algorithms for basic graph problems that have optimal per-edge processing times and therefore surpass all previous semi-streaming algorithms for these tasks. The semi-streaming model, which is appropriate when dealing with massive graphs, forbids random access to the input and restricts the memory to O(n*polylog n) bits.   Particularly, the formerly best per-edge processing times for finding the connected components and a bipartition are O(alpha(n)), for determining k-vertex and k-edge connectivity O(k^2n) and O(n*log n) respectively for any constant k and for computing a minimum spanning forest O(log n). All these time bounds we reduce to O(1).   Every presented algorithm determines a solution asymptotically as fast as the best corresponding algorithm up to date in the classical RAM model, which therefore cannot convert the advantage of unlimited memory and random access into superior computing times for these problems.",
        "published": "2007-08-31T07:19:27Z",
        "link": "http://arxiv.org/abs/0708.4284v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Pattern Matching in Trees and Strings",
        "authors": [
            "Philip Bille"
        ],
        "summary": "We study the design of efficient algorithms for combinatorial pattern matching. More concretely, we study algorithms for tree matching, string matching, and string matching in compressed texts.",
        "published": "2007-08-31T08:07:32Z",
        "link": "http://arxiv.org/abs/0708.4288v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Type-IV DCT, DST, and MDCT algorithms with reduced numbers of arithmetic   operations",
        "authors": [
            "Xuancheng Shao",
            "Steven G. Johnson"
        ],
        "summary": "We present algorithms for the type-IV discrete cosine transform (DCT-IV) and discrete sine transform (DST-IV), as well as for the modified discrete cosine transform (MDCT) and its inverse, that achieve a lower count of real multiplications and additions than previously published algorithms, without sacrificing numerical accuracy. Asymptotically, the operation count is reduced from ~2NlogN to ~(17/9)NlogN for a power-of-two transform size N, and the exact count is strictly lowered for all N > 4. These results are derived by considering the DCT to be a special case of a DFT of length 8N, with certain symmetries, and then pruning redundant operations from a recent improved fast Fourier transform algorithm (based on a recursive rescaling of the conjugate-pair split radix algorithm). The improved algorithms for DST-IV and MDCT follow immediately from the improved count for the DCT-IV.",
        "published": "2007-08-31T18:00:33Z",
        "link": "http://arxiv.org/abs/0708.4399v2",
        "categories": [
            "cs.DS",
            "cs.NA"
        ]
    },
    {
        "title": "Double Clustering and Graph Navigability",
        "authors": [
            "Oskar Sandberg"
        ],
        "summary": "Graphs are called navigable if one can find short paths through them using only local knowledge. It has been shown that for a graph to be navigable, its construction needs to meet strict criteria. Since such graphs nevertheless seem to appear in nature, it is of interest to understand why these criteria should be fulfilled.   In this paper we present a simple method for constructing graphs based on a model where nodes vertices are ``similar'' in two different ways, and tend to connect to those most similar to them - or cluster - with respect to both. We prove that this leads to navigable networks for several cases, and hypothesize that it also holds in great generality. Enough generality, perhaps, to explain the occurrence of navigable networks in nature.",
        "published": "2007-09-04T19:38:14Z",
        "link": "http://arxiv.org/abs/0709.0511v1",
        "categories": [
            "math.PR",
            "cs.DS",
            "math.CO",
            "60C05, 68R10, 68W20"
        ]
    },
    {
        "title": "On Faster Integer Calculations using Non-Arithmetic Primitives",
        "authors": [
            "Katharina Lürwer-Brüggemeier",
            "Martin Ziegler"
        ],
        "summary": "The unit cost model is both convenient and largely realistic for describing integer decision algorithms over (+,*). Additional operations like division with remainder or bitwise conjunction, although equally supported by computing hardware, may lead to a considerable drop in complexity. We show a variety of concrete problems to benefit from such NON-arithmetic primitives by presenting and analyzing corresponding fast algorithms.",
        "published": "2007-09-05T11:34:54Z",
        "link": "http://arxiv.org/abs/0709.0624v1",
        "categories": [
            "cs.DS",
            "F.1.1; F.2.2"
        ]
    },
    {
        "title": "Using Data Compressors to Construct Rank Tests",
        "authors": [
            "Daniil Ryabko",
            "Juergen Schmidhuber"
        ],
        "summary": "Nonparametric rank tests for homogeneity and component independence are proposed, which are based on data compressors. For homogeneity testing the idea is to compress the binary string obtained by ordering the two joint samples and writing 0 if the element is from the first sample and 1 if it is from the second sample and breaking ties by randomization (extension to the case of multiple samples is straightforward). $H_0$ should be rejected if the string is compressed (to a certain degree) and accepted otherwise. We show that such a test obtained from an ideal data compressor is valid against all alternatives. Component independence is reduced to homogeneity testing by constructing two samples, one of which is the first half of the original and the other is the second half with one of the components randomly permuted.",
        "published": "2007-09-05T15:06:04Z",
        "link": "http://arxiv.org/abs/0709.0670v1",
        "categories": [
            "cs.DS",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Complexity of Protein Local Structure Alignment Under the   Discrete Fréchet Distance",
        "authors": [
            "Binhai Zhu"
        ],
        "summary": "We show that given $m$ proteins (or protein backbones, which are modeled as 3D polygonal chains each of length O(n)) the problem of protein local structure alignment under the discrete Fr\\'{e}chet distance is as hard as Independent Set. So the problem does not admit any approximation of factor $n^{1-\\epsilon}$. This is the strongest negative result regarding the protein local structure alignment problem. On the other hand, if $m$ is a constant, then the problem can be solved in polygnomial time.",
        "published": "2007-09-05T15:30:54Z",
        "link": "http://arxiv.org/abs/0709.0677v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Finding Paths and Cycles in Graphs",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "A polynomial time algorithm which detects all paths and cycles of all lengths in form of vertex pairs (start, finish).",
        "published": "2007-09-07T00:04:20Z",
        "link": "http://arxiv.org/abs/0709.0974v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS",
            "math.CO",
            "G.2.2; G.2.0; F.1.3; F.1.0"
        ]
    },
    {
        "title": "Efficient Algorithms for Node Disjoint Subgraph Homeomorphism   Determination",
        "authors": [
            "Yanghua Xiao",
            "Wentao Wu",
            "Wei Wang",
            "Zhengying He"
        ],
        "summary": "Recently, great efforts have been dedicated to researches on the management of large scale graph based data such as WWW, social networks, biological networks. In the study of graph based data management, node disjoint subgraph homeomorphism relation between graphs is more suitable than (sub)graph isomorphism in many cases, especially in those cases that node skipping and node mismatching are allowed. However, no efficient node disjoint subgraph homeomorphism determination (ndSHD) algorithms have been available. In this paper, we propose two computationally efficient ndSHD algorithms based on state spaces searching with backtracking, which employ many heuristics to prune the search spaces. Experimental results on synthetic data sets show that the proposed algorithms are efficient, require relative little time in most of the testing cases, can scale to large or dense graphs, and can accommodate to more complex fuzzy matching cases.",
        "published": "2007-09-08T18:14:47Z",
        "link": "http://arxiv.org/abs/0709.1227v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Distribution of PageRank Mass Among Principle Components of the Web",
        "authors": [
            "Konstantin Avrachenkov",
            "Nelly Litvak",
            "Kim Son Pham"
        ],
        "summary": "We study the PageRank mass of principal components in a bow-tie Web Graph, as a function of the damping factor c. Using a singular perturbation approach, we show that the PageRank share of IN and SCC components remains high even for very large values of the damping factor, in spite of the fact that it drops to zero when c goes to one. However, a detailed study of the OUT component reveals the presence ``dead-ends'' (small groups of pages linking only to each other) that receive an unfairly high ranking when c is close to one. We argue that this problem can be mitigated by choosing c as small as 1/2.",
        "published": "2007-09-13T08:29:53Z",
        "link": "http://arxiv.org/abs/0709.2016v1",
        "categories": [
            "cs.NI",
            "cs.DS"
        ]
    },
    {
        "title": "When are recommender systems useful?",
        "authors": [
            "Marcel Blattner",
            "Alexander Hunziker",
            "Paolo Laureti"
        ],
        "summary": "Recommender systems are crucial tools to overcome the information overload brought about by the Internet. Rigorous tests are needed to establish to what extent sophisticated methods can improve the quality of the predictions. Here we analyse a refined correlation-based collaborative filtering algorithm and compare it with a novel spectral method for recommending. We test them on two databases that bear different statistical properties (MovieLens and Jester) without filtering out the less active users and ordering the opinions in time, whenever possible. We find that, when the distribution of user-user correlations is narrow, simple averages work nearly as well as advanced methods. Recommender systems can, on the other hand, exploit a great deal of additional information in systems where external influence is negligible and peoples' tastes emerge entirely. These findings are validated by simulations with artificially generated data.",
        "published": "2007-09-17T09:27:07Z",
        "link": "http://arxiv.org/abs/0709.2562v1",
        "categories": [
            "cs.IR",
            "cs.CY",
            "cs.DL",
            "cs.DS",
            "physics.data-an",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Incremental Satisfiability and Implication for UTVPI Constraints",
        "authors": [
            "Andreas Schutt",
            "Peter J. Stuckey"
        ],
        "summary": "Unit two-variable-per-inequality (UTVPI) constraints form one of the largest class of integer constraints which are polynomial time solvable (unless P=NP). There is considerable interest in their use for constraint solving, abstract interpretation, spatial databases, and theorem proving. In this paper we develop a new incremental algorithm for UTVPI constraint satisfaction and implication checking that requires O(m + n log n + p) time and O(n+m+p) space to incrementally check satisfiability of m UTVPI constraints on n variables and check implication of p UTVPI constraints.",
        "published": "2007-09-19T06:58:05Z",
        "link": "http://arxiv.org/abs/0709.2961v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "cs.LO",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Query Evaluation in P2P Systems of Taxonomy-based Sources: Algorithms,   Complexity, and Optimizations",
        "authors": [
            "Carlo Meghini",
            "Yannis Tzitzikas",
            "Anastasia Analyti"
        ],
        "summary": "In this study, we address the problem of answering queries over a peer-to-peer system of taxonomy-based sources. A taxonomy states subsumption relationships between negation-free DNF formulas on terms and negation-free conjunctions of terms. To the end of laying the foundations of our study, we first consider the centralized case, deriving the complexity of the decision problem and of query evaluation. We conclude by presenting an algorithm that is efficient in data complexity and is based on hypergraphs. More expressive forms of taxonomies are also investigated, which however lead to intractability. We then move to the distributed case, and introduce a logical model of a network of taxonomy-based sources. On such network, a distributed version of the centralized algorithm is then presented, based on a message passing paradigm, and its correctness is proved. We finally discuss optimization issues, and relate our work to the literature.",
        "published": "2007-09-19T15:10:05Z",
        "link": "http://arxiv.org/abs/0709.3034v1",
        "categories": [
            "cs.DB",
            "cs.DC",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "Weighted Matching in the Semi-Streaming Model",
        "authors": [
            "Mariano Zelke"
        ],
        "summary": "We reduce the best known approximation ratio for finding a weighted matching of a graph using a one-pass semi-streaming algorithm from 5.828 to 5.585. The semi-streaming model forbids random access to the input and restricts the memory to O(n*polylog(n)) bits. It was introduced by Muthukrishnan in 2003 and is appropriate when dealing with massive graphs.",
        "published": "2007-09-21T09:34:19Z",
        "link": "http://arxiv.org/abs/0709.3384v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Set Matrices and The Path/Cycle Problem",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Presentation of set matrices and demonstration of their efficiency as a tool using the path/cycle problem.",
        "published": "2007-09-26T21:44:10Z",
        "link": "http://arxiv.org/abs/0709.4273v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS",
            "math.CO",
            "G.2.2; G.2.0; F.1.3; F.1.0"
        ]
    },
    {
        "title": "Sorting and Selection with Random Costs",
        "authors": [
            "Stanislav Angelov",
            "Keshav Kunal",
            "Andrew McGregor"
        ],
        "summary": "There is a growing body of work on sorting and selection in models other than the unit-cost comparison model. This work is the first treatment of a natural stochastic variant of the problem where the cost of comparing two elements is a random variable. Each cost is chosen independently and is known to the algorithm. In particular we consider the following three models: each cost is chosen uniformly in the range $[0,1]$, each cost is 0 with some probability $p$ and 1 otherwise, or each cost is 1 with probability $p$ and infinite otherwise. We present lower and upper bounds (optimal in most cases) for these problems. We obtain our upper bounds by carefully designing algorithms to ensure that the costs incurred at various stages are independent and using properties of random partial orders when appropriate.",
        "published": "2007-09-29T18:10:28Z",
        "link": "http://arxiv.org/abs/0710.0083v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Incomplete Lineage Sorting: Consistent Phylogeny Estimation From   Multiple Loci",
        "authors": [
            "Elchanan Mossel",
            "Sebastien Roch"
        ],
        "summary": "We introduce a simple algorithm for reconstructing phylogenies from multiple gene trees in the presence of incomplete lineage sorting, that is, when the topology of the gene trees may differ from that of the species tree. We show that our technique is statistically consistent under standard stochastic assumptions, that is, it returns the correct tree given sufficiently many unlinked loci. We also show that it can tolerate moderate estimation errors.",
        "published": "2007-10-01T11:11:43Z",
        "link": "http://arxiv.org/abs/0710.0262v2",
        "categories": [
            "q-bio.PE",
            "cs.CE",
            "cs.DS",
            "math.PR",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Fast minimum-weight double-tree shortcutting for Metric TSP: Is the best   one good enough?",
        "authors": [
            "Vladimir Deineko",
            "Alexander Tiskin"
        ],
        "summary": "The Metric Traveling Salesman Problem (TSP) is a classical NP-hard optimization problem. The double-tree shortcutting method for Metric TSP yields an exponentially-sized space of TSP tours, each of which approximates the optimal solution within at most a factor of 2. We consider the problem of finding among these tours the one that gives the closest approximation, i.e.\\ the \\emph{minimum-weight double-tree shortcutting}. Burkard et al. gave an algorithm for this problem, running in time $O(n^3+2^d n^2)$ and memory $O(2^d n^2)$, where $d$ is the maximum node degree in the rooted minimum spanning tree. We give an improved algorithm for the case of small $d$ (including planar Euclidean TSP, where $d \\leq 4$), running in time $O(4^d n^2)$ and memory $O(4^d n)$. This improvement allows one to solve the problem on much larger instances than previously attempted. Our computational experiments suggest that in terms of the time-quality tradeoff, the minimum-weight double-tree shortcutting method provides one of the best known tour-constructing heuristics.",
        "published": "2007-10-01T15:25:18Z",
        "link": "http://arxiv.org/abs/0710.0318v3",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "A Novel Solution to the ATT48 Benchmark Problem",
        "authors": [
            "Anthony A. Ruffa"
        ],
        "summary": "A solution to the benchmark ATT48 Traveling Salesman Problem (from the TSPLIB95 library) results from isolating the set of vertices into ten open-ended zones with nine lengthwise boundaries. In each zone, a minimum-length Hamiltonian Path (HP) is found for each combination of boundary vertices, leading to an approximation for the minimum-length Hamiltonian Cycle (HC). Determination of the optimal HPs for subsequent zones has the effect of automatically filtering out non-optimal HPs from earlier zones. Although the optimal HC for ATT48 involves only two crossing edges between all zones (with one exception), adding inter-zone edges can accommodate more complex problems.",
        "published": "2007-10-02T14:26:33Z",
        "link": "http://arxiv.org/abs/0710.0539v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Connectivity of Random 1-Dimensional Networks",
        "authors": [
            "V. Kurlin",
            "L. Mihaylova"
        ],
        "summary": "An important problem in wireless sensor networks is to find the minimal number of randomly deployed sensors making a network connected with a given probability. In practice sensors are often deployed one by one along a trajectory of a vehicle, so it is natural to assume that arbitrary probability density functions of distances between successive sensors in a segment are given. The paper computes the probability of connectivity and coverage of 1-dimensional networks and gives estimates for a minimal number of sensors for important distributions.",
        "published": "2007-10-04T12:57:34Z",
        "link": "http://arxiv.org/abs/0710.1001v2",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "stat.AP",
            "C.2.1"
        ]
    },
    {
        "title": "Faster Least Squares Approximation",
        "authors": [
            "Petros Drineas",
            "Michael W. Mahoney",
            "S. Muthukrishnan",
            "Tamas Sarlos"
        ],
        "summary": "Least squares approximation is a technique to find an approximate solution to a system of linear equations that has no exact solution. In a typical setting, one lets $n$ be the number of constraints and $d$ be the number of variables, with $n \\gg d$. Then, existing exact methods find a solution vector in $O(nd^2)$ time. We present two randomized algorithms that provide very accurate relative-error approximations to the optimal value and the solution vector of a least squares approximation problem more rapidly than existing exact algorithms. Both of our algorithms preprocess the data with the Randomized Hadamard Transform. One then uniformly randomly samples constraints and solves the smaller problem on those constraints, and the other performs a sparse random projection and solves the smaller problem on those projected coordinates. In both cases, solving the smaller problem provides relative-error approximations, and, if $n$ is sufficiently larger than $d$, the approximate solution can be computed in $O(nd \\log d)$ time.",
        "published": "2007-10-07T17:37:37Z",
        "link": "http://arxiv.org/abs/0710.1435v4",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Efficient Optimally Lazy Algorithms for Minimal-Interval Semantics",
        "authors": [
            "Sebastiano Vigna",
            "Paolo Boldi"
        ],
        "summary": "Minimal-interval semantics associates with each query over a document a set of intervals, called witnesses, that are incomparable with respect to inclusion (i.e., they form an antichain): witnesses define the minimal regions of the document satisfying the query. Minimal-interval semantics makes it easy to define and compute several sophisticated proximity operators, provides snippets for user presentation, and can be used to rank documents. In this paper we provide algorithms for computing conjunction and disjunction that are linear in the number of intervals and logarithmic in the number of operands; for additional operators, such as ordered conjunction and Brouwerian difference, we provide linear algorithms. In all cases, space is linear in the number of operands. More importantly, we define a formal notion of optimal laziness, and either prove it, or prove its impossibility, for each algorithm. We cast our results in a general framework of antichains of intervals on total orders, making our algorithms directly applicable to other domains.",
        "published": "2007-10-08T12:15:48Z",
        "link": "http://arxiv.org/abs/0710.1525v2",
        "categories": [
            "cs.DS",
            "cs.IR",
            "F.2.m; H.3.3"
        ]
    },
    {
        "title": "An explicit universal cycle for the (n-1)-permutations of an n-set",
        "authors": [
            "Frank Ruskey",
            "Aaron Williams"
        ],
        "summary": "We show how to construct an explicit Hamilton cycle in the directed Cayley graph Cay({\\sigma_n, sigma_{n-1}} : \\mathbb{S}_n), where \\sigma_k = (1 2 >... k). The existence of such cycles was shown by Jackson (Discrete Mathematics, 149 (1996) 123-129) but the proof only shows that a certain directed graph is Eulerian, and Knuth (Volume 4 Fascicle 2, Generating All Tuples and Permutations (2005)) asks for an explicit construction. We show that a simple recursion describes our Hamilton cycle and that the cycle can be generated by an iterative algorithm that uses O(n) space. Moreover, the algorithm produces each successive edge of the cycle in constant time; such algorithms are said to be loopless.",
        "published": "2007-10-09T18:06:05Z",
        "link": "http://arxiv.org/abs/0710.1842v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "G.2.1"
        ]
    },
    {
        "title": "Sleeping on the Job: Energy-Efficient Broadcast for Radio Networks",
        "authors": [
            "Valerie King",
            "Cynthia Phillips",
            "Jared Saia",
            "Maxwell Young"
        ],
        "summary": "We address the problem of minimizing power consumption when performing reliable broadcast on a radio network under the following popular model. Each node in the network is located on a point in a two dimensional grid, and whenever a node sends a message, all awake nodes within distance r receive the message. In the broadcast problem, some node wants to successfully send a message to all other nodes in the network even when up to a 1/2 fraction of the nodes within every neighborhood can be deleted by an adversary. The set of deleted nodes is carefully chosen by the adversary to foil our algorithm and moreover, the set of deleted nodes may change periodically. This models worst-case behavior due to mobile nodes, static nodes losing power or simply some points in the grid being unoccupied. A trivial solution requires each node in the network to be awake roughly 1/2 the time, and a trivial lower bound shows that each node must be awake for at least a 1/n fraction of the time. Our first result is an algorithm that requires each node to be awake for only a 1/sqrt(n) fraction of the time in expectation. Our algorithm achieves this while ensuring correctness with probability 1, and keeping optimal values for other resource costs such as latency and number of messages sent. We give a lower-bound that shows that this reduction in power consumption is asymptotically optimal when latency and number of messages sent must be optimal. If we can increase the latency and messages sent by only a log*n factor we give a Las Vegas algorithm that requires each node to be awake for only a (log*n)/n expected fraction of the time; we give a lower-bound showing that this second algorithm is near optimal. Finally, we show how to ensure energy-efficient broadcast in the presence of Byzantine faults.",
        "published": "2007-10-12T19:56:45Z",
        "link": "http://arxiv.org/abs/0710.2532v1",
        "categories": [
            "cs.DS",
            "F.2.0; C.2.1"
        ]
    },
    {
        "title": "Bloom maps",
        "authors": [
            "David Talbot",
            "John Talbot"
        ],
        "summary": "We consider the problem of succinctly encoding a static map to support approximate queries. We derive upper and lower bounds on the space requirements in terms of the error rate and the entropy of the distribution of values over keys: our bounds differ by a factor log e. For the upper bound we introduce a novel data structure, the Bloom map, generalising the Bloom filter to this problem. The lower bound follows from an information theoretic argument.",
        "published": "2007-10-17T09:35:14Z",
        "link": "http://arxiv.org/abs/0710.3246v1",
        "categories": [
            "cs.DS",
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "On a Clique-Based Integer Programming Formulation of Vertex Colouring   with Applications in Course Timetabling",
        "authors": [
            "Edmund K. Burke",
            "Jakub Marecek",
            "Andrew J. Parkes",
            "Hana Rudova"
        ],
        "summary": "Vertex colouring is a well-known problem in combinatorial optimisation, whose alternative integer programming formulations have recently attracted considerable attention. This paper briefly surveys seven known formulations of vertex colouring and introduces a formulation of vertex colouring using a suitable clique partition of the graph. This formulation is applicable in timetabling applications, where such a clique partition of the conflict graph is given implicitly. In contrast with some alternatives, the presented formulation can also be easily extended to accommodate complex performance indicators (``soft constraints'') imposed in a number of real-life course timetabling applications. Its performance depends on the quality of the clique partition, but encouraging empirical results for the Udine Course Timetabling problem are reported.",
        "published": "2007-10-18T21:38:37Z",
        "link": "http://arxiv.org/abs/0710.3603v3",
        "categories": [
            "cs.DM",
            "cs.DS",
            "math.CO",
            "G.1.6; G.2.2; G.2.3"
        ]
    },
    {
        "title": "On the Complexity of Spill Everywhere under SSA Form",
        "authors": [
            "Florent Bouchez",
            "Alain Darte",
            "Fabrice Rastello"
        ],
        "summary": "Compilation for embedded processors can be either aggressive (time consuming cross-compilation) or just in time (embedded and usually dynamic). The heuristics used in dynamic compilation are highly constrained by limited resources, time and memory in particular. Recent results on the SSA form open promising directions for the design of new register allocation heuristics for embedded systems and especially for embedded compilation. In particular, heuristics based on tree scan with two separated phases -- one for spilling, then one for coloring/coalescing -- seem good candidates for designing memory-friendly, fast, and competitive register allocators. Still, also because of the side effect on power consumption, the minimization of loads and stores overhead (spilling problem) is an important issue. This paper provides an exhaustive study of the complexity of the ``spill everywhere'' problem in the context of the SSA form. Unfortunately, conversely to our initial hopes, many of the questions we raised lead to NP-completeness results. We identify some polynomial cases but that are impractical in JIT context. Nevertheless, they can give hints to simplify formulations for the design of aggressive allocators.",
        "published": "2007-10-19T07:24:58Z",
        "link": "http://arxiv.org/abs/0710.3642v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Deterministic Secure Positioning in Wireless Sensor Networks",
        "authors": [
            "Sylvie Delaët",
            "Partha Sarathi Mandal",
            "Mariusz Rokicki",
            "Sébastien Tixeuil"
        ],
        "summary": "Properly locating sensor nodes is an important building block for a large subset of wireless sensor networks (WSN) applications. As a result, the performance of the WSN degrades significantly when misbehaving nodes report false location and distance information in order to fake their actual location. In this paper we propose a general distributed deterministic protocol for accurate identification of faking sensors in a WSN. Our scheme does \\emph{not} rely on a subset of \\emph{trusted} nodes that are not allowed to misbehave and are known to every node in the network. Thus, any subset of nodes is allowed to try faking its position. As in previous approaches, our protocol is based on distance evaluation techniques developed for WSN. On the positive side, we show that when the received signal strength (RSS) technique is used, our protocol handles at most $\\lfloor \\frac{n}{2} \\rfloor-2$ faking sensors. Also, when the time of flight (ToF) technique is used, our protocol manages at most $\\lfloor \\frac{n}{2} \\rfloor - 3$ misbehaving sensors. On the negative side, we prove that no deterministic protocol can identify faking sensors if their number is $\\lceil \\frac{n}{2}\\rceil -1$. Thus our scheme is almost optimal with respect to the number of faking sensors. We discuss application of our technique in the trusted sensor model. More precisely our results can be used to minimize the number of trusted sensors that are needed to defeat faking ones.",
        "published": "2007-10-22T07:29:13Z",
        "link": "http://arxiv.org/abs/0710.3824v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.DS",
            "cs.NI"
        ]
    },
    {
        "title": "A Multi-level Blocking Distinct Degree Factorization Algorithm",
        "authors": [
            "Richard Brent",
            "Paul Zimmermann"
        ],
        "summary": "We give a new algorithm for performing the distinct-degree factorization of a polynomial P(x) over GF(2), using a multi-level blocking strategy. The coarsest level of blocking replaces GCD computations by multiplications, as suggested by Pollard (1975), von zur Gathen and Shoup (1992), and others. The novelty of our approach is that a finer level of blocking replaces multiplications by squarings, which speeds up the computation in GF(2)[x]/P(x) of certain interval polynomials when P(x) is sparse. As an application we give a fast algorithm to search for all irreducible trinomials x^r + x^s + 1 of degree r over GF(2), while producing a certificate that can be checked in less time than the full search. Naive algorithms cost O(r^2) per trinomial, thus O(r^3) to search over all trinomials of given degree r. Under a plausible assumption about the distribution of factors of trinomials, the new algorithm has complexity O(r^2 (log r)^{3/2}(log log r)^{1/2}) for the search over all trinomials of degree r. Our implementation achieves a speedup of greater than a factor of 560 over the naive algorithm in the case r = 24036583 (a Mersenne exponent). Using our program, we have found two new primitive trinomials of degree 24036583 over GF(2) (the previous record degree was 6972593).",
        "published": "2007-10-24T09:18:33Z",
        "link": "http://arxiv.org/abs/0710.4410v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Code Similarity on High Level Programs",
        "authors": [
            "M. Miron Bernal",
            "H. Coyote Estrada",
            "J. Figueroa Nazuno"
        ],
        "summary": "This paper presents a new approach for code similarity on High Level programs. Our technique is based on Fast Dynamic Time Warping, that builds a warp path or points relation with local restrictions. The source code is represented into Time Series using the operators inside programming languages that makes possible the comparison. This makes possible subsequence detection that represent similar code instructions. In contrast with other code similarity algorithms, we do not make features extraction. The experiments show that two source codes are similar when their respective Time Series are similar.",
        "published": "2007-10-29T22:39:21Z",
        "link": "http://arxiv.org/abs/0710.5547v1",
        "categories": [
            "cs.CV",
            "cs.DS",
            "I.5.2"
        ]
    },
    {
        "title": "Convex and linear models of NP-problems",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Reducing the NP-problems to the convex/linear analysis on the Birkhoff polytope.",
        "published": "2007-11-01T08:33:07Z",
        "link": "http://arxiv.org/abs/0711.0086v2",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS",
            "math.CO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "A Tutorial on Spectral Clustering",
        "authors": [
            "Ulrike von Luxburg"
        ],
        "summary": "In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.",
        "published": "2007-11-01T19:04:43Z",
        "link": "http://arxiv.org/abs/0711.0189v1",
        "categories": [
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Faster Algorithms for Online Topological Ordering",
        "authors": [
            "Telikepalli Kavitha",
            "Rogers Mathew"
        ],
        "summary": "We present two algorithms for maintaining the topological order of a directed acyclic graph with n vertices, under an online edge insertion sequence of m edges. Efficient algorithms for online topological ordering have many applications, including online cycle detection, which is to discover the first edge that introduces a cycle under an arbitrary sequence of edge insertions in a directed graph. In this paper we present efficient algorithms for the online topological ordering problem.   We first present a simple algorithm with running time O(n^{5/2}) for the online topological ordering problem. This is the current fastest algorithm for this problem on dense graphs, i.e., when m > n^{5/3}. We then present an algorithm with running time O((m + nlog n)\\sqrt{m}); this is more efficient for sparse graphs. Our results yield an improved upper bound of O(min(n^{5/2}, (m + nlog n)sqrt{m})) for the online topological ordering problem.",
        "published": "2007-11-02T06:42:43Z",
        "link": "http://arxiv.org/abs/0711.0251v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Improving the LP bound of a MILP by branching concurrently",
        "authors": [
            "H. Georg Buesching"
        ],
        "summary": "We'll measure the differences of the dual variables and the gain of the objective function when creating new problems, which each has one inequality more than the starting LP-instance. These differences of the dual variables are naturally connected to the branches. Then we'll choose those differences of dual variables, so that for all combinations of choices at the connected branches, all dual inequalities will hold for sure. By adding the gain of each chosen branching, we get a total gain, which gives a better limit of the original problem. By this technique it is also possible to create cuts.",
        "published": "2007-11-02T13:57:41Z",
        "link": "http://arxiv.org/abs/0711.0311v2",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Simple Recursive Games",
        "authors": [
            "Daniel Andersson",
            "Kristoffer Arnsfelt Hansen",
            "Peter Bro Miltersen",
            "Troels Bjerre Sorensen"
        ],
        "summary": "We define the class of \"simple recursive games\". A simple recursive game is defined as a simple stochastic game (a notion due to Anne Condon), except that we allow arbitrary real payoffs but disallow moves of chance. We study the complexity of solving simple recursive games and obtain an almost-linear time comparison-based algorithm for computing an equilibrium of such a game. The existence of a linear time comparison-based algorithm remains an open problem.",
        "published": "2007-11-07T10:23:47Z",
        "link": "http://arxiv.org/abs/0711.1055v1",
        "categories": [
            "cs.GT",
            "cs.DS"
        ]
    },
    {
        "title": "Data Structures for Mergeable Trees",
        "authors": [
            "Loukas Georgiadis",
            "Haim Kaplan",
            "Nira Shafrir",
            "Robert E. Tarjan",
            "Renato F. Werneck"
        ],
        "summary": "Motivated by an application in computational topology, we consider a novel variant of the problem of efficiently maintaining dynamic rooted trees. This variant requires merging two paths in a single operation. In contrast to the standard problem, in which only one tree arc changes at a time, a single merge operation can change many arcs. In spite of this, we develop a data structure that supports merges on an n-node forest in O(log^2 n) amortized time and all other standard tree operations in O(log n) time (amortized, worst-case, or randomized depending on the underlying data structure). For the special case that occurs in the motivating application, in which arbitrary arc deletions (cuts) are not allowed, we give a data structure with an O(log n) time bound per operation. This is asymptotically optimal under certain assumptions. For the even-more special case in which both cuts and parent queries are disallowed, we give an alternative O(log n)-time solution that uses standard dynamic trees as a black box. This solution also applies to the motivating application. Our methods use previous work on dynamic trees in various ways, but the analysis of each algorithm requires novel ideas. We also investigate lower bounds for the problem under various assumptions.",
        "published": "2007-11-11T21:28:20Z",
        "link": "http://arxiv.org/abs/0711.1682v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "On Approximating Multi-Criteria TSP",
        "authors": [
            "Bodo Manthey"
        ],
        "summary": "We present approximation algorithms for almost all variants of the multi-criteria traveling salesman problem (TSP).   First, we devise randomized approximation algorithms for multi-criteria maximum traveling salesman problems (Max-TSP). For multi-criteria Max-STSP, where the edge weights have to be symmetric, we devise an algorithm with an approximation ratio of 2/3 - eps. For multi-criteria Max-ATSP, where the edge weights may be asymmetric, we present an algorithm with a ratio of 1/2 - eps. Our algorithms work for any fixed number k of objectives. Furthermore, we present a deterministic algorithm for bi-criteria Max-STSP that achieves an approximation ratio of 7/27.   Finally, we present a randomized approximation algorithm for the asymmetric multi-criteria minimum TSP with triangle inequality Min-ATSP. This algorithm achieves a ratio of log n + eps.",
        "published": "2007-11-14T10:53:49Z",
        "link": "http://arxiv.org/abs/0711.2157v3",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Minimum-weight double-tree shortcutting for Metric TSP: Bounding the   approximation ratio",
        "authors": [
            "Vladimir Deineko",
            "Alexander Tiskin"
        ],
        "summary": "The Metric Traveling Salesman Problem (TSP) is a classical NP-hard optimization problem. The double-tree shortcutting method for Metric TSP yields an exponentially-sized space of TSP tours, each of which approximates the optimal solution within at most a factor of 2. We consider the problem of finding among these tours the one that gives the closest approximation, i.e.\\ the \\emph{minimum-weight double-tree shortcutting}. Previously, we gave an efficient algorithm for this problem, and carried out its experimental analysis. In this paper, we address the related question of the worst-case approximation ratio for the minimum-weight double-tree shortcutting method. In particular, we give lower bounds on the approximation ratio in some specific metric spaces: the ratio of 2 in the discrete shortest path metric, 1.622 in the planar Euclidean metric, and 1.666 in the planar Minkowski metric. The first of these lower bounds is tight; we conjecture that the other two bounds are also tight, and in particular that the minimum-weight double-tree method provides a 1.622-approximation for planar Euclidean TSP.",
        "published": "2007-11-15T13:19:01Z",
        "link": "http://arxiv.org/abs/0711.2399v3",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Computing the Tutte polynomial in vertex-exponential time",
        "authors": [
            "Andreas Björklund",
            "Thore Husfeldt",
            "Petteri Kaski",
            "Mikko Koivisto"
        ],
        "summary": "The deletion--contraction algorithm is perhaps the most popular method for computing a host of fundamental graph invariants such as the chromatic, flow, and reliability polynomials in graph theory, the Jones polynomial of an alternating link in knot theory, and the partition functions of the models of Ising, Potts, and Fortuin--Kasteleyn in statistical physics. Prior to this work, deletion--contraction was also the fastest known general-purpose algorithm for these invariants, running in time roughly proportional to the number of spanning trees in the input graph. Here, we give a substantially faster algorithm that computes the Tutte polynomial--and hence, all the aforementioned invariants and more--of an arbitrary graph in time within a polynomial factor of the number of connected vertex sets. The algorithm actually evaluates a multivariate generalization of the Tutte polynomial by making use of an identity due to Fortuin and Kasteleyn. We also provide a polynomial-space variant of the algorithm and give an analogous result for Chung and Graham's cover polynomial. An implementation of the algorithm outperforms deletion--contraction also in practice.",
        "published": "2007-11-16T10:51:10Z",
        "link": "http://arxiv.org/abs/0711.2585v4",
        "categories": [
            "cs.DS",
            "cond-mat.stat-mech",
            "math.CO",
            "F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Finding a Feasible Flow in a Strongly Connected Network",
        "authors": [
            "Bernhard Haeupler",
            "Robert E. Tarjan"
        ],
        "summary": "We consider the problem of finding a feasible single-commodity flow in a strongly connected network with fixed supplies and demands, provided that the sum of supplies equals the sum of demands and the minimum arc capacity is at least this sum. A fast algorithm for this problem improves the worst-case time bound of the Goldberg-Rao maximum flow method by a constant factor. Erlebach and Hagerup gave an linear-time feasible flow algorithm. We give an arguably simpler one.",
        "published": "2007-11-17T01:59:53Z",
        "link": "http://arxiv.org/abs/0711.2710v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Improved Fully Dynamic Reachability Algorithm for Directed Graph",
        "authors": [
            "Venkata Seshu Kumar Kurapati"
        ],
        "summary": "We propose a fully dynamic algorithm for maintaining reachability information in directed graphs. The proposed deterministic dynamic algorithm has an update time of $O((ins*n^{2}) + (del * (m+n*log(n))))$ where $m$ is the current number of edges, $n$ is the number of vertices in the graph, $ins$ is the number of edge insertions and $del$ is the number of edge deletions. Each query can be answered in O(1) time after each update. The proposed algorithm combines existing fully dynamic reachability algorithm with well known witness counting technique to improve efficiency of maintaining reachability information when edges are deleted. The proposed algorithm improves by a factor of $O(\\frac{n^2}{m+n*log(n)})$ for edge deletion over the best existing fully dynamic algorithm for maintaining reachability information.",
        "published": "2007-11-21T03:22:12Z",
        "link": "http://arxiv.org/abs/0711.3250v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Weak vs. Self vs. Probabilistic Stabilization",
        "authors": [
            "Stéphane Devismes",
            "Sébastien Tixeuil",
            "Masafumi Yamashita"
        ],
        "summary": "Self-stabilization is a strong property that guarantees that a network always resume correct behavior starting from an arbitrary initial state. Weaker guarantees have later been introduced to cope with impossibility results: probabilistic stabilization only gives probabilistic convergence to a correct behavior. Also, weak stabilization only gives the possibility of convergence. In this paper, we investigate the relative power of weak, self, and probabilistic stabilization, with respect to the set of problems that can be solved. We formally prove that in that sense, weak stabilization is strictly stronger that self-stabilization. Also, we refine previous results on weak stabilization to prove that, for practical schedule instances, a deterministic weak-stabilizing protocol can be turned into a probabilistic self-stabilizing one. This latter result hints at more practical use of weak-stabilization, as such algorthms are easier to design and prove than their (probabilistic) self-stabilizing counterparts.",
        "published": "2007-11-23T07:17:25Z",
        "link": "http://arxiv.org/abs/0711.3672v2",
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.NI"
        ]
    },
    {
        "title": "Approximation Algorithms for Restless Bandit Problems",
        "authors": [
            "Sudipto Guha",
            "Kamesh Munagala",
            "Peng Shi"
        ],
        "summary": "The restless bandit problem is one of the most well-studied generalizations of the celebrated stochastic multi-armed bandit problem in decision theory. In its ultimate generality, the restless bandit problem is known to be PSPACE-Hard to approximate to any non-trivial factor, and little progress has been made despite its importance in modeling activity allocation under uncertainty.   We consider a special case that we call Feedback MAB, where the reward obtained by playing each of n independent arms varies according to an underlying on/off Markov process whose exact state is only revealed when the arm is played. The goal is to design a policy for playing the arms in order to maximize the infinite horizon time average expected reward. This problem is also an instance of a Partially Observable Markov Decision Process (POMDP), and is widely studied in wireless scheduling and unmanned aerial vehicle (UAV) routing. Unlike the stochastic MAB problem, the Feedback MAB problem does not admit to greedy index-based optimal policies.   We develop a novel and general duality-based algorithmic technique that yields a surprisingly simple and intuitive 2+epsilon-approximate greedy policy to this problem. We then define a general sub-class of restless bandit problems that we term Monotone bandits, for which our policy is a 2-approximation. Our technique is robust enough to handle generalizations of these problems to incorporate various side-constraints such as blocking plays and switching costs. This technique is also of independent interest for other restless bandit problems. By presenting the first (and efficient) O(1) approximations for non-trivial instances of restless bandits as well as of POMDPs, our work initiates the study of approximation algorithms in both these contexts.",
        "published": "2007-11-25T18:01:35Z",
        "link": "http://arxiv.org/abs/0711.3861v5",
        "categories": [
            "cs.DS",
            "F.2.2; G.3"
        ]
    },
    {
        "title": "An FPT Algorithm for Directed Spanning k-Leaf",
        "authors": [
            "Paul Bonsma",
            "Frederic Dorn"
        ],
        "summary": "An out-branching of a directed graph is a rooted spanning tree with all arcs directed outwards from the root. We consider the problem of deciding whether a given directed graph D has an out-branching with at least k leaves (Directed Spanning k-Leaf). We prove that this problem is fixed parameter tractable, when k is chosen as the parameter. Previously this was only known for restricted classes of directed graphs.   The main new ingredient in our approach is a lemma that shows that given a locally optimal out-branching of a directed graph in which every arc is part of at least one out-branching, either an out-branching with at least k leaves exists, or a path decomposition with width O(k^3) can be found. This enables a dynamic programming based algorithm of running time 2^{O(k^3 \\log k)} n^{O(1)}, where n=|V(D)|.",
        "published": "2007-11-26T17:05:38Z",
        "link": "http://arxiv.org/abs/0711.4052v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "A Note On Computing Set Overlap Classes",
        "authors": [
            "Pierre Charbit",
            "Michel Habib",
            "Vincent Limouzy",
            "Fabien De Montgolfier",
            "Mathieu Raffinot",
            "Michaël Rao"
        ],
        "summary": "Let ${\\cal V}$ be a finite set of $n$ elements and ${\\cal F}=\\{X_1,X_2, >..., X_m\\}$ a family of $m$ subsets of ${\\cal V}.$ Two sets $X_i$ and $X_j$ of ${\\cal F}$ overlap if $X_i \\cap X_j \\neq \\emptyset,$ $X_j \\setminus X_i \\neq \\emptyset,$ and $X_i \\setminus X_j \\neq \\emptyset.$ Two sets $X,Y\\in {\\cal F}$ are in the same overlap class if there is a series $X=X_1,X_2, ..., X_k=Y$ of sets of ${\\cal F}$ in which each $X_iX_{i+1}$ overlaps. In this note, we focus on efficiently identifying all overlap classes in $O(n+\\sum_{i=1}^m |X_i|)$ time. We thus revisit the clever algorithm of Dahlhaus of which we give a clear presentation and that we simplify to make it practical and implementable in its real worst case complexity. An useful variant of Dahlhaus's approach is also explained.",
        "published": "2007-11-28T20:07:46Z",
        "link": "http://arxiv.org/abs/0711.4573v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Approximation Algorithms for Orienteering with Time Windows",
        "authors": [
            "Chandra Chekuri",
            "Nitish Korula"
        ],
        "summary": "Orienteering is the following optimization problem: given an edge-weighted graph (directed or undirected), two nodes s,t and a time limit T, find an s-t walk of total length at most T that maximizes the number of distinct nodes visited by the walk. One obtains a generalization, namely orienteering with time-windows (also referred to as TSP with time-windows), if each node v has a specified time-window [R(v), D(v)] and a node v is counted as visited by the walk only if v is visited during its time-window. For the time-window problem, an O(\\log \\opt) approximation can be achieved even for directed graphs if the algorithm is allowed quasi-polynomial time. However, the best known polynomial time approximation ratios are O(\\log^2 \\opt) for undirected graphs and O(\\log^4 \\opt) in directed graphs. In this paper we make some progress towards closing this discrepancy, and in the process obtain improved approximation ratios in several natural settings. Let L(v) = D(v) - R(v) denote the length of the time-window for v and let \\lmax = \\max_v L(v) and \\lmin = \\min_v L(v). Our results are given below with \\alpha denoting the known approximation ratio for orienteering (without time-windows). Currently \\alpha = (2+\\eps) for undirected graphs and \\alpha = O(\\log^2 \\opt) in directed graphs.   1. An O(\\alpha \\log \\lmax) approximation when R(v) and D(v) are integer valued for each v.   2. An O(\\alpha \\max{\\log \\opt, \\log \\frac{\\lmax}{\\lmin}}) approximation.   3. An O(\\alpha \\log \\frac{\\lmax}{\\lmin}) approximation when no start and end points are specified.   In particular, if \\frac{\\lmax}{\\lmin} is poly-bounded, we obtain an O(\\log n) approximation for the time-window problem in undirected graphs.",
        "published": "2007-11-29T21:10:48Z",
        "link": "http://arxiv.org/abs/0711.4825v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Circumspect descent prevails in solving random constraint satisfaction   problems",
        "authors": [
            "Mikko Alava",
            "John Ardelius",
            "Erik Aurell",
            "Petteri Kaski",
            "Supriya Krishnamurthy",
            "Pekka Orponen",
            "Sakari Seitz"
        ],
        "summary": "We study the performance of stochastic local search algorithms for random instances of the $K$-satisfiability ($K$-SAT) problem. We introduce a new stochastic local search algorithm, ChainSAT, which moves in the energy landscape of a problem instance by {\\em never going upwards} in energy. ChainSAT is a \\emph{focused} algorithm in the sense that it considers only variables occurring in unsatisfied clauses. We show by extensive numerical investigations that ChainSAT and other focused algorithms solve large $K$-SAT instances almost surely in linear time, up to high clause-to-variable ratios $\\alpha$; for example, for K=4 we observe linear-time performance well beyond the recently postulated clustering and condensation transitions in the solution space. The performance of ChainSAT is a surprise given that by design the algorithm gets trapped into the first local energy minimum it encounters, yet no such minima are encountered. We also study the geometry of the solution space as accessed by stochastic local search algorithms.",
        "published": "2007-11-30T11:01:40Z",
        "link": "http://arxiv.org/abs/0711.4902v1",
        "categories": [
            "cs.DS",
            "cond-mat.stat-mech",
            "cs.AI"
        ]
    },
    {
        "title": "Finding the growth rate of a regular language in polynomial time",
        "authors": [
            "Dalia Krieger",
            "Narad Rampersad",
            "Jeffrey Shallit"
        ],
        "summary": "We give an O(n^3+n^2 t) time algorithm to determine whether an NFA with n states and t transitions accepts a language of polynomial or exponential growth. We also show that given a DFA accepting a language of polynomial growth, we can determine the order of polynomial growth in quadratic time.",
        "published": "2007-11-30T17:48:00Z",
        "link": "http://arxiv.org/abs/0711.4990v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "F.4.3"
        ]
    },
    {
        "title": "On Using Unsatisfiability for Solving Maximum Satisfiability",
        "authors": [
            "Joao Marques-Silva",
            "Jordi Planes"
        ],
        "summary": "Maximum Satisfiability (MaxSAT) is a well-known optimization pro- blem, with several practical applications. The most widely known MAXS AT algorithms are ineffective at solving hard problems instances from practical application domains. Recent work proposed using efficient Boolean Satisfiability (SAT) solvers for solving the MaxSAT problem, based on identifying and eliminating unsatisfiable subformulas. However, these algorithms do not scale in practice. This paper analyzes existing MaxSAT algorithms based on unsatisfiable subformula identification. Moreover, the paper proposes a number of key optimizations to these MaxSAT algorithms and a new alternative algorithm. The proposed optimizations and the new algorithm provide significant performance improvements on MaxSAT instances from practical applications. Moreover, the efficiency of the new generation of unsatisfiability-based MaxSAT solvers becomes effectively indexed to the ability of modern SAT solvers to proving unsatisfiability and identifying unsatisfiable subformulas.",
        "published": "2007-12-07T09:21:58Z",
        "link": "http://arxiv.org/abs/0712.1097v1",
        "categories": [
            "cs.AI",
            "cs.DS"
        ]
    },
    {
        "title": "Efficient modularity optimization by multistep greedy algorithm and   vertex mover refinement",
        "authors": [
            "Philipp Schuetz",
            "Amedeo Caflisch"
        ],
        "summary": "Identifying strongly connected substructures in large networks provides insight into their coarse-grained organization. Several approaches based on the optimization of a quality function, e.g., the modularity, have been proposed. We present here a multistep extension of the greedy algorithm (MSG) that allows the merging of more than one pair of communities at each iteration step. The essential idea is to prevent the premature condensation into few large communities. Upon convergence of the MSG a simple refinement procedure called \"vertex mover\" (VM) is used for reassigning vertices to neighboring communities to improve the final modularity value. With an appropriate choice of the step width, the combined MSG-VM algorithm is able to find solutions of higher modularity than those reported previously. The multistep extension does not alter the scaling of computational cost of the greedy algorithm.",
        "published": "2007-12-07T15:48:31Z",
        "link": "http://arxiv.org/abs/0712.1163v2",
        "categories": [
            "cs.DS",
            "cond-mat.dis-nn",
            "cs.DM",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Delaunay Edge Flips in Dense Surface Triangulations",
        "authors": [
            "Siu-Wing Cheng",
            "Tamal K. Dey"
        ],
        "summary": "Delaunay flip is an elegant, simple tool to convert a triangulation of a point set to its Delaunay triangulation. The technique has been researched extensively for full dimensional triangulations of point sets. However, an important case of triangulations which are not full dimensional is surface triangulations in three dimensions. In this paper we address the question of converting a surface triangulation to a subcomplex of the Delaunay triangulation with edge flips. We show that the surface triangulations which closely approximate a smooth surface with uniform density can be transformed to a Delaunay triangulation with a simple edge flip algorithm. The condition on uniformity becomes less stringent with increasing density of the triangulation. If the condition is dropped completely, the flip algorithm still terminates although the output surface triangulation becomes \"almost Delaunay\" instead of exactly Delaunay.",
        "published": "2007-12-12T15:45:53Z",
        "link": "http://arxiv.org/abs/0712.1959v1",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Approximation Algorithms for the Highway Problem under the Coupon Model",
        "authors": [
            "Ryoso Hamane",
            "Toshiya Itoh",
            "Kouhei Tomita"
        ],
        "summary": "When a store sells items to customers, the store wishes to determine the prices of the items to maximize its profit. Intuitively, if the store sells the items with low (resp. high) prices, the customers buy more (resp. less) items, which provides less profit to the store. So it would be hard for the store to decide the prices of items. Assume that the store has a set V of n items and there is a set E of m customers who wish to buy those items, and also assume that each item i \\in V has the production cost d_i and each customer e_j \\in E has the valuation v_j on the bundle e_j \\subseteq V of items. When the store sells an item i \\in V at the price r_i, the profit for the item i is p_i=r_i-d_i. The goal of the store is to decide the price of each item to maximize its total profit. In most of the previous works, the item pricing problem was considered under the assumption that p_i \\geq 0 for each i \\in V, however, Balcan, et al. [In Proc. of WINE, LNCS 4858, 2007] introduced the notion of loss-leader, and showed that the seller can get more total profit in the case that p_i < 0 is allowed than in the case that p_i < 0 is not allowed. In this paper, we consider the line and the cycle highway problem, and show approximation algorithms for the line and/or cycle highway problem for which the smallest valuation is s and the largest valuation is \\ell or all valuations are identical.",
        "published": "2007-12-17T04:47:38Z",
        "link": "http://arxiv.org/abs/0712.2629v2",
        "categories": [
            "cs.DS",
            "F.2.2; G.1.2; G.2.1"
        ]
    },
    {
        "title": "Algorithms for Generating Convex Sets in Acyclic Digraphs",
        "authors": [
            "P. Balister",
            "S. Gerke",
            "G. Gutin",
            "A. Johnstone",
            "J. Reddington",
            "E. Scott",
            "A. Soleimanfallah",
            "A. Yeo"
        ],
        "summary": "A set $X$ of vertices of an acyclic digraph $D$ is convex if $X\\neq \\emptyset$ and there is no directed path between vertices of $X$ which contains a vertex not in $X$. A set $X$ is connected if $X\\neq \\emptyset$ and the underlying undirected graph of the subgraph of $D$ induced by $X$ is connected. Connected convex sets and convex sets of acyclic digraphs are of interest in the area of modern embedded processor technology. We construct an algorithm $\\cal A$ for enumeration of all connected convex sets of an acyclic digraph $D$ of order $n$. The time complexity of $\\cal A$ is $O(n\\cdot cc(D))$, where $cc(D)$ is the number of connected convex sets in $D$. We also give an optimal algorithm for enumeration of all (not just connected) convex sets of an acyclic digraph $D$ of order $n$. In computational experiments we demonstrate that our algorithms outperform the best algorithms in the literature.   Using the same approach as for $\\cal A$, we design an algorithm for generating all connected sets of a connected undirected graph $G$. The complexity of the algorithm is $O(n\\cdot c(G)),$ where $n$ is the order of $G$ and $c(G)$ is the number of connected sets of $G.$ The previously reported algorithm for connected set enumeration is of running time $O(mn\\cdot c(G))$, where $m$ is the number of edges in $G.$",
        "published": "2007-12-17T09:18:57Z",
        "link": "http://arxiv.org/abs/0712.2661v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "An Approximation Ratio for Biclustering",
        "authors": [
            "Kai Puolamäki",
            "Sami Hanhijärvi",
            "Gemma C. Garriga"
        ],
        "summary": "The problem of biclustering consists of the simultaneous clustering of rows and columns of a matrix such that each of the submatrices induced by a pair of row and column clusters is as uniform as possible. In this paper we approximate the optimal biclustering by applying one-way clustering algorithms independently on the rows and on the columns of the input matrix. We show that such a solution yields a worst-case approximation ratio of 1+sqrt(2) under L1-norm for 0-1 valued matrices, and of 2 under L2-norm for real valued matrices.",
        "published": "2007-12-17T11:45:42Z",
        "link": "http://arxiv.org/abs/0712.2682v2",
        "categories": [
            "cs.DS",
            "stat.ML"
        ]
    },
    {
        "title": "Solving Medium-Density Subset Sum Problems in Expected Polynomial Time:   An Enumeration Approach",
        "authors": [
            "Changlin Wan",
            "Zhongzhi Shi"
        ],
        "summary": "The subset sum problem (SSP) can be briefly stated as: given a target integer $E$ and a set $A$ containing $n$ positive integer $a_j$, find a subset of $A$ summing to $E$. The \\textit{density} $d$ of an SSP instance is defined by the ratio of $n$ to $m$, where $m$ is the logarithm of the largest integer within $A$. Based on the structural and statistical properties of subset sums, we present an improved enumeration scheme for SSP, and implement it as a complete and exact algorithm (EnumPlus). The algorithm always equivalently reduces an instance to be low-density, and then solve it by enumeration. Through this approach, we show the possibility to design a sole algorithm that can efficiently solve arbitrary density instance in a uniform way. Furthermore, our algorithm has considerable performance advantage over previous algorithms. Firstly, it extends the density scope, in which SSP can be solved in expected polynomial time. Specifically, It solves SSP in expected $O(n\\log{n})$ time when density $d \\geq c\\cdot \\sqrt{n}/\\log{n}$, while the previously best density scope is $d \\geq c\\cdot n/(\\log{n})^{2}$. In addition, the overall expected time and space requirement in the average case are proven to be $O(n^5\\log n)$ and $O(n^5)$ respectively. Secondly, in the worst case, it slightly improves the previously best time complexity of exact algorithms for SSP. Specifically, the worst-case time complexity of our algorithm is proved to be $O((n-6)2^{n/2}+n)$, while the previously best result is $O(n2^{n/2})$.",
        "published": "2007-12-19T14:43:50Z",
        "link": "http://arxiv.org/abs/0712.3203v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.CR",
            "F.2.2; G.2.1; G.1.6; I.2.8; E.3"
        ]
    },
    {
        "title": "On the approximability of the vertex cover and related problems",
        "authors": [
            "Qiaoming Han",
            "Abraham P. Punnen"
        ],
        "summary": "In this paper we show that the problem of identifying an edge $(i,j)$ in a graph $G$ such that there exists an optimal vertex cover $S$ of $G$ containing exactly one of the nodes $i$ and $j$ is NP-hard. Such an edge is called a weak edge. We then develop a polynomial time approximation algorithm for the vertex cover problem with performance guarantee $2-\\frac{1}{1+\\sigma}$, where $\\sigma$ is an upper bound on a measure related to a weak edge of a graph. Further, we discuss a new relaxation of the vertex cover problem which is used in our approximation algorithm to obtain smaller values of $\\sigma$. We also obtain linear programming representations of the vertex cover problem for special graphs. Our results provide new insights into the approximability of the vertex cover problem - a long standing open problem.",
        "published": "2007-12-20T06:35:05Z",
        "link": "http://arxiv.org/abs/0712.3333v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2; G.2.2; G.1.6"
        ]
    },
    {
        "title": "A polynomial time $\\frac 3 2$ -approximation algorithm for the vertex   cover problem on a class of graphs",
        "authors": [
            "Qiaoming Han",
            "Abraham P. Punnen",
            "Yinyu Ye"
        ],
        "summary": "We develop a polynomial time 3/2-approximation algorithm to solve the vertex cover problem on a class of graphs satisfying a property called ``active edge hypothesis''. The algorithm also guarantees an optimal solution on specially structured graphs. Further, we give an extended algorithm which guarantees a vertex cover $S_1$ on an arbitrary graph such that $|S_1|\\leq {3/2} |S^*|+\\xi$ where $S^*$ is an optimal vertex cover and $\\xi$ is an error bound identified by the algorithm. We obtained $\\xi = 0$ for all the test problems we have considered which include specially constructed instances that were expected to be hard. So far we could not construct a graph that gives $\\xi \\not= 0$.",
        "published": "2007-12-20T06:53:30Z",
        "link": "http://arxiv.org/abs/0712.3335v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2; G.2.2; G.1.6"
        ]
    },
    {
        "title": "Compressed Text Indexes:From Theory to Practice!",
        "authors": [
            "Paolo Ferragina",
            "Rodrigo Gonzalez",
            "Gonzalo Navarro",
            "Rossano Venturini"
        ],
        "summary": "A compressed full-text self-index represents a text in a compressed form and still answers queries efficiently. This technology represents a breakthrough over the text indexing techniques of the previous decade, whose indexes required several times the size of the text. Although it is relatively new, this technology has matured up to a point where theoretical research is giving way to practical developments. Nonetheless this requires significant programming skills, a deep engineering effort, and a strong algorithmic background to dig into the research results. To date only isolated implementations and focused comparisons of compressed indexes have been reported, and they missed a common API, which prevented their re-use or deployment within other applications.   The goal of this paper is to fill this gap. First, we present the existing implementations of compressed indexes from a practitioner's point of view. Second, we introduce the Pizza&Chili site, which offers tuned implementations and a standardized API for the most successful compressed full-text self-indexes, together with effective testbeds and scripts for their automatic validation and test. Third, we show the results of our extensive experiments on these codes with the aim of demonstrating the practical relevance of this novel and exciting technology.",
        "published": "2007-12-20T10:42:54Z",
        "link": "http://arxiv.org/abs/0712.3360v1",
        "categories": [
            "cs.DS",
            "F.2.2; H.2.1; H.3.2; H.3.3"
        ]
    },
    {
        "title": "A Partition-Based Relaxation For Steiner Trees",
        "authors": [
            "Jochen Konemann",
            "David Pritchard",
            "Kunlun Tan"
        ],
        "summary": "The Steiner tree problem is a classical NP-hard optimization problem with a wide range of practical applications. In an instance of this problem, we are given an undirected graph G=(V,E), a set of terminals R, and non-negative costs c_e for all edges e in E. Any tree that contains all terminals is called a Steiner tree; the goal is to find a minimum-cost Steiner tree. The nodes V R are called Steiner nodes.   The best approximation algorithm known for the Steiner tree problem is due to Robins and Zelikovsky (SIAM J. Discrete Math, 2005); their greedy algorithm achieves a performance guarantee of 1+(ln 3)/2 ~ 1.55. The best known linear (LP)-based algorithm, on the other hand, is due to Goemans and Bertsimas (Math. Programming, 1993) and achieves an approximation ratio of 2-2/|R|. In this paper we establish a link between greedy and LP-based approaches by showing that Robins and Zelikovsky's algorithm has a natural primal-dual interpretation with respect to a novel partition-based linear programming relaxation. We also exhibit surprising connections between the new formulation and existing LPs and we show that the new LP is stronger than the bidirected cut formulation.   An instance is b-quasi-bipartite if each connected component of G R has at most b vertices. We show that Robins' and Zelikovsky's algorithm has an approximation ratio better than 1+(ln 3)/2 for such instances, and we prove that the integrality gap of our LP is between 8/7 and (2b+1)/(b+1).",
        "published": "2007-12-20T21:06:35Z",
        "link": "http://arxiv.org/abs/0712.3568v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Quantum Property Testing of Group Solvability",
        "authors": [
            "Yoshifumi Inui",
            "Francois Le Gall"
        ],
        "summary": "Testing efficiently whether a finite set with a binary operation over it, given as an oracle, is a group is a well-known open problem in the field of property testing. Recently, Friedl, Ivanyos and Santha have made a significant step in the direction of solving this problem by showing that it it possible to test efficiently whether the input is an Abelian group or is far, with respect to some distance, from any Abelian group. In this paper, we make a step further and construct an efficient quantum algorithm that tests whether the input is a solvable group, or is far from any solvable group. More precisely, the number of queries used by our algorithm is polylogarithmic in the size of the set.",
        "published": "2007-12-22T04:47:03Z",
        "link": "http://arxiv.org/abs/0712.3829v2",
        "categories": [
            "quant-ph",
            "cs.DS"
        ]
    },
    {
        "title": "Bottleneck flows in networks",
        "authors": [
            "Abraham P. Punnen",
            "Ruonan Zhang"
        ],
        "summary": "The bottleneck network flow problem (BNFP) is a generalization of several well-studied bottleneck problems such as the bottleneck transportation problem (BTP), bottleneck assignment problem (BAP), bottleneck path problem (BPP), and so on. In this paper we provide a review of important results on this topic and its various special cases. We observe that the BNFP can be solved as a sequence of $O(\\log n)$ maximum flow problems. However, special augmenting path based algorithms for the maximum flow problem can be modified to obtain algorithms for the BNFP with the property that these variations and the corresponding maximum flow algorithms have identical worst case time complexity. On unit capacity network we show that BNFP can be solved in $O(\\min \\{{m(n\\log n)}^{{2/3}}, m^{{3/2}}\\sqrt{\\log n}\\})$. This improves the best available algorithm by a factor of $\\sqrt{\\log n}$. On unit capacity simple graphs, we show that BNFP can be solved in $O(m \\sqrt {n \\log n})$ time. As a consequence we have an $O(m \\sqrt {n \\log n})$ algorithm for the BTP with unit arc capacities.",
        "published": "2007-12-22T13:49:45Z",
        "link": "http://arxiv.org/abs/0712.3858v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Explicit Non-Adaptive Combinatorial Group Testing Schemes",
        "authors": [
            "Ely Porat",
            "Amir Rothschild"
        ],
        "summary": "Group testing is a long studied problem in combinatorics: A small set of $r$ ill people should be identified out of the whole ($n$ people) by using only queries (tests) of the form \"Does set X contain an ill human?\". In this paper we provide an explicit construction of a testing scheme which is better (smaller) than any known explicit construction. This scheme has $\\bigT{\\min[r^2 \\ln n,n]}$ tests which is as many as the best non-explicit schemes have. In our construction we use a fact that may have a value by its own right: Linear error-correction codes with parameters $[m,k,\\delta m]_q$ meeting the Gilbert-Varshamov bound may be constructed quite efficiently, in $\\bigT{q^km}$ time.",
        "published": "2007-12-22T21:04:34Z",
        "link": "http://arxiv.org/abs/0712.3876v5",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Lagrangian Relaxation and Partial Cover",
        "authors": [
            "Julián Mestre"
        ],
        "summary": "Lagrangian relaxation has been used extensively in the design of approximation algorithms. This paper studies its strengths and limitations when applied to Partial Cover.",
        "published": "2007-12-23T18:33:36Z",
        "link": "http://arxiv.org/abs/0712.3936v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "G.2.1"
        ]
    },
    {
        "title": "Accurate and Efficient Expression Evaluation and Linear Algebra",
        "authors": [
            "James Demmel",
            "Ioana Dumitriu",
            "Olga Holtz",
            "Plamen Koev"
        ],
        "summary": "We survey and unify recent results on the existence of accurate algorithms for evaluating multivariate polynomials, and more generally for accurate numerical linear algebra with structured matrices. By \"accurate\" we mean that the computed answer has relative error less than 1, i.e., has some correct leading digits. We also address efficiency, by which we mean algorithms that run in polynomial time in the size of the input. Our results will depend strongly on the model of arithmetic: Most of our results will use the so-called Traditional Model (TM). We give a set of necessary and sufficient conditions to decide whether a high accuracy algorithm exists in the TM, and describe progress toward a decision procedure that will take any problem and provide either a high accuracy algorithm or a proof that none exists. When no accurate algorithm exists in the TM, it is natural to extend the set of available accurate operations by a library of additional operations, such as $x+y+z$, dot products, or indeed any enumerable set which could then be used to build further accurate algorithms. We show how our accurate algorithms and decision procedure for finding them extend to this case. Finally, we address other models of arithmetic, and the relationship between (im)possibility in the TM and (in)efficient algorithms operating on numbers represented as bit strings.",
        "published": "2007-12-24T20:14:50Z",
        "link": "http://arxiv.org/abs/0712.4027v1",
        "categories": [
            "math.NA",
            "cs.CC",
            "cs.DS",
            "math.RA",
            "65Y20, 68Q05, 68Q25, 65F30, 68W40, 68W25"
        ]
    },
    {
        "title": "Faster polynomial multiplication via multipoint Kronecker substitution",
        "authors": [
            "David Harvey"
        ],
        "summary": "We give several new algorithms for dense polynomial multiplication based on the Kronecker substitution method. For moderately sized input polynomials, the new algorithms improve on the performance of the standard Kronecker substitution by a sizeable constant, both in theory and in empirical tests.",
        "published": "2007-12-25T04:57:04Z",
        "link": "http://arxiv.org/abs/0712.4046v1",
        "categories": [
            "cs.SC",
            "cs.DS"
        ]
    },
    {
        "title": "Exact Quantum Algorithms for the Leader Election Problem",
        "authors": [
            "Seiichiro Tani",
            "Hirotada Kobayashi",
            "Keiji Matsumoto"
        ],
        "summary": "This paper gives the first separation of quantum and classical pure (i.e., non-cryptographic) computing abilities with no restriction on the amount of available computing resources, by considering the exact solvability of a celebrated unsolvable problem in classical distributed computing, the ``leader election problem'' on anonymous networks. The goal of the leader election problem is to elect a unique leader from among distributed parties. The paper considers this problem for anonymous networks, in which each party has the same identifier. It is well-known that no classical algorithm can solve exactly (i.e., in bounded time without error) the leader election problem in anonymous networks, even if it is given the number of parties. This paper gives two quantum algorithms that, given the number of parties, can exactly solve the problem for any network topology in polynomial rounds and polynomial communication/time complexity with respect to the number of parties, when the parties are connected by quantum communication links.",
        "published": "2007-12-27T10:52:52Z",
        "link": "http://arxiv.org/abs/0712.4213v1",
        "categories": [
            "quant-ph",
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Reserved-Length Prefix Coding",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "Huffman coding finds an optimal prefix code for a given probability mass function. Consider situations in which one wishes to find an optimal code with the restriction that all codewords have lengths that lie in a user-specified set of lengths (or, equivalently, no codewords have lengths that lie in a complementary set). This paper introduces a polynomial-time dynamic programming algorithm that finds optimal codes for this reserved-length prefix coding problem. This has applications to quickly encoding and decoding lossless codes. In addition, one modification of the approach solves any quasiarithmetic prefix coding problem, while another finds optimal codes restricted to the set of codes with g codeword lengths for user-specified g (e.g., g=2).",
        "published": "2007-12-30T00:14:24Z",
        "link": "http://arxiv.org/abs/0801.0102v1",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "G.2.2; F.2; E.4; H.1.1"
        ]
    },
    {
        "title": "Finding Astronomical Communities Through Co-readership Analysis",
        "authors": [
            "Edwin A. Henneken",
            "Michael J. Kurtz",
            "Guenther Eichhorn",
            "Alberto Accomazzi",
            "Carolyn S. Grant",
            "Donna Thompson",
            "Elizabeth Bohlen",
            "Stephen S. Murray"
        ],
        "summary": "Whenever a large group of people are engaged in an activity, communities will form. The nature of these communities depends on the relationship considered. In the group of people who regularly use scholarly literature, a relationship like ``person i and person j have cited the same paper'' might reveal communities of people working in a particular field. On this poster, we will investigate the relationship ``person i and person j have read the same paper''. Using the data logs of the NASA/Smithsonian Astrophysics Data System (ADS), we first determine the population that will participate by requiring that a user queries the ADS at a certain rate. Next, we apply the relationship to this population. The result of this will be an abstract ``relationship space'', which we will describe in terms of various ``representations''. Examples of such ``representations'' are the projection of co-read vectors onto Principal Components and the spectral density of the co-read network. We will show that the co-read relationship results in structure, we will describe this structure and we will provide a first attempt in the classification of this structure in terms of astronomical communities.   The ADS is funded by NASA Grant NNG06GG68G.",
        "published": "2007-01-06T00:02:53Z",
        "link": "http://arxiv.org/abs/cs/0701035v1",
        "categories": [
            "cs.DL",
            "astro-ph"
        ]
    },
    {
        "title": "On the robustness of the h-index",
        "authors": [
            "Jerome K Vanclay"
        ],
        "summary": "The h-index (Hirsch, 2005) is robust, remaining relatively unaffected by errors in the long tails of the citations-rank distribution, such as typographic errors that short-change frequently-cited papers and create bogus additional records. This robustness, and the ease with which h-indices can be verified, support the use of a Hirsch-type index over alternatives such as the journal impact factor. These merits of the h-index apply to both individuals and to journals.",
        "published": "2007-01-11T00:48:45Z",
        "link": "http://arxiv.org/abs/cs/0701074v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Citation advantage of Open Access articles likely explained by quality   differential and media effects",
        "authors": [
            "Philip M. Davis"
        ],
        "summary": "In a study of articles published in the Proceedings of the National Academy of Sciences, Gunther Eysenbach discovered a significant citation advantage for those articles made freely-available upon publication (Eysenbach 2006). While the author attempted to control for confounding factors that may have explained the citation differential, the study was unable to control for characteristics of the article that may have led some authors to pay the additional page charges ($1,000) for immediate OA status. OA articles published in PNAS were more than twice as likely to be featured on the front cover of the journal (3.3% vs. 1.4%), nearly twice as likely to be picked up by the media (15% vs. 8%) and when cited reached, on average, nearly twice as many news outlets as subscription-based articles (4.2 vs. 2.6). The citation advantage of Open Access articles in PNAS may likely be explained by a quality differential and the amplification of media effects.",
        "published": "2007-01-16T20:00:35Z",
        "link": "http://arxiv.org/abs/cs/0701101v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Why is a new Journal of Informetrics needed?",
        "authors": [
            "Philipp Mayr",
            "Walther Umstaetter"
        ],
        "summary": "In our study we analysed 3.889 records which were indexed in the Library and Information Science Abstracts (LISA) database in the research field of informetrics. We can show the core journals of the field via a Bradford (power law) distribution and corroborate on the basis of the restricted LISA data set that it was the appropriate time to found a new specialized journal dedicated to informetrics. According to Bradford's Law of scattering (pure quantitative calculation), Egghe's Journal of Informetrics (JOI) first issue to appear in 2007, comes most probable at the right time.",
        "published": "2007-01-17T12:17:53Z",
        "link": "http://arxiv.org/abs/cs/0701104v1",
        "categories": [
            "cs.DL",
            "cs.DB"
        ]
    },
    {
        "title": "Citation Advantage For OA Self-Archiving Is Independent of Journal   Impact Factor, Article Age, and Number of Co-Authors",
        "authors": [
            "Chawki Hajjem",
            "Stevan Harnad"
        ],
        "summary": "Eysenbach has suggested that the OA (Green) self-archiving advantage might just be an artifact of potential uncontrolled confounding factors such as article age (older articles may be both more cited and more likely to be self-archived), number of authors (articles with more authors might be more cited and more self-archived), subject matter (the subjects that are cited more, self-archive more), country (same thing), number of authors, citation counts of authors, etc. Chawki Hajjem (doctoral candidate, UQaM) had already shown that the OA advantage was present in all cases when articles were analysed separately by age, subject matter or country. He has now done a multiple regression analysis jointly testing (1) article age, (2) journal impact factor, (3) number of authors, and (4) OA self-archiving as separate factors for 442,750 articles in 576 (biomedical) journals across 11 years, and has shown that each of the four factors contributes an independent, statistically significant increment to the citation counts. The OA-self-archiving advantage remains a robust, independent factor. Having successfully responded to his challenge, we now challenge Eysenbach to demonstrate -- by testing a sufficiently broad and representative sample of journals at all levels of the journal quality, visibility and prestige hierarchy -- that his finding of a citation advantage for Gold OA (articles published OA on the high-profile website of the only journal he tested (PNAS) over Green OA articles in the same journal (self-archived on the author's website) was not just an artifact of having tested only one very high-profile journal.",
        "published": "2007-01-22T02:14:10Z",
        "link": "http://arxiv.org/abs/cs/0701136v1",
        "categories": [
            "cs.IR",
            "cs.DL"
        ]
    },
    {
        "title": "The Open Access Citation Advantage: Quality Advantage Or Quality Bias?",
        "authors": [
            "Chawki Hajjem",
            "Stevan Harnad"
        ],
        "summary": "Many studies have now reported the positive correlation between Open Access (OA) self-archiving and citation counts (\"OA Advantage,\" OAA). But does this OAA occur because (QB) authors are more likely to self-selectively self-archive articles that are more likely to be cited (self-selection \"Quality Bias\": QB)? or because (QA) articles that are self-archived are more likely to be cited (\"Quality Advantage\": QA)? The probable answer is both. Three studies [by (i) Kurtz and co-workers in astrophysics, (ii) Moed in condensed matter physics, and (iii) Davis & Fromerth in mathematics] had reported the OAA to be due to QB [plus Early Advantage, EA, from self-archiving the preprint before publication, in (i) and (ii)] rather than QA. These three fields, however, (1) have less of a postprint access problem than most other fields and (i) and (ii) also happen to be among the minority of fields that (2) make heavy use of prepublication preprints. Chawki Hajjem has now analyzed preliminary evidence based on over 100,000 articles from multiple fields, comparing self-selected self-archiving with mandated self-archiving to estimate the contributions of QB and QA to the OAA. Both factors contribute, and the contribution of QA is greater.",
        "published": "2007-01-22T02:19:16Z",
        "link": "http://arxiv.org/abs/cs/0701137v1",
        "categories": [
            "cs.IR",
            "cs.DL"
        ]
    },
    {
        "title": "On the Software and Knowledge Engineering Aspects of the Educational   Process",
        "authors": [
            "Th. Hadzilacos",
            "D. Kalles",
            "M. Pouliopoulou"
        ],
        "summary": "The Hellenic Open University has embarked on a large-scale effort to enhance its textbook-based material with content that demonstrably supports the basic tenets of distance learning. The challenge is to set up a framework that allows for the production-level creation, distribution and consumption of content, and at the same time, evaluate the effort in terms of technological, educational and organizational knowledge gained. This paper presents a model of the educational process that is used as a development backbone and argues about its conceptual and technical practicality at large.",
        "published": "2007-01-26T14:51:38Z",
        "link": "http://arxiv.org/abs/cs/0701175v1",
        "categories": [
            "cs.SE",
            "cs.DL"
        ]
    },
    {
        "title": "Plagiarism Detection in arXiv",
        "authors": [
            "Daria Sorokina",
            "Johannes Gehrke",
            "Simeon Warner",
            "Paul Ginsparg"
        ],
        "summary": "We describe a large-scale application of methods for finding plagiarism in research document collections. The methods are applied to a collection of 284,834 documents collected by arXiv.org over a 14 year period, covering a few different research disciplines. The methodology efficiently detects a variety of problematic author behaviors, and heuristics are developed to reduce the number of false positives. The methods are also efficient enough to implement as a real-time submission screen for a collection many times larger.",
        "published": "2007-02-01T20:52:13Z",
        "link": "http://arxiv.org/abs/cs/0702012v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Exploring the academic invisible web",
        "authors": [
            "Dirk Lewandowski",
            "Philipp Mayr"
        ],
        "summary": "Purpose: To provide a critical review of Bergman's 2001 study on the Deep Web. In addition, we bring a new concept into the discussion, the Academic Invisible Web (AIW). We define the Academic Invisible Web as consisting of all databases and collections relevant to academia but not searchable by the general-purpose internet search engines. Indexing this part of the Invisible Web is central to scientific search engines. We provide an overview of approaches followed thus far. Design/methodology/approach: Discussion of measures and calculations, estimation based on informetric laws. Literature review on approaches for uncovering information from the Invisible Web. Findings: Bergman's size estimate of the Invisible Web is highly questionable. We demonstrate some major errors in the conceptual design of the Bergman paper. A new (raw) size estimate is given. Research limitations/implications: The precision of our estimate is limited due to a small sample size and lack of reliable data. Practical implications: We can show that no single library alone will be able to index the Academic Invisible Web. We suggest collaboration to accomplish this task. Originality/value: Provides library managers and those interested in developing academic search engines with data on the size and attributes of the Academic Invisible Web.",
        "published": "2007-02-18T20:56:34Z",
        "link": "http://arxiv.org/abs/cs/0702103v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Assessing the Value of Coooperation in Wikipedia",
        "authors": [
            "Dennis M. Wilkinson",
            "Bernardo A. Huberman"
        ],
        "summary": "Since its inception six years ago, the online encyclopedia Wikipedia has accumulated 6.40 million articles and 250 million edits, contributed in a predominantly undirected and haphazard fashion by 5.77 million unvetted volunteers. Despite the apparent lack of order, the 50 million edits by 4.8 million contributors to the 1.5 million articles in the English-language Wikipedia follow strong certain overall regularities. We show that the accretion of edits to an article is described by a simple stochastic mechanism, resulting in a heavy tail of highly visible articles with a large number of edits. We also demonstrate a crucial correlation between article quality and number of edits, which validates Wikipedia as a successful collaborative effort.",
        "published": "2007-02-23T17:59:06Z",
        "link": "http://arxiv.org/abs/cs/0702140v1",
        "categories": [
            "cs.DL",
            "cs.CY",
            "physics.soc-ph"
        ]
    },
    {
        "title": "A Comparison of On-Line Computer Science Citation Databases",
        "authors": [
            "Vaclav Petricek",
            "Ingemar J. Cox",
            "Hui Han",
            "Isaac G. Councill",
            "C. Lee Giles"
        ],
        "summary": "This paper examines the difference and similarities between the two on-line computer science citation databases DBLP and CiteSeer. The database entries in DBLP are inserted manually while the CiteSeer entries are obtained autonomously via a crawl of the Web and automatic processing of user submissions. CiteSeer's autonomous citation database can be considered a form of self-selected on-line survey. It is important to understand the limitations of such databases, particularly when citation information is used to assess the performance of authors, institutions and funding bodies.   We show that the CiteSeer database contains considerably fewer single author papers. This bias can be modeled by an exponential process with intuitive explanation. The model permits us to predict that the DBLP database covers approximately 24% of the entire literature of Computer Science. CiteSeer is also biased against low-cited papers.   Despite their difference, both databases exhibit similar and significantly different citation distributions compared with previous analysis of the Physics community. In both databases, we also observe that the number of authors per paper has been increasing over time.",
        "published": "2007-03-09T16:00:55Z",
        "link": "http://arxiv.org/abs/cs/0703043v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Characterization of Search Engine Caches",
        "authors": [
            "Frank McCown",
            "Michael L. Nelson"
        ],
        "summary": "Search engines provide cached copies of indexed content so users will have something to \"click on\" if the remote resource is temporarily or permanently unavailable. Depending on their proprietary caching strategies, search engines will purge their indexes and caches of resources that exceed a threshold of unavailability. Although search engine caches are provided only as an aid to the interactive user, we are interested in building reliable preservation services from the aggregate of these limited caching services. But first, we must understand the contents of search engine caches. In this paper, we have examined the cached contents of Ask, Google, MSN and Yahoo to profile such things as overlap between index and cache, size, MIME type and \"staleness\" of the cached resources. We also examined the overlap of the various caches with the holdings of the Internet Archive.",
        "published": "2007-03-15T15:16:08Z",
        "link": "http://arxiv.org/abs/cs/0703083v2",
        "categories": [
            "cs.DL",
            "cs.CY"
        ]
    },
    {
        "title": "We cite as we communicate: A communication model for the citation   process",
        "authors": [
            "Victor V. Kryssanov",
            "Evgeny L. Kuleshov",
            "Frank J. Rinaldo",
            "Hitoshi Ogawa"
        ],
        "summary": "Building on ideas from linguistics, psychology, and social sciences about the possible mechanisms of human decision-making, we propose a novel theoretical framework for the citation analysis. Given the existing trend to investigate citation statistics in the context of various forms of power and Zipfian laws, we show that the popular models of citation have poor predictive ability and can hardly provide for an adequate explanation of the observed behavior of the empirical data. An alternative model is then derived, using the apparatus of statistical mechanics. The model is applied to approximate the citation frequencies of scientific articles from two large collections, and it demonstrates a predictive potential much superior to the one of any of the citation models known to the authors from the literature. Some analytical properties of the developed model are discussed, and conclusions are drawn. Directions for future work are also given at the paper's end.",
        "published": "2007-03-23T05:30:45Z",
        "link": "http://arxiv.org/abs/cs/0703115v2",
        "categories": [
            "cs.DL",
            "cs.CY",
            "physics.data-an"
        ]
    },
    {
        "title": "Open Access Scientometrics and the UK Research Assessment Exercise",
        "authors": [
            "Stevan Harnad"
        ],
        "summary": "Scientometric predictors of research performance need to be validated by showing that they have a high correlation with the external criterion they are trying to predict. The UK Research Assessment Exercise (RAE), together with the growing movement toward making the full-texts of research articles freely available on the web -- offer a unique opportunity to test and validate a wealth of old and new scientometric predictors, through multiple regression analysis: Publications, journal impact factors, citations, co-citations, citation chronometrics (age, growth, latency to peak, decay rate), hub/authority scores, h-index, prior funding, student counts, co-authorship scores, endogamy/exogamy, textual proximity, download/co-downloads and their chronometrics, etc. can all be tested and validated jointly, discipline by discipline, against their RAE panel rankings in the forthcoming parallel panel-based and metric RAE in 2008. The weights of each predictor can be calibrated to maximize the joint correlation with the rankings. Open Access Scientometrics will provide powerful new means of navigating, evaluating, predicting and analyzing the growing Open Access database, as well as powerful incentives for making it grow faster. ~",
        "published": "2007-03-26T15:40:08Z",
        "link": "http://arxiv.org/abs/cs/0703131v1",
        "categories": [
            "cs.IR",
            "cs.DL"
        ]
    },
    {
        "title": "Supporting Knowledge and Expertise Finding within Australia's Defence   Science and Technology Organisation",
        "authors": [
            "Paul Prekop"
        ],
        "summary": "This paper reports on work aimed at supporting knowledge and expertise finding within a large Research and Development (R&D) organisation. The paper first discusses the nature of knowledge important to R&D organisations and presents a prototype information system developed to support knowledge and expertise finding. The paper then discusses a trial of the system within an R&D organisation, the implications and limitations of the trial, and discusses future research questions.",
        "published": "2007-04-11T06:49:06Z",
        "link": "http://arxiv.org/abs/0704.1353v1",
        "categories": [
            "cs.OH",
            "cs.DB",
            "cs.DL",
            "cs.HC"
        ]
    },
    {
        "title": "Exploiting Social Annotation for Automatic Resource Discovery",
        "authors": [
            "Anon Plangprasopchok",
            "Kristina Lerman"
        ],
        "summary": "Information integration applications, such as mediators or mashups, that require access to information resources currently rely on users manually discovering and integrating them in the application. Manual resource discovery is a slow process, requiring the user to sift through results obtained via keyword-based search. Although search methods have advanced to include evidence from document contents, its metadata and the contents and link structure of the referring pages, they still do not adequately cover information sources -- often called ``the hidden Web''-- that dynamically generate documents in response to a query. The recently popular social bookmarking sites, which allow users to annotate and share metadata about various information sources, provide rich evidence for resource discovery. In this paper, we describe a probabilistic model of the user annotation process in a social bookmarking system del.icio.us. We then use the model to automatically find resources relevant to a particular information domain. Our experimental results on data obtained from \\emph{del.icio.us} show this approach as a promising method for helping automate the resource discovery task.",
        "published": "2007-04-12T23:24:19Z",
        "link": "http://arxiv.org/abs/0704.1675v1",
        "categories": [
            "cs.AI",
            "cs.CY",
            "cs.DL"
        ]
    },
    {
        "title": "Personalizing Image Search Results on Flickr",
        "authors": [
            "Kristina Lerman",
            "Anon Plangprasopchok",
            "Chio Wong"
        ],
        "summary": "The social media site Flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. Flickr offers multiple ways of browsing or searching it. One option is tag search, which returns all images tagged with a specific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. We claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. We show how to exploit this metadata to personalize search results for the user, thereby improving search performance. First, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. Secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. The users' interests can similarly be described by the tags they used for annotating their images. The latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.",
        "published": "2007-04-12T23:31:04Z",
        "link": "http://arxiv.org/abs/0704.1676v1",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CY",
            "cs.DL",
            "cs.HC"
        ]
    },
    {
        "title": "Using Access Data for Paper Recommendations on ArXiv.org",
        "authors": [
            "Stefan Pohl"
        ],
        "summary": "This thesis investigates in the use of access log data as a source of information for identifying related scientific papers. This is done for arXiv.org, the authority for publication of e-prints in several fields of physics.   Compared to citation information, access logs have the advantage of being immediately available, without manual or automatic extraction of the citation graph. Because of that, a main focus is on the question, how far user behavior can serve as a replacement for explicit meta-data, which potentially might be expensive or completely unavailable. Therefore, we compare access, content, and citation-based measures of relatedness on different recommendation tasks. As a final result, an online recommendation system has been built that can help scientists to find further relevant literature, without having to search for them actively.",
        "published": "2007-04-23T15:52:47Z",
        "link": "http://arxiv.org/abs/0704.2963v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Recommending Related Papers Based on Digital Library Access Records",
        "authors": [
            "Stefan Pohl",
            "Filip Radlinski",
            "Thorsten Joachims"
        ],
        "summary": "An important goal for digital libraries is to enable researchers to more easily explore related work. While citation data is often used as an indicator of relatedness, in this paper we demonstrate that digital access records (e.g. http-server logs) can be used as indicators as well. In particular, we show that measures based on co-access provide better coverage than co-citation, that they are available much sooner, and that they are more accurate for recent papers.",
        "published": "2007-04-23T16:51:40Z",
        "link": "http://arxiv.org/abs/0704.2902v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.7; H.3.3"
        ]
    },
    {
        "title": "Evaluating Personal Archiving Strategies for Internet-based Information",
        "authors": [
            "Catherine C. Marshall",
            "Frank McCown",
            "Michael L. Nelson"
        ],
        "summary": "Internet-based personal digital belongings present different vulnerabilities than locally stored materials. We use responses to a survey of people who have recovered lost websites, in combination with supplementary interviews, to paint a fuller picture of current curatorial strategies and practices. We examine the types of personal, topical, and commercial websites that respondents have lost and the reasons they have lost this potentially valuable material. We further explore what they have tried to recover and how the loss influences their subsequent practices. We found that curation of personal digital materials in online stores bears some striking similarities to the curation of similar materials stored locally in that study participants continue to archive personal assets by relying on a combination of benign neglect, sporadic backups, and unsystematic file replication. However, we have also identified issues specific to Internet-based material: how risk is spread by distributing the files among multiple servers and services; the circular reasoning participants use when they discuss the safety of their digital assets; and the types of online material that are particularly vulnerable to loss. The study reveals ways in which expectations of permanence and notification are violated and situations in which benign neglect has far greater consequences for the long-term fate of important digital assets.",
        "published": "2007-04-27T01:38:55Z",
        "link": "http://arxiv.org/abs/0704.3647v1",
        "categories": [
            "cs.DL",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "The Long Term Fate of Our Digital Belongings: Toward a Service Model for   Personal Archives",
        "authors": [
            "Catherine C. Marshall",
            "Sara Bly",
            "Francoise Brun-Cottan"
        ],
        "summary": "We conducted a preliminary field study to understand the current state of personal digital archiving in practice. Our aim is to design a service for the long-term storage, preservation, and access of digital belongings by examining how personal archiving needs intersect with existing and emerging archiving technologies, best practices, and policies. Our findings not only confirmed that experienced home computer users are creating, receiving, and finding an increasing number of digital belongings, but also that they have already lost irreplaceable digital artifacts such as photos, creative efforts, and records. Although participants reported strategies such as backup and file replication for digital safekeeping, they were seldom able to implement them consistently. Four central archiving themes emerged from the data: (1) people find it difficult to evaluate the worth of accumulated materials; (2) personal storage is highly distributed both on- and offline; (3) people are experiencing magnified curatorial problems associated with managing files in the aggregate, creating appropriate metadata, and migrating materials to maintainable formats; and (4) facilities for long-term access are not supported by the current desktop metaphor. Four environmental factors further complicate archiving in consumer settings: the pervasive influence of malware; consumer reliance on ad hoc IT providers; an accretion of minor system and registry inconsistencies; and strong consumer beliefs about the incorruptibility of digital forms, the reliability of digital technologies, and the social vulnerability of networked storage.",
        "published": "2007-04-27T02:35:57Z",
        "link": "http://arxiv.org/abs/0704.3653v1",
        "categories": [
            "cs.DL",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "Approximate textual retrieval",
        "authors": [
            "Pere Constans"
        ],
        "summary": "An approximate textual retrieval algorithm for searching sources with high levels of defects is presented. It considers splitting the words in a query into two overlapping segments and subsequently building composite regular expressions from interlacing subsets of the segments. This procedure reduces the probability of missed occurrences due to source defects, yet diminishes the retrieval of irrelevant, non-contextual occurrences.",
        "published": "2007-05-05T17:27:42Z",
        "link": "http://arxiv.org/abs/0705.0751v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.3; I.2.8"
        ]
    },
    {
        "title": "Tracking User Attention in Collaborative Tagging Communities",
        "authors": [
            "Elizeu Santos-Neto",
            "Matei Ripeanu",
            "Adriana Iamnitchi"
        ],
        "summary": "Collaborative tagging has recently attracted the attention of both industry and academia due to the popularity of content-sharing systems such as CiteULike, del.icio.us, and Flickr. These systems give users the opportunity to add data items and to attach their own metadata (or tags) to stored data. The result is an effective content management tool for individual users. Recent studies, however, suggest that, as tagging communities grow, the added content and the metadata become harder to manage due to an ease in content diversity. Thus, mechanisms that cope with increase of diversity are fundamental to improve the scalability and usability of collaborative tagging systems. This paper analyzes whether usage patterns can be harnessed to improve navigability in a growing knowledge space. To this end, it presents a characterization of two collaborative tagging communities that target scientific literature: CiteULike and Bibsonomy. We explore three main directions: First, we analyze the tagging activity distribution across the user population. Second, we define new metrics for similarity in user interest and use these metrics to uncover the structure of the tagging communities we study. The structure we uncover suggests a clear segmentation of interests into a large number of individuals with unique preferences and a core set of users with interspersed interests. Finally, we offer preliminary results that demonstrate that the interest-based structure of the tagging community can be used to facilitate content usage as communities scale.",
        "published": "2007-05-07T23:57:46Z",
        "link": "http://arxiv.org/abs/0705.1013v4",
        "categories": [
            "cs.DL",
            "cs.CY",
            "H.1.1; H.3.5"
        ]
    },
    {
        "title": "Scientific citations in Wikipedia",
        "authors": [
            "Finn Aarup Nielsen"
        ],
        "summary": "The Internet-based encyclopaedia Wikipedia has grown to become one of the most visited web-sites on the Internet. However, critics have questioned the quality of entries, and an empirical study has shown Wikipedia to contain errors in a 2005 sample of science entries. Biased coverage and lack of sources are among the \"Wikipedia risks\". The present work describes a simple assessment of these aspects by examining the outbound links from Wikipedia articles to articles in scientific journals with a comparison against journal statistics from Journal Citation Reports such as impact factors. The results show an increasing use of structured citation markup and good agreement with the citation pattern seen in the scientific literature though with a slight tendency to cite articles in high-impact journals such as Nature and Science. These results increase confidence in Wikipedia as an good information organizer for science in general.",
        "published": "2007-05-15T09:42:30Z",
        "link": "http://arxiv.org/abs/0705.2106v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.7; H.3.5; H.3.1"
        ]
    },
    {
        "title": "Open Access Publishing in Particle Physics: A Brief Introduction for the   non-Expert",
        "authors": [
            "Travis C. Brooks"
        ],
        "summary": "Open Access to particle physics literature does not sound particularly new or exciting, since particle physicists have been reading preprints for decades, and arXiv.org for 15 years. However new movements in Europe are attempting to make the peer-reviewed literature of the field fully Open Access. This is not a new movement, nor is it restricted to this field. However, given the field's history of preprints and eprints, it is well suited to a change to a fully Open Access publishing model. Data shows that 90% of HEP published literature is freely available online, meaning that HEP libraries have little need for expensive journal subscriptions. As libraries begin to cancel journal subscriptions, the peer review process will lose its primary source of funding. Open Access publishing models can potentially address this issue. European physicists and funding agencies are proposing a consortium, SCOAP3, that might solve many of the objections to traditional Open Access publishing models in Particle Physics. These proposed changes should be viewed as a starting point for a serious look at the field's publication model, and are at least worthy of attention, if not adoption.",
        "published": "2007-05-23T20:28:47Z",
        "link": "http://arxiv.org/abs/0705.3466v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Submission of content to a digital object repository using a   configurable workflow system",
        "authors": [
            "Andreas Hense",
            "Johannes Mueller"
        ],
        "summary": "The prototype of a workflow system for the submission of content to a digital object repository is here presented. It is based entirely on open-source standard components and features a service-oriented architecture. The front-end consists of Java Business Process Management (jBPM), Java Server Faces (JSF), and Java Server Pages (JSP). A Fedora Repository and a mySQL data base management system serve as a back-end. The communication between front-end and back-end uses a SOAP minimal binding stub. We describe the design principles and the construction of the prototype and discuss the possibilities and limitations of work ow creation by administrators. The code of the prototype is open-source and can be retrieved in the project escipub at http://sourceforge.net",
        "published": "2007-06-03T19:37:41Z",
        "link": "http://arxiv.org/abs/0706.0306v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Removing Manually-Generated Boilerplate from Electronic Texts:   Experiments with Project Gutenberg e-Books",
        "authors": [
            "Owen Kaser",
            "Daniel Lemire"
        ],
        "summary": "Collaborative work on unstructured or semi-structured documents, such as in literature corpora or source code, often involves agreed upon templates containing metadata. These templates are not consistent across users and over time. Rule-based parsing of these templates is expensive to maintain and tends to fail as new documents are added. Statistical techniques based on frequent occurrences have the potential to identify automatically a large fraction of the templates, thus reducing the burden on the programmers. We investigate the case of the Project Gutenberg corpus, where most documents are in ASCII format with preambles and epilogues that are often copied and pasted or manually typed. We show that a statistical approach can solve most cases though some documents require knowledge of English. We also survey various technical solutions that make our approach applicable to large data sets.",
        "published": "2007-07-13T02:30:10Z",
        "link": "http://arxiv.org/abs/0707.1913v3",
        "categories": [
            "cs.DL",
            "cs.CL"
        ]
    },
    {
        "title": "OA@MPS - a colourful view",
        "authors": [
            "Laurent Romary"
        ],
        "summary": "The open access agenda of the Max Planck Society, initiator of the Berlin Declaration, envisions the support of both the green way and the golden way to open access. For the implementation of the green way the Max Planck Society through its newly established unit (Max Planck Digital Library) follows the idea of providing a centralized technical platform for publications and a local support for editorial issues. With regard to the golden way, the Max Planck Society fosters the development of open access publication models and experiments new publishing concepts like the Living Reviews journals.",
        "published": "2007-07-19T12:30:42Z",
        "link": "http://arxiv.org/abs/0707.2886v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "An exploratory study of Google Scholar",
        "authors": [
            "Philipp Mayr",
            "Anne-Kathrin Walter"
        ],
        "summary": "The paper discusses and analyzes the scientific search service Google Scholar (GS). The focus is on an exploratory study which investigates the coverage of scientific serials in GS. The study shows deficiencies in the coverage and up-to-dateness of the GS index. Furthermore, the study points up which Web servers are the most important data providers for this search service and which information sources are highly represented. We can show that there is a relatively large gap in Google Scholars coverage of German literature as well as weaknesses in the accessibility of Open Access content.   Keywords: Search engines, Digital libraries, Worldwide Web, Serials, Electronic journals",
        "published": "2007-07-24T19:19:36Z",
        "link": "http://arxiv.org/abs/0707.3575v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Reconstruction of Protein-Protein Interaction Pathways by Mining   Subject-Verb-Objects Intermediates",
        "authors": [
            "Maurice HT Ling",
            "Christophe Lefevre",
            "Kevin R. Nicholas",
            "Feng Lin"
        ],
        "summary": "The exponential increase in publication rate of new articles is limiting access of researchers to relevant literature. This has prompted the use of text mining tools to extract key biological information. Previous studies have reported extensive modification of existing generic text processors to process biological text. However, this requirement for modification had not been examined. In this study, we have constructed Muscorian, using MontyLingua, a generic text processor. It uses a two-layered generalization-specialization paradigm previously proposed where text was generically processed to a suitable intermediate format before domain-specific data extraction techniques are applied at the specialization layer. Evaluation using a corpus and experts indicated 86-90% precision and approximately 30% recall in extracting protein-protein interactions, which was comparable to previous studies using either specialized biological text processing tools or modified existing tools. Our study had also demonstrated the flexibility of the two-layered generalization-specialization paradigm by using the same generalization layer for two specialized information extraction tasks.",
        "published": "2007-08-06T01:22:46Z",
        "link": "http://arxiv.org/abs/0708.0694v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "cs.DL"
        ]
    },
    {
        "title": "A Practical Ontology for the Large-Scale Modeling of Scholarly Artifacts   and their Usage",
        "authors": [
            "Marko A. Rodriguez",
            "Johah Bollen",
            "Herbert Van de Sompel"
        ],
        "summary": "The large-scale analysis of scholarly artifact usage is constrained primarily by current practices in usage data archiving, privacy issues concerned with the dissemination of usage data, and the lack of a practical ontology for modeling the usage domain. As a remedy to the third constraint, this article presents a scholarly ontology that was engineered to represent those classes for which large-scale bibliographic and usage data exists, supports usage research, and whose instantiation is scalable to the order of 50 million articles along with their associated artifacts (e.g. authors and journals) and an accompanying 1 billion usage events. The real world instantiation of the presented abstract ontology is a semantic network model of the scholarly community which lends the scholarly process to statistical analysis and computational support. We present the ontology, discuss its instantiation, and provide some example inference rules for calculating various scholarly artifact metrics.",
        "published": "2007-08-08T17:06:55Z",
        "link": "http://arxiv.org/abs/0708.1150v1",
        "categories": [
            "cs.DL",
            "cs.AI",
            "H.3.7; I.2.4"
        ]
    },
    {
        "title": "Open Access does not increase citations for research articles from The   Astrophysical Journal",
        "authors": [
            "Michael J. Kurtz",
            "Edwin A. Henneken"
        ],
        "summary": "We demonstrate conclusively that there is no \"Open Access Advantage\" for papers from the Astrophysical Journal. The two to one citation advantage enjoyed by papers deposited in the arXiv e-print server is due entirely to the nature and timing of the deposited papers. This may have implications for other disciplines.",
        "published": "2007-09-06T16:00:43Z",
        "link": "http://arxiv.org/abs/0709.0896v1",
        "categories": [
            "cs.DL",
            "cs.CY"
        ]
    },
    {
        "title": "When are recommender systems useful?",
        "authors": [
            "Marcel Blattner",
            "Alexander Hunziker",
            "Paolo Laureti"
        ],
        "summary": "Recommender systems are crucial tools to overcome the information overload brought about by the Internet. Rigorous tests are needed to establish to what extent sophisticated methods can improve the quality of the predictions. Here we analyse a refined correlation-based collaborative filtering algorithm and compare it with a novel spectral method for recommending. We test them on two databases that bear different statistical properties (MovieLens and Jester) without filtering out the less active users and ordering the opinions in time, whenever possible. We find that, when the distribution of user-user correlations is narrow, simple averages work nearly as well as advanced methods. Recommender systems can, on the other hand, exploit a great deal of additional information in systems where external influence is negligible and peoples' tastes emerge entirely. These findings are validated by simulations with artificially generated data.",
        "published": "2007-09-17T09:27:07Z",
        "link": "http://arxiv.org/abs/0709.2562v1",
        "categories": [
            "cs.IR",
            "cs.CY",
            "cs.DL",
            "cs.DS",
            "physics.data-an",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Content Reuse and Interest Sharing in Tagging Communities",
        "authors": [
            "Elizeu Santos-Neto",
            "Matei Ripeanu",
            "Adriana Iamnitchi"
        ],
        "summary": "Tagging communities represent a subclass of a broader class of user-generated content-sharing online communities. In such communities users introduce and tag content for later use. Although recent studies advocate and attempt to harness social knowledge in this context by exploiting collaboration among users, little research has been done to quantify the current level of user collaboration in these communities. This paper introduces two metrics to quantify the level of collaboration: content reuse and shared interest. Using these two metrics, this paper shows that the current level of collaboration in CiteULike and Connotea is consistently low, which significantly limits the potential of harnessing the social knowledge in communities. This study also discusses implications of these findings in the context of recommendation and reputation systems.",
        "published": "2007-11-26T23:05:02Z",
        "link": "http://arxiv.org/abs/0711.4142v2",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Simrank++: Query rewriting through link analysis of the click graph",
        "authors": [
            "Ioannis Antonellis",
            "Hector Garcia-Molina",
            "Chi-Chao Chang"
        ],
        "summary": "We focus on the problem of query rewriting for sponsored search. We base rewrites on a historical click graph that records the ads that have been clicked on in response to past user queries. Given a query q, we first consider Simrank as a way to identify queries similar to q, i.e., queries whose ads a user may be interested in. We argue that Simrank fails to properly identify query similarities in our application, and we present two enhanced version of Simrank: one that exploits weights on click graph edges and another that exploits ``evidence.'' We experimentally evaluate our new schemes against Simrank, using actual click graphs and queries form Yahoo!, and using a variety of metrics. Our results show that the enhanced methods can yield more and better query rewrites.",
        "published": "2007-12-04T12:43:17Z",
        "link": "http://arxiv.org/abs/0712.0499v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "The Importance of Being First: Position Dependent Citation Rates on   arXiv:astro-ph",
        "authors": [
            "J. P. Dietrich"
        ],
        "summary": "We study the dependence of citation counts of e-prints published on the arXiv:astro-ph server on their position in the daily astro-ph listing. Using the SPIRES literature database we reconstruct the astro-ph listings from July 2002 to December 2005 and determine citation counts for e-prints from their ADS entry. We use Zipf plots to analyze the citation distributions for each astro-ph position. We find that e-prints appearing at or near the top of the astro-ph mailings receive significantly more citations than those further down the list. This difference is significant at the 7 sigma level and on average amounts to two times more citations for papers at the top than those further down the listing. We propose three possible non-exclusive explanations for this positional citation effect and try to test them. We conclude that self-promotion by authors plays a role in the observed effect but cannot exclude that increased visibility at the top of the daily listings contributes to higher citation counts as well. We can rule out that the positional dependence of citations is caused by the coincidence of the submission deadline with the working hours of a geographically constrained set of intrinsically higher cited authors. We discuss several ways of mitigating the observed effect, including splitting astro-ph into several subject classes, randomizing the order of e-prints, and a novel approach to sorting entries by relevance to individual readers.",
        "published": "2007-12-06T21:00:11Z",
        "link": "http://arxiv.org/abs/0712.1037v1",
        "categories": [
            "astro-ph",
            "cs.DL"
        ]
    },
    {
        "title": "Ranking forestry journals using the h-index",
        "authors": [
            "Jerome K. Vanclay"
        ],
        "summary": "An expert ranking of forestry journals was compared with journal impact factors and h-indices computed from the ISI Web of Science and internet-based data. Citations reported by Google Scholar appear to offer the most efficient way to rank all journals objectively, in a manner consistent with other indicators. This h-index exhibited a high correlation with the journal impact factor (r=0.92), but is not confined to journals selected by any particular commercial provider. A ranking of 180 forestry journals is presented, on the basis of this index.",
        "published": "2007-12-12T12:09:55Z",
        "link": "http://arxiv.org/abs/0712.1916v4",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Reducing semantic complexity in distributed Digital Libraries: treatment   of term vagueness and document re-ranking",
        "authors": [
            "Philipp Mayr",
            "Peter Mutschke",
            "Vivien Petras"
        ],
        "summary": "The purpose of the paper is to propose models to reduce the semantic complexity in heterogeneous DLs. The aim is to introduce value-added services (treatment of term vagueness and document re-ranking) that gain a certain quality in DLs if they are combined with heterogeneity components established in the project \"Competence Center Modeling and Treatment of Semantic Heterogeneity\". Empirical observations show that freely formulated user terms and terms from controlled vocabularies are often not the same or match just by coincidence. Therefore, a value-added service will be developed which rephrases the natural language searcher terms into suggestions from the controlled vocabulary, the Search Term Recommender (STR). Two methods, which are derived from scientometrics and network analysis, will be implemented with the objective to re-rank result sets by the following structural properties: the ranking of the results by core journals (so-called Bradfordizing) and ranking by centrality of authors in co-authorship networks.",
        "published": "2007-12-14T21:24:26Z",
        "link": "http://arxiv.org/abs/0712.2449v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Determining the Applicability of Agile Practices to Mission and   Life-critical Systems",
        "authors": [
            "Ahmed Sidky",
            "James Arthur"
        ],
        "summary": "Adopting agile practices brings about many benefits and improvements to the system being developed. However, in mission and life-critical systems, adopting an inappropriate agile practice has detrimental impacts on the system in various phases of its lifecycle as well as precludes desired qualities from being actualized. This paper presents a three-stage process that provides guidance to organizations on how to identify the agile practices they can benefit from without causing any impact to the mission and life critical system being developed.",
        "published": "2007-01-01T05:08:06Z",
        "link": "http://arxiv.org/abs/cs/0701010v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Fingerprinting Logic Programs",
        "authors": [
            "Alexander Serebrenik",
            "Wim Vanhoof"
        ],
        "summary": "In this work we present work in progress on functionality duplication detection in logic programs. Eliminating duplicated functionality recently became prominent in context of refactoring. We describe a quantitative approach that allows to measure the ``similarity'' between two predicate definitions. Moreover, we show how to compute a so-called ``fingerprint'' for every predicate. Fingerprints capture those characteristics of the predicate that are significant when searching for duplicated functionality. Since reasoning on fingerprints is much easier than reasoning on predicate definitions, comparing the fingerprints is a promising direction in automated code duplication in logic programs.",
        "published": "2007-01-12T15:39:29Z",
        "link": "http://arxiv.org/abs/cs/0701081v1",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Certifying controls and systems software",
        "authors": [
            "Eric Feron",
            "Mardavij Roozbehani"
        ],
        "summary": "Software system certification presents itself with many challenges, including the necessity to certify the system at the level of functional requirements, code and binary levels, the need to chase down run-time errors, and the need for proving timing properties of the eventual, compiled system. This paper illustrates possible approaches for certifying code that arises from control systems requirements as far as stability properties are concerned. The relative simplicity of the certification process should encourage the development of systematic procedures for certifying control system codes for more complex environments.",
        "published": "2007-01-21T19:34:45Z",
        "link": "http://arxiv.org/abs/cs/0701132v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "On the Software and Knowledge Engineering Aspects of the Educational   Process",
        "authors": [
            "Th. Hadzilacos",
            "D. Kalles",
            "M. Pouliopoulou"
        ],
        "summary": "The Hellenic Open University has embarked on a large-scale effort to enhance its textbook-based material with content that demonstrably supports the basic tenets of distance learning. The challenge is to set up a framework that allows for the production-level creation, distribution and consumption of content, and at the same time, evaluate the effort in terms of technological, educational and organizational knowledge gained. This paper presents a model of the educational process that is used as a development backbone and argues about its conceptual and technical practicality at large.",
        "published": "2007-01-26T14:51:38Z",
        "link": "http://arxiv.org/abs/cs/0701175v1",
        "categories": [
            "cs.SE",
            "cs.DL"
        ]
    },
    {
        "title": "Reasoning from a schema and from an analog in software code reuse",
        "authors": [
            "Françoise Detienne"
        ],
        "summary": "The activity of design involves the decomposition of problems into subproblems and the development and evaluation of solutions. In many cases, solution development is not done from scratch. Designers often evoke and adapt solutions developed in the past. These solutions may come from an internal source, i.e. the memory of the designers, and/or from an external source. The goal of this paper is to analyse the characteristics of the cognitive mechanisms, the knowledge and the representations involved in the code reuse activity performed by experienced programmers. More generally, the focus is the control structure of the reuse activity. Data collected in an experiment in which programmers had to design programs are analyzed. Two code reuse situations are distinguished depending on whether or not the processes involved in reuse start before the elaboration of what acts as a source-solution. Our analysis highlights the use of reasoning from a schema and from an analog in the code reuse activity.",
        "published": "2007-01-31T16:41:02Z",
        "link": "http://arxiv.org/abs/cs/0701200v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Improving Prolog programs: Refactoring for Prolog",
        "authors": [
            "Alexander Serebrenik",
            "Tom Schrijvers",
            "Bart Demoen"
        ],
        "summary": "Refactoring is an established technique from the object-oriented (OO) programming community to restructure code: it aims at improving software readability, maintainability and extensibility. Although refactoring is not tied to the OO-paradigm in particular, its ideas have not been applied to Logic Programming until now.   This paper applies the ideas of refactoring to Prolog programs. A catalogue is presented listing refactorings classified according to scope. Some of the refactorings have been adapted from the OO-paradigm, while others have been specifically designed for Prolog. The discrepancy between intended and operational semantics in Prolog is also addressed by some of the refactorings.   In addition, ViPReSS, a semi-automatic refactoring browser, is discussed and the experience with applying ViPReSS to a large Prolog legacy system is reported. The main conclusion is that refactoring is both a viable technique in Prolog and a rather desirable one.",
        "published": "2007-02-14T09:53:37Z",
        "link": "http://arxiv.org/abs/cs/0702083v1",
        "categories": [
            "cs.SE",
            "D.2.7; D.1.6"
        ]
    },
    {
        "title": "Designing a Resource Broker for Heterogeneous Grids",
        "authors": [
            "Srikumar Venugopal",
            "Krishna Nadiminti",
            "Hussein Gibbins",
            "Rajkumar Buyya"
        ],
        "summary": "Grids provide uniform access to aggregations of heterogeneous resources and services such as computers, networks and storage owned by multiple organizations. However, such a dynamic environment poses many challenges for application composition and deployment. In this paper, we present the design of the Gridbus Grid resource broker that allows users to create applications and specify different objectives through different interfaces without having to deal with the complexity of Grid infrastructure. We present the unique requirements that motivated our design and discuss how these provide flexibility in extending the functionality of the broker to support different low-level middlewares and user interfaces. We evaluate the broker with different job profiles and Grid middleware and conclude with the lessons learnt from our development experience.",
        "published": "2007-02-24T08:35:02Z",
        "link": "http://arxiv.org/abs/cs/0702145v1",
        "categories": [
            "cs.DC",
            "cs.SE",
            "C.2.4; D.2.11"
        ]
    },
    {
        "title": "Pre-Requirement Specification Traceability: Bridging the Complexity Gap   through Capabilities",
        "authors": [
            "Ramya Ravichandar",
            "James D. Arthur",
            "Manuel Pérez-Quiñones"
        ],
        "summary": "Pre-Requirement Specification traceability is the activity of capturing relations between requirements and their sources, in particular user needs. Requirements are formal technical specifications in the solution space; needs are natural language expressions codifying user expectations in the problem space. Current traceability techniques are challenged by the complexity gap that results from the disparity between the spaces, and thereby, often neglect traceability to and from requirements. We identify the existence of an intermediary region -- the transition space -- which structures the progression from needs to requirements. More specifically, our approach to developing change-tolerant systems, termed Capabilities Engineering, identifies highly cohesive, minimally coupled, optimized functional abstractions called Capabilities in the transition space. These Capabilities link the problem and solution spaces through directives (entities derived from user needs). Directives connect the problem and transition spaces; Capabilities link the transition and solution spaces. Furthermore, the process of Capabilities Engineering addresses specific traceability challenges. It supports the evolution of traces, provides semantic and structural information about dependencies, incorporates human factors, generates traceability relations with negligible overhead, and thereby, fosters pre-Requirement Specification traceability.",
        "published": "2007-03-02T20:46:26Z",
        "link": "http://arxiv.org/abs/cs/0703012v1",
        "categories": [
            "cs.SE",
            "D.2.1"
        ]
    },
    {
        "title": "Addressing Components' Evolvement and Execution Behavior to Measure   Component-Based Software Reliability",
        "authors": [
            "Wen-Li Wang",
            "Mei-Huei Tang"
        ],
        "summary": "Software reliability is an important quality attrib-ute, often evaluated as either a function of time or of system structures. The goal of this study is to have this metric cover both for component-based software, be-cause its reliability strongly depends on the quality of constituent components and their interactions. To achieve this, we apply a convolution modeling ap-proach, based on components' execution behavior, to integrate their individual reliability evolvement and simultaneously address failure fixes in the time do-main. Modeling at the component level can be more economical to accommodate software evolution, be-cause the reliability metric can be evaluated by reus-ing the quality measures of unaffected components and adapting only to the affected ones to save cost. The adaptation capability also supports the incremental software development processes that constantly add in new components over time. Experiments were con-ducted to discuss the usefulness of this approach.",
        "published": "2007-03-05T20:58:44Z",
        "link": "http://arxiv.org/abs/cs/0703021v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Recommender System for Online Dating Service",
        "authors": [
            "Lukas Brozovsky",
            "Vaclav Petricek"
        ],
        "summary": "Users of online dating sites are facing information overload that requires them to manually construct queries and browse huge amount of matching user profiles. This becomes even more problematic for multimedia profiles. Although matchmaking is frequently cited as a typical application for recommender systems, there is a surprising lack of work published in this area. In this paper we describe a recommender system we implemented and perform a quantitative comparison of two collaborative filtering (CF) and two global algorithms. Results show that collaborative filtering recommenders significantly outperform global algorithms that are currently used by dating sites. A blind experiment with real users also confirmed that users prefer CF based recommendations to global popularity recommendations. Recommender systems show a great potential for online dating where they could improve the value of the service to users and improve monetization of the service.",
        "published": "2007-03-09T15:38:27Z",
        "link": "http://arxiv.org/abs/cs/0703042v1",
        "categories": [
            "cs.IR",
            "cs.SE",
            "H.3.3"
        ]
    },
    {
        "title": "Portlet Wrappers using JavaScript",
        "authors": [
            "Paul Fodor"
        ],
        "summary": "In this paper we extend the classical portal (with static portlets) design with HTML DOM Web clipping on the client browser using dynamic JavaScript portlets: the portal server supplies the user/passwords for all services through https and the client browser retrieves web pages and cuts/selects/changes the desired parts using paths (XPath) in the Web page structure. This operation brings along a set of advantages: dynamic wrapping of existing legacy websites in the client browser, the reloading of only changed portlets instead of whole portal, low bandwidth on the server, the elimination of re-writing the URL links in the portal, and last but not least, a support for Java applets in portlets by putting the login cookies on the client browser. Our solution is compliant with JSR168 Portlet Specification allowing portability across all vendor platforms.",
        "published": "2007-03-14T23:32:43Z",
        "link": "http://arxiv.org/abs/cs/0703069v3",
        "categories": [
            "cs.SE",
            "H.5.2; D.3.3"
        ]
    },
    {
        "title": "A Systematic Approach to Web-Application Development",
        "authors": [
            "Joy Dutta",
            "Paul Fodor"
        ],
        "summary": "Designing a web-application from a specification involves a series of well-planned and well executed steps leading to the final product. This often involves critical changes in design while testing the application, which itself is slow and cumbersome. Traditional approaches either fully automate the web-application development process, or let developers write everything from scratch. Our approach is based on a middle-ground, with precise control on the workflow and usage of a set of custom-made software tools to automate a significant part of code generation.",
        "published": "2007-03-15T12:55:46Z",
        "link": "http://arxiv.org/abs/cs/0703080v1",
        "categories": [
            "cs.SE",
            "H.5.2; D.3.3"
        ]
    },
    {
        "title": "An architecture-based dependability modeling framework using AADL",
        "authors": [
            "Ana-Elena Rugina",
            "Karama Kanoun",
            "Mohamed Kaaniche"
        ],
        "summary": "For efficiency reasons, the software system designers' will is to use an integrated set of methods and tools to describe specifications and designs, and also to perform analyses such as dependability, schedulability and performance. AADL (Architecture Analysis and Design Language) has proved to be efficient for software architecture modeling. In addition, AADL was designed to accommodate several types of analyses. This paper presents an iterative dependency-driven approach for dependability modeling using AADL. It is illustrated on a small example. This approach is part of a complete framework that allows the generation of dependability analysis and evaluation models from AADL models to support the analysis of software and system architectures, in critical application domains.",
        "published": "2007-04-06T09:33:06Z",
        "link": "http://arxiv.org/abs/0704.0865v1",
        "categories": [
            "cs.PF",
            "cs.SE"
        ]
    },
    {
        "title": "A Disciplined Approach to Adopting Agile Practices: The Agile Adoption   Framework",
        "authors": [
            "Ahmed Sidky",
            "James Arthur",
            "Shawn Bohner"
        ],
        "summary": "Many organizations aspire to adopt agile processes to take advantage of the numerous benefits that it offers to an organization. Those benefits include, but are not limited to, quicker return on investment, better software quality, and higher customer satisfaction. To date however, there is no structured process (at least in the public domain) that guides organizations in adopting agile practices. To address this problem we present the Agile Adoption Framework. The framework consists of two components: an agile measurement index, and a 4-Stage process, that together guide and assist the agile adoption efforts of organizations. More specifically, the agile measurement index is used to identify the agile potential of projects and organizations. The 4-Stage process, on the other hand, helps determine (a) whether or not organizations are ready for agile adoption, and (b) guided by their potential, what set of agile practices can and should be introduced.",
        "published": "2007-04-10T19:11:51Z",
        "link": "http://arxiv.org/abs/0704.1294v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "On How Developers Test Open Source Software Systems",
        "authors": [
            "Andy Zaidman",
            "Bart Van Rompaey",
            "Serge Demeyer",
            "Arie van Deursen"
        ],
        "summary": "Engineering software systems is a multidisciplinary activity, whereby a number of artifacts must be created - and maintained - synchronously. In this paper we investigate whether production code and the accompanying tests co-evolve by exploring a project's versioning system, code coverage reports and size-metrics. Three open source case studies teach us that testing activities usually start later on during the lifetime and are more \"phased\", although we did not observe increasing testing activity before releases. Furthermore, we note large differences in the levels of test coverage given the proportion of test code.",
        "published": "2007-05-24T16:21:35Z",
        "link": "http://arxiv.org/abs/0705.3616v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "PERCEVAL: a Computer-Driven System for Experimentation on Auditory and   Visual Perception",
        "authors": [
            "Carine André",
            "Alain Ghio",
            "Christian Cavé",
            "Bernard Teston"
        ],
        "summary": "Since perception tests are highly time-consuming, there is a need to automate as many operations as possible, such as stimulus generation, procedure control, perception testing, and data analysis. The computer-driven system we are presenting here meets these objectives. To achieve large flexibility, the tests are controlled by scripts. The system's core software resembles that of a lexical-syntactic analyzer, which reads and interprets script files sent to it. The execution sequence (trial) is modified in accordance with the commands and data received. This type of operation provides a great deal of flexibility and supports a wide variety of tests such as auditory-lexical decision making, phoneme monitoring, gating, phonetic categorization, word identification, voice quality, etc. To achieve good performance, we were careful about timing accuracy, which is the greatest problem in computerized perception tests.",
        "published": "2007-05-30T15:31:07Z",
        "link": "http://arxiv.org/abs/0705.4415v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Interpolant-Based Transition Relation Approximation",
        "authors": [
            "Ranjit Jhala",
            "Kenneth L. McMillan"
        ],
        "summary": "In predicate abstraction, exact image computation is problematic, requiring in the worst case an exponential number of calls to a decision procedure. For this reason, software model checkers typically use a weak approximation of the image. This can result in a failure to prove a property, even given an adequate set of predicates. We present an interpolant-based method for strengthening the abstract transition relation in case of such failures. This approach guarantees convergence given an adequate set of predicates, without requiring an exact image computation. We show empirically that the method converges more rapidly than an earlier method based on counterexample analysis.",
        "published": "2007-06-04T20:07:54Z",
        "link": "http://arxiv.org/abs/0706.0523v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SE",
            "D.2.4; F.3.1"
        ]
    },
    {
        "title": "A Generic Model of Contracts for Embedded Systems",
        "authors": [
            "Albert Benveniste",
            "Benoit Caillaud",
            "Roberto Passerone"
        ],
        "summary": "We present the mathematical foundations of the contract-based model developed in the framework of the SPEEDS project. SPEEDS aims at developing methods and tools to support \"speculative design\", a design methodology in which distributed designers develop different aspects of the overall system, in a concurrent but controlled way. Our generic mathematical model of contract supports this style of development. This is achieved by focusing on behaviors, by supporting the notion of \"rich component\" where diverse (functional and non-functional) aspects of the system can be considered and combined, by representing rich components via their set of associated contracts, and by formalizing the whole process of component composition.",
        "published": "2007-06-11T12:22:15Z",
        "link": "http://arxiv.org/abs/0706.1456v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Une sémantique observationnelle du modèle des boîtes pour la   résolution de programmes logiques (version étendue)",
        "authors": [
            "Pierre Deransart",
            "Mireille Ducassé",
            "Gérard Ferrand"
        ],
        "summary": "This report specifies an observational semantics and gives an original presentation of the Byrd's box model. The approach accounts for the semantics of Prolog tracers independently of a particular implementation. Traces are, in general, considered as rather obscure and difficult to use. The proposed formal presentation of a trace constitutes a simple and pedagogical approach for teaching Prolog or for implementing Prolog tracers. It constitutes a form of declarative specification for the tracers. Our approach highlights qualities of the box model which made its success, but also its drawbacks and limits. As a matter of fact, the presented semantics is only one example to illustrate general problems relating to tracers and observing processes. Observing processes know, from observed processes, only their traces. The issue is then to be able to reconstitute by the sole analysis of the trace the main part of the observed process, and if possible, without any loss of information.",
        "published": "2007-06-21T14:20:30Z",
        "link": "http://arxiv.org/abs/0706.3159v2",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "A Comparison of Push and Pull Techniques for Ajax",
        "authors": [
            "Engin Bozdag",
            "Ali Mesbah",
            "Arie van Deursen"
        ],
        "summary": "Ajax applications are designed to have high user interactivity and low user-perceived latency. Real-time dynamic web data such as news headlines, stock tickers, and auction updates need to be propagated to the users as soon as possible. However, Ajax still suffers from the limitations of the Web's request/response architecture which prevents servers from pushing real-time dynamic web data. Such applications usually use a pull style to obtain the latest updates, where the client actively requests the changes based on a predefined interval. It is possible to overcome this limitation by adopting a push style of interaction where the server broadcasts data when a change occurs on the server side. Both these options have their own trade-offs. This paper explores the fundamental limits of browser-based applications and analyzes push solutions for Ajax technology. It also shows the results of an empirical study comparing push and pull.",
        "published": "2007-06-27T09:14:40Z",
        "link": "http://arxiv.org/abs/0706.3984v2",
        "categories": [
            "cs.SE",
            "cs.PF"
        ]
    },
    {
        "title": "Managing Separation of Concerns in Grid Applications Through   Architectural Model Transformations",
        "authors": [
            "David Manset",
            "Herve Verjus",
            "Richard McClatchey"
        ],
        "summary": "Grids enable the aggregation, virtualization and sharing of massive heterogeneous and geographically dispersed resources, using files, applications and storage devices, to solve computation and data intensive problems, across institutions and countries via temporary collaborations called virtual organizations (VO). Most implementations result in complex superposition of software layers, often delivering low quality of service and quality of applications. As a consequence, Grid-based applications design and development is increasingly complex, and the use of most classical engineering practices is unsuccessful. Not only is the development of such applications a time-consuming, error prone and expensive task, but also the resulting applications are often hard-coded for specific Grid configurations, platforms and infra-structures. Having neither guidelines nor rules in the design of a Grid-based application is a paradox since there are many existing architectural approaches for distributed computing, which could ease and promote rigorous engineering methods based on the re-use of software components. It is our belief that ad-hoc and semi-formal engineer-ing approaches, in current use, are insufficient to tackle tomorrows Grid develop-ments requirements. Because Grid-based applications address multi-disciplinary and complex domains (health, military, scientific computation), their engineering requires rigor and control. This paper therefore advocates a formal model-driven engineering process and corresponding design framework and tools for building the next generation of Grids.",
        "published": "2007-07-05T11:10:03Z",
        "link": "http://arxiv.org/abs/0707.0761v1",
        "categories": [
            "cs.SE",
            "cs.DC",
            "D.2.11"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the   Astrobiology Phone-cam",
        "authors": [
            "Alexandra Bartolo",
            "Patrick C. McGuire",
            "Kenneth P. Camilleri",
            "Christopher Spiteri",
            "Jonathan C. Borg",
            "Philip J. Farrugia",
            "Jens Ormo",
            "Javier Gomez-Elvira",
            "Jose Antonio Rodriguez-Manfredi",
            "Enrique Diaz-Martinez",
            "Helge Ritter",
            "Robert Haschke",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "We have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. This camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. We envision that the `Astrobiology Phone-cam' exploration system can be fruitfully used in other problem domains as well.",
        "published": "2007-07-05T15:19:37Z",
        "link": "http://arxiv.org/abs/0707.0808v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.NI",
            "cs.RO",
            "cs.SE"
        ]
    },
    {
        "title": "Interface groups and financial transfer architectures",
        "authors": [
            "Jan A. Bergstra",
            "Alban Ponse"
        ],
        "summary": "Analytic execution architectures have been proposed by the same authors as a means to conceptualize the cooperation between heterogeneous collectives of components such as programs, threads, states and services. Interface groups have been proposed as a means to formalize interface information concerning analytic execution architectures. These concepts are adapted to organization architectures with a focus on financial transfers. Interface groups (and monoids) now provide a technique to combine interface elements into interfaces with the flexibility to distinguish between directions of flow dependent on entity naming.   The main principle exploiting interface groups is that when composing a closed system of a collection of interacting components, the sum of their interfaces must vanish in the interface group modulo reflection. This certainly matters for financial transfer interfaces.   As an example of this, we specify an interface group and within it some specific interfaces concerning the financial transfer architecture for a part of our local academic organization.   Financial transfer interface groups arise as a special case of more general service architecture interfaces.",
        "published": "2007-07-11T14:55:26Z",
        "link": "http://arxiv.org/abs/0707.1639v1",
        "categories": [
            "cs.SE",
            "D.2.2"
        ]
    },
    {
        "title": "An Integrated Crosscutting Concern Migration Strategy and its   Application to JHotDraw",
        "authors": [
            "Marius Marin",
            "Leon Moonen",
            "Arie van Deursen"
        ],
        "summary": "In this paper we propose a systematic strategy for migrating crosscutting concerns in existing object-oriented systems to aspect-based solutions. The proposed strategy consists of four steps: mining, exploration, documentation and refactoring of crosscutting concerns. We discuss in detail a new approach to aspect refactoring that is fully integrated with our strategy, and apply the whole strategy to an object-oriented system, namely the JHotDraw framework. The result of this migration is made available as an open-source project, which is the largest aspect refactoring available to date. We report on our experiences with conducting this case study and reflect on the success and challenges of the migration process, as well as on the feasibility of automatic aspect refactoring.",
        "published": "2007-07-16T09:38:23Z",
        "link": "http://arxiv.org/abs/0707.2291v2",
        "categories": [
            "cs.SE",
            "D.2"
        ]
    },
    {
        "title": "Parsimony Principles for Software Components and Metalanguages",
        "authors": [
            "Todd L. Veldhuizen"
        ],
        "summary": "Software is a communication system. The usual topic of communication is program behavior, as encoded by programs. Domain-specific libraries are codebooks, domain-specific languages are coding schemes, and so forth. To turn metaphor into method, we adapt toolsfrom information theory--the study of efficient communication--to probe the efficiency with which languages and libraries let us communicate programs. In previous work we developed an information-theoretic analysis of software reuse in problem domains. This new paper uses information theory to analyze tradeoffs in the design of components, generators, and metalanguages. We seek answers to two questions: (1) How can we judge whether a component is over- or under-generalized? Drawing on minimum description length principles, we propose that the best component yields the most succinct representation of the use cases. (2) If we view a programming language as an assemblage of metalanguages, each providing a complementary style of abstraction, how can these metalanguages aid or hinder us in efficiently describing software? We describe a complex triangle of interactions between the power of an abstraction mechanism, the amount of reuse it enables, and the cognitive difficulty of its use.",
        "published": "2007-07-27T19:29:29Z",
        "link": "http://arxiv.org/abs/0707.4166v1",
        "categories": [
            "cs.SE",
            "D.2.13; D.2.10"
        ]
    },
    {
        "title": "A Language for Generic Programming in the Large",
        "authors": [
            "Jeremy G. Siek",
            "Andrew Lumsdaine"
        ],
        "summary": "Generic programming is an effective methodology for developing reusable software libraries. Many programming languages provide generics and have features for describing interfaces, but none completely support the idioms used in generic programming. To address this need we developed the language G. The central feature of G is the concept, a mechanism for organizing constraints on generics that is inspired by the needs of modern C++ libraries. G provides modular type checking and separate compilation (even of generics). These characteristics support modular software development, especially the smooth integration of independently developed components. In this article we present the rationale for the design of G and demonstrate the expressiveness of G with two case studies: porting the Standard Template Library and the Boost Graph Library from C++ to G. The design of G shares much in common with the concept extension proposed for the next C++ Standard (the authors participated in its design) but there are important differences described in this article.",
        "published": "2007-08-16T18:06:18Z",
        "link": "http://arxiv.org/abs/0708.2255v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.3"
        ]
    },
    {
        "title": "Improved Linear Parallel Interference Cancellers",
        "authors": [
            "T. Srikanth",
            "K. Vishnu Vardhan",
            "A. Chockalingam",
            "L. B. Milstein"
        ],
        "summary": "In this paper, taking the view that a linear parallel interference canceller (LPIC) can be seen as a linear matrix filter, we propose new linear matrix filters that can result in improved bit error performance compared to other LPICs in the literature. The motivation for the proposed filters arises from the possibility of avoiding the generation of certain interference and noise terms in a given stage that would have been present in a conventional LPIC (CLPIC). In the proposed filters, we achieve such avoidance of the generation of interference and noise terms in a given stage by simply making the diagonal elements of a certain matrix in that stage equal to zero. Hence, the proposed filters do not require additional complexity compared to the CLPIC, and they can allow achieving a certain error performance using fewer LPIC stages. We also extend the proposed matrix filter solutions to a multicarrier DS-CDMA system, where we consider two types of receivers. In one receiver (referred to as Type-I receiver), LPIC is performed on each subcarrier first, followed by multicarrier combining (MCC). In the other receiver (called Type-II receiver), MCC is performed first, followed by LPIC. We show that in both Type-I and Type-II receivers, the proposed matrix filters outperform other matrix filters. Also, Type-II receiver performs better than Type-I receiver because of enhanced accuracy of the interference estimates achieved due to frequency diversity offered by MCC.",
        "published": "2007-09-14T07:56:21Z",
        "link": "http://arxiv.org/abs/0709.2225v1",
        "categories": [
            "cs.IT",
            "cs.SC",
            "cs.SD",
            "cs.SE",
            "math.IT"
        ]
    },
    {
        "title": "Heap Reference Analysis for Functional Programs",
        "authors": [
            "Amey Karkare",
            "Amitabha Sanyal",
            "Uday Khedker"
        ],
        "summary": "Current garbage collectors leave a lot of garbage uncollected because they conservatively approximate liveness by reachability from program variables. In this paper, we describe a sequence of static analyses that takes as input a program written in a first-order, eager functional programming language, and finds at each program point the references to objects that are guaranteed not to be used in the future. Such references are made null by a transformation pass. If this makes the object unreachable, it can be collected by the garbage collector. This causes more garbage to be collected, resulting in fewer collections. Additionally, for those garbage collectors which scavenge live objects, it makes each collection faster.   The interesting aspects of our method are both in the identification of the analyses required to solve the problem and the way they are carried out. We identify three different analyses -- liveness, sharing and accessibility. In liveness and sharing analyses, the function definitions are analyzed independently of the calling context. This is achieved by using a variable to represent the unknown context of the function being analyzed and setting up constraints expressing the effect of the function with respect to the variable. The solution of the constraints is a summary of the function that is parameterized with respect to a calling context and is used to analyze function calls. As a result we achieve context sensitivity at call sites without analyzing the function multiple number of times.",
        "published": "2007-10-08T08:43:58Z",
        "link": "http://arxiv.org/abs/0710.1482v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.2; D.3.4; F.3.2"
        ]
    },
    {
        "title": "Cross-Participants : fostering design-use mediation in an Open Source   Software community",
        "authors": [
            "Flore Barcellini",
            "Françoise Détienne",
            "Jean-Marie Burkhardt"
        ],
        "summary": "Motivation - This research aims at investigating emerging roles and forms of participation fostering design-use mediation during the Open Source Software design process Research approach - We compare online interactions for a successful \"pushed-by-users\" design process with unsuccessful previous proposals. The methodology developed, articulate structural analyses of the discussions (organization of discussions, participation) to actions to the code and documentation made by participants to the project. We focus on the useroriented and the developer-oriented mailing-lists of the Python project. Findings/Design - We find that key-participants, the cross-participants, foster the design process and act as boundary spanners between the users and the developers' communities. Research limitations/Implications - These findings can be reinforced developing software to automate the structural analysis of discussions and actions to the code and documentation. Further analyses, supported by these tools, will be necessary to generalise our results. Originality/Value - The analysis of participation among the three interaction spaces of OSS design (discussion, documentation and implementation) is the main originality of this work compared to other OSS research that mainly analyse one or two spaces. Take away message - Beside the idealistic picture that users may intervene freely in the process, OSS design is boost and framed by some key-participants and specific rules and there can be barriers to users' participation",
        "published": "2007-10-09T14:44:44Z",
        "link": "http://arxiv.org/abs/0710.1772v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "cs.SE"
        ]
    },
    {
        "title": "Implementation, Compilation, Optimization of Object-Oriented Languages,   Programs and Systems - Report on the Workshop ICOOOLPS'2006 at ECOOP'06",
        "authors": [
            "Roland Ducournau",
            "Etienne Gagnon",
            "Chandra Krintz",
            "Philippe Mulet",
            "Jan Vitek",
            "Olivier Zendra"
        ],
        "summary": "ICOOOLPS'2006 was the first edition of ECOOP-ICOOOLPS workshop. It intended to bring researchers and practitioners both from academia and industry together, with a spirit of openness, to try and identify and begin to address the numerous and very varied issues of optimization. This succeeded, as can be seen from the papers, the attendance and the liveliness of the discussions that took place during and after the workshop, not to mention a few new cooperations or postdoctoral contracts. The 22 talented people from different groups who participated were unanimous to appreciate this first edition and recommend that ICOOOLPS be continued next year. A community is thus beginning to form, and should be reinforced by a second edition next year, with all the improvements this first edition made emerge.",
        "published": "2007-10-15T17:53:49Z",
        "link": "http://arxiv.org/abs/0710.2887v1",
        "categories": [
            "cs.PF",
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "UML 2.0 - Overview and Perspectives in SoC Design",
        "authors": [
            "Tim Schattkowsky"
        ],
        "summary": "The design productivity gap requires more efficient design methods. Software systems have faced the same challenge and seem to have mastered it with the introduction of more abstract design methods. The UML has become the standard for software systems modeling and thus the foundation of new design methods. Although the UML is defined as a general purpose modeling language, its application to hardware and hardware/software codesign is very limited. In order to successfully apply the UML at these fields, it is essential to understand its capabilities and to map it to a new domain.",
        "published": "2007-10-25T08:11:39Z",
        "link": "http://arxiv.org/abs/0710.4641v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Applying UML and MDA to Real Systems Design",
        "authors": [
            "Ian Oliver"
        ],
        "summary": "Traditionally system design has been made from a black box/functionality only perspective which forces the developer to concentrate on how the functionality can be decomposed and recomposed into so called components. While this technique is well established and well known it does suffer fromsome drawbacks; namely that the systems produced can often be forced into certain, incompatible architectures, difficult to maintain or reuse and the code itself difficult to debug. Now that ideas such as the OMG's Model Based Architecture (MDA) or Model Based Engineering (MBE) and the ubiquitous modelling language UML are being used (allegedly) and desired we face a number of challenges to existing techniques.",
        "published": "2007-10-25T09:07:10Z",
        "link": "http://arxiv.org/abs/0710.4682v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "A Decompilation Approach to Partitioning Software for   Microprocessor/FPGA Platforms",
        "authors": [
            "Greg Stitt",
            "Frank Vahid"
        ],
        "summary": "In this paper, we present a software compilation approach for microprocessor/FPGA platforms that partitions a software binary onto custom hardware implemented in the FPGA. Our approach imposes less restrictions on software tool flow than previous compiler approaches, allowing software designers to use any software language and compiler. Our approach uses a back-end partitioning tool that utilizes decompilation techniques to recover important high-level information, resulting in performance comparable to high-level compiler-based approaches.",
        "published": "2007-10-25T09:22:50Z",
        "link": "http://arxiv.org/abs/0710.4700v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Model Reuse through Hardware Design Patterns",
        "authors": [
            "Fernando Rincon",
            "Francisco Moya",
            "Jesus Barba",
            "Juan Carlos Lopez"
        ],
        "summary": "Increasing reuse opportunities is a well-known problem for software designers as well as for hardware designers. Nonetheless, current software and hardware engineering practices have embraced different approaches to this problem. Software designs are usually modelled after a set of proven solutions to recurrent problems called design patterns. This approach differs from the component-based reuse usually found in hardware designs: design patterns do not specify unnecessary implementation details. Several authors have already proposed translating structural design patterns concepts to hardware design. In this paper we extend the discussion to behavioural design patterns. Specifically, we describe how the hardware version of the Iterator can be used to enhance model reuse.",
        "published": "2007-10-25T09:53:16Z",
        "link": "http://arxiv.org/abs/0710.4755v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Unified Modeling of Complex Real-Time Control Systems",
        "authors": [
            "He Hai",
            "Zhong Yi-Fang",
            "Cai Chi-Lan"
        ],
        "summary": "Complex real-time control system is a software dense and algorithms dense system, which needs modern software engineering techniques to design. UML is an object-oriented industrial standard modeling language, used more and more in real-time domain. This paper first analyses the advantages and problems of using UML for real-time control systems design. Then, it proposes an extension of UML-RT to support time-continuous subsystems modeling. So we can unify modeling of complex real-time control systems on UML-RT platform, from requirement analysis, model design, simulation, until generation code.",
        "published": "2007-10-25T11:50:54Z",
        "link": "http://arxiv.org/abs/0710.4793v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "AutoMoDe - Model-Based Development of Automotive Software",
        "authors": [
            "Dirk Ziegenbein",
            "Peter Braun",
            "Ulrich Freund",
            "Andreas Bauer",
            "Jan Romberg",
            "Bernhard Schatz"
        ],
        "summary": "This paper describes first results from the AutoMoDe (Automotive Model-Based Development) project. The overall goal of the project is to develop an integrated methodology for model-based development of automotive control software, based on problem-specific design notations with an explicit formal foundation. Based on the existing AutoFOCUS framework, a tool prototype is being developed in order to illustrate and validate the key elements of our approach.",
        "published": "2007-10-25T12:08:52Z",
        "link": "http://arxiv.org/abs/0710.4829v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Source-to-source optimizing transformations of Prolog programs based on   abstract interpretation",
        "authors": [
            "Francois Gobert",
            "Baudouin Le Charlier"
        ],
        "summary": "Making a Prolog program more efficient by transforming its source code, without changing its operational semantics, is not an obvious task. It requires the user to have a clear understanding of how the Prolog compiler works, and in particular, of the effects of impure features like the cut. The way a Prolog code is written - e.g., the order of clauses, the order of literals in a clause, the use of cuts or negations - influences its efficiency. Furthermore, different optimization techniques may be redundant or conflicting when they are applied together, depending on the way a procedure is called - e.g., inserting cuts and enabling indexing. We present an optimiser, based on abstract interpretation, that automatically performs safe code transformations of Prolog procedures in the context of some class of input calls. The method is more effective if procedures are annotated with additional information about modes, types, sharing, number of solutions and the like. Thus the approach is similar to Mercury. It applies to any Prolog program, however.",
        "published": "2007-10-31T15:59:50Z",
        "link": "http://arxiv.org/abs/0710.5895v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Declarative Diagnosis of Floundering",
        "authors": [
            "Lee Naish"
        ],
        "summary": "Many logic programming languages have delay primitives which allow coroutining. This introduces a class of bug symptoms -- computations can flounder when they are intended to succeed or finitely fail. For concurrent logic programs this is normally called deadlock. Similarly, constraint logic programs can fail to invoke certain constraint solvers because variables are insufficiently instantiated or constrained. Diagnosing such faults has received relatively little attention to date. Since delay primitives affect the procedural but not the declarative view of programs, it may be expected that debugging would have to consider the often complex details of interleaved execution. However, recent work on semantics has suggested an alternative approach. In this paper we show how the declarative debugging paradigm can be used to diagnose unexpected floundering, insulating the user from the complexities of the execution.   Keywords: logic programming, coroutining, delay, debugging, floundering, deadlock, constraints",
        "published": "2007-11-01T01:40:50Z",
        "link": "http://arxiv.org/abs/0711.0048v1",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "A Prolog-based Environment for Reasoning about Programming Languages   (Extended abstract)",
        "authors": [
            "Roberto Bagnara",
            "Patricia Hill",
            "Enea Zaffanella"
        ],
        "summary": "ECLAIR is a Prolog-based prototype system aiming to provide a functionally complete environment for the study, development and evaluation of programming language analysis and implementation tools. In this paper, we sketch the overall structure of the system, outlining the main methodologies and technologies underlying its components. We also discuss the appropriateness of Prolog as the implementation language for the system: besides highlighting its strengths, we also point out a few potential weaknesses, hinting at possible solutions.",
        "published": "2007-11-02T16:40:10Z",
        "link": "http://arxiv.org/abs/0711.0345v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Compiling ER Specifications into Declarative Programs",
        "authors": [
            "Bernd Braßel",
            "Michael Hanus",
            "Marion Muller"
        ],
        "summary": "This paper proposes an environment to support high-level database programming in a declarative programming language. In order to ensure safe database updates, all access and update operations related to the database are generated from high-level descriptions in the entity- relationship (ER) model. We propose a representation of ER diagrams in the declarative language Curry so that they can be constructed by various tools and then translated into this representation. Furthermore, we have implemented a compiler from this representation into a Curry program that provides access and update operations based on a high-level API for database programming.",
        "published": "2007-11-02T16:49:30Z",
        "link": "http://arxiv.org/abs/0711.0348v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Automatic Coding Rule Conformance Checking Using Logic Programs",
        "authors": [
            "Guillem Marpons-Ucero",
            "Julio Mariño",
            "Ángel Herranz",
            "Lars-Åke Fredlund",
            "Manuel Carro",
            "Juan José Moreno-Navarro"
        ],
        "summary": "Some approaches to increasing program reliability involve a disciplined use of programming languages so as to minimise the hazards introduced by error-prone features. This is realised by writing code that is constrained to a subset of the a priori admissible programs, and that, moreover, may use only a subset of the language. These subsets are determined by a collection of so-called coding rules.",
        "published": "2007-11-02T16:53:34Z",
        "link": "http://arxiv.org/abs/0711.0344v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Spreadsheet Engineering: A Research Framework",
        "authors": [
            "Thomas A. Grossman"
        ],
        "summary": "Spreadsheet engineering adapts the lessons of software engineering to spreadsheets, providing eight principles as a framework for organizing spreadsheet programming recommendations. Spreadsheets raise issues inadequately addressed by software engineering. Spreadsheets are a powerful modeling language, allowing strategic rapid model change, and enabling exploratory modeling. Spreadsheets users learn slowly with experience because they focus on the problem domain not programming. The heterogeneity of spreadsheet users requires a taxonomy to guide recommendations. Deployment of best practices is difficult and merits research.",
        "published": "2007-11-04T19:24:57Z",
        "link": "http://arxiv.org/abs/0711.0538v1",
        "categories": [
            "cs.SE",
            "D.1.7; D.2.1; D.2.11; D.3.2; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Exploring the Composition of Unit Test Suites",
        "authors": [
            "Bart Van Rompaey",
            "Serge Demeyer"
        ],
        "summary": "In agile software development, test code can considerably contribute to the overall source code size. Being a valuable asset both in terms of verification and documentation, the composition of a test suite needs to be well understood in order to identify opportunities as well as weaknesses for further evolution. In this paper, we argue that the visualization of structural characteristics is a viable means to support the exploration of test suites. Thanks to general agreement on a limited set of key test design principles, such visualizations are relatively easy to interpret. In particular, we present visualizations that support testers in (i) locating test cases; (ii) examining the relation between test code and production code; and (iii) studying the composition of and dependencies within test cases. By means of two case studies, we demonstrate how visual patterns help to identify key test suite characteristics. This approach forms the first step in assisting a developer to build up understanding about test suites beyond code reading.",
        "published": "2007-11-05T11:05:42Z",
        "link": "http://arxiv.org/abs/0711.0607v1",
        "categories": [
            "cs.SE",
            "D.2.5; D.2.10; D.2.7"
        ]
    },
    {
        "title": "PIDoc: Wiki style Literate Programming for Prolog",
        "authors": [
            "Jan Wielemaker",
            "Anjo Anjewierden"
        ],
        "summary": "This document introduces PlDoc, a literate programming system for Prolog. Starting point for PlDoc was minimal distraction from the programming task and maximal immediate reward, attempting to seduce the programmer to use the system. Minimal distraction is achieved using structured comments that are as closely as possible related to common Prolog documentation practices. Immediate reward is provided by a web interface powered from the Prolog development environment that integrates searching and browsing application and system documentation. When accessed from localhost, it is possible to go from documentation shown in a browser to the source code displayed in the user's editor of choice.",
        "published": "2007-11-05T12:13:12Z",
        "link": "http://arxiv.org/abs/0711.0618v1",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Machine structure oriented control code logic",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "Control code is a concept that is closely related to a frequently occurring practitioner's view on what is a program: code that is capable of controlling the behaviour of some machine. We present a logical approach to explain issues concerning control codes that are independent of the details of the behaviours that are controlled. Using this approach, such issues can be explained at a very abstract level. We illustrate this among other things by means of an example about the production of a new compiler from an existing one. The approach is based on abstract machine models, called machine structures. We introduce a model of systems that provide execution environments for the executable codes of machine structures and use it to go into portability of control codes.",
        "published": "2007-11-06T11:01:21Z",
        "link": "http://arxiv.org/abs/0711.0836v3",
        "categories": [
            "cs.SE",
            "D.0; D.3.4; F.1.1; F.4.1"
        ]
    },
    {
        "title": "Applying Software Defect Estimations: Using a Risk Matrix for Tuning   Test Effort",
        "authors": [
            "James Cusick"
        ],
        "summary": "Applying software defect esimation techniques and presenting this information in a compact and impactful decision table can clearly illustrate to collaborative groups how critical this position is in the overall development cycle. The Test Risk Matrix described here has proven to be a valuable addition to the management tools and approaches used in developing large scale software on several releases. Use of this matrix in development planning meetings can clarify the attendant risks and possible consequences of carrying out or bypassing specific test activities.",
        "published": "2007-11-11T18:56:53Z",
        "link": "http://arxiv.org/abs/0711.1669v1",
        "categories": [
            "cs.SE",
            "cs.OH"
        ]
    },
    {
        "title": "Knowware: the third star after Hardware and Software",
        "authors": [
            "Ruqian Lu"
        ],
        "summary": "This book proposes to separate knowledge from software and to make it a commodity that is called knowware. The architecture, representation and function of Knowware are discussed. The principles of knowware engineering and its three life cycle models: furnace model, crystallization model and spiral model are proposed and analyzed. Techniques of software/knowware co-engineering are introduced. A software component whose knowledge is replaced by knowware is called mixware. An object and component oriented development schema of mixware is introduced. In particular, the tower model and ladder model for mixware development are proposed and discussed. Finally, knowledge service and knowware based Web service are introduced and compared with Web service. In summary, knowware, software and hardware should be considered as three equally important underpinnings of IT industry.   Ruqian Lu is a professor of computer science of the Institute of Mathematics, Academy of Mathematics and System Sciences. He is a fellow of Chinese Academy of Sciences. His research interests include artificial intelligence, knowledge engineering and knowledge based software engineering. He has published more than 100 papers and 10 books. He has won two first class awards from the Academia Sinica and a National second class prize from the Ministry of Science and Technology. He has also won the sixth Hua Loo-keng Mathematics Prize.",
        "published": "2007-11-27T17:36:35Z",
        "link": "http://arxiv.org/abs/0711.4309v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.CY"
        ]
    },
    {
        "title": "Recommended Practices for Spreadsheet Testing",
        "authors": [
            "Raymond R. Panko"
        ],
        "summary": "This paper presents the authors recommended practices for spreadsheet testing. Documented spreadsheet error rates are unacceptable in corporations today. Although improvements are needed throughout the systems development life cycle, credible improvement programs must include comprehensive testing. Several forms of testing are possible, but logic inspection is recommended for module testing. Logic inspection appears to be feasible for spreadsheet developers to do, and logic inspection appears to be safe and effective.",
        "published": "2007-12-01T21:27:59Z",
        "link": "http://arxiv.org/abs/0712.0109v1",
        "categories": [
            "cs.SE",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Implementation, Compilation, Optimization of Object-Oriented Languages,   Programs and Systems - Report on the Workshop ICOOOLPS'2007 at ECOOP'07",
        "authors": [
            "Olivier Zendra",
            "Eric Jul",
            "Roland Ducournau",
            "Etienne Gagnon",
            "Richard E. Jones",
            "Chandra Krintz",
            "Philippe Mulet",
            "Jan Vitek"
        ],
        "summary": "ICOOOLPS'2007 was the second edition of the ECOOP-ICOOOLPS workshop. ICOOOLPS intends to bring researchers and practitioners both from academia and industry together, with a spirit of openness, to try and identify and begin to address the numerous and very varied issues of optimization. After a first successful edition, this second one put a stronger emphasis on exchanges and discussions amongst the participants, progressing on the bases set last year in Nantes. The workshop attendance was a success, since the 30-people limit we had set was reached about 2 weeks before the workshop itself. Some of the discussions (e.g. annotations) were so successful that they would required even more time than we were able to dedicate to them. That's one area we plan to further improve for the next edition.",
        "published": "2007-12-07T17:01:52Z",
        "link": "http://arxiv.org/abs/0712.1189v1",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "A Typical Model Audit Approach: Spreadsheet Audit Methodologies in the   City of London",
        "authors": [
            "Grenville J. Croll"
        ],
        "summary": "Spreadsheet audit and review procedures are an essential part of almost all City of London financial transactions. Structured processes are used to discover errors in large financial spreadsheets underpinning major transactions of all types. Serious errors are routinely found and are fed back to model development teams generally under conditions of extreme time urgency. Corrected models form the essence of the completed transaction and firms undertaking model audit and review expose themselves to significant financial liability in the event of any remaining significant error. It is noteworthy that in the United Kingdom, the management of spreadsheet error is almost unheard of outside of the City of London despite the commercial ubiquity of the spreadsheet.",
        "published": "2007-12-16T20:40:57Z",
        "link": "http://arxiv.org/abs/0712.2591v1",
        "categories": [
            "cs.SE",
            "cs.CY",
            "J.1; H.4.1; K.6.4; D.2.9"
        ]
    },
    {
        "title": "Experiments with a Convex Polyhedral Analysis Tool for Logic Programs",
        "authors": [
            "Kim Henriksen",
            "Gourinath Banda",
            "John Gallagher"
        ],
        "summary": "Convex polyhedral abstractions of logic programs have been found very useful in deriving numeric relationships between program arguments in order to prove program properties and in other areas such as termination and complexity analysis. We present a tool for constructing polyhedral analyses of (constraint) logic programs. The aim of the tool is to make available, with a convenient interface, state-of-the-art techniques for polyhedral analysis such as delayed widening, narrowing, \"widening up-to\", and enhanced automatic selection of widening points. The tool is accessible on the web, permits user programs to be uploaded and analysed, and is integrated with related program transformations such as size abstractions and query-answer transformation. We then report some experiments using the tool, showing how it can be conveniently used to analyse transition systems arising from models of embedded systems, and an emulator for a PIC microcontroller which is used for example in wearable computing systems. We discuss issues including scalability, tradeoffs of precision and computation time, and other program transformations that can enhance the results of analysis.",
        "published": "2007-12-17T15:11:36Z",
        "link": "http://arxiv.org/abs/0712.2737v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Software (Re-)Engineering with PSF",
        "authors": [
            "Bob Diertens"
        ],
        "summary": "This paper investigates the usefulness of PSF in software engineering and reengineering. PSF is based on ACP (Algebra of Communicating Processes) and as some architectural description languages are based on process algebra, we investigate whether PSF can be used at the software architecture level, but we also use PSF at lower abstract levels. As a case study we reengineer the compiler from the Toolkit of PSF.",
        "published": "2007-12-18T12:25:02Z",
        "link": "http://arxiv.org/abs/0712.2943v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Optimizing Queries in a Logic-based Information Integration System",
        "authors": [
            "András Gyorgy Békés",
            "Péter Szeredi"
        ],
        "summary": "The SINTAGMA information integration system is an infrastructure for accessing several different information sources together. Besides providing a uniform interface to the information sources (databases, web services, web sites, RDF resources, XML files), semantic integration is also needed. Semantic integration is carried out by providing a high-level model and the mappings to the models of the sources. When executing a query of the high level model, a query is transformed to a low-level query plan, which is a piece of Prolog code that answers the high-level query. This transformation is done in two phases. First, the Query Planner produces a plan as a logic formula expressing the low-level query. Next, the Query Optimizer transforms this formula to executable Prolog code and optimizes it according to structural and statistical information about the information sources.   This article discusses the main ideas of the optimization algorithm and its implementation.",
        "published": "2007-12-19T08:07:30Z",
        "link": "http://arxiv.org/abs/0712.3113v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Software (Re-)Engineering with PSF II: from architecture to   implementation",
        "authors": [
            "Bob Diertens"
        ],
        "summary": "This paper presents ongoing research on the application of PSF in the field of software engineering and reengineering. We build a new implementation for the simulator of the PSF Toolkit starting from the specification in PSF of the architecture of a simple simulator and extend it with features to obtain the architecture of a full simulator. We apply refining and constraining techniques on the specification of the architecture to obtain a specification low enough to build an implementation from.",
        "published": "2007-12-19T08:11:37Z",
        "link": "http://arxiv.org/abs/0712.3115v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Proceedings of the 17th Workshop on Logic-based methods in Programming   Environments (WLPE 2007)",
        "authors": [
            "Patricia Hill",
            "Wim Vanhoof"
        ],
        "summary": "This volume contains the papers presented at WLPE 2007: the 17th Workshop on Logic-based Methods in Programming Environments on 13th September, 2007 in Porto, Portugal. It was held as a satellite workshop of ICLP 2007, the 23th International Conference on Logic Programming.",
        "published": "2007-12-19T08:28:12Z",
        "link": "http://arxiv.org/abs/0712.3116v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Software (Re-)Engineering with PSF III: an IDE for PSF",
        "authors": [
            "Bob Diertens"
        ],
        "summary": "We describe the design of an integrated development environment (IDE) for PSF. In the software engineering process we used process algebra in the form of PSF for the specification of the architecture of the IDE. This specification is refined to a PSF specification of the IDE system as a ToolBus application, by applying vertical and horizontal implementation techniques. We implemented the various tools as specified and connected them with a ToolBus script extracted from the system specification.",
        "published": "2007-12-19T09:46:23Z",
        "link": "http://arxiv.org/abs/0712.3128v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "QIS-XML: A metadata specification for Quantum Information Science",
        "authors": [
            "Pascal Heus",
            "Richard Gomez"
        ],
        "summary": "While Quantum Information Science (QIS) is still in its infancy, the ability for quantum based hardware or computers to communicate and integrate with their classical counterparts will be a major requirement towards their success. Little attention however has been paid to this aspect of QIS. To manage and exchange information between systems, today's classic Information Technology (IT) commonly uses the eXtensible Markup Language (XML) and its related tools. XML is composed of numerous specifications related to various fields of expertise. No such global specification however has been defined for quantum computers. QIS-XML is a proposed XML metadata specification for the description of fundamental components of QIS (gates & circuits) and a platform for the development of a hardware independent low level pseudo-code for quantum algorithms. This paper lays out the general characteristics of the QIS-XML specification and outlines practical applications through prototype use cases.",
        "published": "2007-12-23T15:24:35Z",
        "link": "http://arxiv.org/abs/0712.3925v1",
        "categories": [
            "cs.SE",
            "cs.DB",
            "quant-ph"
        ]
    },
    {
        "title": "The framework for simulation of dynamics of mechanical aggregates",
        "authors": [
            "Petr R. Ivankov",
            "Nikolay P. Ivankov"
        ],
        "summary": "A framework for simulation of dynamics of mechanical aggregates has been developed. This framework enables us to build model of aggregate from models of its parts. Framework is a part of universal framework for science and engineering.",
        "published": "2007-01-19T12:18:14Z",
        "link": "http://arxiv.org/abs/cs/0701119v1",
        "categories": [
            "cs.CE",
            "J.9"
        ]
    },
    {
        "title": "Supporting Finite Element Analysis with a Relational Database Backend,   Part I: There is Life beyond Files",
        "authors": [
            "Gerd Heber",
            "Jim Gray"
        ],
        "summary": "In this paper, we show how to use a Relational Database Management System in support of Finite Element Analysis. We believe it is a new way of thinking about data management in well-understood applications to prepare them for two major challenges, - size and integration (globalization). Neither extreme size nor integration (with other applications over the Web) was a design concern 30 years ago when the paradigm for FEA implementation first was formed. On the other hand, database technology has come a long way since its inception and it is past time to highlight its usefulness to the field of scientific computing and computer based engineering. This series aims to widen the list of applications for database designers and for FEA users and application developers to reap some of the benefits of database development.",
        "published": "2007-01-25T23:02:32Z",
        "link": "http://arxiv.org/abs/cs/0701159v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Supporting Finite Element Analysis with a Relational Database Backend,   Part II: Database Design and Access",
        "authors": [
            "Gerd Heber",
            "Jim Gray"
        ],
        "summary": "This is Part II of a three article series on using databases for Finite Element Analysis (FEA). It discusses (1) db design, (2) data loading, (3) typical use cases during grid building, (4) typical use cases during simulation (get and put), (5) typical use cases during analysis (also done in Part III) and some performance measures of these cases. It argues that using a database is simpler to implement than custom data schemas, has better performance because it can use data parallelism, and better supports FEA modularity and tool evolution because database schema evolution, data independence, and self-defining data.",
        "published": "2007-01-25T23:05:40Z",
        "link": "http://arxiv.org/abs/cs/0701160v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Using Table Valued Functions in SQL Server 2005 To Implement a Spatial   Data Library",
        "authors": [
            "Jim Gray",
            "Alex Szalay",
            "Gyorgy Fekete"
        ],
        "summary": "This article explains how to add spatial search functions (point-near-point and point in polygon) to Microsoft SQL Server 2005 using C# and table-valued functions. It is possible to use this library to add spatial search to your application without writing any special code. The library implements the public-domain C# Hierarchical Triangular Mesh (HTM) algorithms from Johns Hopkins University. That C# library is connected to SQL Server 2005 via a set of scalar-valued and table-valued functions. These functions act as a spatial index.",
        "published": "2007-01-26T00:00:37Z",
        "link": "http://arxiv.org/abs/cs/0701163v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Large-Scale Query and XMatch, Entering the Parallel Zone",
        "authors": [
            "Maria A. Nieto-Santisteban",
            "Aniruddha R. Thakar",
            "Alexander S. Szalay",
            "Jim Gray"
        ],
        "summary": "Current and future astronomical surveys are producing catalogs with millions and billions of objects. On-line access to such big datasets for data mining and cross-correlation is usually as highly desired as unfeasible. Providing these capabilities is becoming critical for the Virtual Observatory framework. In this paper we present various performance tests that show how using Relational Database Management Systems (RDBMS) and a Zoning algorithm to partition and parallelize the computation, we can facilitate large-scale query and cross-match.",
        "published": "2007-01-26T00:33:26Z",
        "link": "http://arxiv.org/abs/cs/0701167v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Life Under Your Feet: An End-to-End Soil Ecology Sensor Network,   Database, Web Server, and Analysis Service",
        "authors": [
            "Katalin Szlavecz",
            "Andreas Terzis",
            "Stuart Ozer",
            "Razvan Musaloiu-E",
            "Joshua Cogan",
            "Sam Small",
            "Randal Burns",
            "Jim Gray",
            "Alex Szalay"
        ],
        "summary": "Wireless sensor networks can revolutionize soil ecology by providing measurements at temporal and spatial granularities previously impossible. This paper presents a soil monitoring system we developed and deployed at an urban forest in Baltimore as a first step towards realizing this vision. Motes in this network measure and save soil moisture and temperature in situ every minute. Raw measurements are periodically retrieved by a sensor gateway and stored in a central database where calibrated versions are derived and stored. The measurement database is published through Web Services interfaces. In addition, analysis tools let scientists analyze current and historical data and help manage the sensor network. The article describes the system design, what we learned from the deployment, and initial results obtained from the sensors. The system measures soil factors with unprecedented temporal precision. However, the deployment required device-level programming, sensor calibration across space and time, and cross-referencing measurements with external sources. The database, web server, and data analysis design required considerable innovation and expertise. So, the ratio of computer-scientists to ecologists was 3:1. Before sensor networks can fulfill their potential as instruments that can be easily deployed by scientists, these technical problems must be addressed so that the ratio is one nerd per ten ecologists.",
        "published": "2007-01-26T05:08:06Z",
        "link": "http://arxiv.org/abs/cs/0701170v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Cross-Matching Multiple Spatial Observations and Dealing with Missing   Data",
        "authors": [
            "Jim Gray",
            "Alex Szalay",
            "Tamas Budavari",
            "Robert Lupton",
            "Maria Nieto-Santisteban",
            "Ani Thakar"
        ],
        "summary": "Cross-match spatially clusters and organizes several astronomical point-source measurements from one or more surveys. Ideally, each object would be found in each survey. Unfortunately, the observation conditions and the objects themselves change continually. Even some stationary objects are missing in some observations; sometimes objects have a variable light flux and sometimes the seeing is worse. In most cases we are faced with a substantial number of differences in object detections between surveys and between observations taken at different times within the same survey or instrument. Dealing with such missing observations is a difficult problem. The first step is to classify misses as ephemeral - when the object moved or simply disappeared, masked - when noise hid or corrupted the object observation, or edge - when the object was near the edge of the observational field. This classification and a spatial library to represent and manipulate observational footprints help construct a Match table recording both hits and misses. Transitive closure clusters friends-of-friends into object bundles. The bundle summary statistics are recorded in a Bundle table. This design is an evolution of the Sloan Digital Sky Survey cross-match design that compared overlapping observations taken at different times. Cross-Matching Multiple Spatial Observations and Dealing with Missing Data.",
        "published": "2007-01-26T05:18:45Z",
        "link": "http://arxiv.org/abs/cs/0701172v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "SkyServer Traffic Report - The First Five Years",
        "authors": [
            "Vik Singh",
            "Jim Gray",
            "Ani Thakar",
            "Alexander S. Szalay",
            "Jordan Raddick",
            "Bill Boroski",
            "Svetlana Lebedeva",
            "Brian Yanny"
        ],
        "summary": "The SkyServer is an Internet portal to the Sloan Digital Sky Survey Catalog Archive Server. From 2001 to 2006, there were a million visitors in 3 million sessions generating 170 million Web hits, 16 million ad-hoc SQL queries, and 62 million page views. The site currently averages 35 thousand visitors and 400 thousand sessions per month. The Web and SQL logs are public. We analyzed traffic and sessions by duration, usage pattern, data product, and client type (mortal or bot) over time. The analysis shows (1) the site's popularity, (2) the educational website that delivered nearly fifty thousand hours of interactive instruction, (3) the relative use of interactive, programmatic, and batch-local access, (4) the success of offering ad-hoc SQL, personal database, and batch job access to scientists as part of the data publication, (5) the continuing interest in \"old\" datasets, (6) the usage of SQL constructs, and (7) a novel approach of using the corpus of correct SQL queries to suggest similar but correct statements when a user presents an incorrect SQL statement.",
        "published": "2007-01-26T05:22:15Z",
        "link": "http://arxiv.org/abs/cs/0701173v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "A Direct Matrix Method for Computing Analytical Jacobians of Discretized   Nonlinear Integro-differential Equations",
        "authors": [
            "Kevin T. Chu"
        ],
        "summary": "In this pedagogical article, we present a simple direct matrix method for analytically computing the Jacobian of nonlinear algebraic equations that arise from the discretization of nonlinear integro-differential equations. The method is based on a formulation of the discretized equations in vector form using only matrix-vector products and component-wise operations. By applying simple matrix-based differentiation rules, the matrix form of the analytical Jacobian can be calculated with little more difficulty than that required when computing derivatives in single-variable calculus. After describing the direct matrix method, we present numerical experiments demonstrating the computational performance of the method, discuss its connection to the Newton-Kantorovich method, and apply it to illustrative 1D and 2D example problems. MATLAB code is provided to demonstrate the low code complexity required by the method.",
        "published": "2007-02-05T19:03:57Z",
        "link": "http://arxiv.org/abs/math/0702116v2",
        "categories": [
            "math.NA",
            "cs.CE",
            "65-01, 65L10, 65N22"
        ]
    },
    {
        "title": "AICA: a New Pair Force Evaluation Method for Parallel Molecular Dynamics   in Arbitrary Geometries",
        "authors": [
            "Graham B. Macpherson",
            "Jason M. Reese"
        ],
        "summary": "A new algorithm for calculating intermolecular pair forces in Molecular Dynamics (MD) simulations on a distributed parallel computer is presented. The Arbitrary Interacting Cells Algorithm (AICA) is designed to operate on geometrical domains defined by an unstructured, arbitrary polyhedral mesh, which has been spatially decomposed into irregular portions for parallelisation. It is intended for nano scale fluid mechanics simulation by MD in complex geometries, and to provide the MD component of a hybrid MD/continuum simulation. AICA has been implemented in the open-source computational toolbox OpenFOAM, and verified against a published MD code.",
        "published": "2007-02-22T17:27:05Z",
        "link": "http://arxiv.org/abs/cs/0702131v1",
        "categories": [
            "cs.CE",
            "cs.DC"
        ]
    },
    {
        "title": "Coupling Control and Human-Centered Automation in Mathematical Models of   Complex Systems",
        "authors": [
            "Roderick V. N. Melnik"
        ],
        "summary": "In this paper we analyze mathematically how human factors can be effectively incorporated into the analysis and control of complex systems. As an example, we focus our discussion around one of the key problems in the Intelligent Transportation Systems (ITS) theory and practice, the problem of speed control, considered here as a decision making process with limited information available. The problem is cast mathematically in the general framework of control problems and is treated in the context of dynamically changing environments where control is coupled to human-centered automation. Since in this case control might not be limited to a small number of control settings, as it is often assumed in the control literature, serious difficulties arise in the solution of this problem. We demonstrate that the problem can be reduced to a set of Hamilton-Jacobi-Bellman equations where human factors are incorporated via estimations of the system Hamiltonian. In the ITS context, these estimations can be obtained with the use of on-board equipment like sensors/receivers/actuators, in-vehicle communication devices, etc. The proposed methodology provides a way to integrate human factor into the solving process of the models for other complex dynamic systems.",
        "published": "2007-02-25T11:09:12Z",
        "link": "http://arxiv.org/abs/cs/0702149v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "cs.HC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Linking Microscopic and Macroscopic Models for Evolution: Markov Chain   Network Training and Conservation Law Approximations",
        "authors": [
            "Roderick V. N. Melnik"
        ],
        "summary": "In this paper, a general framework for the analysis of a connection between the training of artificial neural networks via the dynamics of Markov chains and the approximation of conservation law equations is proposed. This framework allows us to demonstrate an intrinsic link between microscopic and macroscopic models for evolution via the concept of perturbed generalized dynamic systems. The main result is exemplified with a number of illustrative examples where efficient numerical approximations follow directly from network-based computational models, viewed here as Markov chain approximations. Finally, stability and consistency conditions of such computational models are discussed.",
        "published": "2007-02-25T11:19:25Z",
        "link": "http://arxiv.org/abs/cs/0702148v1",
        "categories": [
            "cs.CE",
            "cs.IT",
            "cs.NA",
            "cs.NE",
            "math.IT"
        ]
    },
    {
        "title": "First Passage Time for Multivariate Jump-diffusion Stochastic Models   With Applications in Finance",
        "authors": [
            "Di Zhang",
            "Roderick V. N. Melnik"
        ],
        "summary": "The ``first passage-time'' (FPT) problem is an important problem with a wide range of applications in mathematics, physics, biology and finance. Mathematically, such a problem can be reduced to estimating the probability of a (stochastic) process first to reach a critical level or threshold. While in other areas of applications the FPT problem can often be solved analytically, in finance we usually have to resort to the application of numerical procedures, in particular when we deal with jump-diffusion stochastic processes (JDP). In this paper, we develop a Monte-Carlo-based methodology for the solution of the FPT problem in the context of a multivariate jump-diffusion stochastic process. The developed methodology is tested by using different parameters, the simulation results indicate that the developed methodology is much more efficient than the conventional Monte Carlo method. It is an efficient tool for further practical applications, such as the analysis of default correlation and predicting barrier options in finance.",
        "published": "2007-02-28T10:39:15Z",
        "link": "http://arxiv.org/abs/cs/0702163v1",
        "categories": [
            "cs.CE",
            "cs.NA"
        ]
    },
    {
        "title": "Monte-Carlo Simulations of the First Passage Time for Multivariate   Jump-Diffusion Processes in Financial Applications",
        "authors": [
            "Di Zhang",
            "Roderick V. N. Melnik"
        ],
        "summary": "Many problems in finance require the information on the first passage time (FPT) of a stochastic process. Mathematically, such problems are often reduced to the evaluation of the probability density of the time for such a process to cross a certain level, a boundary, or to enter a certain region. While in other areas of applications the FPT problem can often be solved analytically, in finance we usually have to resort to the application of numerical procedures, in particular when we deal with jump-diffusion stochastic processes (JDP). In this paper, we propose a Monte-Carlo-based methodology for the solution of the first passage time problem in the context of multivariate (and correlated) jump-diffusion processes. The developed technique provide an efficient tool for a number of applications, including credit risk and option pricing. We demonstrate its applicability to the analysis of the default rates and default correlations of several different, but correlated firms via a set of empirical data.",
        "published": "2007-02-28T10:51:16Z",
        "link": "http://arxiv.org/abs/cs/0702164v1",
        "categories": [
            "cs.CE",
            "cs.NA"
        ]
    },
    {
        "title": "Efficient estimation of default correlation for multivariate   jump-diffusion processes",
        "authors": [
            "Di Zhang",
            "Roderick V. N. Melnik"
        ],
        "summary": "Evaluation of default correlation is an important task in credit risk analysis. In many practical situations, it concerns the joint defaults of several correlated firms, the task that is reducible to a first passage time (FPT) problem. This task represents a great challenge for jump-diffusion processes (JDP), where except for very basic cases, there are no analytical solutions for such problems. In this contribution, we generalize our previous fast Monte-Carlo method (non-correlated jump-diffusion cases) for multivariate (and correlated) jump-diffusion processes. This generalization allows us, among other things, to evaluate the default events of several correlated assets based on a set of empirical data. The developed technique is an efficient tool for a number of other applications, including credit risk and option pricing.",
        "published": "2007-02-28T11:22:18Z",
        "link": "http://arxiv.org/abs/cs/0702165v1",
        "categories": [
            "cs.CE",
            "cs.NA"
        ]
    },
    {
        "title": "Solving Stochastic Differential Equations with Jump-Diffusion   Efficiently: Applications to FPT Problems in Credit Risk",
        "authors": [
            "Di Zhang",
            "Roderick V. N. Melnik"
        ],
        "summary": "The first passage time (FPT) problem is ubiquitous in many applications. In finance, we often have to deal with stochastic processes with jump-diffusion, so that the FTP problem is reducible to a stochastic differential equation with jump-diffusion. While the application of the conventional Monte-Carlo procedure is possible for the solution of the resulting model, it becomes computationally inefficient which severely restricts its applicability in many practically interesting cases. In this contribution, we focus on the development of efficient Monte-Carlo-based computational procedures for solving the FPT problem under the multivariate (and correlated) jump-diffusion processes. We also discuss the implementation of the developed Monte-Carlo-based technique for multivariate jump-diffusion processes driving by several compound Poisson shocks. Finally, we demonstrate the application of the developed methodologies for analyzing the default rates and default correlations of differently rated firms via historical data.",
        "published": "2007-02-28T11:48:12Z",
        "link": "http://arxiv.org/abs/cs/0702166v1",
        "categories": [
            "cs.CE",
            "cs.NA"
        ]
    },
    {
        "title": "Finite Volume Analysis of Nonlinear Thermo-mechanical Dynamics of Shape   Memory Alloys",
        "authors": [
            "Linxiang X. Wang",
            "Roderick V. N. Melnik"
        ],
        "summary": "In this paper, the finite volume method is developed to analyze coupled dynamic problems of nonlinear thermoelasticity. The major focus is given to the description of martensitic phase transformations essential in the modelling of shape memory alloys. Computational experiments are carried out to study the thermo-mechanical wave interactions in a shape memory alloy rod, and a patch. Both mechanically and thermally induced phase transformations, as well as hysteresis effects, in a one-dimensional structure are successfully simulated with the developed methodology. In the two-dimensional case, the main focus is given to square-to-rectangular transformations and examples of martensitic combinations under different mechanical loadings are provided.",
        "published": "2007-02-28T13:00:33Z",
        "link": "http://arxiv.org/abs/cs/0702167v1",
        "categories": [
            "cs.CE",
            "cs.NA"
        ]
    },
    {
        "title": "Simulation of Phase Combinations in Shape Memory Alloys Patches by   Hybrid Optimization Methods",
        "authors": [
            "Linxiang X. Wang",
            "Roderick V. N. Melnik"
        ],
        "summary": "In this paper, phase combinations among martensitic variants in shape memory alloys patches and bars are simulated by a hybrid optimization methodology. The mathematical model is based on the Landau theory of phase transformations. Each stable phase is associated with a local minimum of the free energy function, and the phase combinations are simulated by minimizing the bulk energy. At low temperature, the free energy function has double potential wells leading to non-convexity of the optimization problem. The methodology proposed in the present paper is based on an initial estimate of the global solution by a genetic algorithm, followed by a refined quasi-Newton procedure to locally refine the optimum. By combining the local and global search algorithms, the phase combinations are successfully simulated. Numerical experiments are presented for the phase combinations in a SMA patch under several typical mechanical loadings.",
        "published": "2007-02-28T14:09:42Z",
        "link": "http://arxiv.org/abs/cs/0702168v1",
        "categories": [
            "cs.CE",
            "cs.NA"
        ]
    },
    {
        "title": "Numerical Model For Vibration Damping Resulting From the First Order   Phase Transformations",
        "authors": [
            "Linxiang X. Wang",
            "Roderick V. N. Melnik"
        ],
        "summary": "A numerical model is constructed for modelling macroscale damping effects induced by the first order martensite phase transformations in a shape memory alloy rod. The model is constructed on the basis of the modified Landau-Ginzburg theory that couples nonlinear mechanical and thermal fields. The free energy function for the model is constructed as a double well function at low temperature, such that the external energy can be absorbed during the phase transformation and converted into thermal form. The Chebyshev spectral methods are employed together with backward differentiation for the numerical analysis of the problem. Computational experiments performed for different vibration energies demonstrate the importance of taking into account damping effects induced by phase transformations.",
        "published": "2007-02-28T18:31:19Z",
        "link": "http://arxiv.org/abs/cs/0702172v1",
        "categories": [
            "cs.CE",
            "cs.NA"
        ]
    },
    {
        "title": "Error Correction and Digitalization Concepts in Biochemical Computing",
        "authors": [
            "L. Fedichkin",
            "E. Katz",
            "V. Privman"
        ],
        "summary": "We offer a theoretical design of new systems that show promise for digital biochemical computing, including realizations of error correction by utilizing redundancy, as well as signal rectification. The approach includes information processing using encoded DNA sequences, DNAzyme biocatalyzed reactions and the use of DNA-functionalized magnetic nanoparticles. Digital XOR and NAND logic gates and copying (fanout) are designed using the same components.",
        "published": "2007-03-13T20:02:57Z",
        "link": "http://arxiv.org/abs/cond-mat/0703351v2",
        "categories": [
            "cond-mat.soft",
            "cond-mat.dis-nn",
            "cond-mat.mtrl-sci",
            "cs.CE",
            "q-bio.BM",
            "quant-ph"
        ]
    },
    {
        "title": "Option Valuation using Fourier Space Time Stepping",
        "authors": [
            "Kenneth R. Jackson",
            "Sebastian Jaimungal",
            "Vladimir Surkov"
        ],
        "summary": "It is well known that the Black-Scholes-Merton model suffers from several deficiencies. Jump-diffusion and Levy models have been widely used to partially alleviate some of the biases inherent in this classical model. Unfortunately, the resulting pricing problem requires solving a more difficult partial-integro differential equation (PIDE) and although several approaches for solving the PIDE have been suggested in the literature, none are entirely satisfactory. All treat the integral and diffusive terms asymmetrically and are difficult to extend to higher dimensions. We present a new, efficient algorithm, based on transform methods, which symmetrically treats the diffusive and integrals terms, is applicable to a wide class of path-dependent options (such as Bermudan, barrier, and shout options) and options on multiple assets, and naturally extends to regime-switching Levy models. We present a concise study of the precision and convergence properties of our algorithm for several classes of options and Levy models and demonstrate that the algorithm is second-order in space and first-order in time for path-dependent options.",
        "published": "2007-03-14T19:48:42Z",
        "link": "http://arxiv.org/abs/cs/0703068v2",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Real Options for Project Schedules (ROPS)",
        "authors": [
            "Lester Ingber"
        ],
        "summary": "Real Options for Project Schedules (ROPS) has three recursive sampling/optimization shells. An outer Adaptive Simulated Annealing (ASA) optimization shell optimizes parameters of strategic Plans containing multiple Projects containing ordered Tasks. A middle shell samples probability distributions of durations of Tasks. An inner shell samples probability distributions of costs of Tasks. PATHTREE is used to develop options on schedules.. Algorithms used for Trading in Risk Dimensions (TRD) are applied to develop a relative risk analysis among projects.",
        "published": "2007-04-01T14:35:40Z",
        "link": "http://arxiv.org/abs/0704.0090v1",
        "categories": [
            "cs.CE",
            "cond-mat.stat-mech",
            "cs.MS",
            "cs.NA",
            "physics.data-an",
            "C.4; G.1; G.1.6; G.3; J.7"
        ]
    },
    {
        "title": "Assessment and Propagation of Input Uncertainty in Tree-based Option   Pricing Models",
        "authors": [
            "Henryk Gzyl",
            "German Molina",
            "Enrique ter Horst"
        ],
        "summary": "This paper aims to provide a practical example on the assessment and propagation of input uncertainty for option pricing when using tree-based methods. Input uncertainty is propagated into output uncertainty, reflecting that option prices are as unknown as the inputs they are based on. Option pricing formulas are tools whose validity is conditional not only on how close the model represents reality, but also on the quality of the inputs they use, and those inputs are usually not observable. We provide three alternative frameworks to calibrate option pricing tree models, propagating parameter uncertainty into the resulting option prices. We finally compare our methods with classical calibration-based results assuming that there is no options market established. These methods can be applied to pricing of instruments for which there is not an options market, as well as a methodological tool to account for parameter and model uncertainty in theoretical option pricing.",
        "published": "2007-04-13T14:48:41Z",
        "link": "http://arxiv.org/abs/0704.1768v1",
        "categories": [
            "cs.CE",
            "cs.GT",
            "B.5.2"
        ]
    },
    {
        "title": "Comparison of Discrete and Continuous Wavelet Transforms",
        "authors": [
            "Palle E. T. Jorgensen",
            "Myung-Sin Song"
        ],
        "summary": "In this paper we outline several points of view on the interplay between discrete and continuous wavelet transforms; stressing both pure and applied aspects of both. We outline some new links between the two transform technologies based on the theory of representations of generators and relations. By this we mean a finite system of generators which are represented by operators in Hilbert space. We further outline how these representations yield sub-band filter banks for signal and image processing algorithms.",
        "published": "2007-05-01T18:24:52Z",
        "link": "http://arxiv.org/abs/0705.0150v2",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Optimal Cache-Oblivious Mesh Layouts",
        "authors": [
            "Michael A. Bender",
            "Bradley C. Kuszmaul",
            "Shang-Hua Teng",
            "Kebin Wang"
        ],
        "summary": "A mesh is a graph that divides physical space into regularly-shaped regions. Meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. In one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. The performance of a mesh update depends on the layout of the mesh in memory.   This paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. Such a memory layout is called cache-oblivious. Formally, for a $d$-dimensional mesh $G$, block size $B$, and cache size $M$ (where $M=\\Omega(B^d)$), the mesh update of $G$ uses $O(1+|G|/B)$ memory transfers. The paper also shows how the mesh-update performance degrades for smaller caches, where $M=o(B^d)$.   The paper then gives two algorithms for finding cache-oblivious mesh layouts. The first layout algorithm runs in time $O(|G|\\log^2|G|)$ both in expectation and with high probability on a RAM. It uses $O(1+|G|\\log^2(|G|/M)/B)$ memory transfers in expectation and $O(1+(|G|/B)(\\log^2(|G|/M) + \\log|G|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (DAM) models. The layout is obtained by finding a fully balanced decomposition tree of $G$ and then performing an in-order traversal of the leaves of the tree. The second algorithm runs faster by almost a $\\log|G|/\\log\\log|G|$ factor in all three memory models, both in expectation and with high probability. The layout obtained by finding a relax-balanced decomposition tree of $G$ and then performing an in-order traversal of the leaves of the tree.",
        "published": "2007-05-08T05:59:55Z",
        "link": "http://arxiv.org/abs/0705.1033v2",
        "categories": [
            "cs.DS",
            "cs.CE",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "Control of Complex Systems Using Bayesian Networks and Genetic Algorithm",
        "authors": [
            "Tshilidzi Marwala"
        ],
        "summary": "A method based on Bayesian neural networks and genetic algorithm is proposed to control the fermentation process. The relationship between input and output variables is modelled using Bayesian neural network that is trained using hybrid Monte Carlo method. A feedback loop based on genetic algorithm is used to change input variables so that the output variables are as close to the desired target as possible without the loss of confidence level on the prediction that the neural network gives. The proposed procedure is found to reduce the distance between the desired target and measured outputs significantly.",
        "published": "2007-05-09T07:08:58Z",
        "link": "http://arxiv.org/abs/0705.1214v1",
        "categories": [
            "cs.CE",
            "cs.NE"
        ]
    },
    {
        "title": "Machine and Component Residual Life Estimation through the Application   of Neural Networks",
        "authors": [
            "M. A. Herzog",
            "T. Marwala",
            "P. S. Heyns"
        ],
        "summary": "This paper concerns the use of neural networks for predicting the residual life of machines and components. In addition, the advantage of using condition-monitoring data to enhance the predictive capability of these neural networks was also investigated. A number of neural network variations were trained and tested with the data of two different reliability-related datasets. The first dataset represents the renewal case where the failed unit is repaired and restored to a good-as-new condition. Data was collected in the laboratory by subjecting a series of similar test pieces to fatigue loading with a hydraulic actuator. The average prediction error of the various neural networks being compared varied from 431 to 841 seconds on this dataset, where test pieces had a characteristic life of 8,971 seconds. The second dataset was collected from a group of pumps used to circulate a water and magnetite solution within a plant. The data therefore originated from a repaired system affected by reliability degradation. When optimized, the multi-layer perceptron neural networks trained with the Levenberg-Marquardt algorithm and the general regression neural network produced a sum-of-squares error within 11.1% of each other. The potential for using neural networks for residual life prediction and the advantage of incorporating condition-based data into the model were proven for both examples.",
        "published": "2007-05-10T05:52:22Z",
        "link": "http://arxiv.org/abs/0705.1390v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Principal Component Analysis and Automatic Relevance Determination in   Damage Identification",
        "authors": [
            "L. Mdlazi",
            "T. Marwala",
            "C. J. Stander",
            "C. Scheffer",
            "P. S. Heyns"
        ],
        "summary": "This paper compares two neural network input selection schemes, the Principal Component Analysis (PCA) and the Automatic Relevance Determination (ARD) based on Mac-Kay's evidence framework. The PCA takes all the input data and projects it onto a lower dimension space, thereby reduc-ing the dimension of the input space. This input reduction method often results with parameters that have significant influence on the dynamics of the data being diluted by those that do not influence the dynamics of the data. The ARD selects the most relevant input parameters and discards those that do not contribute significantly to the dynamics of the data being modelled. The ARD sometimes results with important input parameters being discarded thereby compromising the dynamics of the data. The PCA and ARD methods are implemented together with a Multi-Layer-Perceptron (MLP) network for fault identification in structures and the performance of the two methods is as-sessed. It is observed that ARD and PCA give similar accu-racy levels when used as input-selection schemes. There-fore, the choice of input-selection scheme is dependent on the nature of the data being processed.",
        "published": "2007-05-11T15:35:22Z",
        "link": "http://arxiv.org/abs/0705.1672v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Using artificial intelligence for data reduction in mechanical   engineering",
        "authors": [
            "L. Mdlazi",
            "C. J. Stander",
            "P. S. Heyns",
            "T. Marwala"
        ],
        "summary": "In this paper artificial neural networks and support vector machines are used to reduce the amount of vibration data that is required to estimate the Time Domain Average of a gear vibration signal. Two models for estimating the time domain average of a gear vibration signal are proposed. The models are tested on data from an accelerated gear life test rig. Experimental results indicate that the required data for calculating the Time Domain Average of a gear vibration signal can be reduced by up to 75% when the proposed models are implemented.",
        "published": "2007-05-11T15:49:40Z",
        "link": "http://arxiv.org/abs/0705.1673v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Evolutionary Optimisation Methods for Template Based Image Registration",
        "authors": [
            "Lukasz A Machowski",
            "Tshilidzi Marwala"
        ],
        "summary": "This paper investigates the use of evolutionary optimisation techniques to register a template with a scene image. An error function is created to measure the correspondence of the template to the image. The problem presented here is to optimise the horizontal, vertical and scaling parameters that register the template with the scene. The Genetic Algorithm, Simulated Annealing and Particle Swarm Optimisations are compared to a Nelder-Mead Simplex optimisation with starting points chosen in a pre-processing stage. The paper investigates the precision and accuracy of each method and shows that all four methods perform favourably for image registration. SA is the most precise, GA is the most accurate. PSO is a good mix of both and the Simplex method returns local minima the most. A pre-processing stage should be investigated for the evolutionary methods in order to improve performance. Discrete versions of the optimisation methods should be investigated to further improve computational performance.",
        "published": "2007-05-11T15:51:36Z",
        "link": "http://arxiv.org/abs/0705.1674v1",
        "categories": [
            "cs.CE",
            "cs.CV"
        ]
    },
    {
        "title": "Option Pricing Using Bayesian Neural Networks",
        "authors": [
            "Michael Maio Pires",
            "Tshilidzi Marwala"
        ],
        "summary": "Options have provided a field of much study because of the complexity involved in pricing them. The Black-Scholes equations were developed to price options but they are only valid for European styled options. There is added complexity when trying to price American styled options and this is why the use of neural networks has been proposed. Neural Networks are able to predict outcomes based on past data. The inputs to the networks here are stock volatility, strike price and time to maturity with the output of the network being the call option price. There are two techniques for Bayesian neural networks used. One is Automatic Relevance Determination (for Gaussian Approximation) and one is a Hybrid Monte Carlo method, both used with Multi-Layer Perceptrons.",
        "published": "2007-05-11T15:55:31Z",
        "link": "http://arxiv.org/abs/0705.1680v1",
        "categories": [
            "cs.CE",
            "cs.NE"
        ]
    },
    {
        "title": "Finite Element Model Updating Using Response Surface Method",
        "authors": [
            "Tshilidzi Marwala"
        ],
        "summary": "This paper proposes the response surface method for finite element model updating. The response surface method is implemented by approximating the finite element model surface response equation by a multi-layer perceptron. The updated parameters of the finite element model were calculated using genetic algorithm by optimizing the surface response equation. The proposed method was compared to the existing methods that use simulated annealing or genetic algorithm together with a full finite element model for finite element model updating. The proposed method was tested on an unsymmetri-cal H-shaped structure. It was observed that the proposed method gave the updated natural frequen-cies and mode shapes that were of the same order of accuracy as those given by simulated annealing and genetic algorithm. Furthermore, it was observed that the response surface method achieved these results at a computational speed that was more than 2.5 times as fast as the genetic algorithm and a full finite element model and 24 times faster than the simulated annealing.",
        "published": "2007-05-12T10:25:22Z",
        "link": "http://arxiv.org/abs/0705.1759v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Dynamic Model Updating Using Particle Swarm Optimization Method",
        "authors": [
            "Tshilidzi Marwala"
        ],
        "summary": "This paper proposes the use of particle swarm optimization method (PSO) for finite element (FE) model updating. The PSO method is compared to the existing methods that use simulated annealing (SA) or genetic algorithms (GA) for FE model for model updating. The proposed method is tested on an unsymmetrical H-shaped structure. It is observed that the proposed method gives updated natural frequencies the most accurate and followed by those given by an updated model that was obtained using the GA and a full FE model. It is also observed that the proposed method gives updated mode shapes that are best correlated to the measured ones, followed by those given by an updated model that was obtained using the SA and a full FE model. Furthermore, it is observed that the PSO achieves this accuracy at a computational speed that is faster than that by the GA and a full FE model which is faster than the SA and a full FE model.",
        "published": "2007-05-12T10:27:07Z",
        "link": "http://arxiv.org/abs/0705.1760v1",
        "categories": [
            "cs.CE",
            "cs.NE"
        ]
    },
    {
        "title": "Computational Intelligence for Condition Monitoring",
        "authors": [
            "Tshilidzi Marwala",
            "Christina Busisiwe Vilakazi"
        ],
        "summary": "Condition monitoring techniques are described in this chapter. Two aspects of condition monitoring process are considered: (1) feature extraction; and (2) condition classification. Feature extraction methods described and implemented are fractals, Kurtosis and Mel-frequency Cepstral Coefficients. Classification methods described and implemented are support vector machines (SVM), hidden Markov models (HMM), Gaussian mixture models (GMM) and extension neural networks (ENN). The effectiveness of these features were tested using SVM, HMM, GMM and ENN on condition monitoring of bearings and are found to give good results.",
        "published": "2007-05-17T21:20:58Z",
        "link": "http://arxiv.org/abs/0705.2604v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Inferring the Composition of a Trader Population in a Financial Market",
        "authors": [
            "Nachi Gupta",
            "Raphael Hauser",
            "Neil F. Johnson"
        ],
        "summary": "We discuss a method for predicting financial movements and finding pockets of predictability in the price-series, which is built around inferring the heterogeneity of trading strategies in a multi-agent trader population. This work explores extensions to our previous framework (arXiv:physics/0506134). Here we allow for more intelligent agents possessing a richer strategy set, and we no longer constrain the estimate for the heterogeneity of the agents to a probability space. We also introduce a scheme which allows the incorporation of models with a wide variety of agent types, and discuss a mechanism for the removal of bias from relevant parameters.",
        "published": "2007-06-06T17:29:42Z",
        "link": "http://arxiv.org/abs/0706.0870v1",
        "categories": [
            "cs.CE",
            "nlin.AO"
        ]
    },
    {
        "title": "Cointegration of the Daily Electric Power System Load and the Weather",
        "authors": [
            "Stefan Z. Stefanov"
        ],
        "summary": "The paper makes a thermal predictive analysis of the electric power system security for a day ahead. This predictive analysis is set as a thermal computation of the expected security. This computation is obtained by cointegrating the daily electric power systen load and the weather, by finding the daily electric power system thermodynamics and by introducing tests for this thermodynamics. The predictive analysis made shows the electricity consumers' wisdom.",
        "published": "2007-06-08T07:04:24Z",
        "link": "http://arxiv.org/abs/0706.1119v2",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Pricing American Options for Jump Diffusions by Iterating Optimal   Stopping Problems for Diffusions",
        "authors": [
            "Erhan Bayraktar",
            "Hao Xing"
        ],
        "summary": "We approximate the price of the American put for jump diffusions by a sequence of functions, which are computed iteratively. This sequence converges to the price function uniformly and exponentially fast. Each element of the approximating sequence solves an optimal stopping problem for geometric Brownian motion, and can be numerically computed using the classical finite difference methods. We prove the convergence of this numerical scheme and present examples to illustrate its performance.",
        "published": "2007-06-15T16:43:14Z",
        "link": "http://arxiv.org/abs/0706.2331v5",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "N-Body Simulations on GPUs",
        "authors": [
            "Erich Elsen",
            "V. Vishal",
            "Mike Houston",
            "Vijay Pande",
            "Pat Hanrahan",
            "Eric Darve"
        ],
        "summary": "Commercial graphics processors (GPUs) have high compute capacity at very low cost, which makes them attractive for general purpose scientific computing. In this paper we show how graphics processors can be used for N-body simulations to obtain improvements in performance over current generation CPUs. We have developed a highly optimized algorithm for performing the O(N^2) force calculations that constitute the major part of stellar and molecular dynamics simulations. In some of the calculations, we achieve sustained performance of nearly 100 GFlops on an ATI X1900XTX. The performance on GPUs is comparable to specialized processors such as GRAPE-6A and MDGRAPE-3, but at a fraction of the cost. Furthermore, the wide availability of GPUs has significant implications for cluster computing and distributed computing efforts like Folding@Home.",
        "published": "2007-06-20T21:02:14Z",
        "link": "http://arxiv.org/abs/0706.3060v1",
        "categories": [
            "cs.CE",
            "cs.DC"
        ]
    },
    {
        "title": "Location and Spectral Estimation of Weak Wave Packets on Noise   Background",
        "authors": [
            "Yu. Bunyak",
            "O. Bunyak"
        ],
        "summary": "The method of location and spectral estimation of weak signals on a noise background is being considered. The method is based on the optimized on order and noise dispersion autoregressive model of a sought signal. A new approach of model order determination is being offered. Available estimation of the noise dispersion is close to the real one. The optimized model allows to define function of empirical data spectral and dynamic features changes. The analysis of the signal as dynamic invariant in respect of the linear shift transformation yields the function of model consistency. Use of these both functions enables to detect short-time and nonstationary wave packets at signal to noise ratio as from -20 dB and above.",
        "published": "2007-07-02T09:47:33Z",
        "link": "http://arxiv.org/abs/0707.0181v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Pricing Options on Defaultable Stocks",
        "authors": [
            "Erhan Bayraktar"
        ],
        "summary": "In this note, we develop stock option price approximations for a model which takes both the risk o default and the stochastic volatility into account. We also let the intensity of defaults be influenced by the volatility. We show that it might be possible to infer the risk neutral default intensity from the stock option prices. Our option price approximation has a rich implied volatility surface structure and fits the data implied volatility well. Our calibration exercise shows that an effective hazard rate from bonds issued by a company can be used to explain the implied volatility skew of the implied volatility of the option prices issued by the same company.",
        "published": "2007-07-03T03:28:35Z",
        "link": "http://arxiv.org/abs/0707.0336v2",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the   Astrobiology Phone-cam",
        "authors": [
            "Alexandra Bartolo",
            "Patrick C. McGuire",
            "Kenneth P. Camilleri",
            "Christopher Spiteri",
            "Jonathan C. Borg",
            "Philip J. Farrugia",
            "Jens Ormo",
            "Javier Gomez-Elvira",
            "Jose Antonio Rodriguez-Manfredi",
            "Enrique Diaz-Martinez",
            "Helge Ritter",
            "Robert Haschke",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "We have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. This camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. We envision that the `Astrobiology Phone-cam' exploration system can be fruitfully used in other problem domains as well.",
        "published": "2007-07-05T15:19:37Z",
        "link": "http://arxiv.org/abs/0707.0808v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.NI",
            "cs.RO",
            "cs.SE"
        ]
    },
    {
        "title": "Tripartitions do not always discriminate phylogenetic networks",
        "authors": [
            "Gabriel Cardona",
            "Francesc Rossello",
            "Gabriel Valiente"
        ],
        "summary": "Phylogenetic networks are a generalization of phylogenetic trees that allow for the representation of non-treelike evolutionary events, like recombination, hybridization, or lateral gene transfer. In a recent series of papers devoted to the study of reconstructibility of phylogenetic networks, Moret, Nakhleh, Warnow and collaborators introduced the so-called {tripartition metric for phylogenetic networks. In this paper we show that, in fact, this tripartition metric does not satisfy the separation axiom of distances (zero distance means isomorphism, or, in a more relaxed version, zero distance means indistinguishability in some specific sense) in any of the subclasses of phylogenetic networks where it is claimed to do so. We also present a subclass of phylogenetic networks whose members can be singled out by means of their sets of tripartitions (or even clusters), and hence where the latter can be used to define a meaningful metric.",
        "published": "2007-07-16T19:59:42Z",
        "link": "http://arxiv.org/abs/0707.2376v1",
        "categories": [
            "q-bio.PE",
            "cs.CE",
            "cs.DM"
        ]
    },
    {
        "title": "Pricing Asian Options for Jump Diffusions",
        "authors": [
            "Erhan Bayraktar",
            "Hao Xing"
        ],
        "summary": "We construct a sequence of functions that uniformly converge (on compact sets) to the price of Asian option, which is written on a stock whose dynamics follows a jump diffusion, exponentially fast. Each of the element in this sequence solves a parabolic partial differen- tial equation (not an integro-differential equation). As a result we obtain a fast numerical approximation scheme whose accuracy versus speed characteristics can be controlled. We analyze the performance of our numerical algorithm on several examples.",
        "published": "2007-07-17T04:55:18Z",
        "link": "http://arxiv.org/abs/0707.2432v7",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Autonomous tools for Grid management, monitoring and optimization",
        "authors": [
            "Wojciech Wislicki"
        ],
        "summary": "We outline design and lines of development of autonomous tools for the computing Grid management, monitoring and optimization. The management is proposed to be based on the notion of utility. Grid optimization is considered to be application-oriented. A generic Grid simulator is proposed as an optimization tool for Grid structure and functionality.",
        "published": "2007-07-22T14:02:21Z",
        "link": "http://arxiv.org/abs/0707.3263v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "hep-ex"
        ]
    },
    {
        "title": "Faster exon assembly by sparse spliced alignment",
        "authors": [
            "Alexander Tiskin"
        ],
        "summary": "Assembling a gene from candidate exons is an important problem in computational biology. Among the most successful approaches to this problem is \\emph{spliced alignment}, proposed by Gelfand et al., which scores different candidate exon chains within a DNA sequence of length $m$ by comparing them to a known related gene sequence of length n, $m = \\Theta(n)$. Gelfand et al.\\ gave an algorithm for spliced alignment running in time O(n^3). Kent et al.\\ considered sparse spliced alignment, where the number of candidate exons is O(n), and proposed an algorithm for this problem running in time O(n^{2.5}). We improve on this result, by proposing an algorithm for sparse spliced alignment running in time O(n^{2.25}). Our approach is based on a new framework of \\emph{quasi-local string comparison}.",
        "published": "2007-07-23T16:35:54Z",
        "link": "http://arxiv.org/abs/0707.3409v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.CE",
            "q-bio.QM"
        ]
    },
    {
        "title": "A Bayesian Framework for Combining Valuation Estimates",
        "authors": [
            "Kenton K. Yee"
        ],
        "summary": "Obtaining more accurate equity value estimates is the starting point for stock selection, value-based indexing in a noisy market, and beating benchmark indices through tactical style rotation. Unfortunately, discounted cash flow, method of comparables, and fundamental analysis typically yield discrepant valuation estimates. Moreover, the valuation estimates typically disagree with market price. Can one form a superior valuation estimate by averaging over the individual estimates, including market price? This article suggests a Bayesian framework for combining two or more estimates into a superior valuation estimate. The framework justifies the common practice of averaging over several estimates to arrive at a final point estimate.",
        "published": "2007-07-24T05:04:53Z",
        "link": "http://arxiv.org/abs/0707.3482v1",
        "categories": [
            "q-fin.ST",
            "cs.CE",
            "nlin.AO",
            "nlin.CD",
            "nlin.SI",
            "physics.pop-ph",
            "physics.soc-ph",
            "stat.AP"
        ]
    },
    {
        "title": "e-Science initiatives in Venezuela",
        "authors": [
            "J. L. Chaves",
            "G. Diaz",
            "V. Hamar",
            "R. Isea",
            "F. Rojas",
            "N. Ruiz",
            "R. Torrens",
            "M. Uzcategui",
            "J. Florez-Lopez",
            "H. Hoeger",
            "C. Mendoza",
            "L. A. Nunez"
        ],
        "summary": "Within the context of the nascent e-Science infrastructure in Venezuela, we describe several web-based scientific applications developed at the Centro Nacional de Calculo Cientifico Universidad de Los Andes (CeCalCULA), Merida, and at the Instituto Venezolano de Investigaciones Cientificas (IVIC), Caracas. The different strategies that have been followed for implementing quantum chemistry and atomic physics applications are presented. We also briefly discuss a damage portal based on dynamic, nonlinear, finite elements of lumped damage mechanics and a biomedical portal developed within the framework of the \\textit{E-Infrastructure shared between Europe and Latin America} (EELA) initiative for searching common sequences and inferring their functions in parasitic diseases such as leishmaniasis, chagas and malaria.",
        "published": "2007-07-24T12:00:43Z",
        "link": "http://arxiv.org/abs/0707.3531v1",
        "categories": [
            "cs.CE",
            "cs.DC"
        ]
    },
    {
        "title": "The Local Fractal Properties of the Financial Time Series on the Polish   Stock Exchange Market",
        "authors": [
            "D. Grech",
            "G. Pamuła"
        ],
        "summary": "We investigate the local fractal properties of the financial time series based on the evolution of the Warsaw Stock Exchange Index (WIG) connected with the largest developing financial market in Europe. Calculating the local Hurst exponent for the WIG time series we find an interesting dependence between the behavior of the local fractal properties of the WIG time series and the crashes appearance on the financial market.",
        "published": "2007-08-02T14:22:30Z",
        "link": "http://arxiv.org/abs/0708.0353v1",
        "categories": [
            "q-fin.ST",
            "cs.CE",
            "physics.data-an"
        ]
    },
    {
        "title": "A preliminary analysis on metaheuristics methods applied to the   Haplotype Inference Problem",
        "authors": [
            "Luca Di Gaspero",
            "Andrea Roli"
        ],
        "summary": "Haplotype Inference is a challenging problem in bioinformatics that consists in inferring the basic genetic constitution of diploid organisms on the basis of their genotype. This information allows researchers to perform association studies for the genetic variants involved in diseases and the individual responses to therapeutic agents.   A notable approach to the problem is to encode it as a combinatorial problem (under certain hypotheses, such as the pure parsimony criterion) and to solve it using off-the-shelf combinatorial optimization techniques. The main methods applied to Haplotype Inference are either simple greedy heuristic or exact methods (Integer Linear Programming, Semidefinite Programming, SAT encoding) that, at present, are adequate only for moderate size instances.   We believe that metaheuristic and hybrid approaches could provide a better scalability. Moreover, metaheuristics can be very easily combined with problem specific heuristics and they can also be integrated with tree-based search techniques, thus providing a promising framework for hybrid systems in which a good trade-off between effectiveness and efficiency can be reached.   In this paper we illustrate a feasibility study of the approach and discuss some relevant design issues, such as modeling and design of approximate solvers that combine constructive heuristics, local search-based improvement strategies and learning mechanisms. Besides the relevance of the Haplotype Inference problem itself, this preliminary analysis is also an interesting case study because the formulation of the problem poses some challenges in modeling and hybrid metaheuristic solver design that can be generalized to other problems.",
        "published": "2007-08-03T12:49:21Z",
        "link": "http://arxiv.org/abs/0708.0505v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.DM",
            "q-bio.QM",
            "I.2.8; J.3.x; F.2.2"
        ]
    },
    {
        "title": "A variant of the Recoil Growth algorithm to generate multi-polymer   systems",
        "authors": [
            "Florian Simatos"
        ],
        "summary": "The Recoil Growth algorithm, proposed in 1999 by Consta et al., is one of the most efficient algorithm available in the literature to sample from a multi-polymer system. Such problems are closely related to the generation of self-avoiding paths. In this paper, we study a variant of the original Recoil Growth algorithm, where we constrain the generation of a new polymer to take place on a specific class of graphs. This makes it possible to make a fine trade-off between computational cost and success rate. We moreover give a simple proof for a lower bound on the irreducibility of this new algorithm, which applies to the original algorithm as well.",
        "published": "2007-08-08T15:07:53Z",
        "link": "http://arxiv.org/abs/0708.1116v3",
        "categories": [
            "cs.CE",
            "cond-mat.stat-mech"
        ]
    },
    {
        "title": "Computational Simulation and 3D Virtual Reality Engineering Tools for   Dynamical Modeling and Imaging of Composite Nanomaterials",
        "authors": [
            "L. -V. Bochkareva",
            "M. -V. Kireitseu",
            "G. R. Tomlinson",
            "H. Altenbach",
            "V. Kompis",
            "D. Hui"
        ],
        "summary": "An adventure at engineering design and modeling is possible with a Virtual Reality Environment (VRE) that uses multiple computer-generated media to let a user experience situations that are temporally and spatially prohibiting. In this paper, an approach to developing some advanced architecture and modeling tools is presented to allow multiple frameworks work together while being shielded from the application program. This architecture is being developed in a framework of workbench interactive tools for next generation nanoparticle-reinforced damping/dynamic systems. Through the use of system, an engineer/programmer can respectively concentrate on tailoring an engineering design concept of novel system and the application software design while using existing databases/software outputs.",
        "published": "2007-08-14T08:17:45Z",
        "link": "http://arxiv.org/abs/0708.1818v1",
        "categories": [
            "cs.CE",
            "cond-mat.other"
        ]
    },
    {
        "title": "Identifying Small Mean Reverting Portfolios",
        "authors": [
            "Alexandre d'Aspremont"
        ],
        "summary": "Given multivariate time series, we study the problem of forming portfolios with maximum mean reversion while constraining the number of assets in these portfolios. We show that it can be formulated as a sparse canonical correlation analysis and study various algorithms to solve the corresponding sparse generalized eigenvalue problems. After discussing penalized parameter estimation procedures, we study the sparsity versus predictability tradeoff and the impact of predictability in various markets.",
        "published": "2007-08-22T16:25:17Z",
        "link": "http://arxiv.org/abs/0708.3048v2",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "A Neural Networks Model of the Venezuelan Economy",
        "authors": [
            "Sabatino Costanzo",
            "Loren Trigo",
            "Luis Jimenez",
            "Juan Gonzalez"
        ],
        "summary": "Besides an indicator of the GDP, the Central Bank of Venezuela generates the so called Monthly Economic Activity General Indicator. The a priori knowledge of this indicator, which represents and sometimes even anticipates the economy's fluctuations, could be helpful in developing public policies and in investment decision making. The purpose of this study is forecasting the IGAEM through non parametric methods, an approach that has proven effective in a wide variety of problems in economics and finance.",
        "published": "2007-08-26T05:10:29Z",
        "link": "http://arxiv.org/abs/0708.3463v1",
        "categories": [
            "cs.CE",
            "cs.NE"
        ]
    },
    {
        "title": "A Non Parametric Study of the Volatility of the Economy as a Country   Risk Predictor",
        "authors": [
            "Sabatino Costanzo",
            "Loren Trigo",
            "Ramses Dominguez",
            "William Moreno"
        ],
        "summary": "This paper intends to explain Venezuela's country spread behavior through the Neural Networks analysis of a monthly economic activity general index of economic indicators constructed by the Central Bank of Venezuela, a measure of the shocks affecting country risk of emerging markets and the U.S. short term interest rate. The use of non parametric methods allowed the finding of non linear relationship between these inputs and the country risk. The networks performance was evaluated using the method of excess predictability.",
        "published": "2007-08-26T05:30:18Z",
        "link": "http://arxiv.org/abs/0708.3464v1",
        "categories": [
            "cs.CE",
            "cs.NE"
        ]
    },
    {
        "title": "An Early Warning System for Bankruptcy Prediction: lessons from the   Venezuelan Bank Crisis",
        "authors": [
            "Loren Trigo",
            "Sabatino Costanzo",
            "Felix Gonzalez",
            "Jose Llamozas"
        ],
        "summary": "During 1993-94 Venezuela experienced a severe banking crisis which ended up with 18 commercial banks intervened by the government. Here we develop an early warning system for detecting credit related bankruptcy through discriminant functions developed on financial and macroeconomic data predating the crisis. A robustness test performed on these functions shows high precision in error estimation. The model calibrated on pre-crisis data could detect abnormal financial tension in the late Banco Capital many months before it was intervened and liquidated.",
        "published": "2007-08-26T05:33:41Z",
        "link": "http://arxiv.org/abs/0708.3465v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Comparison of Tree-Child Phylogenetic Networks",
        "authors": [
            "Gabriel Cardona",
            "Francesc Rossello",
            "Gabriel Valiente"
        ],
        "summary": "Phylogenetic networks are a generalization of phylogenetic trees that allow for the representation of non-treelike evolutionary events, like recombination, hybridization, or lateral gene transfer. In this paper, we present and study a new class of phylogenetic networks, called tree-child phylogenetic networks, where every non-extant species has some descendant through mutation. We provide an injective representation of these networks as multisets of vectors of natural numbers, their path multiplicity vectors, and we use this representation to define a distance on this class and to give an alignment method for pairs of these networks. To the best of our knowledge, they are respectively the first true distance and the first alignment method defined on a meaningful class of phylogenetic networks strictly extending the class of phylogenetic trees. Simple, polynomial algorithms for reconstructing a tree-child phylogenetic network from its path multiplicity vectors, for computing the distance between two tree-child phylogenetic networks, and for aligning a pair of tree-child phylogenetic networks, are provided, and they have been implemented as a Perl package and a Java applet, and they are available at http://bioinfo.uib.es/~recerca/phylonetworks/mudistance",
        "published": "2007-08-27T09:37:55Z",
        "link": "http://arxiv.org/abs/0708.3499v1",
        "categories": [
            "q-bio.PE",
            "cs.CE",
            "cs.DM"
        ]
    },
    {
        "title": "A Non Parametric Model for the Forecasting of the Venezuelan Oil Prices",
        "authors": [
            "Sabatino Costanzo",
            "Loren Trigo",
            "Wafaa Dehne",
            "Hender Prato"
        ],
        "summary": "A neural net model for forecasting the prices of Venezuelan crude oil is proposed. The inputs of the neural net are selected by reference to a dynamic system model of oil prices by Mashayekhi (1995, 2001) and its performance is evaluated using two criteria: the Excess Profitability test by Anatoliev and Gerko (2005) and the characteristics of the equity curve generated by a trading strategy based on the neural net predictions.   -----   Se introduce aqui un modelo no parametrico para pronosticar los precios del petroleo Venezolano cuyos insumos son seleccionados en base a un sistema dinamico que explica los precios en terminos de dichos insumos. Se describe el proceso de recoleccion y pre-procesamiento de datos y la corrida de la red y se evaluan sus pronosticos a traves de un test estadistico de predictibilidad y de las caracteristicas del Equity Curve inducido por la estrategia de compraventa bursatil generada por dichos pronosticos.",
        "published": "2007-08-28T18:29:55Z",
        "link": "http://arxiv.org/abs/0708.3829v1",
        "categories": [
            "cs.CE",
            "cs.NE"
        ]
    },
    {
        "title": "Solution of moving-boundary problems by the spectral element method",
        "authors": [
            "Nicolas Bodard",
            "Roland Bouffanais",
            "Michel O. Deville"
        ],
        "summary": "This paper describes a novel numerical model aiming at solving moving-boundary problems such as free-surface flows or fluid-structure interaction. This model uses a moving-grid technique to solve the Navier--Stokes equations expressed in the arbitrary Lagrangian--Eulerian kinematics. The discretization in space is based on the spectral element method. The coupling of the fluid equations and the moving-grid equations is essentially done through the conditions on the moving boundaries. Two- and three-dimensional simulations are presented: translation and rotation of a cylinder in a fluid, and large-amplitude sloshing in a rectangular tank. The accuracy and robustness of the present numerical model is studied and discussed.",
        "published": "2007-09-04T14:51:56Z",
        "link": "http://arxiv.org/abs/0709.0355v1",
        "categories": [
            "cs.CE",
            "cs.NA"
        ]
    },
    {
        "title": "Sound Generation by a Turbulent Flow in Musical Instruments -   Multiphysics Simulation Approach -",
        "authors": [
            "Taizo Kobayashi",
            "Toshiya Takami",
            "Kin'ya Takahashi",
            "Ryota Mibu",
            "Mutsumi Aoyagi"
        ],
        "summary": "Total computational costs of scientific simulations are analyzed between direct numerical simulations (DNS) and multiphysics simulations (MPS) for sound generation in musical instruments. In order to produce acoustic sound by a turbulent flow in a simple recorder-like instrument, compressible fluid dynamic calculations with a low Mach number are required around the edges and the resonator of the instrument in DNS, while incompressible fluid dynamic calculations coupled with dynamics of sound propagation based on the Lighthill's acoustic analogy are used in MPS. These strategies are evaluated not only from the viewpoint of computational performances but also from the theoretical points of view as tools for scientific simulations of complicated systems.",
        "published": "2007-09-06T13:47:31Z",
        "link": "http://arxiv.org/abs/0709.0787v1",
        "categories": [
            "physics.comp-ph",
            "cs.CE",
            "physics.flu-dyn"
        ]
    },
    {
        "title": "Adaptive Investment Strategies For Periodic Environments",
        "authors": [
            "J. -Emeterio Navarro"
        ],
        "summary": "In this paper, we present an adaptive investment strategy for environments with periodic returns on investment. In our approach, we consider an investment model where the agent decides at every time step the proportion of wealth to invest in a risky asset, keeping the rest of the budget in a risk-free asset. Every investment is evaluated in the market via a stylized return on investment function (RoI), which is modeled by a stochastic process with unknown periodicities and levels of noise. For comparison reasons, we present two reference strategies which represent the case of agents with zero-knowledge and complete-knowledge of the dynamics of the returns. We consider also an investment strategy based on technical analysis to forecast the next return by fitting a trend line to previous received returns. To account for the performance of the different strategies, we perform some computer experiments to calculate the average budget that can be obtained with them over a certain number of time steps. To assure for fair comparisons, we first tune the parameters of each strategy. Afterwards, we compare the performance of these strategies for RoIs with different periodicities and levels of noise.",
        "published": "2007-09-27T19:04:00Z",
        "link": "http://arxiv.org/abs/0709.4464v2",
        "categories": [
            "cs.CE",
            "cs.NE",
            "I.2.8"
        ]
    },
    {
        "title": "Theoretical Engineering and Satellite Comlink of a PTVD-SHAM System",
        "authors": [
            "Philip B. Alipour"
        ],
        "summary": "This paper focuses on super helical memory system's design, 'Engineering, Architectural and Satellite Communications' as a theoretical approach of an invention-model to 'store time-data'. The current release entails three concepts: 1- an in-depth theoretical physics engineering of the chip including its, 2- architectural concept based on VLSI methods, and 3- the time-data versus data-time algorithm. The 'Parallel Time Varying & Data Super-helical Access Memory' (PTVD-SHAM), possesses a waterfall effect in its architecture dealing with the process of voltage output-switch into diverse logic and quantum states described as 'Boolean logic & image-logic', respectively. Quantum dot computational methods are explained by utilizing coiled carbon nanotubes (CCNTs) and CNT field effect transistors (CNFETs) in the chip's architecture. Quantum confinement, categorized quantum well substrate, and B-field flux involvements are discussed in theory. Multi-access of coherent sequences of 'qubit addressing' in any magnitude, gained as pre-defined, here e.g., the 'big O notation' asymptotically confined into singularity while possessing a magnitude of 'infinity' for the orientation of array displacement. Gaussian curvature of k<0 versus k'>(k<0) is debated in aim of specifying the 2D electron gas characteristics, data storage system for defining short and long time cycles for different CCNT diameters where space-time continuum is folded by chance for the particle. Precise pre/post data timing for, e.g., seismic waves before earthquake mantle-reach event occurrence, including time varying self-clocking devices in diverse geographic locations for radar systems is illustrated in the Subsections of the paper. The theoretical fabrication process, electromigration between chip's components is discussed as well.",
        "published": "2007-10-01T09:35:30Z",
        "link": "http://arxiv.org/abs/0710.0244v1",
        "categories": [
            "cs.CE",
            "cs.AR"
        ]
    },
    {
        "title": "Incomplete Lineage Sorting: Consistent Phylogeny Estimation From   Multiple Loci",
        "authors": [
            "Elchanan Mossel",
            "Sebastien Roch"
        ],
        "summary": "We introduce a simple algorithm for reconstructing phylogenies from multiple gene trees in the presence of incomplete lineage sorting, that is, when the topology of the gene trees may differ from that of the species tree. We show that our technique is statistically consistent under standard stochastic assumptions, that is, it returns the correct tree given sufficiently many unlinked loci. We also show that it can tolerate moderate estimation errors.",
        "published": "2007-10-01T11:11:43Z",
        "link": "http://arxiv.org/abs/0710.0262v2",
        "categories": [
            "q-bio.PE",
            "cs.CE",
            "cs.DS",
            "math.PR",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "The Theory of Unified Relativity for a Biovielectroluminescence   Phenomenon via Fly's Visual and Imaging System",
        "authors": [
            "Philip B. Alipour"
        ],
        "summary": "The elucidation upon fly's neuronal patterns as a link to computer graphics and memory cards I/O's, is investigated for the phenomenon by propounding a unified theory of Einstein's two known relativities. It is conclusive that flies could contribute a certain amount of neuromatrices indicating an imagery function of a visual-computational system into computer graphics and storage systems. The visual system involves the time aspect, whereas flies possess faster pulses compared to humans' visual ability due to the E-field state on an active fly's eye surface. This behaviour can be tested on a dissected fly specimen at its ommatidia. Electro-optical contacts and electrodes are wired through the flesh forming organic emitter layer to stimulate light emission, thereby to a computer circuit. The next step is applying a threshold voltage with secondary voltages to the circuit denoting an array of essential electrodes for bit switch. As a result, circuit's dormant pulses versus active pulses at the specimen's area are recorded. The outcome matrix possesses a construction of RGB and time radicals expressing the time problem in consumption, allocating time into computational algorithms, enhancing the technology far beyond. The obtained formulation generates consumed distance cons(x), denoting circuital travel between data source/sink for pixel data and bendable wavelengths. Once 'image logic' is in place, incorporating this point of graphical acceleration permits one to enhance graphics and optimize immensely central processing, data transmissions between memory and computer visual system. The phenomenon can be mainly used in 360-deg. display/viewing, 3D scanning techniques, military and medicine, a robust and cheap substitution for e.g. pre-motion pattern analysis, real-time rendering and LCDs.",
        "published": "2007-10-01T23:55:50Z",
        "link": "http://arxiv.org/abs/0710.0410v1",
        "categories": [
            "cs.CE",
            "cs.CV",
            "I.2.10; I.4; I.5; J.0; J.2; J.3"
        ]
    },
    {
        "title": "Numerical removal of water-vapor effects from THz-TDS measurements",
        "authors": [
            "Withawat Withayachumnankul",
            "Bernd M. Fischer",
            "Samuel P. Mickan",
            "Derek Abbott"
        ],
        "summary": "One source of disturbance in a pulsed T-ray signal is attributed to ambient water vapor. Water molecules in the gas phase selectively absorb T-rays at discrete frequencies corresponding to their molecular rotational transitions. This results in prominent resonances spread over the T-ray spectrum, and in the time domain the T-ray signal is observed as fluctuations after the main pulse. These effects are generally undesired, since they may mask critical spectroscopic data. So, ambient water vapor is commonly removed from the T-ray path by using a closed chamber during the measurement. Yet, in some applications a closed chamber is not applicable. This situation, therefore, motivates the need for another method to reduce these unwanted artifacts. This paper presents a study on a computational means to address the problem. Initially, a complex frequency response of water vapor is modeled from a spectroscopic catalog. Using a deconvolution technique, together with fine tuning of the strength of each resonance, parts of the water-vapor response are removed from a measured T-ray signal, with minimal signal distortion.",
        "published": "2007-10-19T02:17:35Z",
        "link": "http://arxiv.org/abs/0710.3621v1",
        "categories": [
            "cs.CE",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Non-linear estimation is easy",
        "authors": [
            "Michel Fliess",
            "Cédric Join",
            "Hebertt Sira-Ramirez"
        ],
        "summary": "Non-linear state estimation and some related topics, like parametric estimation, fault diagnosis, and perturbation attenuation, are tackled here via a new methodology in numerical differentiation. The corresponding basic system theoretic definitions and properties are presented within the framework of differential algebra, which permits to handle system variables and their derivatives of any order. Several academic examples and their computer simulations, with on-line estimations, are illustrating our viewpoint.",
        "published": "2007-10-24T14:48:39Z",
        "link": "http://arxiv.org/abs/0710.4486v1",
        "categories": [
            "cs.CE",
            "cs.NA",
            "cs.PF",
            "math.AC",
            "math.NA",
            "math.OC"
        ]
    },
    {
        "title": "Combining haplotypers",
        "authors": [
            "Matti Kääriäinen",
            "Niels Landwehr",
            "Sampsa Lappalainen",
            "Taneli Mielikäinen"
        ],
        "summary": "Statistically resolving the underlying haplotype pair for a genotype measurement is an important intermediate step in gene mapping studies, and has received much attention recently. Consequently, a variety of methods for this problem have been developed. Different methods employ different statistical models, and thus implicitly encode different assumptions about the nature of the underlying haplotype structure. Depending on the population sample in question, their relative performance can vary greatly, and it is unclear which method to choose for a particular sample. Instead of choosing a single method, we explore combining predictions returned by different methods in a principled way, and thereby circumvent the problem of method selection.   We propose several techniques for combining haplotype reconstructions and analyze their computational properties. In an experimental study on real-world haplotype data we show that such techniques can provide more accurate and robust reconstructions, and are useful for outlier detection. Typically, the combined prediction is at least as accurate as or even more accurate than the best individual method, effectively circumventing the method selection problem.",
        "published": "2007-10-26T15:13:21Z",
        "link": "http://arxiv.org/abs/0710.5116v1",
        "categories": [
            "cs.LG",
            "cs.CE",
            "q-bio.QM",
            "F.2.2; I.2.6; J.3"
        ]
    },
    {
        "title": "Risk Minimization and Optimal Derivative Design in a Principal Agent   Game",
        "authors": [
            "U. Horst",
            "S. Moreno"
        ],
        "summary": "We consider the problem of Adverse Selection and optimal derivative design within a Principal-Agent framework. The principal's income is exposed to non-hedgeable risk factors arising, for instance, from weather or climate phenomena. She evaluates her risk using a coherent and law invariant risk measure and tries minimize her exposure by selling derivative securities on her income to individual agents. The agents have mean-variance preferences with heterogeneous risk aversion coefficients. An agent's degree of risk aversion is private information and hidden to the principal who only knows the overall distribution. We show that the principal's risk minimization problem has a solution and illustrate the effects of risk transfer on her income by means of two specific examples. Our model extends earlier work of Barrieu and El Karoui (2005) and Carlier, Ekeland and Touzi (2007).",
        "published": "2007-10-29T20:00:15Z",
        "link": "http://arxiv.org/abs/0710.5512v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Addendum to Research MMMCV; A Man/Microbio/Megabio/Computer Vision",
        "authors": [
            "Philip B. Alipour"
        ],
        "summary": "In October 2007, a Research Proposal for the University of Sydney, Australia, the author suggested that biovie-physical phenomenon as `electrodynamic dependant biological vision', is governed by relativistic quantum laws and biovision. The phenomenon on the basis of `biovielectroluminescence', satisfies man/microbio/megabio/computer vision (MMMCV), as a robust candidate for physical and visual sciences. The general aim of this addendum is to present a refined text of Sections 1-3 of that proposal and highlighting the contents of its Appendix in form of a `Mechanisms' Section. We then briefly remind in an article aimed for December 2007, by appending two more equations into Section 3, a theoretical II-time scenario as a time model well-proposed for the phenomenon. The time model within the core of the proposal, plays a significant role in emphasizing the principle points on Objectives no. 1-8, Sub-hypothesis 3.1.2, mentioned in Article [arXiv:0710.0410]. It also expresses the time concept in terms of causing quantized energy f(|E|) of time |t|, emit in regard to shortening the probability of particle loci as predictable patterns of particle's un-occurred motion, a solution to Heisenberg's uncertainty principle (HUP) into a simplistic manner. We conclude that, practical frames via a time algorithm to this model, fixates such predictable patterns of motion of scenery bodies onto recordable observation points of a MMMCV system. It even suppresses/predicts superposition phenomena coming from a human subject and/or other bio-subjects for any decision making event, e.g., brainwave quantum patterns based on vision. Maintaining the existential probability of Riemann surfaces of II-time scenarios in the context of biovielectroluminescence, makes motion-prediction a possibility.",
        "published": "2007-11-06T19:41:22Z",
        "link": "http://arxiv.org/abs/0711.0784v1",
        "categories": [
            "cs.CV",
            "cs.CE",
            "I.2.10; I.4; I.5; J.0; J.2; J.3"
        ]
    },
    {
        "title": "A numerical approach for 3D manufacturing tolerances synthesis",
        "authors": [
            "Frédéric Vignat",
            "François Villeneuve"
        ],
        "summary": "Making a product conform to the functional requirements indicated by the customer suppose to be able to manage the manufacturing process chosen to realise the parts. A simulation step is generally performed to verify that the expected generated deviations fit with these requirements. It is then necessary to assess the actual deviations of the process in progress. This is usually done by the verification of the conformity of the workpiece to manufacturing tolerances at the end of each set-up. It is thus necessary to determine these manufacturing tolerances. This step is called \"manufacturing tolerance synthesis\". In this paper, a numerical method is proposed to perform 3D manufacturing tolerances synthesis. This method uses the result of the numerical analysis of tolerances to determine influent mall displacement of surfaces. These displacements are described by small displacements torsors. An algorithm is then proposed to determine suitable ISO manufacturing tolerances.",
        "published": "2007-11-14T06:21:17Z",
        "link": "http://arxiv.org/abs/0711.2116v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "An Estimation of Distribution Algorithm with Intelligent Local Search   for Rule-based Nurse Rostering",
        "authors": [
            "Uwe Aickelin",
            "Edmund Burke",
            "Jingpeng Li"
        ],
        "summary": "This paper proposes a new memetic evolutionary algorithm to achieve explicit learning in rule-based nurse rostering, which involves applying a set of heuristic rules for each nurse's assignment. The main framework of the algorithm is an estimation of distribution algorithm, in which an ant-miner methodology improves the individual solutions produced in each generation. Unlike our previous work (where learning is implicit), the learning in the memetic estimation of distribution algorithm is explicit, i.e. we are able to identify building blocks directly. The overall approach learns by building a probabilistic model, i.e. an estimation of the probability distribution of individual nurse-rule pairs that are used to construct schedules. The local search processor (i.e. the ant-miner) reinforces nurse-rule pairs that receive higher rewards. A challenging real world nurse rostering problem is used as the test problem. Computational results show that the proposed approach outperforms most existing approaches. It is suggested that the learning methodologies suggested in this paper may be applied to other scheduling problems where schedules are built systematically according to specific rules",
        "published": "2007-11-22T15:16:21Z",
        "link": "http://arxiv.org/abs/0711.3591v2",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "A Perl Package and an Alignment Tool for Phylogenetic Networks",
        "authors": [
            "Gabriel Cardona",
            "Francesc Rossello",
            "Gabriel Valiente"
        ],
        "summary": "Phylogenetic networks are a generalization of phylogenetic trees that allow for the representation of evolutionary events acting at the population level, like recombination between genes, hybridization between lineages, and lateral gene transfer. While most phylogenetics tools implement a wide range of algorithms on phylogenetic trees, there exist only a few applications to work with phylogenetic networks, and there are no open-source libraries either.   In order to improve this situation, we have developed a Perl package that relies on the BioPerl bundle and implements many algorithms on phylogenetic networks. We have also developed a Java applet that makes use of the aforementioned Perl package and allows the user to make simple experiments with phylogenetic networks without having to develop a program or Perl script by herself.   The Perl package has been accepted as part of the BioPerl bundle. It can be downloaded from http://dmi.uib.es/~gcardona/BioInfo/Bio-PhyloNetwork.tgz. The web-based application is available at http://dmi.uib.es/~gcardona/BioInfo/. The Perl package includes full documentation of all its features.",
        "published": "2007-11-22T18:05:49Z",
        "link": "http://arxiv.org/abs/0711.3628v1",
        "categories": [
            "q-bio.PE",
            "cs.CE"
        ]
    },
    {
        "title": "Report on \"American Option Pricing and Hedging Strategies\"",
        "authors": [
            "Jinshan Zhang"
        ],
        "summary": "This paper mainly discusses the American option's hedging strategies via binomialmodel and the basic idea of pricing and hedging American option. Although the essential scheme of hedging is almost the same as European option, small differences may arise when simulating the process for American option holder has more rights, spelling that the option can be exercised at anytime before its maturity. Our method is dynamic-hedging method.",
        "published": "2007-11-27T18:34:40Z",
        "link": "http://arxiv.org/abs/0711.4324v1",
        "categories": [
            "cs.CE",
            "cs.DM",
            "G.3"
        ]
    },
    {
        "title": "Building the Tangent and Adjoint codes of the Ocean General Circulation   Model OPA with the Automatic Differentiation tool TAPENADE",
        "authors": [
            "Moulay Hicham Tber",
            "Laurent Hascoet",
            "Arthur Vidard",
            "Benjamin Dauvergne"
        ],
        "summary": "The ocean general circulation model OPA is developed by the LODYC team at Paris VI university. OPA has recently undergone a major rewriting, migrating to FORTRAN95, and its adjoint code needs to be rebuilt. For earlier versions, the adjoint of OPA was written by hand at a high development cost. We use the Automatic Differentiation tool TAPENADE to build mechanicaly the tangent and adjoint codes of OPA. We validate the differentiated codes by comparison with divided differences, and also with an identical twin experiment. We apply state-of-the-art methods to improve the performance of the adjoint code. In particular we implement the Griewank and Walther's binomial checkpointing algorithm which gives us an optimal trade-off between time and memory consumption. We apply a specific strategy to differentiate the iterative linear solver that comes from the implicit time stepping scheme",
        "published": "2007-11-28T08:04:18Z",
        "link": "http://arxiv.org/abs/0711.4444v2",
        "categories": [
            "cs.MS",
            "cs.CE"
        ]
    },
    {
        "title": "Human-Machine Symbiosis, 50 Years On",
        "authors": [
            "Ian Foster"
        ],
        "summary": "Licklider advocated in 1960 the construction of computers capable of working symbiotically with humans to address problems not easily addressed by humans working alone. Since that time, many of the advances that he envisioned have been achieved, yet the time spent by human problem solvers in mundane activities remains large. I propose here four areas in which improved tools can further advance the goal of enhancing human intellect: services, provenance, knowledge communities, and automation of problem-solving protocols.",
        "published": "2007-12-13T23:00:37Z",
        "link": "http://arxiv.org/abs/0712.2255v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.HC"
        ]
    },
    {
        "title": "The Earth System Grid: Supporting the Next Generation of Climate   Modeling Research",
        "authors": [
            "David Bernholdt",
            "Shishir Bharathi",
            "David Brown",
            "Kasidit Chanchio",
            "Meili Chen",
            "Ann Chervenak",
            "Luca Cinquini",
            "Bob Drach",
            "Ian Foster",
            "Peter Fox",
            "Jose Garcia",
            "Carl Kesselman",
            "Rob Markel",
            "Don Middleton",
            "Veronika Nefedova",
            "Line Pouchard",
            "Arie Shoshani",
            "Alex Sim",
            "Gary Strand",
            "Dean Williams"
        ],
        "summary": "Understanding the earth's climate system and how it might be changing is a preeminent scientific challenge. Global climate models are used to simulate past, present, and future climates, and experiments are executed continuously on an array of distributed supercomputers. The resulting data archive, spread over several sites, currently contains upwards of 100 TB of simulation data and is growing rapidly. Looking toward mid-decade and beyond, we must anticipate and prepare for distributed climate research data holdings of many petabytes. The Earth System Grid (ESG) is a collaborative interdisciplinary project aimed at addressing the challenge of enabling management, discovery, access, and analysis of these critically important datasets in a distributed and heterogeneous computational environment. The problem is fundamentally a Grid problem. Building upon the Globus toolkit and a variety of other technologies, ESG is developing an environment that addresses authentication, authorization for data access, large-scale data transport and management, services and abstractions for high-performance remote data access, mechanisms for scalable data replication, cataloging with rich semantic and syntactic information, data discovery, distributed monitoring, and Web-based portals for using the system.",
        "published": "2007-12-13T23:39:04Z",
        "link": "http://arxiv.org/abs/0712.2262v1",
        "categories": [
            "cs.CE",
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Changing Levels of Description in a Fluid Flow Simulation",
        "authors": [
            "Pierrick Tranouez",
            "Cyrille Bertelle",
            "Damien Olivier"
        ],
        "summary": "We describe here our perception of complex systems, of how we feel the different layers of description are important part of a correct complex system simulation. We describe a rough models categorization between rules based and law based, of how these categories handled the levels of descriptions or scales. We then describe our fluid flow simulation, which combines different fineness of grain in a mixed approach of these categories. This simulation is built keeping in mind an ulterior use inside a more general aquatic ecosystem.",
        "published": "2007-12-17T07:07:06Z",
        "link": "http://arxiv.org/abs/0712.2643v1",
        "categories": [
            "physics.flu-dyn",
            "cs.CE"
        ]
    },
    {
        "title": "Trading in Risk Dimensions (TRD)",
        "authors": [
            "Lester Ingber"
        ],
        "summary": "Previous work, mostly published, developed two-shell recursive trading systems. An inner-shell of Canonical Momenta Indicators (CMI) is adaptively fit to incoming market data. A parameterized trading-rule outer-shell uses the global optimization code Adaptive Simulated Annealing (ASA) to fit the trading system to historical data. A simple fitting algorithm, usually not requiring ASA, is used for the inner-shell fit. An additional risk-management middle-shell has been added to create a three-shell recursive optimization/sampling/fitting algorithm. Portfolio-level distributions of copula-transformed multivariate distributions (with constituent markets possessing different marginal distributions in returns space) are generated by Monte Carlo samplings. ASA is used to importance-sample weightings of these markets.   The core code, Trading in Risk Dimensions (TRD), processes Training and Testing trading systems on historical data, and consistently interacts with RealTime trading platforms at minute resolutions, but this scale can be modified. This approach transforms constituent probability distributions into a common space where it makes sense to develop correlations to further develop probability distributions and risk/uncertainty analyses of the full portfolio. ASA is used for importance-sampling these distributions and for optimizing system parameters.",
        "published": "2007-12-17T18:11:52Z",
        "link": "http://arxiv.org/abs/0712.2789v2",
        "categories": [
            "cs.CE",
            "cs.NA"
        ]
    },
    {
        "title": "Tuplix Calculus",
        "authors": [
            "J. A. Bergstra",
            "A. Ponse",
            "M. B. van der Zwaag"
        ],
        "summary": "We introduce a calculus for tuplices, which are expressions that generalize matrices and vectors. Tuplices have an underlying data type for quantities that are taken from a zero-totalized field. We start with the core tuplix calculus CTC for entries and tests, which are combined using conjunctive composition. We define a standard model and prove that CTC is relatively complete with respect to it. The core calculus is extended with operators for choice, information hiding, scalar multiplication, clearing and encapsulation. We provide two examples of applications; one on incremental financial budgeting, and one on modular financial budget design.",
        "published": "2007-12-20T13:58:14Z",
        "link": "http://arxiv.org/abs/0712.3423v1",
        "categories": [
            "cs.LO",
            "cs.CE"
        ]
    },
    {
        "title": "A Unified Framework for Pricing Credit and Equity Derivatives",
        "authors": [
            "Erhan Bayraktar",
            "Bo Yang"
        ],
        "summary": "We propose a model which can be jointly calibrated to the corporate bond term structure and equity option volatility surface of the same company. Our purpose is to obtain explicit bond and equity option pricing formulas that can be calibrated to find a risk neutral model that matches a set of observed market prices. This risk neutral model can then be used to price more exotic, illiquid or over-the-counter derivatives. We observe that the model implied credit default swap (CDS) spread matches the market CDS spread and that our model produces a very desirable CDS spread term structure. This is observation is worth noticing since without calibrating any parameter to the CDS spread data, it is matched by the CDS spread that our model generates using the available information from the equity options and corporate bond markets. We also observe that our model matches the equity option implied volatility surface well since we properly account for the default risk premium in the implied volatility surface. We demonstrate the importance of accounting for the default risk and stochastic interest rate in equity option pricing by comparing our results to Fouque, Papanicolaou, Sircar and Solna (2003), which only accounts for stochastic volatility.",
        "published": "2007-12-21T02:53:38Z",
        "link": "http://arxiv.org/abs/0712.3617v2",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "TRUST-TECH based Methods for Optimization and Learning",
        "authors": [
            "Chandan K. Reddy"
        ],
        "summary": "Many problems that arise in machine learning domain deal with nonlinearity and quite often demand users to obtain global optimal solutions rather than local optimal ones. Optimization problems are inherent in machine learning algorithms and hence many methods in machine learning were inherited from the optimization literature. Popularly known as the initialization problem, the ideal set of parameters required will significantly depend on the given initialization values. The recently developed TRUST-TECH (TRansformation Under STability-reTaining Equilibria CHaracterization) methodology systematically explores the subspace of the parameters to obtain a complete set of local optimal solutions. In this thesis work, we propose TRUST-TECH based methods for solving several optimization and machine learning problems. Two stages namely, the local stage and the neighborhood-search stage, are repeated alternatively in the solution space to achieve improvements in the quality of the solutions. Our methods were tested on both synthetic and real datasets and the advantages of using this novel framework are clearly manifested. This framework not only reduces the sensitivity to initialization, but also allows the flexibility for the practitioners to use various global and local methods that work well for a particular problem of interest. Other hierarchical stochastic algorithms like evolutionary algorithms and smoothing algorithms are also studied and frameworks for combining these methods with TRUST-TECH have been proposed and evaluated on several test systems.",
        "published": "2007-12-25T03:14:32Z",
        "link": "http://arxiv.org/abs/0712.4126v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.MS",
            "cs.NA",
            "cs.NE",
            "G.1.6; I.5.3; I.5.1"
        ]
    },
    {
        "title": "Applications of Polyhedral Computations to the Analysis and Verification   of Hardware and Software Systems",
        "authors": [
            "Roberto Bagnara",
            "Patricia M. Hill",
            "Enea Zaffanella"
        ],
        "summary": "Convex polyhedra are the basis for several abstractions used in static analysis and computer-aided verification of complex and sometimes mission critical systems. For such applications, the identification of an appropriate complexity-precision trade-off is a particularly acute problem, so that the availability of a wide spectrum of alternative solutions is mandatory. We survey the range of applications of polyhedral computations in this area; give an overview of the different classes of polyhedra that may be adopted; outline the main polyhedral operations required by automatic analyzers and verifiers; and look at some possible combinations of polyhedra with other numerical abstractions that have the potential to improve the precision of the analysis. Areas where further theoretical investigations can result in important contributions are highlighted.",
        "published": "2007-01-19T08:39:34Z",
        "link": "http://arxiv.org/abs/cs/0701122v2",
        "categories": [
            "cs.CG",
            "cs.MS",
            "D.2.4; F.3.1"
        ]
    },
    {
        "title": "Certification of bounds on expressions involving rounded operators",
        "authors": [
            "Marc Daumas",
            "Guillaume Melquiond"
        ],
        "summary": "Gappa uses interval arithmetic to certify bounds on mathematical expressions that involve rounded as well as exact operators. Gappa generates a theorem with its proof for each bound treated. The proof can be checked with a higher order logic automatic proof checker, either Coq or HOL Light, and we have developed a large companion library of verified facts for Coq dealing with the addition, multiplication, division, and square root, in fixed- and floating-point arithmetics. Gappa uses multiple-precision dyadic fractions for the endpoints of intervals and performs forward error analysis on rounded operators when necessary. When asked, Gappa reports the best bounds it is able to reach for a given expression in a given context. This feature is used to quickly obtain coarse bounds. It can also be used to identify where the set of facts and automatic techniques implemented in Gappa becomes insufficient. Gappa handles seamlessly additional properties expressed as interval properties or rewriting rules in order to establish more intricate bounds. Recent work showed that Gappa is perfectly suited to the proof of correctness of small pieces of software. Proof obligations can be written by designers, produced by third-party tools or obtained by overloading arithmetic operators.",
        "published": "2007-01-29T15:36:47Z",
        "link": "http://arxiv.org/abs/cs/0701186v2",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "A canonical form for some piecewise defined functions",
        "authors": [
            "Jacques Carette"
        ],
        "summary": "We define a canonical form for piecewise defined functions. We show that this has a wider range of application as well as better complexity properties than previous work.",
        "published": "2007-02-01T17:54:50Z",
        "link": "http://arxiv.org/abs/cs/0702010v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "I.1.1"
        ]
    },
    {
        "title": "LIBOPT - An environment for testing solvers on heterogeneous collections   of problems - Version 1.0",
        "authors": [
            "Jean Charles Gilbert",
            "Xavier Jonsson"
        ],
        "summary": "The Libopt environment is both a methodology and a set of tools that can be used for testing, comparing, and profiling solvers on problems belonging to various collections. These collections can be heterogeneous in the sense that their problems can have common features that differ from one collection to the other. Libopt brings a unified view on this composite world by offering, for example, the possibility to run any solver on any problem compatible with it, using the same Unix/Linux command. The environment also provides tools for comparing the results obtained by solvers on a specified set of problems. Most of the scripts going with the Libopt environment have been written in Perl.",
        "published": "2007-03-06T14:05:28Z",
        "link": "http://arxiv.org/abs/cs/0703025v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "math.OC"
        ]
    },
    {
        "title": "Why the Standard Data Processing should be changed",
        "authors": [
            "Yefim Bakman"
        ],
        "summary": "The basic statistical methods of data representation did not change since their emergence. Their simplicity was dictated by the intricacies of computations in the before computers epoch. It turns out that such approach is not uniquely possible in the presence of quick computers. The suggested here method improves significantly the reliability of data processing and their graphical representation. In this paper we show problems of the standard data processing which can bring to incorrect results. A method solving these problems is proposed. It is based on modification of data representation. The method was implemented in a computer program Consensus5. The program performances are illustrated through varied examples.",
        "published": "2007-03-08T19:49:35Z",
        "link": "http://arxiv.org/abs/cs/0703040v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Type-II/III DCT/DST algorithms with reduced number of arithmetic   operations",
        "authors": [
            "Xuancheng Shao",
            "Steven G. Johnson"
        ],
        "summary": "We present algorithms for the discrete cosine transform (DCT) and discrete sine transform (DST), of types II and III, that achieve a lower count of real multiplications and additions than previously published algorithms, without sacrificing numerical accuracy. Asymptotically, the operation count is reduced from ~ 2N log_2 N to ~ (17/9) N log_2 N for a power-of-two transform size N. Furthermore, we show that a further N multiplications may be saved by a certain rescaling of the inputs or outputs, generalizing a well-known technique for N=8 by Arai et al. These results are derived by considering the DCT to be a special case of a DFT of length 4N, with certain symmetries, and then pruning redundant operations from a recent improved fast Fourier transform algorithm (based on a recursive rescaling of the conjugate-pair split radix algorithm). The improved algorithms for DCT-III, DST-II, and DST-III follow immediately from the improved count for the DCT-II.",
        "published": "2007-03-30T00:53:48Z",
        "link": "http://arxiv.org/abs/cs/0703150v2",
        "categories": [
            "cs.NA",
            "cs.DS",
            "cs.MS",
            "F.2.1"
        ]
    },
    {
        "title": "Real Options for Project Schedules (ROPS)",
        "authors": [
            "Lester Ingber"
        ],
        "summary": "Real Options for Project Schedules (ROPS) has three recursive sampling/optimization shells. An outer Adaptive Simulated Annealing (ASA) optimization shell optimizes parameters of strategic Plans containing multiple Projects containing ordered Tasks. A middle shell samples probability distributions of durations of Tasks. An inner shell samples probability distributions of costs of Tasks. PATHTREE is used to develop options on schedules.. Algorithms used for Trading in Risk Dimensions (TRD) are applied to develop a relative risk analysis among projects.",
        "published": "2007-04-01T14:35:40Z",
        "link": "http://arxiv.org/abs/0704.0090v1",
        "categories": [
            "cs.CE",
            "cond-mat.stat-mech",
            "cs.MS",
            "cs.NA",
            "physics.data-an",
            "C.4; G.1; G.1.6; G.3; J.7"
        ]
    },
    {
        "title": "Algorithm for Evaluation of the Interval Power Function of Unconstrained   Arguments",
        "authors": [
            "Evgueni Petrov"
        ],
        "summary": "We describe an algorithm for evaluation of the interval extension of the power function of variables x and y given by the expression x^y. Our algorithm reduces the general case to the case of non-negative bases.",
        "published": "2007-04-24T08:33:52Z",
        "link": "http://arxiv.org/abs/0704.3141v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Optimal Cache-Oblivious Mesh Layouts",
        "authors": [
            "Michael A. Bender",
            "Bradley C. Kuszmaul",
            "Shang-Hua Teng",
            "Kebin Wang"
        ],
        "summary": "A mesh is a graph that divides physical space into regularly-shaped regions. Meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. In one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. The performance of a mesh update depends on the layout of the mesh in memory.   This paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. Such a memory layout is called cache-oblivious. Formally, for a $d$-dimensional mesh $G$, block size $B$, and cache size $M$ (where $M=\\Omega(B^d)$), the mesh update of $G$ uses $O(1+|G|/B)$ memory transfers. The paper also shows how the mesh-update performance degrades for smaller caches, where $M=o(B^d)$.   The paper then gives two algorithms for finding cache-oblivious mesh layouts. The first layout algorithm runs in time $O(|G|\\log^2|G|)$ both in expectation and with high probability on a RAM. It uses $O(1+|G|\\log^2(|G|/M)/B)$ memory transfers in expectation and $O(1+(|G|/B)(\\log^2(|G|/M) + \\log|G|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (DAM) models. The layout is obtained by finding a fully balanced decomposition tree of $G$ and then performing an in-order traversal of the leaves of the tree. The second algorithm runs faster by almost a $\\log|G|/\\log\\log|G|$ factor in all three memory models, both in expectation and with high probability. The layout obtained by finding a relax-balanced decomposition tree of $G$ and then performing an in-order traversal of the leaves of the tree.",
        "published": "2007-05-08T05:59:55Z",
        "link": "http://arxiv.org/abs/0705.1033v2",
        "categories": [
            "cs.DS",
            "cs.CE",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in   hypre and PETSc",
        "authors": [
            "A. V. Knyazev",
            "M. E. Argentati",
            "I. Lashuk",
            "E. E. Ovtchinnikov"
        ],
        "summary": "We describe our software package Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) publicly released recently. BLOPEX is available as a stand-alone serial library, as an external package to PETSc (``Portable, Extensible Toolkit for Scientific Computation'', a general purpose suite of tools for the scalable solution of partial differential equations and related problems developed by Argonne National Laboratory), and is also built into {\\it hypre} (``High Performance Preconditioners'', scalable linear solvers package developed by Lawrence Livermore National Laboratory). The present BLOPEX release includes only one solver--the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method for symmetric eigenvalue problems. {\\it hypre} provides users with advanced high-quality parallel preconditioners for linear systems, in particular, with domain decomposition and multigrid preconditioners. With BLOPEX, the same preconditioners can now be efficiently used for symmetric eigenvalue problems. PETSc facilitates the integration of independently developed application modules with strict attention to component interoperability, and makes BLOPEX extremely easy to compile and use with preconditioners that are available via PETSc. We present the LOBPCG algorithm in BLOPEX for {\\it hypre} and PETSc. We demonstrate numerically the scalability of BLOPEX by testing it on a number of distributed and shared memory parallel systems, including a Beowulf system, SUN Fire 880, an AMD dual-core Opteron workstation, and IBM BlueGene/L supercomputer, using PETSc domain decomposition and {\\it hypre} multigrid preconditioning. We test BLOPEX on a model problem, the standard 7-point finite-difference approximation of the 3-D Laplacian, with the problem size in the range $10^5-10^8$.",
        "published": "2007-05-18T02:25:16Z",
        "link": "http://arxiv.org/abs/0705.2626v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "G.4; G.1.3; G.1.8"
        ]
    },
    {
        "title": "Computing Integer Powers in Floating-Point Arithmetic",
        "authors": [
            "Peter Kornerup",
            "Vincent Lefèvre",
            "Jean-Michel Muller"
        ],
        "summary": "We introduce two algorithms for accurately evaluating powers to a positive integer in floating-point arithmetic, assuming a fused multiply-add (fma) instruction is available. We show that our log-time algorithm always produce faithfully-rounded results, discuss the possibility of getting correctly rounded results, and show that results correctly rounded in double precision can be obtained if extended-precision is available with the possibility to round into double precision (with a single rounding).",
        "published": "2007-05-30T11:34:39Z",
        "link": "http://arxiv.org/abs/0705.4369v1",
        "categories": [
            "cs.NA",
            "cs.MS"
        ]
    },
    {
        "title": "Clustering and Feature Selection using Sparse Principal Component   Analysis",
        "authors": [
            "Ronny Luss",
            "Alexandre d'Aspremont"
        ],
        "summary": "In this paper, we study the application of sparse principal component analysis (PCA) to clustering and feature selection problems. Sparse PCA seeks sparse factors, or linear combinations of the data variables, explaining a maximum amount of variance in the data while having only a limited number of nonzero coefficients. PCA is often used as a simple clustering technique and sparse factors allow us here to interpret the clusters in terms of a reduced set of variables. We begin with a brief introduction and motivation on sparse PCA and detail our implementation of the algorithm in d'Aspremont et al. (2005). We then apply these results to some classic clustering and feature selection problems arising in biology.",
        "published": "2007-07-04T21:53:11Z",
        "link": "http://arxiv.org/abs/0707.0701v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.MS"
        ]
    },
    {
        "title": "Fast computing of velocity field for flows in industrial burners and   pumps",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this work we present a technique of fast numerical computation for solutions of Navier-Stokes equations in the case of flows of industrial interest. At first the partial differential equations are translated into a set of nonlinear ordinary differential equations using the geometrical shape of the domain where the flow is developing, then these ODEs are numerically resolved using a set of computations distributed among the available processors. We present some results from simulations on a parallel hardware architecture using native multithreads software and simulating a shared-memory or a distributed-memory environment.",
        "published": "2007-07-10T16:23:43Z",
        "link": "http://arxiv.org/abs/0707.1490v1",
        "categories": [
            "math.NA",
            "cs.MS",
            "65Y05"
        ]
    },
    {
        "title": "Numerical Calculation With Arbitrary Precision",
        "authors": [
            "B. O. Rodrigues",
            "L. A. C. P. da Mota",
            "L. G. S. Duarte"
        ],
        "summary": "The vast use of computers on scientific numerical computation makes the awareness of the limited precision that these machines are able to provide us an essential matter. A limited and insufficient precision allied to the truncation and rounding errors may induce the user to incorrect interpretation of his/hers answer. In this work, we have developed a computational package to minimize this kind of error by offering arbitrary precision numbers and calculation. This is very important in Physics where we can work with numbers too small and too big simultaneously.",
        "published": "2007-07-11T22:24:48Z",
        "link": "http://arxiv.org/abs/0707.1716v1",
        "categories": [
            "cs.NA",
            "cs.MS"
        ]
    },
    {
        "title": "Memory efficient scheduling of Strassen-Winograd's matrix multiplication   algorithm",
        "authors": [
            "Brice Boyer",
            "Jean-Guillaume Dumas",
            "Clément Pernet",
            "Wei Zhou"
        ],
        "summary": "We propose several new schedules for Strassen-Winograd's matrix multiplication algorithm, they reduce the extra memory allocation requirements by three different means: by introducing a few pre-additions, by overwriting the input matrices, or by using a first recursive level of classical multiplication. In particular, we show two fully in-place schedules: one having the same number of operations, if the input matrices can be overwritten; the other one, slightly increasing the constant of the leading term of the complexity, if the input matrices are read-only. Many of these schedules have been found by an implementation of an exhaustive search algorithm based on a pebble game.",
        "published": "2007-07-16T16:02:50Z",
        "link": "http://arxiv.org/abs/0707.2347v5",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Difference Equations in Massive Higher Order Calculations",
        "authors": [
            "I. Bierenbaum",
            "J. Blümlein",
            "S. Klein",
            "C. Schneider"
        ],
        "summary": "The calculation of massive 2--loop operator matrix elements, required for the higher order Wilson coefficients for heavy flavor production in deeply inelastic scattering, leads to new types of multiple infinite sums over harmonic sums and related functions, which depend on the Mellin parameter $N$. We report on the solution of these sums through higher order difference equations using the summation package {\\tt Sigma}.",
        "published": "2007-07-31T16:54:33Z",
        "link": "http://arxiv.org/abs/0707.4659v1",
        "categories": [
            "math-ph",
            "cs.MS",
            "hep-ph",
            "math.MP"
        ]
    },
    {
        "title": "Comments on the Reliability of Lawson and Hanson's Linear Distance   Programming Algorithm: Subroutine LDP",
        "authors": [
            "Alan Rufty"
        ],
        "summary": "This brief paper: (1) Discusses strategies to generate random test cases that can be used to extensively test any Linear Distance Program (LDP) software. (2) Gives three numerical examples of input cases generated by this strategy that cause problems in the Lawson and Hanson LDP module. (3) Proposes, as a standard matter of acceptable implementation procedures, that (unless it is done internally in the software itself, but, in general, this seems to be much rarer than one would expect) all users should test the returned output from any LDP module for self-consistency since it incurs only a small amount of added computational overhead and it is not hard to do.",
        "published": "2007-07-31T17:04:34Z",
        "link": "http://arxiv.org/abs/0707.4651v1",
        "categories": [
            "cs.MS",
            "G.4.x; D.2.5"
        ]
    },
    {
        "title": "Verified Real Number Calculations: A Library for Interval Arithmetic",
        "authors": [
            "Marc Daumas",
            "David Lester",
            "César Muñoz"
        ],
        "summary": "Real number calculations on elementary functions are remarkably difficult to handle in mechanical proofs. In this paper, we show how these calculations can be performed within a theorem prover or proof assistant in a convenient and highly automated as well as interactive way. First, we formally establish upper and lower bounds for elementary functions. Then, based on these bounds, we develop a rational interval arithmetic where real number calculations take place in an algebraic setting. In order to reduce the dependency effect of interval arithmetic, we integrate two techniques: interval splitting and taylor series expansions. This pragmatic approach has been developed, and formally verified, in a theorem prover. The formal development also includes a set of customizable strategies to automate proofs involving explicit calculations over real numbers. Our ultimate goal is to provide guaranteed proofs of numerical properties with minimal human theorem-prover interaction.",
        "published": "2007-08-28T07:14:29Z",
        "link": "http://arxiv.org/abs/0708.3721v1",
        "categories": [
            "cs.MS",
            "cs.LO"
        ]
    },
    {
        "title": "Formally Verified Argument Reduction with a Fused-Multiply-Add",
        "authors": [
            "Sylvie Boldo",
            "Marc Daumas",
            "Ren Cang Li"
        ],
        "summary": "Cody & Waite argument reduction technique works perfectly for reasonably large arguments but as the input grows there are no bit left to approximate the constant with enough accuracy. Under mild assumptions, we show that the result computed with a fused-multiply-add provides a fully accurate result for many possible values of the input with a constant almost accurate to the full working precision. We also present an algorithm for a fully accurate second reduction step to reach double full accuracy (all the significand bits of two numbers are significant) even in the worst cases of argument reduction. Our work recalls the common algorithms and presents proofs of correctness. All the proofs are formally verified using the Coq automatic proof checker.",
        "published": "2007-08-28T07:15:08Z",
        "link": "http://arxiv.org/abs/0708.3722v1",
        "categories": [
            "cs.MS",
            "cs.PF"
        ]
    },
    {
        "title": "A Class of Parallel Tiled Linear Algebra Algorithms for Multicore   Architectures",
        "authors": [
            "Alfredo Buttari",
            "Julien Langou",
            "Jakub Kurzak",
            "Jack Dongarra"
        ],
        "summary": "As multicore systems continue to gain ground in the High Performance Computing world, linear algebra algorithms have to be reformulated or new algorithms have to be developed in order to take advantage of the architectural features on these new processors. Fine grain parallelism becomes a major requirement and introduces the necessity of loose synchronization in the parallel execution of an operation. This paper presents an algorithm for the Cholesky, LU and QR factorization where the operations can be represented as a sequence of small tasks that operate on square blocks of data. These tasks can be dynamically scheduled for execution based on the dependencies among them and on the availability of computational resources. This may result in an out of order execution of the tasks which will completely hide the presence of intrinsically sequential tasks in the factorization. Performance comparisons are presented with the LAPACK algorithms where parallelism can only be exploited at the level of the BLAS operations and vendor implementations.",
        "published": "2007-09-09T16:32:46Z",
        "link": "http://arxiv.org/abs/0709.1272v3",
        "categories": [
            "cs.MS",
            "cs.DC"
        ]
    },
    {
        "title": "Building the Tangent and Adjoint codes of the Ocean General Circulation   Model OPA with the Automatic Differentiation tool TAPENADE",
        "authors": [
            "Moulay Hicham Tber",
            "Laurent Hascoet",
            "Arthur Vidard",
            "Benjamin Dauvergne"
        ],
        "summary": "The ocean general circulation model OPA is developed by the LODYC team at Paris VI university. OPA has recently undergone a major rewriting, migrating to FORTRAN95, and its adjoint code needs to be rebuilt. For earlier versions, the adjoint of OPA was written by hand at a high development cost. We use the Automatic Differentiation tool TAPENADE to build mechanicaly the tangent and adjoint codes of OPA. We validate the differentiated codes by comparison with divided differences, and also with an identical twin experiment. We apply state-of-the-art methods to improve the performance of the adjoint code. In particular we implement the Griewank and Walther's binomial checkpointing algorithm which gives us an optimal trade-off between time and memory consumption. We apply a specific strategy to differentiate the iterative linear solver that comes from the implicit time stepping scheme",
        "published": "2007-11-28T08:04:18Z",
        "link": "http://arxiv.org/abs/0711.4444v2",
        "categories": [
            "cs.MS",
            "cs.CE"
        ]
    },
    {
        "title": "TRUST-TECH based Methods for Optimization and Learning",
        "authors": [
            "Chandan K. Reddy"
        ],
        "summary": "Many problems that arise in machine learning domain deal with nonlinearity and quite often demand users to obtain global optimal solutions rather than local optimal ones. Optimization problems are inherent in machine learning algorithms and hence many methods in machine learning were inherited from the optimization literature. Popularly known as the initialization problem, the ideal set of parameters required will significantly depend on the given initialization values. The recently developed TRUST-TECH (TRansformation Under STability-reTaining Equilibria CHaracterization) methodology systematically explores the subspace of the parameters to obtain a complete set of local optimal solutions. In this thesis work, we propose TRUST-TECH based methods for solving several optimization and machine learning problems. Two stages namely, the local stage and the neighborhood-search stage, are repeated alternatively in the solution space to achieve improvements in the quality of the solutions. Our methods were tested on both synthetic and real datasets and the advantages of using this novel framework are clearly manifested. This framework not only reduces the sensitivity to initialization, but also allows the flexibility for the practitioners to use various global and local methods that work well for a particular problem of interest. Other hierarchical stochastic algorithms like evolutionary algorithms and smoothing algorithms are also studied and frameworks for combining these methods with TRUST-TECH have been proposed and evaluated on several test systems.",
        "published": "2007-12-25T03:14:32Z",
        "link": "http://arxiv.org/abs/0712.4126v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.MS",
            "cs.NA",
            "cs.NE",
            "G.1.6; I.5.3; I.5.1"
        ]
    },
    {
        "title": "Maximum Entropy in the framework of Algebraic Statistics: A First Step",
        "authors": [
            "Ambedkar Dukkipati"
        ],
        "summary": "Algebraic statistics is a recently evolving field, where one would treat statistical models as algebraic objects and thereby use tools from computational commutative algebra and algebraic geometry in the analysis and computation of statistical models. In this approach, calculation of parameters of statistical models amounts to solving set of polynomial equations in several variables, for which one can use celebrated Grobner bases theory. Owing to the important role of information theory in statistics, this paper as a first step, explores the possibility of describing maximum and minimum entropy (ME) models in the framework of algebraic statistics. We show that ME-models are toric models (a class of algebraic statistical models) when the constraint functions (that provide the information about the underlying random variable) are integer valued functions, and the set of statistical models that results from ME-methods are indeed an affine variety.",
        "published": "2007-01-18T15:23:29Z",
        "link": "http://arxiv.org/abs/cs/0701117v2",
        "categories": [
            "cs.IT",
            "cs.SC",
            "math.IT"
        ]
    },
    {
        "title": "Signature Sequence of Intersection Curve of Two Quadrics for Exact   Morphological Classification",
        "authors": [
            "Changhe Tu",
            "Wenping Wang",
            "Bernard Mourrain",
            "Jiaye Wang"
        ],
        "summary": "We present an efficient method for classifying the morphology of the intersection curve of two quadrics (QSIC) in PR3, 3D real projective space; here, the term morphology is used in a broad sense to mean the shape, topological, and algebraic properties of a QSIC, including singularity, reducibility, the number of connected components, and the degree of each irreducible component, etc. There are in total 35 different QSIC morphologies with non-degenerate quadric pencils. For each of these 35 QSIC morphologies, through a detailed study of the eigenvalue curve and the index function jump we establish a characterizing algebraic condition expressed in terms of the Segre characteristics and the signature sequence of a quadric pencil. We show how to compute a signature sequence with rational arithmetic so as to determine the morphology of the intersection curve of any two given quadrics. Two immediate applications of our results are the robust topological classification of QSIC in computing B-rep surface representation in solid modeling and the derivation of algebraic conditions for collision detection of quadric primitives.",
        "published": "2007-01-19T08:08:24Z",
        "link": "http://arxiv.org/abs/cs/0701121v1",
        "categories": [
            "cs.CG",
            "cs.SC"
        ]
    },
    {
        "title": "Time- and Space-Efficient Evaluation of Some Hypergeometric Constants",
        "authors": [
            "Howard Cheng",
            "Guillaume Hanrot",
            "Emmanuel Thomé",
            "Eugene Zima",
            "Paul Zimmermann"
        ],
        "summary": "The currently best known algorithms for the numerical evaluation of hypergeometric constants such as $\\zeta(3)$ to $d$ decimal digits have time complexity $O(M(d) \\log^2 d)$ and space complexity of $O(d \\log d)$ or $O(d)$. Following work from Cheng, Gergel, Kim and Zima, we present a new algorithm with the same asymptotic complexity, but more efficient in practice. Our implementation of this algorithm improves slightly over existing programs for the computation of $\\pi$, and we announce a new record of 2 billion digits for $\\zeta(3)$.",
        "published": "2007-01-25T08:07:38Z",
        "link": "http://arxiv.org/abs/cs/0701151v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "A Prototype for Educational Planning Using Course Constraints to   Simulate Student Populations",
        "authors": [
            "T. Hadzilacos",
            "D. Kalles",
            "D. Koumanakos",
            "V. Mitsionis"
        ],
        "summary": "Distance learning universities usually afford their students the flexibility to advance their studies at their own pace. This can lead to a considerable fluctuation of student populations within a program's courses, possibly affecting the academic viability of a program as well as the related required resources. Providing a method that estimates this population could be of substantial help to university management and academic personnel. We describe how to use course precedence constraints to calculate alternative tuition paths and then use Markov models to estimate future populations. In doing so, we identify key issues of a large scale potential deployment.",
        "published": "2007-01-26T08:32:10Z",
        "link": "http://arxiv.org/abs/cs/0701174v3",
        "categories": [
            "cs.AI",
            "cs.CY",
            "cs.DS",
            "cs.SC"
        ]
    },
    {
        "title": "Certification of the QR factor R, and of lattice basis reducedness",
        "authors": [
            "Gilles Villard"
        ],
        "summary": "Given a lattice basis of n vectors in Z^n, we propose an algorithm using 12n^3+O(n^2) floating point operations for checking whether the basis is LLL-reduced. If the basis is reduced then the algorithm will hopefully answer ''yes''. If the basis is not reduced, or if the precision used is not sufficient with respect to n, and to the numerical properties of the basis, the algorithm will answer ''failed''. Hence a positive answer is a rigorous certificate. For implementing the certificate itself, we propose a floating point algorithm for computing (certified) error bounds for the entries of the R factor of the QR matrix factorization. This algorithm takes into account all possible approximation and rounding errors. The cost 12n^3+O(n^2) of the certificate is only six times more than the cost of numerical algorithms for computing the QR factorization itself, and the certificate may be implemented using matrix library routines only. We report experiments that show that for a reduced basis of adequate dimension and quality the certificate succeeds, and establish the effectiveness of the certificate. This effectiveness is applied for certifying the output of fastest existing floating point heuristics of LLL reduction, without slowing down the whole process.",
        "published": "2007-01-29T09:15:35Z",
        "link": "http://arxiv.org/abs/cs/0701183v1",
        "categories": [
            "cs.SC",
            "cs.NA",
            "I.1.2; F.2.1; G.1.3; G.4"
        ]
    },
    {
        "title": "Faster Inversion and Other Black Box Matrix Computations Using Efficient   Block Projections",
        "authors": [
            "Wayne Eberly",
            "Mark Giesbrecht",
            "Pascal Giorgi",
            "Arne Storjohann",
            "Gilles Villard"
        ],
        "summary": "Block projections have been used, in [Eberly et al. 2006], to obtain an efficient algorithm to find solutions for sparse systems of linear equations. A bound of softO(n^(2.5)) machine operations is obtained assuming that the input matrix can be multiplied by a vector with constant-sized entries in softO(n) machine operations. Unfortunately, the correctness of this algorithm depends on the existence of efficient block projections, and this has been conjectured. In this paper we establish the correctness of the algorithm from [Eberly et al. 2006] by proving the existence of efficient block projections over sufficiently large fields. We demonstrate the usefulness of these projections by deriving improved bounds for the cost of several matrix problems, considering, in particular, ``sparse'' matrices that can be be multiplied by a vector using softO(n) field operations. We show how to compute the inverse of a sparse matrix over a field F using an expected number of softO(n^(2.27)) operations in F. A basis for the null space of a sparse matrix, and a certification of its rank, are obtained at the same cost. An application to Kaltofen and Villard's Baby-Steps/Giant-Steps algorithms for the determinant and Smith Form of an integer matrix yields algorithms requiring softO(n^(2.66)) machine operations. The derived algorithms are all probabilistic of the Las Vegas type.",
        "published": "2007-01-29T18:20:30Z",
        "link": "http://arxiv.org/abs/cs/0701188v1",
        "categories": [
            "cs.SC",
            "cs.NA",
            "I.1.2; F.2.1; G.1.3; G.4"
        ]
    },
    {
        "title": "A canonical form for some piecewise defined functions",
        "authors": [
            "Jacques Carette"
        ],
        "summary": "We define a canonical form for piecewise defined functions. We show that this has a wider range of application as well as better complexity properties than previous work.",
        "published": "2007-02-01T17:54:50Z",
        "link": "http://arxiv.org/abs/cs/0702010v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "I.1.1"
        ]
    },
    {
        "title": "Towards a New ODE Solver Based on Cartan's Equivalence Method",
        "authors": [
            "R. Dridi",
            "M. Petitot"
        ],
        "summary": "The aim of the present paper is to propose an algorithm for a new ODE--solver which should improve the abilities of current solvers to handle second order differential equations. The paper provides also a theoretical result revealing the relationship between the change of coordinates, that maps the generic equation to a given target equation, and the symmetry $\\D$-groupoid of this target.",
        "published": "2007-02-10T16:54:12Z",
        "link": "http://arxiv.org/abs/cs/0702065v2",
        "categories": [
            "cs.SC",
            "I.1.2"
        ]
    },
    {
        "title": "The Multithreaded version of FORM",
        "authors": [
            "M. Tentyukov",
            "J. A. M. Vermaseren"
        ],
        "summary": "We present TFORM, the version of the symbolic manipulation system FORM that can make simultaneous use of several processors in a shared memory architecture. The implementation uses Posix threads, also called pthreads, and is therefore easily portable between various operating systems. Most existing FORM programs will be able to take advantage of the increased processing power, without the need for modifications. In some cases some minor additions may be needed. For a computer with two processors a typical improvement factor in the running time is 1.7 when compared to the traditional version of FORM. In the case of computers with 4 processors a typical improvement factor in the execution time is slightly above 3.",
        "published": "2007-02-27T16:35:00Z",
        "link": "http://arxiv.org/abs/hep-ph/0702279v1",
        "categories": [
            "hep-ph",
            "cs.SC"
        ]
    },
    {
        "title": "Formal proof for delayed finite field arithmetic using floating point   operators",
        "authors": [
            "Sylvie Boldo",
            "Marc Daumas",
            "Pascal Giorgi"
        ],
        "summary": "Formal proof checkers such as Coq are capable of validating proofs of correction of algorithms for finite field arithmetics but they require extensive training from potential users. The delayed solution of a triangular system over a finite field mixes operations on integers and operations on floating point numbers. We focus in this report on verifying proof obligations that state that no round off error occurred on any of the floating point operations. We use a tool named Gappa that can be learned in a matter of minutes to generate proofs related to floating point arithmetic and hide technicalities of formal proof checkers. We found that three facilities are missing from existing tools. The first one is the ability to use in Gappa new lemmas that cannot be easily expressed as rewriting rules. We coined the second one ``variable interchange'' as it would be required to validate loop interchanges. The third facility handles massive loop unrolling and argument instantiation by generating traces of execution for a large number of cases. We hope that these facilities may sometime in the future be integrated into mainstream code validation.",
        "published": "2007-03-06T14:55:44Z",
        "link": "http://arxiv.org/abs/cs/0703026v3",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Differential Equations for Algebraic Functions",
        "authors": [
            "Alin Bostan",
            "Frédéric Chyzak",
            "Bruno Salvy",
            "Grégoire Lecerf",
            "Éric Schost"
        ],
        "summary": "It is classical that univariate algebraic functions satisfy linear differential equations with polynomial coefficients. Linear recurrences follow for the coefficients of their power series expansions. We show that the linear differential equation of minimal order has coefficients whose degree is cubic in the degree of the function. We also show that there exists a linear differential equation of order linear in the degree whose coefficients are only of quadratic degree. Furthermore, we prove the existence of recurrences of order and degree close to optimal. We study the complexity of computing these differential equations and recurrences. We deduce a fast algorithm for the expansion of algebraic series.",
        "published": "2007-03-23T19:20:35Z",
        "link": "http://arxiv.org/abs/cs/0703121v2",
        "categories": [
            "cs.SC",
            "math.CA"
        ]
    },
    {
        "title": "The on-line shortest path problem under partial monitoring",
        "authors": [
            "Andras Gyorgy",
            "Tamas Linder",
            "Gabor Lugosi",
            "Gyorgy Ottucsak"
        ],
        "summary": "The on-line shortest path problem is considered under various models of partial monitoring. Given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. In a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. For this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on the number of edges of the graph. The algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges. An extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. Another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. A version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. Applications to routing in packet switched networks along with simulation results are also presented.",
        "published": "2007-04-08T10:15:54Z",
        "link": "http://arxiv.org/abs/0704.1020v1",
        "categories": [
            "cs.LG",
            "cs.SC",
            "C.2.1; C.2.2; F.1.1; I.2.6; I.2.8"
        ]
    },
    {
        "title": "The Invar Tensor Package",
        "authors": [
            "Jose M. Martin-Garcia",
            "Renato Portugal",
            "Leon R. U. Manssur"
        ],
        "summary": "The Invar package is introduced, a fast manipulator of generic scalar polynomial expressions formed from the Riemann tensor of a four-dimensional metric-compatible connection. The package can maximally simplify any polynomial containing tensor products of up to seven Riemann tensors within seconds. It has been implemented both in Mathematica and Maple algebraic systems.",
        "published": "2007-04-13T13:03:59Z",
        "link": "http://arxiv.org/abs/0704.1756v1",
        "categories": [
            "cs.SC",
            "gr-qc",
            "hep-th"
        ]
    },
    {
        "title": "Parallel computation of the rank of large sparse matrices from algebraic   K-theory",
        "authors": [
            "Jean-Guillaume Dumas",
            "Philippe Elbaz-Vincent",
            "Pascal Giorgi",
            "Anna Urbanska"
        ],
        "summary": "This paper deals with the computation of the rank and of some integer Smith forms of a series of sparse matrices arising in algebraic K-theory. The number of non zero entries in the considered matrices ranges from 8 to 37 millions. The largest rank computation took more than 35 days on 50 processors. We report on the actual algorithms we used to build the matrices, their link to the motivic cohomology and the linear algebra and parallelizations required to perform such huge computations. In particular, these results are part of the first computation of the cohomology of the linear group GL_7(Z).",
        "published": "2007-04-18T14:29:28Z",
        "link": "http://arxiv.org/abs/0704.2351v2",
        "categories": [
            "math.KT",
            "cs.DC",
            "cs.SC",
            "math.NT"
        ]
    },
    {
        "title": "Towards an exact adaptive algorithm for the determinant of a rational   matrix",
        "authors": [
            "Anna Urbanska"
        ],
        "summary": "In this paper we propose several strategies for the exact computation of the determinant of a rational matrix. First, we use the Chinese Remaindering Theorem and the rational reconstruction to recover the rational determinant from its modular images. Then we show a preconditioning for the determinant which allows us to skip the rational reconstruction process and reconstruct an integer result. We compare those approaches with matrix preconditioning which allow us to treat integer instead of rational matrices. This allows us to introduce integer determinant algorithms to the rational determinant problem. In particular, we discuss the applicability of the adaptive determinant algorithm of [9] and compare it with the integer Chinese Remaindering scheme. We present an analysis of the complexity of the strategies and evaluate their experimental performance on numerous examples. This experience allows us to develop an adaptive strategy which would choose the best solution at the run time, depending on matrix properties. All strategies have been implemented in LinBox linear algebra library.",
        "published": "2007-05-31T20:23:08Z",
        "link": "http://arxiv.org/abs/0706.0014v1",
        "categories": [
            "cs.SC",
            "I.1.2"
        ]
    },
    {
        "title": "Tropical Implicitization and Mixed Fiber Polytopes",
        "authors": [
            "Bernd Sturmfels",
            "Josephine Yu"
        ],
        "summary": "The software TrIm offers implementations of tropical implicitization and tropical elimination, as developed by Tevelev and the authors. Given a polynomial map with generic coefficients, TrIm computes the tropical variety of the image. When the image is a hypersurface, the output is the Newton polytope of the defining polynomial. TrIm can thus be used to compute mixed fiber polytopes, including secondary polytopes.",
        "published": "2007-06-05T00:53:37Z",
        "link": "http://arxiv.org/abs/0706.0564v2",
        "categories": [
            "cs.SC",
            "math.AG",
            "math.CO",
            "14Q10, 52B20, 52B55, 65D18"
        ]
    },
    {
        "title": "A Proof of a Recursion for Bessel Moments",
        "authors": [
            "Jonathan M. Borwein",
            "Bruno Salvy"
        ],
        "summary": "We provide a proof of a conjecture in (Bailey, Borwein, Borwein, Crandall 2007) on the existence and form of linear recursions for moments of powers of the Bessel function $K_0$.",
        "published": "2007-06-11T06:45:39Z",
        "link": "http://arxiv.org/abs/0706.1409v2",
        "categories": [
            "cs.SC",
            "math.CA"
        ]
    },
    {
        "title": "Obstructions to Genericity in Study of Parametric Problems in Control   Theory",
        "authors": [
            "Viktor Levandovskyy",
            "Eva Zerz"
        ],
        "summary": "We investigate systems of equations, involving parameters from the point of view of both control theory and computer algebra. The equations might involve linear operators such as partial (q-)differentiation, (q-)shift, (q-)difference as well as more complicated ones, which act trivially on the parameters. Such a system can be identified algebraically with a certain left module over a non-commutative algebra, where the operators commute with the parameters. We develop, implement and use in practice the algorithm for revealing all the expressions in parameters, for which e.g. homological properties of a system differ from the generic properties. We use Groebner bases and Groebner basics in rings of solvable type as main tools. In particular, we demonstrate an optimized algorithm for computing the left inverse of a matrix over a ring of solvable type. We illustrate the article with interesting examples. In particular, we provide a complete solution to the \"two pendula, mounted on a cart\" problem from the classical book of Polderman and Willems, including the case, where the friction at the joints is essential . To the best of our knowledge, the latter example has not been solved before in a complete way.",
        "published": "2007-08-15T19:12:15Z",
        "link": "http://arxiv.org/abs/0708.2078v1",
        "categories": [
            "math.OC",
            "cs.SC",
            "math.RA",
            "13P10; 93B25"
        ]
    },
    {
        "title": "Moderate Growth Time Series for Dynamic Combinatorics Modelisation",
        "authors": [
            "Luaï Jaff",
            "Gérard H. E. Duchamp",
            "Hatem Hadj Kacem",
            "Cyrille Bertelle"
        ],
        "summary": "Here, we present a family of time series with a simple growth constraint. This family can be the basis of a model to apply to emerging computation in business and micro-economy where global functions can be expressed from local rules. We explicit a double statistics on these series which allows to establish a one-to-one correspondence between three other ballot-like strunctures.",
        "published": "2007-08-16T14:58:35Z",
        "link": "http://arxiv.org/abs/0708.2213v1",
        "categories": [
            "cs.SC",
            "cs.MA",
            "math.CO"
        ]
    },
    {
        "title": "Bounding the Betti numbers and computing the Euler-Poincaré   characteristic of semi-algebraic sets defined by partly quadratic systems of   polynomials",
        "authors": [
            "Saugata Basu",
            "Dmitrii V. Pasechnik",
            "Marie-Francoise Roy"
        ],
        "summary": "Let $\\R$ be a real closed field, $ {\\mathcal Q} \\subset \\R[Y_1,...,Y_\\ell,X_1,...,X_k], $ with $ \\deg_{Y}(Q) \\leq 2, \\deg_{X}(Q) \\leq d, Q \\in {\\mathcal Q}, #({\\mathcal Q})=m,$ and $ {\\mathcal P} \\subset \\R[X_1,...,X_k] $ with $\\deg_{X}(P) \\leq d, P \\in {\\mathcal P}, #({\\mathcal P})=s$, and $S \\subset \\R^{\\ell+k}$ a semi-algebraic set defined by a Boolean formula without negations, with atoms $P=0, P \\geq 0, P \\leq 0, P \\in {\\mathcal P} \\cup {\\mathcal Q}$. We prove that the sum of the Betti numbers of $S$ is bounded by \\[ \\ell^2 (O(s+\\ell+m)\\ell d)^{k+2m}. \\] This is a common generalization of previous results on bounding the Betti numbers of closed semi-algebraic sets defined by polynomials of degree $d$ and 2, respectively.   We also describe an algorithm for computing the Euler-Poincar\\'e characteristic of such sets, generalizing similar algorithms known before. The complexity of the algorithm is bounded by $(\\ell s m d)^{O(m(m+k))}$.",
        "published": "2007-08-27T01:31:17Z",
        "link": "http://arxiv.org/abs/0708.3522v2",
        "categories": [
            "math.AG",
            "cs.SC",
            "math.AT",
            "math.GT",
            "14P10, 14P25 (Primary), 68W30 (Secondary)"
        ]
    },
    {
        "title": "Implicitization of Bihomogeneous Parametrizations of Algebraic Surfaces   via Linear Syzygies",
        "authors": [
            "Laurent Busé",
            "Marc Dohm"
        ],
        "summary": "We show that the implicit equation of a surface in 3-dimensional projective space parametrized by bi-homogeneous polynomials of bi-degree (d,d), for a given positive integer d, can be represented and computed from the linear syzygies of its parametrization if the base points are isolated and form locally a complete intersection.",
        "published": "2007-08-30T20:31:00Z",
        "link": "http://arxiv.org/abs/0708.4230v1",
        "categories": [
            "math.AG",
            "cs.SC",
            "math.AC"
        ]
    },
    {
        "title": "Improved Linear Parallel Interference Cancellers",
        "authors": [
            "T. Srikanth",
            "K. Vishnu Vardhan",
            "A. Chockalingam",
            "L. B. Milstein"
        ],
        "summary": "In this paper, taking the view that a linear parallel interference canceller (LPIC) can be seen as a linear matrix filter, we propose new linear matrix filters that can result in improved bit error performance compared to other LPICs in the literature. The motivation for the proposed filters arises from the possibility of avoiding the generation of certain interference and noise terms in a given stage that would have been present in a conventional LPIC (CLPIC). In the proposed filters, we achieve such avoidance of the generation of interference and noise terms in a given stage by simply making the diagonal elements of a certain matrix in that stage equal to zero. Hence, the proposed filters do not require additional complexity compared to the CLPIC, and they can allow achieving a certain error performance using fewer LPIC stages. We also extend the proposed matrix filter solutions to a multicarrier DS-CDMA system, where we consider two types of receivers. In one receiver (referred to as Type-I receiver), LPIC is performed on each subcarrier first, followed by multicarrier combining (MCC). In the other receiver (called Type-II receiver), MCC is performed first, followed by LPIC. We show that in both Type-I and Type-II receivers, the proposed matrix filters outperform other matrix filters. Also, Type-II receiver performs better than Type-I receiver because of enhanced accuracy of the interference estimates achieved due to frequency diversity offered by MCC.",
        "published": "2007-09-14T07:56:21Z",
        "link": "http://arxiv.org/abs/0709.2225v1",
        "categories": [
            "cs.IT",
            "cs.SC",
            "cs.SD",
            "cs.SE",
            "math.IT"
        ]
    },
    {
        "title": "An Extension to an Algebraic Method for Linear Time-Invariant System and   Network Theory: The full AC-Calculus",
        "authors": [
            "Eberhard H. -A. Gerbracht"
        ],
        "summary": "Being inspired by phasor analysis in linear circuit theory, and its algebraic counterpart - the AC-(operational)-calculus for sinusoids developed by W. Marten and W. Mathis - we define a complex structure on several spaces of real-valued elementary functions. This is used to algebraize inhomogeneous linear ordinary differential equations with inhomogenities stemming from these spaces. Thus we deduce an effective method to calculate particular solutions of these ODEs in a purely algebraic way.",
        "published": "2007-09-19T00:17:40Z",
        "link": "http://arxiv.org/abs/0709.2935v1",
        "categories": [
            "math.CA",
            "cs.SC",
            "34A05 (Primary); 26A09, 44A40, 68W30, 93C05, 94C05 (Secondary)"
        ]
    },
    {
        "title": "Q-adic Transform revisited",
        "authors": [
            "Jean-Guillaume Dumas"
        ],
        "summary": "We present an algorithm to perform a simultaneous modular reduction of several residues. This algorithm is applied fast modular polynomial multiplication. The idea is to convert the $X$-adic representation of modular polynomials, with $X$ an indeterminate, to a $q$-adic representation where $q$ is an integer larger than the field characteristic. With some control on the different involved sizes it is then possible to perform some of the $q$-adic arithmetic directly with machine integers or floating points. Depending also on the number of performed numerical operations one can then convert back to the $q$-adic or $X$-adic representation and eventually mod out high residues. In this note we present a new version of both conversions: more tabulations and a way to reduce the number of divisions involved in the process are presented. The polynomial multiplication is then applied to arithmetic in small finite field extensions.",
        "published": "2007-10-02T12:02:07Z",
        "link": "http://arxiv.org/abs/0710.0510v5",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Differential invariants of a Lie group action: syzygies on a generating   set",
        "authors": [
            "Evelyne Hubert"
        ],
        "summary": "Given a group action, known by its infinitesimal generators, we exhibit a complete set of syzygies on a generating set of differential invariants. For that we elaborate on the reinterpretation of Cartan's moving frame by Fels and Olver (1999). This provides constructive tools for exploring algebras of differential invariants.",
        "published": "2007-10-23T19:20:10Z",
        "link": "http://arxiv.org/abs/0710.4318v4",
        "categories": [
            "cs.SC",
            "math.DG"
        ]
    },
    {
        "title": "A Numerical Algorithm for Zero Counting. I: Complexity and Accuracy",
        "authors": [
            "Felipe Cucker",
            "Teresa Krick",
            "Gregorio Malajovich",
            "Mario Wschebor"
        ],
        "summary": "We describe an algorithm to count the number of distinct real zeros of a polynomial (square) system f. The algorithm performs O(n D kappa(f)) iterations where n is the number of polynomials (as well as the dimension of the ambient space), D is a bound on the polynomials' degree, and kappa(f) is a condition number for the system. Each iteration uses an exponential number of operations. The algorithm uses finite-precision arithmetic and a polynomial bound for the precision required to ensure the returned output is correct is exhibited. This bound is a major feature of our algorithm since it is in contrast with the exponential precision required by the existing (symbolic) algorithms for counting real zeros. The algorithm parallelizes well in the sense that each iteration can be computed in parallel polynomial time with an exponential number of processors.",
        "published": "2007-10-24T16:33:07Z",
        "link": "http://arxiv.org/abs/0710.4508v2",
        "categories": [
            "cs.CC",
            "cs.NA",
            "cs.SC",
            "math.NA",
            "F.2.1; G.1; I.1.2"
        ]
    },
    {
        "title": "SWI-Prolog and the Web",
        "authors": [
            "Jan Wielemaker",
            "Zhisheng Huang",
            "Lourens van der Meij"
        ],
        "summary": "Where Prolog is commonly seen as a component in a Web application that is either embedded or communicates using a proprietary protocol, we propose an architecture where Prolog communicates to other components in a Web application using the standard HTTP protocol. By avoiding embedding in external Web servers development and deployment become much easier. To support this architecture, in addition to the transfer protocol, we must also support parsing, representing and generating the key Web document types such as HTML, XML and RDF.   This paper motivates the design decisions in the libraries and extensions to Prolog for handling Web documents and protocols. The design has been guided by the requirement to handle large documents efficiently. The described libraries support a wide range of Web applications ranging from HTML and XML documents to Semantic Web RDF processing.   To appear in Theory and Practice of Logic Programming (TPLP)",
        "published": "2007-11-06T16:22:39Z",
        "link": "http://arxiv.org/abs/0711.0917v1",
        "categories": [
            "cs.PL",
            "cs.SC"
        ]
    },
    {
        "title": "Some properties of finite meadows",
        "authors": [
            "Inge Bethke",
            "Piet Rodenburg"
        ],
        "summary": "The aim of this note is to describe the structure of finite meadows. We will show that the class of finite meadows is the closure of the class of finite fields under finite products. As a corollary, we obtain a unique representation of minimal meadows in terms of prime fields.",
        "published": "2007-12-06T11:27:44Z",
        "link": "http://arxiv.org/abs/0712.0917v1",
        "categories": [
            "math.RA",
            "cs.SC"
        ]
    },
    {
        "title": "On the equations of the moving curve ideal of a rational algebraic plane   curve",
        "authors": [
            "Laurent Busé"
        ],
        "summary": "Given a parametrization of a rational plane algebraic curve C, some explicit adjoint pencils on C are described in terms of determinants. Moreover, some generators of the Rees algebra associated to this parametrization are presented. The main ingredient developed in this paper is a detailed study of the elimination ideal of two homogeneous polynomials in two homogeneous variables that form a regular sequence.",
        "published": "2007-12-17T10:12:33Z",
        "link": "http://arxiv.org/abs/0712.2671v2",
        "categories": [
            "math.AG",
            "cs.SC",
            "math.AC"
        ]
    },
    {
        "title": "Faster polynomial multiplication via multipoint Kronecker substitution",
        "authors": [
            "David Harvey"
        ],
        "summary": "We give several new algorithms for dense polynomial multiplication based on the Kronecker substitution method. For moderately sized input polynomials, the new algorithms improve on the performance of the standard Kronecker substitution by a sizeable constant, both in theory and in empirical tests.",
        "published": "2007-12-25T04:57:04Z",
        "link": "http://arxiv.org/abs/0712.4046v1",
        "categories": [
            "cs.SC",
            "cs.DS"
        ]
    },
    {
        "title": "Introduction to the Galois Theory of Linear Differential Equations",
        "authors": [
            "Michael F. Singer"
        ],
        "summary": "This is an expanded version of the 10 lectures given as the 2006 London Mathematical Society Invited Lecture Series at the Heriot-Watt University 31 July - 4 August 2006.",
        "published": "2007-12-26T02:40:19Z",
        "link": "http://arxiv.org/abs/0712.4124v2",
        "categories": [
            "math.CA",
            "cs.SC",
            "12H05 (Primary) 34M50, 12H20 (Secondary)"
        ]
    },
    {
        "title": "Computer algebra in systems biology",
        "authors": [
            "Reinhard Laubenbacher",
            "Bernd Sturmfels"
        ],
        "summary": "Systems biology focuses on the study of entire biological systems rather than on their individual components. With the emergence of high-throughput data generation technologies for molecular biology and the development of advanced mathematical modeling techniques, this field promises to provide important new insights. At the same time, with the availability of increasingly powerful computers, computer algebra has developed into a useful tool for many applications. This article illustrates the use of computer algebra in systems biology by way of a well-known gene regulatory network, the Lac Operon in the bacterium E. coli.",
        "published": "2007-12-27T16:01:35Z",
        "link": "http://arxiv.org/abs/0712.4248v2",
        "categories": [
            "cs.SC",
            "q-bio.MN",
            "q-bio.QM"
        ]
    },
    {
        "title": "Asynchronous Implementation of Failure Detectors with partial   connectivity and unknown participants",
        "authors": [
            "Pierre Sens",
            "Luciana Arantes",
            "Mathieu Bouillaguet",
            "Véronique Martin",
            "Fabiola Greve"
        ],
        "summary": "We consider the problem of failure detection in dynamic networks such as MANETs. Unreliable failure detectors are classical mechanisms which provide information about process failures. However, most of current implementations consider that the network is fully connected and that the initial number of nodes of the system is known. This assumption is not applicable to dynamic environments. Furthermore, such implementations are usually timer-based while in dynamic networks there is no upper bound for communication delays since nodes can move. This paper presents an asynchronous implementation of a failure detector for unknown and mobile networks. Our approach does not rely on timers and neither the composition nor the number of nodes in the system are known. We prove that our algorithm can implement failure detectors of class <>S when behavioral properties and connectivity conditions are satisfied by the underlying system.",
        "published": "2007-01-03T12:52:58Z",
        "link": "http://arxiv.org/abs/cs/0701015v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "DMTCP: Transparent Checkpointing for Cluster Computations and the   Desktop",
        "authors": [
            "Jason Ansel",
            "Kapil Arya",
            "Gene Cooperman"
        ],
        "summary": "DMTCP (Distributed MultiThreaded CheckPointing) is a transparent user-level checkpointing package for distributed applications. Checkpointing and restart is demonstrated for a wide range of over 20 well known applications, including MATLAB, Python, TightVNC, MPICH2, OpenMPI, and runCMS. RunCMS runs as a 680 MB image in memory that includes 540 dynamic libraries, and is used for the CMS experiment of the Large Hadron Collider at CERN. DMTCP transparently checkpoints general cluster computations consisting of many nodes, processes, and threads; as well as typical desktop applications. On 128 distributed cores (32 nodes), checkpoint and restart times are typically 2 seconds, with negligible run-time overhead. Typical checkpoint times are reduced to 0.2 seconds when using forked checkpointing. Experimental results show that checkpoint time remains nearly constant as the number of nodes increases on a medium-size cluster.   DMTCP automatically accounts for fork, exec, ssh, mutexes/semaphores, TCP/IP sockets, UNIX domain sockets, pipes, ptys (pseudo-terminals), terminal modes, ownership of controlling terminals, signal handlers, open file descriptors, shared open file descriptors, I/O (including the readline library), shared memory (via mmap), parent-child process relationships, pid virtualization, and other operating system artifacts. By emphasizing an unprivileged, user-space approach, compatibility is maintained across Linux kernels from 2.6.9 through the current 2.6.28. Since DMTCP is unprivileged and does not require special kernel modules or kernel patches, DMTCP can be incorporated and distributed as a checkpoint-restart module within some larger package.",
        "published": "2007-01-06T11:36:50Z",
        "link": "http://arxiv.org/abs/cs/0701037v3",
        "categories": [
            "cs.DC",
            "cs.OS",
            "D.4.5"
        ]
    },
    {
        "title": "Causing Communication Closure: Safe Program Composition with Reliable   Non-FIFO Channels",
        "authors": [
            "Kai Engelhardt",
            "Yoram Moses"
        ],
        "summary": "A semantic framework for analyzing safe composition of distributed programs is presented. Its applicability is illustrated by a study of program composition when communication is reliable but not necessarily FIFO\\@. In this model, special care must be taken to ensure that messages do not accidentally overtake one another in the composed program. We show that barriers do not exist in this model. Indeed, no program that sends or receives messages can automatically be composed with arbitrary programs without jeopardizing their intended behavior. Safety of composition becomes context-sensitive and new tools are needed for ensuring it. A notion of \\emph{sealing} is defined, where if a program $P$ is immediately followed by a program $Q$ that seals $P$ then $P$ will be communication-closed--it will execute as if it runs in isolation. The investigation of sealing in this model reveals a novel connection between Lamport causality and safe composition. A characterization of sealable programs is given, as well as efficient algorithms for testing if $Q$ seals $P$ and for constructing a seal for a significant class of programs. It is shown that every sealable program that is open to interference on $O(n^2)$ channels can be sealed using O(n) messages.",
        "published": "2007-01-09T12:18:14Z",
        "link": "http://arxiv.org/abs/cs/0701064v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Open-architecture Implementation of Fragment Molecular Orbital Method   for Peta-scale Computing",
        "authors": [
            "Toshiya Takami",
            "Jun Maki",
            "Jun-ichi Ooba",
            "Yuichi Inadomi",
            "Hiroaki Honda",
            "Taizo Kobayashi",
            "Rie Nogita",
            "Mutsumi Aoyagi"
        ],
        "summary": "We present our perspective and goals on highperformance computing for nanoscience in accordance with the global trend toward \"peta-scale computing.\" After reviewing our results obtained through the grid-enabled version of the fragment molecular orbital method (FMO) on the grid testbed by the Japanese Grid Project, National Research Grid Initiative (NAREGI), we show that FMO is one of the best candidates for peta-scale applications by predicting its effective performance in peta-scale computers. Finally, we introduce our new project constructing a peta-scale application in an open-architecture implementation of FMO in order to realize both goals of highperformance in peta-scale computers and extendibility to multiphysics simulations.",
        "published": "2007-01-11T04:49:00Z",
        "link": "http://arxiv.org/abs/cs/0701075v1",
        "categories": [
            "cs.DC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Browser-based distributed evolutionary computation: performance and   scaling behavior",
        "authors": [
            "J. J. Merelo",
            "Antonio Mora-Garcia",
            "J. L. J. Laredo",
            "Juan Lupion",
            "Fernando Tricas"
        ],
        "summary": "The challenge of ad-hoc computing is to find the way of taking advantage of spare cycles in an efficient way that takes into account all capabilities of the devices and interconnections available to them. In this paper we explore distributed evolutionary computation based on the Ruby on Rails framework, which overlays a Model-View-Controller on evolutionary computation. It allows anybody with a web browser (that is, mostly everybody connected to the Internet) to participate in an evolutionary computation experiment. Using a straightforward farming model, we consider different factors, such as the size of the population used. We are mostly interested in how they impact on performance, but also the scaling behavior when a non-trivial number of computers is applied to the problem. Experiments show the impact of different packet sizes on performance, as well as a quite limited scaling behavior, due to the characteristics of the server. Several solutions for that problem are proposed.",
        "published": "2007-01-18T09:23:29Z",
        "link": "http://arxiv.org/abs/cs/0701115v1",
        "categories": [
            "cs.DC",
            "cs.NE"
        ]
    },
    {
        "title": "Byzantine Fault Tolerance for Nondeterministic Applications",
        "authors": [
            "Wenbing Zhao"
        ],
        "summary": "All practical applications contain some degree of nondeterminism. When such applications are replicated to achieve Byzantine fault tolerance (BFT), their nondeterministic operations must be controlled to ensure replica consistency. To the best of our knowledge, only the most simplistic types of replica nondeterminism have been dealt with. Furthermore, there lacks a systematic approach to handling common types of nondeterminism. In this paper, we propose a classification of common types of replica nondeterminism with respect to the requirement of achieving Byzantine fault tolerance, and describe the design and implementation of the core mechanisms necessary to handle such nondeterminism within a Byzantine fault tolerance framework.",
        "published": "2007-01-21T20:44:52Z",
        "link": "http://arxiv.org/abs/cs/0701134v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Scatter of Weak Robots",
        "authors": [
            "Yoann Dieudonné",
            "Franck Petit"
        ],
        "summary": "In this paper, we first formalize the problem to be solved, i.e., the Scatter Problem (SP). We then show that SP cannot be deterministically solved. Next, we propose a randomized algorithm for this problem. The proposed solution is trivially self-stabilizing. We then show how to design a self-stabilizing version of any deterministic solution for the Pattern Formation and the Gathering problems.",
        "published": "2007-01-27T06:43:04Z",
        "link": "http://arxiv.org/abs/cs/0701179v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A Peer-to-Peer Browsable File Index using a Popularity Based Global   Namespace",
        "authors": [
            "Thomas Jacobs",
            "Aaron Harwood"
        ],
        "summary": "The distribution of files using decentralized, peer-to-peer (P2P) systems, has significant advantages over centralized approaches. It is however more difficult to settle on the best approach for file sharing. Most file sharing systems are based on query string searches, leading to a relatively simple but inefficient broadcast or to an efficient but relatively complicated index in a structured environment. In this paper we use a browsable peer-to-peer file index consisting of files which serve as directory nodes, interconnecting to form a directory network. We implemented the system based on BitTorrent and Kademlia. The directory network inherits all of the advantages of decentralization and provides browsable, efficient searching. To avoid conflict between users in the P2P system while also imposing no additional restrictions, we allow multiple versions of each directory node to simultaneously exist -- using popularity as the basis for default browsing behavior. Users can freely add files and directory nodes to the network. We show, using a simulation of user behavior and file quality, that the popularity based system consistently leads users to a high quality directory network; above the average quality of user updates. Q",
        "published": "2007-01-30T04:00:30Z",
        "link": "http://arxiv.org/abs/cs/0701190v1",
        "categories": [
            "cs.DC",
            "cs.NI",
            "C.2.4"
        ]
    },
    {
        "title": "A New Self-Stabilizing Maximal Matching Algorithm",
        "authors": [
            "Fredrik Manne",
            "Morten Mjelde",
            "Laurence Pilard",
            "Sébastien Tixeuil"
        ],
        "summary": "The maximal matching problem has received considerable attention in the self-stabilizing community. Previous work has given different self-stabilizing algorithms that solves the problem for both the adversarial and fair distributed daemon, the sequential adversarial daemon, as well as the synchronous daemon. In the following we present a single self-stabilizing algorithm for this problem that unites all of these algorithms in that it stabilizes in the same number of moves as the previous best algorithms for the sequential adversarial, the distributed fair, and the synchronous daemon. In addition, the algorithm improves the previous best moves complexities for the distributed adversarial daemon from O(n^2) and O(delta m) to O(m) where n is the number of processes, m is thenumber of edges, and delta is the maximum degree in the graph.",
        "published": "2007-01-30T14:52:37Z",
        "link": "http://arxiv.org/abs/cs/0701189v1",
        "categories": [
            "cs.DS",
            "cs.DC"
        ]
    },
    {
        "title": "A Formal Model for Programming Wireless Sensor Networks",
        "authors": [
            "Luis Lopes",
            "Francisco Martins",
            "Miguel S. Silva",
            "Joao Barros"
        ],
        "summary": "In this paper we present new developments in the expressiveness and in the theory of a Calculus for Sensor Networks (CSN). We combine a network layer of sensor devices with a local object model to describe sensor devices with state. The resulting calculus is quite small and yet very expressive. We also present a type system and a type invariance result for the calculus. These results provide the fundamental framework for the development of programming languages and run-time environments.",
        "published": "2007-02-07T14:17:29Z",
        "link": "http://arxiv.org/abs/cs/0702042v1",
        "categories": [
            "cs.DC",
            "cs.PL"
        ]
    },
    {
        "title": "Comments on \"Design and performance evaluation of load distribution   strategies for multiple loads on heterogeneous linear daisy chain networks''",
        "authors": [
            "Matthieu Gallet",
            "Yves Robert",
            "Frédéric Vivien"
        ],
        "summary": "Min, Veeravalli, and Barlas proposed strategies to minimize the overall execution time of one or several divisible loads on a heterogeneous linear network, using one or more installments. We show on a very simple example that the proposed approach does not always produce a solution and that, when it does, the solution is often suboptimal. We also show how to find an optimal scheduling for any instance, once the number of installments per load is given. Finally, we formally prove that under a linear cost model, as in the original paper, an optimal schedule has an infinite number of installments. Such a cost model can therefore not be sed to design practical multi-installment strategies.",
        "published": "2007-02-10T17:43:35Z",
        "link": "http://arxiv.org/abs/cs/0702066v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A First Step Towards Automatically Building Network Representations",
        "authors": [
            "Lionel Eyraud-Dubois",
            "Arnaud Legrand",
            "Martin Quinson",
            "Frédéric Vivien"
        ],
        "summary": "To fully harness Grids, users or middlewares must have some knowledge on the topology of the platform interconnection network. As such knowledge is usually not available, one must uses tools which automatically build a topological network model through some measurements. In this article, we define a methodology to assess the quality of these network model building tools, and we apply this methodology to representatives of the main classes of model builders and to two new algorithms. We show that none of the main existing techniques build models that enable to accurately predict the running time of simple application kernels for actual platforms. However some of the new algorithms we propose give excellent results in a wide range of situations.",
        "published": "2007-02-13T16:35:04Z",
        "link": "http://arxiv.org/abs/cs/0702076v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Social Behaviours Applied to P2P Systems: An efficient Algorithm for   Resource Organisation",
        "authors": [
            "V. Carchiolo",
            "M. Malgeri",
            "G. Mangioni",
            "V. Nicosia"
        ],
        "summary": "P2P systems are a great solution to the problem of distributing resources. The main issue of P2P networks is that searching and retrieving resources shared by peers is usually expensive and does not take into account similarities among peers. In this paper we present preliminary simulations of PROSA, a novel algorithm for P2P network structuring, inspired by social behaviours. Peers in PROSA self--organise in social groups of similar peers, called ``semantic--groups'', depending on the resources they are sharing. Such a network smoothly evolves to a small--world graph, where queries for resources are efficiently and effectively routed.",
        "published": "2007-02-14T11:53:14Z",
        "link": "http://arxiv.org/abs/cs/0702085v1",
        "categories": [
            "cs.DC",
            "cs.IR"
        ]
    },
    {
        "title": "Fast Computation of Small Cuts via Cycle Space Sampling",
        "authors": [
            "David Pritchard",
            "Ramakrishna Thurimella"
        ],
        "summary": "We describe a new sampling-based method to determine cuts in an undirected graph. For a graph (V, E), its cycle space is the family of all subsets of E that have even degree at each vertex. We prove that with high probability, sampling the cycle space identifies the cuts of a graph. This leads to simple new linear-time sequential algorithms for finding all cut edges and cut pairs (a set of 2 edges that form a cut) of a graph.   In the model of distributed computing in a graph G=(V, E) with O(log V)-bit messages, our approach yields faster algorithms for several problems. The diameter of G is denoted by Diam, and the maximum degree by Delta. We obtain simple O(Diam)-time distributed algorithms to find all cut edges, 2-edge-connected components, and cut pairs, matching or improving upon previous time bounds. Under natural conditions these new algorithms are universally optimal --- i.e. a Omega(Diam)-time lower bound holds on every graph. We obtain a O(Diam+Delta/log V)-time distributed algorithm for finding cut vertices; this is faster than the best previous algorithm when Delta, Diam = O(sqrt(V)). A simple extension of our work yields the first distributed algorithm with sub-linear time for 3-edge-connected components. The basic distributed algorithms are Monte Carlo, but they can be made Las Vegas without increasing the asymptotic complexity.   In the model of parallel computing on the EREW PRAM our approach yields a simple algorithm with optimal time complexity O(log V) for finding cut pairs and 3-edge-connected components.",
        "published": "2007-02-20T03:00:33Z",
        "link": "http://arxiv.org/abs/cs/0702113v5",
        "categories": [
            "cs.DC",
            "cs.DS",
            "F.1.2; F.2.2; G.2.2; G.3"
        ]
    },
    {
        "title": "Nearest Neighbor Network Traversal",
        "authors": [
            "David Pritchard"
        ],
        "summary": "A mobile agent in a network wants to visit every node of an n-node network, using a small number of steps. We investigate the performance of the following ``nearest neighbor'' heuristic: always go to the nearest unvisited node. If the network graph never changes, then from (Rosenkrantz, Stearns and Lewis, 1977) and (Hurkens and Woeginger, 2004) it follows that Theta(n log n) steps are necessary and sufficient in the worst case. We give a simpler proof of the upper bound and an example that improves the best known lower bound.   We investigate how the performance of this heuristic changes when it is distributively implemented in a network. Even if network edges are allow to fail over time, we show that the nearest neighbor strategy never runs for more than O(n^2) iterations. We also show that any strategy can be forced to take at least n(n-1)/2 steps before all nodes are visited, if the edges of the network are deleted in an adversarial way.",
        "published": "2007-02-20T03:54:12Z",
        "link": "http://arxiv.org/abs/cs/0702114v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Efficient Searching and Retrieval of Documents in PROSA",
        "authors": [
            "V. Nicosia",
            "G. Mangioni",
            "V. Carchiolo",
            "M. Malgeri"
        ],
        "summary": "Retrieving resources in a distributed environment is more difficult than finding data in centralised databases. In the last decade P2P system arise as new and effective distributed architectures for resource sharing, but searching in such environments could be difficult and time-consuming. In this paper we discuss efficiency of resource discovery in PROSA, a self-organising P2P system heavily inspired by social networks. All routing choices in PROSA are made locally, looking only at the relevance of the next peer to each query. We show that PROSA is able to effectively answer queries for rare documents, forwarding them through the most convenient path to nodes that much probably share matching resources. This result is heavily related to the small-world structure that naturally emerges in PROSA.",
        "published": "2007-02-22T10:22:39Z",
        "link": "http://arxiv.org/abs/cs/0702126v1",
        "categories": [
            "cs.DC",
            "cs.IR"
        ]
    },
    {
        "title": "Exploiting social networks dynamics for P2P resource organisation",
        "authors": [
            "V. Nicosia",
            "G. Mangioni",
            "V. Carchiolo",
            "M. Malgeri"
        ],
        "summary": "In this paper we present a formal description of PROSA, a P2P resource management system heavily inspired by social networks. Social networks have been deeply studied in the last two decades in order to understand how communities of people arise and grow. It is a widely known result that networks of social relationships usually evolves to small-worlds, i.e. networks where nodes are strongly connected to neighbours and separated from all other nodes by a small amount of hops. This work shows that algorithms implemented into PROSA allow to obtain an efficient small-world P2P network.",
        "published": "2007-02-22T10:35:50Z",
        "link": "http://arxiv.org/abs/cs/0702127v1",
        "categories": [
            "cs.DC",
            "cs.IR"
        ]
    },
    {
        "title": "AICA: a New Pair Force Evaluation Method for Parallel Molecular Dynamics   in Arbitrary Geometries",
        "authors": [
            "Graham B. Macpherson",
            "Jason M. Reese"
        ],
        "summary": "A new algorithm for calculating intermolecular pair forces in Molecular Dynamics (MD) simulations on a distributed parallel computer is presented. The Arbitrary Interacting Cells Algorithm (AICA) is designed to operate on geometrical domains defined by an unstructured, arbitrary polyhedral mesh, which has been spatially decomposed into irregular portions for parallelisation. It is intended for nano scale fluid mechanics simulation by MD in complex geometries, and to provide the MD component of a hybrid MD/continuum simulation. AICA has been implemented in the open-source computational toolbox OpenFOAM, and verified against a published MD code.",
        "published": "2007-02-22T17:27:05Z",
        "link": "http://arxiv.org/abs/cs/0702131v1",
        "categories": [
            "cs.CE",
            "cs.DC"
        ]
    },
    {
        "title": "Designing a Resource Broker for Heterogeneous Grids",
        "authors": [
            "Srikumar Venugopal",
            "Krishna Nadiminti",
            "Hussein Gibbins",
            "Rajkumar Buyya"
        ],
        "summary": "Grids provide uniform access to aggregations of heterogeneous resources and services such as computers, networks and storage owned by multiple organizations. However, such a dynamic environment poses many challenges for application composition and deployment. In this paper, we present the design of the Gridbus Grid resource broker that allows users to create applications and specify different objectives through different interfaces without having to deal with the complexity of Grid infrastructure. We present the unique requirements that motivated our design and discuss how these provide flexibility in extending the functionality of the broker to support different low-level middlewares and user interfaces. We evaluate the broker with different job profiles and Grid middleware and conclude with the lessons learnt from our development experience.",
        "published": "2007-02-24T08:35:02Z",
        "link": "http://arxiv.org/abs/cs/0702145v1",
        "categories": [
            "cs.DC",
            "cs.SE",
            "C.2.4; D.2.11"
        ]
    },
    {
        "title": "In Search of Simplicity: A Self-Organizing Multi-Source Multicast   Overlay",
        "authors": [
            "Matei Ripeanu",
            "Adriana Iamnitchi",
            "Ian Foster",
            "Anne Rogers"
        ],
        "summary": "Multicast communication primitives have broad utility as building blocks for distributed applications. The challenge is to create and maintain the distributed structures that support these primitives while accounting for volatile end nodes and variable network characteristics. Most solutions proposed to date rely on complex algorithms or global information, thus limiting the scale of deployments and acceptance outside the academic realm. This article introduces a low-complexity, self organizing solution for maintaining multicast trees, that we refer to as UMM (Unstructured Multi-source Multicast). UMM uses traditional distributed systems techniques: layering, soft-state, and passive data collection to adapt to the dynamics of the physical network and maintain data dissemination trees. The result is a simple, adaptive system with lower overheads than more complex alternatives. We have implemented UMM and evaluated it on a 100-node PlanetLab testbed and on up to 1024-node emulated ModelNet networks Extensive experimental evaluations demonstrate UMM's low overhead, efficient network usage compared to alternative solutions, and ability to quickly adapt to network changes and to recover from failures.",
        "published": "2007-02-27T19:42:40Z",
        "link": "http://arxiv.org/abs/cs/0702157v1",
        "categories": [
            "cs.DC",
            "cs.NI",
            "cs.PF"
        ]
    },
    {
        "title": "Graphic processors to speed-up simulations for the design of high   performance solar receptors",
        "authors": [
            "Sylvain Collange",
            "Marc Daumas",
            "David Defour"
        ],
        "summary": "Graphics Processing Units (GPUs) are now powerful and flexible systems adapted and used for other purposes than graphics calculations (General Purpose computation on GPU -- GPGPU). We present here a prototype to be integrated into simulation codes that estimate temperature, velocity and pressure to design next generations of solar receptors. Such codes will delegate to our contribution on GPUs the computation of heat transfers due to radiations. We use Monte-Carlo line-by-line ray-tracing through finite volumes. This means data-parallel arithmetic transformations on large data structures. Our prototype is inspired on the source code of GPUBench. Our performances on two recent graphics cards (Nvidia 7800GTX and ATI RX1800XL) show some speed-up higher than 400 compared to CPU implementations leaving most of CPU computing resources available. As there were some questions pending about the accuracy of the operators implemented in GPUs, we start this report with a survey and some contributed tests on the various floating point units available on GPUs.",
        "published": "2007-03-06T19:58:17Z",
        "link": "http://arxiv.org/abs/cs/0703028v2",
        "categories": [
            "cs.DC",
            "physics.class-ph"
        ]
    },
    {
        "title": "Towards Distributed Petascale Computing",
        "authors": [
            "A. G. Hoekstra",
            "S. F. Portegies Zwart",
            "M. Bubak",
            "P. M. A. Sloot"
        ],
        "summary": "In this chapter we will argue that studying such multi-scale multi-science systems gives rise to inherently hybrid models containing many different algorithms best serviced by different types of computing environments (ranging from massively parallel computers, via large-scale special purpose machines to clusters of PC's) whose total integrated computing capacity can easily reach the PFlop/s scale. Such hybrid models, in combination with the by now inherently distributed nature of the data on which the models `feed' suggest a distributed computing model, where parts of the multi-scale multi-science model are executed on the most suitable computing environment, and/or where the computations are carried out close to the required data (i.e. bring the computations to the data instead of the other way around). We presents an estimate for the compute requirements to simulate the Galaxy as a typical example of a multi-scale multi-physics application, requiring distributed Petaflop/s computational power.",
        "published": "2007-03-19T13:25:03Z",
        "link": "http://arxiv.org/abs/astro-ph/0703485v1",
        "categories": [
            "astro-ph",
            "cs.DC"
        ]
    },
    {
        "title": "Geographic Routing Around Obstacles in Wireless Sensor Networks",
        "authors": [
            "Olivier Powell",
            "Sotiris Nikolesteas"
        ],
        "summary": "Geographic routing is becoming the protocol of choice for many sensor network applications. The current state of the art is unsatisfactory: some algorithms are very efficient, however they require a preliminary planarization of the communication graph. Planarization induces overhead and is not realistic in many scenarios. On the otherhand, georouting algorithms which do not rely on planarization have fairly low success rates and either fail to route messages around all but the simplest obstacles or have a high topology control overhead (e.g. contour detection algorithms). To overcome these limitations, we propose GRIC, the first lightweight and efficient on demand (i.e. all-to-all) geographic routing algorithm which does not require planarization and has almost 100% delivery rates (when no obstacles are added). Furthermore, the excellent behavior of our algorithm is maintained even in the presence of large convex obstacles. The case of hard concave obstacles is also studied; such obstacles are hard instances for which performance diminishes.",
        "published": "2007-03-20T12:17:57Z",
        "link": "http://arxiv.org/abs/cs/0703094v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Approximation Algorithms for Multiprocessor Scheduling under Uncertainty",
        "authors": [
            "Guolong Lin",
            "Rajmohan Rajaraman"
        ],
        "summary": "Motivated by applications in grid computing and project management, we study multiprocessor scheduling in scenarios where there is uncertainty in the successful execution of jobs when assigned to processors. We consider the problem of multiprocessor scheduling under uncertainty, in which we are given n unit-time jobs and m machines, a directed acyclic graph C giving the dependencies among the jobs, and for every job j and machine i, the probability p_{ij} of the successful completion of job j when scheduled on machine i in any given particular step. The goal of the problem is to find a schedule that minimizes the expected makespan, that is, the expected completion time of all the jobs.   The problem of multiprocessor scheduling under uncertainty was introduced by Malewicz and was shown to be NP-hard even when all the jobs are independent. In this paper, we present polynomial-time approximation algorithms for the problem, for special cases of the dag C. We obtain an O(log(n))-approximation for the case of independent jobs, an O(log(m)log(n)log(n+m)/loglog(n+m))-approximation when C is a collection of disjoint chains, an O(log(m)log^2(n))-approximation when C is a collection of directed out- or in-trees, and an O(log(m)log^2(n)log(n+m)/loglog(n+m))-approximation when C is a directed forest.",
        "published": "2007-03-21T20:35:40Z",
        "link": "http://arxiv.org/abs/cs/0703100v1",
        "categories": [
            "cs.DC",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "User-level DSM System for Modern High-Performance Interconnection   Networks",
        "authors": [
            "Bharath Ramesh",
            "Srinidhi Varadarajan"
        ],
        "summary": "In this paper, we introduce a new user-level DSM system which has the ability to directly interact with underlying interconnection networks. The DSM system provides the application programmer a flexible API to program parallel applications either using shared memory semantics over physically distributed memory or to use an efficient remote memory demand paging technique. We also introduce a new time slice based memory consistency protocol which is used by the DSM system. We present preliminary results from our implementation on a small Opteron Linux cluster interconnected over Myrinet.",
        "published": "2007-03-22T19:15:00Z",
        "link": "http://arxiv.org/abs/cs/0703112v1",
        "categories": [
            "cs.DC",
            "C.2.4; D.1.3; D.4.2"
        ]
    },
    {
        "title": "Self-adaptive Gossip Policies for Distributed Population-based   Algorithms",
        "authors": [
            "J. L. J. Laredo",
            "E. A. Eiben",
            "M. Schoenauer",
            "P. A. Castillo",
            "A. M. Mora",
            "F. Fernandez",
            "J. J. Merelo"
        ],
        "summary": "Gossipping has demonstrate to be an efficient mechanism for spreading information among P2P networks. Within the context of P2P computing, we propose the so-called Evolvable Agent Model for distributed population-based algorithms which uses gossipping as communication policy, and represents every individual as a self-scheduled single thread. The model avoids obsolete nodes in the population by defining a self-adaptive refresh rate which depends on the latency and bandwidth of the network. Such a mechanism balances the migration rate to the congestion of the links pursuing global population coherence. We perform an experimental evaluation of this model on a real parallel system and observe how solution quality and algorithm speed scale with the number of processors with this seamless approach.",
        "published": "2007-03-23T11:29:10Z",
        "link": "http://arxiv.org/abs/cs/0703117v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Rapid Almost-Complete Broadcasting in Faulty Networks",
        "authors": [
            "Rastislav Královič",
            "Richard Královič"
        ],
        "summary": "This paper studies the problem of broadcasting in synchronous point-to-point networks, where one initiator owns a piece of information that has to be transmitted to all other vertices as fast as possible. The model of fractional dynamic faults with threshold is considered: in every step either a fixed number $T$, or a fraction $\\alpha$, of sent messages can be lost depending on which quantity is larger.   As the main result we show that in complete graphs and hypercubes it is possible to inform all but a constant number of vertices, exhibiting only a logarithmic slowdown, i.e. in time $O(D\\log n)$ where $D$ is the diameter of the network and $n$ is the number of vertices.   Moreover, for complete graphs under some additional conditions (sense of direction, or $\\alpha<0.55$) the remaining constant number of vertices can be informed in the same time, i.e. $O(\\log n)$.",
        "published": "2007-03-23T22:38:06Z",
        "link": "http://arxiv.org/abs/cs/0703122v1",
        "categories": [
            "cs.DC",
            "C.2.1"
        ]
    },
    {
        "title": "ReSHAPE: A Framework for Dynamic Resizing and Scheduling of Homogeneous   Applications in a Parallel Environment",
        "authors": [
            "Rajesh Sudarsan",
            "Calvin J. Ribbens"
        ],
        "summary": "Applications in science and engineering often require huge computational resources for solving problems within a reasonable time frame. Parallel supercomputers provide the computational infrastructure for solving such problems. A traditional application scheduler running on a parallel cluster only supports static scheduling where the number of processors allocated to an application remains fixed throughout the lifetime of execution of the job. Due to the unpredictability in job arrival times and varying resource requirements, static scheduling can result in idle system resources thereby decreasing the overall system throughput. In this paper we present a prototype framework called ReSHAPE, which supports dynamic resizing of parallel MPI applications executed on distributed memory platforms. The framework includes a scheduler that supports resizing of applications, an API to enable applications to interact with the scheduler, and a library that makes resizing viable. Applications executed using the ReSHAPE scheduler framework can expand to take advantage of additional free processors or can shrink to accommodate a high priority application, without getting suspended. In our research, we have mainly focused on structured applications that have two-dimensional data arrays distributed across a two-dimensional processor grid. The resize library includes algorithms for processor selection and processor mapping. Experimental results show that the ReSHAPE framework can improve individual job turn-around time and overall system throughput.",
        "published": "2007-03-27T23:27:54Z",
        "link": "http://arxiv.org/abs/cs/0703137v1",
        "categories": [
            "cs.DC",
            "C.1.4"
        ]
    },
    {
        "title": "Transaction-Oriented Simulation In Ad Hoc Grids",
        "authors": [
            "Gerald Krafft"
        ],
        "summary": "This paper analyses the possibilities of performing parallel transaction-oriented simulations with a special focus on the space-parallel approach and discrete event simulation synchronisation algorithms that are suitable for transaction-oriented simulation and the target environment of Ad Hoc Grids. To demonstrate the findings a Java-based parallel transaction-oriented simulator for the simulation language GPSS/H is implemented on the basis of the promising Shock Resistant Time Warp synchronisation algorithm and using the Grid framework ProActive. The validation of this parallel simulator shows that the Shock Resistant Time Warp algorithm can successfully reduce the number of rolled back Transaction moves but it also reveals circumstances in which the Shock Resistant Time Warp algorithm can be outperformed by the normal Time Warp algorithm. The conclusion of this paper suggests possible improvements to the Shock Resistant Time Warp algorithm to avoid such problems.",
        "published": "2007-04-06T15:59:27Z",
        "link": "http://arxiv.org/abs/0704.1827v1",
        "categories": [
            "cs.DC",
            "I.6.3; I.6.1; I.6.7"
        ]
    },
    {
        "title": "Parallel computing for the finite element method",
        "authors": [
            "Christian Vollaire",
            "Laurent Nicolas",
            "Alain Nicolas"
        ],
        "summary": "A finite element method is presented to compute time harmonic microwave fields in three dimensional configurations. Nodal-based finite elements have been coupled with an absorbing boundary condition to solve open boundary problems. This paper describes how the modeling of large devices has been made possible using parallel computation, New algorithms are then proposed to implement this formulation on a cluster of workstations (10 DEC ALPHA 300X) and on a CRAY C98. Analysis of the computation efficiency is performed using simple problems. The electromagnetic scattering of a plane wave by a perfect electric conducting airplane is finally given as example.",
        "published": "2007-04-18T13:20:25Z",
        "link": "http://arxiv.org/abs/0704.2344v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Parallel computation of the rank of large sparse matrices from algebraic   K-theory",
        "authors": [
            "Jean-Guillaume Dumas",
            "Philippe Elbaz-Vincent",
            "Pascal Giorgi",
            "Anna Urbanska"
        ],
        "summary": "This paper deals with the computation of the rank and of some integer Smith forms of a series of sparse matrices arising in algebraic K-theory. The number of non zero entries in the considered matrices ranges from 8 to 37 millions. The largest rank computation took more than 35 days on 50 processors. We report on the actual algorithms we used to build the matrices, their link to the motivic cohomology and the linear algebra and parallelizations required to perform such huge computations. In particular, these results are part of the first computation of the cohomology of the linear group GL_7(Z).",
        "published": "2007-04-18T14:29:28Z",
        "link": "http://arxiv.org/abs/0704.2351v2",
        "categories": [
            "math.KT",
            "cs.DC",
            "cs.SC",
            "math.NT"
        ]
    },
    {
        "title": "A Nice Labelling for Tree-Like Event Structures of Degree 3",
        "authors": [
            "Luigi Santocanale"
        ],
        "summary": "We address the problem of &#64257;nding nice labellings for event structures of degree 3. We develop a minimum theory by which we prove that the labelling number of an event structure of degree 3 is bounded by a linear function of the height. The main theorem we present in this paper states that event structures of degree 3 whose causality order is a tree have a nice labelling with 3 colors. Finally, we exemplify how to use this theorem to construct upper bounds for the labelling number of other event structures of degree 3.",
        "published": "2007-04-18T14:39:09Z",
        "link": "http://arxiv.org/abs/0704.2355v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Lower Bounds on Implementing Robust and Resilient Mediators",
        "authors": [
            "Ittai Abraham",
            "Danny Dolev",
            "Joseph Y. Halpern"
        ],
        "summary": "We consider games that have (k,t)-robust equilibria when played with a mediator, where an equilibrium is (k,t)-robust if it tolerates deviations by coalitions of size up to k and deviations by up to $t$ players with unknown utilities. We prove lower bounds that match upper bounds on the ability to implement such mediators using cheap talk (that is, just allowing communication among the players). The bounds depend on (a) the relationship between k, t, and n, the total number of players in the system; (b) whether players know the exact utilities of other players; (c) whether there are broadcast channels or just point-to-point channels; (d) whether cryptography is available; and (e) whether the game has a $k+t)-punishment strategy; that is, a strategy that, if used by all but at most $k+t$ players, guarantees that every player gets a worse outcome than they do with the equilibrium strategy.",
        "published": "2007-04-27T01:32:15Z",
        "link": "http://arxiv.org/abs/0704.3646v2",
        "categories": [
            "cs.GT",
            "cs.CR",
            "cs.DC"
        ]
    },
    {
        "title": "An algorithm for clock synchronization with the gradient property in   sensor networks",
        "authors": [
            "Rodolfo M. Pussente",
            "Valmir C. Barbosa"
        ],
        "summary": "We introduce a distributed algorithm for clock synchronization in sensor networks. Our algorithm assumes that nodes in the network only know their immediate neighborhoods and an upper bound on the network's diameter. Clock-synchronization messages are only sent as part of the communication, assumed reasonably frequent, that already takes place among nodes. The algorithm has the gradient property of [2], achieving an O(1) worst-case skew between the logical clocks of neighbors. As in the case of [3,8], the algorithm's actions are such that no constant lower bound exists on the rate at which logical clocks progress in time, and for this reason the lower bound of [2,5] that forbids constant skew between neighbors does not apply.",
        "published": "2007-04-30T19:59:14Z",
        "link": "http://arxiv.org/abs/0704.3890v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Mean Field Models of Message Throughput in Dynamic Peer-to-Peer Systems",
        "authors": [
            "Aaron Harwood",
            "Olga Ohrimenko"
        ],
        "summary": "The churn rate of a peer-to-peer system places direct limitations on the rate at which messages can be effectively communicated to a group of peers. These limitations are independent of the topology and message transmission latency. In this paper we consider a peer-to-peer network, based on the Engset model, where peers arrive and depart independently at random. We show how the arrival and departure rates directly limit the capacity for message streams to be broadcast to all other peers, by deriving mean field models that accurately describe the system behavior. Our models cover the unit and more general k buffer cases, i.e. where a peer can buffer at most k messages at any one time, and we give results for both single and multi-source message streams. We define coverage rate as peer-messages per unit time, i.e. the rate at which a number of peers receive messages, and show that the coverage rate is limited by the churn rate and buffer size. Our theory introduces an Instantaneous Message Exchange (IME) model and provides a template for further analysis of more complicated systems. Using the IME model, and assuming random processes, we have obtained very accurate equations of the system dynamics in a variety of interesting cases, that allow us to tune a peer-to-peer system. It remains to be seen if we can maintain this accuracy for general processes and when applying a non-instantaneous model.",
        "published": "2007-05-15T01:42:44Z",
        "link": "http://arxiv.org/abs/0705.2065v1",
        "categories": [
            "cs.DC",
            "cs.PF"
        ]
    },
    {
        "title": "TrustMIX: Trustworthy MIX for Energy Saving in Sensor Networks",
        "authors": [
            "Olivier Powell",
            "Luminita Moraru",
            "Jean-Marc Seigneur"
        ],
        "summary": "MIX has recently been proposed as a new sensor scheme with better energy management for data-gathering in Wireless Sensor Networks. However, it is not known how it performs when some of the sensors carry out sinkhole attacks. In this paper, we propose a variant of MIX with adjunct computational trust management to limit the impact of such sinkhole attacks. We evaluate how MIX resists sinkhole attacks with and without computational trust management. The main result of this paper is to find that MIX is very vulnerable to sinkhole attacks but that the adjunct trust management efficiently reduces the impact of such attacks while preserving the main feature of MIX: increased lifetime of the network.",
        "published": "2007-05-16T09:22:21Z",
        "link": "http://arxiv.org/abs/0705.2313v1",
        "categories": [
            "cs.DC",
            "cs.CR",
            "cs.NI",
            "C.2.4"
        ]
    },
    {
        "title": "An Extensible Timing Infrastructure for Adaptive Large-scale   Applications",
        "authors": [
            "Dylan Stark",
            "Gabrielle Allen",
            "Tom Goodale",
            "Thomas Radke",
            "Erik Schnetter"
        ],
        "summary": "Real-time access to accurate and reliable timing information is necessary to profile scientific applications, and crucial as simulations become increasingly complex, adaptive, and large-scale. The Cactus Framework provides flexible and extensible capabilities for timing information through a well designed infrastructure and timing API. Applications built with Cactus automatically gain access to built-in timers, such as gettimeofday and getrusage, system-specific hardware clocks, and high-level interfaces such as PAPI. We describe the Cactus timer interface, its motivation, and its implementation. We then demonstrate how this timing information can be used by an example scientific application to profile itself, and to dynamically adapt itself to a changing environment at run time.",
        "published": "2007-05-21T19:00:25Z",
        "link": "http://arxiv.org/abs/0705.3015v1",
        "categories": [
            "cs.PF",
            "cs.DC"
        ]
    },
    {
        "title": "Power-Efficient Direct-Voting Assurance for Data Fusion in Wireless   Sensor Networks",
        "authors": [
            "H. -T. Pai",
            "Y. S. Han"
        ],
        "summary": "Wireless sensor networks place sensors into an area to collect data and send them back to a base station. Data fusion, which fuses the collected data before they are sent to the base station, is usually implemented over the network. Since the sensor is typically placed in locations accessible to malicious attackers, information assurance of the data fusion process is very important. A witness-based approach has been proposed to validate the fusion data. In this approach, the base station receives the fusion data and \"votes\" on the data from a randomly chosen sensor node. The vote comes from other sensor nodes, called \"witnesses,\" to verify the correctness of the fusion data. Because the base station obtains the vote through the chosen node, the chosen node could forge the vote if it is compromised. Thus, the witness node must encrypt the vote to prevent this forgery. Compared with the vote, the encryption requires more bits, increasing transmission burden from the chosen node to the base station. The chosen node consumes more power. This work improves the witness-based approach using direct voting mechanism such that the proposed scheme has better performance in terms of assurance, overhead, and delay. The witness node transmits the vote directly to the base station. Forgery is not a problem in this scheme. Moreover, fewer bits are necessary to represent the vote, significantly reducing the power consumption. Performance analysis and simulation results indicate that the proposed approach can achieve a 40 times better overhead than the witness-based approach.",
        "published": "2007-05-25T02:56:47Z",
        "link": "http://arxiv.org/abs/0705.3683v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Small Worlds: Strong Clustering in Wireless Networks",
        "authors": [
            "Matthias R. Brust",
            "Steffen Rothkugel"
        ],
        "summary": "Small-worlds represent efficient communication networks that obey two distinguishing characteristics: a high clustering coefficient together with a small characteristic path length. This paper focuses on an interesting paradox, that removing links in a network can increase the overall clustering coefficient. Reckful Roaming, as introduced in this paper, is a 2-localized algorithm that takes advantage of this paradox in order to selectively remove superfluous links, this way optimizing the clustering coefficient while still retaining a sufficiently small characteristic path length.",
        "published": "2007-06-07T19:42:51Z",
        "link": "http://arxiv.org/abs/0706.1063v2",
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "WACA: A Hierarchical Weighted Clustering Algorithm optimized for Mobile   Hybrid Networks",
        "authors": [
            "Matthias R. Brust",
            "Adrian Andronache",
            "Steffen Rothkugel"
        ],
        "summary": "Clustering techniques create hierarchal network structures, called clusters, on an otherwise flat network. In a dynamic environment-in terms of node mobility as well as in terms of steadily changing device parameters-the clusterhead election process has to be re-invoked according to a suitable update policy. Cluster re-organization causes additional message exchanges and computational complexity and it execution has to be optimized. Our investigations focus on the problem of minimizing clusterhead re-elections by considering stability criteria. These criteria are based on topological characteristics as well as on device parameters. This paper presents a weighted clustering algorithm optimized to avoid needless clusterhead re-elections for stable clusters in mobile ad-hoc networks. The proposed localized algorithm deals with mobility, but does not require geographical, speed or distances information.",
        "published": "2007-06-07T20:47:19Z",
        "link": "http://arxiv.org/abs/0706.1080v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Localized Support for Injection Point Election in Hybrid Networks",
        "authors": [
            "Matthias R. Brust",
            "Steffen Rothkugel"
        ],
        "summary": "Ad-hoc networks, a promising trend in wireless technology, fail to work properly in a global setting. In most cases, self-organization and cost-free local communication cannot compensate the need for being connected, gathering urgent information just-in-time. Equipping mobile devices additionally with GSM or UMTS adapters in order to communicate with arbitrary remote devices or even a fixed network infrastructure provides an opportunity. Devices that operate as intermediate nodes between the ad-hoc network and a reliable backbone network are potential injection points. They allow disseminating received information within the local neighborhood. The effectiveness of different devices to serve as injection point differs substantially. For practical reasons the determination of injection points should be done locally, within the ad-hoc network partitions. We analyze different localized algorithms using at most 2-hop neighboring information. Results show that devices selected this way spread information more efficiently through the ad-hoc network. Our results can also be applied in order to support the election process for clusterheads in the field of clustering mechanisms.",
        "published": "2007-06-08T09:36:41Z",
        "link": "http://arxiv.org/abs/0706.1142v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "A taxonomic Approach to Topology Control in Ad-hoc and Wireless Networks",
        "authors": [
            "Matthias R. Brust",
            "Steffen Rothkugel"
        ],
        "summary": "Topology Control (TC) aims at tuning the topology of highly dynamic networks to provide better control over network resources and to increase the efficiency of communication. Recently, many TC protocols have been proposed. The protocols are designed for preserving connectivity, minimizing energy consumption, maximizing the overall network coverage or network capacity. Each TC protocol makes different assumptions about the network topology, environment detection resources, and control capacities. This circumstance makes it extremely difficult to comprehend the role and purpose of each protocol. To tackle this situation, a taxonomy for TC protocols is presented throughout this paper. Additionally, some TC protocols are classified based upon this taxonomy.",
        "published": "2007-06-08T10:20:29Z",
        "link": "http://arxiv.org/abs/0706.1151v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "C.2.1; H.3.3"
        ]
    },
    {
        "title": "Non-Cooperative Scheduling of Multiple Bag-of-Task Applications",
        "authors": [
            "Arnaud Legrand",
            "Corinne Touati"
        ],
        "summary": "Multiple applications that execute concurrently on heterogeneous platforms compete for CPU and network resources. In this paper we analyze the behavior of $K$ non-cooperative schedulers using the optimal strategy that maximize their efficiency while fairness is ensured at a system level ignoring applications characteristics. We limit our study to simple single-level master-worker platforms and to the case where each scheduler is in charge of a single application consisting of a large number of independent tasks. The tasks of a given application all have the same computation and communication requirements, but these requirements can vary from one application to another. In this context, we assume that each scheduler aims at maximizing its throughput. We give closed-form formula of the equilibrium reached by such a system and study its performance. We characterize the situations where this Nash equilibrium is optimal (in the Pareto sense) and show that even though no catastrophic situation (Braess-like paradox) can occur, such an equilibrium can be arbitrarily bad for any classical performance measure.",
        "published": "2007-06-12T06:39:33Z",
        "link": "http://arxiv.org/abs/0706.1614v1",
        "categories": [
            "cs.DC",
            "cs.GT"
        ]
    },
    {
        "title": "Building Portable Thread Schedulers for Hierarchical Multiprocessors:   the BubbleSched Framework",
        "authors": [
            "Samuel Thibault",
            "Raymond Namyst",
            "Pierre-André Wacrenier"
        ],
        "summary": "Exploiting full computational power of current more and more hierarchical multiprocessor machines requires a very careful distribution of threads and data among the underlying non-uniform architecture. Unfortunately, most operating systems only provide a poor scheduling API that does not allow applications to transmit valuable scheduling hints to the system. In a previous paper, we showed that using a bubble-based thread scheduler can significantly improve applications' performance in a portable way. However, since multithreaded applications have various scheduling requirements, there is no universal scheduler that could meet all these needs. In this paper, we present a framework that allows scheduling experts to implement and experiment with customized thread schedulers. It provides a powerful API for dynamically distributing bubbles among the machine in a high-level, portable, and efficient way. Several examples show how experts can then develop, debug and tune their own portable bubble schedulers.",
        "published": "2007-06-14T09:35:30Z",
        "link": "http://arxiv.org/abs/0706.2069v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Efficient Multidimensional Data Redistribution for Resizable Parallel   Computations",
        "authors": [
            "Rajesh Sudarsan",
            "Calvin J. Ribbens"
        ],
        "summary": "Traditional parallel schedulers running on cluster supercomputers support only static scheduling, where the number of processors allocated to an application remains fixed throughout the execution of the job. This results in under-utilization of idle system resources thereby decreasing overall system throughput. In our research, we have developed a prototype framework called ReSHAPE, which supports dynamic resizing of parallel MPI applications executing on distributed memory platforms. The resizing library in ReSHAPE includes support for releasing and acquiring processors and efficiently redistributing application state to a new set of processors. In this paper, we derive an algorithm for redistributing two-dimensional block-cyclic arrays from $P$ to $Q$ processors, organized as 2-D processor grids. The algorithm ensures a contention-free communication schedule for data redistribution if $P_r \\leq Q_r$ and $P_c \\leq Q_c$. In other cases, the algorithm implements circular row and column shifts on the communication schedule to minimize node contention.",
        "published": "2007-06-14T15:54:10Z",
        "link": "http://arxiv.org/abs/0706.2146v1",
        "categories": [
            "cs.DC",
            "D.1.3; G.1.0"
        ]
    },
    {
        "title": "Dualheap Selection Algorithm: Efficient, Inherently Parallel and   Somewhat Mysterious",
        "authors": [
            "Greg Sepesi"
        ],
        "summary": "An inherently parallel algorithm is proposed that efficiently performs selection: finding the K-th largest member of a set of N members. Selection is a common component of many more complex algorithms and therefore is a widely studied problem.   Not much is new in the proposed dualheap selection algorithm: the heap data structure is from J.W.J.Williams, the bottom-up heap construction is from R.W. Floyd, and the concept of a two heap data structure is from J.W.J. Williams and D.E. Knuth. The algorithm's novelty is limited to a few relatively minor implementation twists: 1) the two heaps are oriented with their roots at the partition values rather than at the minimum and maximum values, 2)the coding of one of the heaps (the heap of smaller values) employs negative indexing, and 3) the exchange phase of the algorithm is similar to a bottom-up heap construction, but navigates the heap with a post-order tree traversal.   When run on a single processor, the dualheap selection algorithm's performance is competitive with quickselect with median estimation, a common variant of C.A.R. Hoare's quicksort algorithm. When run on parallel processors, the dualheap selection algorithm is superior due to its subtasks that are easily partitioned and innately balanced.",
        "published": "2007-06-14T16:11:24Z",
        "link": "http://arxiv.org/abs/0706.2155v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DC",
            "C.1.4; F.2.0; G.2.2"
        ]
    },
    {
        "title": "Dualheap Sort Algorithm: An Inherently Parallel Generalization of   Heapsort",
        "authors": [
            "Greg Sepesi"
        ],
        "summary": "A generalization of the heapsort algorithm is proposed. At the expense of about 50% more comparison and move operations for typical cases, the dualheap sort algorithm offers several advantages over heapsort: improved cache performance, better performance if the input happens to be already sorted, and easier parallel implementations.",
        "published": "2007-06-20T14:42:45Z",
        "link": "http://arxiv.org/abs/0706.2893v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DC"
        ]
    },
    {
        "title": "A Generic Deployment Framework for Grid Computing and Distributed   Applications",
        "authors": [
            "Areski Flissi",
            "Philippe Merle"
        ],
        "summary": "Deployment of distributed applications on large systems, and especially on grid infrastructures, becomes a more and more complex task. Grid users spend a lot of time to prepare, install and configure middleware and application binaries on nodes, and eventually start their applications. The problem is that the deployment process is composed of many heterogeneous tasks that have to be orchestrated in a specific correct order. As a consequence, the automatization of the deployment process is currently very difficult to reach. To address this problem, we propose in this paper a generic deployment framework allowing to automatize the execution of heterogeneous tasks composing the whole deployment process. Our approach is based on a reification as software components of all required deployment mechanisms or existing tools. Grid users only have to describe the configuration to deploy in a simple natural language instead of programming or scripting how the deployment process is executed. As a toy example, this framework is used to deploy CORBA component-based applications and OpenCCM middleware on one thousand nodes of the French Grid5000 infrastructure.",
        "published": "2007-06-20T15:17:47Z",
        "link": "http://arxiv.org/abs/0706.3008v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "N-Body Simulations on GPUs",
        "authors": [
            "Erich Elsen",
            "V. Vishal",
            "Mike Houston",
            "Vijay Pande",
            "Pat Hanrahan",
            "Eric Darve"
        ],
        "summary": "Commercial graphics processors (GPUs) have high compute capacity at very low cost, which makes them attractive for general purpose scientific computing. In this paper we show how graphics processors can be used for N-body simulations to obtain improvements in performance over current generation CPUs. We have developed a highly optimized algorithm for performing the O(N^2) force calculations that constitute the major part of stellar and molecular dynamics simulations. In some of the calculations, we achieve sustained performance of nearly 100 GFlops on an ATI X1900XTX. The performance on GPUs is comparable to specialized processors such as GRAPE-6A and MDGRAPE-3, but at a fraction of the cost. Furthermore, the wide availability of GPUs has significant implications for cluster computing and distributed computing efforts like Folding@Home.",
        "published": "2007-06-20T21:02:14Z",
        "link": "http://arxiv.org/abs/0706.3060v1",
        "categories": [
            "cs.CE",
            "cs.DC"
        ]
    },
    {
        "title": "Optimal Replica Placement in Tree Networks with QoS and Bandwidth   Constraints and the Closest Allocation Policy",
        "authors": [
            "Veronika Rehn-Sonigo"
        ],
        "summary": "This paper deals with the replica placement problem on fully homogeneous tree networks known as the Replica Placement optimization problem. The client requests are known beforehand, while the number and location of the servers are to be determined. We investigate the latter problem using the Closest access policy when adding QoS and bandwidth constraints. We propose an optimal algorithm in two passes using dynamic programming.",
        "published": "2007-06-22T15:01:35Z",
        "link": "http://arxiv.org/abs/0706.3350v3",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "stdchk: A Checkpoint Storage System for Desktop Grid Computing",
        "authors": [
            "Samer Al Kiswany",
            "Matei Ripeanu",
            "Sudharshan S. Vazhkudai",
            "Abdullah Gharaibeh"
        ],
        "summary": "Checkpointing is an indispensable technique to provide fault tolerance for long-running high-throughput applications like those running on desktop grids. This paper argues that a dedicated checkpoint storage system, optimized to operate in these environments, can offer multiple benefits: reduce the load on a traditional file system, offer high-performance through specialization, and, finally, optimize data management by taking into account checkpoint application semantics. Such a storage system can present a unifying abstraction to checkpoint operations, while hiding the fact that there are no dedicated resources to store the checkpoint data. We prototype stdchk, a checkpoint storage system that uses scavenged disk space from participating desktops to build a low-cost storage system, offering a traditional file system interface for easy integration with applications. This paper presents the stdchk architecture, key performance optimizations, support for incremental checkpointing, and increased data availability. Our evaluation confirms that the stdchk approach is viable in a desktop grid setting and offers a low cost storage system with desirable performance characteristics: high write throughput and reduced storage space and network effort to save checkpoint images.",
        "published": "2007-06-25T01:24:46Z",
        "link": "http://arxiv.org/abs/0706.3546v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Self-Stabilizing Wavelets and r-Hops Coordination",
        "authors": [
            "Christian Boulinier",
            "Franck Petit"
        ],
        "summary": "We introduce a simple tool called the wavelet (or, r-wavelet) scheme. Wavelets deals with coordination among processes which are at most r hops away of each other. We present a selfstabilizing solution for this scheme. Our solution requires no underlying structure and works in arbritrary anonymous networks, i.e., no process identifier is required. Moreover, our solution works under any (even unfair) daemon. Next, we use the wavelet scheme to design self-stabilizing layer clocks. We show that they provide an efficient device in the design of local coordination problems at distance r, i.e., r-barrier synchronization and r-local resource allocation (LRA) such as r-local mutual exclusion (LME), r-group mutual exclusion (GME), and r-Reader/Writers. Some solutions to the r-LRA problem (e.g., r-LME) also provide transformers to transform algorithms written assuming any r-central daemon into algorithms working with any distributed daemon.",
        "published": "2007-06-27T12:53:06Z",
        "link": "http://arxiv.org/abs/0706.4015v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Multi-criteria scheduling of pipeline workflows",
        "authors": [
            "Anne Benoit",
            "Veronika Rehn-Sonigo",
            "Yves Robert"
        ],
        "summary": "Mapping workflow applications onto parallel platforms is a challenging problem, even for simple application patterns such as pipeline graphs. Several antagonist criteria should be optimized, such as throughput and latency (or a combination). In this paper, we study the complexity of the bi-criteria mapping problem for pipeline graphs on communication homogeneous platforms. In particular, we assess the complexity of the well-known chains-to-chains problem for different-speed processors, which turns out to be NP-hard. We provide several efficient polynomial bi-criteria heuristics, and their relative performance is evaluated through extensive simulations.",
        "published": "2007-06-27T13:43:16Z",
        "link": "http://arxiv.org/abs/0706.4009v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Scheduling multiple divisible loads on a linear processor network",
        "authors": [
            "Matthieu Gallet",
            "Yves Robert",
            "Frédéric Vivien"
        ],
        "summary": "Min, Veeravalli, and Barlas have recently proposed strategies to minimize the overall execution time of one or several divisible loads on a heterogeneous linear network, using one or more installments. We show on a very simple example that their approach does not always produce a solution and that, when it does, the solution is often suboptimal. We also show how to find an optimal schedule for any instance, once the number of installments per load is given. Then, we formally state that any optimal schedule has an infinite number of installments under a linear cost model as the one assumed in the original papers. Therefore, such a cost model cannot be used to design practical multi-installment strategies. Finally, through extensive simulations we confirmed that the best solution is always produced by the linear programming approach, while solutions of the original papers can be far away from the optimal.",
        "published": "2007-06-27T14:43:13Z",
        "link": "http://arxiv.org/abs/0706.4038v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Getting More From Your Multicore: Exploiting OpenMP From An Open Source   Numerical Scripting Language",
        "authors": [
            "Michael S. Noble"
        ],
        "summary": "We introduce SLIRP, a module generator for the S-Lang numerical scripting language, with a focus on its vectorization capabilities. We demonstrate how both SLIRP and S-Lang were easily adapted to exploit the inherent parallelism of high-level mathematical languages with OpenMP, allowing general users to employ tightly-coupled multiprocessors in scriptable research calculations while requiring no special knowledge of parallel programming. Motivated by examples in the ISIS astrophysical modeling & analysis tool, performance figures are presented for several machine and compiler configurations, demonstrating beneficial speedups for real-world operations.",
        "published": "2007-06-27T15:21:28Z",
        "link": "http://arxiv.org/abs/0706.4048v1",
        "categories": [
            "cs.DC",
            "astro-ph",
            "D.3.2; D.3.4; D.2.8"
        ]
    },
    {
        "title": "Unison as a Self-Stabilizing Wave Stream Algorithm in Asynchronous   Anonymous Networks",
        "authors": [
            "Christian Boulinier"
        ],
        "summary": "How to pass from local to global scales in anonymous networks? How to organize a selfstabilizing propagation of information with feedback. From the Angluin impossibility results, we cannot elect a leader in a general anonymous network. Thus, it is impossible to build a rooted spanning tree. Many problems can only be solved by probabilistic methods. In this paper we show how to use Unison to design a self-stabilizing barrier synchronization in an anonymous network. We show that the commuication structure of this barrier synchronization designs a self-stabilizing wave-stream, or pipelining wave, in anonymous networks. We introduce two variants of Wave: the strong waves and the wavelets. A strong wave can be used to solve the idempotent r-operator parametrized computation problem. A wavelet deals with k-distance computation. We show how to use Unison to design a self-stabilizing wave stream, a self-stabilizing strong wave stream and a self-stabilizing wavelet stream.",
        "published": "2007-06-28T18:51:36Z",
        "link": "http://arxiv.org/abs/0706.4298v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Performance Analysis of Publish/Subscribe Systems",
        "authors": [
            "Heithem Abbes",
            "Christophe Cérin",
            "Jean-Christophe Dubacq",
            "Mohamed Jemni"
        ],
        "summary": "The Desktop Grid offers solutions to overcome several challenges and to answer increasingly needs of scientific computing. Its technology consists mainly in exploiting resources, geographically dispersed, to treat complex applications needing big power of calculation and/or important storage capacity. However, as resources number increases, the need for scalability, self-organisation, dynamic reconfigurations, decentralisation and performance becomes more and more essential. Since such properties are exhibited by P2P systems, the convergence of grid computing and P2P computing seems natural. In this context, this paper evaluates the scalability and performance of P2P tools for discovering and registering services. Three protocols are used for this purpose: Bonjour, Avahi and Free-Pastry. We have studied the behaviour of theses protocols related to two criteria: the elapsed time for registrations services and the needed time to discover new services. Our aim is to analyse these results in order to choose the best protocol we can use in order to create a decentralised middleware for desktop grid.",
        "published": "2007-07-03T09:02:45Z",
        "link": "http://arxiv.org/abs/0707.0365v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A Multi Interface Grid Discovery System",
        "authors": [
            "A. Ali",
            "A. Anjum",
            "J. Bunn",
            "F. Khan",
            "R. McClatchey",
            "H. Newman",
            "C. Steenberg",
            "M. Thomas",
            "Ian Willers"
        ],
        "summary": "Discovery Systems (DS) can be considered as entry points for global loosely coupled distributed systems. An efficient Discovery System in essence increases the performance, reliability and decision making capability of distributed systems. With the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. They are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. In this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.",
        "published": "2007-07-05T09:22:45Z",
        "link": "http://arxiv.org/abs/0707.0740v1",
        "categories": [
            "cs.DC",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "Mobile Computing in Physics Analysis - An Indicator for eScience",
        "authors": [
            "A. Ali",
            "A. Anjum",
            "T. Azim",
            "J. Bunn",
            "A. Ikram",
            "R. McClatchey",
            "H. Newman",
            "C. Steenberg",
            "M. Thomas",
            "I. Willers"
        ],
        "summary": "This paper presents the design and implementation of a Grid-enabled physics analysis environment for handheld and other resource-limited computing devices as one example of the use of mobile devices in eScience. Handheld devices offer great potential because they provide ubiquitous access to data and round-the-clock connectivity over wireless links. Our solution aims to provide users of handheld devices the capability to launch heavy computational tasks on computational and data Grids, monitor the jobs status during execution, and retrieve results after job completion. Users carry their jobs on their handheld devices in the form of executables (and associated libraries). Users can transparently view the status of their jobs and get back their outputs without having to know where they are being executed. In this way, our system is able to act as a high-throughput computing environment where devices ranging from powerful desktop machines to small handhelds can employ the power of the Grid. The results shown in this paper are readily applicable to the wider eScience community.",
        "published": "2007-07-05T09:32:29Z",
        "link": "http://arxiv.org/abs/0707.0742v1",
        "categories": [
            "cs.DC",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "DIANA Scheduling Hierarchies for Optimizing Bulk Job Scheduling",
        "authors": [
            "A. Anjum",
            "R. McClatchey",
            "H. Stockinger",
            "A. Ali",
            "I. Willers",
            "M. Thomas",
            "M. Sagheer",
            "K. Hasham",
            "O. Alvi"
        ],
        "summary": "The use of meta-schedulers for resource management in large-scale distributed systems often leads to a hierarchy of schedulers. In this paper, we discuss why existing meta-scheduling hierarchies are sometimes not sufficient for Grid systems due to their inability to re-organise jobs already scheduled locally. Such a job re-organisation is required to adapt to evolving loads which are common in heavily used Grid infrastructures. We propose a peer-to-peer scheduling model and evaluate it using case studies and mathematical modelling. We detail the DIANA (Data Intensive and Network Aware) scheduling algorithm and its queue management system for coping with the load distribution and for supporting bulk job scheduling. We demonstrate that such a system is beneficial for dynamic, distributed and self-organizing resource management and can assist in optimizing load or job distribution in complex Grid infrastructures.",
        "published": "2007-07-05T09:36:18Z",
        "link": "http://arxiv.org/abs/0707.0743v1",
        "categories": [
            "cs.DC",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "Experiences of Engineering Grid-Based Medical Software",
        "authors": [
            "F. Estrella",
            "T. Hauer",
            "R. McClatchey",
            "M. Odeh",
            "D Rogulin",
            "T. Solomonides"
        ],
        "summary": "Objectives: Grid-based technologies are emerging as potential solutions for managing and collaborating distributed resources in the biomedical domain. Few examples exist, however, of successful implementations of Grid-enabled medical systems and even fewer have been deployed for evaluation in practice. The objective of this paper is to evaluate the use in clinical practice of a Grid-based imaging prototype and to establish directions for engineering future medical Grid developments and their subsequent deployment. Method: The MammoGrid project has deployed a prototype system for clinicians using the Grid as its information infrastructure. To assist in the specification of the system requirements (and for the first time in healthgrid applications), use-case modelling has been carried out in close collaboration with clinicians and radiologists who had no prior experience of this modelling technique. A critical qualitative and, where possible, quantitative analysis of the MammoGrid prototype is presented leading to a set of recommendations from the delivery of the first deployed Grid-based medical imaging application. Results: We report critically on the application of software engineering techniques in the specification and implementation of the MammoGrid project and show that use-case modelling is a suitable vehicle for representing medical requirements and for communicating effectively with the clinical community. This paper also discusses the practical advantages and limitations of applying the Grid to real-life clinical applications and presents the consequent lessons learned.",
        "published": "2007-07-05T10:06:41Z",
        "link": "http://arxiv.org/abs/0707.0748v1",
        "categories": [
            "cs.DC",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "Managing Separation of Concerns in Grid Applications Through   Architectural Model Transformations",
        "authors": [
            "David Manset",
            "Herve Verjus",
            "Richard McClatchey"
        ],
        "summary": "Grids enable the aggregation, virtualization and sharing of massive heterogeneous and geographically dispersed resources, using files, applications and storage devices, to solve computation and data intensive problems, across institutions and countries via temporary collaborations called virtual organizations (VO). Most implementations result in complex superposition of software layers, often delivering low quality of service and quality of applications. As a consequence, Grid-based applications design and development is increasingly complex, and the use of most classical engineering practices is unsuccessful. Not only is the development of such applications a time-consuming, error prone and expensive task, but also the resulting applications are often hard-coded for specific Grid configurations, platforms and infra-structures. Having neither guidelines nor rules in the design of a Grid-based application is a paradox since there are many existing architectural approaches for distributed computing, which could ease and promote rigorous engineering methods based on the re-use of software components. It is our belief that ad-hoc and semi-formal engineer-ing approaches, in current use, are insufficient to tackle tomorrows Grid develop-ments requirements. Because Grid-based applications address multi-disciplinary and complex domains (health, military, scientific computation), their engineering requires rigor and control. This paper therefore advocates a formal model-driven engineering process and corresponding design framework and tools for building the next generation of Grids.",
        "published": "2007-07-05T11:10:03Z",
        "link": "http://arxiv.org/abs/0707.0761v1",
        "categories": [
            "cs.SE",
            "cs.DC",
            "D.2.11"
        ]
    },
    {
        "title": "PhantomOS: A Next Generation Grid Operating System",
        "authors": [
            "Irfan Habib",
            "Kamran Soomro",
            "Ashiq Anjum",
            "Richard McClatchey",
            "Arshad Ali",
            "Peter Bloodsworth"
        ],
        "summary": "Grid Computing has made substantial advances in the past decade; these are primarily due to the adoption of standardized Grid middleware. However Grid computing has not yet become pervasive because of some barriers that we believe have been caused by the adoption of middleware centric approaches. These barriers include: scant support for major types of applications such as interactive applications; lack of flexible, autonomic and scalable Grid architectures; lack of plug-and-play Grid computing and, most importantly, no straightforward way to setup and administer Grids. PhantomOS is a project which aims to address many of these barriers. Its goal is the creation of a user friendly pervasive Grid computing platform that facilitates the rapid deployment and easy maintenance of Grids whilst providing support for major types of applications on Grids of almost any topology. In this paper we present the detailed system architecture and an overview of its implementation.",
        "published": "2007-07-05T11:14:45Z",
        "link": "http://arxiv.org/abs/0707.0762v1",
        "categories": [
            "cs.DC",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "Scheduling in Data Intensive and Network Aware (DIANA) Grid Environments",
        "authors": [
            "Richard McClatchey",
            "Ashiq Anjum",
            "Heinz Stockinger",
            "Arshad Ali",
            "Ian Willers",
            "Michael Thomas"
        ],
        "summary": "In Grids scheduling decisions are often made on the basis of jobs being either data or computation intensive: in data intensive situations jobs may be pushed to the data and in computation intensive situations data may be pulled to the jobs. This kind of scheduling, in which there is no consideration of network characteristics, can lead to performance degradation in a Grid environment and may result in large processing queues and job execution delays due to site overloads. In this paper we describe a Data Intensive and Network Aware (DIANA) meta-scheduling approach, which takes into account data, processing power and network characteristics when making scheduling decisions across multiple sites. Through a practical implementation on a Grid testbed, we demonstrate that queue and execution times of data-intensive jobs can be significantly improved when we introduce our proposed DIANA scheduler. The basic scheduling decisions are dictated by a weighting factor for each potential target location which is a calculated function of network characteristics, processing cycles and data location and size. The job scheduler provides a global ranking of the computing resources and then selects an optimal one on the basis of this overall access and execution cost. The DIANA approach considers the Grid as a combination of active network elements and takes network characteristics as a first class criterion in the scheduling decision matrix along with computation and data. The scheduler can then make informed decisions by taking into account the changing state of the network, locality and size of the data and the pool of available processing cycles.",
        "published": "2007-07-05T19:46:51Z",
        "link": "http://arxiv.org/abs/0707.0862v1",
        "categories": [
            "cs.DC",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "Cactus Framework: Black Holes to Gamma Ray Bursts",
        "authors": [
            "Erik Schnetter",
            "Christian D. Ott",
            "Gabrielle Allen",
            "Peter Diener",
            "Tom Goodale",
            "Thomas Radke",
            "Edward Seidel",
            "John Shalf"
        ],
        "summary": "Gamma Ray Bursts (GRBs) are intense narrowly-beamed flashes of gamma-rays of cosmological origin. They are among the most scientifically interesting astrophysical systems, and the riddle concerning their central engines and emission mechanisms is one of the most complex and challenging problems of astrophysics today. In this article we outline our petascale approach to the GRB problem and discuss the computational toolkits and numerical codes that are currently in use and that will be scaled up to run on emerging petaflop scale computing platforms in the near future.   Petascale computing will require additional ingredients over conventional parallelism. We consider some of the challenges which will be caused by future petascale architectures, and discuss our plans for the future development of the Cactus framework and its applications to meet these challenges in order to profit from these new architectures.",
        "published": "2007-07-11T13:01:50Z",
        "link": "http://arxiv.org/abs/0707.1607v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Multi-physics Extension of OpenFMO Framework",
        "authors": [
            "Toshiya Takami",
            "Jun Maki",
            "Jun'ichi Ooba",
            "Yuuichi Inadomi",
            "Hiroaki Honda",
            "Ryutaro Susukita",
            "Koji Inoue",
            "Taizo Kobayashi",
            "Rie Nogita",
            "Mutsumi Aoyagi"
        ],
        "summary": "OpenFMO framework, an open-source software (OSS) platform for Fragment Molecular Orbital (FMO) method, is extended to multi-physics simulations (MPS). After reviewing the several FMO implementations on distributed computer environments, the subsequent development planning corresponding to MPS is presented. It is discussed which should be selected as a scientific software, lightweight and reconfigurable form or large and self-contained form.",
        "published": "2007-07-18T05:34:08Z",
        "link": "http://arxiv.org/abs/0707.2630v1",
        "categories": [
            "cs.DC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Autonomous tools for Grid management, monitoring and optimization",
        "authors": [
            "Wojciech Wislicki"
        ],
        "summary": "We outline design and lines of development of autonomous tools for the computing Grid management, monitoring and optimization. The management is proposed to be based on the notion of utility. Grid optimization is considered to be application-oriented. A generic Grid simulator is proposed as an optimization tool for Grid structure and functionality.",
        "published": "2007-07-22T14:02:21Z",
        "link": "http://arxiv.org/abs/0707.3263v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "hep-ex"
        ]
    },
    {
        "title": "A Knowledge-Based Analysis of Global Function Computation",
        "authors": [
            "Joseph Y. Halpern",
            "Sabina Petride"
        ],
        "summary": "Consider a distributed system N in which each agent has an input value and each communication link has a weight. Given a global function, that is, a function f whose value depends on the whole network, the goal is for every agent to eventually compute the value f(N). We call this problem global function computation. Various solutions for instances of this problem, such as Boolean function computation, leader election, (minimum) spanning tree construction, and network determination, have been proposed, each under particular assumptions about what processors know about the system and how this knowledge can be acquired. We give a necessary and sufficient condition for the problem to be solvable that generalizes a number of well-known results. We then provide a knowledge-based (kb) program (like those of Fagin, Halpern, Moses, and Vardi) that solves global function computation whenever possible. Finally, we improve the message overhead inherent in our initial kb program by giving a counterfactual belief-based program that also solves the global function computation whenever possible, but where agents send messages only when they believe it is necessary to do so. The latter program is shown to be implemented by a number of well-known algorithms for solving leader election.",
        "published": "2007-07-23T18:49:28Z",
        "link": "http://arxiv.org/abs/0707.3435v1",
        "categories": [
            "cs.DC",
            "cs.LO"
        ]
    },
    {
        "title": "e-Science initiatives in Venezuela",
        "authors": [
            "J. L. Chaves",
            "G. Diaz",
            "V. Hamar",
            "R. Isea",
            "F. Rojas",
            "N. Ruiz",
            "R. Torrens",
            "M. Uzcategui",
            "J. Florez-Lopez",
            "H. Hoeger",
            "C. Mendoza",
            "L. A. Nunez"
        ],
        "summary": "Within the context of the nascent e-Science infrastructure in Venezuela, we describe several web-based scientific applications developed at the Centro Nacional de Calculo Cientifico Universidad de Los Andes (CeCalCULA), Merida, and at the Instituto Venezolano de Investigaciones Cientificas (IVIC), Caracas. The different strategies that have been followed for implementing quantum chemistry and atomic physics applications are presented. We also briefly discuss a damage portal based on dynamic, nonlinear, finite elements of lumped damage mechanics and a biomedical portal developed within the framework of the \\textit{E-Infrastructure shared between Europe and Latin America} (EELA) initiative for searching common sequences and inferring their functions in parasitic diseases such as leishmaniasis, chagas and malaria.",
        "published": "2007-07-24T12:00:43Z",
        "link": "http://arxiv.org/abs/0707.3531v1",
        "categories": [
            "cs.CE",
            "cs.DC"
        ]
    },
    {
        "title": "Recent Advances in Solving the Protein Threading Problem",
        "authors": [
            "Rumen Andonov",
            "Guillaume Collet",
            "Jean-François Gibrat",
            "Antoine Marin",
            "Vincent Poirriez",
            "Nikola Yanev"
        ],
        "summary": "The fold recognition methods are promissing tools for capturing the structure of a protein by its amino acid residues sequence but their use is still restricted by the needs of huge computational resources and suitable efficient algorithms as well. In the recent version of FROST (Fold Recognition Oriented Search Tool) package the most efficient algorithm for solving the Protein Threading Problem (PTP) is implemented due to the strong collaboration between the SYMBIOSE group in IRISA and MIG in Jouy-en-Josas. In this paper, we present the diverse components of FROST, emphasizing on the recent advances in formulating and solving new versions of the PTP and on the way of solving on a computer cluster a million of instances in a easonable time.",
        "published": "2007-07-25T14:05:59Z",
        "link": "http://arxiv.org/abs/0707.3750v2",
        "categories": [
            "q-bio.QM",
            "cs.DC"
        ]
    },
    {
        "title": "Resource Allocation in Public Cluster with Extended Optimization   Algorithm",
        "authors": [
            "Z. Akbar",
            "L. T. Handoko"
        ],
        "summary": "We introduce an optimization algorithm for resource allocation in the LIPI Public Cluster to optimize its usage according to incoming requests from users. The tool is an extended and modified genetic algorithm developed to match specific natures of public cluster. We present a detail analysis of optimization, and compare the results with the exact calculation. We show that it would be very useful and could realize an automatic decision making system for public clusters.",
        "published": "2007-08-04T05:15:05Z",
        "link": "http://arxiv.org/abs/0708.0608v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Real-time control and monitoring system for LIPI's Public Cluster",
        "authors": [
            "I. Firmansyah",
            "B. Hermanto",
            "Hadiyanto",
            "L. T. Handoko"
        ],
        "summary": "We have developed a monitoring and control system for LIPI's Public Cluster. The system consists of microcontrollers and full web-based user interfaces for daily operation. It is argued that, due to its special natures, the cluster requires fully dedicated and self developed control and monitoring system. We discuss the implementation of using parallel port and dedicated micro-controller for this purpose. We also show that integrating such systems enables an autonomous control system based on the real time monitoring, for instance an autonomous power supply control based on the actual temperature, etc.",
        "published": "2007-08-04T05:16:43Z",
        "link": "http://arxiv.org/abs/0708.0607v1",
        "categories": [
            "cs.DC",
            "cs.RO"
        ]
    },
    {
        "title": "Open and Free Cluster for Public",
        "authors": [
            "Z. Akbar",
            "Slamet",
            "B. I. Ajinagoro",
            "G. I. Ohara",
            "I. Firmansyah",
            "B. Hermanto",
            "L. T. Handoko"
        ],
        "summary": "We introduce the LIPI Public Cluster, the first parallel machine facility fully open for public and for free in Indonesia and surrounding countries. In this paper, we focus on explaining our globally new concept on open cluster, and how to realize and manage it to meet the users needs. We show that after 2 years trial running and several upgradings, the Public Cluster performs well and is able to fulfil all requirements as expected.",
        "published": "2007-08-04T05:17:34Z",
        "link": "http://arxiv.org/abs/0708.0605v1",
        "categories": [
            "cs.DC",
            "cs.CY"
        ]
    },
    {
        "title": "Public Cluster : parallel machine with multi-block approach",
        "authors": [
            "Z. Akbar",
            "Slamet",
            "B. I. Ajinagoro",
            "G. I. Ohara",
            "I. Firmansyah",
            "B. Hermanto",
            "L. T. Handoko"
        ],
        "summary": "We introduce a new approach to enable an open and public parallel machine which is accessible for multi users with multi jobs belong to different blocks running at the same time. The concept is required especially for parallel machines which are dedicated for public use as implemented at the LIPI Public Cluster. We have deployed the simplest technique by running multi daemons of parallel processing engine with different configuration files specified for each user assigned to access the system, and also developed an integrated system to fully control and monitor the whole system over web. A brief performance analysis is also given for Message Parsing Interface (MPI) engine. It is shown that the proposed approach is quite reliable and affect the whole performances only slightly.",
        "published": "2007-08-04T05:18:21Z",
        "link": "http://arxiv.org/abs/0708.0603v1",
        "categories": [
            "cs.DC",
            "cs.CY"
        ]
    },
    {
        "title": "ADS-Directory Services for Mobile Ad-Hoc Networks Based on an   Information Market Model",
        "authors": [
            "Christian Hutter",
            "Matthias R. Brust",
            "Steffen Rothkugel"
        ],
        "summary": "Ubiquitous computing based on small mobile devices using wireless communication links is becoming very attractive. The computational power and storage capacities provided allow the execution of sophisticated applications. Due to the fact that sharing of information is a central problem for distributed applications, the development of self organizing middleware services providing high level interfaces for information managing is essential. ADS is a directory service for mobile ad-hoc networks dealing with local and nearby information as well as providing access to distant information. The approach discussed throughout this paper is based upon the concept of information markets.",
        "published": "2007-08-04T13:21:55Z",
        "link": "http://arxiv.org/abs/0708.0624v1",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "ADS as Information Management Service in an M-Learning Environment",
        "authors": [
            "Matthias R. Brust",
            "Daniel Goergen",
            "Christian Hutter",
            "Steffen Rothkugel"
        ],
        "summary": "Leveraging the potential power of even small handheld devices able to communicate wirelessly requires dedicated support. In particular, collaborative applications need sophisticated assistance in terms of querying and exchanging different kinds of data. Using a concrete example from the domain of mobile learning, the general need for information dissemination is motivated. Subsequently, and driven by infrastructural conditions, realization strategies of an appropriate middleware service are discussed.",
        "published": "2007-08-04T13:32:07Z",
        "link": "http://arxiv.org/abs/0708.0627v1",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "On the Self-stabilization of Mobile Robots in Graphs",
        "authors": [
            "Lélia Blin",
            "Maria Gradinariu Potop-Butucaru",
            "Sébastien Tixeuil"
        ],
        "summary": "Self-stabilization is a versatile technique to withstand any transient fault in a distributed system. Mobile robots (or agents) are one of the emerging trends in distributed computing as they mimic autonomous biologic entities. The contribution of this paper is threefold. First, we present a new model for studying mobile entities in networks subject to transient faults. Our model differs from the classical robot model because robots have constraints about the paths they are allowed to follow, and from the classical agent model because the number of agents remains fixed throughout the execution of the protocol. Second, in this model, we study the possibility of designing self-stabilizing algorithms when those algorithms are run by mobile robots (or agents) evolving on a graph. We concentrate on the core building blocks of robot and agents problems: naming and leader election. Not surprisingly, when no constraints are given on the network graph topology and local execution model, both problems are impossible to solve. Finally, using minimal hypothesis with respect to impossibility results, we provide deterministic and probabilistic solutions to both problems, and show equivalence of these problems by an algorithmic reduction mechanism.",
        "published": "2007-08-07T09:34:14Z",
        "link": "http://arxiv.org/abs/0708.0909v2",
        "categories": [
            "cs.DS",
            "cs.DC"
        ]
    },
    {
        "title": "A Light-Based Device for Solving the Hamiltonian Path Problem",
        "authors": [
            "Mihai Oltean"
        ],
        "summary": "In this paper we suggest the use of light for performing useful computations. Namely, we propose a special device which uses light rays for solving the Hamiltonian path problem on a directed graph. The device has a graph-like representation and the light is traversing it following the routes given by the connections between nodes. In each node the rays are uniquely marked so that they can be easily identified. At the destination node we will search only for particular rays that have passed only once through each node. We show that the proposed device can solve small and medium instances of the problem in reasonable time.",
        "published": "2007-08-10T18:12:52Z",
        "link": "http://arxiv.org/abs/0708.1496v1",
        "categories": [
            "cs.AR",
            "cs.DC"
        ]
    },
    {
        "title": "Solving the Hamiltonian path problem with a light-based computer",
        "authors": [
            "Mihai Oltean"
        ],
        "summary": "In this paper we propose a special computational device which uses light rays for solving the Hamiltonian path problem on a directed graph. The device has a graph-like representation and the light is traversing it by following the routes given by the connections between nodes. In each node the rays are uniquely marked so that they can be easily identified. At the destination node we will search only for particular rays that have passed only once through each node. We show that the proposed device can solve small and medium instances of the problem in reasonable time.",
        "published": "2007-08-10T20:01:24Z",
        "link": "http://arxiv.org/abs/0708.1512v1",
        "categories": [
            "cs.AR",
            "cs.DC"
        ]
    },
    {
        "title": "A Data-Parallel Version of Aleph",
        "authors": [
            "Stasinos Konstantopoulos"
        ],
        "summary": "This is to present work on modifying the Aleph ILP system so that it evaluates the hypothesised clauses in parallel by distributing the data-set among the nodes of a parallel or distributed machine. The paper briefly discusses MPI, the interface used to access message- passing libraries for parallel computers and clusters. It then proceeds to describe an extension of YAP Prolog with an MPI interface and an implementation of data-parallel clause evaluation for Aleph through this interface. The paper concludes by testing the data-parallel Aleph on artificially constructed data-sets.",
        "published": "2007-08-10T23:32:16Z",
        "link": "http://arxiv.org/abs/0708.1527v1",
        "categories": [
            "cs.AI",
            "cs.DC"
        ]
    },
    {
        "title": "Exact Cover with light",
        "authors": [
            "Mihai Oltean",
            "Oana Muntean"
        ],
        "summary": "We suggest a new optical solution for solving the YES/NO version of the Exact Cover problem by using the massive parallelism of light. The idea is to build an optical device which can generate all possible solutions of the problem and then to pick the correct one. In our case the device has a graph-like representation and the light is traversing it by following the routes given by the connections between nodes. The nodes are connected by arcs in a special way which lets us to generate all possible covers (exact or not) of the given set. For selecting the correct solution we assign to each item, from the set to be covered, a special integer number. These numbers will actually represent delays induced to light when it passes through arcs. The solution is represented as a subray arriving at a certain moment in the destination node. This will tell us if an exact cover does exist or not.",
        "published": "2007-08-14T21:41:55Z",
        "link": "http://arxiv.org/abs/0708.1962v1",
        "categories": [
            "cs.AR",
            "cs.DC"
        ]
    },
    {
        "title": "Solving the subset-sum problem with a light-based device",
        "authors": [
            "Mihai Oltean",
            "Oana Muntean"
        ],
        "summary": "We propose a special computational device which uses light rays for solving the subset-sum problem. The device has a graph-like representation and the light is traversing it by following the routes given by the connections between nodes. The nodes are connected by arcs in a special way which lets us to generate all possible subsets of the given set. To each arc we assign either a number from the given set or a predefined constant. When the light is passing through an arc it is delayed by the amount of time indicated by the number placed in that arc. At the destination node we will check if there is a ray whose total delay is equal to the target value of the subset sum problem (plus some constants).",
        "published": "2007-08-14T21:46:32Z",
        "link": "http://arxiv.org/abs/0708.1964v1",
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.DC"
        ]
    },
    {
        "title": "Multi and Independent Block Approach in Public Cluster",
        "authors": [
            "Z. Akbar",
            "L. T. Handoko"
        ],
        "summary": "We present extended multi block approach in the LIPI Public Cluster. The multi block approach enables a cluster to be divided into several independent blocks which run jobs owned by different users simultaneously. Previously, we have maintained the blocks using single master node for all blocks due to efficiency and resource limitations. Following recent advancements and expansion of node\\'s number, we have modified the multi block approach with multiple master nodes, each of them is responsible for a single block. We argue that this approach improves the overall performance significantly, for especially data intensive computational works.",
        "published": "2007-08-25T19:46:52Z",
        "link": "http://arxiv.org/abs/0708.3446v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Searching for a dangerous host: randomized vs. deterministic",
        "authors": [
            "Igor Nitto",
            "Rossano Venturini"
        ],
        "summary": "A Black Hole is an harmful host in a network that destroys incoming agents without leaving any trace of such event. The problem of locating the black hole in a network through a team of agent coordinated by a common protocol is usually referred in literature as the Black Hole Search problem (or BHS for brevity) and it is a consolidated research topic in the area of distributed algorithms. The aim of this paper is to extend the results for BHS by considering more general (and hence harder) classes of dangerous host. In particular we introduce rB-hole as a probabilistic generalization of the Black Hole, in which the destruction of an incoming agent is a purely random event happening with some fixed probability (like flipping a biased coin). The main result we present is that if we tolerate an arbitrarily small error probability in the result then the rB-hole Search problem, or RBS, is not harder than the usual BHS. We establish this result in two different communication model, specifically both in presence or absence of whiteboards non-located at the homebase. The core of our methods is a general reduction tool for transforming algorithms for the black hole into algorithms for the rB-hole.",
        "published": "2007-08-28T09:24:13Z",
        "link": "http://arxiv.org/abs/0708.3734v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Computational performance of a parallelized high-order spectral and   mortar element toolbox",
        "authors": [
            "Roland Bouffanais",
            "Vincent Keller",
            "Ralf Gruber",
            "Michel O. Deville"
        ],
        "summary": "In this paper, a comprehensive performance review of a MPI-based high-order spectral and mortar element method C++ toolbox is presented. The focus is put on the performance evaluation of several aspects with a particular emphasis on the parallel efficiency. The performance evaluation is analyzed and compared to predictions given by a heuristic model, the so-called Gamma model. A tailor-made CFD computation benchmark case is introduced and used to carry out this review, stressing the particular interest for commodity clusters. Conclusions are drawn from this extensive series of analyses and modeling leading to specific recommendations concerning such toolbox development and parallel implementation.",
        "published": "2007-09-07T08:52:32Z",
        "link": "http://arxiv.org/abs/0709.1024v1",
        "categories": [
            "cs.DC",
            "cs.PF"
        ]
    },
    {
        "title": "A Class of Parallel Tiled Linear Algebra Algorithms for Multicore   Architectures",
        "authors": [
            "Alfredo Buttari",
            "Julien Langou",
            "Jakub Kurzak",
            "Jack Dongarra"
        ],
        "summary": "As multicore systems continue to gain ground in the High Performance Computing world, linear algebra algorithms have to be reformulated or new algorithms have to be developed in order to take advantage of the architectural features on these new processors. Fine grain parallelism becomes a major requirement and introduces the necessity of loose synchronization in the parallel execution of an operation. This paper presents an algorithm for the Cholesky, LU and QR factorization where the operations can be represented as a sequence of small tasks that operate on square blocks of data. These tasks can be dynamically scheduled for execution based on the dependencies among them and on the availability of computational resources. This may result in an out of order execution of the tasks which will completely hide the presence of intrinsically sequential tasks in the factorization. Performance comparisons are presented with the LAPACK algorithms where parallelism can only be exploited at the level of the BLAS operations and vendor implementations.",
        "published": "2007-09-09T16:32:46Z",
        "link": "http://arxiv.org/abs/0709.1272v3",
        "categories": [
            "cs.MS",
            "cs.DC"
        ]
    },
    {
        "title": "Distributed Decision Through Self-Synchronizing Sensor Networks in the   Presence of Propagation Delays and Asymmetric Channels",
        "authors": [
            "Gesualdo Scutari",
            "Sergio Barbarossa",
            "Loreto Pescosolido"
        ],
        "summary": "In this paper we propose and analyze a distributed algorithm for achieving globally optimal decisions, either estimation or detection, through a self-synchronization mechanism among linearly coupled integrators initialized with local measurements. We model the interaction among the nodes as a directed graph with weights (possibly) dependent on the radio channels and we pose special attention to the effect of the propagation delay occurring in the exchange of data among sensors, as a function of the network geometry. We derive necessary and sufficient conditions for the proposed system to reach a consensus on globally optimal decision statistics. One of the major results proved in this work is that a consensus is reached with exponential convergence speed for any bounded delay condition if and only if the directed graph is quasi-strongly connected. We provide a closed form expression for the global consensus, showing that the effect of delays is, in general, the introduction of a bias in the final decision. Finally, we exploit our closed form expression to devise a double-step consensus mechanism able to provide an unbiased estimate with minimum extra complexity, without the need to know or estimate the channel parameters.",
        "published": "2007-09-15T08:40:18Z",
        "link": "http://arxiv.org/abs/0709.2410v2",
        "categories": [
            "cs.MA",
            "cs.DC"
        ]
    },
    {
        "title": "Non-Blocking Signature of very large SOAP Messages",
        "authors": [
            "G. A. Kohring",
            "L. Lo Iacono"
        ],
        "summary": "Data transfer and staging services are common components in Grid-based, or more generally, in service-oriented applications. Security mechanisms play a central role in such services, especially when they are deployed in sensitive application fields like e-health. The adoption of WS-Security and related standards to SOAP-based transfer services is, however, problematic as a straightforward adoption of SOAP with MTOM introduces considerable inefficiencies in the signature generation process when large data sets are involved. This paper proposes a non-blocking, signature generation approach enabling a stream-like processing with considerable performance enhancements.",
        "published": "2007-09-17T13:50:42Z",
        "link": "http://arxiv.org/abs/0709.2635v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Query Evaluation in P2P Systems of Taxonomy-based Sources: Algorithms,   Complexity, and Optimizations",
        "authors": [
            "Carlo Meghini",
            "Yannis Tzitzikas",
            "Anastasia Analyti"
        ],
        "summary": "In this study, we address the problem of answering queries over a peer-to-peer system of taxonomy-based sources. A taxonomy states subsumption relationships between negation-free DNF formulas on terms and negation-free conjunctions of terms. To the end of laying the foundations of our study, we first consider the centralized case, deriving the complexity of the decision problem and of query evaluation. We conclude by presenting an algorithm that is efficient in data complexity and is based on hypergraphs. More expressive forms of taxonomies are also investigated, which however lead to intractability. We then move to the distributed case, and introduce a logical model of a network of taxonomy-based sources. On such network, a distributed version of the centralized algorithm is then presented, based on a message passing paradigm, and its correctness is proved. We finally discuss optimization issues, and relate our work to the literature.",
        "published": "2007-09-19T15:10:05Z",
        "link": "http://arxiv.org/abs/0709.3034v1",
        "categories": [
            "cs.DB",
            "cs.DC",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "Static Deadlock Detection in MPI Synchronization Communication",
        "authors": [
            "Liao Ming-Xue",
            "He Xiao-Xin",
            "Fan Zhi-Hua"
        ],
        "summary": "It is very common to use dynamic methods to detect deadlocks in MPI programs for the reason that static methods have some restrictions. To guarantee high reliability of some important MPI-based application software, a model of MPI synchronization communication is abstracted and a type of static method is devised to examine deadlocks in such modes. The model has three forms with different complexity: sequential model, single-loop model and nested-loop model. Sequential model is a base for all models. Single-loop model must be treated with a special type of equation group and nested-loop model extends the methods for the other two models. A standard Java-based software framework originated from these methods is constructed for determining whether MPI programs are free from synchronization communication deadlocks. Our practice shows the software framework is better than those tools using dynamic methods because it can dig out all synchronization communication deadlocks before an MPI-based program goes into running.",
        "published": "2007-09-24T05:33:29Z",
        "link": "http://arxiv.org/abs/0709.3689v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Deadlock Detection in Basic Models of MPI Synchronization Communication   Programs",
        "authors": [
            "Ming-xue Liao",
            "Zhi-hua Fan"
        ],
        "summary": "A model of MPI synchronization communication programs is presented and its three basic simplified models are also defined. A series of theorems and methods for deciding whether deadlocks will occur among the three models are given and proved strictly. These theories and methods for simple models' deadlock detection are the necessary base for real MPI program deadlock detection. The methods are based on a static analysis through programs and with runtime detection in necessary cases and they are able to determine before compiling whether it will be deadlocked for two of the three basic models. For another model, some deadlock cases can be found before compiling and others at runtime. Our theorems can be used to prove the correctness of currently popular MPI program deadlock detection algorithms. Our methods may decrease codes that those algorithms need to change to MPI source or profiling interface and may detects deadlocks ahead of program execution, thus the overheads can be reduced greatly.",
        "published": "2007-09-24T06:02:29Z",
        "link": "http://arxiv.org/abs/0709.3692v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Algorithm of Static Deadlock Detection in MPI Synchronization   Communication Sequential Model",
        "authors": [
            "Liao Ming-Xue",
            "He Xiao-Xin",
            "Fan Zhi-Hua"
        ],
        "summary": "Detecting deadlocks in MPI synchronization communication programs is very difficult and need building program models. All complex models are based on sequential models. The sequential model is mapped into a set of character strings and its deadlock detection problem is translated into an equivalent multi-queue string matching problem. An algorithm is devised and implemented to statically detect deadlocks in sequential models of MPI synchronization communication programs. The time and space complexity of the algorithm is O(n) where n is the amount of message in model. The algorithm is better than usual circle-detection methods and can adapt well to dynamic message stream.",
        "published": "2007-09-24T06:06:34Z",
        "link": "http://arxiv.org/abs/0709.3693v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Research Paper on Transaction-Oriented Simulation In Ad Hoc Grids",
        "authors": [
            "Gerald Krafft"
        ],
        "summary": "This paper analyses the requirements of performing parallel transaction-oriented simulations with a special focus on the space-parallel approach and discrete event simulation synchronisation algorithms that are suitable for transaction-oriented simulation and the target environment of Ad Hoc Grids. To demonstrate the findings a Java-based parallel transaction-oriented simulator for the simulation language GPSS/H is implemented on the basis of the most promising Shock Resistant Time Warp synchronisation algorithm and using the Grid framework ProActive. The validation of this parallel simulator shows that the Shock Resistant Time Warp algorithm can successfully reduce the number of rolled back Transaction moves but it also reveals circumstances in which the Shock Resistant Time Warp algorithm can be outperformed by the normal Time Warp algorithm. The conclusion of this paper suggests possible improvements to the Shock Resistant Time Warp algorithm to avoid such problems.",
        "published": "2007-09-24T19:25:27Z",
        "link": "http://arxiv.org/abs/0709.3826v1",
        "categories": [
            "cs.DC",
            "I.6.3; I.6.1; I.6.7"
        ]
    },
    {
        "title": "A Symphony Conducted by Brunet",
        "authors": [
            "P. Oscar Boykin",
            "Jesse S. A. Bridgewater",
            "Joseph S. Kong",
            "Kamen M. Lozev",
            "Behnam A. Rezaei",
            "Vwani P. Roychowdhury"
        ],
        "summary": "We introduce BruNet, a general P2P software framework which we use to produce the first implementation of Symphony, a 1-D Kleinberg small-world architecture. Our framework is designed to easily implement and measure different P2P protocols over different transport layers such as TCP or UDP. This paper discusses our implementation of the Symphony network, which allows each node to keep $k \\le \\log N$ shortcut connections and to route to any other node with a short average delay of $O(\\frac{1}{k}\\log^2 N)$. %This provides a continuous trade-off between node degree and routing latency. We present experimental results taken from several PlanetLab deployments of size up to 1060 nodes. These succes sful deployments represent some of the largest PlanetLab deployments of P2P overlays found in the literature, and show our implementation's robustness to massive node dynamics in a WAN environment.",
        "published": "2007-09-25T22:09:17Z",
        "link": "http://arxiv.org/abs/0709.4048v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Distributed N-body Simulation on the Grid Using Dedicated Hardware",
        "authors": [
            "Derek Groen",
            "Simon Portegies Zwart",
            "Steve McMillan",
            "Jun Makino"
        ],
        "summary": "We present performance measurements of direct gravitational N -body simulation on the grid, with and without specialized (GRAPE-6) hardware. Our inter-continental virtual organization consists of three sites, one in Tokyo, one in Philadelphia and one in Amsterdam. We run simulations with up to 196608 particles for a variety of topologies. In many cases, high performance simulations over the entire planet are dominated by network bandwidth rather than latency. With this global grid of GRAPEs our calculation time remains dominated by communication over the entire range of N, which was limited due to the use of three sites. Increasing the number of particles will result in a more efficient execution. Based on these timings we construct and calibrate a model to predict the performance of our simulation on any grid infrastructure with or without GRAPE. We apply this model to predict the simulation performance on the Netherlands DAS-3 wide area computer. Equipping the DAS-3 with GRAPE-6Af hardware would achieve break-even between calculation and communication at a few million particles, resulting in a compute time of just over ten hours for 1 N -body time unit. Key words: high-performance computing, grid, N-body simulation, performance modelling",
        "published": "2007-09-28T08:48:31Z",
        "link": "http://arxiv.org/abs/0709.4552v2",
        "categories": [
            "astro-ph",
            "cs.DC"
        ]
    },
    {
        "title": "Practical Multiwriter Lock-Free Queues for \"Hard Real-Time\" Systems   without CAS",
        "authors": [
            "Jeremy Lee"
        ],
        "summary": "FIFO queues with a single reader and writer can be insufficient for \"hard real-time\" systems where interrupt handlers require wait-free guarantees when writing to message queues. We present an algorithm which elegantly and practically solves this problem on small processors that are often found in embedded systems. The algorithm does not require special CPU instructions (such as atomic CAS), and therefore is more robust than many existing methods that suffer the ABA problem associated with swing pointers. The algorithm gives \"first-in, almost first-out\" guarantees under pathological interrupt conditions, which manifests as arbitrary \"shoving\" among nearly-simultaneous arrivals at the end of the queue.",
        "published": "2007-09-28T09:04:27Z",
        "link": "http://arxiv.org/abs/0709.4558v1",
        "categories": [
            "cs.OS",
            "cs.DC",
            "E.1"
        ]
    },
    {
        "title": "An Analytical Study of a Structured Overlay in the presence of Dynamic   Membership",
        "authors": [
            "Supriya Krishnamurthy",
            "Sameh El-Ansary",
            "Erik Aurell",
            "Seif Haridi"
        ],
        "summary": "In this paper we present an analytical study of dynamic membership (aka churn) in structured peer-to-peer networks. We use a fluid model approach to describe steady-state or transient phenomena, and apply it to the Chord system. For any rate of churn and stabilization rates, and any system size, we accurately account for the functional form of the probability of network disconnection as well as the fraction of failed or incorrect successor and finger pointers. We show how we can use these quantities to predict both the performance and consistency of lookups under churn. All theoretical predictions match simulation results. The analysis includes both features that are generic to structured overlays deploying a ring as well as Chord-specific details, and opens the door to a systematic comparative analysis of, at least, ring-based structured overlay systems under churn.",
        "published": "2007-10-01T12:17:43Z",
        "link": "http://arxiv.org/abs/0710.0270v1",
        "categories": [
            "cs.NI",
            "cond-mat.stat-mech",
            "cs.DC"
        ]
    },
    {
        "title": "Comparing Maintenance Strategies for Overlays",
        "authors": [
            "Supriya Krishnamurthy",
            "Sameh El-Ansary",
            "Erik Aurell",
            "Seif Haridi"
        ],
        "summary": "In this paper, we present an analytical tool for understanding the performance of structured overlay networks under churn based on the master-equation approach of physics. We motivate and derive an equation for the average number of hops taken by lookups during churn, for the Chord network. We analyse this equation in detail to understand the behaviour with and without churn. We then use this understanding to predict how lookups will scale for varying peer population as well as varying the sizes of the routing tables. We then consider a change in the maintenance algorithm of the overlay, from periodic stabilisation to a reactive one which corrects fingers only when a change is detected. We generalise our earlier analysis to underdstand how the reactive strategy compares with the periodic one.",
        "published": "2007-10-01T20:45:04Z",
        "link": "http://arxiv.org/abs/0710.0386v1",
        "categories": [
            "cs.NI",
            "cond-mat.stat-mech",
            "cs.DC"
        ]
    },
    {
        "title": "Polish grid infrastructure for science and research",
        "authors": [
            "Ryszard Gokieli",
            "Krzysztof Nawrocki",
            "Adam Padee",
            "Dorota Stojda",
            "Karol Wawrzyniak",
            "Wojciech Wislicki"
        ],
        "summary": "Structure, functionality, parameters and organization of the computing Grid in Poland is described, mainly from the perspective of high-energy particle physics community, currently its largest consumer and developer. It represents distributed Tier-2 in the worldwide Grid infrastructure. It also provides services and resources for data-intensive applications in other sciences.",
        "published": "2007-10-07T17:45:59Z",
        "link": "http://arxiv.org/abs/0710.1436v1",
        "categories": [
            "cs.DC",
            "hep-ex",
            "C.2.4"
        ]
    },
    {
        "title": "Superrecursive Features of Interactive Computation",
        "authors": [
            "Mark Burgin"
        ],
        "summary": "Functioning and interaction of distributed devices and concurrent algorithms are analyzed in the context of the theory of algorithms. Our main concern here is how and under what conditions algorithmic interactive devices can be more powerful than the recursive models of computation, such as Turing machines. Realization of such a higher computing power makes these systems superrecursive. We find here five sources for superrecursiveness in interaction. In addition, we prove that when all of these sources are excluded, the algorithmic interactive system in question is able to perform only recursive computations. These results provide computer scientists with necessary and sufficient conditions for achieving superrecursiveness by algorithmic interactive devices.",
        "published": "2007-10-08T01:56:20Z",
        "link": "http://arxiv.org/abs/0710.1455v1",
        "categories": [
            "cs.DC",
            "cs.PF",
            "F.1.1; F.1.2; I.2.11; C.1.4; C.2.1"
        ]
    },
    {
        "title": "The structure and modeling results of the parallel spatial switching   system",
        "authors": [
            "Denis Kutuzov"
        ],
        "summary": "Problems of the switching parallel system designing provided spatial switching of packets from random time are discussed. Results of modeling of switching system as systems of mass service are resulted.",
        "published": "2007-10-08T08:39:50Z",
        "link": "http://arxiv.org/abs/0710.1484v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "C.1.4; B.4.3"
        ]
    },
    {
        "title": "Approximating max-min linear programs with local algorithms",
        "authors": [
            "Patrik Floréen",
            "Petteri Kaski",
            "Topi Musto",
            "Jukka Suomela"
        ],
        "summary": "A local algorithm is a distributed algorithm where each node must operate solely based on the information that was available at system startup within a constant-size neighbourhood of the node. We study the applicability of local algorithms to max-min LPs where the objective is to maximise $\\min_k \\sum_v c_{kv} x_v$ subject to $\\sum_v a_{iv} x_v \\le 1$ for each $i$ and $x_v \\ge 0$ for each $v$. Here $c_{kv} \\ge 0$, $a_{iv} \\ge 0$, and the support sets $V_i = \\{v : a_{iv} > 0 \\}$, $V_k = \\{v : c_{kv}>0 \\}$, $I_v = \\{i : a_{iv} > 0 \\}$ and $K_v = \\{k : c_{kv} > 0 \\}$ have bounded size. In the distributed setting, each agent $v$ is responsible for choosing the value of $x_v$, and the communication network is a hypergraph $\\mathcal{H}$ where the sets $V_k$ and $V_i$ constitute the hyperedges. We present inapproximability results for a wide range of structural assumptions; for example, even if $|V_i|$ and $|V_k|$ are bounded by some constants larger than 2, there is no local approximation scheme. To contrast the negative results, we present a local approximation algorithm which achieves good approximation ratios if we can bound the relative growth of the vertex neighbourhoods in $\\mathcal{H}$.",
        "published": "2007-10-08T09:46:47Z",
        "link": "http://arxiv.org/abs/0710.1499v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Designing a commutative replicated data type",
        "authors": [
            "Marc Shapiro",
            "Nuno Preguiça"
        ],
        "summary": "Commuting operations greatly simplify consistency in distributed systems. This paper focuses on designing for commutativity, a topic neglected previously. We show that the replicas of \\emph{any} data type for which concurrent operations commute converges to a correct value, under some simple and standard assumptions. We also show that such a data type supports transactions with very low cost. We identify a number of approaches and techniques to ensure commutativity. We re-use some existing ideas (non-destructive updates coupled with invariant identification), but propose a much more efficient implementation. Furthermore, we propose a new technique, background consensus. We illustrate these ideas with a shared edit buffer data type.",
        "published": "2007-10-09T14:38:50Z",
        "link": "http://arxiv.org/abs/0710.1784v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Symmetric and Synchronous Communication in Peer-to-Peer Networks",
        "authors": [
            "Andreas Witzel"
        ],
        "summary": "Motivated by distributed implementations of game-theoretical algorithms, we study symmetric process systems and the problem of attaining common knowledge between processes. We formalize our setting by defining a notion of peer-to-peer networks(*) and appropriate symmetry concepts in the context of Communicating Sequential Processes (CSP), due to the common knowledge creating effects of its synchronous communication primitives. We then prove that CSP with input and output guards makes common knowledge in symmetric peer-to-peer networks possible, but not the restricted version which disallows output statements in guards and is commonly implemented.   (*) Please note that we are not dealing with fashionable incarnations such as file-sharing networks, but merely use this name for a mathematical notion of a network consisting of directly connected peers \"treated on an equal footing\", i.e. not having a client-server structure or otherwise pre-determined roles.)",
        "published": "2007-10-11T16:14:38Z",
        "link": "http://arxiv.org/abs/0710.2284v4",
        "categories": [
            "cs.DC",
            "cs.GT"
        ]
    },
    {
        "title": "Deterministic Secure Positioning in Wireless Sensor Networks",
        "authors": [
            "Sylvie Delaët",
            "Partha Sarathi Mandal",
            "Mariusz Rokicki",
            "Sébastien Tixeuil"
        ],
        "summary": "Properly locating sensor nodes is an important building block for a large subset of wireless sensor networks (WSN) applications. As a result, the performance of the WSN degrades significantly when misbehaving nodes report false location and distance information in order to fake their actual location. In this paper we propose a general distributed deterministic protocol for accurate identification of faking sensors in a WSN. Our scheme does \\emph{not} rely on a subset of \\emph{trusted} nodes that are not allowed to misbehave and are known to every node in the network. Thus, any subset of nodes is allowed to try faking its position. As in previous approaches, our protocol is based on distance evaluation techniques developed for WSN. On the positive side, we show that when the received signal strength (RSS) technique is used, our protocol handles at most $\\lfloor \\frac{n}{2} \\rfloor-2$ faking sensors. Also, when the time of flight (ToF) technique is used, our protocol manages at most $\\lfloor \\frac{n}{2} \\rfloor - 3$ misbehaving sensors. On the negative side, we prove that no deterministic protocol can identify faking sensors if their number is $\\lceil \\frac{n}{2}\\rceil -1$. Thus our scheme is almost optimal with respect to the number of faking sensors. We discuss application of our technique in the trusted sensor model. More precisely our results can be used to minimize the number of trusted sensors that are needed to defeat faking ones.",
        "published": "2007-10-22T07:29:13Z",
        "link": "http://arxiv.org/abs/0710.3824v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.DS",
            "cs.NI"
        ]
    },
    {
        "title": "CANE: The Content Addressed Network Environment",
        "authors": [
            "Paul Gardner-Stephen"
        ],
        "summary": "The fragmented nature and asymmetry of local and remote file access and network access, combined with the current lack of robust authenticity and privacy, hamstrings the current internet. The collection of disjoint and often ad-hoc technologies currently in use are at least partially responsible for the magnitude and potency of the plagues besetting the information economy, of which spam and email borne virii are canonical examples. The proposed replacement for the internet, Internet Protocol Version 6 (IPv6), does little to tackle these underlying issues, instead concentrating on addressing the technical issues of a decade ago.   This paper introduces CANE, a Content Addressed Network Environment, and compares it against current internet and related technologies. Specifically, CANE presents a simple computing environment in which location is abstracted away in favour of identity, and trust is explicitly defined. Identity is cryptographically verified and yet remains pervasively open in nature. It is argued that this approach is capable of being generalised such that file storage and network access can be unified and subsequently combined with human interfaces to result in a Unified Theory of Access, which addresses many of the significant problems besetting the internet community of the early 21st century.",
        "published": "2007-10-26T07:20:13Z",
        "link": "http://arxiv.org/abs/0710.5006v1",
        "categories": [
            "cs.NI",
            "cs.CR",
            "cs.DC",
            "C.2.0; C.2.2; C.2.4; D.4.2; D.4.3; D.4.4; D.4.6; D.4.7; E.2; E.3;\n  E.5; H.1.2; H.2.4; H.3.2; H.3.4; H.5.2; I.3.2; I.3.3; I.3.6"
        ]
    },
    {
        "title": "Towards Grid Monitoring and deployment in Jade, using ProActive",
        "authors": [
            "Cristian Ruz",
            "Françoise Baude",
            "Virginie Legrand Contes"
        ],
        "summary": "This document describes our current effort to gridify Jade, a java-based environment for the autonomic management of clustered J2EE application servers, developed in the INRIA SARDES research team. Towards this objective, we use the java ProActive grid technology. We first present some of the challenges to turn such an autonomic management system initially dedicated to distributed applications running on clusters of machines, into one that can provide self-management capabilities to large-scale systems, i.e. deployed on grid infrastructures. This leads us to a brief state of the art on grid monitoring systems. Then, we recall the architecture of Jade, and consequently propose to reorganize it in a potentially more scalable way. Practical experiments pertain to the use of the grid deployment feature offered by ProActive to easily conduct the deployment of the Jade system or its revised version on any sort of grid.",
        "published": "2007-10-29T10:44:35Z",
        "link": "http://arxiv.org/abs/0710.5348v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Resource and Application Models for Advanced Grid Schedulers",
        "authors": [
            "Aleksandar Lazarevic",
            "Lionel Sacks"
        ],
        "summary": "As Grid computing is becoming an inevitable future, managing, scheduling and monitoring dynamic, heterogeneous resources will present new challenges. Solutions will have to be agile and adaptive, support self-organization and autonomous management, while maintaining optimal resource utilisation. Presented in this paper are basic principles and architectural concepts for efficient resource allocation in heterogeneous Grid environment.",
        "published": "2007-11-02T14:03:46Z",
        "link": "http://arxiv.org/abs/0711.0314v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Measuring and Monitoring Grid Resource Utilisation",
        "authors": [
            "Aleksandar Lazarevic",
            "Lionel Sacks"
        ],
        "summary": "Effective resource utilisation monitoring and highly granular yet adaptive measurements are prerequisites for a more efficient Grid scheduler. We present a suite of measurement applications able to monitor per-process resource utilisation, and a customisable tool for emulating observed utilisation models.",
        "published": "2007-11-02T14:12:27Z",
        "link": "http://arxiv.org/abs/0711.0315v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A Study of Grid Applications: Scheduling Perspective",
        "authors": [
            "Aleksandar Lazarevic",
            "Lionel Sacks"
        ],
        "summary": "As the Grid evolves from a high performance cluster middleware to a multipurpose utility computing framework, a good understanding of Grid applications, their statistics and utilisation patterns is required. This study looks at job execution times and resource utilisations in a Grid environment, and their significance in cluster and network dimensioning, local level scheduling and resource management.",
        "published": "2007-11-02T14:15:45Z",
        "link": "http://arxiv.org/abs/0711.0316v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Self-Organising management of Grid environments",
        "authors": [
            "Ioannis Liabotis",
            "Ognjen Prnjat",
            "Tope Olukemi",
            "Adrian Li Mow Ching",
            "Aleksandar Lazarevic",
            "Lionel Sacks",
            "Mike Fisher",
            "Paul McKee"
        ],
        "summary": "This paper presents basic concepts, architectural principles and algorithms for efficient resource and security management in cluster computing environments and the Grid. The work presented in this paper is funded by BTExacT and the EPSRC project SO-GRM (GR/S21939).",
        "published": "2007-11-02T15:26:48Z",
        "link": "http://arxiv.org/abs/0711.0325v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Enabling Adaptive Grid Scheduling and Resource Management",
        "authors": [
            "Aleksandar Lazarevic",
            "Lionel Sacks",
            "Ognjen Prnjat"
        ],
        "summary": "Wider adoption of the Grid concept has led to an increasing amount of federated computational, storage and visualisation resources being available to scientists and researchers. Distributed and heterogeneous nature of these resources renders most of the legacy cluster monitoring and management approaches inappropriate, and poses new challenges in workflow scheduling on such systems. Effective resource utilisation monitoring and highly granular yet adaptive measurements are prerequisites for a more efficient Grid scheduler. We present a suite of measurement applications able to monitor per-process resource utilisation, and a customisable tool for emulating observed utilisation models. We also outline our future work on a predictive and probabilistic Grid scheduler. The research is undertaken as part of UK e-Science EPSRC sponsored project SO-GRM (Self-Organising Grid Resource Management) in cooperation with BT.",
        "published": "2007-11-02T15:30:53Z",
        "link": "http://arxiv.org/abs/0711.0326v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Managing Uncertainty: A Case for Probabilistic Grid Scheduling",
        "authors": [
            "Aleksandar Lazarevic",
            "Lionel Sacks",
            "Ognjen Prnjat"
        ],
        "summary": "The Grid technology is evolving into a global, service-orientated architecture, a universal platform for delivering future high demand computational services. Strong adoption of the Grid and the utility computing concept is leading to an increasing number of Grid installations running a wide range of applications of different size and complexity. In this paper we address the problem of elivering deadline/economy based scheduling in a heterogeneous application environment using statistical properties of job historical executions and its associated meta-data. This approach is motivated by a study of six-month computational load generated by Grid applications in a multi-purpose Grid cluster serving a community of twenty e-Science projects. The observed job statistics, resource utilisation and user behaviour is discussed in the context of management approaches and models most suitable for supporting a probabilistic and autonomous scheduling architecture.",
        "published": "2007-11-02T15:36:36Z",
        "link": "http://arxiv.org/abs/0711.0327v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Web-based Interface in Public Cluster",
        "authors": [
            "Z. Akbar",
            "L. T. Handoko"
        ],
        "summary": "A web-based interface dedicated for cluster computer which is publicly accessible for free is introduced. The interface plays an important role to enable secure public access, while providing user-friendly computational environment for end-users and easy maintainance for administrators as well. The whole architecture which integrates both aspects of hardware and software is briefly explained. It is argued that the public cluster is globally a unique approach, and could be a new kind of e-learning system especially for parallel programming communities.",
        "published": "2007-11-04T16:39:47Z",
        "link": "http://arxiv.org/abs/0711.0528v1",
        "categories": [
            "cs.DC",
            "cs.CY"
        ]
    },
    {
        "title": "A parallel gravitational N-body kernel",
        "authors": [
            "Simon Portegies Zwart",
            "Steve McMillan",
            "Derek Groen",
            "Alessia Gualandris",
            "Michael Sipior",
            "Willem Vermin"
        ],
        "summary": "We describe source code level parallelization for the {\\tt kira} direct gravitational $N$-body integrator, the workhorse of the {\\tt starlab} production environment for simulating dense stellar systems. The parallelization strategy, called ``j-parallelization'', involves the partition of the computational domain by distributing all particles in the system among the available processors. Partial forces on the particles to be advanced are calculated in parallel by their parent processors, and are then summed in a final global operation. Once total forces are obtained, the computing elements proceed to the computation of their particle trajectories. We report the results of timing measurements on four different parallel computers, and compare them with theoretical predictions. The computers employ either a high-speed interconnect, a NUMA architecture to minimize the communication overhead or are distributed in a grid. The code scales well in the domain tested, which ranges from 1024 - 65536 stars on 1 - 128 processors, providing satisfactory speedup. Running the production environment on a grid becomes inefficient for more than 60 processors distributed across three sites.",
        "published": "2007-11-05T14:11:32Z",
        "link": "http://arxiv.org/abs/0711.0643v1",
        "categories": [
            "astro-ph",
            "cs.DC"
        ]
    },
    {
        "title": "Optimizing Latency and Reliability of Pipeline Workflow Applications",
        "authors": [
            "Anne Benoit",
            "Veronika Rehn-Sonigo",
            "Yves Robert"
        ],
        "summary": "Mapping applications onto heterogeneous platforms is a difficult challenge, even for simple application patterns such as pipeline graphs. The problem is even more complex when processors are subject to failure during the execution of the application. In this paper, we study the complexity of a bi-criteria mapping which aims at optimizing the latency (i.e., the response time) and the reliability (i.e., the probability that the computation will be successful) of the application. Latency is minimized by using faster processors, while reliability is increased by replicating computations on a set of processors. However, replication increases latency (additional communications, slower processors). The application fails to be executed only if all the processors fail during execution. While simple polynomial algorithms can be found for fully homogeneous platforms, the problem becomes NP-hard when tackling heterogeneous platforms. This is yet another illustration of the additional complexity added by heterogeneity.",
        "published": "2007-11-08T14:45:12Z",
        "link": "http://arxiv.org/abs/0711.1231v3",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A Mobile Computing Architecture for Numerical Simulation",
        "authors": [
            "Cyril Dumont",
            "Fabrice Mourlin"
        ],
        "summary": "The domain of numerical simulation is a place where the parallelization of numerical code is common. The definition of a numerical context means the configuration of resources such as memory, processor load and communication graph, with an evolving feature: the resources availability. A feature is often missing: the adaptability. It is not predictable and the adaptable aspect is essential. Without calling into question these implementations of these codes, we create an adaptive use of these implementations. Because the execution has to be driven by the availability of main resources, the components of a numeric computation have to react when their context changes. This paper offers a new architecture, a mobile computing architecture, based on mobile agents and JavaSpace. At the end of this paper, we apply our architecture to several case studies and obtain our first results.",
        "published": "2007-11-12T14:39:34Z",
        "link": "http://arxiv.org/abs/0711.1786v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Autoregressive Time Series Forecasting of Computational Demand",
        "authors": [
            "Thomas Sandholm"
        ],
        "summary": "We study the predictive power of autoregressive moving average models when forecasting demand in two shared computational networks, PlanetLab and Tycoon. Demand in these networks is very volatile, and predictive techniques to plan usage in advance can improve the performance obtained drastically.   Our key finding is that a random walk predictor performs best for one-step-ahead forecasts, whereas ARIMA(1,1,0) and adaptive exponential smoothing models perform better for two and three-step-ahead forecasts. A Monte Carlo bootstrap test is proposed to evaluate the continuous prediction performance of different models with arbitrary confidence and statistical significance levels. Although the prediction results differ between the Tycoon and PlanetLab networks, we observe very similar overall statistical properties, such as volatility dynamics.",
        "published": "2007-11-14T00:57:13Z",
        "link": "http://arxiv.org/abs/0711.2062v1",
        "categories": [
            "cs.DC",
            "G.3"
        ]
    },
    {
        "title": "A System for Distributed Mechanisms: Design, Implementation and   Applications",
        "authors": [
            "Krzysztof R. Apt",
            "Farhad Arbab",
            "Huiye Ma"
        ],
        "summary": "We describe here a structured system for distributed mechanism design appropriate for both Intranet and Internet applications. In our approach the players dynamically form a network in which they know neither their neighbours nor the size of the network and interact to jointly take decisions. The only assumption concerning the underlying communication layer is that for each pair of processes there is a path of neighbours connecting them. This allows us to deal with arbitrary network topologies.   We also discuss the implementation of this system which consists of a sequence of layers. The lower layers deal with the operations that implement the basic primitives of distributed computing, namely low level communication and distributed termination, while the upper layers use these primitives to implement high level communication among players, including broadcasting and multicasting, and distributed decision making.   This yields a highly flexible distributed system whose specific applications are realized as instances of its top layer. This design is implemented in Java.   The system supports at various levels fault-tolerance and includes a provision for distributed policing the purpose of which is to exclude `dishonest' players. Also, it can be used for repeated creation of dynamically formed networks of players interested in a joint decision making implemented by means of a tax-based mechanism. We illustrate its flexibility by discussing a number of implemented examples.",
        "published": "2007-11-16T14:10:16Z",
        "link": "http://arxiv.org/abs/0711.2618v4",
        "categories": [
            "cs.DC",
            "cs.GT",
            "C.2.4; J.4"
        ]
    },
    {
        "title": "Weak vs. Self vs. Probabilistic Stabilization",
        "authors": [
            "Stéphane Devismes",
            "Sébastien Tixeuil",
            "Masafumi Yamashita"
        ],
        "summary": "Self-stabilization is a strong property that guarantees that a network always resume correct behavior starting from an arbitrary initial state. Weaker guarantees have later been introduced to cope with impossibility results: probabilistic stabilization only gives probabilistic convergence to a correct behavior. Also, weak stabilization only gives the possibility of convergence. In this paper, we investigate the relative power of weak, self, and probabilistic stabilization, with respect to the set of problems that can be solved. We formally prove that in that sense, weak stabilization is strictly stronger that self-stabilization. Also, we refine previous results on weak stabilization to prove that, for practical schedule instances, a deterministic weak-stabilizing protocol can be turned into a probabilistic self-stabilizing one. This latter result hints at more practical use of weak-stabilization, as such algorthms are easier to design and prove than their (probabilistic) self-stabilizing counterparts.",
        "published": "2007-11-23T07:17:25Z",
        "link": "http://arxiv.org/abs/0711.3672v2",
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.NI"
        ]
    },
    {
        "title": "An Adaptive Checkpointing Scheme for Peer-to-Peer Based Volunteer   Computing Work Flows",
        "authors": [
            "Lei Ni",
            "Aaron Harwood"
        ],
        "summary": "Volunteer Computing, sometimes called Public Resource Computing, is an emerging computational model that is very suitable for work-pooled parallel processing. As more complex grid applications make use of work flows in their design and deployment it is reasonable to consider the impact of work flow deployment over a Volunteer Computing infrastructure. In this case, the inter work flow I/O can lead to a significant increase in I/O demands at the work pool server. A possible solution is the use of a Peer-to- Peer based parallel computing architecture to off-load this I/O demand to the workers; where the workers can fulfill some aspects of work flow coordination and I/O checking, etc. However, achieving robustness in such a large scale system is a challenging hurdle towards the decentralized execution of work flows and general parallel processes. To increase robustness, we propose and show the merits of using an adaptive checkpoint scheme that efficiently checkpoints the status of the parallel processes according to the estimation of relevant network and peer parameters. Our scheme uses statistical data observed during runtime to dynamically make checkpoint decisions in a completely de- centralized manner. The results of simulation show support for our proposed approach in terms of reduced required runtime.",
        "published": "2007-11-26T06:41:23Z",
        "link": "http://arxiv.org/abs/0711.3949v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Transactional WaveCache: Towards Speculative and Out-of-Order DataFlow   Execution of Memory Operations",
        "authors": [
            "Leandro A. J. Marzulo",
            "Felipe M. G. França",
            "Vítor Santos Costa"
        ],
        "summary": "The WaveScalar is the first DataFlow Architecture that can efficiently provide the sequential memory semantics required by imperative languages. This work presents an alternative memory ordering mechanism for this architecture, the Transaction WaveCache. Our mechanism maintains the execution order of memory operations within blocks of code, called Waves, but adds the ability to speculatively execute, out-of-order, operations from different waves. This ordering mechanism is inspired by progress in supporting Transactional Memories. Waves are considered as atomic regions and executed as nested transactions. If a wave has finished the execution of all its memory operations, as soon as the previous waves are committed, it can be committed. If a hazard is detected in a speculative Wave, all the following Waves (children) are aborted and re-executed. We evaluate the WaveCache on a set artificial benchmarks. If the benchmark does not access memory often, we could achieve speedups of around 90%. Speedups of 33.1% and 24% were observed on more memory intensive applications, and slowdowns up to 16% arise if memory bandwidth is a bottleneck. For an application full of WAW, WAR and RAW hazards, a speedup of 139.7% was verified.",
        "published": "2007-12-07T15:59:37Z",
        "link": "http://arxiv.org/abs/0712.1167v1",
        "categories": [
            "cs.AR",
            "cs.DC",
            "C.1.3"
        ]
    },
    {
        "title": "Human-Machine Symbiosis, 50 Years On",
        "authors": [
            "Ian Foster"
        ],
        "summary": "Licklider advocated in 1960 the construction of computers capable of working symbiotically with humans to address problems not easily addressed by humans working alone. Since that time, many of the advances that he envisioned have been achieved, yet the time spent by human problem solvers in mundane activities remains large. I propose here four areas in which improved tools can further advance the goal of enhancing human intellect: services, provenance, knowledge communities, and automation of problem-solving protocols.",
        "published": "2007-12-13T23:00:37Z",
        "link": "http://arxiv.org/abs/0712.2255v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.HC"
        ]
    },
    {
        "title": "The Earth System Grid: Supporting the Next Generation of Climate   Modeling Research",
        "authors": [
            "David Bernholdt",
            "Shishir Bharathi",
            "David Brown",
            "Kasidit Chanchio",
            "Meili Chen",
            "Ann Chervenak",
            "Luca Cinquini",
            "Bob Drach",
            "Ian Foster",
            "Peter Fox",
            "Jose Garcia",
            "Carl Kesselman",
            "Rob Markel",
            "Don Middleton",
            "Veronika Nefedova",
            "Line Pouchard",
            "Arie Shoshani",
            "Alex Sim",
            "Gary Strand",
            "Dean Williams"
        ],
        "summary": "Understanding the earth's climate system and how it might be changing is a preeminent scientific challenge. Global climate models are used to simulate past, present, and future climates, and experiments are executed continuously on an array of distributed supercomputers. The resulting data archive, spread over several sites, currently contains upwards of 100 TB of simulation data and is growing rapidly. Looking toward mid-decade and beyond, we must anticipate and prepare for distributed climate research data holdings of many petabytes. The Earth System Grid (ESG) is a collaborative interdisciplinary project aimed at addressing the challenge of enabling management, discovery, access, and analysis of these critically important datasets in a distributed and heterogeneous computational environment. The problem is fundamentally a Grid problem. Building upon the Globus toolkit and a variety of other technologies, ESG is developing an environment that addresses authentication, authorization for data access, large-scale data transport and management, services and abstractions for high-performance remote data access, mechanisms for scalable data replication, cataloging with rich semantic and syntactic information, data discovery, distributed monitoring, and Web-based portals for using the system.",
        "published": "2007-12-13T23:39:04Z",
        "link": "http://arxiv.org/abs/0712.2262v1",
        "categories": [
            "cs.CE",
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Data access optimizations for highly threaded multi-core CPUs with   multiple memory controllers",
        "authors": [
            "Georg Hager",
            "Thomas Zeiser",
            "Gerhard Wellein"
        ],
        "summary": "Processor and system architectures that feature multiple memory controllers are prone to show bottlenecks and erratic performance numbers on codes with regular access patterns. Although such effects are well known in the form of cache thrashing and aliasing conflicts, they become more severe when memory access is involved. Using the new Sun UltraSPARC T2 processor as a prototypical multi-core design, we analyze performance patterns in low-level and application benchmarks and show ways to circumvent bottlenecks by careful data layout and padding.",
        "published": "2007-12-14T08:14:20Z",
        "link": "http://arxiv.org/abs/0712.2302v2",
        "categories": [
            "cs.DC",
            "cs.PF"
        ]
    },
    {
        "title": "Middleware-based Database Replication: The Gaps between Theory and   Practice",
        "authors": [
            "Emmanuel Cecchet",
            "George Candea",
            "Anastasia Ailamaki"
        ],
        "summary": "The need for high availability and performance in data management systems has been fueling a long running interest in database replication from both academia and industry. However, academic groups often attack replication problems in isolation, overlooking the need for completeness in their solutions, while commercial teams take a holistic approach that often misses opportunities for fundamental innovation. This has created over time a gap between academic research and industrial practice.   This paper aims to characterize the gap along three axes: performance, availability, and administration. We build on our own experience developing and deploying replication systems in commercial and academic settings, as well as on a large body of prior related work. We sift through representative examples from the last decade of open-source, academic, and commercial database replication systems and combine this material with case studies from real systems deployed at Fortune 500 customers. We propose two agendas, one for academic research and one for industrial R&D, which we believe can bridge the gap within 5-10 years. This way, we hope to both motivate and help researchers in making the theory and practice of middleware-based database replication more relevant to each other.",
        "published": "2007-12-17T18:42:15Z",
        "link": "http://arxiv.org/abs/0712.2773v2",
        "categories": [
            "cs.DB",
            "cs.DC",
            "cs.PF",
            "H.2; C.2.4; C.4; D.4.5"
        ]
    },
    {
        "title": "RZBENCH: Performance evaluation of current HPC architectures using   low-level and application benchmarks",
        "authors": [
            "Georg Hager",
            "Holger Stengel",
            "Thomas Zeiser",
            "Gerhard Wellein"
        ],
        "summary": "RZBENCH is a benchmark suite that was specifically developed to reflect the requirements of scientific supercomputer users at the University of Erlangen-Nuremberg (FAU). It comprises a number of application and low-level codes under a common build infrastructure that fosters maintainability and expandability. This paper reviews the structure of the suite and briefly introduces the most relevant benchmarks. In addition, some widely known standard benchmark codes are reviewed in order to emphasize the need for a critical review of often-cited performance results. Benchmark data is presented for the HLRB-II at LRZ Munich and a local InfiniBand Woodcrest cluster as well as two uncommon system architectures: A bandwidth-optimized InfiniBand cluster based on single socket nodes (\"Port Townsend\") and an early version of Sun's highly threaded T2 architecture (\"Niagara 2\").",
        "published": "2007-12-20T12:15:26Z",
        "link": "http://arxiv.org/abs/0712.3389v1",
        "categories": [
            "cs.DC",
            "cs.PF"
        ]
    },
    {
        "title": "Distributed Slicing in Dynamic Systems",
        "authors": [
            "Antonio Fernandez",
            "Vincent Gramoli",
            "Ernesto Jimenez",
            "Anne-Marie Kermarrec",
            "Michel Raynal"
        ],
        "summary": "Peer to peer (P2P) systems are moving from application specific architectures to a generic service oriented design philosophy. This raises interesting problems in connection with providing useful P2P middleware services capable of dealing with resource assignment and management in a large-scale, heterogeneous and unreliable environment. The slicing service, has been proposed to allow for an automatic partitioning of P2P networks into groups (slices) that represent a controllable amount of some resource and that are also relatively homogeneous with respect to that resource. In this paper we propose two gossip-based algorithms to solve the distributed slicing problem. The first algorithm speeds up an existing algorithm sorting a set of uniform random numbers. The second algorithm statistically approximates the rank of nodes in the ordering. The scalability, efficiency and resilience to dynamics of both algorithms rely on their gossip-based models. These algorithms are proved viable theoretically and experimentally.",
        "published": "2007-12-26T13:55:47Z",
        "link": "http://arxiv.org/abs/0712.3980v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Exact Quantum Algorithms for the Leader Election Problem",
        "authors": [
            "Seiichiro Tani",
            "Hirotada Kobayashi",
            "Keiji Matsumoto"
        ],
        "summary": "This paper gives the first separation of quantum and classical pure (i.e., non-cryptographic) computing abilities with no restriction on the amount of available computing resources, by considering the exact solvability of a celebrated unsolvable problem in classical distributed computing, the ``leader election problem'' on anonymous networks. The goal of the leader election problem is to elect a unique leader from among distributed parties. The paper considers this problem for anonymous networks, in which each party has the same identifier. It is well-known that no classical algorithm can solve exactly (i.e., in bounded time without error) the leader election problem in anonymous networks, even if it is given the number of parties. This paper gives two quantum algorithms that, given the number of parties, can exactly solve the problem for any network topology in polynomial rounds and polynomial communication/time complexity with respect to the number of parties, when the parties are connected by quantum communication links.",
        "published": "2007-12-27T10:52:52Z",
        "link": "http://arxiv.org/abs/0712.4213v1",
        "categories": [
            "quant-ph",
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "A Virtual Logo Keyboard for People with Motor Disabilities",
        "authors": [
            "Stephane Norte",
            "Fernando G. Lobo"
        ],
        "summary": "In our society, people with motor impairments are oftentimes socially excluded from their environment. This is unfortunate because every human being should have the possibility to obtain the necessary conditions to live a normal life. Although there is technology to assist people with motor impairments, few systems are targeted for programming environments. We have created a system, called Logo Keyboard, to assist people with motor disabilities to program with the Logo programming language. With this special keyboard we can help more people to get involved into computer programming and to develop projects in different areas.",
        "published": "2007-01-31T16:36:43Z",
        "link": "http://arxiv.org/abs/cs/0701199v2",
        "categories": [
            "cs.HC",
            "H.5.2; K.3.1; K.4.2"
        ]
    },
    {
        "title": "Measuring Cognitive Activities in Software Engineering",
        "authors": [
            "Pierre Robillard",
            "Patrick D'Astous",
            "Françoise Détienne",
            "Willemien Visser"
        ],
        "summary": "This paper presents an approach to the study of cognitive activities in collaborative software development. This approach has been developed by a multidisciplinary team made up of software engineers and cognitive psychologists. The basis of this approach is to improve our understanding of software development by observing professionals at work. The goal is to derive lines of conduct or good practices based on observations and analyses of the processes that are naturally used by software engineers. The strategy involved is derived from a standard approach in cognitive science. It is based on the videotaping of the activities of software engineers, transcription of the videos, coding of the transcription, defining categories from the coded episodes and defining cognitive behaviors or dialogs from the categories. This project presents two original contributions that make this approach generic in software engineering. The first contribution is the introduction of a formal hierarchical coding scheme, which will enable comparison of various types of observations. The second is the merging of psychological and statistical analysis approaches to build a cognitive model. The details of this new approach are illustrated with the initial data obtained from the analysis of technical review meetings.",
        "published": "2007-02-01T09:01:55Z",
        "link": "http://arxiv.org/abs/cs/0702001v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "The Effect of Object-Oriented Programming Expertise in Several   Dimensions of Comprehension Strategies",
        "authors": [
            "Jean-Marie Burkhardt",
            "Françoise Détienne",
            "Susan Wiedenbeck"
        ],
        "summary": "This study analyzes object-oriented (OO) program comprehension by experts and novices. We examine the effect of expertise in three dimensions of comprehension strategies: the scope of the comprehension, the top-down versus bottom-up direction of the processes, and the guidance of the comprehension activity. Overall, subjects were similar in the scope of their comprehension, although the experts tended to consult more files. We found strong evidence of top-down, inference-driven behaviors, as well as multiple guidance in expert comprehension. We also found evidence of execution-based guidance and less use of top-down processes in novice comprehension. Guidance by inheritance and composition relationships in the OO program was not dominant, but nevertheless played a substantial role in expert program comprehension. However, these static relationships more closely tied to the OO nature of the program were exploited poorly by novices. To conclude, these results are discussed with respect to the literature on procedural program comprehension.",
        "published": "2007-02-01T09:02:17Z",
        "link": "http://arxiv.org/abs/cs/0702002v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Expert Programming Knowledge: a Schema-Based Approach",
        "authors": [
            "Françoise Détienne"
        ],
        "summary": "The topic of this chapter is the role of expert programming knowledge in the understanding activity. In the \"schema-based approach\", the role of semantic structures is emphasized whereas, in the \"control-flow approach\", the role of syntactic structures is emphasized. Data which support schema-based models of understanding are presented. Data which are more consistent with the \"control-flow approach\" allow to discuss the limits of the former kind of models.",
        "published": "2007-02-01T09:02:38Z",
        "link": "http://arxiv.org/abs/cs/0702003v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "What model(s) for program understanding?",
        "authors": [
            "Françoise Détienne"
        ],
        "summary": "The first objective of this paper is to present and discuss various types of models of program understanding. They are discussed in relation to models of text understanding. The second objective of this paper is to assess the effect of purpose for reading, or more specifically programming task, on the cognitive processes involved and representations constructed in program understanding. This is done in the theoretical framework of van Dijk and Kintsch's model of text understanding (1983).",
        "published": "2007-02-01T09:03:19Z",
        "link": "http://arxiv.org/abs/cs/0702004v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "An empirical study of software reuse by experts in object-oriented   design",
        "authors": [
            "Jean-Marie Burkhardt",
            "Françoise Détienne"
        ],
        "summary": "This paper presents an empirical study of the software reuse activity by expert designers in the context of object-oriented design. Our study focuses on the three following aspects of reuse : (1) the interaction between some design processes, e.g. constructing a problem representation, searching for and evaluating solutions, and reuse processes, i.e. retrieving and using previous solutions, (2) the mental processes involved in reuse, e.g. example-based retrieval or bottom-up versus top-down expanding of the solution, and (3) the mental representations constructed throughout the reuse activity, e.g. dynamic versus static representations. Some implications of these results for the specification of software reuse support environments are discussed.",
        "published": "2007-02-01T09:03:59Z",
        "link": "http://arxiv.org/abs/cs/0702005v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Negotiation in collaborative assessment of design solutions: an   empirical study on a Concurrent Engineering process",
        "authors": [
            "Géraldine Martin",
            "Françoise Détienne",
            "Elisabeth Lavigne"
        ],
        "summary": "In Concurrent engineering, design solutions are not only produced by individuals specialized in a given field. Due to the team nature of the design activity, solutions are negotiated. Our objective is to analyse the argumentation processes leading to these negotiated solutions. These processes take place in the meetings which group together specialists with a co-design aim. We conducted cognitive ergonomics research work during the definition phase of an aeronautical design project in which the participants work in Concurrent Engineering. We recorded, retranscribed and analysed 7 multi-speciality meetings. These meetings were organised, as needed, to assess the integration of the solutions of each speciality into a global solution. We found that there are three main design proposal assessment modes which can be combined in these meetings: (a) analytical assessment mode, (b) comparative assessment mode (c) analogical assessment mode. Within these assessment modes, different types of arguments are used. Furthermore we found a typical temporal negotiation process.",
        "published": "2007-02-01T09:05:13Z",
        "link": "http://arxiv.org/abs/cs/0702006v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Coupling Control and Human-Centered Automation in Mathematical Models of   Complex Systems",
        "authors": [
            "Roderick V. N. Melnik"
        ],
        "summary": "In this paper we analyze mathematically how human factors can be effectively incorporated into the analysis and control of complex systems. As an example, we focus our discussion around one of the key problems in the Intelligent Transportation Systems (ITS) theory and practice, the problem of speed control, considered here as a decision making process with limited information available. The problem is cast mathematically in the general framework of control problems and is treated in the context of dynamically changing environments where control is coupled to human-centered automation. Since in this case control might not be limited to a small number of control settings, as it is often assumed in the control literature, serious difficulties arise in the solution of this problem. We demonstrate that the problem can be reduced to a set of Hamilton-Jacobi-Bellman equations where human factors are incorporated via estimations of the system Hamiltonian. In the ITS context, these estimations can be obtained with the use of on-board equipment like sensors/receivers/actuators, in-vehicle communication devices, etc. The proposed methodology provides a way to integrate human factor into the solving process of the models for other complex dynamic systems.",
        "published": "2007-02-25T11:09:12Z",
        "link": "http://arxiv.org/abs/cs/0702149v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "cs.HC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Strategies in object-oriented design",
        "authors": [
            "Sophie Chatel",
            "Françoise Détienne"
        ],
        "summary": "This paper presents a study aiming to analyse the design strategies of experts in object-oriented programming. We report an experiment conducted with four experts. Each subject solved three problems. Our results show that three strategies may be used in program design according to the solution structure. An object-centred strategy and a function-centred strategy are used when the solution has a hierarchical structure with vertical communication between objects. In this case, the plan which guides the design activity is declarative. A procedure-centred strategy is used when the solution has a flat structure with horizontal communication between objects. In this case, the plan which guides the design activity is procedural. These results are discussed in relation with results on design strategies in procedural design. Furthermore, our results provide insight into the knowledge structures of experts in object-oriented design. To conclude, we point out limitations of this study and discuss implications of our results for Human-Computer Interaction systems, in particular for systems assisting experts in their design activity.",
        "published": "2007-03-02T12:50:12Z",
        "link": "http://arxiv.org/abs/cs/0703008v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "A Methodological Framework for Socio-Cognitive Analyses of Collaborative   Design of Open Source Software",
        "authors": [
            "Warren Sack",
            "Françoise Détienne",
            "Nicholas Ducheneaut",
            "Jean-Marie Burkhardt",
            "Dilan Mahendran",
            "Flore Barcellini"
        ],
        "summary": "Open Source Software (OSS) development challenges traditional software engineering practices. In particular, OSS projects are managed by a large number of volunteers, working freely on the tasks they choose to undertake. OSS projects also rarely rely on explicit system-level design, or on project plans or schedules. Moreover, OSS developers work in arbitrary locations and collaborate almost exclusively over the Internet, using simple tools such as email and software code tracking databases (e.g. CVS). All the characteristics above make OSS development akin to weaving a tapestry of heterogeneous components. The OSS design process relies on various types of actors: people with prescribed roles, but also elements coming from a variety of information spaces (such as email and software code). The objective of our research is to understand the specific hybrid weaving accomplished by the actors of this distributed, collective design process. This, in turn, challenges traditional methodologies used to understand distributed software engineering: OSS development is simply too \"fibrous\" to lend itself well to analysis under a single methodological lens. In this paper, we describe the methodological framework we articulated to analyze collaborative design in the Open Source world. Our framework focuses on the links between the heterogeneous components of a project's hybrid network. We combine ethnography, text mining, and socio-technical network analysis and visualization to understand OSS development in its totality. This way, we are able to simultaneously consider the social, technical, and cognitive aspects of OSS development. We describe our methodology in detail, and discuss its implications for future research on distributed collective practices.",
        "published": "2007-03-02T13:51:29Z",
        "link": "http://arxiv.org/abs/cs/0703009v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "BrlAPI: Simple, Portable, Concurrent, Application-level Control of   Braille Terminals",
        "authors": [
            "Samuel Thibault",
            "Sebastien Hinderer"
        ],
        "summary": "Screen readers can drive braille devices for allowing visually impaired users to access computer environments, by providing them the same information as sighted users. But in some cases, this view is not easy to use on a braille device. In such cases, it would be much more useful to let applications provide their own braille feedback, specially adapted to visually impaired users. Such applications would then need the ability to output braille ; however, allowing both screen readers and applications access a wide panel of braille devices is not a trivial task. We present an abstraction layer that applications may use to communicate with braille devices. They do not need to deal with the specificities of each device, but can do so if necessary. We show how several applications can communicate with one braille device concurrently, with BrlAPI making sensible choices about which application eventually gets access to the device. The description of a widely used implementation of BrlAPI is included.",
        "published": "2007-03-09T16:55:23Z",
        "link": "http://arxiv.org/abs/cs/0703044v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Flexible Audio Streams",
        "authors": [
            "Paul Fodor"
        ],
        "summary": "Tremendous research effort was invested in audio browsers and machine learning techniques to decode the structure of Web pages in order to put them into an audio format. In this paper, we address a simpler and efficient solution for the creation of an audio browser of VOICEXML generated from RSS/Atom stream feeds. We developed a multimodal (audio and graphical) portal application that offers RSS/Atom feeds. By utilizing sing our system, the user can interact using voice or graphic commands, listen and watch digital content, such as news, blogs feeds, podcasts, and even access email and personal schedules. The portal system permits the use of security credentials (user/password authentication) to collect secure RSS/Atom stream in the multimodal browser to connect the user to specific personal services. A series experiments have been conducted to evaluate the performance of the RSS reader and navigator. Our system is extremely beneficial for a wide range of applications, from interfaces for the visual impaired users to browsers for mobile telephonic interfaces.",
        "published": "2007-03-15T00:00:06Z",
        "link": "http://arxiv.org/abs/cs/0703070v3",
        "categories": [
            "cs.HC",
            "H.5.2"
        ]
    },
    {
        "title": "Social Information Processing in Social News Aggregation",
        "authors": [
            "Kristina Lerman"
        ],
        "summary": "The rise of the social media sites, such as blogs, wikis, Digg and Flickr among others, underscores the transformation of the Web to a participatory medium in which users are collaboratively creating, evaluating and distributing information. The innovations introduced by social media has lead to a new paradigm for interacting with information, what we call 'social information processing'. In this paper, we study how social news aggregator Digg exploits social information processing to solve the problems of document recommendation and rating. First, we show, by tracking stories over time, that social networks play an important role in document recommendation. The second contribution of this paper consists of two mathematical models. The first model describes how collaborative rating and promotion of stories emerges from the independent decisions made by many users. The second model describes how a user's influence, the number of promoted stories and the user's social network, changes in time. We find qualitative agreement between predictions of the model and user data gathered from Digg.",
        "published": "2007-03-15T22:37:22Z",
        "link": "http://arxiv.org/abs/cs/0703087v2",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.HC",
            "cs.MA"
        ]
    },
    {
        "title": "Supporting Knowledge and Expertise Finding within Australia's Defence   Science and Technology Organisation",
        "authors": [
            "Paul Prekop"
        ],
        "summary": "This paper reports on work aimed at supporting knowledge and expertise finding within a large Research and Development (R&D) organisation. The paper first discusses the nature of knowledge important to R&D organisations and presents a prototype information system developed to support knowledge and expertise finding. The paper then discusses a trial of the system within an R&D organisation, the implications and limitations of the trial, and discusses future research questions.",
        "published": "2007-04-11T06:49:06Z",
        "link": "http://arxiv.org/abs/0704.1353v1",
        "categories": [
            "cs.OH",
            "cs.DB",
            "cs.DL",
            "cs.HC"
        ]
    },
    {
        "title": "Personalizing Image Search Results on Flickr",
        "authors": [
            "Kristina Lerman",
            "Anon Plangprasopchok",
            "Chio Wong"
        ],
        "summary": "The social media site Flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. Flickr offers multiple ways of browsing or searching it. One option is tag search, which returns all images tagged with a specific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. We claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. We show how to exploit this metadata to personalize search results for the user, thereby improving search performance. First, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. Secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. The users' interests can similarly be described by the tags they used for annotating their images. The latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.",
        "published": "2007-04-12T23:31:04Z",
        "link": "http://arxiv.org/abs/0704.1676v1",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CY",
            "cs.DL",
            "cs.HC"
        ]
    },
    {
        "title": "Narratives within immersive technologies",
        "authors": [
            "Joan Llobera"
        ],
        "summary": "The main goal of this project is to research technical advances in order to enhance the possibility to develop narratives within immersive mediated environments. An important part of the research is concerned with the question of how a script can be written, annotated and realized for an immersive context. A first description of the main theoretical framework and the ongoing work and a first script example is provided. This project is part of the program for presence research, and it will exploit physiological feedback and Computational Intelligence within virtual reality.",
        "published": "2007-04-19T14:27:25Z",
        "link": "http://arxiv.org/abs/0704.2542v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Sabbath Day Home Automation: \"It's Like Mixing Technology and Religion\"",
        "authors": [
            "Allison Woodruff",
            "Sally Augustin",
            "Brooke Foucault"
        ],
        "summary": "We present a qualitative study of 20 American Orthodox Jewish families' use of home automation for religious purposes. These lead users offer insight into real-life, long-term experience with home automation technologies. We discuss how automation was seen by participants to contribute to spiritual experience and how participants oriented to the use of automation as a religious custom. We also discuss the relationship of home automation to family life. We draw design implications for the broader population, including surrender of control as a design resource, home technologies that support long-term goals and lifestyle choices, and respite from technology.",
        "published": "2007-04-27T00:42:22Z",
        "link": "http://arxiv.org/abs/0704.3643v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Evaluating Personal Archiving Strategies for Internet-based Information",
        "authors": [
            "Catherine C. Marshall",
            "Frank McCown",
            "Michael L. Nelson"
        ],
        "summary": "Internet-based personal digital belongings present different vulnerabilities than locally stored materials. We use responses to a survey of people who have recovered lost websites, in combination with supplementary interviews, to paint a fuller picture of current curatorial strategies and practices. We examine the types of personal, topical, and commercial websites that respondents have lost and the reasons they have lost this potentially valuable material. We further explore what they have tried to recover and how the loss influences their subsequent practices. We found that curation of personal digital materials in online stores bears some striking similarities to the curation of similar materials stored locally in that study participants continue to archive personal assets by relying on a combination of benign neglect, sporadic backups, and unsystematic file replication. However, we have also identified issues specific to Internet-based material: how risk is spread by distributing the files among multiple servers and services; the circular reasoning participants use when they discuss the safety of their digital assets; and the types of online material that are particularly vulnerable to loss. The study reveals ways in which expectations of permanence and notification are violated and situations in which benign neglect has far greater consequences for the long-term fate of important digital assets.",
        "published": "2007-04-27T01:38:55Z",
        "link": "http://arxiv.org/abs/0704.3647v1",
        "categories": [
            "cs.DL",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "The Long Term Fate of Our Digital Belongings: Toward a Service Model for   Personal Archives",
        "authors": [
            "Catherine C. Marshall",
            "Sara Bly",
            "Francoise Brun-Cottan"
        ],
        "summary": "We conducted a preliminary field study to understand the current state of personal digital archiving in practice. Our aim is to design a service for the long-term storage, preservation, and access of digital belongings by examining how personal archiving needs intersect with existing and emerging archiving technologies, best practices, and policies. Our findings not only confirmed that experienced home computer users are creating, receiving, and finding an increasing number of digital belongings, but also that they have already lost irreplaceable digital artifacts such as photos, creative efforts, and records. Although participants reported strategies such as backup and file replication for digital safekeeping, they were seldom able to implement them consistently. Four central archiving themes emerged from the data: (1) people find it difficult to evaluate the worth of accumulated materials; (2) personal storage is highly distributed both on- and offline; (3) people are experiencing magnified curatorial problems associated with managing files in the aggregate, creating appropriate metadata, and migrating materials to maintainable formats; and (4) facilities for long-term access are not supported by the current desktop metaphor. Four environmental factors further complicate archiving in consumer settings: the pervasive influence of malware; consumer reliance on ad hoc IT providers; an accretion of minor system and registry inconsistencies; and strong consumer beliefs about the incorruptibility of digital forms, the reliability of digital technologies, and the social vulnerability of networked storage.",
        "published": "2007-04-27T02:35:57Z",
        "link": "http://arxiv.org/abs/0704.3653v1",
        "categories": [
            "cs.DL",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "An Automated Evaluation Metric for Chinese Text Entry",
        "authors": [
            "Mike Tian-Jian Jiang",
            "James Zhan",
            "Jaimie Lin",
            "Jerry Lin",
            "Wen-Lien Hsu"
        ],
        "summary": "In this paper, we propose an automated evaluation metric for text entry. We also consider possible improvements to existing text entry evaluation metrics, such as the minimum string distance error rate, keystrokes per character, cost per correction, and a unified approach proposed by MacKenzie, so they can accommodate the special characteristics of Chinese text. Current methods lack an integrated concern about both typing speed and accuracy for Chinese text entry evaluation. Our goal is to remove the bias that arises due to human factors. First, we propose a new metric, called the correction penalty (P), based on Fitts' law and Hick's law. Next, we transform it into the approximate amortized cost (AAC) of information theory. An analysis of the AAC of Chinese text input methods with different context lengths is also presented.",
        "published": "2007-04-27T05:34:10Z",
        "link": "http://arxiv.org/abs/0704.3662v1",
        "categories": [
            "cs.HC",
            "cs.CL"
        ]
    },
    {
        "title": "On the Development of Text Input Method - Lessons Learned",
        "authors": [
            "Mike Tian-Jian Jiang",
            "Deng Liu",
            "Meng-Juei Hsieh",
            "Wen-Lien Hsu"
        ],
        "summary": "Intelligent Input Methods (IM) are essential for making text entries in many East Asian scripts, but their application to other languages has not been fully explored. This paper discusses how such tools can contribute to the development of computer processing of other oriental languages. We propose a design philosophy that regards IM as a text service platform, and treats the study of IM as a cross disciplinary subject from the perspectives of software engineering, human-computer interaction (HCI), and natural language processing (NLP). We discuss these three perspectives and indicate a number of possible future research directions.",
        "published": "2007-04-27T05:58:32Z",
        "link": "http://arxiv.org/abs/0704.3665v1",
        "categories": [
            "cs.CL",
            "cs.HC"
        ]
    },
    {
        "title": "Can the Internet cope with stress?",
        "authors": [
            "Andreas Martin Lisewski"
        ],
        "summary": "When will the Internet become aware of itself? In this note the problem is approached by asking an alternative question: Can the Internet cope with stress? By extrapolating the psychological difference between coping and defense mechanisms a distributed software experiment is outlined which could reject the hypothesis that the Internet is not a conscious entity.",
        "published": "2007-05-01T15:44:17Z",
        "link": "http://arxiv.org/abs/0705.0025v1",
        "categories": [
            "cs.HC",
            "cs.AI"
        ]
    },
    {
        "title": "NodeTrix: Hybrid Representation for Analyzing Social Networks",
        "authors": [
            "Nathalie Henry",
            "Jean-Daniel Fekete",
            "Michael Mcguffin"
        ],
        "summary": "The need to visualize large social networks is growing as hardware capabilities make analyzing large networks feasible and many new data sets become available. Unfortunately, the visualizations in existing systems do not satisfactorily answer the basic dilemma of being readable both for the global structure of the network and also for detailed analysis of local communities. To address this problem, we present NodeTrix, a hybrid representation for networks that combines the advantages of two traditional representations: node-link diagrams are used to show the global structure of a network, while arbitrary portions of the network can be shown as adjacency matrices to better support the analysis of communities. A key contribution is a set of interaction techniques. These allow analysts to create a NodeTrix visualization by dragging selections from either a node-link or a matrix, flexibly manipulate the NodeTrix representation to explore the dataset, and create meaningful summary visualizations of their findings. Finally, we present a case study applying NodeTrix to the analysis of the InfoVis 2004 coauthorship dataset to illustrate the capabilities of NodeTrix as both an exploration tool and an effective means of communicating results.",
        "published": "2007-05-04T11:50:07Z",
        "link": "http://arxiv.org/abs/0705.0599v3",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Subjective Evaluation of Forms in an Immersive Environment",
        "authors": [
            "Jean-François Petiot",
            "Damien Chablat"
        ],
        "summary": "User's perception of product, by essence subjective, is a major topic in marketing and industrial design. Many methods, based on users' tests, are used so as to characterise this perception. We are interested in three main methods: multidimensional scaling, semantic differential method, and preference mapping. These methods are used to built a perceptual space, in order to position the new product, to specify requirements by the study of user's preferences, to evaluate some product attributes, related in particular to style (aesthetic). These early stages of the design are primordial for a good orientation of the project. In parallel, virtual reality tools and interfaces are more and more efficient for suggesting to the user complex feelings, and creating in this way various levels of perceptions. In this article, we present on an example the use of multidimensional scaling, semantic differential method and preference mapping for the subjective assessment of virtual products. These products, which geometrical form is variable, are defined with a CAD model and are proposed to the user with a spacemouse and stereoscopic glasses. Advantages and limitations of such evaluation is next discussed..",
        "published": "2007-05-10T06:54:11Z",
        "link": "http://arxiv.org/abs/0705.1395v1",
        "categories": [
            "cs.HC",
            "cs.RO"
        ]
    },
    {
        "title": "Subjective Information Measure and Rate Fidelity Theory",
        "authors": [
            "Chenguang Lu"
        ],
        "summary": "Using fish-covering model, this paper intuitively explains how to extend Hartley's information formula to the generalized information formula step by step for measuring subjective information: metrical information (such as conveyed by thermometers), sensory information (such as conveyed by color vision), and semantic information (such as conveyed by weather forecasts). The pivotal step is to differentiate condition probability and logical condition probability of a message. The paper illustrates the rationality of the formula, discusses the coherence of the generalized information formula and Popper's knowledge evolution theory. For optimizing data compression, the paper discusses rate-of-limiting-errors and its similarity to complexity-distortion based on Kolmogorov's complexity theory, and improves the rate-distortion theory into the rate-fidelity theory by replacing Shannon's distortion with subjective mutual information. It is proved that both the rate-distortion function and the rate-fidelity function are equivalent to a rate-of-limiting-errors function with a group of fuzzy sets as limiting condition, and can be expressed by a formula of generalized mutual information for lossy coding, or by a formula of generalized entropy for lossless coding. By analyzing the rate-fidelity function related to visual discrimination and digitized bits of pixels of images, the paper concludes that subjective information is less than or equal to objective (Shannon's) information; there is an optimal matching point at which two kinds of information are equal; the matching information increases with visual discrimination (defined by confusing probability) rising; for given visual discrimination, too high resolution of images or too much objective information is wasteful.",
        "published": "2007-05-24T19:33:43Z",
        "link": "http://arxiv.org/abs/0705.3644v1",
        "categories": [
            "cs.IT",
            "cs.HC",
            "math.IT",
            "H.1.1"
        ]
    },
    {
        "title": "A collaborative framework to exchange and share product information   within a supply chain context",
        "authors": [
            "Hichem Geryville",
            "Yacine Ouzrout",
            "Abdelaziz Bouras",
            "Nikolaos Sapidis"
        ],
        "summary": "The new requirement for \"collaboration\" between multidisciplinary collaborators induces to exchange and share adequate information on the product, processes throughout the products' lifecycle. Thus, effective capture of information, and also its extraction, recording, exchange, sharing, and reuse become increasingly critical. These lead companies to adopt new improved methodologies in managing the exchange and sharing of information. The aim of this paper is to describe a collaborative framework system to exchange and share information, which is based on: (i) The Product Process Collaboration Organization model (PPCO) which defines product and process information, and the various collaboration methods for the organizations involved in the supply chain. (ii) Viewpoint model describes relationships between each actor and the comprehensive Product/Process model, defining each actor's \"domain of interest\" within the evolving product definition. (iii) A layer which defines the comprehensive organization and collaboration relationships between the actors within the supply chain. (iv) Based on the above relationships, the last layer proposes a typology of exchanged messages. A communication method, based on XML, is developed that supports optimal exchange/sharing of information. To illustrate the proposed framework system, an example is presented related to collaborative design of a new piston for an automotive engine. The focus is on user-viewpoint integration to ensure that the adequate information is retrieved from the PPCO.",
        "published": "2007-06-04T19:48:07Z",
        "link": "http://arxiv.org/abs/0706.0507v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "On Anomalies in Annotation Systems",
        "authors": [
            "Matthias R. Brust",
            "Steffen Rothkugel"
        ],
        "summary": "Today's computer-based annotation systems implement a wide range of functionalities that often go beyond those available in traditional paper-and-pencil annotations. Conceptually, annotation systems are based on thoroughly investigated psycho-sociological and pedagogical learning theories. They offer a huge diversity of annotation types that can be placed in textual as well as in multimedia format. Additionally, annotations can be published or shared with a group of interested parties via well-organized repositories. Although highly sophisticated annotation systems exist both conceptually as well as technologically, we still observe that their acceptance is somewhat limited. In this paper, we argue that nowadays annotation systems suffer from several fundamental problems that are inherent in the traditional paper-and-pencil annotation paradigm. As a solution, we propose to shift the annotation paradigm for the implementation of annotation system.",
        "published": "2007-06-07T21:23:40Z",
        "link": "http://arxiv.org/abs/0706.1087v1",
        "categories": [
            "cs.HC",
            "cs.CY"
        ]
    },
    {
        "title": "Redesigning Computer-based Learning Environments: Evaluation as   Communication",
        "authors": [
            "Matthias R. Brust",
            "Christian M. Adriano",
            "Ivan M. L. Ricarte"
        ],
        "summary": "In the field of evaluation research, computer scientists live constantly upon dilemmas and conflicting theories. As evaluation is differently perceived and modeled among educational areas, it is not difficult to become trapped in dilemmas, which reflects an epistemological weakness. Additionally, designing and developing a computer-based learning scenario is not an easy task. Advancing further, with end-users probing the system in realistic settings, is even harder. Computer science research in evaluation faces an immense challenge, having to cope with contributions from several conflicting and controversial research fields. We believe that deep changes must be made in our field if we are to advance beyond the CBT (computer-based training) learning model and to build an adequate epistemology for this challenge. The first task is to relocate our field by building upon recent results from philosophy, psychology, social sciences, and engineering. In this article we locate evaluation in respect to communication studies. Evaluation presupposes a definition of goals to be reached, and we suggest that it is, by many means, a silent communication between teacher and student, peers, and institutional entities. If we accept that evaluation can be viewed as set of invisible rules known by nobody, but somehow understood by everybody, we should add anthropological inquiries to our research toolkit. The paper is organized around some elements of the social communication and how they convey new insights to evaluation research for computer and related scientists. We found some technical limitations and offer discussions on how we relate to technology at same time we establish expectancies and perceive others work.",
        "published": "2007-06-08T08:12:21Z",
        "link": "http://arxiv.org/abs/0706.1127v1",
        "categories": [
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "A Communication Model for Adaptive Service Provisioning in Hybrid   Wireless Networks",
        "authors": [
            "Matthias R. Brust",
            "Steffen Rothkugel"
        ],
        "summary": "Mobile entities with wireless links are able to form a mobile ad-hoc network. Such an infrastructureless network does not have to be administrated. However, self-organizing principles have to be applied to deal with upcoming problems, e.g. information dissemination. These kinds of problems are not easy to tackle, requiring complex algorithms. Moreover, the usefulness of pure ad-hoc networks is arguably limited. Hence, enthusiasm for mobile ad-hoc networks, which could eliminate the need for any fixed infrastructure, has been damped. The goal is to overcome the limitations of pure ad-hoc networks by augmenting them with instant Internet access, e.g. via integration of UMTS respectively GSM links. However, this raises multiple questions at the technical as well as the organizational level. Motivated by characteristics of small-world networks that describe an efficient network even without central or organized design, this paper proposes to combine mobile ad-hoc networks and infrastructured networks to form hybrid wireless networks. One main objective is to investigate how this approach can reduce the costs of a permanent backbone link and providing in the same way the benefits of useful information from Internet connectivity or service providers. For the purpose of bridging between the different types of networks, an adequate middleware service is the focus of our investigation. This paper shows our first steps forward to this middleware by introducing the Injection Communication paradigm as principal concept.",
        "published": "2007-06-08T08:23:14Z",
        "link": "http://arxiv.org/abs/0706.1130v1",
        "categories": [
            "cs.NI",
            "cs.AR",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "The multiple viewpoints as approach to information retrieval within   collaborative development context",
        "authors": [
            "Hichem Geryville",
            "Yacine Ouzrout",
            "Abdelaziz Bouras",
            "Nikolaos Sapidis"
        ],
        "summary": "Nowadays, to achieve competitive advantage, the industrial companies are considering that success is sustained to great product development. That is to manage the product throughout its entire lifecycle. Achieving this goal requires a tight collaboration between actors from a wide variety of domains, using different software tools producing various product data types and formats. The actors' collaboration is mainly based on the exchange /share product information. The representation of the actors' viewpoints is the underlying requirement of the collaborative product development. The multiple viewpoints approach was designed to provide an organizational framework following the actors' perspectives in the collaboration, and their relationships. The approach acknowledges the inevitability of multiple integration of product information as different views, promotes gathering of actors' interest, and encourages retrieved adequate information while providing support for integration through PLM and/or SCM collaboration. In this paper, a multiple viewpoints representation is proposed. The product, process, organization information models are discussed. A series of issues referring to the viewpoints representation are discussed in detail. Based on XML standard, taking electrical connector as an example, an application case of part of product information modeling is stated.",
        "published": "2007-06-08T11:29:24Z",
        "link": "http://arxiv.org/abs/0706.1162v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Developing a Collaborative and Autonomous Training and Learning   Environment for Hybrid Wireless Networks",
        "authors": [
            "Jose Eduardo M. Lobo",
            "Jorge Luis Risco Becerra",
            "Matthias R. Brust",
            "Steffen Rothkugel",
            "Christian M. Adriano"
        ],
        "summary": "With larger memory capacities and the ability to link into wireless networks, more and more students uses palmtop and handheld computers for learning activities. However, existing software for Web-based learning is not well-suited for such mobile devices, both due to constrained user interfaces as well as communication effort required. A new generation of applications for the learning domain that is explicitly designed to work on these kinds of small mobile devices has to be developed. For this purpose, we introduce CARLA, a cooperative learning system that is designed to act in hybrid wireless networks. As a cooperative environment, CARLA aims at disseminating teaching material, notes, and even components of itself through both fixed and mobile networks to interested nodes. Due to the mobility of nodes, CARLA deals with upcoming problems such as network partitions and synchronization of teaching material, resource dependencies, and time constraints.",
        "published": "2007-06-08T16:38:32Z",
        "link": "http://arxiv.org/abs/0706.1201v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "cs.NI",
            "K.3.1"
        ]
    },
    {
        "title": "Analyzing Design Process and Experiments on the AnITA Generic Tutoring   System",
        "authors": [
            "Matthias R. Brust",
            "Steffen Rothkugel"
        ],
        "summary": "In the field of tutoring systems, investigations have shown that there are many tutoring systems specific to a specific domain that, because of their static architecture, cannot be adapted to other domains. As consequence, often neither methods nor knowledge can be reused. In addition, the knowledge engineer must have programming skills in order to enhance and evaluate the system. One particular challenge is to tackle these problems with the development of a generic tutoring system. AnITA, as a stand-alone application, has been developed and implemented particularly for this purpose. However, in the testing phase, we discovered that this architecture did not fully match the user's intuitive understanding of the use of a learning tool. Therefore, AnITA has been redesigned to exclusively work as a client/server application and renamed to AnITA2. This paper discusses the evolvements made on the AnITA tutoring system, the goal of which is to use generic principles for system re-use in any domain. Two experiments were conducted, and the results are presented in this paper.",
        "published": "2007-06-11T06:09:25Z",
        "link": "http://arxiv.org/abs/0706.1402v2",
        "categories": [
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "Le travail collaboratif dans le cadre d'un projet architectural",
        "authors": [
            "Marie-France Ango-Obiang"
        ],
        "summary": "The analysis of the practices and the tendencies of the users at the time of the search for information on Internet makes it possible to highlight several points. The search for information becomes powerful after knowledge of the typology of the various systems of research. This typology supports the adoption of a methodology of research which one can characterize by pull systems, intelligent agents, etc. In addition, the importance of the structure of the electronic document, correctly elaborated in advance, will support a higher relevance ratio to find information. In our article, the problems turn around the study of the behavior of the users in situation of search for information, as well as the constitution of a pole of documentary resources within a framework of an architectural project. It is noted that the evolution of the documentary resources is related to information technologies.",
        "published": "2007-06-12T20:06:38Z",
        "link": "http://arxiv.org/abs/0706.1780v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "EasyVoice: Integrating voice synthesis with Skype",
        "authors": [
            "Paulo A. Condado",
            "Fernando G. Lobo"
        ],
        "summary": "This paper presents EasyVoice, a system that integrates voice synthesis with Skype. EasyVoice allows a person with voice disabilities to talk with another person located anywhere in the world, removing an important obstacle that affect these people during a phone or VoIP-based conversation.",
        "published": "2007-06-21T12:04:40Z",
        "link": "http://arxiv.org/abs/0706.3132v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "K.4.2; H.4.3; H.5.2"
        ]
    },
    {
        "title": "A solution for actors' viewpoints representation with collaborative   product development",
        "authors": [
            "Hichem Geryville",
            "Abdelaziz Bouras",
            "Yacine Ouzrout",
            "Nikolaos Sapidis"
        ],
        "summary": "As product complexity and marketing competition increase, a collaborative product development is necessary for companies which develop high quality products in short lead-times. To support product actors from different fields, disciplines, and locations, wishing to exchange and share information, the representation of the actors' viewpoints is the underlying requirement of the collaborative product development. The actors' viewpoints approach was designed to provide an organisational framework following the actors' perspectives in the collaboration, and their relationships, could be explicitly gathered and formatted. The approach acknowledges the inevitability of multiple integration of product information as different views, promotes gathering of actors' interests, and encourages retrieved adequate information while providing support for integration through PLM and/or SCM collaboration. In this paper, a solution for neutral viewpoints representation is proposed. The product, process, and organisation information models are seriatim discussed. A series of issues referring to the viewpoints representation are discussed in detail. Based on XML standard, taking cyclone vessel as an example, an application case of part of product information modelling is stated.",
        "published": "2007-06-21T14:40:25Z",
        "link": "http://arxiv.org/abs/0706.3165v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "User driven applications - new design paradigm",
        "authors": [
            "Sergey Andreyev"
        ],
        "summary": "Programs for complicated engineering and scientific tasks always have to deal with a problem of showing numerous graphical results. The limits of the screen space and often opposite requirements from different users are the cause of the infinite discussions between designers and users, but the source of this ongoing conflict is not in the level of interface design, but in the basic principle of current graphical output: user may change some views and details, but in general the output view is absolutely defined and fixed by the developer. Author was working for several years on the algorithm that will allow eliminating this problem thus allowing stepping from designer-driven applications to user-driven. Such type of applications in which user is deciding what, when and how to show on the screen, is the dream of scientists and engineers working on the analysis of the most complicated tasks. The new paradigm is based on movable and resizable graphics, and such type of graphics can be widely used not only for scientific and engineering applications.",
        "published": "2007-06-28T13:19:04Z",
        "link": "http://arxiv.org/abs/0706.4224v1",
        "categories": [
            "cs.GR",
            "cs.HC"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the   Astrobiology Phone-cam",
        "authors": [
            "Alexandra Bartolo",
            "Patrick C. McGuire",
            "Kenneth P. Camilleri",
            "Christopher Spiteri",
            "Jonathan C. Borg",
            "Philip J. Farrugia",
            "Jens Ormo",
            "Javier Gomez-Elvira",
            "Jose Antonio Rodriguez-Manfredi",
            "Enrique Diaz-Martinez",
            "Helge Ritter",
            "Robert Haschke",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "We have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. This camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. We envision that the `Astrobiology Phone-cam' exploration system can be fruitfully used in other problem domains as well.",
        "published": "2007-07-05T15:19:37Z",
        "link": "http://arxiv.org/abs/0707.0808v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.NI",
            "cs.RO",
            "cs.SE"
        ]
    },
    {
        "title": "IRVO: an Interaction Model for designing Collaborative Mixed Reality   systems",
        "authors": [
            "René Chalon",
            "Bertrand T. David"
        ],
        "summary": "This paper presents an interaction model adapted to mixed reality environments known as IRVO (Interacting with Real and Virtual Objects). IRVO aims at modeling the interaction between one or more users and the Mixed Reality system by representing explicitly the objects and tools involved and their relationship. IRVO covers the design phase of the life cycle and models the intended use of the system. In a first part, we present a brief review of related HCI models. The second part is devoted to the IRVO model, its notation and some examples. In the third part, we present how IRVO is used for designing applications and in particular we show how this model can be integrated in a Model-Based Approach (CoCSys) which is currently designed at our lab.",
        "published": "2007-07-10T16:01:30Z",
        "link": "http://arxiv.org/abs/0707.1480v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "The Trade-offs with Space Time Cube Representation of Spatiotemporal   Patterns",
        "authors": [
            "Per Ola Kristensson",
            "Nils Dahlback",
            "Daniel Anundi",
            "Marius Bjornstad",
            "Hanna Gillberg",
            "Jonas Haraldsson",
            "Ingrid Martensson",
            "Matttias Nordvall",
            "Josefin Stahl"
        ],
        "summary": "Space time cube representation is an information visualization technique where spatiotemporal data points are mapped into a cube. Fast and correct analysis of such information is important in for instance geospatial and social visualization applications. Information visualization researchers have previously argued that space time cube representation is beneficial in revealing complex spatiotemporal patterns in a dataset to users. The argument is based on the fact that both time and spatial information are displayed simultaneously to users, an effect difficult to achieve in other representations. However, to our knowledge the actual usefulness of space time cube representation in conveying complex spatiotemporal patterns to users has not been empirically validated. To fill this gap we report on a between-subjects experiment comparing novice users error rates and response times when answering a set of questions using either space time cube or a baseline 2D representation. For some simple questions the error rates were lower when using the baseline representation. For complex questions where the participants needed an overall understanding of the spatiotemporal structure of the dataset, the space time cube representation resulted in on average twice as fast response times with no difference in error rates compared to the baseline. These results provide an empirical foundation for the hypothesis that space time cube representation benefits users when analyzing complex spatiotemporal patterns.",
        "published": "2007-07-11T13:39:34Z",
        "link": "http://arxiv.org/abs/0707.1618v1",
        "categories": [
            "cs.HC",
            "cs.GR"
        ]
    },
    {
        "title": "Practical Approach to Knowledge-based Question Answering with Natural   Language Understanding and Advanced Reasoning",
        "authors": [
            "Wilson Wong"
        ],
        "summary": "This research hypothesized that a practical approach in the form of a solution framework known as Natural Language Understanding and Reasoning for Intelligence (NaLURI), which combines full-discourse natural language understanding, powerful representation formalism capable of exploiting ontological information and reasoning approach with advanced features, will solve the following problems without compromising practicality factors: 1) restriction on the nature of question and response, and 2) limitation to scale across domains and to real-life natural language text.",
        "published": "2007-07-24T14:30:27Z",
        "link": "http://arxiv.org/abs/0707.3559v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.IR",
            "I.2.7; H.5.2; H.3.4; H.3.3"
        ]
    },
    {
        "title": "The Review and Analysis of Human Computer Interaction (HCI) Principles",
        "authors": [
            "V. Hinze-Hoare"
        ],
        "summary": "The History of HCI is briefly reviewed together with three HCI models and structure including CSCW, CSCL and CSCR. It is shown that a number of authorities consider HCI to be a fragmented discipline with no agreed set of unifying design principles. An analysis of usability criteria based upon citation frequency of authors is performed in order to discover the eight most recognised HCI principles.",
        "published": "2007-07-24T20:19:47Z",
        "link": "http://arxiv.org/abs/0707.3638v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "An Application of Chromatic Prototypes",
        "authors": [
            "Matthew McCool"
        ],
        "summary": "This paper has been withdrawn.",
        "published": "2007-08-04T02:38:19Z",
        "link": "http://arxiv.org/abs/0708.0598v2",
        "categories": [
            "cs.HC",
            "cs.MM"
        ]
    },
    {
        "title": "A Portal Analysis for the Design of a Collaborative Research Environment   for Students and Supervisors (CRESS) within the CSCR Domain",
        "authors": [
            "V. Hinze-Hoare"
        ],
        "summary": "In a previous paper the CSCR domain was defined. Here this is taken to the next stage where we consider the design of a particular Collaborative Research Environment to support Students and Supervisors CRESS. Following the CSCR structure a preliminary design for CRESS has been established and a portal framework analysis is undertaken in order to determine the most appropriate set of tools for its implementation.",
        "published": "2007-08-07T19:53:24Z",
        "link": "http://arxiv.org/abs/0708.0877v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Designing a Collaborative Research Environment for Students and their   Supervisors (CRESS)",
        "authors": [
            "V. Hinze-Hoare"
        ],
        "summary": "In a previous paper the CSCR domain was defined. Here this is taken to the next stage where the design of a particular Collaborative Research Environment to support Students and Supervisors (CRESS) is considered. Following the CSCR structure this paper deals with an analysis of 13 collaborative working environments to determine a preliminary design for CRESS in order to discover the most appropriate set of tools for its implementation.",
        "published": "2007-08-12T19:58:37Z",
        "link": "http://arxiv.org/abs/0708.1624v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Design: One, but in different forms",
        "authors": [
            "Willemien Visser"
        ],
        "summary": "This overview paper defends an augmented cognitively oriented generic-design hypothesis: there are both significant similarities between the design activities implemented in different situations and crucial differences between these and other cognitive activities; yet, characteristics of a design situation (related to the design process, the designers, and the artefact) introduce specificities in the corresponding cognitive activities and structures that are used, and in the resulting designs. We thus augment the classical generic-design hypothesis with that of different forms of designing. We review the data available in the cognitive design research literature and propose a series of candidates underlying such forms of design, outlining a number of directions requiring further elaboration.",
        "published": "2007-08-13T15:00:37Z",
        "link": "http://arxiv.org/abs/0708.1725v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Browsing through 3D representations of unstructured picture collections:   an empirical study",
        "authors": [
            "Olivier Christmann",
            "Noëlle Carbonell"
        ],
        "summary": "The paper presents a 3D interactive representation of fairly large picture collections which facilitates browsing through unstructured sets of icons or pictures. Implementation of this representation implies choosing between two visualization strategies: users may either manipulate the view (OV) or be immersed in it (IV). The paper first presents this representation, then describes an empirical study (17 participants) aimed at assessing the utility and usability of each view. Subjective judgements in questionnaires and debriefings were varied: 7 participants preferred the IV view, 4 the OV one, and 6 could not choose between the two. Visual acuity and visual exploration strategies seem to have exerted a greater influence on participants' preferences than task performance or feeling of immersion.",
        "published": "2007-08-24T13:33:38Z",
        "link": "http://arxiv.org/abs/0708.3341v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Gaze as a Supplementary Modality for Interacting with Ambient   Intelligence Environments",
        "authors": [
            "Daniel Gepner",
            "Jérôme Simonin",
            "Noëlle Carbonell"
        ],
        "summary": "We present our current research on the implementation of gaze as an efficient and usable pointing modality supplementary to speech, for interacting with augmented objects in our daily environment or large displays, especially immersive virtual reality environments, such as reality centres and caves. We are also addressing issues relating to the use of gaze as the main interaction input modality. We have designed and developed two operational user interfaces: one for providing motor-disabled users with easy gaze-based access to map applications and graphical software; the other for iteratively testing and improving the usability of gaze-contingent displays.",
        "published": "2007-08-26T18:53:41Z",
        "link": "http://arxiv.org/abs/0708.3505v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "How really effective are Multimodal Hints in enhancing Visual Target   Spotting? Some evidence from a usability study",
        "authors": [
            "Suzanne Kieffer",
            "Noëlle Carbonell"
        ],
        "summary": "The main aim of the work presented here is to contribute to computer science advances in the multimodal usability area, in-as-much as it addresses one of the major issues relating to the generation of effective oral system messages: how to design messages which effectively help users to locate specific graphical objects in information visualisations? An experimental study was carried out to determine whether oral messages including coarse information on the locations of graphical objects on the current display may facilitate target detection tasks sufficiently for making it worth while to integrate such messages in GUIs. The display spatial layout varied in order to test the influence of visual presentation structure on the contribution of these messages to facilitating visual search on crowded displays. Finally, three levels of task difficulty were defined, based mainly on the target visual complexity and the number of distractors in the scene. The findings suggest that spatial information messages improve participants' visual search performances significantly; they are more appropriate to radial structures than to matrix, random and elleptic structures; and, they are particularly useful for performing difficult visual search tasks.",
        "published": "2007-08-27T11:53:35Z",
        "link": "http://arxiv.org/abs/0708.3575v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Ambient Multimodality: an Asset for Developing Universal Access to the   Information Society",
        "authors": [
            "Noëlle Carbonell"
        ],
        "summary": "The paper tries to point out the benefits that can be derived from research advances in the implementation of concepts such as ambient intelligence (AmI) and ubiquitous or pervasive computing for promoting Universal Access (UA) to the Information Society, that is, for contributing to enable everybody, especially Physically Disabled (PD) people, to have easy access to all computing resources and information services that the coming worldwide Information Society will soon make available to the general public. Following definitions of basic concepts relating to multimodal interaction, the significant contribution of multimodality to developing UA is briefly argued. Then, a short state of the art in AmI research is presented. In the last section we bring out the potential contribution of advances in AmI research and technology to the improvement of computer access for PD people. This claim is supported by the following observations: (i) most projects aiming at implementing AmI focus on the design of new interaction modalities and flexible multimodal user interfaces which may facilitate PD users' computer access ; (ii) targeted applications will support users in a wide range of daily activities which will be performed simultaneously with supporting computing tasks; therefore, users will be placed in contexts where they will be confronted with similar difficulties to those encountered by PD users; (iii) AmI applications being intended for the general public, a wide range of new interaction devices and flexible processing software will be available, making it possible to provide PD users with human-computer facilities tailored to their specific needs at reasonable expense..",
        "published": "2007-08-27T12:11:56Z",
        "link": "http://arxiv.org/abs/0708.3580v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Plate-forme Magicien d'Oz pour l'étude de l'apport des ACAs à   l'interaction",
        "authors": [
            "Jérôme Simonin",
            "Marius Hategan",
            "Noëlle Carbonell"
        ],
        "summary": "In order to evaluate the contribution of Embodied (Animated) Conversational Agents (ECAs) to the effectiveness and usability of human-computer interaction, we developed a software platform meant to collect usage data. This platform, which implements the wizard of Oz paradigm, makes it possible to simulate user interfaces integrating ACAs for any Windows software application. It can also save and \"replay\" a rich interaction trace including user and system events, screen captures, users' speech and eye fixations. This platform has been used to assess users' subjective judgements and reactions to a multimodal online help system meant to facilitate the use of software for the general public (Flash). The online help system is embodied using a 3D talking head (developed by FT R&D) which \"says\" oral help messages illustrated with Flash screen copies.",
        "published": "2007-08-28T09:56:05Z",
        "link": "http://arxiv.org/abs/0708.3740v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Interfaces adaptatives Adaptation dynamique à l'utilisateur courant",
        "authors": [
            "Jérôme Simonin",
            "Noëlle Carbonell"
        ],
        "summary": "We present a survey of recent research studies of the implementation of adaptive user models in human-computer interaction. A classification of research directions on adaptive user interfaces is first proposed; it takes account of the user characteristics that are modelled, the distribution of initiative and control of the system evolution between user and system, and the role of dynamic adaptation. Then, a few representative research studies are briefly presented to illustrate this classification. In the conclusion, some major issues regarding the utility and usability of adaptive user interfaces and the design of an appropriate methodology for assessing the ergonomic quality of this new form of interaction are mentioned.",
        "published": "2007-08-28T09:57:43Z",
        "link": "http://arxiv.org/abs/0708.3742v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Aides en ligne à l'utilisation de logiciels grand public : problèmes   spécifiques de conception et solutions potentielles",
        "authors": [
            "Antonio Capobianco",
            "Noëlle Carbonell"
        ],
        "summary": "The observation that novice users seldom consult online help was made over twenty years ago. This observation still holds nowadays, although online help to the use of software for the general public has greatly improved in usability during this period. The paper first demonstrates the necessity of online help to the use of new software whatever the transparency of the user interface, as whether online help systems are meant to compensate for interface design weaknesses or actually do provide necessary assistance to the discovery of a new software package functionalities is still an unsolved issue. The discussion relies on results of empirical and experimental studies and theoretical arguments. In the second part, we analyse the specific difficulties raised by the design of effective online help systems for current software intended for the general public so as to try and understand the reluctance of novice users to use online help. In the last part, we present and discuss the possible contributions of various approaches to solving this issue. Recent interaction paradigms and techniques are considered, such as, static and dynamic personalisation, contextual online help and new forms of multimodality.",
        "published": "2007-08-30T06:39:15Z",
        "link": "http://arxiv.org/abs/0708.4082v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Effective Generation of Subjectively Random Binary Sequences",
        "authors": [
            "Yasmine B. Sanderson"
        ],
        "summary": "We present an algorithm for effectively generating binary sequences which would be rated by people as highly likely to have been generated by a random process, such as flipping a fair coin.",
        "published": "2007-09-03T09:32:28Z",
        "link": "http://arxiv.org/abs/0709.0178v2",
        "categories": [
            "cs.HC",
            "cs.AI",
            "I.2.10; I.6.8; J.4; G.3"
        ]
    },
    {
        "title": "An Integrated Simulation System for Human Factors Study",
        "authors": [
            "Ying Wang",
            "Wei Zhang",
            "Fouad Bennis",
            "Damien Chablat"
        ],
        "summary": "It has been reported that virtual reality can be a useful tool for ergonomics study. The proposed integrated simulation system aims at measuring operator's performance in an interactive way for 2D control panel design. By incorporating some sophisticated virtual reality hardware/software, the system allows natural human-system and/or human-human interaction in a simulated virtual environment; enables dynamic objective measurement of human performance; and evaluates the quality of the system design in human factors perspective based on the measurement. It can also be for operation training for some 2D control panels.",
        "published": "2007-09-04T09:20:03Z",
        "link": "http://arxiv.org/abs/0709.0370v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Do oral messages help visual search?",
        "authors": [
            "Noëlle Carbonell",
            "Suzanne Kieffer"
        ],
        "summary": "A preliminary experimental study is presented, that aims at eliciting the contribution of oral messages to facilitating visual search tasks on crowded visual displays. Results of quantitative and qualitative analyses suggest that appropriate verbal messages can improve both target selection time and accuracy. In particular, multimodal messages including a visual presentation of the isolated target together with absolute spatial oral information on its location in the displayed scene seem most effective. These messages also got top-ranking ratings from most subjects.",
        "published": "2007-09-04T13:23:40Z",
        "link": "http://arxiv.org/abs/0709.0426v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Oral messages improve visual search",
        "authors": [
            "Suzanne Kieffer",
            "Noëlle Carbonell"
        ],
        "summary": "Input multimodality combining speech and hand gestures has motivated numerous usability studies. Contrastingly, issues relating to the design and ergonomic evaluation of multimodal output messages combining speech with visual modalities have not yet been addressed extensively. The experimental study presented here addresses one of these issues. Its aim is to assess the actual efficiency and usability of oral system messages including brief spatial information for helping users to locate objects on crowded displays rapidly. Target presentation mode, scene spatial structure and task difficulty were chosen as independent variables. Two conditions were defined: the visual target presentation mode (VP condition) and the multimodal target presentation mode (MP condition). Each participant carried out two blocks of visual search tasks (120 tasks per block, and one block per condition). Scene target presentation mode, scene structure and task difficulty were found to be significant factors. Multimodal target presentation proved to be more efficient than visual target presentation. In addition, participants expressed very positive judgments on multimodal target presentations which were preferred to visual presentations by a majority of participants. Besides, the contribution of spatial messages to visual search speed and accuracy was influenced by scene spatial structure and task difficulty: (i) messages improved search efficiency to a lesser extent for 2D array layouts than for some other symmetrical layouts, although the use of 2D arrays for displaying pictures is currently prevailing; (ii) message usefulness increased with task difficulty. Most of these results are statistically significant.",
        "published": "2007-09-04T13:27:33Z",
        "link": "http://arxiv.org/abs/0709.0428v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "A Sudoku Game for People with Motor Impairments",
        "authors": [
            "Stephane Norte"
        ],
        "summary": "Computer games are motivating and beneficial in learning different educational skills. Most people use their fingers, hands, and arms when using a computer game. However, for people with motor disabilities this task can be a barrier. We present a new Sudoku game for people whose motion is impaired, called Sudoku 4ALL. With this special interface a person can control the game with the voice or with a single switch. Our research aims to cautiously search for issues that might be appropriate for computational support and to build enabling technologies that increase individuals' functional independence in a game environment.",
        "published": "2007-09-07T11:59:22Z",
        "link": "http://arxiv.org/abs/0709.1056v3",
        "categories": [
            "cs.HC",
            "cs.CY",
            "H.5.2; K.4.2; K.8.0"
        ]
    },
    {
        "title": "Design of moveable and resizable graphics",
        "authors": [
            "Sergey Andreyev"
        ],
        "summary": "We are communicating with computers on two different levels. On upper level we have a very flexible system of windows: we can move them, resize, overlap or put side by side. At any moment we decide what would be the best view and reorganize the whole view easily. Then we start any application, go to the inner level, and everything changes. Here we are stripped of all the flexibility and can work only inside the scenario, developed by the designer of the program. Interface will allow us to change some tiny details, but in general everything is fixed: graphics is neither moveable, nor resizable, and the same with controls. Author designed an extremely powerful mechanism of turning graphical objects and controls into moveable and resizable. This can not only significantly improve the existing applications, but this will bring the applications to another level. (To estimate the possible difference, try to imagine the Windows system without its flexibility and compare it with the current one.) This article explains in details the construction and use of moveable and resizable graphical objects.",
        "published": "2007-09-22T00:21:08Z",
        "link": "http://arxiv.org/abs/0709.3553v1",
        "categories": [
            "cs.GR",
            "cs.HC"
        ]
    },
    {
        "title": "Systèmes interactifs sensibles aux émotions : architecture   logicielle",
        "authors": [
            "Alexis Clay"
        ],
        "summary": "We propose a software architecture for interactive systems which allows integrating the user's emotion. Emotion can be involved in interaction at several levels. In our application case - ballet dance - emotions is explicitely manipulated by the interactive system to produce emotion-wise output. Our architecture model to develop emotion-wise applications is based on the PAC-Amodeus model. We add a branch to this model, divided into three components: Data capture, analysis and cue extraction, and finally interpretation of those cues. We show the different data flows between this architecture's components depending on the entry point of the emotion branch within the system. We then illustrate our model by describing our application case: capturing a ballet dancer's movement to extract the emotions he expresses and use these emotions to generate graphical content that is displayed on stage.",
        "published": "2007-10-03T17:23:59Z",
        "link": "http://arxiv.org/abs/0710.0842v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Emotion capture based on body postures and movements",
        "authors": [
            "Alexis Clay",
            "Nadine Couture",
            "Laurence Nigay"
        ],
        "summary": "In this paper we present a preliminary study for designing interactive systems that are sensible to human emotions based on the body movements. To do so, we first review the literature on the various approaches for defining and characterizing human emotions. After justifying the adopted characterization space for emotions, we then focus on the movement characteristics that must be captured by the system for being able to recognize the human emotions.",
        "published": "2007-10-03T17:36:07Z",
        "link": "http://arxiv.org/abs/0710.0847v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Assistance orale à la recherche visuelle - étude expérimentale de   l'apport d'indications spatiales à la détection de cibles",
        "authors": [
            "Suzanne Kieffer",
            "Noëlle Carbonell"
        ],
        "summary": "This paper describes an experimental study that aims at assessing the actual contribution of voice system messages to visual search efficiency and comfort. Messages which include spatial information on the target location are meant to support search for familiar targets in collections of photographs (30 per display). 24 participants carried out 240 visual search tasks in two conditions differing from each other in initial target presentation only. The isolated target was presented either simultaneously with an oral message (multimodal presentation, MP), or without any message (visual presentation, VP). Averaged target selection times were thrice longer and errors almost twice more frequent in the VP condition than in the MP condition. In addition, the contribution of spatial messages to visual search rapidity and accuracy was influenced by display layout and task difficulty. Most results are statistically significant. Besides, subjective judgments indicate that oral messages were well accepted.",
        "published": "2007-10-03T18:30:07Z",
        "link": "http://arxiv.org/abs/0710.0859v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Cross-Participants : fostering design-use mediation in an Open Source   Software community",
        "authors": [
            "Flore Barcellini",
            "Françoise Détienne",
            "Jean-Marie Burkhardt"
        ],
        "summary": "Motivation - This research aims at investigating emerging roles and forms of participation fostering design-use mediation during the Open Source Software design process Research approach - We compare online interactions for a successful \"pushed-by-users\" design process with unsuccessful previous proposals. The methodology developed, articulate structural analyses of the discussions (organization of discussions, participation) to actions to the code and documentation made by participants to the project. We focus on the useroriented and the developer-oriented mailing-lists of the Python project. Findings/Design - We find that key-participants, the cross-participants, foster the design process and act as boundary spanners between the users and the developers' communities. Research limitations/Implications - These findings can be reinforced developing software to automate the structural analysis of discussions and actions to the code and documentation. Further analyses, supported by these tools, will be necessary to generalise our results. Originality/Value - The analysis of participation among the three interaction spaces of OSS design (discussion, documentation and implementation) is the main originality of this work compared to other OSS research that mainly analyse one or two spaces. Take away message - Beside the idealistic picture that users may intervene freely in the process, OSS design is boost and framed by some key-participants and specific rules and there can be barriers to users' participation",
        "published": "2007-10-09T14:44:44Z",
        "link": "http://arxiv.org/abs/0710.1772v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "cs.SE"
        ]
    },
    {
        "title": "Success and failure of programming environments - report on the design   and use of a graphic abstract syntax tree editor",
        "authors": [
            "C. Recanati"
        ],
        "summary": "The STAPLE project investigated (at the end of the eighties), a persistent architecture for functional programming. Work has been done in two directions: the development of a programming environment for a functional language within a persistent system and an experiment on transferring the expertise of functional prototyping into industry. This paper is a report on the first activity. The first section gives a general description of Absynte - the abstract syntax tree editor developed within the Project. Following sections make an attempt at measuring the effectiveness of such an editor and discuss the problems raised by structured syntax editing - specially environments based on abstract syntax trees.",
        "published": "2007-10-11T23:40:47Z",
        "link": "http://arxiv.org/abs/0710.2358v1",
        "categories": [
            "cs.PL",
            "cs.HC"
        ]
    },
    {
        "title": "L'analyse de l'expertise du point de vue de l'ergonomie cognitive",
        "authors": [
            "Willemien Visser"
        ],
        "summary": "This paper presents a review of methods for collecting and analysing data on complex activities. Starting with methods developed for design, we examine the possibility to transpose them to other complex activities, especially activities referring to sensorial expertise. R\\'esum\\'e Ce texte pr\\'esente une revue de m\\'ethodes pour recueillir et analyser des donn\\'ees sur des actvit\\'es complexes. A partir de m\\'ethodes d\\'evelopp\\'ees pour des actvit\\'es de conception, nous examinons la possibilit\\'e de les transposer \\`a d'autres actvit\\'es complexes, notamment des actvit\\'es faisant \\`a appel \\`a des expertises sensorielles.",
        "published": "2007-10-26T06:32:22Z",
        "link": "http://arxiv.org/abs/0710.4999v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Social Browsing & Information Filtering in Social Media",
        "authors": [
            "Kristina Lerman"
        ],
        "summary": "Social networks are a prominent feature of many social media sites, a new generation of Web sites that allow users to create and share content. Sites such as Digg, Flickr, and Del.icio.us allow users to designate others as \"friends\" or \"contacts\" and provide a single-click interface to track friends' activity. How are these social networks used? Unlike pure social networking sites (e.g., LinkedIn and Facebook), which allow users to articulate their online professional and personal relationships, social media sites are not, for the most part, aimed at helping users create or foster online relationships. Instead, we claim that social media users create social networks to express their tastes and interests, and use them to filter the vast stream of new submissions to find interesting content. Social networks, in fact, facilitate new ways of interacting with information: what we call social browsing. Through an extensive analysis of data from Digg and Flickr, we show that social browsing is one of the primary usage modalities on these social media sites. This finding has implications for how social media sites rate and personalize content.",
        "published": "2007-10-30T16:38:54Z",
        "link": "http://arxiv.org/abs/0710.5697v1",
        "categories": [
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "Dynamic aspects of individual design activities. A cognitive ergonomics   viewpoint",
        "authors": [
            "Willemien Visser"
        ],
        "summary": "This paper focuses on the use of knowledge possessed by designers. Data collection was based on observations (by the cognitive ergonomics researcher) and simultaneous verbalisations (by the designers) in empirical studies conducted in the context of industrial design projects. The contribution of this research is typical of cognitive ergonomics, in that it provides data on actual activities implemented by designers in their actual work situation (rather than on prescribed and/or idealised processes and methods). Data presented concern global strategies (the way in which designers actually organise their activity) and local strategies (reuse in design). Results from cognitive ergonomics and other research that challenges the way in which people are supposed to work with existing systems are generally not received warmly. Abundant corroboration of such results is required before industry may consider taking them into account. The opportunistic organisation of design activity is taken here as an example of this reluctance. The results concerning this aspect of design have been verified repeatedly, but only prototypes and experimental systems implementing some of the requirements formulated on their basis, are under development.",
        "published": "2007-11-08T09:29:57Z",
        "link": "http://arxiv.org/abs/0711.1226v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Designing as Construction of Representations: A Dynamic Viewpoint in   Cognitive Design Research",
        "authors": [
            "Willemien Visser"
        ],
        "summary": "This article presents a cognitively oriented viewpoint on design. It focuses on cognitive, dynamic aspects of real design, i.e., the actual cognitive activity implemented by designers during their work on professional design projects. Rather than conceiving de-signing as problem solving - Simon's symbolic information processing (SIP) approach - or as a reflective practice or some other form of situated activity - the situativity (SIT) approach - we consider that, from a cognitive viewpoint, designing is most appropriately characterised as a construction of representations. After a critical discussion of the SIP and SIT approaches to design, we present our view-point. This presentation concerns the evolving nature of representations regarding levels of abstraction and degrees of precision, the function of external representations, and specific qualities of representation in collective design. Designing is described at three levels: the organisation of the activity, its strategies, and its design-representation construction activities (different ways to generate, trans-form, and evaluate representations). Even if we adopt a \"generic design\" stance, we claim that design can take different forms depending on the nature of the artefact, and we propose some candidates for dimensions that allow a distinction to be made between these forms of design. We discuss the potential specificity of HCI design, and the lack of cognitive design research occupied with the quality of design. We close our discussion of representational structures and activities by an outline of some directions regarding their functional linkages.",
        "published": "2007-11-08T09:33:19Z",
        "link": "http://arxiv.org/abs/0711.1227v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Mise en place de scénarios pour la conception d'outils en Chirurgie   Minimalement Invasive",
        "authors": [
            "Guillaume Thomann",
            "Jean Caelen",
            "Morgan Verdier",
            "Brigitte Meillon"
        ],
        "summary": "Nowadays, more and more surgical interventions are carried out in Minimally Invasive Surgery, to make the post-operative constraints less painful for the patient. Actually, new surgical tools or medical products are designed after informal discussions between surgeons and designers. The user requirements documents are the main document used by the mechanical designers to design and improve the product. Medical terms, often used by surgeons and employed to explain their needs, don't allow for an instantaneous understanding by designers. Unfortunately, this relation causes a dysfunction in the definition cycle of the product. Our aim is to work on the design process, its assistance thanks to methods and tools and on its organisation for better understandings and more complementarities between surgeons and designers. We propose on this article a design method already tested in the informatics domain: User Centred Design which takes the user into account more effectively in the process design. We already propose a scenario oriented design method centred on the user and represented as a scenario (Scenario-Based Design). We start in this article by clarifying these concepts before detailing their implementation for the design of a new surgical tool.",
        "published": "2007-11-14T16:18:42Z",
        "link": "http://arxiv.org/abs/0711.2239v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "An annotation based approach to support design communication",
        "authors": [
            "Onur Hisarciklilar",
            "Jean-François Boujut"
        ],
        "summary": "The aim of this paper is to propose an approach based on the concept of annotation for supporting design communication. In this paper, we describe a co-operative design case study where we analyse some annotation practices, mainly focused on design minutes recorded during project reviews. We point out specific requirements concerning annotation needs. Based on these requirements, we propose an annotation model, inspired from the Speech Act Theory (SAT) to support communication in a 3D digital environment. We define two types of annotations in the engineering design context, locutionary and illocutionary annotations. The annotations we describe in this paper are materialised by a set of digital artefacts, which have a semantic dimension allowing express/record elements of technical justifications, traces of contradictory debates, etc. In this paper, we first clarify the semantic annotation concept, and we define general properties of annotations in the engineering design context, and the role of annotations in different design project situations. After the description of the case study, where we observe and analyse annotations usage during the design reviews and minute making, the last section is dedicated to present our approach. We then describe the SAT concept, and define the concept of annotation acts. We conclude with a description of basic annotation functionalities that are actually implemented in a software, based on our approach.",
        "published": "2007-11-15T19:22:16Z",
        "link": "http://arxiv.org/abs/0711.2486v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Computer Supported Collaborative Research",
        "authors": [
            "Vita Hinze-Hoare"
        ],
        "summary": "It is suggested that a new area of CSCR (Computer Supported Collaborative Research) is distinguished from CSCW (Computer Supported Collaborative Work) and CSCL (Computer Supported Collaborative Learning) and that the demarcation between the three areas could do with greater clarification and prescription.   Although the areas of Human Computer Interaction (HCI), CSCW, and CSCL are now relatively well established, the related field of Computer Supported Collaborative Research (CSCR) is new and little understood. An analysis of the principles and issues behind CSCR is undertaken with a view to determining precisely its nature and scope and to delineate it clearly from CSCW and CSCL. This determination is such that it is generally applicable to the building, design and evaluation of collaborative research environments.   A particular instance of the CSCR domain is then examined in order to determine the requirements of a collaborative research environment for students and supervisors (CRESS).",
        "published": "2007-11-17T20:08:41Z",
        "link": "http://arxiv.org/abs/0711.2760v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Une approche par les modèles pour le suivi de l'activité de   construction d'un bâtiment. Bat'iViews : une interface multi-vues   orientée gestion de chantier",
        "authors": [
            "Gilles Halin",
            "Sylvain Kubicki"
        ],
        "summary": "Cooperation between actors in design and construction activities in architecture is an essential stake nowadays. In professional practices the actors involved in construction projects use numerous tools. The project is unique but the \"views\" that actors manipulate are various and sometimes fundamentally different. Their common characteristic is that they partially represent the cooperation context through a specific point of view. \"Bat'iViews\" suggests to the actors a multi-view interface of the context and enables to navigate through the different views. This proposition is based on a model-driven approach. We distinguish between \"context modelling\" and modelling of concepts represented in each \"businessview\". A model integrative infrastructure allows us to develop the prototype and to manage user interaction through the definition of models' transformations.",
        "published": "2007-11-18T19:10:18Z",
        "link": "http://arxiv.org/abs/0711.2811v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "IT services design to support coordination practices in the   Luxembourguish AEC sector",
        "authors": [
            "Sylvain Kubicki",
            "Annie Guerriéro",
            "Damien Hanser",
            "Gilles Halin"
        ],
        "summary": "In the Architecture Engineering and Construction sector (AEC) cooperation between actors is essential for project success. The configuration of actors' organization takes different forms like the associated coordination mechanisms. Our approach consists in analyzing these coordination mechanisms through the identification of the \"base practices\" realized by the actors of a construction project to cooperate. We also try with practitioners to highlight the \"best practices\" of cooperation. Then we suggest here two prototypes of IT services aiming to demonstrate the value added of IT to support cooperation. These prototype tools allow us to sensitize the actors through terrain experiments and then to bring inch by inch the Luxembourgish AEC sector towards electronic cooperation.",
        "published": "2007-11-19T16:36:02Z",
        "link": "http://arxiv.org/abs/0711.2971v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Virtual Laboratories and Virtual Worlds",
        "authors": [
            "Piet Hut"
        ],
        "summary": "Since we cannot put stars in a laboratory, astrophysicists had to wait till the invention of computers before becoming laboratory scientists. For half a century now, we have been conducting experiments in our virtual laboratories. However, we ourselves have remained behind the keyboard, with the screen of the monitor separating us from the world we are simulating. Recently, 3D on-line technology, developed first for games but now deployed in virtual worlds like Second Life, is beginning to make it possible for astrophysicists to enter their virtual labs themselves, in virtual form as avatars. This has several advantages, from new possibilities to explore the results of the simulations to a shared presence in a virtual lab with remote collaborators on different continents. I will report my experiences with the use of Qwaq Forums, a virtual world developed by a new company (see http://www.qwaq.com)",
        "published": "2007-12-11T07:05:36Z",
        "link": "http://arxiv.org/abs/0712.1655v1",
        "categories": [
            "astro-ph",
            "cs.HC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "A Web-based System for Observing and Analyzing Computer Mediated   Communications",
        "authors": [
            "Madeth May",
            "Sébastien George",
            "Patrick Prévôt"
        ],
        "summary": "Tracking data of user's activities resulting from Computer Mediated Communication (CMC) tools (forum, chat, etc.) is often carried out in an ad-hoc manner, which either confines the reusability of data in different purposes or makes data exploitation difficult. Our research works are biased toward methodological challenges involved in designing and developing a generic system for tracking user's activities while interacting with asynchronous communication tools like discussion forums. We present in this paper, an approach for building a Web-based system for observing and analyzing user activity on any type of discussion forums.",
        "published": "2007-12-11T16:47:54Z",
        "link": "http://arxiv.org/abs/0712.1759v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Conceptions et usages des plates-formes de formation, Revue Sciences et   Technologies de l'Information et de la Communication pour l'Éducation et la   Formation",
        "authors": [
            "Sébastien George",
            "Alain Derycke"
        ],
        "summary": "Educative platforms are at the heart of the development of online education. They can not only be reduced to technological aspects. Underlying models impact teaching and learning from the preparing of lessons to the learning sessions. Research related to these platforms are numerous and their stakes are important. For these reasons, we launched a call to a special issue on \"Designs and uses of educative platforms\" An educative platform is a computer system designed to automate various functions relating to the organization of the course, to the management of their content, to the monitoring of learners and supervision of persons in charge of various formations (Office de la langue fran\\c{c}aise, 2005). So educative platforms are Learning Management Systems (LMS) which are specific to education contexts.",
        "published": "2007-12-11T16:59:19Z",
        "link": "http://arxiv.org/abs/0712.1768v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Conception d'outils de communication spécifiques au contexte   éducatif",
        "authors": [
            "Sébastien George",
            "Cécile Bothorel"
        ],
        "summary": "In a distance learning context, providing usual communication tools (forum, chat, ...) is not always enough to create efficient interactions between learners and to favour collective knowledge building. A solution consists in setting-up collective activities which encourage learners to communicate. But, even in that case, tools can sometimes become a barrier to communication. We present in this paper examples of specific tools that are designed in order to favour and to guide communications in an educational context, but also to foster interactions during learning activities that are not inherently collaborative. We describe synchronous communication tools (semi-structured chat), asynchronous tools (temporally structured forum, contextual forum) and a system which promotes mutual aid between learners.",
        "published": "2007-12-11T19:30:46Z",
        "link": "http://arxiv.org/abs/0712.1800v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Study of conditions of use of E-services accessible to visually disabled   persons",
        "authors": [
            "Marc-Eric Bobiller-Chaumon",
            "Michel Dubois",
            "Françoise Sandoz-Guermond"
        ],
        "summary": "The aim of this paper is to determine the expectations that French-speaking disabled persons have for electronic administrative sites (utility). At the same time, it is a matter of identifying the difficulties of use that the manipulation of these E-services poses concretely for blind people (usability) and of evaluating the psychosocial impacts on the way of life of these people with specific needs. We show that the lack of numerical accessibility is likely to accentuate the social exclusion of which these people are victim by establishing a numerical glass ceiling.",
        "published": "2007-12-13T15:14:01Z",
        "link": "http://arxiv.org/abs/0712.2168v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Apports des démarches d'inspection et des tests d'usage dans   l'évaluation de l'accessibilité de E-services",
        "authors": [
            "Marc-Eric Bobiller-Chaumon",
            "Françoise Sandoz-Guermond"
        ],
        "summary": "This article proposes to describe and compare the contributions of various techniques of evaluation of the accessibility of E-services carried out starting from (i) methods of inspection (on the basis of traditional ergonomic criteria and accessibility) and (ii) of tests of use. It show that these are the latter which show the best rate of identification of the problems of uses for the poeple with disabilities",
        "published": "2007-12-13T16:43:56Z",
        "link": "http://arxiv.org/abs/0712.2183v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Human-Machine Symbiosis, 50 Years On",
        "authors": [
            "Ian Foster"
        ],
        "summary": "Licklider advocated in 1960 the construction of computers capable of working symbiotically with humans to address problems not easily addressed by humans working alone. Since that time, many of the advances that he envisioned have been achieved, yet the time spent by human problem solvers in mundane activities remains large. I propose here four areas in which improved tools can further advance the goal of enhancing human intellect: services, provenance, knowledge communities, and automation of problem-solving protocols.",
        "published": "2007-12-13T23:00:37Z",
        "link": "http://arxiv.org/abs/0712.2255v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.HC"
        ]
    },
    {
        "title": "L'accessibilité des E-services aux personnes non-voyantes :   difficultés d'usage et recommandations",
        "authors": [
            "Françoise Sandoz-Guermond",
            "Marc-Eric Bobiller-Chaumon"
        ],
        "summary": "While taking into account handicapped people in the design of technologies represents a social and political stake that becomes important (in particular with the recent law on equal rights for all the citizens, March 2004), this paper aims at evaluating the level of accessibility of two sites of E-services thanks to tests of use and proposing a set of recommendations in order to increase usability for the largest amount of people.",
        "published": "2007-12-19T15:54:07Z",
        "link": "http://arxiv.org/abs/0712.3215v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "AccelKey Selection Method for Mobile Devices",
        "authors": [
            "Vadim Zaliva"
        ],
        "summary": "Portable Electronic Devices usually utilize a small screen with limited viewing area and a keyboard with a limited number of keys. This makes it difficult to perform quick searches in data arrays containing more than dozen items such an address book or song list. In this article we present a new data selection method which allows the user to quickly select an entry from a list using 4-way navigation device such as joystick, trackball or 4-way key pad. This method allows for quick navigation using just one hand, without looking at the screen.",
        "published": "2007-12-19T23:31:55Z",
        "link": "http://arxiv.org/abs/0712.3433v1",
        "categories": [
            "cs.HC",
            "H.5.2"
        ]
    }
]