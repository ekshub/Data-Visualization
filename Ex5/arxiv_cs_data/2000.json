[
    {
        "title": "Minimum Description Length and Compositionality",
        "authors": [
            "Wlodek Zadrozny"
        ],
        "summary": "We present a non-vacuous definition of compositionality. It is based on the idea of combining the minimum description length principle with the original definition of compositionality (that is, that the meaning of the whole is a function of the meaning of the parts).   The new definition is intuitive and allows us to distinguish between compositional and non-compositional semantics, and between idiomatic and non-idiomatic expressions. It is not ad hoc, since it does not make any references to non-intrinsic properties of meaning functions (like being a polynomial). Moreover, it allows us to compare different meaning functions with respect to how compositional they are. It bridges linguistic and corpus-based, statistical approaches to natural language understanding.",
        "published": "2000-01-04T21:46:29Z",
        "link": "http://arxiv.org/abs/cs/0001002v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Multi-Agent Only Knowing",
        "authors": [
            "Joseph Y. Halpern",
            "Gerhard Lakemeyer"
        ],
        "summary": "Levesque introduced a notion of ``only knowing'', with the goal of capturing certain types of nonmonotonic reasoning. Levesque's logic dealt with only the case of a single agent. Recently, both Halpern and Lakemeyer independently attempted to extend Levesque's logic to the multi-agent case. Although there are a number of similarities in their approaches, there are some significant differences. In this paper, we reexamine the notion of only knowing, going back to first principles. In the process, we simplify Levesque's completeness proof, and point out some problems with the earlier definitions. This leads us to reconsider what the properties of only knowing ought to be. We provide an axiom system that captures our desiderata, and show that it has a semantics that corresponds to it. The axiom system has an added feature of interest: it includes a modal operator for satisfiability, and thus provides a complete axiomatization for satisfiability in the logic K45.",
        "published": "2000-01-19T22:13:38Z",
        "link": "http://arxiv.org/abs/cs/0001015v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4, F.4.1"
        ]
    },
    {
        "title": "Computing large and small stable models",
        "authors": [
            "Miroslaw Truszczynski"
        ],
        "summary": "In this paper, we focus on the problem of existence and computing of small and large stable models. We show that for every fixed integer k, there is a linear-time algorithm to decide the problem LSM (large stable models problem): does a logic program P have a stable model of size at least |P|-k. In contrast, we show that the problem SSM (small stable models problem) to decide whether a logic program P has a stable model of size at most k is much harder. We present two algorithms for this problem but their running time is given by polynomials of order depending on k. We show that the problem SSM is fixed-parameter intractable by demonstrating that it is W[2]-hard. This result implies that it is unlikely, an algorithm exists to compute stable models of size at most k that would run in time O(n^c), where c is a constant independent of k. We also provide an upper bound on the fixed-parameter complexity of the problem SSM by showing that it belongs to the class W[3].",
        "published": "2000-02-03T21:15:34Z",
        "link": "http://arxiv.org/abs/cs/0002001v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3;I.2.4"
        ]
    },
    {
        "title": "Uniform semantic treatment of default and autoepistemic logics",
        "authors": [
            "Marc Denecker",
            "Victor W. Marek",
            "Miroslaw Truszczynski"
        ],
        "summary": "We revisit the issue of connections between two leading formalisms in nonmonotonic reasoning: autoepistemic logic and default logic. For each logic we develop a comprehensive semantic framework based on the notion of a belief pair. The set of all belief pairs together with the so called knowledge ordering forms a complete lattice. For each logic, we introduce several semantics by means of fixpoints of operators on the lattice of belief pairs. Our results elucidate an underlying isomorphism of the respective semantic constructions. In particular, we show that the interpretation of defaults as modal formulas proposed by Konolige allows us to represent all semantics for default logic in terms of the corresponding semantics for autoepistemic logic. Thus, our results conclusively establish that default logic can indeed be viewed as a fragment of autoepistemic logic. However, as we also demonstrate, the semantics of Moore and Reiter are given by different operators and occupy different locations in their corresponding families of semantics. This result explains the source of the longstanding difficulty to formally relate these two semantics. In the paper, we also discuss approximating skeptical reasoning with autoepistemic and default logics and establish constructive principles behind such approximations.",
        "published": "2000-02-03T21:44:57Z",
        "link": "http://arxiv.org/abs/cs/0002002v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "On the accuracy and running time of GSAT",
        "authors": [
            "Deborah East",
            "Miroslaw Truszczynski"
        ],
        "summary": "Randomized algorithms for deciding satisfiability were shown to be effective in solving problems with thousands of variables. However, these algorithms are not complete. That is, they provide no guarantee that a satisfying assignment, if one exists, will be found. Thus, when studying randomized algorithms, there are two important characteristics that need to be considered: the running time and, even more importantly, the accuracy --- a measure of likelihood that a satisfying assignment will be found, provided one exists. In fact, we argue that without a reference to the accuracy, the notion of the running time for randomized algorithms is not well-defined. In this paper, we introduce a formal notion of accuracy. We use it to define a concept of the running time. We use both notions to study the random walk strategy GSAT algorithm. We investigate the dependence of accuracy on properties of input formulas such as clause-to-variable ratio and the number of satisfying assignments. We demonstrate that the running time of GSAT grows exponentially in the number of variables of the input formula for randomly generated 3-CNF formulas and for the formulas encoding 3- and 4-colorability of graphs.",
        "published": "2000-02-04T12:53:57Z",
        "link": "http://arxiv.org/abs/cs/0002003v1",
        "categories": [
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "Syntactic Autonomy: Why There is no Autonomy without Symbols and How   Self-Organization Might Evolve Them",
        "authors": [
            "Luis M. Rocha"
        ],
        "summary": "Two different types of agency are discussed based on dynamically coherent and incoherent couplings with an environment respectively. I propose that until a private syntax (syntactic autonomy) is discovered by dynamically coherent agents, there are no significant or interesting types of closure or autonomy. When syntactic autonomy is established, then, because of a process of description-based selected self-organization, open-ended evolution is enabled. At this stage, agents depend, in addition to dynamics, on localized, symbolic memory, thus adding a level of dynamical incoherence to their interaction with the environment. Furthermore, it is the appearance of syntactic autonomy which enables much more interesting types of closures amongst agents which share the same syntax. To investigate how we can study the emergence of syntax from dynamical systems, experiments with cellular automata leading to emergent computation to solve non-trivial tasks are discussed. RNA editing is also mentioned as a process that may have been used to obtain a primordial biological code necessary open-ended evolution.",
        "published": "2000-02-16T18:09:20Z",
        "link": "http://arxiv.org/abs/cs/0002009v1",
        "categories": [
            "cs.AI",
            "A.m"
        ]
    },
    {
        "title": "Genetic Algorithms for Extension Search in Default Logic",
        "authors": [
            "P. Nicolas",
            "F. Saubion",
            "I. Stephan"
        ],
        "summary": "A default theory can be characterized by its sets of plausible conclusions, called its extensions. But, due to the theoretical complexity of Default Logic (Sigma_2p-complete), the problem of finding such an extension is very difficult if one wants to deal with non trivial knowledge bases. Based on the principle of natural selection, Genetic Algorithms have been quite successfully applied to combinatorial problems and seem useful for problems with huge search spaces and when no tractable algorithm is available. The purpose of this paper is to show that techniques issued from Genetic Algorithms can be used in order to build an efficient default reasoning system. After providing a formal description of the components required for an extension search based on Genetic Algorithms principles, we exhibit some experimental results.",
        "published": "2000-02-24T16:09:04Z",
        "link": "http://arxiv.org/abs/cs/0002015v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Safe cooperative robot dynamics on graphs",
        "authors": [
            "Robert Ghrist",
            "Daniel Koditschek"
        ],
        "summary": "This paper initiates the use of vector fields to design, optimize, and implement reactive schedules for safe cooperative robot patterns on planar graphs. We consider Automated Guided Vehicles (AGV's) operating upon a predefined network of pathways. In contrast to the case of locally Euclidean configuration spaces, regularization of collisions is no longer a local procedure, and issues concerning the global topology of configuration spaces must be addressed. The focus of the present inquiry is the achievement of safe, efficient, cooperative patterns in the simplest nontrivial example (a pair of robots on a Y-network) by means of a state-event heirarchical controller.",
        "published": "2000-02-24T18:13:33Z",
        "link": "http://arxiv.org/abs/cs/0002014v1",
        "categories": [
            "cs.RO",
            "cs.AI",
            "I.2.9"
        ]
    },
    {
        "title": "SLT-Resolution for the Well-Founded Semantics",
        "authors": [
            "Yi-Dong Shen",
            "Li-Yan Yuan",
            "Jia-Huai You"
        ],
        "summary": "Global SLS-resolution and SLG-resolution are two representative mechanisms for top-down evaluation of the well-founded semantics of general logic programs. Global SLS-resolution is linear for query evaluation but suffers from infinite loops and redundant computations. In contrast, SLG-resolution resolves infinite loops and redundant computations by means of tabling, but it is not linear. The principal disadvantage of a non-linear approach is that it cannot be implemented using a simple, efficient stack-based memory structure nor can it be easily extended to handle some strictly sequential operators such as cuts in Prolog.   In this paper, we present a linear tabling method, called SLT-resolution, for top-down evaluation of the well-founded semantics. SLT-resolution is a substantial extension of SLDNF-resolution with tabling. Its main features include: (1) It resolves infinite loops and redundant computations while preserving the linearity. (2) It is terminating, and sound and complete w.r.t. the well-founded semantics for programs with the bounded-term-size property with non-floundering queries. Its time complexity is comparable with SLG-resolution and polynomial for function-free logic programs. (3) Because of its linearity for query evaluation, SLT-resolution bridges the gap between the well-founded semantics and standard Prolog implementation techniques. It can be implemented by an extension to any existing Prolog abstract machines such as WAM or ATOAM.",
        "published": "2000-02-27T19:20:05Z",
        "link": "http://arxiv.org/abs/cs/0002016v3",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.3.1; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Prospects for in-depth story understanding by computer",
        "authors": [
            "Erik T. Mueller"
        ],
        "summary": "While much research on the hard problem of in-depth story understanding by computer was performed starting in the 1970s, interest shifted in the 1990s to information extraction and word sense disambiguation. Now that a degree of success has been achieved on these easier problems, I propose it is time to return to in-depth story understanding. In this paper I examine the shift away from story understanding, discuss some of the major problems in building a story understanding system, present some possible solutions involving a set of interacting understanding agents, and provide pointers to useful tools and resources for building story understanding systems.",
        "published": "2000-03-01T18:01:06Z",
        "link": "http://arxiv.org/abs/cs/0003003v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A database and lexicon of scripts for ThoughtTreasure",
        "authors": [
            "Erik T. Mueller"
        ],
        "summary": "Since scripts were proposed in the 1970's as an inferencing mechanism for AI and natural language processing programs, there have been few attempts to build a database of scripts. This paper describes a database and lexicon of scripts that has been added to the ThoughtTreasure commonsense platform. The database provides the following information about scripts: sequence of events, roles, props, entry conditions, results, goals, emotions, places, duration, frequency, and cost. English and French words and phrases are linked to script concepts.",
        "published": "2000-03-01T18:07:02Z",
        "link": "http://arxiv.org/abs/cs/0003004v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "I.2.7; I.2.4"
        ]
    },
    {
        "title": "Computing Circumscriptive Databases by Integer Programming: Revisited   (Extended Abstract)",
        "authors": [
            "Ken Satoh",
            "Hidenori Okamoto"
        ],
        "summary": "In this paper, we consider a method of computing minimal models in circumscription using integer programming in propositional logic and first-order logic with domain closure axioms and unique name axioms. This kind of treatment is very important since this enable to apply various technique developed in operations research to nonmonotonic reasoning.   Nerode et al. (1995) are the first to propose a method of computing circumscription using integer programming. They claimed their method was correct for circumscription with fixed predicate, but we show that their method does not correctly reflect their claim. We show a correct method of computing all the minimal models not only with fixed predicates but also with varied predicates and we extend our method to compute prioritized circumscription as well.",
        "published": "2000-03-05T09:57:49Z",
        "link": "http://arxiv.org/abs/cs/0003007v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Consistency Management of Normal Logic Program by Top-down Abductive   Proof Procedure",
        "authors": [
            "Ken Satoh"
        ],
        "summary": "This paper presents a method of computing a revision of a function-free normal logic program. If an added rule is inconsistent with a program, that is, if it leads to a situation such that no stable model exists for a new program, then deletion and addition of rules are performed to avoid inconsistency. We specify a revision by translating a normal logic program into an abductive logic program with abducibles to represent deletion and addition of rules. To compute such deletion and addition, we propose an adaptation of our top-down abductive proof procedure to compute a relevant abducibles to an added rule. We compute a minimally revised program, by choosing a minimal set of abducibles among all the sets of abducibles computed by a top-down proof procedure.",
        "published": "2000-03-05T10:29:03Z",
        "link": "http://arxiv.org/abs/cs/0003008v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Conditional indifference and conditional preservation",
        "authors": [
            "Gabriele Kern-Isberner"
        ],
        "summary": "The idea of preserving conditional beliefs emerged recently as a new paradigm apt to guide the revision of epistemic states. Conditionals are substantially different from propositional beliefs and need specific treatment. In this paper, we present a new approach to conditionals, capturing particularly well their dynamic part as revision policies. We thoroughly axiomatize a principle of conditional preservation as an indifference property with respect to conditional structures of worlds. This principle is developed in a semi-quantitative setting, so as to reveal its fundamental meaning for belief revision in quantitative as well as in qualitative frameworks. In fact, it is shown to cover other proposed approaches to conditional preservation.",
        "published": "2000-03-06T12:08:06Z",
        "link": "http://arxiv.org/abs/cs/0003009v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.0; I.2.3; I.2.6"
        ]
    },
    {
        "title": "Automatic Belief Revision in SNePS",
        "authors": [
            "Stuart C. Shapiro",
            "Frances L. Johnson"
        ],
        "summary": "SNePS is a logic- and network- based knowledge representation, reasoning, and acting system, based on a monotonic, paraconsistent, first-order term logic, with compositional intensional semantics. It has an ATMS-style facility for belief contraction, and an acting component, including a well-defined syntax and semantics for primitive and composite acts, as well as for ``rules'' that allow for acting in support of reasoning and reasoning in support of acting. SNePS has been designed to support natural language competent cognitive agents.   When the current version of SNePS detects an explicit contradiction, it interacts with the user, providing information that helps the user decide what to remove from the knowledge base in order to remove the contradiction. The forthcoming SNePS 2.6 will also do automatic belief contraction if the information in the knowledge base warrents it.",
        "published": "2000-03-06T16:55:16Z",
        "link": "http://arxiv.org/abs/cs/0003011v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Defeasible Reasoning in OSCAR",
        "authors": [
            "John L. Pollock"
        ],
        "summary": "This is a system description for the OSCAR defeasible reasoner.",
        "published": "2000-03-06T22:23:00Z",
        "link": "http://arxiv.org/abs/cs/0003012v1",
        "categories": [
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "A flexible framework for defeasible logics",
        "authors": [
            "G. Antoniou",
            "D. Billigton",
            "G. Governatori",
            "M. J. Maher"
        ],
        "summary": "Logics for knowledge representation suffer from over-specialization: while each logic may provide an ideal representation formalism for some problems, it is less than optimal for others. A solution to this problem is to choose from several logics and, when necessary, combine the representations. In general, such an approach results in a very difficult problem of combination. However, if we can choose the logics from a uniform framework then the problem of combining them is greatly simplified. In this paper, we develop such a framework for defeasible logics. It supports all defeasible logics that satisfy a strong negation principle. We use logic meta-programs as the basis for the framework.",
        "published": "2000-03-07T01:24:26Z",
        "link": "http://arxiv.org/abs/cs/0003013v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; D.1.6"
        ]
    },
    {
        "title": "Applying Maxi-adjustment to Adaptive Information Filtering Agents",
        "authors": [
            "Raymond Lau",
            "Arthur H. M. ter Hofstede",
            "Peter D. Bruza"
        ],
        "summary": "Learning and adaptation is a fundamental property of intelligent agents. In the context of adaptive information filtering, a filtering agent's beliefs about a user's information needs have to be revised regularly with reference to the user's most current information preferences. This learning and adaptation process is essential for maintaining the agent's filtering performance. The AGM belief revision paradigm provides a rigorous foundation for modelling rational and minimal changes to an agent's beliefs. In particular, the maxi-adjustment method, which follows the AGM rationale of belief change, offers a sound and robust computational mechanism to develop adaptive agents so that learning autonomy of these agents can be enhanced. This paper describes how the maxi-adjustment method is applied to develop the learning components of adaptive information filtering agents, and discusses possible difficulties of applying such a framework to these agents.",
        "published": "2000-03-07T02:12:55Z",
        "link": "http://arxiv.org/abs/cs/0003014v2",
        "categories": [
            "cs.AI",
            "cs.MA",
            "I.2.3;I.2.11;H.3.3"
        ]
    },
    {
        "title": "Abductive and Consistency-Based Diagnosis Revisited: a Modeling   Perspective",
        "authors": [
            "Daniele Theseider Dupre'"
        ],
        "summary": "Diagnostic reasoning has been characterized logically as consistency-based reasoning or abductive reasoning. Previous analyses in the literature have shown, on the one hand, that choosing the (in general more restrictive) abductive definition may be appropriate or not, depending on the content of the knowledge base [Console&Torasso91], and, on the other hand, that, depending on the choice of the definition the same knowledge should be expressed in different form [Poole94].   Since in Model-Based Diagnosis a major problem is finding the right way of abstracting the behavior of the system to be modeled, this paper discusses the relation between modeling, and in particular abstraction in the model, and the notion of diagnosis.",
        "published": "2000-03-07T11:39:53Z",
        "link": "http://arxiv.org/abs/cs/0003016v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "The lexicographic closure as a revision process",
        "authors": [
            "Richard Booth"
        ],
        "summary": "The connections between nonmonotonic reasoning and belief revision are well-known. A central problem in the area of nonmonotonic reasoning is the problem of default entailment, i.e., when should an item of default information representing \"if A is true then, normally, B is true\" be said to follow from a given set of items of such information. Many answers to this question have been proposed but, surprisingly, virtually none have attempted any explicit connection to belief revision. The aim of this paper is to give an example of how such a connection can be made by showing how the lexicographic closure of a set of defaults may be conceptualised as a process of iterated revision by sets of sentences. Specifically we use the revision process of Nayak.",
        "published": "2000-03-07T11:44:35Z",
        "link": "http://arxiv.org/abs/cs/0003017v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "On the semantics of merging",
        "authors": [
            "Thomas Meyer"
        ],
        "summary": "Intelligent agents are often faced with the problem of trying to merge possibly conflicting pieces of information obtained from different sources into a consistent view of the world. We propose a framework for the modelling of such merging operations with roots in the work of Spohn (1988, 1991). Unlike most approaches we focus on the merging of epistemic states, not knowledge bases. We construct a number of plausible merging operations and measure them against various properties that merging operations ought to satisfy. Finally, we discuss the connection between merging and the use of infobases Meyer (1999) and Meyer et al. (2000).",
        "published": "2000-03-07T12:26:34Z",
        "link": "http://arxiv.org/abs/cs/0003015v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; I.2.4; I.2.11"
        ]
    },
    {
        "title": "Description of GADEL",
        "authors": [
            "I. Stephan",
            "F. Saubion",
            "P. Nicolas"
        ],
        "summary": "This article describes the first implementation of the GADEL system : a Genetic Algorithm for Default Logic. The goal of GADEL is to compute extensions in Reiter's default logic. It accepts every kind of finite propositional default theories and is based on evolutionary principles of Genetic Algorithms. Its first experimental results on certain instances of the problem show that this new approach of the problem can be successful.",
        "published": "2000-03-07T14:46:23Z",
        "link": "http://arxiv.org/abs/cs/0003018v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Extending Classical Logic with Inductive Definitions",
        "authors": [
            "Marc Denecker"
        ],
        "summary": "The goal of this paper is to extend classical logic with a generalized notion of inductive definition supporting positive and negative induction, to investigate the properties of this logic, its relationships to other logics in the area of non-monotonic reasoning, logic programming and deductive databases, and to show its application for knowledge representation by giving a typology of definitional knowledge.",
        "published": "2000-03-07T15:44:08Z",
        "link": "http://arxiv.org/abs/cs/0003019v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3;I.2.4;F.4.1"
        ]
    },
    {
        "title": "ACLP: Integrating Abduction and Constraint Solving",
        "authors": [
            "Antonis Kakas"
        ],
        "summary": "ACLP is a system which combines abductive reasoning and constraint solving by integrating the frameworks of Abductive Logic Programming (ALP) and Constraint Logic Programming (CLP). It forms a general high-level knowledge representation environment for abductive problems in Artificial Intelligence and other areas. In ACLP, the task of abduction is supported and enhanced by its non-trivial integration with constraint solving facilitating its application to complex problems. The ACLP system is currently implemented on top of the CLP language of ECLiPSe as a meta-interpreter exploiting its underlying constraint solver for finite domains. It has been applied to the problems of planning and scheduling in order to test its computational effectiveness compared with the direct use of the (lower level) constraint solving framework of CLP on which it is built. These experiments provide evidence that the abductive framework of ACLP does not compromise significantly the computational efficiency of the solutions. Other experiments show the natural ability of ACLP to accommodate easily and in a robust way new or changing requirements of the original problem.",
        "published": "2000-03-07T22:47:13Z",
        "link": "http://arxiv.org/abs/cs/0003020v2",
        "categories": [
            "cs.AI",
            "I.2.4;F.4.1"
        ]
    },
    {
        "title": "Relevance Sensitive Non-Monotonic Inference on Belief Sequences",
        "authors": [
            "Samir Chopra",
            "Konstantinos Georgatos",
            "Rohit Parikh"
        ],
        "summary": "We present a method for relevance sensitive non-monotonic inference from belief sequences which incorporates insights pertaining to prioritized inference and relevance sensitive, inconsistency tolerant belief revision.   Our model uses a finite, logically open sequence of propositional formulas as a representation for beliefs and defines a notion of inference from maxiconsistent subsets of formulas guided by two orderings: a temporal sequencing and an ordering based on relevance relations between the conclusion and formulas in the sequence. The relevance relations are ternary (using context as a parameter) as opposed to standard binary axiomatizations. The inference operation thus defined easily handles iterated revision by maintaining a revision history, blocks the derivation of inconsistent answers from a possibly inconsistent sequence and maintains the distinction between explicit and implicit beliefs. In doing so, it provides a finitely presented formalism and a plausible model of reasoning for automated agents.",
        "published": "2000-03-08T03:03:36Z",
        "link": "http://arxiv.org/abs/cs/0003021v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "A Compiler for Ordered Logic Programs",
        "authors": [
            "James P. Delgrande",
            "Torsten Schaub",
            "Hans Tompits"
        ],
        "summary": "This paper describes a system, called PLP, for compiling ordered logic programs into standard logic programs under the answer set semantics. In an ordered logic program, rules are named by unique terms, and preferences among rules are given by a set of dedicated atoms. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed theory correspond with the preferred answer sets of the original theory. Since the result of the translation is an extended logic program, existing logic programming systems can be used as underlying reasoning engine. In particular, PLP is conceived as a front-end to the logic programming systems dlv and smodels.",
        "published": "2000-03-08T10:15:51Z",
        "link": "http://arxiv.org/abs/cs/0003024v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Probabilistic Default Reasoning with Conditional Constraints",
        "authors": [
            "Thomas Lukasiewicz"
        ],
        "summary": "We propose a combination of probabilistic reasoning from conditional constraints with approaches to default reasoning from conditional knowledge bases. In detail, we generalize the notions of Pearl's entailment in system Z, Lehmann's lexicographic entailment, and Geffner's conditional entailment to conditional constraints. We give some examples that show that the new notions of z-, lexicographic, and conditional entailment have similar properties like their classical counterparts. Moreover, we show that the new notions of z-, lexicographic, and conditional entailment are proper generalizations of both their classical counterparts and the classical notion of logical entailment for conditional constraints.",
        "published": "2000-03-08T11:05:45Z",
        "link": "http://arxiv.org/abs/cs/0003023v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "SLDNFA-system",
        "authors": [
            "Bert Van Nuffelen"
        ],
        "summary": "The SLDNFA-system results from the LP+ project at the K.U.Leuven, which investigates logics and proof procedures for these logics for declarative knowledge representation. Within this project inductive definition logic (ID-logic) is used as representation logic. Different solvers are being developed for this logic and one of these is SLDNFA. A prototype of the system is available and used for investigating how to solve efficiently problems represented in ID-logic.",
        "published": "2000-03-08T13:22:44Z",
        "link": "http://arxiv.org/abs/cs/0003027v1",
        "categories": [
            "cs.AI",
            "F.4.1;I.2.3;I.2.4"
        ]
    },
    {
        "title": "Logic Programming for Describing and Solving Planning Problems",
        "authors": [
            "Maurice Bruynooghe"
        ],
        "summary": "A logic programming paradigm which expresses solutions to problems as stable models has recently been promoted as a declarative approach to solving various combinatorial and search problems, including planning problems. In this paradigm, all program rules are considered as constraints and solutions are stable models of the rule set. This is a rather radical departure from the standard paradigm of logic programming. In this paper we revisit abductive logic programming and argue that it allows a programming style which is as declarative as programming based on stable models. However, within abductive logic programming, one has two kinds of rules. On the one hand predicate definitions (which may depend on the abducibles) which are nothing else than standard logic programs (with their non-monotonic semantics when containing with negation); on the other hand rules which constrain the models for the abducibles. In this sense abductive logic programming is a smooth extension of the standard paradigm of logic programming, not a radical departure.",
        "published": "2000-03-08T14:00:35Z",
        "link": "http://arxiv.org/abs/cs/0003025v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "D.1.6; D.3.1; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Logic Programs with Compiled Preferences",
        "authors": [
            "James P. Delgrande",
            "Torsten Schaub",
            "Hans Tompits"
        ],
        "summary": "We describe an approach for compiling preferences into logic programs under the answer set semantics. An ordered logic program is an extended logic program in which rules are named by unique terms, and in which preferences among rules are given by a set of dedicated atoms. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed theory correspond with the preferred answer sets of the original theory. Our approach allows both the specification of static orderings (as found in most previous work), in which preferences are external to a logic program, as well as orderings on sets of rules. In large part then, we are interested in describing a general methodology for uniformly incorporating preference information in a logic program. Since the result of our translation is an extended logic program, we can make use of existing implementations, such as dlv and smodels. To this end, we have developed a compiler, available on the web, as a front-end for these programming systems.",
        "published": "2000-03-08T14:09:56Z",
        "link": "http://arxiv.org/abs/cs/0003028v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Fuzzy Approaches to Abductive Inference",
        "authors": [
            "Nedra Mellouli",
            "Bernadette Bouchon-Meunier"
        ],
        "summary": "This paper proposes two kinds of fuzzy abductive inference in the framework of fuzzy rule base. The abductive inference processes described here depend on the semantic of the rule. We distinguish two classes of interpretation of a fuzzy rule, certainty generation rules and possible generation rules. In this paper we present the architecture of abductive inference in the first class of interpretation. We give two kinds of problem that we can resolve by using the proposed models of inference.",
        "published": "2000-03-08T14:56:58Z",
        "link": "http://arxiv.org/abs/cs/0003029v1",
        "categories": [
            "cs.AI",
            "Artificial intelligence and nonmonotonic reasoning and belief\n  revision"
        ]
    },
    {
        "title": "Problem solving in ID-logic with aggregates: some experiments",
        "authors": [
            "Bert Van Nuffelen",
            "Marc Denecker"
        ],
        "summary": "The goal of the LP+ project at the K.U.Leuven is to design an expressive logic, suitable for declarative knowledge representation, and to develop intelligent systems based on Logic Programming technology for solving computational problems using the declarative specifications. The ID-logic is an integration of typed classical logic and a definition logic. Different abductive solvers for this language are being developed. This paper is a report of the integration of high order aggregates into ID-logic and the consequences on the solver SLDNFA.",
        "published": "2000-03-08T15:39:14Z",
        "link": "http://arxiv.org/abs/cs/0003030v1",
        "categories": [
            "cs.AI",
            "F.4.1;I.2.4;I.2.3"
        ]
    },
    {
        "title": "Optimal Belief Revision",
        "authors": [
            "Carmen Vodislav",
            "Robert E. Mercer"
        ],
        "summary": "We propose a new approach to belief revision that provides a way to change knowledge bases with a minimum of effort. We call this way of revising belief states optimal belief revision. Our revision method gives special attention to the fact that most belief revision processes are directed to a specific informational objective. This approach to belief change is founded on notions such as optimal context and accessibility. For the sentential model of belief states we provide both a formal description of contexts as sub-theories determined by three parameters and a method to construct contexts. Next, we introduce an accessibility ordering for belief sets, which we then use for selecting the best (optimal) contexts with respect to the processing effort involved in the revision. Then, for finitely axiomatizable knowledge bases, we characterize a finite accessibility ranking from which the accessibility ordering for the entire base is generated and show how to determine the ranking of an arbitrary sentence in the language. Finally, we define the adjustment of the accessibility ranking of a revised base of a belief set.",
        "published": "2000-03-08T15:54:50Z",
        "link": "http://arxiv.org/abs/cs/0003031v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Hypothetical revision and matter-of-fact supposition",
        "authors": [
            "Horacio Arlo-Costa"
        ],
        "summary": "The paper studies the notion of supposition encoded in non-Archimedean conditional probability (and revealed in the acceptance of the so-called indicative conditionals). The notion of qualitative change of view that thus arises is axiomatized and compared with standard notions like AGM and UPDATE. Applications in the following fields are discussed: (1) theory of games and decisions, (2) causal models, (3) non-monotonic logic.",
        "published": "2000-03-08T16:06:58Z",
        "link": "http://arxiv.org/abs/cs/0003022v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "I.2.4; I.2.6; I.2.7; I.2.11"
        ]
    },
    {
        "title": "cc-Golog: Towards More Realistic Logic-Based Robot Controllers",
        "authors": [
            "Henrik Grosskreutz",
            "Gerhard Lakemeyer"
        ],
        "summary": "High-level robot controllers in realistic domains typically deal with processes which operate concurrently, change the world continuously, and where the execution of actions is event-driven as in ``charge the batteries as soon as the voltage level is low''. While non-logic-based robot control languages are well suited to express such scenarios, they fare poorly when it comes to projecting, in a conspicuous way, how the world evolves when actions are executed. On the other hand, a logic-based control language like \\congolog, based on the situation calculus, is well-suited for the latter. However, it has problems expressing event-driven behavior. In this paper, we show how these problems can be overcome by first extending the situation calculus to support continuous change and event-driven behavior and then presenting \\ccgolog, a variant of \\congolog which is based on the extended situation calculus. One benefit of \\ccgolog is that it narrows the gap in expressiveness compared to non-logic-based control languages while preserving a semantically well-founded projection mechanism.",
        "published": "2000-03-08T16:14:08Z",
        "link": "http://arxiv.org/abs/cs/0003032v1",
        "categories": [
            "cs.AI",
            "I.2.3;I.2.8"
        ]
    },
    {
        "title": "E-RES: A System for Reasoning about Actions, Events and Observations",
        "authors": [
            "Antonis Kakas",
            "Rob Miller",
            "Francesca Toni"
        ],
        "summary": "E-RES is a system that implements the Language E, a logic for reasoning about narratives of action occurrences and observations. E's semantics is model-theoretic, but this implementation is based on a sound and complete reformulation of E in terms of argumentation, and uses general computational techniques of argumentation frameworks. The system derives sceptical non-monotonic consequences of a given reformulated theory which exactly correspond to consequences entailed by E's model-theory. The computation relies on a complimentary ability of the system to derive credulous non-monotonic consequences together with a set of supporting assumptions which is sufficient for the (credulous) conclusion to hold. E-RES allows theories to contain general action laws, statements about action occurrences, observations and statements of ramifications (or universal laws). It is able to derive consequences both forward and backward in time. This paper gives a short overview of the theoretical basis of E-RES and illustrates its use on a variety of examples. Currently, E-RES is being extended so that the system can be used for planning.",
        "published": "2000-03-08T16:18:52Z",
        "link": "http://arxiv.org/abs/cs/0003034v2",
        "categories": [
            "cs.AI",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Declarative Representation of Revision Strategies",
        "authors": [
            "Gerhard Brewka"
        ],
        "summary": "In this paper we introduce a nonmonotonic framework for belief revision in which reasoning about the reliability of different pieces of information based on meta-knowledge about the information is possible, and where revision strategies can be described declaratively. The approach is based on a Poole-style system for default reasoning in which entrenchment information is represented in the logical language. A notion of inference based on the least fixed point of a monotone operator is used to make sure that all theories possess a consistent set of conclusions.",
        "published": "2000-03-08T16:22:03Z",
        "link": "http://arxiv.org/abs/cs/0003035v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "QUIP - A Tool for Computing Nonmonotonic Reasoning Tasks",
        "authors": [
            "Uwe Egly",
            "Thomas Eiter",
            "Hans Tompits",
            "Stefan Woltran"
        ],
        "summary": "In this paper, we outline the prototype of an automated inference tool, called QUIP, which provides a uniform implementation for several nonmonotonic reasoning formalisms. The theoretical basis of QUIP is derived from well-known results about the computational complexity of nonmonotonic logics and exploits a representation of the different reasoning tasks in terms of quantified boolean formulae.",
        "published": "2000-03-08T17:18:08Z",
        "link": "http://arxiv.org/abs/cs/0003037v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "DLV - A System for Declarative Problem Solving",
        "authors": [
            "Thomas Eiter",
            "Wolfgang Faber",
            "Christoph Koch",
            "Nicola Leone",
            "Gerald Pfeifer"
        ],
        "summary": "DLV is an efficient logic programming and non-monotonic reasoning (LPNMR) system with advanced knowledge representation mechanisms and interfaces to classic relational database systems.   Its core language is disjunctive datalog (function-free disjunctive logic programming) under the Answer Set Semantics with integrity constraints, both default and strong (or explicit) negation, and queries. Integer arithmetics and various built-in predicates are also supported.   In addition DLV has several frontends, namely brave and cautious reasoning, abductive diagnosis, consistency-based diagnosis, a subset of SQL3, planning with action languages, and logic programming with inheritance.",
        "published": "2000-03-08T18:17:33Z",
        "link": "http://arxiv.org/abs/cs/0003036v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "D.1.6; D.3.2; I.2.4; F.4.1"
        ]
    },
    {
        "title": "A Splitting Set Theorem for Epistemic Specifications",
        "authors": [
            "Richard Watson"
        ],
        "summary": "Over the past decade a considerable amount of research has been done to expand logic programming languages to handle incomplete information. One such language is the language of epistemic specifications. As is usual with logic programming languages, the problem of answering queries is intractable in the general case. For extended disjunctive logic programs, an idea that has proven useful in simplifying the investigation of answer sets is the use of splitting sets. In this paper we will present an extended definition of splitting sets that will be applicable to epistemic specifications. Furthermore, an extension of the splitting set theorem will be presented. Also, a characterization of stratified epistemic specifications will be given in terms of splitting sets. This characterization leads us to an algorithmic method of computing world views of a subclass of epistemic logic programs.",
        "published": "2000-03-08T20:40:31Z",
        "link": "http://arxiv.org/abs/cs/0003038v1",
        "categories": [
            "cs.AI",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Implementing Integrity Constraints in an Existing Belief Revision System",
        "authors": [
            "Frances L. Johnson",
            "Stuart C. Shapiro"
        ],
        "summary": "SNePS is a mature knowledge representation, reasoning, and acting system that has long contained a belief revision subsystem, called SNeBR. SNeBR is triggered when an explicit contradiction is introduced into the SNePS belief space, either because of a user's new assertion, or because of a user's query. SNeBR then makes the user decide what belief to remove from the belief space in order to restore consistency, although it provides information to help the user in making that decision. We have recently added automatic belief revision to SNeBR, by which, under certain circumstances, SNeBR decides by itself which belief to remove, and then informs the user of the decision and its consequences. We have used the well-known belief revision integrity constraints as a guide in designing automatic belief revision, taking into account, however, that SNePS's belief space is not deductively closed, and that it would be infeasible to form the deductive closure in order to decide what belief to remove. This paper briefly describes SNeBR both before and after this revision, discusses how we adapted the integrity constraints for this purpose, and gives an example of the new SNeBR in action.",
        "published": "2000-03-08T20:42:29Z",
        "link": "http://arxiv.org/abs/cs/0003040v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3;I.2.4"
        ]
    },
    {
        "title": "DES: a Challenge Problem for Nonmonotonic Reasoning Systems",
        "authors": [
            "Maarit Hietalahti",
            "Fabio Massacci",
            "Ilkka Niemela"
        ],
        "summary": "The US Data Encryption Standard, DES for short, is put forward as an interesting benchmark problem for nonmonotonic reasoning systems because (i) it provides a set of test cases of industrial relevance which shares features of randomly generated problems and real-world problems, (ii) the representation of DES using normal logic programs with the stable model semantics is simple and easy to understand, and (iii) this subclass of logic programs can be seen as an interesting special case for many other formalizations of nonmonotonic reasoning. In this paper we present two encodings of DES as logic programs: a direct one out of the standard specifications and an optimized one extending the work of Massacci and Marraro. The computational properties of the encodings are studied by using them for DES key search with the Smodels system as the implementation of the stable model semantics. Results indicate that the encodings and Smodels are quite competitive: they outperform state-of-the-art SAT-checkers working with an optimized encoding of DES into SAT and are comparable with a SAT-checker that is customized and tuned for the optimized SAT encoding.",
        "published": "2000-03-08T21:49:57Z",
        "link": "http://arxiv.org/abs/cs/0003039v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Smodels: A System for Answer Set Programming",
        "authors": [
            "Ilkka Niemela",
            "Patrik Simons",
            "Tommi Syrjanen"
        ],
        "summary": "The Smodels system implements the stable model semantics for normal logic programs. It handles a subclass of programs which contain no function symbols and are domain-restricted but supports extensions including built-in functions as well as cardinality and weight constraints. On top of this core engine more involved systems can be built. As an example, we have implemented total and partial stable model computation for disjunctive logic programs. An interesting application method is based on answer set programming, i.e., encoding an application problem as a set of rules so that its solutions are captured by the stable models of the rules. Smodels has been applied to a number of areas including planning, model checking, reachability analysis, product configuration, dynamic constraint satisfaction, and feature interaction.",
        "published": "2000-03-08T23:25:51Z",
        "link": "http://arxiv.org/abs/cs/0003033v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Coherence, Belief Expansion and Bayesian Networks",
        "authors": [
            "Luc Bovens",
            "Stephan Hartmann"
        ],
        "summary": "We construct a probabilistic coherence measure for information sets which determines a partial coherence ordering. This measure is applied in constructing a criterion for expanding our beliefs in the face of new information. A number of idealizations are being made which can be relaxed by an appeal to Bayesian Networks.",
        "published": "2000-03-08T23:34:52Z",
        "link": "http://arxiv.org/abs/cs/0003041v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.0; G.3"
        ]
    },
    {
        "title": "Fages' Theorem and Answer Set Programming",
        "authors": [
            "Yuliya Babovich",
            "Esra Erdem",
            "Vladimir Lifschitz"
        ],
        "summary": "We generalize a theorem by Francois Fages that describes the relationship between the completion semantics and the answer set semantics for logic programs with negation as failure. The study of this relationship is important in connection with the emergence of answer set programming. Whenever the two semantics are equivalent, answer sets can be computed by a satisfiability solver, and the use of answer set solvers such as smodels and dlv is unnecessary. A logic programming representation of the blocks world due to Ilkka Niemelae is discussed as an example.",
        "published": "2000-03-09T00:28:21Z",
        "link": "http://arxiv.org/abs/cs/0003042v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "On the tractable counting of theory models and its application to belief   revision and truth maintenance",
        "authors": [
            "Adnan Darwiche"
        ],
        "summary": "We introduced decomposable negation normal form (DNNF) recently as a tractable form of propositional theories, and provided a number of powerful logical operations that can be performed on it in polynomial time. We also presented an algorithm for compiling any conjunctive normal form (CNF) into DNNF and provided a structure-based guarantee on its space and time complexity. We present in this paper a linear-time algorithm for converting an ordered binary decision diagram (OBDD) representation of a propositional theory into an equivalent DNNF, showing that DNNFs scale as well as OBDDs. We also identify a subclass of DNNF which we call deterministic DNNF, d-DNNF, and show that the previous complexity guarantees on compiling DNNF continue to hold for this stricter subclass, which has stronger properties. In particular, we present a new operation on d-DNNF which allows us to count its models under the assertion, retraction and flipping of every literal by traversing the d-DNNF twice. That is, after such traversal, we can test in constant-time: the entailment of any literal by the d-DNNF, and the consistency of the d-DNNF under the retraction or flipping of any literal. We demonstrate the significance of these new operations by showing how they allow us to implement linear-time, complete truth maintenance systems and linear-time, complete belief revision systems for two important classes of propositional theories.",
        "published": "2000-03-09T08:58:15Z",
        "link": "http://arxiv.org/abs/cs/0003044v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Linear Tabulated Resolution Based on Prolog Control Strategy",
        "authors": [
            "Yi-Dong Shen",
            "Li-Yan Yuan",
            "Jia-Huai You",
            "Neng-Fa Zhou"
        ],
        "summary": "Infinite loops and redundant computations are long recognized open problems in Prolog. Two ways have been explored to resolve these problems: loop checking and tabling. Loop checking can cut infinite loops, but it cannot be both sound and complete even for function-free logic programs. Tabling seems to be an effective way to resolve infinite loops and redundant computations. However, existing tabulated resolutions, such as OLDT-resolution, SLG- resolution, and Tabulated SLS-resolution, are non-linear because they rely on the solution-lookup mode in formulating tabling. The principal disadvantage of non-linear resolutions is that they cannot be implemented using a simple stack-based memory structure like that in Prolog. Moreover, some strictly sequential operators such as cuts may not be handled as easily as in Prolog.   In this paper, we propose a hybrid method to resolve infinite loops and redundant computations. We combine the ideas of loop checking and tabling to establish a linear tabulated resolution called TP-resolution. TP-resolution has two distinctive features: (1) It makes linear tabulated derivations in the same way as Prolog except that infinite loops are broken and redundant computations are reduced. It handles cuts as effectively as Prolog. (2) It is sound and complete for positive logic programs with the bounded-term-size property. The underlying algorithm can be implemented by an extension to any existing Prolog abstract machines such as WAM or ATOAM.",
        "published": "2000-03-09T17:11:39Z",
        "link": "http://arxiv.org/abs/cs/0003046v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "BDD-based reasoning in the fluent calculus - first results",
        "authors": [
            "Steffen Hoelldobler",
            "Hans-Peter Stoerr"
        ],
        "summary": "The paper reports on first preliminary results and insights gained in a project aiming at implementing the fluent calculus using methods and techniques based on binary decision diagrams. After reporting on an initial experiment showing promising results we discuss our findings concerning various techniques and heuristics used to speed up the reasoning process.",
        "published": "2000-03-09T17:18:12Z",
        "link": "http://arxiv.org/abs/cs/0003047v1",
        "categories": [
            "cs.AI",
            "I.2.8; I.2.3; F.4.1"
        ]
    },
    {
        "title": "PAL: Pertinence Action Language",
        "authors": [
            "Pedro Cabalar",
            "Manuel Cabarcos",
            "Ramon P. Otero"
        ],
        "summary": "The current document contains a brief description of a system for Reasoning about Actions and Change called PAL (Pertinence Action Language) which makes use of several reasoning properties extracted from a Temporal Expert Systems tool called Medtool.",
        "published": "2000-03-09T19:50:50Z",
        "link": "http://arxiv.org/abs/cs/0003048v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Planning with Incomplete Information",
        "authors": [
            "Antonis Kakas",
            "Rob Miller",
            "Francesca Toni"
        ],
        "summary": "Planning is a natural domain of application for frameworks of reasoning about actions and change. In this paper we study how one such framework, the Language E, can form the basis for planning under (possibly) incomplete information. We define two types of plans: weak and safe plans, and propose a planner, called the E-Planner, which is often able to extend an initial weak plan into a safe plan even though the (explicit) information available is incomplete, e.g. for cases where the initial state is not completely known. The E-Planner is based upon a reformulation of the Language E in argumentation terms and a natural proof theory resulting from the reformulation. It uses an extension of this proof theory by means of abduction for the generation of plans and adopts argumentation-based techniques for extending weak plans into safe plans. We provide representative examples illustrating the behaviour of the E-Planner, in particular for cases where the status of fluents is incompletely known.",
        "published": "2000-03-09T22:30:27Z",
        "link": "http://arxiv.org/abs/cs/0003049v1",
        "categories": [
            "cs.AI",
            "I.2.3;I.2.4;I.2.8"
        ]
    },
    {
        "title": "A tableau methodology for deontic conditional logics",
        "authors": [
            "Alberto Artosi",
            "Guido Governatori"
        ],
        "summary": "In this paper we present a theorem proving methodology for a restricted but significant fragment of the conditional language made up of (boolean combinations of) conditional statements with unnested antecedents. The method is based on the possible world semantics for conditional logics. The KEM label formalism, designed to account for the semantics of normal modal logics, is easily adapted to the semantics of conditional logics by simply indexing labels with formulas. The inference rules are provided by the propositional system KE+ - a tableau-like analytic proof system devised to be used both as a refutation and a direct method of proof - enlarged with suitable elimination rules for the conditional connective. The theorem proving methodology we are going to present can be viewed as a first step towards developing an appropriate algorithmic framework for several conditional logics for (defeasible) conditional obligation.",
        "published": "2000-03-10T07:30:32Z",
        "link": "http://arxiv.org/abs/cs/0003050v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Local Diagnosis",
        "authors": [
            "Renata Wassermann"
        ],
        "summary": "In an earlier work, we have presented operations of belief change which only affect the relevant part of a belief base. In this paper, we propose the application of the same strategy to the problem of model-based diangosis. We first isolate the subset of the system description which is relevant for a given observation and then solve the diagnosis problem for this subset.",
        "published": "2000-03-10T22:54:55Z",
        "link": "http://arxiv.org/abs/cs/0003051v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "A Consistency-Based Model for Belief Change: Preliminary Report",
        "authors": [
            "James Delgrande",
            "Torsten Schaub"
        ],
        "summary": "We present a general, consistency-based framework for belief change. Informally, in revising K by A, we begin with A and incorporate as much of K as consistently possible. Formally, a knowledge base K and sentence A are expressed, via renaming propositions in K, in separate languages. Using a maximization process, we assume the languages are the same insofar as consistently possible. Lastly, we express the resultant knowledge base in a single language. There may be more than one way in which A can be so extended by K: in choice revision, one such ``extension'' represents the revised state; alternately revision consists of the intersection of all such extensions.   The most general formulation of our approach is flexible enough to express other approaches to revision and update, the merging of knowledge bases, and the incorporation of static and dynamic integrity constraints. Our framework differs from work based on ordinal conditional functions, notably with respect to iterated revision. We argue that the approach is well-suited for implementation: the choice revision operator gives better complexity results than general revision; the approach can be expressed in terms of a finite knowledge base; and the scope of a revision can be restricted to just those propositions mentioned in the sentence for revision A.",
        "published": "2000-03-11T06:29:02Z",
        "link": "http://arxiv.org/abs/cs/0003052v3",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "A note on the Declarative reading(s) of Logic Programming",
        "authors": [
            "Marc Denecker"
        ],
        "summary": "This paper analyses the declarative readings of logic programming. Logic programming - and negation as failure - has no unique declarative reading. One common view is that logic programming is a logic for default reasoning, a sub-formalism of default logic or autoepistemic logic. In this view, negation as failure is a modal operator. In an alternative view, a logic program is interpreted as a definition. In this view, negation as failure is classical objective negation. From a commonsense point of view, there is definitely a difference between these views. Surprisingly though, both types of declarative readings lead to grosso modo the same model semantics. This note investigates the causes for this.",
        "published": "2000-03-13T16:21:41Z",
        "link": "http://arxiv.org/abs/cs/0003056v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3;I.2.4;F.4.1"
        ]
    },
    {
        "title": "XNMR: A tool for knowledge bases exploration",
        "authors": [
            "L. Castro",
            "D. Warren"
        ],
        "summary": "XNMR is a system designed to explore the results of combining the well-founded semantics system XSB with the stable-models evaluator SMODELS. Its main goal is to work as a tool for fast and interactive exploration of knowledge bases.",
        "published": "2000-03-13T23:18:12Z",
        "link": "http://arxiv.org/abs/cs/0003057v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6"
        ]
    },
    {
        "title": "SATEN: An Object-Oriented Web-Based Revision and Extraction Engine",
        "authors": [
            "Mary-Anne Williams",
            "Aidan Sims"
        ],
        "summary": "SATEN is an object-oriented web-based extraction and belief revision engine. It runs on any computer via a Java 1.1 enabled browser such as Netscape 4. SATEN performs belief revision based on the AGM approach. The extraction and belief revision reasoning engines operate on a user specified ranking of information. One of the features of SATEN is that it can be used to integrate mutually inconsistent commensuate rankings into a consistent ranking.",
        "published": "2000-03-14T04:58:18Z",
        "link": "http://arxiv.org/abs/cs/0003059v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "dcs: An Implementation of DATALOG with Constraints",
        "authors": [
            "Deborah East",
            "Miroslaw Truszczynski"
        ],
        "summary": "Answer-set programming (ASP) has emerged recently as a viable programming paradigm. We describe here an ASP system, DATALOG with constraints or DC, based on non-monotonic logic. Informally, DC theories consist of propositional clauses (constraints) and of Horn rules. The semantics is a simple and natural extension of the semantics of the propositional logic. However, thanks to the presence of Horn rules in the system, modeling of transitive closure becomes straightforward. We describe the syntax, use and implementation of DC and provide experimental results.",
        "published": "2000-03-14T18:06:38Z",
        "link": "http://arxiv.org/abs/cs/0003061v1",
        "categories": [
            "cs.AI",
            "I.2.3;I.2.4;I.2.8;F.4.1;F.2.2"
        ]
    },
    {
        "title": "Detecting Unsolvable Queries for Definite Logic Programs",
        "authors": [
            "Maurice Bruynooghe",
            "Henk Vandecasteele",
            "D. Andre de Waal",
            "Marc Denecker"
        ],
        "summary": "In solving a query, the SLD proof procedure for definite programs sometimes searches an infinite space for a non existing solution. For example, querying a planner for an unreachable goal state. Such programs motivate the development of methods to prove the absence of a solution. Considering the definite program and the query ``<- Q'' as clauses of a first order theory, one can apply model generators which search for a finite interpretation in which the program clauses as well as the clause ``false <- Q'' are true. This paper develops a new approach which exploits the fact that all clauses are definite. It is based on a goal directed abductive search in the space of finite pre-interpretations for a pre-interpretation such that ``Q'' is false in the least model of the program based on it. Several methods for efficiently searching the space of pre-interpretations are presented. Experimental results confirm that our approach find solutions with less search than with the use of a first order model generator.",
        "published": "2000-03-17T10:59:03Z",
        "link": "http://arxiv.org/abs/cs/0003067v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6; F.3.1; F.4.1"
        ]
    },
    {
        "title": "Proceedings of the 8th International Workshop on Non-Monotonic   Reasoning, NMR'2000",
        "authors": [
            "Chitta Baral",
            "Miroslaw Truszczynski"
        ],
        "summary": "The papers gathered in this collection were presented at the 8th International Workshop on Nonmonotonic Reasoning, NMR2000. The series was started by John McCarthy in 1978. The first international NMR workshop was held at Mohonk Mountain House, New Paltz, New York in June, 1984, and was organized by Ray Reiter and Bonnie Webber.   In the last 10 years the area of nonmonotonic reasoning has seen a number of important developments. Significant theoretical advances were made in the understanding of general abstract principles underlying nonmonotonicity. Key results on the expressibility and computational complexity of nonmonotonic logics were established. The role of nonmonotonic reasoning in belief revision, abduction, reasoning about action, planing and uncertainty was further clarified. Several successful NMR systems were built and used in applications such as planning, scheduling, logic programming and constraint satisfaction.   The papers in the proceedings reflect these recent advances in the field. They are grouped into sections corresponding to special sessions as they were held at the workshop:   1. General NMR track   2. Abductive reasonig   3. Belief revision: theory and practice   4. Representing action and planning   5. Systems descriptions and demonstrations   6. Uncertainty frameworks in NMR",
        "published": "2000-03-22T15:33:20Z",
        "link": "http://arxiv.org/abs/cs/0003073v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I2.2;I2.3;I2.4;I2.8;F4.1"
        ]
    },
    {
        "title": "Constraint Programming viewed as Rule-based Programming",
        "authors": [
            "Krzysztof R. Apt",
            "Eric Monfroy"
        ],
        "summary": "We study here a natural situation when constraint programming can be entirely reduced to rule-based programming. To this end we explain first how one can compute on constraint satisfaction problems using rules represented by simple first-order formulas. Then we consider constraint satisfaction problems that are based on predefined, explicitly given constraints. To solve them we first derive rules from these explicitly given constraints and limit the computation process to a repeated application of these rules, combined with labeling.We consider here two types of rules. The first type, that we call equality rules, leads to a new notion of local consistency, called {\\em rule consistency} that turns out to be weaker than arc consistency for constraints of arbitrary arity (called hyper-arc consistency in \\cite{MS98b}). For Boolean constraints rule consistency coincides with the closure under the well-known propagation rules for Boolean constraints. The second type of rules, that we call membership rules, yields a rule-based characterization of arc consistency. To show feasibility of this rule-based approach to constraint programming we show how both types of rules can be automatically generated, as {\\tt CHR} rules of \\cite{fruhwirth-constraint-95}. This yields an implementation of this approach to programming by means of constraint logic programming. We illustrate the usefulness of this approach to constraint programming by discussing various examples, including Boolean constraints, two typical examples of many valued logics, constraints dealing with Waltz's language for describing polyhedral scenes, and Allen's qualitative approach to temporal logic.",
        "published": "2000-03-24T16:12:22Z",
        "link": "http://arxiv.org/abs/cs/0003076v2",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.3.3;I.2.2;I.2.3"
        ]
    },
    {
        "title": "DATALOG with constraints - an answer-set programming system",
        "authors": [
            "Deborah East",
            "Miroslaw Truszczynski"
        ],
        "summary": "Answer-set programming (ASP) has emerged recently as a viable programming paradigm well attuned to search problems in AI, constraint satisfaction and combinatorics. Propositional logic is, arguably, the simplest ASP system with an intuitive semantics supporting direct modeling of problem constraints. However, for some applications, especially those requiring that transitive closure be computed, it requires additional variables and results in large theories. Consequently, it may not be a practical computational tool for such problems. On the other hand, ASP systems based on nonmonotonic logics, such as stable logic programming, can handle transitive closure computation efficiently and, in general, yield very concise theories as problem representations. Their semantics is, however, more complex. Searching for the middle ground, in this paper we introduce a new nonmonotonic logic, DATALOG with constraints or DC. Informally, DC theories consist of propositional clauses (constraints) and of Horn rules. The semantics is a simple and natural extension of the semantics of the propositional logic. However, thanks to the presence of Horn rules in the system, modeling of transitive closure becomes straightforward. We describe the syntax and semantics of DC, and study its properties. We discuss an implementation of DC and present results of experimental study of the effectiveness of DC, comparing it with CSAT, a satisfiability checker and SMODELS implementation of stable logic programming. Our results show that DC is competitive with the other two approaches, in case of many search problems, often yielding much more efficient solutions.",
        "published": "2000-03-24T19:09:59Z",
        "link": "http://arxiv.org/abs/cs/0003077v1",
        "categories": [
            "cs.AI",
            "D.1.6;F.2.2;F.4.2;I.2.8;I.6.5"
        ]
    },
    {
        "title": "Some Remarks on Boolean Constraint Propagation",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "We study here the well-known propagation rules for Boolean constraints. First we propose a simple notion of completeness for sets of such rules and establish a completeness result. Then we show an equivalence in an appropriate sense between Boolean constraint propagation and unit propagation, a form of resolution for propositional logic.   Subsequently we characterize one set of such rules by means of the notion of hyper-arc consistency introduced in (Mohr and Masini 1988). Also, we clarify the status of a similar, though different, set of rules introduced in (Simonis 1989a) and more fully in (Codognet and Diaz 1996).",
        "published": "2000-03-28T11:49:37Z",
        "link": "http://arxiv.org/abs/cs/0003080v1",
        "categories": [
            "cs.AI",
            "D.3.2;D.3.3"
        ]
    },
    {
        "title": "Representation results for defeasible logic",
        "authors": [
            "G. Antoniou",
            "D. Billington",
            "G. Governatori",
            "M. J. Maher"
        ],
        "summary": "The importance of transformations and normal forms in logic programming, and generally in computer science, is well documented. This paper investigates transformations and normal forms in the context of Defeasible Logic, a simple but efficient formalism for nonmonotonic reasoning based on rules and priorities. The transformations described in this paper have two main benefits: on one hand they can be used as a theoretical tool that leads to a deeper understanding of the formalism, and on the other hand they have been used in the development of an efficient implementation of defeasible logic.",
        "published": "2000-03-30T02:23:21Z",
        "link": "http://arxiv.org/abs/cs/0003082v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "A Theory of Universal Artificial Intelligence based on Algorithmic   Complexity",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameterless theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline for a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning, how the AIXI model can formally solve them. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXI-tl, which is still effectively more intelligent than any other time t and space l bounded agent. The computation time of AIXI-tl is of the order tx2^l. Other discussed topics are formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches.",
        "published": "2000-04-03T06:16:16Z",
        "link": "http://arxiv.org/abs/cs/0004001v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "I.2; F.1.3; E.4"
        ]
    },
    {
        "title": "Programming in Alma-0, or Imperative and Declarative Programming   Reconciled",
        "authors": [
            "Krzysztof R. Apt",
            "Andrea Schaerf"
        ],
        "summary": "In (Apt et al, TOPLAS 1998) we introduced the imperative programming language Alma-0 that supports declarative programming. In this paper we illustrate the hybrid programming style of Alma-0 by means of various examples that complement those presented in (Apt et al, TOPLAS 1998). The presented Alma-0 programs illustrate the versatility of the language and show that ``don't know'' nondeterminism can be naturally combined with assignment.",
        "published": "2000-04-05T16:04:26Z",
        "link": "http://arxiv.org/abs/cs/0004002v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL",
            "D.3.2;F.3.2;F.3.3;I.2.8;I.5.5"
        ]
    },
    {
        "title": "Searching for Spaceships",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We describe software that searches for spaceships in Conway's Game of Life and related two-dimensional cellular automata. Our program searches through a state space related to the de Bruijn graph of the automaton, using a method that combines features of breadth first and iterative deepening search, and includes fast bit-parallel graph reachability and path enumeration algorithms for finding the successors of each state. Successful results include a new 2c/7 spaceship in Life, found by searching a space with 2^126 states.",
        "published": "2000-04-10T20:55:55Z",
        "link": "http://arxiv.org/abs/cs/0004003v2",
        "categories": [
            "cs.AI",
            "nlin.CG",
            "I.2.8; F.1.1"
        ]
    },
    {
        "title": "Exact Phase Transitions in Random Constraint Satisfaction Problems",
        "authors": [
            "Ke Xu",
            "Wei Li"
        ],
        "summary": "In this paper we propose a new type of random CSP model, called Model RB, which is a revision to the standard Model B. It is proved that phase transitions from a region where almost all problems are satisfiable to a region where almost all problems are unsatisfiable do exist for Model RB as the number of variables approaches infinity. Moreover, the critical values at which the phase transitions occur are also known exactly. By relating the hardness of Model RB to Model B, it is shown that there exist a lot of hard instances in Model RB.",
        "published": "2000-04-16T07:13:09Z",
        "link": "http://arxiv.org/abs/cs/0004005v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DM",
            "I.2.8; G.3"
        ]
    },
    {
        "title": "A Denotational Semantics for First-Order Logic",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "In Apt and Bezem [AB99] (see cs.LO/9811017) we provided a computational interpretation of first-order formulas over arbitrary interpretations. Here we complement this work by introducing a denotational semantics for first-order logic. Additionally, by allowing an assignment of a non-ground term to a variable we introduce in this framework logical variables.   The semantics combines a number of well-known ideas from the areas of semantics of imperative programming languages and logic programming. In the resulting computational view conjunction corresponds to sequential composition, disjunction to ``don't know'' nondeterminism, existential quantification to declaration of a local variable, and negation to the ``negation as finite failure'' rule. The soundness result shows correctness of the semantics with respect to the notion of truth. The proof resembles in some aspects the proof of the soundness of the SLDNF-resolution.",
        "published": "2000-05-08T12:23:07Z",
        "link": "http://arxiv.org/abs/cs/0005008v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "F.3.2; D.3.2"
        ]
    },
    {
        "title": "PSPACE Reasoning for Graded Modal Logics",
        "authors": [
            "Stephan Tobies"
        ],
        "summary": "We present a PSPACE algorithm that decides satisfiability of the graded modal logic Gr(K_R)---a natural extension of propositional modal logic K_R by counting expressions---which plays an important role in the area of knowledge representation. The algorithm employs a tableaux approach and is the first known algorithm which meets the lower bound for the complexity of the problem. Thus, we exactly fix the complexity of the problem and refute an ExpTime-hardness conjecture. We extend the results to the logic Gr(K_(R \\cap I)), which augments Gr(K_R) with inverse relations and intersection of accessibility relations. This establishes a kind of ``theoretical benchmark'' that all algorithmic approaches can be measured against.",
        "published": "2000-05-08T14:51:58Z",
        "link": "http://arxiv.org/abs/cs/0005009v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "F.4.1"
        ]
    },
    {
        "title": "Extending and Implementing the Stable Model Semantics",
        "authors": [
            "Patrik Simons"
        ],
        "summary": "An algorithm for computing the stable model semantics of logic programs is developed. It is shown that one can extend the semantics and the algorithm to handle new and more expressive types of rules. Emphasis is placed on the use of efficient implementation techniques. In particular, an implementation of lookahead that safely avoids testing every literal for failure and that makes the use of lookahead feasible is presented. In addition, a good heuristic is derived from the principle that the search space should be minimized.   Due to the lack of competitive algorithms and implementations for the computation of stable models, the system is compared with three satisfiability solvers. This shows that the heuristic can be improved by breaking ties, but leaves open the question of how to break them. It also demonstrates that the more expressive rules of the stable model semantics make the semantics clearly preferable over propositional logic when a problem has a more compact logic program representation. Conjunctive normal form representations are never more compact than logic program ones.",
        "published": "2000-05-08T18:26:54Z",
        "link": "http://arxiv.org/abs/cs/0005010v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3; I.2.8; F.4.1"
        ]
    },
    {
        "title": "An Average Analysis of Backtracking on Random Constraint Satisfaction   Problems",
        "authors": [
            "Ke Xu",
            "Wei Li"
        ],
        "summary": "In this paper we propose a random CSP model, called Model GB, which is a natural generalization of standard Model B. It is proved that Model GB in which each constraint is easy to satisfy exhibits non-trivial behaviour (not trivially satisfiable or unsatisfiable) as the number of variables approaches infinity. A detailed analysis to obtain an asymptotic estimate (good to 1+o(1)) of the average number of nodes in a search tree used by the backtracking algorithm on Model GB is also presented. It is shown that the average number of nodes required for finding all solutions or proving that no solution exists grows exponentially with the number of variables. So this model might be an interesting distribution for studying the nature of hard instances and evaluating the performance of CSP algorithms. In addition, we further investigate the behaviour of the average number of nodes as r (the ratio of constraints to variables) varies. The results indicate that as r increases, random CSP instances get easier and easier to solve, and the base for the average number of nodes that is exponential in r tends to 1 as r approaches infinity. Therefore, although the average number of nodes used by the backtracking algorithm on random CSP is exponential, many CSP instances will be very easy to solve when r is sufficiently large.",
        "published": "2000-05-09T02:12:58Z",
        "link": "http://arxiv.org/abs/cs/0005011v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "Reasoning with Axioms: Theory and Pratice",
        "authors": [
            "Ian Horrocks",
            "Stephan Tobies"
        ],
        "summary": "When reasoning in description, modal or temporal logics it is often useful to consider axioms representing universal truths in the domain of discourse. Reasoning with respect to an arbitrary set of axioms is hard, even for relatively inexpressive logics, and it is essential to deal with such axioms in an efficient manner if implemented systems are to be effective in real applications. This is particularly relevant to Description Logics, where subsumption reasoning with respect to a terminology is a fundamental problem. Two optimisation techniques that have proved to be particularly effective in dealing with terminologies are lazy unfolding and absorption. In this paper we seek to improve our theoretical understanding of these important techniques. We define a formal framework that allows the techniques to be precisely described, establish conditions under which they can be safely applied, and prove that, provided these conditions are respected, subsumption testing algorithms will still function correctly. These results are used to show that the procedures used in the FaCT system are correct and, moreover, to show how efficiency can be significantly improved, while still retaining the guarantee of correctness, by relaxing the safety conditions for absorption.",
        "published": "2000-05-09T07:17:29Z",
        "link": "http://arxiv.org/abs/cs/0005012v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1, I.2.3, I.2.4"
        ]
    },
    {
        "title": "Practical Reasoning for Very Expressive Description Logics",
        "authors": [
            "Ian Horrocks",
            "Ulrike Sattler",
            "Stephan Tobies"
        ],
        "summary": "Description Logics (DLs) are a family of knowledge representation formalisms mainly characterised by constructors to build complex concepts and roles from atomic ones. Expressive role constructors are important in many applications, but can be computationally problematical. We present an algorithm that decides satisfiability of the DL ALC extended with transitive and inverse roles and functional restrictions with respect to general concept inclusion axioms and role hierarchies; early experiments indicate that this algorithm is well-suited for implementation. Additionally, we show that ALC extended with just transitive and inverse roles is still in PSPACE. We investigate the limits of decidability for this family of DLs, showing that relaxing the constraints placed on the kinds of roles used in number restrictions leads to the undecidability of all inference problems. Finally, we describe a number of optimisation techniques that are crucial in obtaining implementations of the decision procedures, which, despite the worst-case complexity of the problem, exhibit good performance with real-life problems.",
        "published": "2000-05-09T13:02:40Z",
        "link": "http://arxiv.org/abs/cs/0005013v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1, I.2.3, I.2.4"
        ]
    },
    {
        "title": "Practical Reasoning for Expressive Description Logics",
        "authors": [
            "Ian Horrocks",
            "Ulrike Sattler",
            "Stephan Tobies"
        ],
        "summary": "Description Logics (DLs) are a family of knowledge representation formalisms mainly characterised by constructors to build complex concepts and roles from atomic ones. Expressive role constructors are important in many applications, but can be computationally problematical. We present an algorithm that decides satisfiability of the DL ALC extended with transitive and inverse roles, role hierarchies, and qualifying number restrictions. Early experiments indicate that this algorithm is well-suited for implementation. Additionally, we show that ALC extended with just transitive and inverse roles is still in PSPACE. Finally, we investigate the limits of decidability for this family of DLs.",
        "published": "2000-05-10T08:19:41Z",
        "link": "http://arxiv.org/abs/cs/0005014v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1, I.2.3, I.2.4"
        ]
    },
    {
        "title": "Reasoning with Individuals for the Description Logic SHIQ",
        "authors": [
            "Ian Horrock",
            "Ulrike Sattler",
            "Stephan Tobies"
        ],
        "summary": "While there has been a great deal of work on the development of reasoning algorithms for expressive description logics, in most cases only Tbox reasoning is considered. In this paper we present an algorithm for combined Tbox and Abox reasoning in the SHIQ description logic. This algorithm is of particular interest as it can be used to decide the problem of (database) conjunctive query containment w.r.t. a schema. Moreover, the realisation of an efficient implementation should be relatively straightforward as it can be based on an existing highly optimised implementation of the Tbox algorithm in the FaCT system.",
        "published": "2000-05-11T08:16:21Z",
        "link": "http://arxiv.org/abs/cs/0005017v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1, I.2.3, I.2.4"
        ]
    },
    {
        "title": "Centroid-based summarization of multiple documents: sentence extraction,   utility-based evaluation, and user studies",
        "authors": [
            "Dragomir R. Radev",
            "Hongyan Jing",
            "Malgorzata Budzikowska"
        ],
        "summary": "We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.",
        "published": "2000-05-12T17:24:06Z",
        "link": "http://arxiv.org/abs/cs/0005020v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DL",
            "cs.HC",
            "cs.IR",
            "H.3.1; H.3.4; H.3.7; H.5.2; I.2.7"
        ]
    },
    {
        "title": "Modeling the Uncertainty in Complex Engineering Systems",
        "authors": [
            "A. Guergachi"
        ],
        "summary": "Existing procedures for model validation have been deemed inadequate for many engineering systems. The reason of this inadequacy is due to the high degree of complexity of the mechanisms that govern these systems. It is proposed in this paper to shift the attention from modeling the engineering system itself to modeling the uncertainty that underlies its behavior. A mathematical framework for modeling the uncertainty in complex engineering systems is developed. This framework uses the results of computational learning theory. It is based on the premise that a system model is a learning machine.",
        "published": "2000-05-14T14:35:20Z",
        "link": "http://arxiv.org/abs/cs/0005021v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.6;I.6.4;J.2;G.3"
        ]
    },
    {
        "title": "The SAT Phase Transition",
        "authors": [
            "Ke Xu",
            "Wei Li"
        ],
        "summary": "Phase transition is an important feature of SAT problem. For random k-SAT model, it is proved that as r (ratio of clauses to variables) increases, the structure of solutions will undergo a sudden change like satisfiability phase transition when r reaches a threshold point. This phenomenon shows that the satisfying truth assignments suddenly shift from being relatively different from each other to being very similar to each other.",
        "published": "2000-05-22T04:45:53Z",
        "link": "http://arxiv.org/abs/cs/0005024v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "F.2.m; I.2.8"
        ]
    },
    {
        "title": "Applying MDL to Learning Best Model Granularity",
        "authors": [
            "Qiong Gao",
            "Ming Li",
            "Paul Vitanyi"
        ],
        "summary": "The Minimum Description Length (MDL) principle is solidly based on a provably ideal method of inference using Kolmogorov complexity. We test how the theory behaves in practice on a general problem in model selection: that of learning the best model granularity. The performance of a model depends critically on the granularity, for example the choice of precision of the parameters. Too high precision generally involves modeling of accidental noise and too low precision may lead to confusion of models that should be distinguished. This precision is often determined ad hoc. In MDL the best model is the one that most compresses a two-part code of the data set: this embodies ``Occam's Razor.'' In two quite different experimental settings the theoretical value determined using MDL coincides with the best value found experimentally. In the first experiment the task is to recognize isolated handwritten characters in one subject's handwriting, irrespective of size and orientation. Based on a new modification of elastic matching, using multiple prototypes per character, the optimal prediction rate is predicted for the learned parameter (length of sampling interval) considered most likely by MDL, which is shown to coincide with the best value found experimentally. In the second experiment the task is to model a robot arm with two degrees of freedom using a three layer feed-forward neural network where we need to determine the number of nodes in the hidden layer giving best modeling performance. The optimal model (the one that extrapolizes best on unseen examples) is predicted for the number of nodes in the hidden layer considered most likely by MDL, which again is found to coincide with the best value found experimentally.",
        "published": "2000-05-23T14:50:07Z",
        "link": "http://arxiv.org/abs/physics/0005062v1",
        "categories": [
            "physics.data-an",
            "cs.AI",
            "cs.CV"
        ]
    },
    {
        "title": "Axiomatizing Causal Reasoning",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "Causal models defined in terms of a collection of equations, as defined by Pearl, are axiomatized here. Axiomatizations are provided for three successively more general classes of causal models: (1) the class of recursive theories (those without feedback), (2) the class of theories where the solutions to the equations are unique, (3) arbitrary theories (where the equations may not have solutions and, if they do, they are not necessarily unique). It is shown that to reason about causality in the most general third class, we must extend the language used by Galles and Pearl. In addition, the complexity of the decision procedures is characterized for all the languages and classes of models considered.",
        "published": "2000-05-30T18:56:46Z",
        "link": "http://arxiv.org/abs/cs/0005030v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Conditional Plausibility Measures and Bayesian Networks",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "A general notion of algebraic conditional plausibility measures is defined. Probability measures, ranking functions, possibility measures, and (under the appropriate definitions) sets of probability measures can all be viewed as defining algebraic conditional plausibility measures. It is shown that algebraic conditional plausibility measures can be represented using Bayesian networks.",
        "published": "2000-05-30T19:05:21Z",
        "link": "http://arxiv.org/abs/cs/0005031v3",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Knowledge and common knowledge in a distributed environment",
        "authors": [
            "Joseph Y. Halpern",
            "Yoram Moses"
        ],
        "summary": "Reasoning about knowledge seems to play a fundamental role in distributed systems. Indeed, such reasoning is a central part of the informal intuitive arguments used in the design of distributed protocols. Communication in a distributed system can be viewed as the act of transforming the system's state of knowledge. This paper presents a general framework for formalizing and reasoning about knowledge in distributed systems. We argue that states of knowledge of groups of processors are useful concepts for the design and analysis of distributed protocols. In particular, distributed knowledge corresponds to knowledge that is ``distributed'' among the members of the group, while common knowledge corresponds to a fact being ``publicly known''. The relationship between common knowledge and a variety of desirable actions in a distributed system is illustrated. Furthermore, it is shown that, formally speaking, in practical systems common knowledge cannot be attained. A number of weaker variants of common knowledge that are attainable in many cases of interest are introduced and investigated.",
        "published": "2000-06-02T18:43:33Z",
        "link": "http://arxiv.org/abs/cs/0006009v1",
        "categories": [
            "cs.DC",
            "cs.AI",
            "C.2.2, C.2.4, D.2.4, I.2.4, F.3.1, F.3.1"
        ]
    },
    {
        "title": "An evaluation of Naive Bayesian anti-spam filtering",
        "authors": [
            "Ion Androutsopoulos",
            "John Koutsias",
            "Konstantinos V. Chandrinos",
            "George Paliouras",
            "Constantine D. Spyropoulos"
        ],
        "summary": "It has recently been argued that a Naive Bayesian classifier can be used to filter unsolicited bulk e-mail (\"spam\"). We conduct a thorough evaluation of this proposal on a corpus that we make publicly available, contributing towards standard benchmarks. At the same time we investigate the effect of attribute-set size, training-corpus size, lemmatization, and stop-lists on the filter's performance, issues that had not been previously explored. After introducing appropriate cost-sensitive evaluation measures, we reach the conclusion that additional safety nets are needed for the Naive Bayesian anti-spam filter to be viable in practice.",
        "published": "2000-06-07T11:10:50Z",
        "link": "http://arxiv.org/abs/cs/0006013v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "H.4.3; I.2.6; I.2.7; I.5.4; K.4.1"
        ]
    },
    {
        "title": "Verifying Termination of General Logic Programs with Concrete Queries",
        "authors": [
            "Yi-Dong Shen",
            "Li-Yan Yuan",
            "Jia-Huai You"
        ],
        "summary": "We introduce a method of verifying termination of logic programs with respect to concrete queries (instead of abstract query patterns). A necessary and sufficient condition is established and an algorithm for automatic verification is developed. In contrast to existing query pattern-based approaches, our method has the following features: (1) It applies to all general logic programs with non-floundering queries. (2) It is very easy to automate because it does not need to search for a level mapping or a model, nor does it need to compute an interargument relation based on additional mode or type information. (3) It bridges termination analysis with loop checking, the two problems that have been studied separately in the past despite their close technical relation with each other.",
        "published": "2000-06-21T17:30:30Z",
        "link": "http://arxiv.org/abs/cs/0006031v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "D.3.1; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Using a Diathesis Model for Semantic Parsing",
        "authors": [
            "Jordi Atserias",
            "Irene Castellon",
            "Montse Civit",
            "German Rigau"
        ],
        "summary": "This paper presents a semantic parsing approach for unrestricted texts. Semantic parsing is one of the major bottlenecks of Natural Language Understanding (NLU) systems and usually requires building expensive resources not easily portable to other domains. Our approach obtains a case-role analysis, in which the semantic roles of the verb are identified. In order to cover all the possible syntactic realisations of a verb, our system combines their argument structure with a set of general semantic labelled diatheses models. Combining them, the system builds a set of syntactic-semantic patterns with their own role-case representation. Once the patterns are build, we use an approximate tree pattern-matching algorithm to identify the most reliable pattern for a sentence. The pattern matching is performed between the syntactic-semantic patterns and the feature-structure tree representing the morphological, syntactical and semantic information of the analysed sentence. For sentences assigned to the correct model, the semantic parsing system we are presenting identifies correctly more than 73% of possible semantic case-roles.",
        "published": "2000-06-29T07:44:16Z",
        "link": "http://arxiv.org/abs/cs/0006041v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7;I.5"
        ]
    },
    {
        "title": "Semantic Parsing based on Verbal Subcategorization",
        "authors": [
            "Jordi Atserias",
            "Irene Castellon",
            "Montse Civit",
            "German Rigau"
        ],
        "summary": "The aim of this work is to explore new methodologies on Semantic Parsing for unrestricted texts. Our approach follows the current trends in Information Extraction (IE) and is based on the application of a verbal subcategorization lexicon (LEXPIR) by means of complex pattern recognition techniques. LEXPIR is framed on the theoretical model of the verbal subcategorization developed in the Pirapides project.",
        "published": "2000-06-29T09:17:45Z",
        "link": "http://arxiv.org/abs/cs/0006042v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7;I.5"
        ]
    },
    {
        "title": "Constraint compiling into rules formalism constraint compiling into   rules formalism for dynamic CSPs computing",
        "authors": [
            "S. Piechowiak",
            "J. Rodriguez"
        ],
        "summary": "In this paper we present a rule based formalism for filtering variables domains of constraints. This formalism is well adapted for solving dynamic CSP. We take diagnosis as an instance problem to illustrate the use of these rules. A diagnosis problem is seen like finding all the minimal sets of constraints to be relaxed in the constraint network that models the device to be diagnosed",
        "published": "2000-06-30T10:25:06Z",
        "link": "http://arxiv.org/abs/cs/0006043v1",
        "categories": [
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Constraint Exploration and Envelope of Simulation Trajectories",
        "authors": [
            "Oswaldo Teran",
            "Bruce Edmonds",
            "Steve Wallis"
        ],
        "summary": "The implicit theory that a simulation represents is precisely not in the individual choices but rather in the 'envelope' of possible trajectories - what is important is the shape of the whole envelope. Typically a huge amount of computation is required when experimenting with factors bearing on the dynamics of a simulation to tease out what affects the shape of this envelope. In this paper we present a methodology aimed at systematically exploring this envelope. We propose a method for searching for tendencies and proving their necessity relative to a range of parameterisations of the model and agents' choices, and to the logic of the simulation language. The exploration consists of a forward chaining generation of the trajectories associated to and constrained by such a range of parameterisations and choices. Additionally, we propose a computational procedure that helps implement this exploration by translating a Multi Agent System simulation into a constraint-based search over possible trajectories by 'compiling' the simulation rules into a more specific form, namely by partitioning the simulation rules using appropriate modularity in the simulation. An example of this procedure is exhibited.   Keywords: Constraint Search, Constraint Logic Programming, Proof, Emergence, Tendencies",
        "published": "2000-07-03T10:10:09Z",
        "link": "http://arxiv.org/abs/cs/0007001v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.LO",
            "D.3.3; F.3.1; F.4.1"
        ]
    },
    {
        "title": "Interval Constraint Solving for Camera Control and Motion Planning",
        "authors": [
            "Frederic Benhamou",
            "Frederic Goualard",
            "Eric Languenou",
            "Marc Christie"
        ],
        "summary": "Many problems in robust control and motion planning can be reduced to either find a sound approximation of the solution space determined by a set of nonlinear inequalities, or to the ``guaranteed tuning problem'' as defined by Jaulin and Walter, which amounts to finding a value for some tuning parameter such that a set of inequalities be verified for all the possible values of some perturbation vector. A classical approach to solve these problems, which satisfies the strong soundness requirement, involves some quantifier elimination procedure such as Collins' Cylindrical Algebraic Decomposition symbolic method. Sound numerical methods using interval arithmetic and local consistency enforcement to prune the search space are presented in this paper as much faster alternatives for both soundly solving systems of nonlinear inequalities, and addressing the guaranteed tuning problem whenever the perturbation vector has dimension one. The use of these methods in camera control is investigated, and experiments with the prototype of a declarative modeller to express camera motion using a cinematic language are reported and commented.",
        "published": "2000-07-03T17:03:39Z",
        "link": "http://arxiv.org/abs/cs/0007002v2",
        "categories": [
            "cs.AI",
            "cs.NA",
            "math.NA",
            "D.3.3;D.2.2;G.1.0;H.5.1"
        ]
    },
    {
        "title": "Brainstorm/J: a Java Framework for Intelligent Agents",
        "authors": [
            "Alejandro Zunino",
            "Analia Amandi"
        ],
        "summary": "Despite the effort of many researchers in the area of multi-agent systems (MAS) for designing and programming agents, a few years ago the research community began to take into account that common features among different MAS exists. Based on these common features, several tools have tackled the problem of agent development on specific application domains or specific types of agents. As a consequence, their scope is restricted to a subset of the huge application domain of MAS. In this paper we propose a generic infrastructure for programming agents whose name is Brainstorm/J. The infrastructure has been implemented as an object oriented framework. As a consequence, our approach supports a broader scope of MAS applications than previous efforts, being flexible and reusable.",
        "published": "2000-07-04T16:31:40Z",
        "link": "http://arxiv.org/abs/cs/0007004v1",
        "categories": [
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "Boosting Applied to Word Sense Disambiguation",
        "authors": [
            "Gerard Escudero",
            "Lluis Marquez",
            "German Rigau"
        ],
        "summary": "In this paper Schapire and Singer's AdaBoost.MH boosting algorithm is applied to the Word Sense Disambiguation (WSD) problem. Initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses Naive Bayes and Exemplar-based approaches, which represent state-of-the-art accuracy on supervised WSD. In order to make boosting practical for a real learning domain of thousands of words, several ways of accelerating the algorithm by reducing the feature space are studied. The best variant, which we call LazyBoosting, is tested on the largest sense-tagged corpus available containing 192,800 examples of the 191 most frequent and ambiguous English words. Again, boosting compares favourably to the other benchmark algorithms.",
        "published": "2000-07-07T14:10:05Z",
        "link": "http://arxiv.org/abs/cs/0007010v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7;I.2.6"
        ]
    },
    {
        "title": "Naive Bayes and Exemplar-Based approaches to Word Sense Disambiguation   Revisited",
        "authors": [
            "Gerard Escudero",
            "Lluis Marquez",
            "German Rigau"
        ],
        "summary": "This paper describes an experimental comparison between two standard supervised learning methods, namely Naive Bayes and Exemplar-based classification, on the Word Sense Disambiguation (WSD) problem. The aim of the work is twofold. Firstly, it attempts to contribute to clarify some confusing information about the comparison between both methods appearing in the related literature. In doing so, several directions have been explored, including: testing several modifications of the basic learning algorithms and varying the feature space. Secondly, an improvement of both algorithms is proposed, in order to deal with large attribute sets. This modification, which basically consists in using only the positive information appearing in the examples, allows to improve greatly the efficiency of the methods, with no loss in accuracy. The experiments have been performed on the largest sense-tagged corpus available containing the most frequent and ambiguous English words. Results show that the Exemplar-based approach to WSD is generally superior to the Bayesian approach, especially when a specific metric for dealing with symbolic attributes is used.",
        "published": "2000-07-07T15:00:44Z",
        "link": "http://arxiv.org/abs/cs/0007011v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7;I.2.6"
        ]
    },
    {
        "title": "Using Learning-based Filters to Detect Rule-based Filtering Obsolescence",
        "authors": [
            "Francis Wolinski",
            "Frantz Vichot",
            "Mathieu Stricker"
        ],
        "summary": "For years, Caisse des Depots et Consignations has produced information filtering applications. To be operational, these applications require high filtering performances which are achieved by using rule-based filters. With this technique, an administrator has to tune a set of rules for each topic. However, filters become obsolescent over time. The decrease of their performances is due to diachronic polysemy of terms that involves a loss of precision and to diachronic polymorphism of concepts that involves a loss of recall.   To help the administrator to maintain his filters, we have developed a method which automatically detects filtering obsolescence. It consists in making a learning-based control filter using a set of documents which have already been categorised as relevant or not relevant by the rule-based filter. The idea is to supervise this filter by processing a differential comparison of its outcomes with those of the control one.   This method has many advantages. It is simple to implement since the training set used by the learning is supplied by the rule-based filter. Thus, both the making and the use of the control filter are fully automatic. With automatic detection of obsolescence, learning-based filtering finds a rich application which offers interesting prospects.",
        "published": "2000-07-07T15:13:09Z",
        "link": "http://arxiv.org/abs/cs/0007012v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "H.3.3; I.2.6"
        ]
    },
    {
        "title": "Two Steps Feature Selection and Neural Network Classification for the   TREC-8 Routing",
        "authors": [
            "Mathieu Stricker",
            "Frantz Vichot",
            "Gerard Dreyfus",
            "Francis Wolinski"
        ],
        "summary": "For the TREC-8 routing, one specific filter is built for each topic. Each filter is a classifier trained to recognize the documents that are relevant to the topic. When presented with a document, each classifier estimates the probability for the document to be relevant to the topic for which it has been trained. Since the procedure for building a filter is topic-independent, the system is fully automatic.   By making use of a sample of documents that have previously been evaluated as relevant or not relevant to a particular topic, a term selection is performed, and a neural network is trained. Each document is represented by a vector of frequencies of a list of selected terms. This list depends on the topic to be filtered; it is constructed in two steps. The first step defines the characteristic words used in the relevant documents of the corpus; the second one chooses, among the previous list, the most discriminant ones. The length of the vector is optimized automatically for each topic. At the end of the term selection, a vector of typically 25 words is defined for the topic, so that each document which has to be processed is represented by a vector of term frequencies.   This vector is subsequently input to a classifier that is trained from the same sample. After training, the classifier estimates for each document of a test set its probability of being relevant; for submission to TREC, the top 1000 documents are ranked in order of decreasing relevance.",
        "published": "2000-07-11T13:21:03Z",
        "link": "http://arxiv.org/abs/cs/0007016v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "H.3.3; K.3.2"
        ]
    },
    {
        "title": "Polynomial-time Computation via Local Inference Relations",
        "authors": [
            "Robert Givan",
            "David McAllester"
        ],
        "summary": "We consider the concept of a local set of inference rules. A local rule set can be automatically transformed into a rule set for which bottom-up evaluation terminates in polynomial time. The local-rule-set transformation gives polynomial-time evaluation strategies for a large variety of rule sets that cannot be given terminating evaluation strategies by any other known automatic technique. This paper discusses three new results. First, it is shown that every polynomial-time predicate can be defined by an (unstratified) local rule set. Second, a new machine-recognizable subclass of the local rule sets is identified. Finally we show that locality, as a property of rule sets, is undecidable in general.",
        "published": "2000-07-13T17:19:43Z",
        "link": "http://arxiv.org/abs/cs/0007020v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL",
            "I.2.2; I.2.3; I.2.4; F.4.m"
        ]
    },
    {
        "title": "Integrating E-Commerce and Data Mining: Architecture and Challenges",
        "authors": [
            "Suhail Ansari",
            "Ron Kohavi",
            "Llew Mason",
            "Zijian Zheng"
        ],
        "summary": "We show that the e-commerce domain can provide all the right ingredients for successful data mining and claim that it is a killer domain for data mining. We describe an integrated architecture, based on our expe-rience at Blue Martini Software, for supporting this integration. The architecture can dramatically reduce the pre-processing, cleaning, and data understanding effort often documented to take 80% of the time in knowledge discovery projects. We emphasize the need for data collection at the application server layer (not the web server) in order to support logging of data and metadata that is essential to the discovery process. We describe the data transformation bridges required from the transaction processing systems and customer event streams (e.g., clickstreams) to the data warehouse. We detail the mining workbench, which needs to provide multiple views of the data through reporting, data mining algorithms, visualization, and OLAP. We con-clude with a set of challenges.",
        "published": "2000-07-14T00:33:12Z",
        "link": "http://arxiv.org/abs/cs/0007026v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DB",
            "I.2.6;H.2.8"
        ]
    },
    {
        "title": "Knowledge on Treelike Spaces",
        "authors": [
            "Konstantinos Georgatos"
        ],
        "summary": "This paper presents a bimodal logic for reasoning about knowledge during knowledge acquisition. One of the modalities represents (effort during) non-deterministic time and the other represents knowledge. The semantics of this logic are tree-like spaces which are a generalization of semantics used for modeling branching time and historical necessity. A finite system of axiom schemes is shown to be canonically complete for the formentioned spaces. A characterization of the satisfaction relation implies the small model property and decidability for this system.",
        "published": "2000-07-21T09:52:25Z",
        "link": "http://arxiv.org/abs/cs/0007032v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.0"
        ]
    },
    {
        "title": "To Preference via Entrenchment",
        "authors": [
            "Konstantinos Georgatos"
        ],
        "summary": "We introduce a simple generalization of Gardenfors and Makinson's epistemic entrenchment called partial entrenchment. We show that preferential inference can be generated as the sceptical counterpart of an inference mechanism defined directly on partial entrenchment.",
        "published": "2000-07-21T10:16:47Z",
        "link": "http://arxiv.org/abs/cs/0007033v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3"
        ]
    },
    {
        "title": "Modal Logics for Topological Spaces",
        "authors": [
            "Konstantinos Georgatos"
        ],
        "summary": "In this thesis we shall present two logical systems, MP and MP, for the purpose of reasoning about knowledge and effort. These logical systems will be interpreted in a spatial context and therefore, the abstract concepts of knowledge and effort will be defined by concrete mathematical concepts.",
        "published": "2000-07-26T18:41:17Z",
        "link": "http://arxiv.org/abs/cs/0007038v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Ordering-based Representations of Rational Inference",
        "authors": [
            "Konstantinos Georgatos"
        ],
        "summary": "Rational inference relations were introduced by Lehmann and Magidor as the ideal systems for drawing conclusions from a conditional base. However, there has been no simple characterization of these relations, other than its original representation by preferential models. In this paper, we shall characterize them with a class of total preorders of formulas by improving and extending Gardenfors and Makinson's results for expectation inference relations. A second representation is application-oriented and is obtained by considering a class of consequence operators that grade sets of defaults according to our reliance on them. The finitary fragment of this class of consequence operators has been employed by recent default logic formalisms based on maxiconsistency.",
        "published": "2000-07-26T18:58:09Z",
        "link": "http://arxiv.org/abs/cs/0007039v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3"
        ]
    },
    {
        "title": "Entrenchment Relations: A Uniform Approach to Nonmonotonicity",
        "authors": [
            "Konstantinos Georgatos"
        ],
        "summary": "We show that Gabbay's nonmonotonic consequence relations can be reduced to a new family of relations, called entrenchment relations. Entrenchment relations provide a direct generalization of epistemic entrenchment and expectation ordering introduced by Gardenfors and Makinson for the study of belief revision and expectation inference, respectively.",
        "published": "2000-07-26T19:20:35Z",
        "link": "http://arxiv.org/abs/cs/0007040v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3"
        ]
    },
    {
        "title": "On the Average Similarity Degree between Solutions of Random k-SAT and   Random CSPs",
        "authors": [
            "Ke Xu",
            "Wei Li"
        ],
        "summary": "To study the structure of solutions for random k-SAT and random CSPs, this paper introduces the concept of average similarity degree to characterize how solutions are similar to each other. It is proved that under certain conditions, as r (i.e. the ratio of constraints to variables) increases, the limit of average similarity degree when the number of variables approaches infinity exhibits phase transitions at a threshold point, shifting from a smaller value to a larger value abruptly. For random k-SAT this phenomenon will occur when k>4 . It is further shown that this threshold point is also a singular point with respect to r in the asymptotic estimate of the second moment of the number of solutions. Finally, we discuss how this work is helpful to understand the hardness of solving random instances and a possible application of it to the design of search algorithms.",
        "published": "2000-08-11T08:10:25Z",
        "link": "http://arxiv.org/abs/cs/0008008v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DM",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "Processing Self Corrections in a speech to speech system",
        "authors": [
            "Joerg Spilker",
            "Martin Klarner",
            "Guenther Goerz"
        ],
        "summary": "Speech repairs occur often in spontaneous spoken dialogues. The ability to detect and correct those repairs is necessary for any spoken language system. We present a framework to detect and correct speech repairs where all relevant levels of information, i.e., acoustics, lexis, syntax and semantics can be integrated. The basic idea is to reduce the search space for repairs as soon as possible by cascading filters that involve more and more features. At first an acoustic module generates hypotheses about the existence of a repair. Second a stochastic model suggests a correction for every hypothesis. Well scored corrections are inserted as new paths in the word lattice. Finally a lattice parser decides on accepting the rep air.",
        "published": "2000-08-21T10:54:11Z",
        "link": "http://arxiv.org/abs/cs/0008016v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I 2.7"
        ]
    },
    {
        "title": "Explaining away ambiguity: Learning verb selectional preference with   Bayesian networks",
        "authors": [
            "Massimiliano Ciaramita",
            "Mark Johnson"
        ],
        "summary": "This paper presents a Bayesian model for unsupervised learning of verb selectional preferences. For each verb the model creates a Bayesian network whose architecture is determined by the lexical hierarchy of Wordnet and whose parameters are estimated from a list of verb-object pairs found from a corpus. ``Explaining away'', a well-known property of Bayesian networks, helps the model deal in a natural fashion with word sense ambiguity in the training data. On a word sense disambiguation test our model performed better than other state of the art systems for unsupervised learning of selectional preferences. Computational complexity problems, ways of improving this approach and methods for implementing ``explaining away'' in other graphical frameworks are discussed.",
        "published": "2000-08-22T15:01:21Z",
        "link": "http://arxiv.org/abs/cs/0008020v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Modeling Ambiguity in a Multi-Agent System",
        "authors": [
            "Christof Monz"
        ],
        "summary": "This paper investigates the formal pragmatics of ambiguous expressions by modeling ambiguity in a multi-agent system. Such a framework allows us to give a more refined notion of the kind of information that is conveyed by ambiguous expressions. We analyze how ambiguity affects the knowledge of the dialog participants and, especially, what they know about each other after an ambiguous sentence has been uttered. The agents communicate with each other by means of a TELL-function, whose application is constrained by an implementation of some of Grice's maxims. The information states of the multi-agent system itself are represented as a Kripke structures and TELL is an update function on those structures. This framework enables us to distinguish between the information conveyed by ambiguous sentences vs. the information conveyed by disjunctions, and between semantic ambiguity vs. perceived ambiguity.",
        "published": "2000-09-19T15:43:18Z",
        "link": "http://arxiv.org/abs/cs/0009012v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.MA",
            "F.4.1; I.2.7"
        ]
    },
    {
        "title": "Contextual Inference in Computational Semantics",
        "authors": [
            "Christof Monz"
        ],
        "summary": "In this paper, an application of automated theorem proving techniques to computational semantics is considered. In order to compute the presuppositions of a natural language discourse, several inference tasks arise. Instead of treating these inferences independently of each other, we show how integrating techniques from formal approaches to context into deduction can help to compute presuppositions more efficiently. Contexts are represented as Discourse Representation Structures and the way they are nested is made explicit. In addition, a tableau calculus is present which keeps track of contextual information, and thereby allows to avoid carrying out redundant inference steps as it happens in approaches that neglect explicit nesting of contexts.",
        "published": "2000-09-20T13:41:06Z",
        "link": "http://arxiv.org/abs/cs/0009016v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "F.4.1; I.2.7"
        ]
    },
    {
        "title": "A Tableau Calculus for Pronoun Resolution",
        "authors": [
            "Christof Monz",
            "Maarten de Rijke"
        ],
        "summary": "We present a tableau calculus for reasoning in fragments of natural language. We focus on the problem of pronoun resolution and the way in which it complicates automated theorem proving for natural language processing. A method for explicitly manipulating contextual information during deduction is proposed, where pronouns are resolved against this context during deduction. As a result, pronoun resolution and deduction can be interleaved in such a way that pronouns are only resolved if this is licensed by a deduction rule; this helps us to avoid the combinatorial complexity of total pronoun disambiguation.",
        "published": "2000-09-21T14:49:19Z",
        "link": "http://arxiv.org/abs/cs/0009017v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "F.4.1; I.2.7"
        ]
    },
    {
        "title": "A Resolution Calculus for Dynamic Semantics",
        "authors": [
            "Christof Monz",
            "Maarten de Rijke"
        ],
        "summary": "This paper applies resolution theorem proving to natural language semantics. The aim is to circumvent the computational complexity triggered by natural language ambiguities like pronoun binding, by interleaving pronoun binding with resolution deduction. Therefore disambiguation is only applied to expression that actually occur during derivations.",
        "published": "2000-09-21T15:21:01Z",
        "link": "http://arxiv.org/abs/cs/0009018v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "F.4.1; I.2.7"
        ]
    },
    {
        "title": "Computing Presuppositions by Contextual Reasoning",
        "authors": [
            "Christof Monz"
        ],
        "summary": "This paper describes how automated deduction methods for natural language processing can be applied more efficiently by encoding context in a more elaborate way. Our work is based on formal approaches to context, and we provide a tableau calculus for contextual reasoning. This is explained by considering an example from the problem area of presupposition projection.",
        "published": "2000-09-21T15:32:17Z",
        "link": "http://arxiv.org/abs/cs/0009019v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "F.4.1; I.2.7"
        ]
    },
    {
        "title": "A Comparison between Supervised Learning Algorithms for Word Sense   Disambiguation",
        "authors": [
            "Gerard Escudero",
            "Lluis Marquez",
            "German Rigau"
        ],
        "summary": "This paper describes a set of comparative experiments, including cross-corpus evaluation, between five alternative algorithms for supervised Word Sense Disambiguation (WSD), namely Naive Bayes, Exemplar-based learning, SNoW, Decision Lists, and Boosting. Two main conclusions can be drawn: 1) The LazyBoosting algorithm outperforms the other four state-of-the-art algorithms in terms of accuracy and ability to tune to new domains; 2) The domain dependence of WSD systems seems very strong and suggests that some kind of adaptation or tuning is required for cross-corpus application.",
        "published": "2000-09-22T15:02:26Z",
        "link": "http://arxiv.org/abs/cs/0009022v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7;I.2.6"
        ]
    },
    {
        "title": "A Classification Approach to Word Prediction",
        "authors": [
            "Yair Even-Zohar",
            "Dan Roth"
        ],
        "summary": "The eventual goal of a language model is to accurately predict the value of a missing word given its context. We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context. This approach raises a few new questions that we address. First, in order to learn good word representations it is necessary to use an expressive representation of the context. We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction. Second, since the number of words ``competing'' for each prediction is large, there is a need to ``focus the attention'' on a smaller subset of these. We exhibit the contribution of a ``focus of attention'' mechanism to the performance of the word predictor. Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks.",
        "published": "2000-09-28T14:25:51Z",
        "link": "http://arxiv.org/abs/cs/0009027v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "I.2.6;I.2.7"
        ]
    },
    {
        "title": "Noise-Tolerant Learning, the Parity Problem, and the Statistical Query   Model",
        "authors": [
            "Avrim Blum",
            "Adam Kalai",
            "Hal Wasserman"
        ],
        "summary": "We describe a slightly sub-exponential time algorithm for learning parity functions in the presence of random classification noise. This results in a polynomial-time algorithm for the case of parity functions that depend on only the first O(log n log log n) bits of input. This is the first known instance of an efficient noise-tolerant algorithm for a concept class that is provably not learnable in the Statistical Query model of Kearns. Thus, we demonstrate that the set of problems learnable in the statistical query model is a strict subset of those problems learnable in the presence of noise in the PAC model.   In coding-theory terms, what we give is a poly(n)-time algorithm for decoding linear k by n codes in the presence of random noise for the case of k = c log n loglog n for some c > 0. (The case of k = O(log n) is trivial since one can just individually check each of the 2^k possible messages and choose the one that yields the closest codeword.)   A natural extension of the statistical query model is to allow queries about statistical properties that involve t-tuples of examples (as opposed to single examples). The second result of this paper is to show that any class of functions learnable (strongly or weakly) with t-wise queries for t = O(log n) is also weakly learnable with standard unary queries. Hence this natural extension to the statistical query model does not increase the set of weakly learnable functions.",
        "published": "2000-10-15T20:14:08Z",
        "link": "http://arxiv.org/abs/cs/0010022v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DS",
            "I.2.6"
        ]
    },
    {
        "title": "Oracle Complexity and Nontransitivity in Pattern Recognition",
        "authors": [
            "Vadim Bulitko"
        ],
        "summary": "Different mathematical models of recognition processes are known. In the present paper we consider a pattern recognition algorithm as an oracle computation on a Turing machine. Such point of view seems to be useful in pattern recognition as well as in recursion theory. Use of recursion theory in pattern recognition shows connection between a recognition algorithm comparison problem and complexity problems of oracle computation. That is because in many cases we can take into account only the number of sign computations or in other words volume of oracle information needed. Therefore, the problem of recognition algorithm preference can be formulated as a complexity optimization problem of oracle computation. Furthermore, introducing a certain \"natural\" preference relation on a set of recognizing algorithms, we discover it to be nontransitive. This relates to the well known nontransitivity paradox in probability theory.   Keywords: Pattern Recognition, Recursion Theory, Nontransitivity, Preference Relation",
        "published": "2000-10-16T07:42:23Z",
        "link": "http://arxiv.org/abs/cs/0010023v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "cs.CV",
            "cs.DS",
            "F.2.2; F.4.1; I.2.10; I.5.2"
        ]
    },
    {
        "title": "Super Logic Programs",
        "authors": [
            "Stefan Brass",
            "Juergen Dix",
            "Teodor C. Przymusinski"
        ],
        "summary": "The Autoepistemic Logic of Knowledge and Belief (AELB) is a powerful nonmonotic formalism introduced by Teodor Przymusinski in 1994. In this paper, we specialize it to a class of theories called `super logic programs'. We argue that these programs form a natural generalization of standard logic programs. In particular, they allow disjunctions and default negation of arbibrary positive objective formulas.   Our main results are two new and powerful characterizations of the static semant ics of these programs, one syntactic, and one model-theoretic. The syntactic fixed point characterization is much simpler than the fixed point construction of the static semantics for arbitrary AELB theories. The model-theoretic characterization via Kripke models allows one to construct finite representations of the inherently infinite static expansions.   Both characterizations can be used as the basis of algorithms for query answering under the static semantics. We describe a query-answering interpreter for super programs which we developed based on the model-theoretic characterization and which is available on the web.",
        "published": "2000-10-25T13:32:51Z",
        "link": "http://arxiv.org/abs/cs/0010032v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "On the relationship between fuzzy logic and four-valued relevance logic",
        "authors": [
            "Umberto Straccia"
        ],
        "summary": "In fuzzy propositional logic, to a proposition a partial truth in [0,1] is assigned. It is well known that under certain circumstances, fuzzy logic collapses to classical logic. In this paper, we will show that under dual conditions, fuzzy logic collapses to four-valued (relevance) logic, where propositions have truth-value true, false, unknown, or contradiction. As a consequence, fuzzy entailment may be considered as ``in between'' four-valued (relevance) entailment and classical entailment.",
        "published": "2000-10-31T14:14:26Z",
        "link": "http://arxiv.org/abs/cs/0010037v1",
        "categories": [
            "cs.AI",
            "F.4.1;I.2.3;I.2.4"
        ]
    },
    {
        "title": "Tree-gram Parsing: Lexical Dependencies and Structural Relations",
        "authors": [
            "Khalil Sima'an"
        ],
        "summary": "This paper explores the kinds of probabilistic relations that are important in syntactic disambiguation. It proposes that two widely used kinds of relations, lexical dependencies and structural relations, have complementary disambiguation capabilities. It presents a new model based on structural relations, the Tree-gram model, and reports experiments showing that structural relations should benefit from enrichment by lexical dependencies.",
        "published": "2000-11-06T13:56:42Z",
        "link": "http://arxiv.org/abs/cs/0011007v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "I.2; K.3.2; J.5"
        ]
    },
    {
        "title": "A Lambda-Calculus with letrec, case, constructors and non-determinism",
        "authors": [
            "Manfred Schmidt-Schauß",
            "Michael Huber"
        ],
        "summary": "A non-deterministic call-by-need lambda-calculus \\calc with case, constructors, letrec and a (non-deterministic) erratic choice, based on rewriting rules is investigated. A standard reduction is defined as a variant of left-most outermost reduction. The semantics is defined by contextual equivalence of expressions instead of using $\\alpha\\beta(\\eta)$-equivalence. It is shown that several program transformations are correct, for example all (deterministic) rules of the calculus, and in addition the rules for garbage collection, removing indirections and unique copy.   This shows that the combination of a context lemma and a meta-rewriting on reductions using complete sets of commuting (forking, resp.) diagrams is a useful and successful method for providing a semantics of a functional programming language and proving correctness of program transformations.",
        "published": "2000-11-06T14:52:33Z",
        "link": "http://arxiv.org/abs/cs/0011008v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.SC",
            "F.4.1;D.3.2;I.2.2"
        ]
    },
    {
        "title": "Causes and Explanations: A Structural-Model Approach, Part I: Causes",
        "authors": [
            "Joseph Y. Halpern",
            "Judea Pearl"
        ],
        "summary": "We propose a new definition of actual cause, using structural equations to model counterfactuals. We show that the definition yields a plausible and elegant account of causation that handles well examples which have caused problems for other definitions and resolves major difficulties in the traditional account.",
        "published": "2000-11-07T23:21:38Z",
        "link": "http://arxiv.org/abs/cs/0011012v3",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Logic Programming Approaches for Representing and Solving Constraint   Satisfaction Problems: A Comparison",
        "authors": [
            "Nikolay Pelov",
            "Emmanuel De Mot",
            "Marc Denecker"
        ],
        "summary": "Many logic programming based approaches can be used to describe and solve combinatorial search problems. On the one hand there is constraint logic programming which computes a solution as an answer substitution to a query containing the variables of the constraint satisfaction problem. On the other hand there are systems based on stable model semantics, abductive systems, and first order logic model generators which compute solutions as models of some theory. This paper compares these different approaches from the point of view of knowledge representation (how declarative are the programs) and from the point of view of performance (how good are they at solving typical problems).",
        "published": "2000-11-21T13:56:21Z",
        "link": "http://arxiv.org/abs/cs/0011030v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Order-consistent programs are cautiously monotonic",
        "authors": [
            "Hudson Turner"
        ],
        "summary": "Some normal logic programs under the answer set (stable model) semantics lack the appealing property of \"cautious monotonicity.\" That is, augmenting a program with one of its consequences may cause it to lose another of its consequences. The syntactic condition of \"order-consistency\" was shown by Fages to guarantee existence of an answer set. This note establishes that order-consistent programs are not only consistent, but cautiously monotonic.   From this it follows that they are also \"cumulative.\" That is, augmenting an order-consistent with some of its consequences does not alter its consequences. In fact, as we show, its answer sets remain unchanged.",
        "published": "2000-11-27T19:21:10Z",
        "link": "http://arxiv.org/abs/cs/0011042v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3"
        ]
    },
    {
        "title": "Algorithmic Theories of Everything",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "The probability distribution P from which the history of our universe is sampled represents a theory of everything or TOE. We assume P is formally describable. Since most (uncountably many) distributions are not, this imposes a strong inductive bias. We show that P(x) is small for any universe x lacking a short description, and study the spectrum of TOEs spanned by two Ps, one reflecting the most compact constructive descriptions, the other the fastest way of computing everything. The former derives from generalizations of traditional computability, Solomonoff's algorithmic probability, Kolmogorov complexity, and objects more random than Chaitin's Omega, the latter from Levin's universal search and a natural resource-oriented postulate: the cumulative prior probability of all x incomputable within time t by this optimal algorithm should be 1/t. Between both Ps we find a universal cumulatively enumerable measure that dominates traditional enumerable measures; any such CEM must assign low probability to any universe lacking a short enumerating program. We derive P-specific consequences for evolving observers, inductive reasoning, quantum physics, philosophy, and the expected duration of our universe.",
        "published": "2000-11-30T14:23:55Z",
        "link": "http://arxiv.org/abs/quant-ph/0011122v2",
        "categories": [
            "quant-ph",
            "cs.AI",
            "cs.CC",
            "cs.LG",
            "hep-th",
            "math-ph",
            "math.MP",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Improving Performance of heavily loaded agents",
        "authors": [
            "Fatma Ozcan",
            "VS Subrahmanian",
            "Juergen Dix"
        ],
        "summary": "With the increase in agent-based applications, there are now agent systems that support \\emph{concurrent} client accesses. The ability to process large volumes of simultaneous requests is critical in many such applications. In such a setting, the traditional approach of serving these requests one at a time via queues (e.g. \\textsf{FIFO} queues, priority queues) is insufficient. Alternative models are essential to improve the performance of such \\emph{heavily loaded} agents. In this paper, we propose a set of \\emph{cost-based algorithms} to \\emph{optimize} and \\emph{merge} multiple requests submitted to an agent. In order to merge a set of requests, one first needs to identify commonalities among such requests. First, we provide an \\emph{application independent framework} within which an agent developer may specify relationships (called \\emph{invariants}) between requests. Second, we provide two algorithms (and various accompanying heuristics) which allow an agent to automatically rewrite requests so as to avoid redundant work---these algorithms take invariants associated with the agent into account. Our algorithms are independent of any specific agent framework. For an implementation, we implemented both these algorithms on top of the \\impact agent development platform, and on top of a (non-\\impact) geographic database agent. Based on these implementations, we conducted experiments and show that our algorithms are considerably more efficient than methods that use the $A^*$ algorithm.",
        "published": "2000-12-11T10:17:36Z",
        "link": "http://arxiv.org/abs/cs/0012004v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.12;I.2.3;D.2.12;H.2.4"
        ]
    },
    {
        "title": "The Role of Commutativity in Constraint Propagation Algorithms",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "Constraint propagation algorithms form an important part of most of the constraint programming systems. We provide here a simple, yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way. In this framework we proceed in two steps. First, we introduce a generic iteration algorithm on partial orderings and prove its correctness in an abstract setting. Then we instantiate this algorithm with specific partial orderings and functions to obtain specific constraint propagation algorithms.   In particular, using the notions commutativity and semi-commutativity, we show that the {\\tt AC-3}, {\\tt PC-2}, {\\tt DAC} and {\\tt DPC} algorithms for achieving (directional) arc consistency and (directional) path consistency are instances of a single generic algorithm. The work reported here extends and simplifies that of Apt \\citeyear{Apt99b}.",
        "published": "2000-12-15T14:04:28Z",
        "link": "http://arxiv.org/abs/cs/0012010v1",
        "categories": [
            "cs.PF",
            "cs.AI",
            "D.3.3;I.1.2;I.1.3"
        ]
    },
    {
        "title": "Towards a Universal Theory of Artificial Intelligence based on   Algorithmic Probability and Sequential Decision Theory",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown distribution. We unify both theories and give strong arguments that the resulting universal AIXI model behaves optimal in any computable environment. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXI^tl, which is still superior to any other time t and space l bounded agent. The computation time of AIXI^tl is of the order t x 2^l.",
        "published": "2000-12-16T09:38:13Z",
        "link": "http://arxiv.org/abs/cs/0012011v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "I.2; I.2.3; I.2.6; I.2.8; F.1.3; F.2"
        ]
    },
    {
        "title": "Creativity and Delusions: A Neurocomputational Approach",
        "authors": [
            "Daniele Quintella Mendes",
            "Luis Alfredo Vidal de Carvalho"
        ],
        "summary": "Thinking is one of the most interesting mental processes. Its complexity is sometimes simplified and its different manifestations are classified into normal and abnormal, like the delusional and disorganized thought or the creative one. The boundaries between these facets of thinking are fuzzy causing difficulties in medical, academic, and philosophical discussions. Considering the dopaminergic signal-to-noise neuronal modulation in the central nervous system, and the existence of semantic maps in human brain, a self-organizing neural network model was developed to unify the different thought processes into a single neurocomputational substrate. Simulations were performed varying the dopaminergic modulation and observing the different patterns that emerged at the semantic map. Assuming that the thought process is the total pattern elicited at the output layer of the neural network, the model shows how the normal and abnormal thinking are generated and that there are no borders between their different manifestations. Actually, a continuum of different qualitative reasoning, ranging from delusion to disorganization of thought, and passing through the normal and the creative thinking, seems to be more plausible. The model is far from explaining the complexities of human thinking but, at least, it seems to be a good metaphorical and unifying view of the many facets of this phenomenon usually studied in separated settings.",
        "published": "2000-12-22T12:00:07Z",
        "link": "http://arxiv.org/abs/cs/0012020v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.5.1"
        ]
    },
    {
        "title": "Multiplicative Algorithm for Orthgonal Groups and Independent Component   Analysis",
        "authors": [
            "Toshinao Akuzawa"
        ],
        "summary": "The multiplicative Newton-like method developed by the author et al. is extended to the situation where the dynamics is restricted to the orthogonal group. A general framework is constructed without specifying the cost function. Though the restriction to the orthogonal groups makes the problem somewhat complicated, an explicit expression for the amount of individual jumps is obtained. This algorithm is exactly second-order-convergent. The global instability inherent in the Newton method is remedied by a Levenberg-Marquardt-type variation. The method thus constructed can readily be applied to the independent component analysis. Its remarkable performance is illustrated by a numerical simulation.",
        "published": "2000-01-07T06:20:53Z",
        "link": "http://arxiv.org/abs/cs/0001004v1",
        "categories": [
            "cs.LG",
            "G.1.6"
        ]
    },
    {
        "title": "Predicting the expected behavior of agents that learn about agents: the   CLRI framework",
        "authors": [
            "Jose M. Vidal",
            "Edmund H. Durfee"
        ],
        "summary": "We describe a framework and equations used to model and predict the behavior of multi-agent systems (MASs) with learning agents. A difference equation is used for calculating the progression of an agent's error in its decision function, thereby telling us how the agent is expected to fare in the MAS. The equation relies on parameters which capture the agent's learning abilities, such as its change rate, learning rate and retention rate, as well as relevant aspects of the MAS such as the impact that agents have on each other. We validate the framework with experimental results using reinforcement learning agents in a market system, as well as with other experimental results gathered from the AI literature. Finally, we use PAC-theory to show how to calculate bounds on the values of the learning parameters.",
        "published": "2000-01-12T20:57:59Z",
        "link": "http://arxiv.org/abs/cs/0001008v3",
        "categories": [
            "cs.MA",
            "cs.LG",
            "I.2.11"
        ]
    },
    {
        "title": "Pattern Discovery and Computational Mechanics",
        "authors": [
            "Cosma Rohilla Shalizi",
            "James P. Crutchfield"
        ],
        "summary": "Computational mechanics is a method for discovering, describing and quantifying patterns, using tools from statistical physics. It constructs optimal, minimal models of stochastic processes and their underlying causal structures. These models tell us about the intrinsic computation embedded within a process---how it stores and transforms information. Here we summarize the mathematics of computational mechanics, especially recent optimality and uniqueness results. We also expound the principles and motivations underlying computational mechanics, emphasizing its connections to the minimum description length principle, PAC theory, and other aspects of machine learning.",
        "published": "2000-01-29T01:23:54Z",
        "link": "http://arxiv.org/abs/cs/0001027v1",
        "categories": [
            "cs.LG",
            "cs.NE",
            "I.2.6; F.1.3; G.3; H.1.1"
        ]
    },
    {
        "title": "Multiplicative Nonholonomic/Newton -like Algorithm",
        "authors": [
            "Toshinao Akuzawa",
            "Noboru Murata"
        ],
        "summary": "We construct new algorithms from scratch, which use the fourth order cumulant of stochastic variables for the cost function. The multiplicative updating rule here constructed is natural from the homogeneous nature of the Lie group and has numerous merits for the rigorous treatment of the dynamics. As one consequence, the second order convergence is shown. For the cost function, functions invariant under the componentwise scaling are choosen. By identifying points which can be transformed to each other by the scaling, we assume that the dynamics is in a coset space. In our method, a point can move toward any direction in this coset. Thus, no prewhitening is required.",
        "published": "2000-02-09T06:44:28Z",
        "link": "http://arxiv.org/abs/cs/0002006v1",
        "categories": [
            "cs.LG",
            "G.1.6"
        ]
    },
    {
        "title": "MOO: A Methodology for Online Optimization through Mining the Offline   Optimum",
        "authors": [
            "Jason W. H. Lee",
            "Y. C. Tay",
            "Anthony K. H. Tung"
        ],
        "summary": "Ports, warehouses and courier services have to decide online how an arriving task is to be served in order that cost is minimized (or profit maximized). These operators have a wealth of historical data on task assignments; can these data be mined for knowledge or rules that can help the decision-making?   MOO is a novel application of data mining to online optimization. The idea is to mine (logged) expert decisions or the offline optimum for rules that can be used for online decisions. It requires little knowledge about the task distribution and cost structure, and is applicable to a wide range of problems.   This paper presents a feasibility study of the methodology for the well-known k-server problem. Experiments with synthetic data show that optimization can be recast as classification of the optimum decisions; the resulting heuristic can achieve the optimum for strong request patterns, consistently outperforms other heuristics for weak patterns, and is robust despite changes in cost model.",
        "published": "2000-03-22T12:49:38Z",
        "link": "http://arxiv.org/abs/cs/0003072v1",
        "categories": [
            "cs.DS",
            "cs.LG",
            "F.2.2;H.2.8;F.1.2"
        ]
    },
    {
        "title": "A Theory of Universal Artificial Intelligence based on Algorithmic   Complexity",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameterless theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline for a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning, how the AIXI model can formally solve them. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXI-tl, which is still effectively more intelligent than any other time t and space l bounded agent. The computation time of AIXI-tl is of the order tx2^l. Other discussed topics are formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches.",
        "published": "2000-04-03T06:16:16Z",
        "link": "http://arxiv.org/abs/cs/0004001v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "I.2; F.1.3; E.4"
        ]
    },
    {
        "title": "The information bottleneck method",
        "authors": [
            "Naftali Tishby",
            "Fernando C. Pereira",
            "William Bialek"
        ],
        "summary": "We define the relevant information in a signal $x\\in X$ as being the information that this signal provides about another signal $y\\in \\Y$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal $x$ requires more than just predicting $y$, it also requires specifying which features of $\\X$ play a role in the prediction. We formalize this problem as that of finding a short code for $\\X$ that preserves the maximum information about $\\Y$. That is, we squeeze the information that $\\X$ provides about $\\Y$ through a `bottleneck' formed by a limited set of codewords $\\tX$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure $d(x,\\x)$ emerges from the joint statistics of $\\X$ and $\\Y$. This approach yields an exact set of self consistent equations for the coding rules $X \\to \\tX$ and $\\tX \\to \\Y$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.",
        "published": "2000-04-24T15:22:30Z",
        "link": "http://arxiv.org/abs/physics/0004057v1",
        "categories": [
            "physics.data-an",
            "cond-mat.dis-nn",
            "cs.LG",
            "nlin.AO"
        ]
    },
    {
        "title": "Modeling the Uncertainty in Complex Engineering Systems",
        "authors": [
            "A. Guergachi"
        ],
        "summary": "Existing procedures for model validation have been deemed inadequate for many engineering systems. The reason of this inadequacy is due to the high degree of complexity of the mechanisms that govern these systems. It is proposed in this paper to shift the attention from modeling the engineering system itself to modeling the uncertainty that underlies its behavior. A mathematical framework for modeling the uncertainty in complex engineering systems is developed. This framework uses the results of computational learning theory. It is based on the premise that a system model is a learning machine.",
        "published": "2000-05-14T14:35:20Z",
        "link": "http://arxiv.org/abs/cs/0005021v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.6;I.6.4;J.2;G.3"
        ]
    },
    {
        "title": "A Bayesian Reflection on Surfaces",
        "authors": [
            "David R. Wolf"
        ],
        "summary": "The topic of this paper is a novel Bayesian continuous-basis field representation and inference framework. Within this paper several problems are solved: The maximally informative inference of continuous-basis fields, that is where the basis for the field is itself a continuous object and not representable in a finite manner; the tradeoff between accuracy of representation in terms of information learned, and memory or storage capacity in bits; the approximation of probability distributions so that a maximal amount of information about the object being inferred is preserved; an information theoretic justification for multigrid methodology. The maximally informative field inference framework is described in full generality and denoted the Generalized Kalman Filter. The Generalized Kalman Filter allows the update of field knowledge from previous knowledge at any scale, and new data, to new knowledge at any other scale. An application example instance, the inference of continuous surfaces from measurements (for example, camera image data), is presented.",
        "published": "2000-05-26T20:24:48Z",
        "link": "http://arxiv.org/abs/cs/0005027v1",
        "categories": [
            "cs.CV",
            "cs.DS",
            "cs.LG",
            "math.PR",
            "nlin.AO",
            "physics.data-an",
            "G.3;I.2.4;I.2.6;I.2.10;I.4.1;I.4.4;I.4.5;I.4.10"
        ]
    },
    {
        "title": "Information Bottlenecks, Causal States, and Statistical Relevance Bases:   How to Represent Relevant Information in Memoryless Transduction",
        "authors": [
            "Cosma Rohilla Shalizi",
            "James P. Crutchfield"
        ],
        "summary": "Discovering relevant, but possibly hidden, variables is a key step in constructing useful and predictive theories about the natural world. This brief note explains the connections between three approaches to this problem: the recently introduced information-bottleneck method, the computational mechanics approach to inferring optimal models, and Salmon's statistical relevance basis.",
        "published": "2000-06-16T17:01:39Z",
        "link": "http://arxiv.org/abs/nlin/0006025v1",
        "categories": [
            "nlin.AO",
            "cond-mat.dis-nn",
            "cs.LG",
            "physics.data-an"
        ]
    },
    {
        "title": "Algorithmic Statistics",
        "authors": [
            "Peter Gacs",
            "John Tromp",
            "Paul Vitanyi"
        ],
        "summary": "While Kolmogorov complexity is the accepted absolute measure of information content of an individual finite object, a similarly absolute notion is needed for the relation between an individual data sample and an individual model summarizing the information in the data, for example, a finite set (or probability distribution) where the data sample typically came from. The statistical theory based on such relations between individual objects can be called algorithmic statistics, in contrast to classical statistical theory that deals with relations between probabilistic ensembles. We develop the algorithmic theory of statistic, sufficient statistic, and minimal sufficient statistic. This theory is based on two-part codes consisting of the code for the statistic (the model summarizing the regularity, the meaningful information, in the data) and the model-to-data code. In contrast to the situation in probabilistic statistical theory, the algorithmic relation of (minimal) sufficiency is an absolute relation between the individual model and the individual data sample. We distinguish implicit and explicit descriptions of the models. We give characterizations of algorithmic (Kolmogorov) minimal sufficient statistic for all data samples for both description modes--in the explicit mode under some constraints. We also strengthen and elaborate earlier results on the ``Kolmogorov structure function'' and ``absolutely non-stochastic objects''--those rare objects for which the simplest models that summarize their relevant information (minimal sufficient statistics) are at least as complex as the objects themselves. We demonstrate a close relation between the probabilistic notions and the algorithmic ones.",
        "published": "2000-06-30T17:19:06Z",
        "link": "http://arxiv.org/abs/math/0006233v3",
        "categories": [
            "math.ST",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.PR",
            "physics.data-an",
            "stat.TH",
            "62B05, 62B10, 68Q32, 68Q30, 60AXX, 68T04"
        ]
    },
    {
        "title": "Integrating E-Commerce and Data Mining: Architecture and Challenges",
        "authors": [
            "Suhail Ansari",
            "Ron Kohavi",
            "Llew Mason",
            "Zijian Zheng"
        ],
        "summary": "We show that the e-commerce domain can provide all the right ingredients for successful data mining and claim that it is a killer domain for data mining. We describe an integrated architecture, based on our expe-rience at Blue Martini Software, for supporting this integration. The architecture can dramatically reduce the pre-processing, cleaning, and data understanding effort often documented to take 80% of the time in knowledge discovery projects. We emphasize the need for data collection at the application server layer (not the web server) in order to support logging of data and metadata that is essential to the discovery process. We describe the data transformation bridges required from the transaction processing systems and customer event streams (e.g., clickstreams) to the data warehouse. We detail the mining workbench, which needs to provide multiple views of the data through reporting, data mining algorithms, visualization, and OLAP. We con-clude with a set of challenges.",
        "published": "2000-07-14T00:33:12Z",
        "link": "http://arxiv.org/abs/cs/0007026v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DB",
            "I.2.6;H.2.8"
        ]
    },
    {
        "title": "Predictability, complexity and learning",
        "authors": [
            "William Bialek",
            "Ilya Nemenman",
            "Naftali Tishby"
        ],
        "summary": "We define {\\em predictive information} $I_{\\rm pred} (T)$ as the mutual information between the past and the future of a time series. Three qualitatively different behaviors are found in the limit of large observation times $T$: $I_{\\rm pred} (T)$ can remain finite, grow logarithmically, or grow as a fractional power law. If the time series allows us to learn a model with a finite number of parameters, then $I_{\\rm pred} (T)$ grows logarithmically with a coefficient that counts the dimensionality of the model space. In contrast, power--law growth is associated, for example, with the learning of infinite parameter (or nonparametric) models such as continuous functions with smoothness constraints. There are connections between the predictive information and measures of complexity that have been defined both in learning theory and in the analysis of physical systems through statistical mechanics and dynamical systems theory. Further, in the same way that entropy provides the unique measure of available information consistent with some simple and plausible conditions, we argue that the divergent part of $I_{\\rm pred} (T)$ provides the unique measure for the complexity of dynamics underlying a time series. Finally, we discuss how these ideas may be useful in different problems in physics, statistics, and biology.",
        "published": "2000-07-20T00:45:11Z",
        "link": "http://arxiv.org/abs/physics/0007070v3",
        "categories": [
            "physics.data-an",
            "cond-mat.dis-nn",
            "cond-mat.other",
            "cs.LG",
            "nlin.AO",
            "q-bio.OT"
        ]
    },
    {
        "title": "Data Mining to Measure and Improve the Success of Web Sites",
        "authors": [
            "Myra Spiliopoulou",
            "Carsten Pohle"
        ],
        "summary": "For many companies, competitiveness in e-commerce requires a successful presence on the web. Web sites are used to establish the company's image, to promote and sell goods and to provide customer support. The success of a web site affects and reflects directly the success of the company in the electronic market. In this study, we propose a methodology to improve the ``success'' of web sites, based on the exploitation of navigation pattern discovery. In particular, we present a theory, in which success is modelled on the basis of the navigation behaviour of the site's users. We then exploit WUM, a navigation pattern discovery miner, to study how the success of a site is reflected in the users' behaviour. With WUM we measure the success of a site's components and obtain concrete indications of how the site should be improved. We report on our first experiments with an online catalog, the success of which we have studied. Our mining analysis has shown very promising results, on the basis of which the site is currently undergoing concrete improvements.",
        "published": "2000-08-15T15:20:18Z",
        "link": "http://arxiv.org/abs/cs/0008009v1",
        "categories": [
            "cs.LG",
            "cs.DB",
            "I.2.6; H.2.8"
        ]
    },
    {
        "title": "An Experimental Comparison of Naive Bayesian and Keyword-Based Anti-Spam   Filtering with Personal E-mail Messages",
        "authors": [
            "Ion Androutsopoulos",
            "John Koutsias",
            "Konstantinos V. Chandrinos",
            "Constantine D. Spyropoulos"
        ],
        "summary": "The growing problem of unsolicited bulk e-mail, also known as \"spam\", has generated a need for reliable anti-spam e-mail filters. Filters of this type have so far been based mostly on manually constructed keyword patterns. An alternative approach has recently been proposed, whereby a Naive Bayesian classifier is trained automatically to detect spam messages. We test this approach on a large collection of personal e-mail messages, which we make publicly available in \"encrypted\" form contributing towards standard benchmarks. We introduce appropriate cost-sensitive measures, investigating at the same time the effect of attribute-set size, training-corpus size, lemmatization, and stop lists, issues that have not been explored in previous experiments. Finally, the Naive Bayesian filter is compared, in terms of performance, to a filter that uses keyword patterns, and which is part of a widely used e-mail reader.",
        "published": "2000-08-22T11:20:14Z",
        "link": "http://arxiv.org/abs/cs/0008019v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.4.3; I.2.6; I.2.7; I.5.4; K.4.1"
        ]
    },
    {
        "title": "A Learning Approach to Shallow Parsing",
        "authors": [
            "Marcia Muñoz",
            "Vasin Punyakanok",
            "Dan Roth",
            "Dav Zimak"
        ],
        "summary": "A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.",
        "published": "2000-08-22T21:37:50Z",
        "link": "http://arxiv.org/abs/cs/0008022v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Complexity analysis for algorithmically simple strings",
        "authors": [
            "Andrei N. Soklakov"
        ],
        "summary": "Given a reference computer, Kolmogorov complexity is a well defined function on all binary strings. In the standard approach, however, only the asymptotic properties of such functions are considered because they do not depend on the reference computer. We argue that this approach can be more useful if it is refined to include an important practical case of simple binary strings. Kolmogorov complexity calculus may be developed for this case if we restrict the class of available reference computers. The interesting problem is to define a class of computers which is restricted in a {\\it natural} way modeling the real-life situation where only a limited class of computers is physically available to us. We give an example of what such a natural restriction might look like mathematically, and show that under such restrictions some error terms, even logarithmic in complexity, can disappear from the standard complexity calculus.   Keywords: Kolmogorov complexity; Algorithmic information theory.",
        "published": "2000-09-05T18:54:58Z",
        "link": "http://arxiv.org/abs/cs/0009001v3",
        "categories": [
            "cs.LG",
            "E.4; F.2; I.2"
        ]
    },
    {
        "title": "Information theory and learning: a physical approach",
        "authors": [
            "Ilya Nemenman"
        ],
        "summary": "We try to establish a unified information theoretic approach to learning and to explore some of its applications. First, we define {\\em predictive information} as the mutual information between the past and the future of a time series, discuss its behavior as a function of the length of the series, and explain how other quantities of interest studied previously in learning theory - as well as in dynamical systems and statistical mechanics - emerge from this universally definable concept. We then prove that predictive information provides the {\\em unique measure for the complexity} of dynamics underlying the time series and show that there are classes of models characterized by {\\em power-law growth of the predictive information} that are qualitatively more complex than any of the systems that have been investigated before. Further, we investigate numerically the learning of a nonparametric probability density, which is an example of a problem with power-law complexity, and show that the proper Bayesian formulation of this problem provides for the `Occam' factors that punish overly complex models and thus allow one {\\em to learn not only a solution within a specific model class, but also the class itself} using the data only and with very few a priori assumptions. We study a possible {\\em information theoretic method} that regularizes the learning of an undersampled discrete variable, and show that learning in such a setup goes through stages of very different complexities. Finally, we discuss how all of these ideas may be useful in various problems in physics, statistics, and, most importantly, biology.",
        "published": "2000-09-08T23:30:26Z",
        "link": "http://arxiv.org/abs/physics/0009032v1",
        "categories": [
            "physics.data-an",
            "cond-mat.dis-nn",
            "cs.LG",
            "nlin.AO"
        ]
    },
    {
        "title": "Occam factors and model-independent Bayesian learning of continuous   distributions",
        "authors": [
            "Ilya Nemenman",
            "William Bialek"
        ],
        "summary": "Learning of a smooth but nonparametric probability density can be regularized using methods of Quantum Field Theory. We implement a field theoretic prior numerically, test its efficacy, and show that the data and the phase space factors arising from the integration over the model space determine the free parameter of the theory (\"smoothness scale\") self-consistently. This persists even for distributions that are atypical in the prior and is a step towards a model-independent theory for learning continuous distributions. Finally, we point out that a wrong parameterization of a model family may sometimes be advantageous for small data sets.",
        "published": "2000-09-11T22:51:53Z",
        "link": "http://arxiv.org/abs/cond-mat/0009165v2",
        "categories": [
            "cond-mat",
            "cs.LG",
            "nlin.AO",
            "physics.data-an"
        ]
    },
    {
        "title": "Robust Classification for Imprecise Environments",
        "authors": [
            "Foster Provost",
            "Tom Fawcett"
        ],
        "summary": "In real-world environments it usually is difficult to specify target operating conditions precisely, for example, target misclassification costs. This uncertainty makes building robust classification systems problematic. We show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. In some cases, the performance of the hybrid actually can surpass that of the best known classifier. This robust performance extends across a wide variety of comparison frameworks, including the optimization of metrics such as accuracy, expected cost, lift, precision, recall, and workforce utilization. The hybrid also is efficient to build, to store, and to update. The hybrid is based on a method for the comparison of classifier performance that is robust to imprecise class distributions and misclassification costs. The ROC convex hull (ROCCH) method combines techniques from ROC analysis, decision analysis and computational geometry, and adapts them to the particulars of analyzing learned classifiers. The method is efficient and incremental, minimizes the management of classifier performance data, and allows for clear visual comparisons and sensitivity analyses. Finally, we point to empirical evidence that a robust hybrid classifier indeed is needed for many real-world problems.",
        "published": "2000-09-13T21:09:47Z",
        "link": "http://arxiv.org/abs/cs/0009007v1",
        "categories": [
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "Learning to Filter Spam E-Mail: A Comparison of a Naive Bayesian and a   Memory-Based Approach",
        "authors": [
            "Ion Androutsopoulos",
            "Georgios Paliouras",
            "Vangelis Karkaletsis",
            "Georgios Sakkis",
            "Constantine D. Spyropoulos",
            "Panagiotis Stamatopoulos"
        ],
        "summary": "We investigate the performance of two machine learning algorithms in the context of anti-spam filtering. The increasing volume of unsolicited bulk e-mail (spam) has generated a need for reliable anti-spam filters. Filters of this type have so far been based mostly on keyword patterns that are constructed by hand and perform poorly. The Naive Bayesian classifier has recently been suggested as an effective method to construct automatically anti-spam filters with superior performance. We investigate thoroughly the performance of the Naive Bayesian filter on a publicly available corpus, contributing towards standard benchmarks. At the same time, we compare the performance of the Naive Bayesian filter to an alternative memory-based learning approach, after introducing suitable cost-sensitive evaluation measures. Both methods achieve very accurate spam filtering, outperforming clearly the keyword-based filter of a widely used e-mail reader.",
        "published": "2000-09-18T14:05:13Z",
        "link": "http://arxiv.org/abs/cs/0009009v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.4.3; I.2.6; I.2.7; I.5.4; K.4.1"
        ]
    },
    {
        "title": "A Classification Approach to Word Prediction",
        "authors": [
            "Yair Even-Zohar",
            "Dan Roth"
        ],
        "summary": "The eventual goal of a language model is to accurately predict the value of a missing word given its context. We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context. This approach raises a few new questions that we address. First, in order to learn good word representations it is necessary to use an expressive representation of the context. We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction. Second, since the number of words ``competing'' for each prediction is large, there is a need to ``focus the attention'' on a smaller subset of these. We exhibit the contribution of a ``focus of attention'' mechanism to the performance of the word predictor. Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks.",
        "published": "2000-09-28T14:25:51Z",
        "link": "http://arxiv.org/abs/cs/0009027v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "I.2.6;I.2.7"
        ]
    },
    {
        "title": "Design of an Electro-Hydraulic System Using Neuro-Fuzzy Techniques",
        "authors": [
            "P. J. Costa Branco",
            "J. A. Dente"
        ],
        "summary": "Increasing demands in performance and quality make drive systems fundamental parts in the progressive automation of industrial processes. Their conventional models become inappropriate and have limited scope if one requires a precise and fast performance. So, it is important to incorporate learning capabilities into drive systems in such a way that they improve their accuracy in realtime, becoming more autonomous agents with some degree of intelligence. To investigate this challenge, this chapter presents the development of a learning control system that uses neuro-fuzzy techniques in the design of a tracking controller to an experimental electro-hydraulic actuator. We begin the chapter by presenting the neuro-fuzzy modeling process of the actuator. This part surveys the learning algorithm, describes the laboratorial system, and presents the modeling steps as the choice of actuator representative variables, the acquisition of training and testing data sets, and the acquisition of the neuro-fuzzy inverse-model of the actuator. In the second part of the chapter, we use the extracted neuro-fuzzy model and its learning capabilities to design the actuator position controller based on the feedback-error-learning technique. Through a set of experimental results, we show the generalization properties of the controller, its learning capability in actualizing in realtime the initial neuro-fuzzy inverse-model, and its compensation action improving the electro-hydraulics tracking performance.",
        "published": "2000-09-30T11:47:42Z",
        "link": "http://arxiv.org/abs/cs/0010001v1",
        "categories": [
            "cs.RO",
            "cs.LG",
            "C.3; C.4; F.1.1; I.2.6; I.2.9; I.6.5; J.2, J.7"
        ]
    },
    {
        "title": "Noise Effects in Fuzzy Modelling Systems",
        "authors": [
            "P. J. Costa Branco",
            "J. A. Dente"
        ],
        "summary": "Noise is source of ambiguity for fuzzy systems. Although being an important aspect, the effects of noise in fuzzy modeling have been little investigated. This paper presents a set of tests using three well-known fuzzy modeling algorithms. These evaluate perturbations in the extracted rule-bases caused by noise polluting the learning data, and the corresponding deformations in each learned functional relation. We present results to show: 1) how these fuzzy modeling systems deal with noise; 2) how the established fuzzy model structure influences noise sensitivity of each algorithm; and 3) whose characteristics of the learning algorithms are relevant to noise attenuation.",
        "published": "2000-09-30T14:37:23Z",
        "link": "http://arxiv.org/abs/cs/0010002v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "I.2.6; I.5.1; I.5.2"
        ]
    },
    {
        "title": "Torque Ripple Minimization in a Switched Reluctance Drive by Neuro-Fuzzy   Compensation",
        "authors": [
            "L. Henriques",
            "L. Rolim",
            "W. Suemitsu",
            "P. J. Costa Branco",
            "J. A. Dente"
        ],
        "summary": "Simple power electronic drive circuit and fault tolerance of converter are specific advantages of SRM drives, but excessive torque ripple has limited its use to special applications. It is well known that controlling the current shape adequately can minimize the torque ripple. This paper presents a new method for shaping the motor currents to minimize the torque ripple, using a neuro-fuzzy compensator. In the proposed method, a compensating signal is added to the output of a PI controller, in a current-regulated speed control loop. Numerical results are presented in this paper, with an analysis of the effects of changing the form of the membership function of the neuro-fuzzy compensator.",
        "published": "2000-09-30T15:31:16Z",
        "link": "http://arxiv.org/abs/cs/0010003v1",
        "categories": [
            "cs.RO",
            "cs.LG",
            "I.2.9; I.2.6"
        ]
    },
    {
        "title": "A Fuzzy Relational Identification Algorithm and Its Application to   Predict The Behaviour of a Motor Drive System",
        "authors": [
            "P. J. Costa Branco",
            "J. A. Dente"
        ],
        "summary": "Fuzzy relational identification builds a relational model describing systems behaviour by a nonlinear mapping between its variables. In this paper, we propose a new fuzzy relational algorithm based on simplified max-min relational equation. The algorithm presents an adaptation method applied to gravity-center of each fuzzy set based on error integral value between measured and predicted system output, and uses the concept of time-variant universe of discourses. The identification algorithm also includes a method to attenuate noise influence in extracted system relational model using a fuzzy filtering mechanism. The algorithm is applied to one-step forward prediction of a simulated and experimental motor drive system. The identified model has its input-output variables (stator-reference current and motor speed signal) treated as fuzzy sets, whereas the relations existing between them are described by means of a matrix R defining the relational model extracted by the algorithm. The results show the good potentialities of the algorithm in predict the behaviour of the system and attenuate through the fuzzy filtering method possible noise distortions in the relational model.",
        "published": "2000-09-30T15:42:55Z",
        "link": "http://arxiv.org/abs/cs/0010004v1",
        "categories": [
            "cs.RO",
            "cs.LG",
            "I.2.9; I.2.6"
        ]
    },
    {
        "title": "Applications of Data Mining to Electronic Commerce",
        "authors": [
            "Ron Kohavi",
            "Foster Provost"
        ],
        "summary": "Electronic commerce is emerging as the killer domain for data mining technology.   The following are five desiderata for success. Seldom are they they all present in one data mining application.   1. Data with rich descriptions. For example, wide customer records with many potentially useful fields allow data mining algorithms to search beyond obvious correlations.   2. A large volume of data. The large model spaces corresponding to rich data demand many training instances to build reliable models.   3. Controlled and reliable data collection. Manual data entry and integration from legacy systems both are notoriously problematic; fully automated collection is considerably better.   4. The ability to evaluate results. Substantial, demonstrable return on investment can be very convincing.   5. Ease of integration with existing processes. Even if pilot studies show potential benefit, deploying automated solutions to previously manual processes is rife with pitfalls. Building a system to take advantage of the mined knowledge can be a substantial undertaking. Furthermore, one often must deal with social and political issues involved in the automation of a previously manual business process.",
        "published": "2000-10-02T12:16:17Z",
        "link": "http://arxiv.org/abs/cs/0010006v1",
        "categories": [
            "cs.LG",
            "cs.DB",
            "I.2.6;H.2.8"
        ]
    },
    {
        "title": "Fault Detection using Immune-Based Systems and Formal Language   Algorithms",
        "authors": [
            "J. F. Martins",
            "P. J. Costa Branco",
            "A. J. Pires",
            "J. A. Dente"
        ],
        "summary": "This paper describes two approaches for fault detection: an immune-based mechanism and a formal language algorithm. The first one is based on the feature of immune systems in distinguish any foreign cell from the body own cell. The formal language approach assumes the system as a linguistic source capable of generating a certain language, characterised by a grammar. Each algorithm has particular characteristics, which are analysed in the paper, namely in what cases they can be used with advantage. To test their practicality, both approaches were applied on the problem of fault detection in an induction motor.",
        "published": "2000-10-03T17:54:38Z",
        "link": "http://arxiv.org/abs/cs/0010010v1",
        "categories": [
            "cs.CE",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Noise-Tolerant Learning, the Parity Problem, and the Statistical Query   Model",
        "authors": [
            "Avrim Blum",
            "Adam Kalai",
            "Hal Wasserman"
        ],
        "summary": "We describe a slightly sub-exponential time algorithm for learning parity functions in the presence of random classification noise. This results in a polynomial-time algorithm for the case of parity functions that depend on only the first O(log n log log n) bits of input. This is the first known instance of an efficient noise-tolerant algorithm for a concept class that is provably not learnable in the Statistical Query model of Kearns. Thus, we demonstrate that the set of problems learnable in the statistical query model is a strict subset of those problems learnable in the presence of noise in the PAC model.   In coding-theory terms, what we give is a poly(n)-time algorithm for decoding linear k by n codes in the presence of random noise for the case of k = c log n loglog n for some c > 0. (The case of k = O(log n) is trivial since one can just individually check each of the 2^k possible messages and choose the one that yields the closest codeword.)   A natural extension of the statistical query model is to allow queries about statistical properties that involve t-tuples of examples (as opposed to single examples). The second result of this paper is to show that any class of functions learnable (strongly or weakly) with t-wise queries for t = O(log n) is also weakly learnable with standard unary queries. Hence this natural extension to the statistical query model does not increase the set of weakly learnable functions.",
        "published": "2000-10-15T20:14:08Z",
        "link": "http://arxiv.org/abs/cs/0010022v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DS",
            "I.2.6"
        ]
    },
    {
        "title": "Top-down induction of clustering trees",
        "authors": [
            "Hendrik Blockeel",
            "Luc De Raedt",
            "Jan Ramon"
        ],
        "summary": "An approach to clustering is presented that adapts the basic top-down induction of decision trees method towards clustering. To this aim, it employs the principles of instance based learning. The resulting methodology is implemented in the TIC (Top down Induction of Clustering trees) system for first order clustering. The TIC system employs the first order logical decision tree representation of the inductive logic programming system Tilde. Various experiments with TIC are presented, in both propositional and relational domains.",
        "published": "2000-11-21T21:51:01Z",
        "link": "http://arxiv.org/abs/cs/0011032v1",
        "categories": [
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "Web Mining Research: A Survey",
        "authors": [
            "Raymond Kosala",
            "Hendrik Blockeel"
        ],
        "summary": "With the huge amount of information available online, the World Wide Web is a fertile area for data mining research. The Web mining research is at the cross road of research from several research communities, such as database, information retrieval, and within AI, especially the sub-areas of machine learning and natural language processing. However, there is a lot of confusions when comparing research efforts from different point of views. In this paper, we survey the research in the area of Web mining, point out some confusions regarded the usage of the term Web mining and suggest three Web mining categories. Then we situate some of the research with respect to these three categories. We also explore the connection between the Web mining categories and the related agent paradigm. For the survey, we focus on representation issues, on the process, on the learning algorithm, and on the application of the recent works as the criteria. We conclude the paper with some research issues.",
        "published": "2000-11-22T09:41:53Z",
        "link": "http://arxiv.org/abs/cs/0011033v1",
        "categories": [
            "cs.LG",
            "cs.DB",
            "I.2.6; H.2.8"
        ]
    },
    {
        "title": "Provably Fast and Accurate Recovery of Evolutionary Trees through   Harmonic Greedy Triplets",
        "authors": [
            "Miklos Csuros",
            "Ming-Yang Kao"
        ],
        "summary": "We give a greedy learning algorithm for reconstructing an evolutionary tree based on a certain harmonic average on triplets of terminal taxa. After the pairwise distances between terminal taxa are estimated from sequence data, the algorithm runs in O(n^2) time using O(n) work space, where n is the number of terminal taxa. These time and space complexities are optimal in the sense that the size of an input distance matrix is n^2 and the size of an output tree is n. Moreover, in the Jukes-Cantor model of evolution, the algorithm recovers the correct tree topology with high probability using sample sequences of length polynomial in (1) n, (2) the logarithm of the error probability, and (3) the inverses of two small parameters.",
        "published": "2000-11-23T14:48:53Z",
        "link": "http://arxiv.org/abs/cs/0011038v1",
        "categories": [
            "cs.DS",
            "cs.LG",
            "E.1; F.2.2; G.2.1; G.2.2; G.2.3; G.3; G.4; J.3"
        ]
    },
    {
        "title": "Scaling Up Inductive Logic Programming by Learning from Interpretations",
        "authors": [
            "Hendrik Blockeel",
            "Luc De Raedt",
            "Nico Jacobs",
            "Bart Demoen"
        ],
        "summary": "When comparing inductive logic programming (ILP) and attribute-value learning techniques, there is a trade-off between expressive power and efficiency. Inductive logic programming techniques are typically more expressive but also less efficient. Therefore, the data sets handled by current inductive logic programming systems are small according to general standards within the data mining community. The main source of inefficiency lies in the assumption that several examples may be related to each other, so they cannot be handled independently.   Within the learning from interpretations framework for inductive logic programming this assumption is unnecessary, which allows to scale up existing ILP algorithms. In this paper we explain this learning setting in the context of relational databases. We relate the setting to propositional data mining and to the classical ILP setting, and show that learning from interpretations corresponds to learning from multiple relations and thus extends the expressiveness of propositional learning, while maintaining its efficiency to a large extent (which is not the case in the classical ILP setting).   As a case study, we present two alternative implementations of the ILP system Tilde (Top-down Induction of Logical DEcision trees): Tilde-classic, which loads all data in main memory, and Tilde-LDS, which loads the examples one by one. We experimentally compare the implementations, showing Tilde-LDS can handle large data sets (in the order of 100,000 examples or 100 MB) and indeed scales up linearly in the number of examples.",
        "published": "2000-11-29T12:14:50Z",
        "link": "http://arxiv.org/abs/cs/0011044v1",
        "categories": [
            "cs.LG",
            "I.2.6 ; I.2.3"
        ]
    },
    {
        "title": "Algorithmic Theories of Everything",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "The probability distribution P from which the history of our universe is sampled represents a theory of everything or TOE. We assume P is formally describable. Since most (uncountably many) distributions are not, this imposes a strong inductive bias. We show that P(x) is small for any universe x lacking a short description, and study the spectrum of TOEs spanned by two Ps, one reflecting the most compact constructive descriptions, the other the fastest way of computing everything. The former derives from generalizations of traditional computability, Solomonoff's algorithmic probability, Kolmogorov complexity, and objects more random than Chaitin's Omega, the latter from Levin's universal search and a natural resource-oriented postulate: the cumulative prior probability of all x incomputable within time t by this optimal algorithm should be 1/t. Between both Ps we find a universal cumulatively enumerable measure that dominates traditional enumerable measures; any such CEM must assign low probability to any universe lacking a short enumerating program. We derive P-specific consequences for evolving observers, inductive reasoning, quantum physics, philosophy, and the expected duration of our universe.",
        "published": "2000-11-30T14:23:55Z",
        "link": "http://arxiv.org/abs/quant-ph/0011122v2",
        "categories": [
            "quant-ph",
            "cs.AI",
            "cs.CC",
            "cs.LG",
            "hep-th",
            "math-ph",
            "math.MP",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Towards a Universal Theory of Artificial Intelligence based on   Algorithmic Probability and Sequential Decision Theory",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown distribution. We unify both theories and give strong arguments that the resulting universal AIXI model behaves optimal in any computable environment. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXI^tl, which is still superior to any other time t and space l bounded agent. The computation time of AIXI^tl is of the order t x 2^l.",
        "published": "2000-12-16T09:38:13Z",
        "link": "http://arxiv.org/abs/cs/0012011v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "I.2; I.2.3; I.2.6; I.2.8; F.1.3; F.2"
        ]
    },
    {
        "title": "Learning Complexity Dimensions for a Continuous-Time Control System",
        "authors": [
            "Pirkko Kuusela",
            "Daniel Ocone",
            "Eduardo D. Sontag"
        ],
        "summary": "This paper takes a computational learning theory approach to a problem of linear systems identification. It is assumed that input signals have only a finite number k of frequency components, and systems to be identified have dimension no greater than n. The main result establishes that the sample complexity needed for identification scales polynomially with n and logarithmically with k.",
        "published": "2000-12-18T10:35:00Z",
        "link": "http://arxiv.org/abs/math/0012163v2",
        "categories": [
            "math.OC",
            "cs.LG",
            "93C05"
        ]
    },
    {
        "title": "The number of guards needed by a museum: A phase transition in vertex   covering of random graphs",
        "authors": [
            "Martin Weigt",
            "Alexander K. Hartmann"
        ],
        "summary": "In this letter we study the NP-complete vertex cover problem on finite connectivity random graphs. When the allowed size of the cover set is decreased, a discontinuous transition in solvability and typical-case complexity occurs. This transition is characterized by means of exact numerical simulations as well as by analytical replica calculations. The replica symmetric phase diagram is in excellent agreement with numerical findings up to average connectivity $e$, where replica symmetry becomes locally unstable.",
        "published": "2000-01-11T14:58:56Z",
        "link": "http://arxiv.org/abs/cond-mat/0001137v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Nondeterministic Quantum Query and Quantum Communication Complexities",
        "authors": [
            "Ronald de Wolf"
        ],
        "summary": "We study nondeterministic quantum algorithms for Boolean functions f. Such algorithms have positive acceptance probability on input x iff f(x)=1. In the setting of query complexity, we show that the nondeterministic quantum complexity of a Boolean function is equal to its ``nondeterministic polynomial'' degree. We also prove a quantum-vs-classical gap of 1 vs n for nondeterministic query complexity for a total function. In the setting of communication complexity, we show that the nondeterministic quantum complexity of a two-party function is equal to the logarithm of the rank of a nondeterministic version of the communication matrix. This implies that the quantum communication complexities of the equality and disjointness functions are n+1 if we do not allow any error probability. We also exhibit a total function in which the nondeterministic quantum communication complexity is exponentially smaller than its classical counterpart.",
        "published": "2000-01-19T09:31:14Z",
        "link": "http://arxiv.org/abs/cs/0001014v3",
        "categories": [
            "cs.CC",
            "quant-ph",
            "E.4; F.1.1; F.1.2; F.1.3; F.2.0"
        ]
    },
    {
        "title": "Query Complexity: Worst-Case Quantum Versus Average-Case Classical",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "In this note we investigate the relationship between worst-case quantum query complexity and average-case classical query complexity. Specifically, we show that if a quantum computer can evaluate a total Boolean function f with bounded error using T queries in the worst case, then a deterministic classical computer can evaluate f using O(T^5) queries in the average case, under a uniform distribution of inputs. If f is monotone, we show furthermore that only O(T^3) queries are needed. Previously, Beals et al. (1998) showed that if a quantum computer can evaluate f with bounded error using T queries in the worst case, then a deterministic classical computer can evaluate f using O(T^6) queries in the worst case, or O(T^4) if f is monotone. The optimal bound is conjectured to be O(T^2), but improving on O(T^6) remains an open problem. Relating worst-case quantum complexity to average-case classical complexity may suggest new ways to reduce the polynomial gap in the ordinary worst-case versus worst-case setting.",
        "published": "2000-01-19T17:32:35Z",
        "link": "http://arxiv.org/abs/cs/0001013v3",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.2"
        ]
    },
    {
        "title": "On The Closest String and Substring Problems",
        "authors": [
            "Ming Li",
            "Bin Ma",
            "Lusheng Wang"
        ],
        "summary": "The problem of finding a center string that is `close' to every given string arises and has many applications in computational biology and coding theory. This problem has two versions: the Closest String problem and the Closest Substring problem. Assume that we are given a set of strings ${\\cal S}=\\{s_1, s_2, ..., s_n\\}$ of strings, say, each of length $m$. The Closest String problem asks for the smallest $d$ and a string $s$ of length $m$ which is within Hamming distance $d$ to each $s_i\\in {\\cal S}$. This problem comes from coding theory when we are looking for a code not too far away from a given set of codes. The problem is NP-hard. Berman et al give a polynomial time algorithm for constant $d$. For super-logarithmic $d$, Ben-Dor et al give an efficient approximation algorithm using linear program relaxation technique. The best polynomial time approximation has ratio 4/3 for all $d$ given by Lanctot et al and Gasieniec et al. The Closest Substring problem looks for a string $t$ which is within Hamming distance $d$ away from a substring of each $s_i$. This problem only has a $2- \\frac{2}{2|\\Sigma|+1}$ approximation algorithm previously Lanctot et al and is much more elusive than the Closest String problem, but it has many applications in finding conserved regions, genetic drug target identification, and genetic probes in molecular biology. Whether there are efficient approximation algorithms for both problems are major open questions in this area. We present two polynomial time approxmation algorithms with approximation ratio $1+ \\epsilon$ for any small $\\epsilon$ to settle both questions.",
        "published": "2000-02-17T23:06:06Z",
        "link": "http://arxiv.org/abs/cs/0002012v1",
        "categories": [
            "cs.CE",
            "cs.CC",
            "F.2;J.3"
        ]
    },
    {
        "title": "Quantum lower bounds by quantum arguments",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "We propose a new method for proving lower bounds on quantum query algorithms. Instead of a classical adversary that runs the algorithm with one input and then modifies the input, we use a quantum adversary that runs the algorithm with a superposition of inputs. If the algorithm works correctly, its state becomes entangled with the superposition over inputs. We bound the number of queries needed to achieve a sufficient entanglement and this implies a lower bound on the number of queries for the computation.   Using this method, we prove two new $\\Omega(\\sqrt{N})$ lower bounds on computing AND of ORs and inverting a permutation and also provide more uniform proofs for several known lower bounds which have been previously proven via variety of different techniques.",
        "published": "2000-02-24T04:46:18Z",
        "link": "http://arxiv.org/abs/quant-ph/0002066v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "One Complexity Theorist's View of Quantum Computing",
        "authors": [
            "Lance Fortnow"
        ],
        "summary": "The complexity of quantum computation remains poorly understood. While physicists attempt to find ways to create quantum computers, we still do not have much evidence one way or the other as to how useful these machines will be. The tools of computational complexity theory should come to bear on these important questions. Quantum computing often scares away many potential researchers from computer science because of the apparent background need in quantum mechanics and the alien looking notation used in papers on the topic. This paper will give an overview of quantum computation from the point of view of a complexity theorist. We will see that one can think of BQP as yet another complexity class and study its power without focusing on the physical aspects behind it.",
        "published": "2000-03-09T20:37:29Z",
        "link": "http://arxiv.org/abs/quant-ph/0003035v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "A Century of Controvery Over the Foundations of Mathematics II",
        "authors": [
            "G. J. Chaitin"
        ],
        "summary": "Transcript of G.J. Chaitin's 2 March 2000 Carnegie Mellon University School of Computer Science Distinguished Lecture. The notion of randomness is taken from physics and applied to pure mathematics in order to shed light on the incompleteness phenomenon discovered by K. Godel.",
        "published": "2000-04-04T19:44:58Z",
        "link": "http://arxiv.org/abs/nlin/0004007v2",
        "categories": [
            "nlin.CD",
            "cs.CC",
            "math.HO"
        ]
    },
    {
        "title": "Exact Phase Transitions in Random Constraint Satisfaction Problems",
        "authors": [
            "Ke Xu",
            "Wei Li"
        ],
        "summary": "In this paper we propose a new type of random CSP model, called Model RB, which is a revision to the standard Model B. It is proved that phase transitions from a region where almost all problems are satisfiable to a region where almost all problems are unsatisfiable do exist for Model RB as the number of variables approaches infinity. Moreover, the critical values at which the phase transitions occur are also known exactly. By relating the hardness of Model RB to Model B, it is shown that there exist a lot of hard instances in Model RB.",
        "published": "2000-04-16T07:13:09Z",
        "link": "http://arxiv.org/abs/cs/0004005v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DM",
            "I.2.8; G.3"
        ]
    },
    {
        "title": "Deciding first-order properties of locally tree-decomposable structures",
        "authors": [
            "Markus Frick",
            "Martin Grohe"
        ],
        "summary": "We introduce the concept of a class of graphs, or more generally, relational structures, being locally tree-decomposable. There are numerous examples of locally tree-decomposable classes, among them the class of planar graphs and all classes of bounded valence or of bounded tree-width. We also consider a slightly more general concept of a class of structures having bounded local tree-width.   We show that for each property P of structures that is definable in first-order logic and for each locally tree-decomposable class C of graphs, there is a linear time algorithm deciding whether a given structure A in C has property P. For classes C of bounded local tree-width, we show that for every k\\ge 1 there is an algorithm that solves the same problem in time O(n^{1+(1/k)}) (where n is the cardinality of the input structure).",
        "published": "2000-04-17T16:14:08Z",
        "link": "http://arxiv.org/abs/cs/0004007v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DB",
            "F.2.2; G.2.2; H.2.4; F.1.3"
        ]
    },
    {
        "title": "Separating the complexity classes NL and NP",
        "authors": [
            "David B. Benson"
        ],
        "summary": "Withdrawn since -order- was overlooked. First order reductions without order are much too weak to separate.",
        "published": "2000-04-17T22:17:33Z",
        "link": "http://arxiv.org/abs/cs/0004009v2",
        "categories": [
            "cs.CC",
            "F.1.3; F.4.1"
        ]
    },
    {
        "title": "PSPACE Reasoning for Graded Modal Logics",
        "authors": [
            "Stephan Tobies"
        ],
        "summary": "We present a PSPACE algorithm that decides satisfiability of the graded modal logic Gr(K_R)---a natural extension of propositional modal logic K_R by counting expressions---which plays an important role in the area of knowledge representation. The algorithm employs a tableaux approach and is the first known algorithm which meets the lower bound for the complexity of the problem. Thus, we exactly fix the complexity of the problem and refute an ExpTime-hardness conjecture. We extend the results to the logic Gr(K_(R \\cap I)), which augments Gr(K_R) with inverse relations and intersection of accessibility relations. This establishes a kind of ``theoretical benchmark'' that all algorithmic approaches can be measured against.",
        "published": "2000-05-08T14:51:58Z",
        "link": "http://arxiv.org/abs/cs/0005009v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "F.4.1"
        ]
    },
    {
        "title": "An Average Analysis of Backtracking on Random Constraint Satisfaction   Problems",
        "authors": [
            "Ke Xu",
            "Wei Li"
        ],
        "summary": "In this paper we propose a random CSP model, called Model GB, which is a natural generalization of standard Model B. It is proved that Model GB in which each constraint is easy to satisfy exhibits non-trivial behaviour (not trivially satisfiable or unsatisfiable) as the number of variables approaches infinity. A detailed analysis to obtain an asymptotic estimate (good to 1+o(1)) of the average number of nodes in a search tree used by the backtracking algorithm on Model GB is also presented. It is shown that the average number of nodes required for finding all solutions or proving that no solution exists grows exponentially with the number of variables. So this model might be an interesting distribution for studying the nature of hard instances and evaluating the performance of CSP algorithms. In addition, we further investigate the behaviour of the average number of nodes as r (the ratio of constraints to variables) varies. The results indicate that as r increases, random CSP instances get easier and easier to solve, and the base for the average number of nodes that is exponential in r tends to 1 as r approaches infinity. Therefore, although the average number of nodes used by the backtracking algorithm on random CSP is exponential, many CSP instances will be very easy to solve when r is sufficiently large.",
        "published": "2000-05-09T02:12:58Z",
        "link": "http://arxiv.org/abs/cs/0005011v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "The SAT Phase Transition",
        "authors": [
            "Ke Xu",
            "Wei Li"
        ],
        "summary": "Phase transition is an important feature of SAT problem. For random k-SAT model, it is proved that as r (ratio of clauses to variables) increases, the structure of solutions will undergo a sudden change like satisfiability phase transition when r reaches a threshold point. This phenomenon shows that the satisfying truth assignments suddenly shift from being relatively different from each other to being very similar to each other.",
        "published": "2000-05-22T04:45:53Z",
        "link": "http://arxiv.org/abs/cs/0005024v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "F.2.m; I.2.8"
        ]
    },
    {
        "title": "On the computational capabilities of physical systems part I: the   impossibility of infallible computation",
        "authors": [
            "David H. Wolpert"
        ],
        "summary": "In this first of two papers, strong limits on the accuracy of physical computation are established. First it is proven that there cannot be a physical computer C to which one can pose any and all computational tasks concerning the physical universe. Next it is proven that no physical computer C can correctly carry out any computational task in the subset of such tasks that can be posed to C. As a particular example, this means that there cannot be a physical computer that can, for any physical system external to that computer, take the specification of that external system's state as input and then correctly predict its future state before that future state actually occurs. The results also mean that there cannot exist an infallible, general-purpose observation apparatus, and that there cannot be an infallible, general-purpose control apparatus. These results do not rely on systems that are infinite, and/or non-classical, and/or obey chaotic dynamics. They also hold even if one uses an infinitely fast, infinitely dense computer, with computational powers greater than that of a Turing Machine.",
        "published": "2000-05-23T00:51:11Z",
        "link": "http://arxiv.org/abs/physics/0005058v1",
        "categories": [
            "physics.comp-ph",
            "cond-mat.stat-mech",
            "cs.CC",
            "math-ph",
            "math.MP",
            "physics.gen-ph"
        ]
    },
    {
        "title": "On the computational capabilities of physical systems part II:   relationship with conventional computer science",
        "authors": [
            "David H. Wolpert"
        ],
        "summary": "In the first of this pair of papers, it was proven that that no physical computer can correctly carry out all computational tasks that can be posed to it. The generality of this result follows from its use of a novel definition of computation, ``physical computation''. This second paper of the pair elaborates the mathematical structure and impossibility results associated with physical computation. Analogues of Chomsky hierarcy results concerning universal Turing Machines and the Halting theorem are derived, as are results concerning the (im)possibility of certain kinds of error-correcting codes. In addition, an analogue of algorithmic information complexity, ``prediction complexity'', is elaborated. A task-independent bound is derived on how much the prediction complexity of a computational task can differ for two different universal physical computers used to solve that task, a bound similar to the ``encoding'' bound governing how much the algorithm information complexity of a Turing machine calculation can differ for two universal Turing machines. Finally, it is proven that either the Hamiltonian of our universe proscribes a certain type of computation, or prediction complexity is unique (unlike algorithmic information complexity).",
        "published": "2000-05-23T00:58:41Z",
        "link": "http://arxiv.org/abs/physics/0005059v1",
        "categories": [
            "physics.comp-ph",
            "cond-mat.stat-mech",
            "cs.CC",
            "math.OC",
            "physics.gen-ph"
        ]
    },
    {
        "title": "Interaction in Quantum Communication Complexity",
        "authors": [
            "Ashwin Nayak",
            "Amnon Ta-Shma",
            "David Zuckerman"
        ],
        "summary": "One of the most intriguing facts about communication using quantum states is that these states cannot be used to transmit more classical bits than the number of qubits used, yet there are ways of conveying information with exponentially fewer qubits than possible classically. Moreover, these methods have a very simple structure---they involve little interaction between the communicating parties. We look more closely at the ways in which information encoded in quantum states may be manipulated, and consider the question as to whether every classical protocol may be transformed to a ``simpler'' quantum protocol of similar efficiency. By a simpler protocol, we mean a protocol that uses fewer message exchanges. We show that for any constant k, there is a problem such that its k+1 message classical communication complexity is exponentially smaller than its k message quantum communication complexity, thus answering the above question in the negative. Our result builds on two primitives, local transitions in bi-partite states (based on previous work) and average encoding which may be of significance in other applications as well.",
        "published": "2000-05-25T14:58:17Z",
        "link": "http://arxiv.org/abs/quant-ph/0005106v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Evolution of Biological Complexity",
        "authors": [
            "Christoph Adami",
            "Charles Ofria",
            "Travis C. Collier"
        ],
        "summary": "In order to make a case for or against a trend in the evolution of complexity in biological evolution, complexity needs to be both rigorously defined and measurable. A recent information-theoretic (but intuitively evident) definition identifies genomic complexity with the amount of information a sequence stores about its environment. We investigate the evolution of genomic complexity in populations of digital organisms and monitor in detail the evolutionary transitions that increase complexity. We show that because natural selection forces genomes to behave as a natural ``Maxwell Demon'', within a fixed environment genomic complexity is forced to increase.",
        "published": "2000-05-26T17:23:31Z",
        "link": "http://arxiv.org/abs/physics/0005074v1",
        "categories": [
            "physics.bio-ph",
            "cond-mat.stat-mech",
            "cs.CC",
            "nlin.AO",
            "physics.data-an",
            "q-bio.PE"
        ]
    },
    {
        "title": "Computational Complexity and Phase Transitions",
        "authors": [
            "Gabriel Istrate"
        ],
        "summary": "Phase transitions in combinatorial problems have recently been shown to be useful in locating \"hard\" instances of combinatorial problems. The connection between computational complexity and the existence of phase transitions has been addressed in Statistical Mechanics and Artificial Intelligence, but not studied rigorously.   We take a step in this direction by investigating the existence of sharp thresholds for the class of generalized satisfiability problems defined by Schaefer. In the case when all constraints are clauses we give a complete characterization of such problems that have a sharp threshold.   While NP-completeness does not imply (even in this restricted case) the existence of a sharp threshold, it \"almost implies\" this, since clausal generalized satisfiability problems that lack a sharp threshold are either   1. polynomial time solvable, or   2. predicted, with success probability lower bounded by some positive constant by across all the probability range, by a single, trivial procedure.",
        "published": "2000-05-31T04:05:58Z",
        "link": "http://arxiv.org/abs/cs/0005032v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Statistical mechanics perspective on the phase transition in vertex   covering finite-connectivity random graphs",
        "authors": [
            "Alexander K. Hartmann",
            "Martin Weigt"
        ],
        "summary": "The vertex-cover problem is studied for random graphs $G_{N,cN}$ having $N$ vertices and $cN$ edges. Exact numerical results are obtained by a branch-and-bound algorithm. It is found that a transition in the coverability at a $c$-dependent threshold $x=x_c(c)$ appears, where $xN$ is the cardinality of the vertex cover. This transition coincides with a sharp peak of the typical numerical effort, which is needed to decide whether there exists a cover with $xN$ vertices or not. For small edge concentrations $c\\ll 0.5$, a cluster expansion is performed, giving very accurate results in this regime. These results are extended using methods developed in statistical physics. The so called annealed approximation reproduces a rigorous bound on $x_c(c)$ which was known previously. The main part of the paper contains an application of the replica method. Within the replica symmetric ansatz the threshold $x_c(c)$ and the critical backbone size $b_c(c)$ can be calculated. For $c<e/2$ the results show an excellent agreement with the numerical findings. At average vertex degree $2c=e$, an instability of the simple replica symmetric solution occurs.",
        "published": "2000-06-21T09:36:34Z",
        "link": "http://arxiv.org/abs/cond-mat/0006316v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "The Quantum Complexity of Set Membership",
        "authors": [
            "Jaikumar Radhakrishnan",
            "Pranab Sen",
            "S. Venkatesh"
        ],
        "summary": "We study the quantum complexity of the static set membership problem: given a subset S (|S| \\leq n) of a universe of size m (m \\gg n), store it as a table of bits so that queries of the form `Is x \\in S?' can be answered. The goal is to use a small table and yet answer queries using few bitprobes. This problem was considered recently by Buhrman, Miltersen, Radhakrishnan and Venkatesh, where lower and upper bounds were shown for this problem in the classical deterministic and randomized models. In this paper, we formulate this problem in the \"quantum bitprobe model\" and show tradeoff results between space and time.In this model, the storage scheme is classical but the query scheme is quantum.We show, roughly speaking, that similar lower bounds hold in the quantum model as in the classical model, which imply that the classical upper bounds are more or less tight even in the quantum case. Our lower bounds are proved using linear algebraic techniques.",
        "published": "2000-07-07T16:38:30Z",
        "link": "http://arxiv.org/abs/quant-ph/0007021v4",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "A Moment of Perfect Clarity I: The Parallel Census Technique",
        "authors": [
            "Christian Glasser",
            "Lane A. Hemaspaandra"
        ],
        "summary": "We discuss the history and uses of the parallel census technique---an elegant tool in the study of certain computational objects having polynomially bounded census functions. A sequel will discuss advances (including Cai, Naik, and Sivakumar [CNS95] and Glasser [Gla00]), some related to the parallel census technique and some due to other approaches, in the complexity-class collapses that follow if NP has sparse hard sets under reductions weaker than (full) truth-table reductions.",
        "published": "2000-07-13T22:14:04Z",
        "link": "http://arxiv.org/abs/cs/0007025v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.1.2"
        ]
    },
    {
        "title": "Efficient cache use for stencil operations on structured discretization   grids",
        "authors": [
            "Michael A. Frumkin",
            "Rob F. Van der Wijngaart"
        ],
        "summary": "We derive tight bounds on cache misses for evaluation of explicit stencil operators on structured grids. Our lower bound is based on the isoperimetrical property of the discrete octahedron. Our upper bound is based on good surface to volume ratio of a parallelepiped spanned by a reduced basis of the inter- ference lattice of a grid. Measurements show that our algorithm typically reduces the number of cache misses by factor of three relative to a compiler optimized code. We show that stencil calculations on grids whose interference lattice have a short vector feature abnormally high numbers of cache misses. We call such grids unfavorable and suggest to avoid these in computations by appropriate padding. By direct measurements on MIPS R10000 we show a good correlation of abnormally high cache misses and unfavorable three-dimensional grids.",
        "published": "2000-07-14T17:44:41Z",
        "link": "http://arxiv.org/abs/cs/0007027v1",
        "categories": [
            "cs.PF",
            "cs.CC",
            "C.4;B.8"
        ]
    },
    {
        "title": "Base Encryption: Dynamic algorithms, Keys, and Symbol Set",
        "authors": [
            "Po-Han Lin"
        ],
        "summary": "All the current modern encryption algorithms utilize fixed symbols for plaintext and cyphertext. What I mean by fixed is that there is a set and limited number of symbols to represent the characters, numbers, and punctuations. In addition, they are usually the same (the plaintext symbols have the same and equivalent counterpart in the cyphertext symbols). Almost all the encryption algorithms rely on a predefined keyspace and length for the encryption/decription keys, and it is usually fixed (number of bits). In addition, the algorithms used by the encryptions are static. There is a predefined number of operatiors, and a predefined order (loops included) of operations. The algorithm stays the same, and the plaintext and cyphertext along with the key are churned through this cypherblock.   Base Encryption does the opposite: It utilizes the novel concepts of base conversion, symbol remapping, and dynamic algorithms (dynamic operators and dynamic operations). Base Encryption solves the weakness in todays encryption schemes, namely... Fixed symbols (base) Fixed keylengths Fixed algorithms (fixed number of operations and operators)   Unique features... Immune from plain-text-attacks. Immune from brute-force-attacks. Can utilize throwaway algorithms (as opposed to throw away keys). Plug-And-Play engine (other cyphers can be augmentated to it)",
        "published": "2000-07-18T16:44:14Z",
        "link": "http://arxiv.org/abs/cs/0007028v1",
        "categories": [
            "cs.CR",
            "cs.CC",
            "E, E.3"
        ]
    },
    {
        "title": "A Numerical Study of the Performance of a Quantum Adiabatic Evolution   Algorithm for Satisfiability",
        "authors": [
            "Edward Farhi",
            "Jeffrey Goldstone",
            "Sam Gutmann"
        ],
        "summary": "Quantum computation by adiabatic evolution, as described in quant-ph/0001106, will solve satisfiability problems if the running time is long enough. In certain special cases (that are classically easy) we know that the quantum algorithm requires a running time that grows as a polynomial in the number of bits. In this paper we present numerical results on randomly generated instances of an NP-complete problem and of a problem that can be solved classically in polynomial time. We simulate a quantum computer (of up to 16 qubits) by integrating the Schrodinger equation on a conventional computer. For both problems considered, for the set of instances studied, the required running time appears to grow slowly as a function of the number of bits.",
        "published": "2000-07-19T20:47:25Z",
        "link": "http://arxiv.org/abs/quant-ph/0007071v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "On the Average Similarity Degree between Solutions of Random k-SAT and   Random CSPs",
        "authors": [
            "Ke Xu",
            "Wei Li"
        ],
        "summary": "To study the structure of solutions for random k-SAT and random CSPs, this paper introduces the concept of average similarity degree to characterize how solutions are similar to each other. It is proved that under certain conditions, as r (i.e. the ratio of constraints to variables) increases, the limit of average similarity degree when the number of variables approaches infinity exhibits phase transitions at a threshold point, shifting from a smaller value to a larger value abruptly. For random k-SAT this phenomenon will occur when k>4 . It is further shown that this threshold point is also a singular point with respect to r in the asymptotic estimate of the second moment of the number of solutions. Finally, we discuss how this work is helpful to understand the hardness of solving random instances and a possible application of it to the design of search algorithms.",
        "published": "2000-08-11T08:10:25Z",
        "link": "http://arxiv.org/abs/cs/0008008v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DM",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "Quantum Algorithms for Weighing Matrices and Quadratic Residues",
        "authors": [
            "Wim van Dam"
        ],
        "summary": "In this article we investigate how we can employ the structure of combinatorial objects like Hadamard matrices and weighing matrices to device new quantum algorithms. We show how the properties of a weighing matrix can be used to construct a problem for which the quantum query complexity is ignificantly lower than the classical one. It is pointed out that this scheme captures both Bernstein & Vazirani's inner-product protocol, as well as Grover's search algorithm.   In the second part of the article we consider Paley's construction of Hadamard matrices, which relies on the properties of quadratic characters over finite fields. We design a query problem that uses the Legendre symbol chi (which indicates if an element of a finite field F_q is a quadratic residue or not). It is shown how for a shifted Legendre function f_s(i)=chi(i+s), the unknown s in F_q can be obtained exactly with only two quantum calls to f_s. This is in sharp contrast with the observation that any classical, probabilistic procedure requires more than log(q) + log((1-e)/2) queries to solve the same problem.",
        "published": "2000-08-11T23:49:07Z",
        "link": "http://arxiv.org/abs/quant-ph/0008059v3",
        "categories": [
            "quant-ph",
            "cs.CC",
            "math.CO"
        ]
    },
    {
        "title": "Phutball Endgames are Hard",
        "authors": [
            "Erik D. Demaine",
            "Martin L. Demaine",
            "David Eppstein"
        ],
        "summary": "We show that, in John Conway's board game Phutball (or Philosopher's Football), it is NP-complete to determine whether the current player has a move that immediately wins the game. In contrast, the similar problems of determining whether there is an immediately winning move in checkers, or a move that kings a man, are both solvable in polynomial time.",
        "published": "2000-08-23T21:56:15Z",
        "link": "http://arxiv.org/abs/cs/0008025v2",
        "categories": [
            "cs.CC",
            "cs.GT",
            "F.1.3,K.8.0"
        ]
    },
    {
        "title": "The definition of a random sequence of qubits: from Noncommutative   Algorithmic Probability Theory to Quantum Algorithmic Information Theory and   back",
        "authors": [
            "Gavriel Segre"
        ],
        "summary": "The issue of defining a random sequence of qubits is studied in the framework of Algorithmic Free Probability Theory.Its connection with Quantum Algorithmic Information Theory is shown",
        "published": "2000-09-04T15:21:03Z",
        "link": "http://arxiv.org/abs/quant-ph/0009009v5",
        "categories": [
            "quant-ph",
            "cs.CC",
            "math-ph",
            "math.MP"
        ]
    },
    {
        "title": "Succinct quantum proofs for properties of finite groups",
        "authors": [
            "John Watrous"
        ],
        "summary": "In this paper we consider a quantum computational variant of nondeterminism based on the notion of a quantum proof, which is a quantum state that plays a role similar to a certificate in an NP-type proof. Specifically, we consider quantum proofs for properties of black-box groups, which are finite groups whose elements are encoded as strings of a given length and whose group operations are performed by a group oracle. We prove that for an arbitrary group oracle there exist succinct (polynomial-length) quantum proofs for the Group Non-Membership problem that can be checked with small error in polynomial time on a quantum computer. Classically this is impossible--it is proved that there exists a group oracle relative to which this problem does not have succinct proofs that can be checked classically with bounded error in polynomial time (i.e., the problem is not in MA relative to the group oracle constructed). By considering a certain subproblem of the Group Non-Membership problem we obtain a simple proof that there exists an oracle relative to which BQP is not contained in MA. Finally, we show that quantum proofs for non-membership and classical proofs for various other group properties can be combined to yield succinct quantum proofs for other group properties not having succinct proofs in the classical setting, such as verifying that a number divides the order of a group and verifying that a group is not a simple group.",
        "published": "2000-09-08T00:53:35Z",
        "link": "http://arxiv.org/abs/cs/0009002v1",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.3;F.1.2"
        ]
    },
    {
        "title": "Typical solution time for a vertex-covering algorithm on   finite-connectivity random graphs",
        "authors": [
            "Martin Weigt",
            "Alexander K. Hartmann"
        ],
        "summary": "In this letter, we analytically describe the typical solution time needed by a backtracking algorithm to solve the vertex-cover problem on finite-connectivity random graphs. We find two different transitions: The first one is algorithm-dependent and marks the dynamical transition from linear to exponential solution times. The second one gives the maximum computational complexity, and is found exactly at the threshold where the system undergoes an algorithm-independent phase transition in its solvability. Analytical results are corroborated by numerical simulations.",
        "published": "2000-09-27T08:51:55Z",
        "link": "http://arxiv.org/abs/cond-mat/0009417v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Low Ambiguity in Strong, Total, Associative, One-Way Functions",
        "authors": [
            "Christopher M. Homan"
        ],
        "summary": "Rabi and Sherman present a cryptographic paradigm based on associative, one-way functions that are strong (i.e., hard to invert even if one of their arguments is given) and total. Hemaspaandra and Rothe proved that such powerful one-way functions exist exactly if (standard) one-way functions exist, thus showing that the associative one-way function approach is as plausible as previous approaches. In the present paper, we study the degree of ambiguity of one-way functions. Rabiand Sherman showed that no associative one-way function (over a universe having at least two elements) can be unambiguous (i.e., one-to-one). Nonetheless, we prove that if standard, unambiguous, one-way functions exist, then there exist strong, total, associative, one-way functions that are $\\mathcal{O}(n)$-to-one. This puts a reasonable upper bound on the ambiguity.",
        "published": "2000-10-02T05:25:20Z",
        "link": "http://arxiv.org/abs/cs/0010005v1",
        "categories": [
            "cs.CC",
            "f.1.3"
        ]
    },
    {
        "title": "The Light Lexicographic path Ordering",
        "authors": [
            "E. A. Cichon",
            "J-Y. Marion"
        ],
        "summary": "We introduce syntactic restrictions of the lexicographic path ordering to obtain the Light Lexicographic Path Ordering. We show that the light lexicographic path ordering leads to a characterisation of the functions computable in space bounded by a polynomial in the size of the inputs.",
        "published": "2000-10-03T15:38:46Z",
        "link": "http://arxiv.org/abs/cs/0010008v1",
        "categories": [
            "cs.PL",
            "cs.CC",
            "F.1.3; I.2.2"
        ]
    },
    {
        "title": "If P \\neq NP then Some Strongly Noninvertible Functions are Invertible",
        "authors": [
            "Lane A. Hemaspaandra",
            "Kari Pasanen",
            "Jörg Rothe"
        ],
        "summary": "Rabi, Rivest, and Sherman alter the standard notion of noninvertibility to a new notion they call strong noninvertibility, and show -- via explicit cryptographic protocols for secret-key agreement ([RS93,RS97] attribute this to Rivest and Sherman) and digital signatures [RS93,RS97] -- that strongly noninvertible functions would be very useful components in protocol design. Their definition of strong noninvertibility has a small twist (``respecting the argument given'') that is needed to ensure cryptographic usefulness. In this paper, we show that this small twist has a large, unexpected consequence: Unless P=NP, some strongly noninvertible functions are invertible.",
        "published": "2000-10-06T18:45:21Z",
        "link": "http://arxiv.org/abs/cs/0010011v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "On Exponential-Time Completeness of the Circularity Problem for   Attribute Grammars",
        "authors": [
            "Pei-Chi Wu"
        ],
        "summary": "Attribute grammars (AGs) are a formal technique for defining semantics of programming languages. Existing complexity proofs on the circularity problem of AGs are based on automata theory, such as writing pushdown acceptor and alternating Turing machines. They reduced the acceptance problems of above automata, which are exponential-time (EXPTIME) complete, to the AG circularity problem. These proofs thus show that the circularity problem is EXPTIME-hard, at least as hard as the most difficult problems in EXPTIME. However, none has given a proof for the EXPTIME-completeness of the problem. This paper first presents an alternating Turing machine for the circularity problem. The alternating Turing machine requires polynomial space. Thus, the circularity problem is in EXPTIME and is then EXPTIME-complete.",
        "published": "2000-10-10T09:26:35Z",
        "link": "http://arxiv.org/abs/cs/0010015v1",
        "categories": [
            "cs.PL",
            "cs.CC",
            "D.3.1; D.3.4; F.2.2; F.4.2"
        ]
    },
    {
        "title": "Towards Understanding the Predictability of Stock Markets from the   Perspective of Computational Complexity",
        "authors": [
            "James Aspnes",
            "David F. Fischer",
            "Michael J. Fischer",
            "Ming-Yang Kao",
            "Alok Kumar"
        ],
        "summary": "This paper initiates a study into the century-old issue of market predictability from the perspective of computational complexity. We develop a simple agent-based model for a stock market where the agents are traders equipped with simple trading strategies, and their trades together determine the stock prices. Computer simulations show that a basic case of this model is already capable of generating price graphs which are visually similar to the recent price movements of high tech stocks. In the general model, we prove that if there are a large number of traders but they employ a relatively small number of strategies, then there is a polynomial-time algorithm for predicting future price movements with high accuracy. On the other hand, if the number of strategies is large, market prediction becomes complete in two new computational complexity classes CPP and BCPP, which are between P^NP[O(log n)] and PP. These computational completeness results open up a novel possibility that the price graph of an actual stock could be sufficiently deterministic for various prediction goals but appear random to all polynomial-time prediction algorithms.",
        "published": "2000-10-14T14:01:17Z",
        "link": "http://arxiv.org/abs/cs/0010021v2",
        "categories": [
            "cs.CE",
            "cs.CC",
            "F.1.3;F.2.2;G.2.1;G.2.3;G.3;J.1"
        ]
    },
    {
        "title": "Oracle Complexity and Nontransitivity in Pattern Recognition",
        "authors": [
            "Vadim Bulitko"
        ],
        "summary": "Different mathematical models of recognition processes are known. In the present paper we consider a pattern recognition algorithm as an oracle computation on a Turing machine. Such point of view seems to be useful in pattern recognition as well as in recursion theory. Use of recursion theory in pattern recognition shows connection between a recognition algorithm comparison problem and complexity problems of oracle computation. That is because in many cases we can take into account only the number of sign computations or in other words volume of oracle information needed. Therefore, the problem of recognition algorithm preference can be formulated as a complexity optimization problem of oracle computation. Furthermore, introducing a certain \"natural\" preference relation on a set of recognizing algorithms, we discover it to be nontransitive. This relates to the well known nontransitivity paradox in probability theory.   Keywords: Pattern Recognition, Recursion Theory, Nontransitivity, Preference Relation",
        "published": "2000-10-16T07:42:23Z",
        "link": "http://arxiv.org/abs/cs/0010023v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "cs.CV",
            "cs.DS",
            "F.2.2; F.4.1; I.2.10; I.5.2"
        ]
    },
    {
        "title": "Simplest random K-satisfiability problem",
        "authors": [
            "F. Ricci-Tersenghi",
            "M. Weigt",
            "R. Zecchina"
        ],
        "summary": "We study a simple and exactly solvable model for the generation of random satisfiability problems. These consist of $\\gamma N$ random boolean constraints which are to be satisfied simultaneously by $N$ logical variables. In statistical-mechanics language, the considered model can be seen as a diluted p-spin model at zero temperature. While such problems become extraordinarily hard to solve by local search methods in a large region of the parameter space, still at least one solution may be superimposed by construction. The statistical properties of the model can be studied exactly by the replica method and each single instance can be analyzed in polynomial time by a simple global solution method. The geometrical/topological structures responsible for dynamic and static phase transitions as well as for the onset of computational complexity in local search method are thoroughly analyzed. Numerical analysis on very large samples allows for a precise characterization of the critical scaling behaviour.",
        "published": "2000-11-10T09:42:40Z",
        "link": "http://arxiv.org/abs/cond-mat/0011181v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Designing Proxies for Stock Market Indices is Computationally Hard",
        "authors": [
            "Ming-Yang Kao",
            "Stephen R. Tate"
        ],
        "summary": "In this paper, we study the problem of designing proxies (or portfolios) for various stock market indices based on historical data. We use four different methods for computing market indices, all of which are formulas used in actual stock market analysis. For each index, we consider three criteria for designing the proxy: the proxy must either track the market index, outperform the market index, or perform within a margin of error of the index while maintaining a low volatility. In eleven of the twelve cases (all combinations of four indices with three criteria except the problem of sacrificing return for less volatility using the price-relative index) we show that the problem is NP-hard, and hence most likely intractable.",
        "published": "2000-11-13T02:51:49Z",
        "link": "http://arxiv.org/abs/cs/0011016v1",
        "categories": [
            "cs.CE",
            "cs.CC",
            "F.2.2;G.2.3;J.4"
        ]
    },
    {
        "title": "Efficient Quantum Algorithms for Shifted Quadratic Character Problems",
        "authors": [
            "Wim van Dam",
            "Sean Hallgren"
        ],
        "summary": "We introduce the Shifted Legendre Symbol Problem and some variants along with efficient quantum algorithms to solve them. The problems and their algorithms are different from previous work on quantum computation in that they do not appear to fit into the framework of the Hidden Subgroup Problem. The classical complexity of the problem is unknown despite the various results on the irregularity of Legendre Sequences.",
        "published": "2000-11-15T23:56:15Z",
        "link": "http://arxiv.org/abs/quant-ph/0011067v2",
        "categories": [
            "quant-ph",
            "cs.CC",
            "math.NT"
        ]
    },
    {
        "title": "A Moment of Perfect Clarity II: Consequences of Sparse Sets Hard for NP   with Respect to Weak Reductions",
        "authors": [
            "Christian Glasser",
            "Lane A. Hemaspaandra"
        ],
        "summary": "This paper discusses advances, due to the work of Cai, Naik, and Sivakumar and Glasser, in the complexity class collapses that follow if NP has sparse hard sets under reductions weaker than (full) truth-table reductions.",
        "published": "2000-11-16T17:12:20Z",
        "link": "http://arxiv.org/abs/cs/0011019v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.1.3; F.1.2"
        ]
    },
    {
        "title": "Algorithmic Theories of Everything",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "The probability distribution P from which the history of our universe is sampled represents a theory of everything or TOE. We assume P is formally describable. Since most (uncountably many) distributions are not, this imposes a strong inductive bias. We show that P(x) is small for any universe x lacking a short description, and study the spectrum of TOEs spanned by two Ps, one reflecting the most compact constructive descriptions, the other the fastest way of computing everything. The former derives from generalizations of traditional computability, Solomonoff's algorithmic probability, Kolmogorov complexity, and objects more random than Chaitin's Omega, the latter from Levin's universal search and a natural resource-oriented postulate: the cumulative prior probability of all x incomputable within time t by this optimal algorithm should be 1/t. Between both Ps we find a universal cumulatively enumerable measure that dominates traditional enumerable measures; any such CEM must assign low probability to any universe lacking a short enumerating program. We derive P-specific consequences for evolving observers, inductive reasoning, quantum physics, philosophy, and the expected duration of our universe.",
        "published": "2000-11-30T14:23:55Z",
        "link": "http://arxiv.org/abs/quant-ph/0011122v2",
        "categories": [
            "quant-ph",
            "cs.AI",
            "cs.CC",
            "cs.LG",
            "hep-th",
            "math-ph",
            "math.MP",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Towards a Universal Theory of Artificial Intelligence based on   Algorithmic Probability and Sequential Decision Theory",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown distribution. We unify both theories and give strong arguments that the resulting universal AIXI model behaves optimal in any computable environment. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXI^tl, which is still superior to any other time t and space l bounded agent. The computation time of AIXI^tl is of the order t x 2^l.",
        "published": "2000-12-16T09:38:13Z",
        "link": "http://arxiv.org/abs/cs/0012011v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "I.2; I.2.3; I.2.6; I.2.8; F.1.3; F.2"
        ]
    },
    {
        "title": "Towards Robust Quantum Computation",
        "authors": [
            "Debbie W. Leung"
        ],
        "summary": "Quantum computation is a subject of much theoretical promise, but has not been realized in large scale, despite the discovery of fault-tolerant procedures to overcome decoherence. Part of the reason is that the theoretically modest requirements still present daunting experimental challenges. The goal of this Dissertation is to reduce various resources required for robust quantum computation, focusing on quantum error correcting codes and solution NMR quantum computation. A variety of techniques have been developed, including high rate quantum codes for amplitude damping, relaxed criteria for quantum error correction, systematic construction of fault-tolerant gates, recipes for quantum process tomography, techniques in bulk thermal state computation, and efficient decoupling techniques to implement selective coupled logic gates. A detailed experimental study of a quantum error correcting code in NMR is also presented. The Dissertation clarifies and extends results previously reported in quant-ph/9610043, quant-ph/9704002, quant-ph/9811068, quant-ph/9904100, quant-ph/9906112, quant-ph/0002039. Additionally, a procedure for quantum process tomography using maximally entangled states, and a review on NMR quantum computation are included.",
        "published": "2000-12-20T22:35:14Z",
        "link": "http://arxiv.org/abs/cs/0012017v1",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.m"
        ]
    },
    {
        "title": "The Tale of One-way Functions",
        "authors": [
            "Leonid A. Levin"
        ],
        "summary": "The existence of one-way functions is arguably the most important problem in computer theory. The article discusses and refines a number of concepts relevant to this problem. For instance, it gives the first combinatorial complete owf, i.e., a function which is one-way if any function is. There are surprisingly many subtleties in basic definitions. Some of these subtleties are discussed or hinted at in the literature and some are overlooked. Here, a unified approach is attempted.",
        "published": "2000-12-26T22:48:10Z",
        "link": "http://arxiv.org/abs/cs/0012023v5",
        "categories": [
            "cs.CR",
            "cs.CC",
            "F.1; F.0; G.3"
        ]
    },
    {
        "title": "Compositionality, Synonymy, and the Systematic Representation of Meaning",
        "authors": [
            "Shalom Lappin",
            "Wlodek Zadrozny"
        ],
        "summary": "In a recent issue of Linguistics and Philosophy Kasmi and Pelletier (1998) (K&P), and Westerstahl (1998) criticize Zadrozny's (1994) argument that any semantics can be represented compositionally. The argument is based upon Zadrozny's theorem that every meaning function m can be encoded by a function \\mu such that (i) for any expression E of a specified language L, m(E) can be recovered from \\mu(E), and (ii) \\mu is a homomorphism from the syntactic structures of L to interpretations of L.   In both cases, the primary motivation for the objections brought against Zadrozny's argument is the view that his encoding of the original meaning function does not properly reflect the synonymy relations posited for the language.   In this paper, we argue that these technical criticisms do not go through. In particular, we prove that \\mu properly encodes synonymy relations, i.e. if two expressions are synonymous, then their compositional meanings are identical. This corrects some misconceptions about the function \\mu, e.g. Janssen (1997).   We suggest that the reason that semanticists have been anxious to preserve compositionality as a significant constraint on semantic theory is that it has been mistakenly regarded as a condition that must be satisfied by any theory that sustains a systematic connection between the meaning of an expression and the meanings of its parts. Recent developments in formal and computational semantics show that systematic theories of meanings need not be compositional.",
        "published": "2000-01-09T23:54:15Z",
        "link": "http://arxiv.org/abs/cs/0001006v1",
        "categories": [
            "cs.CL",
            "cs.LO",
            "I.2.7; F.4.1"
        ]
    },
    {
        "title": "Multi-Agent Only Knowing",
        "authors": [
            "Joseph Y. Halpern",
            "Gerhard Lakemeyer"
        ],
        "summary": "Levesque introduced a notion of ``only knowing'', with the goal of capturing certain types of nonmonotonic reasoning. Levesque's logic dealt with only the case of a single agent. Recently, both Halpern and Lakemeyer independently attempted to extend Levesque's logic to the multi-agent case. Although there are a number of similarities in their approaches, there are some significant differences. In this paper, we reexamine the notion of only knowing, going back to first principles. In the process, we simplify Levesque's completeness proof, and point out some problems with the earlier definitions. This leads us to reconsider what the properties of only knowing ought to be. We provide an axiom system that captures our desiderata, and show that it has a semantics that corresponds to it. The axiom system has an added feature of interest: it includes a modal operator for satisfiability, and thus provides a complete axiomatization for satisfiability in the logic K45.",
        "published": "2000-01-19T22:13:38Z",
        "link": "http://arxiv.org/abs/cs/0001015v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4, F.4.1"
        ]
    },
    {
        "title": "A Logic for SDSI's Linked Local Name Spaces",
        "authors": [
            "Joseph Y. Halpern",
            "Ron van der Meyden"
        ],
        "summary": "Abadi has introduced a logic to explicate the meaning of local names in SDSI, the Simple Distributed Security Infrastructure proposed by Rivest and Lampson. Abadi's logic does not correspond precisely to SDSI, however; it draws conclusions about local names that do not follow from SDSI's name resolution algorithm. Moreover, its semantics is somewhat unintuitive. This paper presents the Logic of Local Name Containment, which does not suffer from these deficiencies. It has a clear semantics and provides a tight characterization of SDSI name resolution. The semantics is shown to be closely related to that of logic programs, leading to an approach to the efficient implementation of queries concerning local names. A complete axiomatization of the logic is also provided.",
        "published": "2000-01-28T20:50:40Z",
        "link": "http://arxiv.org/abs/cs/0001026v1",
        "categories": [
            "cs.CR",
            "cs.LO",
            "D.4.6, K.6.5, C.2.0, F,4,1"
        ]
    },
    {
        "title": "Computing large and small stable models",
        "authors": [
            "Miroslaw Truszczynski"
        ],
        "summary": "In this paper, we focus on the problem of existence and computing of small and large stable models. We show that for every fixed integer k, there is a linear-time algorithm to decide the problem LSM (large stable models problem): does a logic program P have a stable model of size at least |P|-k. In contrast, we show that the problem SSM (small stable models problem) to decide whether a logic program P has a stable model of size at most k is much harder. We present two algorithms for this problem but their running time is given by polynomials of order depending on k. We show that the problem SSM is fixed-parameter intractable by demonstrating that it is W[2]-hard. This result implies that it is unlikely, an algorithm exists to compute stable models of size at most k that would run in time O(n^c), where c is a constant independent of k. We also provide an upper bound on the fixed-parameter complexity of the problem SSM by showing that it belongs to the class W[3].",
        "published": "2000-02-03T21:15:34Z",
        "link": "http://arxiv.org/abs/cs/0002001v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3;I.2.4"
        ]
    },
    {
        "title": "Stochastic Model Checking for Multimedia",
        "authors": [
            "Jeremy Bryans",
            "Howard Bowman",
            "John Derrick"
        ],
        "summary": "Modern distributed systems include a class of applications in which non-functional requirements are important. In particular, these applications include multimedia facilities where real time constraints are crucial to their correct functioning. In order to specify such systems it is necessary to describe that events occur at times given by probability distributions and stochastic automata have emerged as a useful technique by which such systems can be specified and verified.   However, stochastic descriptions are very general, in particular they allow the use of general probability distribution functions, and therefore their verification can be complex. In the last few years, model checking has emerged as a useful verification tool for large systems.   In this paper we describe two model checking algorithms for stochastic automata. These algorithms consider how properties written in a simple probabilistic real-time logic can be checked against a given stochastic automaton.",
        "published": "2000-02-04T18:42:13Z",
        "link": "http://arxiv.org/abs/cs/0002004v1",
        "categories": [
            "cs.MM",
            "cs.LO",
            "F.3.1; F.4.1; G.3"
        ]
    },
    {
        "title": "Computing and Comparing Semantics of Programs in Multi-valued Logics",
        "authors": [
            "Y. Loyer",
            "N. Spyratos",
            "D. Stamate"
        ],
        "summary": "The different semantics that can be assigned to a logic program correspond to different assumptions made concerning the atoms whose logical values cannot be inferred from the rules. Thus, the well founded semantics corresponds to the assumption that every such atom is false, while the Kripke-Kleene semantics corresponds to the assumption that every such atom is unknown. In this paper, we propose to unify and extend this assumption-based approach by introducing parameterized semantics for logic programs. The parameter holds the value that one assumes for all atoms whose logical values cannot be inferred from the rules. We work within multi-valued logic with bilattice structure, and we consider the class of logic programs defined by Fitting.   Following Fitting's approach, we define a simple operator that allows us to compute the parameterized semantics, and to compare and combine semantics obtained for different values of the parameter. The semantics proposed by Fitting corresponds to the value false. We also show that our approach captures and extends the usual semantics of conventional logic programs thereby unifying their computation.",
        "published": "2000-02-18T13:54:03Z",
        "link": "http://arxiv.org/abs/cs/0002013v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "H.0;I.2.3"
        ]
    },
    {
        "title": "Genetic Algorithms for Extension Search in Default Logic",
        "authors": [
            "P. Nicolas",
            "F. Saubion",
            "I. Stephan"
        ],
        "summary": "A default theory can be characterized by its sets of plausible conclusions, called its extensions. But, due to the theoretical complexity of Default Logic (Sigma_2p-complete), the problem of finding such an extension is very difficult if one wants to deal with non trivial knowledge bases. Based on the principle of natural selection, Genetic Algorithms have been quite successfully applied to combinatorial problems and seem useful for problems with huge search spaces and when no tractable algorithm is available. The purpose of this paper is to show that techniques issued from Genetic Algorithms can be used in order to build an efficient default reasoning system. After providing a formal description of the components required for an extension search based on Genetic Algorithms principles, we exhibit some experimental results.",
        "published": "2000-02-24T16:09:04Z",
        "link": "http://arxiv.org/abs/cs/0002015v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Computing Circumscriptive Databases by Integer Programming: Revisited   (Extended Abstract)",
        "authors": [
            "Ken Satoh",
            "Hidenori Okamoto"
        ],
        "summary": "In this paper, we consider a method of computing minimal models in circumscription using integer programming in propositional logic and first-order logic with domain closure axioms and unique name axioms. This kind of treatment is very important since this enable to apply various technique developed in operations research to nonmonotonic reasoning.   Nerode et al. (1995) are the first to propose a method of computing circumscription using integer programming. They claimed their method was correct for circumscription with fixed predicate, but we show that their method does not correctly reflect their claim. We show a correct method of computing all the minimal models not only with fixed predicates but also with varied predicates and we extend our method to compute prioritized circumscription as well.",
        "published": "2000-03-05T09:57:49Z",
        "link": "http://arxiv.org/abs/cs/0003007v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Conditional indifference and conditional preservation",
        "authors": [
            "Gabriele Kern-Isberner"
        ],
        "summary": "The idea of preserving conditional beliefs emerged recently as a new paradigm apt to guide the revision of epistemic states. Conditionals are substantially different from propositional beliefs and need specific treatment. In this paper, we present a new approach to conditionals, capturing particularly well their dynamic part as revision policies. We thoroughly axiomatize a principle of conditional preservation as an indifference property with respect to conditional structures of worlds. This principle is developed in a semi-quantitative setting, so as to reveal its fundamental meaning for belief revision in quantitative as well as in qualitative frameworks. In fact, it is shown to cover other proposed approaches to conditional preservation.",
        "published": "2000-03-06T12:08:06Z",
        "link": "http://arxiv.org/abs/cs/0003009v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.0; I.2.3; I.2.6"
        ]
    },
    {
        "title": "Automatic Belief Revision in SNePS",
        "authors": [
            "Stuart C. Shapiro",
            "Frances L. Johnson"
        ],
        "summary": "SNePS is a logic- and network- based knowledge representation, reasoning, and acting system, based on a monotonic, paraconsistent, first-order term logic, with compositional intensional semantics. It has an ATMS-style facility for belief contraction, and an acting component, including a well-defined syntax and semantics for primitive and composite acts, as well as for ``rules'' that allow for acting in support of reasoning and reasoning in support of acting. SNePS has been designed to support natural language competent cognitive agents.   When the current version of SNePS detects an explicit contradiction, it interacts with the user, providing information that helps the user decide what to remove from the knowledge base in order to remove the contradiction. The forthcoming SNePS 2.6 will also do automatic belief contraction if the information in the knowledge base warrents it.",
        "published": "2000-03-06T16:55:16Z",
        "link": "http://arxiv.org/abs/cs/0003011v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "A flexible framework for defeasible logics",
        "authors": [
            "G. Antoniou",
            "D. Billigton",
            "G. Governatori",
            "M. J. Maher"
        ],
        "summary": "Logics for knowledge representation suffer from over-specialization: while each logic may provide an ideal representation formalism for some problems, it is less than optimal for others. A solution to this problem is to choose from several logics and, when necessary, combine the representations. In general, such an approach results in a very difficult problem of combination. However, if we can choose the logics from a uniform framework then the problem of combining them is greatly simplified. In this paper, we develop such a framework for defeasible logics. It supports all defeasible logics that satisfy a strong negation principle. We use logic meta-programs as the basis for the framework.",
        "published": "2000-03-07T01:24:26Z",
        "link": "http://arxiv.org/abs/cs/0003013v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; D.1.6"
        ]
    },
    {
        "title": "The lexicographic closure as a revision process",
        "authors": [
            "Richard Booth"
        ],
        "summary": "The connections between nonmonotonic reasoning and belief revision are well-known. A central problem in the area of nonmonotonic reasoning is the problem of default entailment, i.e., when should an item of default information representing \"if A is true then, normally, B is true\" be said to follow from a given set of items of such information. Many answers to this question have been proposed but, surprisingly, virtually none have attempted any explicit connection to belief revision. The aim of this paper is to give an example of how such a connection can be made by showing how the lexicographic closure of a set of defaults may be conceptualised as a process of iterated revision by sets of sentences. Specifically we use the revision process of Nayak.",
        "published": "2000-03-07T11:44:35Z",
        "link": "http://arxiv.org/abs/cs/0003017v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "On the semantics of merging",
        "authors": [
            "Thomas Meyer"
        ],
        "summary": "Intelligent agents are often faced with the problem of trying to merge possibly conflicting pieces of information obtained from different sources into a consistent view of the world. We propose a framework for the modelling of such merging operations with roots in the work of Spohn (1988, 1991). Unlike most approaches we focus on the merging of epistemic states, not knowledge bases. We construct a number of plausible merging operations and measure them against various properties that merging operations ought to satisfy. Finally, we discuss the connection between merging and the use of infobases Meyer (1999) and Meyer et al. (2000).",
        "published": "2000-03-07T12:26:34Z",
        "link": "http://arxiv.org/abs/cs/0003015v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; I.2.4; I.2.11"
        ]
    },
    {
        "title": "Description of GADEL",
        "authors": [
            "I. Stephan",
            "F. Saubion",
            "P. Nicolas"
        ],
        "summary": "This article describes the first implementation of the GADEL system : a Genetic Algorithm for Default Logic. The goal of GADEL is to compute extensions in Reiter's default logic. It accepts every kind of finite propositional default theories and is based on evolutionary principles of Genetic Algorithms. Its first experimental results on certain instances of the problem show that this new approach of the problem can be successful.",
        "published": "2000-03-07T14:46:23Z",
        "link": "http://arxiv.org/abs/cs/0003018v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Extending Classical Logic with Inductive Definitions",
        "authors": [
            "Marc Denecker"
        ],
        "summary": "The goal of this paper is to extend classical logic with a generalized notion of inductive definition supporting positive and negative induction, to investigate the properties of this logic, its relationships to other logics in the area of non-monotonic reasoning, logic programming and deductive databases, and to show its application for knowledge representation by giving a typology of definitional knowledge.",
        "published": "2000-03-07T15:44:08Z",
        "link": "http://arxiv.org/abs/cs/0003019v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3;I.2.4;F.4.1"
        ]
    },
    {
        "title": "A Comparison of Logic Programming Approaches for Representation and   Solving of Constraint Satisfaction Problems",
        "authors": [
            "Nikolay Pelov",
            "Emmanuel De Mot",
            "Maurice Bruynooghe"
        ],
        "summary": "Many logic programming based approaches can be used to describe and solve combinatorial search problems. On the one hand there are definite programs and constraint logic programs that compute a solution as an answer substitution to a query containing the variables of the constraint satisfaction problem. On the other hand there are approaches based on stable model semantics, abduction, and first-order logic model generation that compute solutions as models of some theory. This paper compares these different approaches from point of view of knowledge representation (how declarative are the programs) and from point of view of performance (how good are they at solving typical problems).",
        "published": "2000-03-08T12:52:32Z",
        "link": "http://arxiv.org/abs/cs/0003026v1",
        "categories": [
            "cs.LO",
            "I.2.3: Logic programming, Nonmonotonic reasoning; I.2.4; F.4.1:\n  Logic and constraint programming; Experiments"
        ]
    },
    {
        "title": "Logic Programming for Describing and Solving Planning Problems",
        "authors": [
            "Maurice Bruynooghe"
        ],
        "summary": "A logic programming paradigm which expresses solutions to problems as stable models has recently been promoted as a declarative approach to solving various combinatorial and search problems, including planning problems. In this paradigm, all program rules are considered as constraints and solutions are stable models of the rule set. This is a rather radical departure from the standard paradigm of logic programming. In this paper we revisit abductive logic programming and argue that it allows a programming style which is as declarative as programming based on stable models. However, within abductive logic programming, one has two kinds of rules. On the one hand predicate definitions (which may depend on the abducibles) which are nothing else than standard logic programs (with their non-monotonic semantics when containing with negation); on the other hand rules which constrain the models for the abducibles. In this sense abductive logic programming is a smooth extension of the standard paradigm of logic programming, not a radical departure.",
        "published": "2000-03-08T14:00:35Z",
        "link": "http://arxiv.org/abs/cs/0003025v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "D.1.6; D.3.1; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Declarative Representation of Revision Strategies",
        "authors": [
            "Gerhard Brewka"
        ],
        "summary": "In this paper we introduce a nonmonotonic framework for belief revision in which reasoning about the reliability of different pieces of information based on meta-knowledge about the information is possible, and where revision strategies can be described declaratively. The approach is based on a Poole-style system for default reasoning in which entrenchment information is represented in the logical language. A notion of inference based on the least fixed point of a monotone operator is used to make sure that all theories possess a consistent set of conclusions.",
        "published": "2000-03-08T16:22:03Z",
        "link": "http://arxiv.org/abs/cs/0003035v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "DLV - A System for Declarative Problem Solving",
        "authors": [
            "Thomas Eiter",
            "Wolfgang Faber",
            "Christoph Koch",
            "Nicola Leone",
            "Gerald Pfeifer"
        ],
        "summary": "DLV is an efficient logic programming and non-monotonic reasoning (LPNMR) system with advanced knowledge representation mechanisms and interfaces to classic relational database systems.   Its core language is disjunctive datalog (function-free disjunctive logic programming) under the Answer Set Semantics with integrity constraints, both default and strong (or explicit) negation, and queries. Integer arithmetics and various built-in predicates are also supported.   In addition DLV has several frontends, namely brave and cautious reasoning, abductive diagnosis, consistency-based diagnosis, a subset of SQL3, planning with action languages, and logic programming with inheritance.",
        "published": "2000-03-08T18:17:33Z",
        "link": "http://arxiv.org/abs/cs/0003036v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "D.1.6; D.3.2; I.2.4; F.4.1"
        ]
    },
    {
        "title": "Implementing Integrity Constraints in an Existing Belief Revision System",
        "authors": [
            "Frances L. Johnson",
            "Stuart C. Shapiro"
        ],
        "summary": "SNePS is a mature knowledge representation, reasoning, and acting system that has long contained a belief revision subsystem, called SNeBR. SNeBR is triggered when an explicit contradiction is introduced into the SNePS belief space, either because of a user's new assertion, or because of a user's query. SNeBR then makes the user decide what belief to remove from the belief space in order to restore consistency, although it provides information to help the user in making that decision. We have recently added automatic belief revision to SNeBR, by which, under certain circumstances, SNeBR decides by itself which belief to remove, and then informs the user of the decision and its consequences. We have used the well-known belief revision integrity constraints as a guide in designing automatic belief revision, taking into account, however, that SNePS's belief space is not deductively closed, and that it would be infeasible to form the deductive closure in order to decide what belief to remove. This paper briefly describes SNeBR both before and after this revision, discusses how we adapted the integrity constraints for this purpose, and gives an example of the new SNeBR in action.",
        "published": "2000-03-08T20:42:29Z",
        "link": "http://arxiv.org/abs/cs/0003040v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3;I.2.4"
        ]
    },
    {
        "title": "Coherence, Belief Expansion and Bayesian Networks",
        "authors": [
            "Luc Bovens",
            "Stephan Hartmann"
        ],
        "summary": "We construct a probabilistic coherence measure for information sets which determines a partial coherence ordering. This measure is applied in constructing a criterion for expanding our beliefs in the face of new information. A number of idealizations are being made which can be relaxed by an appeal to Bayesian Networks.",
        "published": "2000-03-08T23:34:52Z",
        "link": "http://arxiv.org/abs/cs/0003041v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.0; G.3"
        ]
    },
    {
        "title": "Termination Proofs for Logic Programs with Tabling",
        "authors": [
            "Sofie Verbaeten",
            "Danny De Schreye",
            "Konstantinos Sagonas"
        ],
        "summary": "Tabled logic programming is receiving increasing attention in the Logic Programming community. It avoids many of the shortcomings of SLD execution and provides a more flexible and often extremely efficient execution mechanism for logic programs. In particular, tabled execution of logic programs terminates more often than execution based on SLD-resolution. In this article, we introduce two notions of universal termination of logic programming with Tabling: quasi-termination and (the stronger notion of) LG-termination. We present sufficient conditions for these two notions of termination, namely quasi-acceptability and LG-acceptability, and we show that these conditions are also necessary in case the tabling is well-chosen. Starting from these conditions, we give modular termination proofs, i.e., proofs capable of combining termination proofs of separate programs to obtain termination proofs of combined programs. Finally, in the presence of mode information, we state sufficient conditions which form the basis for automatically proving termination in a constraint-based way.",
        "published": "2000-03-09T16:27:22Z",
        "link": "http://arxiv.org/abs/cs/0003045v1",
        "categories": [
            "cs.LO",
            "I.2.2; I.2.3; D.1.6"
        ]
    },
    {
        "title": "Linear Tabulated Resolution Based on Prolog Control Strategy",
        "authors": [
            "Yi-Dong Shen",
            "Li-Yan Yuan",
            "Jia-Huai You",
            "Neng-Fa Zhou"
        ],
        "summary": "Infinite loops and redundant computations are long recognized open problems in Prolog. Two ways have been explored to resolve these problems: loop checking and tabling. Loop checking can cut infinite loops, but it cannot be both sound and complete even for function-free logic programs. Tabling seems to be an effective way to resolve infinite loops and redundant computations. However, existing tabulated resolutions, such as OLDT-resolution, SLG- resolution, and Tabulated SLS-resolution, are non-linear because they rely on the solution-lookup mode in formulating tabling. The principal disadvantage of non-linear resolutions is that they cannot be implemented using a simple stack-based memory structure like that in Prolog. Moreover, some strictly sequential operators such as cuts may not be handled as easily as in Prolog.   In this paper, we propose a hybrid method to resolve infinite loops and redundant computations. We combine the ideas of loop checking and tabling to establish a linear tabulated resolution called TP-resolution. TP-resolution has two distinctive features: (1) It makes linear tabulated derivations in the same way as Prolog except that infinite loops are broken and redundant computations are reduced. It handles cuts as effectively as Prolog. (2) It is sound and complete for positive logic programs with the bounded-term-size property. The underlying algorithm can be implemented by an extension to any existing Prolog abstract machines such as WAM or ATOAM.",
        "published": "2000-03-09T17:11:39Z",
        "link": "http://arxiv.org/abs/cs/0003046v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "PAL: Pertinence Action Language",
        "authors": [
            "Pedro Cabalar",
            "Manuel Cabarcos",
            "Ramon P. Otero"
        ],
        "summary": "The current document contains a brief description of a system for Reasoning about Actions and Change called PAL (Pertinence Action Language) which makes use of several reasoning properties extracted from a Temporal Expert Systems tool called Medtool.",
        "published": "2000-03-09T19:50:50Z",
        "link": "http://arxiv.org/abs/cs/0003048v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "A tableau methodology for deontic conditional logics",
        "authors": [
            "Alberto Artosi",
            "Guido Governatori"
        ],
        "summary": "In this paper we present a theorem proving methodology for a restricted but significant fragment of the conditional language made up of (boolean combinations of) conditional statements with unnested antecedents. The method is based on the possible world semantics for conditional logics. The KEM label formalism, designed to account for the semantics of normal modal logics, is easily adapted to the semantics of conditional logics by simply indexing labels with formulas. The inference rules are provided by the propositional system KE+ - a tableau-like analytic proof system devised to be used both as a refutation and a direct method of proof - enlarged with suitable elimination rules for the conditional connective. The theorem proving methodology we are going to present can be viewed as a first step towards developing an appropriate algorithmic framework for several conditional logics for (defeasible) conditional obligation.",
        "published": "2000-03-10T07:30:32Z",
        "link": "http://arxiv.org/abs/cs/0003050v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "A note on the Declarative reading(s) of Logic Programming",
        "authors": [
            "Marc Denecker"
        ],
        "summary": "This paper analyses the declarative readings of logic programming. Logic programming - and negation as failure - has no unique declarative reading. One common view is that logic programming is a logic for default reasoning, a sub-formalism of default logic or autoepistemic logic. In this view, negation as failure is a modal operator. In an alternative view, a logic program is interpreted as a definition. In this view, negation as failure is classical objective negation. From a commonsense point of view, there is definitely a difference between these views. Surprisingly though, both types of declarative readings lead to grosso modo the same model semantics. This note investigates the causes for this.",
        "published": "2000-03-13T16:21:41Z",
        "link": "http://arxiv.org/abs/cs/0003056v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3;I.2.4;F.4.1"
        ]
    },
    {
        "title": "XNMR: A tool for knowledge bases exploration",
        "authors": [
            "L. Castro",
            "D. Warren"
        ],
        "summary": "XNMR is a system designed to explore the results of combining the well-founded semantics system XSB with the stable-models evaluator SMODELS. Its main goal is to work as a tool for fast and interactive exploration of knowledge bases.",
        "published": "2000-03-13T23:18:12Z",
        "link": "http://arxiv.org/abs/cs/0003057v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6"
        ]
    },
    {
        "title": "A note on knowledge-based programs and specifications",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "Knowledge-based program are programs with explicit tests for knowledge. They have been used successfully in a number of applications. Sanders has pointed out what seem to be a counterintuitive property of knowledge-based programs. Roughly speaking, they do not satisfy a certain monotonicity property, while standard programs (ones without tests for knowledge) do. It is shown that there are two ways of defining the monotonicity property, which agree for standard programs. Knowledge-based programs satisfy the first, but do not satisfy the second. It is further argued by example that the fact that they do not satisfy the second is actually a feature, not a problem. Moreover, once we allow the more general class of knowledge-based specifications, standard programs do not satisfy the monotonicity property either.",
        "published": "2000-03-13T23:35:34Z",
        "link": "http://arxiv.org/abs/cs/0003058v1",
        "categories": [
            "cs.DC",
            "cs.LO",
            "D.1.3; F.3.1; D.2.1"
        ]
    },
    {
        "title": "Reasoning with Higher-Order Abstract Syntax in a Logical Framework",
        "authors": [
            "Raymond C. McDowell",
            "Dale A. Miller"
        ],
        "summary": "Logical frameworks based on intuitionistic or linear logics with higher-type quantification have been successfully used to give high-level, modular, and formal specifications of many important judgments in the area of programming languages and inference systems. Given such specifications, it is natural to consider proving properties about the specified systems in the framework: for example, given the specification of evaluation for a functional programming language, prove that the language is deterministic or that evaluation preserves types. One challenge in developing a framework for such reasoning is that higher-order abstract syntax (HOAS), an elegant and declarative treatment of object-level abstraction and substitution, is difficult to treat in proofs involving induction. In this paper, we present a meta-logic that can be used to reason about judgments coded using HOAS; this meta-logic is an extension of a simple intuitionistic logic that admits higher-order quantification over simply typed lambda-terms (key ingredients for HOAS) as well as induction and a notion of definition. We explore the difficulties of formal meta-theoretic analysis of HOAS encodings by considering encodings of intuitionistic and linear logics, and formally derive the admissibility of cut for important subsets of these logics. We then propose an approach to avoid the apparent tradeoff between the benefits of higher-order abstract syntax and the ability to analyze the resulting encodings. We illustrate this approach through examples involving the simple functional and imperative programming languages PCF and PCF:=. We formally derive such properties as unicity of typing, subject reduction, determinacy of evaluation, and the equivalence of transition semantics and natural semantics presentations of evaluation.",
        "published": "2000-03-14T22:35:00Z",
        "link": "http://arxiv.org/abs/cs/0003062v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.1; F.3.1; D.2.4; F.4.1"
        ]
    },
    {
        "title": "Detecting Unsolvable Queries for Definite Logic Programs",
        "authors": [
            "Maurice Bruynooghe",
            "Henk Vandecasteele",
            "D. Andre de Waal",
            "Marc Denecker"
        ],
        "summary": "In solving a query, the SLD proof procedure for definite programs sometimes searches an infinite space for a non existing solution. For example, querying a planner for an unreachable goal state. Such programs motivate the development of methods to prove the absence of a solution. Considering the definite program and the query ``<- Q'' as clauses of a first order theory, one can apply model generators which search for a finite interpretation in which the program clauses as well as the clause ``false <- Q'' are true. This paper develops a new approach which exploits the fact that all clauses are definite. It is based on a goal directed abductive search in the space of finite pre-interpretations for a pre-interpretation such that ``Q'' is false in the least model of the program based on it. Several methods for efficiently searching the space of pre-interpretations are presented. Experimental results confirm that our approach find solutions with less search than with the use of a first order model generator.",
        "published": "2000-03-17T10:59:03Z",
        "link": "http://arxiv.org/abs/cs/0003067v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6; F.3.1; F.4.1"
        ]
    },
    {
        "title": "A Polyvariant Binding-Time Analysis for Off-line Partial Deduction",
        "authors": [
            "Maurice Bruynooghe",
            "Michael Leuschel",
            "Konstantinos Sagonas"
        ],
        "summary": "We study the notion of binding-time analysis for logic programs. We formalise the unfolding aspect of an on-line partial deduction system as a Prolog program. Using abstract interpretation, we collect information about the run-time behaviour of the program. We use this information to make the control decisions about the unfolding at analysis time and to turn the on-line system into an off-line system. We report on some initial experiments.",
        "published": "2000-03-17T12:51:47Z",
        "link": "http://arxiv.org/abs/cs/0003068v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.3.0; D.1.6; F.3.1"
        ]
    },
    {
        "title": "Proving Failure of Queries for Definite Logic Programs Using XSB-Prolog",
        "authors": [
            "Nikolay Pelov",
            "Maurice Bruynooghe"
        ],
        "summary": "Proving failure of queries for definite logic programs can be done by constructing a finite model of the program in which the query is false. A general purpose model generator for first order logic can be used for this. A recent paper presented at PLILP98 shows how the peculiarities of definite programs can be exploited to obtain a better solution. There a procedure is described which combines abduction with tabulation and uses a meta-interpreter for heuristic control of the search. The current paper shows how similar results can be obtained by direct execution under the standard tabulation of the XSB-Prolog system. The loss of control is compensated for by better intelligent backtracking and more accurate failure analysis.",
        "published": "2000-03-20T09:45:00Z",
        "link": "http://arxiv.org/abs/cs/0003069v1",
        "categories": [
            "cs.LO",
            "F.3.1; I.2.3"
        ]
    },
    {
        "title": "The (Lazy) Functional Side of Logic Programming",
        "authors": [
            "S. Etalle",
            "J. Mountjoy"
        ],
        "summary": "The possibility of translating logic programs into functional ones has long been a subject of investigation. Common to the many approaches is that the original logic program, in order to be translated, needs to be well-moded and this has led to the common understanding that these programs can be considered to be the ``functional part'' of logic programs. As a consequence of this it has become widely accepted that ``complex'' logical variables, the possibility of a dynamic selection rule, and general properties of non-well-moded programs are exclusive features of logic programs. This is not quite true, as some of these features are naturally found in lazy functional languages. We readdress the old question of what features are exclusive to the logic programming paradigm by defining a simple translation applicable to a wider range of logic programs, and demonstrate that the current circumscription is unreasonably restrictive.",
        "published": "2000-03-20T09:52:20Z",
        "link": "http://arxiv.org/abs/cs/0003070v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.1; D.1.6; D.3.2; F.3.3"
        ]
    },
    {
        "title": "Axiomatic Synthesis of Computer Programs and Computability Theorems",
        "authors": [
            "Charlie Volkstorf"
        ],
        "summary": "We introduce a set of eight universal Rules of Inference by which computer programs with known properties (axioms) are transformed into new programs with known properties (theorems). Axioms are presented to formalize a segment of Number Theory, DataBase retrieval and Computability Theory. The resulting Program Calculus is used to generate programs to (1) Determine if one number is a factor of another. (2) List all employees who earn more than their manager. (3) List the set of programs that halt no on themselves, thus proving that it is recursively enumerable. The well-known fact that the set of programs that do not halt yes on themselves is not recursively enumerable is formalized as a program requirement that has no solution, an Incompleteness Axiom. Thus, any axioms (programs) which could be used to generate this program are themselves unattainable. Such proofs are presented to formally generate several additional theorems, including (4) The halting problem is unsolvable.   Open problems and future research is discussed, including the use of temporary sort files, programs that calculate statistics (such as counts and sums), the synthesis of programs to solve other well-known problems from Number Theory, Logic, DataBase retrieval and Computability Theory, application to Programming Language Semantics, and the formalization of incompleteness results from Logic and the semantic paradoxes.",
        "published": "2000-03-22T00:01:09Z",
        "link": "http://arxiv.org/abs/cs/0003071v1",
        "categories": [
            "cs.LO",
            "D.1.2; D.2.1; F.1.1; F.2.1; F.3.1; F.4.1; H.2.3; H.2.4; H.3.3; I.2.2"
        ]
    },
    {
        "title": "Proceedings of the 8th International Workshop on Non-Monotonic   Reasoning, NMR'2000",
        "authors": [
            "Chitta Baral",
            "Miroslaw Truszczynski"
        ],
        "summary": "The papers gathered in this collection were presented at the 8th International Workshop on Nonmonotonic Reasoning, NMR2000. The series was started by John McCarthy in 1978. The first international NMR workshop was held at Mohonk Mountain House, New Paltz, New York in June, 1984, and was organized by Ray Reiter and Bonnie Webber.   In the last 10 years the area of nonmonotonic reasoning has seen a number of important developments. Significant theoretical advances were made in the understanding of general abstract principles underlying nonmonotonicity. Key results on the expressibility and computational complexity of nonmonotonic logics were established. The role of nonmonotonic reasoning in belief revision, abduction, reasoning about action, planing and uncertainty was further clarified. Several successful NMR systems were built and used in applications such as planning, scheduling, logic programming and constraint satisfaction.   The papers in the proceedings reflect these recent advances in the field. They are grouped into sections corresponding to special sessions as they were held at the workshop:   1. General NMR track   2. Abductive reasonig   3. Belief revision: theory and practice   4. Representing action and planning   5. Systems descriptions and demonstrations   6. Uncertainty frameworks in NMR",
        "published": "2000-03-22T15:33:20Z",
        "link": "http://arxiv.org/abs/cs/0003073v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I2.2;I2.3;I2.4;I2.8;F4.1"
        ]
    },
    {
        "title": "Representation results for defeasible logic",
        "authors": [
            "G. Antoniou",
            "D. Billington",
            "G. Governatori",
            "M. J. Maher"
        ],
        "summary": "The importance of transformations and normal forms in logic programming, and generally in computer science, is well documented. This paper investigates transformations and normal forms in the context of Defeasible Logic, a simple but efficient formalism for nonmonotonic reasoning based on rules and priorities. The transformations described in this paper have two main benefits: on one hand they can be used as a theoretical tool that leads to a deeper understanding of the formalism, and on the other hand they have been used in the development of an efficient implementation of defeasible logic.",
        "published": "2000-03-30T02:23:21Z",
        "link": "http://arxiv.org/abs/cs/0003082v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Programming in Alma-0, or Imperative and Declarative Programming   Reconciled",
        "authors": [
            "Krzysztof R. Apt",
            "Andrea Schaerf"
        ],
        "summary": "In (Apt et al, TOPLAS 1998) we introduced the imperative programming language Alma-0 that supports declarative programming. In this paper we illustrate the hybrid programming style of Alma-0 by means of various examples that complement those presented in (Apt et al, TOPLAS 1998). The presented Alma-0 programs illustrate the versatility of the language and show that ``don't know'' nondeterminism can be naturally combined with assignment.",
        "published": "2000-04-05T16:04:26Z",
        "link": "http://arxiv.org/abs/cs/0004002v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL",
            "D.3.2;F.3.2;F.3.3;I.2.8;I.5.5"
        ]
    },
    {
        "title": "PSPACE Reasoning for Graded Modal Logics",
        "authors": [
            "Stephan Tobies"
        ],
        "summary": "We present a PSPACE algorithm that decides satisfiability of the graded modal logic Gr(K_R)---a natural extension of propositional modal logic K_R by counting expressions---which plays an important role in the area of knowledge representation. The algorithm employs a tableaux approach and is the first known algorithm which meets the lower bound for the complexity of the problem. Thus, we exactly fix the complexity of the problem and refute an ExpTime-hardness conjecture. We extend the results to the logic Gr(K_(R \\cap I)), which augments Gr(K_R) with inverse relations and intersection of accessibility relations. This establishes a kind of ``theoretical benchmark'' that all algorithmic approaches can be measured against.",
        "published": "2000-05-08T14:51:58Z",
        "link": "http://arxiv.org/abs/cs/0005009v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "F.4.1"
        ]
    },
    {
        "title": "Extending and Implementing the Stable Model Semantics",
        "authors": [
            "Patrik Simons"
        ],
        "summary": "An algorithm for computing the stable model semantics of logic programs is developed. It is shown that one can extend the semantics and the algorithm to handle new and more expressive types of rules. Emphasis is placed on the use of efficient implementation techniques. In particular, an implementation of lookahead that safely avoids testing every literal for failure and that makes the use of lookahead feasible is presented. In addition, a good heuristic is derived from the principle that the search space should be minimized.   Due to the lack of competitive algorithms and implementations for the computation of stable models, the system is compared with three satisfiability solvers. This shows that the heuristic can be improved by breaking ties, but leaves open the question of how to break them. It also demonstrates that the more expressive rules of the stable model semantics make the semantics clearly preferable over propositional logic when a problem has a more compact logic program representation. Conjunctive normal form representations are never more compact than logic program ones.",
        "published": "2000-05-08T18:26:54Z",
        "link": "http://arxiv.org/abs/cs/0005010v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3; I.2.8; F.4.1"
        ]
    },
    {
        "title": "Reasoning with Axioms: Theory and Pratice",
        "authors": [
            "Ian Horrocks",
            "Stephan Tobies"
        ],
        "summary": "When reasoning in description, modal or temporal logics it is often useful to consider axioms representing universal truths in the domain of discourse. Reasoning with respect to an arbitrary set of axioms is hard, even for relatively inexpressive logics, and it is essential to deal with such axioms in an efficient manner if implemented systems are to be effective in real applications. This is particularly relevant to Description Logics, where subsumption reasoning with respect to a terminology is a fundamental problem. Two optimisation techniques that have proved to be particularly effective in dealing with terminologies are lazy unfolding and absorption. In this paper we seek to improve our theoretical understanding of these important techniques. We define a formal framework that allows the techniques to be precisely described, establish conditions under which they can be safely applied, and prove that, provided these conditions are respected, subsumption testing algorithms will still function correctly. These results are used to show that the procedures used in the FaCT system are correct and, moreover, to show how efficiency can be significantly improved, while still retaining the guarantee of correctness, by relaxing the safety conditions for absorption.",
        "published": "2000-05-09T07:17:29Z",
        "link": "http://arxiv.org/abs/cs/0005012v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1, I.2.3, I.2.4"
        ]
    },
    {
        "title": "Practical Reasoning for Very Expressive Description Logics",
        "authors": [
            "Ian Horrocks",
            "Ulrike Sattler",
            "Stephan Tobies"
        ],
        "summary": "Description Logics (DLs) are a family of knowledge representation formalisms mainly characterised by constructors to build complex concepts and roles from atomic ones. Expressive role constructors are important in many applications, but can be computationally problematical. We present an algorithm that decides satisfiability of the DL ALC extended with transitive and inverse roles and functional restrictions with respect to general concept inclusion axioms and role hierarchies; early experiments indicate that this algorithm is well-suited for implementation. Additionally, we show that ALC extended with just transitive and inverse roles is still in PSPACE. We investigate the limits of decidability for this family of DLs, showing that relaxing the constraints placed on the kinds of roles used in number restrictions leads to the undecidability of all inference problems. Finally, we describe a number of optimisation techniques that are crucial in obtaining implementations of the decision procedures, which, despite the worst-case complexity of the problem, exhibit good performance with real-life problems.",
        "published": "2000-05-09T13:02:40Z",
        "link": "http://arxiv.org/abs/cs/0005013v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1, I.2.3, I.2.4"
        ]
    },
    {
        "title": "Practical Reasoning for Expressive Description Logics",
        "authors": [
            "Ian Horrocks",
            "Ulrike Sattler",
            "Stephan Tobies"
        ],
        "summary": "Description Logics (DLs) are a family of knowledge representation formalisms mainly characterised by constructors to build complex concepts and roles from atomic ones. Expressive role constructors are important in many applications, but can be computationally problematical. We present an algorithm that decides satisfiability of the DL ALC extended with transitive and inverse roles, role hierarchies, and qualifying number restrictions. Early experiments indicate that this algorithm is well-suited for implementation. Additionally, we show that ALC extended with just transitive and inverse roles is still in PSPACE. Finally, we investigate the limits of decidability for this family of DLs.",
        "published": "2000-05-10T08:19:41Z",
        "link": "http://arxiv.org/abs/cs/0005014v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1, I.2.3, I.2.4"
        ]
    },
    {
        "title": "Reasoning with Individuals for the Description Logic SHIQ",
        "authors": [
            "Ian Horrock",
            "Ulrike Sattler",
            "Stephan Tobies"
        ],
        "summary": "While there has been a great deal of work on the development of reasoning algorithms for expressive description logics, in most cases only Tbox reasoning is considered. In this paper we present an algorithm for combined Tbox and Abox reasoning in the SHIQ description logic. This algorithm is of particular interest as it can be used to decide the problem of (database) conjunctive query containment w.r.t. a schema. Moreover, the realisation of an efficient implementation should be relatively straightforward as it can be based on an existing highly optimised implementation of the Tbox algorithm in the FaCT system.",
        "published": "2000-05-11T08:16:21Z",
        "link": "http://arxiv.org/abs/cs/0005017v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1, I.2.3, I.2.4"
        ]
    },
    {
        "title": "On Modular Termination Proofs of General Logic Programs",
        "authors": [
            "Annalisa Bossi",
            "Nicoletta Cocco",
            "Sandro Etalle",
            "Sabina Rossi"
        ],
        "summary": "We propose a modular method for proving termination of general logic programs (i.e., logic programs with negation). It is based on the notion of acceptable programs, but it allows us to prove termination in a truly modular way. We consider programs consisting of a hierarchy of modules and supply a general result for proving termination by dealing with each module separately. For programs which are in a certain sense well-behaved, namely well-moded or well-typed programs, we derive both a simple verification technique and an iterative proof method. Some examples show how our system allows for greatly simplified proofs.",
        "published": "2000-05-11T15:27:49Z",
        "link": "http://arxiv.org/abs/cs/0005018v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.2; D.3; F.3.1; F.3.2"
        ]
    },
    {
        "title": "Axiomatizing Causal Reasoning",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "Causal models defined in terms of a collection of equations, as defined by Pearl, are axiomatized here. Axiomatizations are provided for three successively more general classes of causal models: (1) the class of recursive theories (those without feedback), (2) the class of theories where the solutions to the equations are unique, (3) arbitrary theories (where the equations may not have solutions and, if they do, they are not necessarily unique). It is shown that to reason about causality in the most general third class, we must extend the language used by Galles and Pearl. In addition, the complexity of the decision procedures is characterized for all the languages and classes of models considered.",
        "published": "2000-05-30T18:56:46Z",
        "link": "http://arxiv.org/abs/cs/0005030v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Light Affine Logic (Proof Nets, Programming Notation, P-Time Correctness   and Completeness)",
        "authors": [
            "Andrea Asperti",
            "Luca Roversi"
        ],
        "summary": "This paper is a structured introduction to Light Affine Logic, and to its intuitionistic fragment. Light Affine Logic has a polynomially costing cut elimination (P-Time correctness), and encodes all P-Time Turing machines (P-Time completeness). P-Time correctness is proved by introducing the Proof nets for Intuitionistic Light Affine Logic. P-Time completeness is demonstrated in full details thanks to a very compact program notation. On one side, the proof of P-Time correctness describes how the complexity of cut elimination is controlled, thanks to a suitable cut elimination strategy that exploits structural properties of the Proof nets. This allows to have a good catch on the meaning of the ``paragraph'' modality, which is a peculiarity of light logics. On the other side, the proof of P-Time completeness, together with a lot of programming examples, gives a flavor of the non trivial task of programming with resource limitations, using Intuitionistic Light Affine Logic derivations as programs.",
        "published": "2000-06-05T12:06:50Z",
        "link": "http://arxiv.org/abs/cs/0006010v1",
        "categories": [
            "cs.LO",
            "F.4.1; F.1.1; I.2.3"
        ]
    },
    {
        "title": "Verifying Termination of General Logic Programs with Concrete Queries",
        "authors": [
            "Yi-Dong Shen",
            "Li-Yan Yuan",
            "Jia-Huai You"
        ],
        "summary": "We introduce a method of verifying termination of logic programs with respect to concrete queries (instead of abstract query patterns). A necessary and sufficient condition is established and an algorithm for automatic verification is developed. In contrast to existing query pattern-based approaches, our method has the following features: (1) It applies to all general logic programs with non-floundering queries. (2) It is very easy to automate because it does not need to search for a level mapping or a model, nor does it need to compute an interargument relation based on additional mode or type information. (3) It bridges termination analysis with loop checking, the two problems that have been studied separately in the past despite their close technical relation with each other.",
        "published": "2000-06-21T17:30:30Z",
        "link": "http://arxiv.org/abs/cs/0006031v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "D.3.1; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Verifying Termination and Error-Freedom of Logic Programs with block   Declarations",
        "authors": [
            "Jan-Georg Smaus",
            "Patricia M. Hill",
            "Andy King"
        ],
        "summary": "We present verification methods for logic programs with delay declarations. The verified properties are termination and freedom from errors related to built-ins. Concerning termination, we present two approaches. The first approach tries to eliminate the well-known problem of speculative output bindings. The second approach is based on identifying the predicates for which the textual position of an atom using this predicate is irrelevant with respect to termination. Three features are distinctive of this work: it allows for predicates to be used in several modes; it shows that block declarations, which are a very simple delay construct, are sufficient to ensure the desired properties; it takes the selection rule into account, assuming it to be as in most Prolog implementations. The methods can be used to verify existing programs and assist in writing new programs.",
        "published": "2000-06-23T14:27:09Z",
        "link": "http://arxiv.org/abs/cs/0006033v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.1; D.3.2"
        ]
    },
    {
        "title": "Security Policy Consistency",
        "authors": [
            "Carlos Ribeiro",
            "Andre Zuquete",
            "Paulo Ferreira",
            "Paulo Guedes"
        ],
        "summary": "With the advent of wide security platforms able to express simultaneously all the policies comprising an organization's global security policy, the problem of inconsistencies within security policies become harder and more relevant.   We have defined a tool based on the CHR language which is able to detect several types of inconsistencies within and between security policies and other specifications, namely workflow specifications.   Although the problem of security conflicts has been addressed by several authors, to our knowledge none has addressed the general problem of security inconsistencies, on its several definitions and target specifications.",
        "published": "2000-06-30T20:07:59Z",
        "link": "http://arxiv.org/abs/cs/0006045v1",
        "categories": [
            "cs.LO",
            "cs.CR",
            "F.4.1; D.4.6; K.6.5"
        ]
    },
    {
        "title": "Constraint Exploration and Envelope of Simulation Trajectories",
        "authors": [
            "Oswaldo Teran",
            "Bruce Edmonds",
            "Steve Wallis"
        ],
        "summary": "The implicit theory that a simulation represents is precisely not in the individual choices but rather in the 'envelope' of possible trajectories - what is important is the shape of the whole envelope. Typically a huge amount of computation is required when experimenting with factors bearing on the dynamics of a simulation to tease out what affects the shape of this envelope. In this paper we present a methodology aimed at systematically exploring this envelope. We propose a method for searching for tendencies and proving their necessity relative to a range of parameterisations of the model and agents' choices, and to the logic of the simulation language. The exploration consists of a forward chaining generation of the trajectories associated to and constrained by such a range of parameterisations and choices. Additionally, we propose a computational procedure that helps implement this exploration by translating a Multi Agent System simulation into a constraint-based search over possible trajectories by 'compiling' the simulation rules into a more specific form, namely by partitioning the simulation rules using appropriate modularity in the simulation. An example of this procedure is exhibited.   Keywords: Constraint Search, Constraint Logic Programming, Proof, Emergence, Tendencies",
        "published": "2000-07-03T10:10:09Z",
        "link": "http://arxiv.org/abs/cs/0007001v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.LO",
            "D.3.3; F.3.1; F.4.1"
        ]
    },
    {
        "title": "Polynomial-time Computation via Local Inference Relations",
        "authors": [
            "Robert Givan",
            "David McAllester"
        ],
        "summary": "We consider the concept of a local set of inference rules. A local rule set can be automatically transformed into a rule set for which bottom-up evaluation terminates in polynomial time. The local-rule-set transformation gives polynomial-time evaluation strategies for a large variety of rule sets that cannot be given terminating evaluation strategies by any other known automatic technique. This paper discusses three new results. First, it is shown that every polynomial-time predicate can be defined by an (unstratified) local rule set. Second, a new machine-recognizable subclass of the local rule sets is identified. Finally we show that locality, as a property of rule sets, is undecidable in general.",
        "published": "2000-07-13T17:19:43Z",
        "link": "http://arxiv.org/abs/cs/0007020v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL",
            "I.2.2; I.2.3; I.2.4; F.4.m"
        ]
    },
    {
        "title": "A theory of normed simulations",
        "authors": [
            "W. O. D. Griffioen",
            "F. W. Vaandrager"
        ],
        "summary": "In existing simulation proof techniques, a single step in a lower-level specification may be simulated by an extended execution fragment in a higher-level one. As a result, it is cumbersome to mechanize these techniques using general purpose theorem provers. Moreover, it is undecidable whether a given relation is a simulation, even if tautology checking is decidable for the underlying specification logic. This paper introduces various types of normed simulations. In a normed simulation, each step in a lower-level specification can be simulated by at most one step in the higher-level one, for any related pair of states. In earlier work we demonstrated that normed simulations are quite useful as a vehicle for the formalization of refinement proofs via theorem provers. Here we show that normed simulations also have pleasant theoretical properties: (1) under some reasonable assumptions, it is decidable whether a given relation is a normed forward simulation, provided tautology checking is decidable for the underlying logic; (2) at the semantic level, normed forward and backward simulations together form a complete proof method for establishing behavior inclusion, provided that the higher-level specification has finite invisible nondeterminism.",
        "published": "2000-07-19T14:38:48Z",
        "link": "http://arxiv.org/abs/cs/0007030v2",
        "categories": [
            "cs.LO",
            "F.1.1; F.3.1"
        ]
    },
    {
        "title": "Knowledge on Treelike Spaces",
        "authors": [
            "Konstantinos Georgatos"
        ],
        "summary": "This paper presents a bimodal logic for reasoning about knowledge during knowledge acquisition. One of the modalities represents (effort during) non-deterministic time and the other represents knowledge. The semantics of this logic are tree-like spaces which are a generalization of semantics used for modeling branching time and historical necessity. A finite system of axiom schemes is shown to be canonically complete for the formentioned spaces. A characterization of the satisfaction relation implies the small model property and decidability for this system.",
        "published": "2000-07-21T09:52:25Z",
        "link": "http://arxiv.org/abs/cs/0007032v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.0"
        ]
    },
    {
        "title": "To Preference via Entrenchment",
        "authors": [
            "Konstantinos Georgatos"
        ],
        "summary": "We introduce a simple generalization of Gardenfors and Makinson's epistemic entrenchment called partial entrenchment. We show that preferential inference can be generated as the sceptical counterpart of an inference mechanism defined directly on partial entrenchment.",
        "published": "2000-07-21T10:16:47Z",
        "link": "http://arxiv.org/abs/cs/0007033v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3"
        ]
    },
    {
        "title": "Knowledge Theoretic Properties of Topological Spaces",
        "authors": [
            "Konstantinos Georgatos"
        ],
        "summary": "We study the topological models of a logic of knowledge for topological reasoning, introduced by Larry Moss and Rohit Parikh. Among our results is a solution of a conjecture by the formentioned authors, finite satisfiability property and decidability for the theory of topological models.",
        "published": "2000-07-26T18:36:37Z",
        "link": "http://arxiv.org/abs/cs/0007037v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Modal Logics for Topological Spaces",
        "authors": [
            "Konstantinos Georgatos"
        ],
        "summary": "In this thesis we shall present two logical systems, MP and MP, for the purpose of reasoning about knowledge and effort. These logical systems will be interpreted in a spatial context and therefore, the abstract concepts of knowledge and effort will be defined by concrete mathematical concepts.",
        "published": "2000-07-26T18:41:17Z",
        "link": "http://arxiv.org/abs/cs/0007038v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Ordering-based Representations of Rational Inference",
        "authors": [
            "Konstantinos Georgatos"
        ],
        "summary": "Rational inference relations were introduced by Lehmann and Magidor as the ideal systems for drawing conclusions from a conditional base. However, there has been no simple characterization of these relations, other than its original representation by preferential models. In this paper, we shall characterize them with a class of total preorders of formulas by improving and extending Gardenfors and Makinson's results for expectation inference relations. A second representation is application-oriented and is obtained by considering a class of consequence operators that grade sets of defaults according to our reliance on them. The finitary fragment of this class of consequence operators has been employed by recent default logic formalisms based on maxiconsistency.",
        "published": "2000-07-26T18:58:09Z",
        "link": "http://arxiv.org/abs/cs/0007039v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3"
        ]
    },
    {
        "title": "Entrenchment Relations: A Uniform Approach to Nonmonotonicity",
        "authors": [
            "Konstantinos Georgatos"
        ],
        "summary": "We show that Gabbay's nonmonotonic consequence relations can be reduced to a new family of relations, called entrenchment relations. Entrenchment relations provide a direct generalization of epistemic entrenchment and expectation ordering introduced by Gardenfors and Makinson for the study of belief revision and expectation inference, respectively.",
        "published": "2000-07-26T19:20:35Z",
        "link": "http://arxiv.org/abs/cs/0007040v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3"
        ]
    },
    {
        "title": "Relevance as Deduction: A Logical View of Information Retrieval",
        "authors": [
            "Gianni Amati",
            "Konstantinos Georgatos"
        ],
        "summary": "The problem of Information Retrieval is, given a set of documents D and a query q, providing an algorithm for retrieving all documents in D relevant to q. However, retrieval should depend and be updated whenever the user is able to provide as an input a preferred set of relevant documents; this process is known as em relevance feedback. Recent work in IR has been paying great attention to models which employ a logical approach; the advantage being that one can have a simple computable characterization of retrieval on the basis of a pure logical analysis of retrieval. Most of the logical models make use of probabilities or similar belief functions in order to introduce the inductive component whereby uncertainty is treated. Their general paradigm is the following: em find the nature of conditional $d\\imp q$ and then define a probability on the top of it. We just reverse this point of view; first use the numerical information, frequencies or probabilities, then define your own logical consequence. More generally, we claim that retrieval is a form of deduction. We introduce a simple but powerful logical framework of relevance feedback, derived from the well founded area of nonmonotonic logic. This description can help us evaluate, describe and compare from a theoretical point of view previous approaches based on conditionals or probabilities.",
        "published": "2000-07-26T19:34:35Z",
        "link": "http://arxiv.org/abs/cs/0007041v1",
        "categories": [
            "cs.IR",
            "cs.LO",
            "I.2.3;H.3.0;H.3.3"
        ]
    },
    {
        "title": "Boolean Satisfiability with Transitivity Constraints",
        "authors": [
            "Randal E. Bryant",
            "Miroslav N. Velev"
        ],
        "summary": "We consider a variant of the Boolean satisfiability problem where a subset E of the propositional variables appearing in formula Fsat encode a symmetric, transitive, binary relation over N elements. Each of these relational variables, e[i,j], for 1 <= i < j <= N, expresses whether or not the relation holds between elements i and j. The task is to either find a satisfying assignment to Fsat that also satisfies all transitivity constraints over the relational variables (e.g., e[1,2] & e[2,3] ==> e[1,3]), or to prove that no such assignment exists. Solving this satisfiability problem is the final and most difficult step in our decision procedure for a logic of equality with uninterpreted functions. This procedure forms the core of our tool for verifying pipelined microprocessors.   To use a conventional Boolean satisfiability checker, we augment the set of clauses expressing Fsat with clauses expressing the transitivity constraints. We consider methods to reduce the number of such clauses based on the sparse structure of the relational variables.   To use Ordered Binary Decision Diagrams (OBDDs), we show that for some sets E, the OBDD representation of the transitivity constraints has exponential size for all possible variable orderings. By considering only those relational variables that occur in the OBDD representation of Fsat, our experiments show that we can readily construct an OBDD representation of the relevant transitivity constraints and thus solve the constrained satisfiability problem.",
        "published": "2000-08-01T13:51:56Z",
        "link": "http://arxiv.org/abs/cs/0008001v1",
        "categories": [
            "cs.LO",
            "I2.3; G2.2"
        ]
    },
    {
        "title": "The Bisimulation Problem for equational graphs of finite out-degree",
        "authors": [
            "G. Senizergues"
        ],
        "summary": "The \"bisimulation problem\" for equational graphs of finite out-degree is shown to be decidable. We reduce this problem to the bisimulation problem for deterministic rational (vectors of) boolean series on the alphabet of a dpda M. We then exhibit a complete formal system for deducing equivalent pairs of such vectors.",
        "published": "2000-08-22T10:46:03Z",
        "link": "http://arxiv.org/abs/cs/0008018v1",
        "categories": [
            "cs.LO",
            "cs.DM",
            "F.1.1;F.4.2;F.4.3;G.2.2"
        ]
    },
    {
        "title": "Verifying the Unification Algorithm in LCF",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "Manna and Waldinger's theory of substitutions and unification has been verified using the Cambridge LCF theorem prover. A proof of the monotonicity of substitution is presented in detail, as an example of interaction with LCF. Translating the theory into LCF's domain-theoretic logic is largely straightforward. Well-founded induction on a complex ordering is translated into nested structural inductions. Correctness of unification is expressed using predicates for such properties as idempotence and most-generality. The verification is presented as a series of lemmas. The LCF proofs are compared with the original ones, and with other approaches. It appears difficult to find a logic that is both simple and flexible, especially for proving termination.",
        "published": "2000-09-29T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9301101v1",
        "categories": [
            "cs.LO",
            "D.2.4; F.3.1; F.4.1"
        ]
    },
    {
        "title": "Proving Termination of Normalization Functions for Conditional   Expressions",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "Boyer and Moore have discussed a recursive function that puts conditional expressions into normal form [1]. It is difficult to prove that this function terminates on all inputs. Three termination proofs are compared: (1) using a measure function, (2) in domain theory using LCF, (3) showing that its recursion relation, defined by the pattern of recursive calls, is well-founded. The last two proofs are essentially the same though conducted in markedly different logical frameworks. An obviously total variant of the normalize function is presented as the `computational meaning' of those two proofs. A related function makes nested recursive calls. The three termination proofs become more complex: termination and correctness must be proved simultaneously. The recursion relation approach seems flexible enough to handle subtle termination proofs where previously domain theory seemed essential.",
        "published": "2000-10-02T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9301103v1",
        "categories": [
            "cs.LO",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Sequence-Based Abstract Interpretation of Prolog",
        "authors": [
            "Baudouin Le Charlier",
            "Sabina Rossi",
            "Pascal Van Hentenryck"
        ],
        "summary": "Many abstract interpretation frameworks and analyses for Prolog have been proposed, which seek to extract information useful for program optimization. Although motivated by practical considerations, notably making Prolog competitive with imperative languages, such frameworks fail to capture some of the control structures of existing implementations of the language.   In this paper we propose a novel framework for the abstract interpretation of Prolog which handles the depth-first search rule and the cut operator. It relies on the notion of substitution sequence to model the result of the execution of a goal. The framework consists of (i) a denotational concrete semantics, (ii) a safe abstraction of the concrete semantics defined in terms of a class of post-fixpoints, and (iii) a generic abstract interpretation algorithm. We show that traditional abstract domains of substitutions may easily be adapted to the new framework, and provide experimental evidence of the effectiveness of our approach. We also show that previous work on determinacy analysis, that was not expressible by existing abstract interpretation frameworks, can be seen as an instance of our framework.",
        "published": "2000-10-19T11:00:12Z",
        "link": "http://arxiv.org/abs/cs/0010028v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.2; D.3; F.3.1; F.3.2"
        ]
    },
    {
        "title": "Using Modes to Ensure Subject Reduction for Typed Logic Programs with   Subtyping",
        "authors": [
            "Jan-Georg Smaus",
            "Francois Fages",
            "Pierre Deransart"
        ],
        "summary": "We consider a general prescriptive type system with parametric polymorphism and subtyping for logic programs. The property of subject reduction expresses the consistency of the type system w.r.t. the execution model: if a program is \"well-typed\", then all derivations starting in a \"well-typed\" goal are again \"well-typed\". It is well-established that without subtyping, this property is readily obtained for logic programs w.r.t. their standard (untyped) execution model. Here we give syntactic conditions that ensure subject reduction also in the presence of general subtyping relations between type constructors. The idea is to consider logic programs with a fixed dataflow, given by modes.",
        "published": "2000-10-20T08:04:15Z",
        "link": "http://arxiv.org/abs/cs/0010029v2",
        "categories": [
            "cs.LO",
            "D1.6; D3.3"
        ]
    },
    {
        "title": "Super Logic Programs",
        "authors": [
            "Stefan Brass",
            "Juergen Dix",
            "Teodor C. Przymusinski"
        ],
        "summary": "The Autoepistemic Logic of Knowledge and Belief (AELB) is a powerful nonmonotic formalism introduced by Teodor Przymusinski in 1994. In this paper, we specialize it to a class of theories called `super logic programs'. We argue that these programs form a natural generalization of standard logic programs. In particular, they allow disjunctions and default negation of arbibrary positive objective formulas.   Our main results are two new and powerful characterizations of the static semant ics of these programs, one syntactic, and one model-theoretic. The syntactic fixed point characterization is much simpler than the fixed point construction of the static semantics for arbitrary AELB theories. The model-theoretic characterization via Kripke models allows one to construct finite representations of the inherently infinite static expansions.   Both characterizations can be used as the basis of algorithms for query answering under the static semantics. We describe a query-answering interpreter for super programs which we developed based on the model-theoretic characterization and which is available on the web.",
        "published": "2000-10-25T13:32:51Z",
        "link": "http://arxiv.org/abs/cs/0010032v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Static Analysis Techniques for Equational Logic Programming",
        "authors": [
            "Rakesh M. Verma"
        ],
        "summary": "An equational logic program is a set of directed equations or rules, which are used to compute in the obvious way (by replacing equals with ``simpler'' equals). We present static analysis techniques for efficient equational logic programming, some of which have been implemented in $LR^2$, a laboratory for developing and evaluating fast, efficient, and practical rewriting techniques. Two novel features of $LR^2$ are that non-left-linear rules are allowed in most contexts and it has a tabling option based on the congruence-closure based algorithm to compute normal forms. Although, the focus of this research is on the tabling approach some of the techniques are applicable to the untabled approach as well. Our presentation is in the context of $LR^2$, which is an interpreter, but some of the techniques apply to compilation as well.",
        "published": "2000-10-27T20:47:23Z",
        "link": "http://arxiv.org/abs/cs/0010034v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.2; D.3.2"
        ]
    },
    {
        "title": "Constructing Recursion Operators in Intuitionistic Type Theory",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "Martin-L\\\"of's Intuitionistic Theory of Types is becoming popular for formal reasoning about computer programs. To handle recursion schemes other than primitive recursion, a theory of well-founded relations is presented. Using primitive recursion over higher types, induction and recursion are formally derived for a large class of well-founded relations. Included are < on natural numbers, and relations formed by inverse images, addition, multiplication, and exponentiation of other relations. The constructions are given in full detail to allow their use in theorem provers for Type Theory, such as Nuprl. The theory is compared with work in the field of ordinal recursion over higher types.",
        "published": "2000-10-31T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9301102v1",
        "categories": [
            "cs.LO",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Natural Deduction as Higher-Order Resolution",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "An interactive theorem prover, Isabelle, is under development. In LCF, each inference rule is represented by one function for forwards proof and another (a tactic) for backwards proof. In Isabelle, each inference rule is represented by a Horn clause. Resolution gives both forwards and backwards proof, supporting a large class of logics. Isabelle has been used to prove theorems in Martin-L\\\"of's Constructive Type Theory. Quantifiers pose several difficulties: substitution, bound variables, Skolemization. Isabelle's representation of logical syntax is the typed lambda-calculus, requiring higher- order unification. It may have potential for logic programming. Depth-first subgoaling along inference rules constitutes a higher-order Prolog.",
        "published": "2000-10-31T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9301104v1",
        "categories": [
            "cs.LO",
            "D.2.4; F.3.1; F.4.1"
        ]
    },
    {
        "title": "The Foundation of a Generic Theorem Prover",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "Isabelle is an interactive theorem prover that supports a variety of logics. It represents rules as propositions (not as functions) and builds proofs by combining rules. These operations constitute a meta-logic (or `logical framework') in which the object-logics are formalized. Isabelle is now based on higher-order logic -- a precise and well-understood foundation. Examples illustrate use of this meta-logic to formalize logics and proofs. Axioms for first-order logic are shown sound and complete. Backwards proof is formalized by meta-reasoning about object-level entailment. Higher-order logic has several practical advantages over other meta-logics. Many proof techniques are known, such as Huet's higher-order unification procedure.",
        "published": "2000-10-31T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9301105v1",
        "categories": [
            "cs.LO",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Isabelle: The Next 700 Theorem Provers",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "Isabelle is a generic theorem prover, designed for interactive reasoning in a variety of formal theories. At present it provides useful proof procedures for Constructive Type Theory, various first-order logics, Zermelo-Fraenkel set theory, and higher-order logic. This survey of Isabelle serves as an introduction to the literature. It explains why generic theorem proving is beneficial. It gives a thorough history of Isabelle, beginning with its origins in the LCF system. It presents an account of how logics are represented, illustrated using classical logic. The approach is compared with the Edinburgh Logical Framework. Several of the Isabelle object-logics are presented.",
        "published": "2000-10-31T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9301106v1",
        "categories": [
            "cs.LO",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "A Formulation of the Simple Theory of Types (for Isabelle)",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "Simple type theory is formulated for use with the generic theorem prover Isabelle. This requires explicit type inference rules. There are function, product, and subset types, which may be empty. Descriptions (the eta-operator) introduce the Axiom of Choice. Higher-order logic is obtained through reflection between formulae and terms of type bool. Recursive types and functions can be formally constructed. Isabelle proof procedures are described. The logic appears suitable for general mathematics as well as computational problems.",
        "published": "2000-10-31T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9301107v1",
        "categories": [
            "cs.LO",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Set Theory for Verification: I. From Foundations to Functions",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "A logic for specification and verification is derived from the axioms of Zermelo-Fraenkel set theory. The proofs are performed using the proof assistant Isabelle. Isabelle is generic, supporting several different logics. Isabelle has the flexibility to adapt to variants of set theory. Its higher-order syntax supports the definition of new binding operators. Unknowns in subgoals can be instantiated incrementally. The paper describes the derivation of rules for descriptions, relations and functions, and discusses interactive proofs of Cantor's Theorem, the Composition of Homomorphisms challenge [9], and Ramsey's Theorem [5]. A generic proof assistant can stand up against provers dedicated to particular logics.",
        "published": "2000-10-31T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9311103v1",
        "categories": [
            "cs.LO",
            "D.2.4; F.3.1; F.4.1"
        ]
    },
    {
        "title": "Mechanizing Coinduction and Corecursion in Higher-order Logic",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "A theory of recursive and corecursive definitions has been developed in higher-order logic (HOL) and mechanized using Isabelle. Least fixedpoints express inductive data types such as strict lists; greatest fixedpoints express coinductive data types, such as lazy lists. Well-founded recursion expresses recursive functions over inductive data types; corecursion expresses functions that yield elements of coinductive data types. The theory rests on a traditional formalization of infinite trees. The theory is intended for use in specification and verification. It supports reasoning about a wide range of computable functions, but it does not formalize their operational semantics and can express noncomputable functions also. The theory is illustrated using finite and infinite lists. Corecursion expresses functions over infinite lists; coinduction reasons about such functions.",
        "published": "2000-10-31T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9711105v1",
        "categories": [
            "cs.LO",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Transformation-Based Bottom-Up Computation of the Well-Founded Model",
        "authors": [
            "Stefan Brass",
            "Juergen Dix",
            "Burkhard Freitag",
            "Ulrich Zukowski"
        ],
        "summary": "We present a framework for expressing bottom-up algorithms to compute the well-founded model of non-disjunctive logic programs. Our method is based on the notion of conditional facts and elementary program transformations studied by Brass and Dix for disjunctive programs. However, even if we restrict their framework to nondisjunctive programs, their residual program can grow to exponential size, whereas for function-free programs our program remainder is always polynomial in the size of the extensional database (EDB).   We show that particular orderings of our transformations (we call them strategies) correspond to well-known computational methods like the alternating fixpoint approach, the well-founded magic sets method and the magic alternating fixpoint procedure. However, due to the confluence of our calculi, we come up with computations of the well-founded model that are provably better than these methods.   In contrast to other approaches, our transformation method treats magic set transformed programs correctly, i.e. it always computes a relevant part of the well-founded model of the original program.",
        "published": "2000-11-08T14:32:48Z",
        "link": "http://arxiv.org/abs/cs/0011013v1",
        "categories": [
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Set Theory for Verification: II. Induction and Recursion",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "A theory of recursive definitions has been mechanized in Isabelle's Zermelo-Fraenkel (ZF) set theory. The objective is to support the formalization of particular recursive definitions for use in verification, semantics proofs and other computational reasoning. Inductively defined sets are expressed as least fixedpoints, applying the Knaster-Tarski Theorem over a suitable set. Recursive functions are defined by well-founded recursion and its derivatives, such as transfinite recursion. Recursive data structures are expressed by applying the Knaster-Tarski Theorem to a set, such as V[omega], that is closed under Cartesian product and disjoint sum. Worked examples include the transitive closure of a relation, lists, variable-branching trees and mutually recursive trees and forests. The Schr\\\"oder-Bernstein Theorem and the soundness of propositional logic are proved in Isabelle sessions.",
        "published": "2000-11-14T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9511102v1",
        "categories": [
            "cs.LO",
            "D.2.4; F.3.1; F.4.1"
        ]
    },
    {
        "title": "A syntactical analysis of non-size-increasing polynomial time   computation",
        "authors": [
            "Klaus Aehlig",
            "Helmut Schwichtenberg"
        ],
        "summary": "A syntactical proof is given that all functions definable in a certain affine linear typed lambda-calculus with iteration in all types are polynomial time computable. The proof provides explicit polynomial bounds that can easily be calculated.",
        "published": "2000-11-23T10:09:06Z",
        "link": "http://arxiv.org/abs/cs/0011037v2",
        "categories": [
            "cs.LO",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "A Complete Characterization of Complete Intersection-Type Theories",
        "authors": [
            "M. Dezani-Ciancaglini",
            "F. Honsell",
            "F. Alessi"
        ],
        "summary": "We characterize those intersection-type theories which yield complete intersection-type assignment systems for lambda-calculi, with respect to the three canonical set-theoretical semantics for intersection-types: the inference semantics, the simple semantics and the F-semantics. These semantics arise by taking as interpretation of types subsets of applicative structures, as interpretation of the intersection constructor set-theoretic inclusion, and by taking the interpretation of the arrow constructor a' la Scott, with respect to either any possible functionality set, or the largest one, or the least one.   These results strengthen and generalize significantly all earlier results in the literature, to our knowledge, in at least three respects. First of all the inference semantics had not been considered before. Secondly, the characterizations are all given just in terms of simple closure conditions on the preorder relation on the types, rather than on the typing judgments themselves. The task of checking the condition is made therefore considerably more tractable. Lastly, we do not restrict attention just to lambda-models, but to arbitrary applicative structures which admit an interpretation function. Thus we allow also for the treatment of models of restricted lambda-calculi. Nevertheless the characterizations we give can be tailored just to the case of lambda-models.",
        "published": "2000-11-23T17:40:24Z",
        "link": "http://arxiv.org/abs/cs/0011039v1",
        "categories": [
            "cs.LO",
            "F.4.1; F.3.3; F.3.2; D.1.1"
        ]
    },
    {
        "title": "Order-consistent programs are cautiously monotonic",
        "authors": [
            "Hudson Turner"
        ],
        "summary": "Some normal logic programs under the answer set (stable model) semantics lack the appealing property of \"cautious monotonicity.\" That is, augmenting a program with one of its consequences may cause it to lose another of its consequences. The syntactic condition of \"order-consistency\" was shown by Fages to guarantee existence of an answer set. This note establishes that order-consistent programs are not only consistent, but cautiously monotonic.   From this it follows that they are also \"cumulative.\" That is, augmenting an order-consistent with some of its consequences does not alter its consequences. In fact, as we show, its answer sets remain unchanged.",
        "published": "2000-11-27T19:21:10Z",
        "link": "http://arxiv.org/abs/cs/0011042v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3"
        ]
    },
    {
        "title": "Rewriting Calculus: Foundations and Applications",
        "authors": [
            "Horatiu Cirstea"
        ],
        "summary": "This thesis is devoted to the study of a calculus that describes the application of conditional rewriting rules and the obtained results at the same level of representation. We introduce the rewriting calculus, also called the rho-calculus, which generalizes the first order term rewriting and lambda-calculus, and makes possible the representation of the non-determinism. In our approach the abstraction operator as well as the application operator are objects of calculus. The result of a reduction in the rewriting calculus is either an empty set representing the application failure, or a singleton representing a deterministic result, or a set having several elements representing a not-deterministic choice of results.   In this thesis we concentrate on the properties of the rewriting calculus where a syntactic matching is used in order to bind the variables to their current values. We define evaluation strategies ensuring the confluence of the calculus and we show that these strategies become trivial for restrictions of the general rewriting calculus to simpler calculi like the lambda-calculus. The rewriting calculus is not terminating in the untyped case but the strong normalization is obtained for the simply typed calculus.   In the rewriting calculus extended with an operator allowing to test the application failure we define terms representing innermost and outermost normalizations with respect to a set of rewriting rules. By using these terms, we obtain a natural and concise description of the conditional rewriting. Finally, starting from the representation of the conditional rewriting rules, we show how the rewriting calculus can be used to give a semantics to ELAN, a language based on the application of rewriting rules controlled by strategies.",
        "published": "2000-11-28T15:01:07Z",
        "link": "http://arxiv.org/abs/cs/0011043v1",
        "categories": [
            "cs.SC",
            "cs.LO",
            "cs.PL",
            "I.1; D.1; D.3; F.4.0; F.4.1"
        ]
    },
    {
        "title": "Well-Typed Logic Programs Are not Wrong",
        "authors": [
            "Pierre Deransart",
            "Jan-Georg Smaus"
        ],
        "summary": "We consider prescriptive type systems for logic programs (as in Goedel or Mercury). In such systems, the typing is static, but it guarantees an operational property: if a program is \"well-typed\", then all derivations starting in a \"well-typed\" query are again \"well-typed\". This property has been called subject reduction. We show that this property can also be phrased as a property of the proof-theoretic semantics of logic programs, thus abstracting from the usual operational (top-down) semantics. This proof-theoretic view leads us to questioning a condition which is usually considered necessary for subject reduction, namely the head condition. It states that the head of each clause must have a type which is a variant (and not a proper instance) of the declared type. We provide a more general condition, thus reestablishing a certain symmetry between heads and body atoms. The condition ensures that in a derivation, the types of two unified terms are themselves unifiable. We discuss possible implications of this result. We also discuss the relationship between the head condition and polymorphic recursion, a concept known in functional programming.",
        "published": "2000-12-20T15:45:32Z",
        "link": "http://arxiv.org/abs/cs/0012015v2",
        "categories": [
            "cs.LO",
            "D.1.6; D.3.3"
        ]
    },
    {
        "title": "Resource-distribution via Boolean constraints",
        "authors": [
            "James Harland",
            "David Pym"
        ],
        "summary": "We consider the problem of searching for proofs in sequential presentations of logics with multiplicative (or intensional) connectives. Specifically, we start with the multiplicative fragment of linear logic and extend, on the one hand, to linear logic with its additives and, on the other, to the additives of the logic of bunched implications, BI. We give an algebraic method for calculating the distribution of the side-formulae in multiplicative rules which allows the occurrence or non-occurrence of a formula on a branch of a proof to be determined once sufficient information is available. Each formula in the conclusion of such a rule is assigned a Boolean expression. As a search proceeds, a set of Boolean constraint equations is generated. We show that a solution to such a set of equations determines a proof corresponding to the given search. We explain a range of strategies, from the lazy to the eager, for solving sets of constraint equations. We indicate how to apply our methods systematically to large family of relevant systems.",
        "published": "2000-12-21T00:24:42Z",
        "link": "http://arxiv.org/abs/cs/0012018v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Predicting the expected behavior of agents that learn about agents: the   CLRI framework",
        "authors": [
            "Jose M. Vidal",
            "Edmund H. Durfee"
        ],
        "summary": "We describe a framework and equations used to model and predict the behavior of multi-agent systems (MASs) with learning agents. A difference equation is used for calculating the progression of an agent's error in its decision function, thereby telling us how the agent is expected to fare in the MAS. The equation relies on parameters which capture the agent's learning abilities, such as its change rate, learning rate and retention rate, as well as relevant aspects of the MAS such as the impact that agents have on each other. We validate the framework with experimental results using reinforcement learning agents in a market system, as well as with other experimental results gathered from the AI literature. Finally, we use PAC-theory to show how to calculate bounds on the values of the learning parameters.",
        "published": "2000-01-12T20:57:59Z",
        "link": "http://arxiv.org/abs/cs/0001008v3",
        "categories": [
            "cs.MA",
            "cs.LG",
            "I.2.11"
        ]
    },
    {
        "title": "Applying Maxi-adjustment to Adaptive Information Filtering Agents",
        "authors": [
            "Raymond Lau",
            "Arthur H. M. ter Hofstede",
            "Peter D. Bruza"
        ],
        "summary": "Learning and adaptation is a fundamental property of intelligent agents. In the context of adaptive information filtering, a filtering agent's beliefs about a user's information needs have to be revised regularly with reference to the user's most current information preferences. This learning and adaptation process is essential for maintaining the agent's filtering performance. The AGM belief revision paradigm provides a rigorous foundation for modelling rational and minimal changes to an agent's beliefs. In particular, the maxi-adjustment method, which follows the AGM rationale of belief change, offers a sound and robust computational mechanism to develop adaptive agents so that learning autonomy of these agents can be enhanced. This paper describes how the maxi-adjustment method is applied to develop the learning components of adaptive information filtering agents, and discusses possible difficulties of applying such a framework to these agents.",
        "published": "2000-03-07T02:12:55Z",
        "link": "http://arxiv.org/abs/cs/0003014v2",
        "categories": [
            "cs.AI",
            "cs.MA",
            "I.2.3;I.2.11;H.3.3"
        ]
    },
    {
        "title": "Multiagent Control of Self-reconfigurable Robots",
        "authors": [
            "Hristo Bojinov",
            "Arancha Casal",
            "Tad Hogg"
        ],
        "summary": "We demonstrate how multiagent systems provide useful control techniques for modular self-reconfigurable (metamorphic) robots. Such robots consist of many modules that can move relative to each other, thereby changing the overall shape of the robot to suit different tasks. Multiagent control is particularly well-suited for tasks involving uncertain and changing environments. We illustrate this approach through simulation experiments of Proteo, a metamorphic robot system currently under development.",
        "published": "2000-06-20T20:17:44Z",
        "link": "http://arxiv.org/abs/cs/0006030v2",
        "categories": [
            "cs.RO",
            "cs.DC",
            "cs.MA",
            "I.2.9; I.2.11; H.3.4"
        ]
    },
    {
        "title": "Modeling Ambiguity in a Multi-Agent System",
        "authors": [
            "Christof Monz"
        ],
        "summary": "This paper investigates the formal pragmatics of ambiguous expressions by modeling ambiguity in a multi-agent system. Such a framework allows us to give a more refined notion of the kind of information that is conveyed by ambiguous expressions. We analyze how ambiguity affects the knowledge of the dialog participants and, especially, what they know about each other after an ambiguous sentence has been uttered. The agents communicate with each other by means of a TELL-function, whose application is constrained by an implementation of some of Grice's maxims. The information states of the multi-agent system itself are represented as a Kripke structures and TELL is an update function on those structures. This framework enables us to distinguish between the information conveyed by ambiguous sentences vs. the information conveyed by disjunctions, and between semantic ambiguity vs. perceived ambiguity.",
        "published": "2000-09-19T15:43:18Z",
        "link": "http://arxiv.org/abs/cs/0009012v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.MA",
            "F.4.1; I.2.7"
        ]
    },
    {
        "title": "Improving Performance of heavily loaded agents",
        "authors": [
            "Fatma Ozcan",
            "VS Subrahmanian",
            "Juergen Dix"
        ],
        "summary": "With the increase in agent-based applications, there are now agent systems that support \\emph{concurrent} client accesses. The ability to process large volumes of simultaneous requests is critical in many such applications. In such a setting, the traditional approach of serving these requests one at a time via queues (e.g. \\textsf{FIFO} queues, priority queues) is insufficient. Alternative models are essential to improve the performance of such \\emph{heavily loaded} agents. In this paper, we propose a set of \\emph{cost-based algorithms} to \\emph{optimize} and \\emph{merge} multiple requests submitted to an agent. In order to merge a set of requests, one first needs to identify commonalities among such requests. First, we provide an \\emph{application independent framework} within which an agent developer may specify relationships (called \\emph{invariants}) between requests. Second, we provide two algorithms (and various accompanying heuristics) which allow an agent to automatically rewrite requests so as to avoid redundant work---these algorithms take invariants associated with the agent into account. Our algorithms are independent of any specific agent framework. For an implementation, we implemented both these algorithms on top of the \\impact agent development platform, and on top of a (non-\\impact) geographic database agent. Based on these implementations, we conducted experiments and show that our algorithms are considerably more efficient than methods that use the $A^*$ algorithm.",
        "published": "2000-12-11T10:17:36Z",
        "link": "http://arxiv.org/abs/cs/0012004v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.12;I.2.3;D.2.12;H.2.4"
        ]
    },
    {
        "title": "Minimum Description Length and Compositionality",
        "authors": [
            "Wlodek Zadrozny"
        ],
        "summary": "We present a non-vacuous definition of compositionality. It is based on the idea of combining the minimum description length principle with the original definition of compositionality (that is, that the meaning of the whole is a function of the meaning of the parts).   The new definition is intuitive and allows us to distinguish between compositional and non-compositional semantics, and between idiomatic and non-idiomatic expressions. It is not ad hoc, since it does not make any references to non-intrinsic properties of meaning functions (like being a polynomial). Moreover, it allows us to compare different meaning functions with respect to how compositional they are. It bridges linguistic and corpus-based, statistical approaches to natural language understanding.",
        "published": "2000-01-04T21:46:29Z",
        "link": "http://arxiv.org/abs/cs/0001002v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Compositionality, Synonymy, and the Systematic Representation of Meaning",
        "authors": [
            "Shalom Lappin",
            "Wlodek Zadrozny"
        ],
        "summary": "In a recent issue of Linguistics and Philosophy Kasmi and Pelletier (1998) (K&P), and Westerstahl (1998) criticize Zadrozny's (1994) argument that any semantics can be represented compositionally. The argument is based upon Zadrozny's theorem that every meaning function m can be encoded by a function \\mu such that (i) for any expression E of a specified language L, m(E) can be recovered from \\mu(E), and (ii) \\mu is a homomorphism from the syntactic structures of L to interpretations of L.   In both cases, the primary motivation for the objections brought against Zadrozny's argument is the view that his encoding of the original meaning function does not properly reflect the synonymy relations posited for the language.   In this paper, we argue that these technical criticisms do not go through. In particular, we prove that \\mu properly encodes synonymy relations, i.e. if two expressions are synonymous, then their compositional meanings are identical. This corrects some misconceptions about the function \\mu, e.g. Janssen (1997).   We suggest that the reason that semanticists have been anxious to preserve compositionality as a significant constraint on semantic theory is that it has been mistakenly regarded as a condition that must be satisfied by any theory that sustains a systematic connection between the meaning of an expression and the meanings of its parts. Recent developments in formal and computational semantics show that systematic theories of meanings need not be compositional.",
        "published": "2000-01-09T23:54:15Z",
        "link": "http://arxiv.org/abs/cs/0001006v1",
        "categories": [
            "cs.CL",
            "cs.LO",
            "I.2.7; F.4.1"
        ]
    },
    {
        "title": "A Real World Implementation of Answer Extraction",
        "authors": [
            "D. Molla",
            "J. Berri",
            "M. Hess"
        ],
        "summary": "In this paper we describe ExtrAns, an answer extraction system. Answer extraction (AE) aims at retrieving those exact passages of a document that directly answer a given user question. AE is more ambitious than information retrieval and information extraction in that the retrieval results are phrases, not entire documents, and in that the queries may be arbitrarily specific. It is less ambitious than full-fledged question answering in that the answers are not generated from a knowledge base but looked up in the text of documents. The current version of ExtrAns is able to parse unedited Unix \"man pages\", and derive the logical form of their sentences. User queries are also translated into logical forms. A theorem prover then retrieves the relevant phrases, which are presented through selective highlighting in their context.",
        "published": "2000-01-14T16:01:31Z",
        "link": "http://arxiv.org/abs/cs/0001010v1",
        "categories": [
            "cs.CL",
            "H.3.1; I.2.3; I.2.7"
        ]
    },
    {
        "title": "Measures of Distributional Similarity",
        "authors": [
            "Lillian Lee"
        ],
        "summary": "We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences. Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification of similarity functions based on the information that they incorporate; and the introduction of a novel function that is superior at evaluating potential proxy distributions.",
        "published": "2000-01-18T23:19:22Z",
        "link": "http://arxiv.org/abs/cs/0001012v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Exploiting Syntactic Structure for Natural Language Modeling",
        "authors": [
            "Ciprian Chelba"
        ],
        "summary": "The thesis presents an attempt at using the syntactic structure in natural language for improved language models for speech recognition. The structured language model merges techniques in automatic parsing and language modeling using an original probabilistic parameterization of a shift-reduce parser. A maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms is employed for training the model. Experiments on the Wall Street Journal, Switchboard and Broadcast News corpora show improvement in both perplexity and word error rate - word lattice rescoring - over the standard 3-gram language model. The significance of the thesis lies in presenting an original approach to language modeling that uses the hierarchical - syntactic - structure in natural language to improve on current 3-gram modeling techniques for large vocabulary speech recognition.",
        "published": "2000-01-24T20:18:16Z",
        "link": "http://arxiv.org/abs/cs/0001020v1",
        "categories": [
            "cs.CL",
            "G.3, I.2.7, I.5.1, I.5.4"
        ]
    },
    {
        "title": "Refinement of a Structured Language Model",
        "authors": [
            "Ciprian Chelba",
            "Frederick Jelinek"
        ],
        "summary": "A new language model for speech recognition inspired by linguistic analysis is presented. The model develops hidden hierarchical structure incrementally and uses it to extract meaningful information from the word history - thus enabling the use of extended distance dependencies - in an attempt to complement the locality of currently used n-gram Markov models. The model, its probabilistic parametrization, a reestimation algorithm for the model parameters and a set of experiments meant to evaluate its potential for speech recognition are presented.",
        "published": "2000-01-24T20:55:17Z",
        "link": "http://arxiv.org/abs/cs/0001021v1",
        "categories": [
            "cs.CL",
            "G.3, I.2.7, I.5.1, I.5.4"
        ]
    },
    {
        "title": "Recognition Performance of a Structured Language Model",
        "authors": [
            "Ciprian Chelba",
            "Frederick Jelinek"
        ],
        "summary": "A new language model for speech recognition inspired by linguistic analysis is presented. The model develops hidden hierarchical structure incrementally and uses it to extract meaningful information from the word history - thus enabling the use of extended distance dependencies - in an attempt to complement the locality of currently used trigram models. The structured language model, its probabilistic parameterization and performance in a two-pass speech recognizer are presented. Experiments on the SWITCHBOARD corpus show an improvement in both perplexity and word error rate over conventional trigram models.",
        "published": "2000-01-24T21:18:37Z",
        "link": "http://arxiv.org/abs/cs/0001022v1",
        "categories": [
            "cs.CL",
            "G.3, I.2.7, I.5.1, I.5.4"
        ]
    },
    {
        "title": "Structured Language Modeling for Speech Recognition",
        "authors": [
            "Ciprian Chelba",
            "Frederick Jelinek"
        ],
        "summary": "A new language model for speech recognition is presented. The model develops hidden hierarchical syntactic-like structure incrementally and uses it to extract meaningful information from the word history, thus complementing the locality of currently used trigram models. The structured language model (SLM) and its performance in a two-pass speech recognizer --- lattice decoding --- are presented. Experiments on the WSJ corpus show an improvement in both perplexity (PPL) and word error rate (WER) over conventional trigram models.",
        "published": "2000-01-25T19:35:01Z",
        "link": "http://arxiv.org/abs/cs/0001023v1",
        "categories": [
            "cs.CL",
            "G.3, I.2.7, I.5.1, I.5.4"
        ]
    },
    {
        "title": "Requirements of Text Processing Lexicons",
        "authors": [
            "K. Litkowski"
        ],
        "summary": "As text processing systems expand in scope, they will require ever larger lexicons along with a parsing capability for discriminating among many senses of a word. Existing systems do not incorporate such subtleties in meaning for their lexicons. Ordinary dictionaries contain such information, but are largely untapped. When the contents of dictionaries are scrutinized, they reveal many requirements that must be satisfied in representing meaning and in developing semantic parsers. These requirements were identified in research designed to find primitive verb concepts. The requirements are outlined and general procedures for satisfying them through the use of ordinary dictionaries are described, illustrated by building frames for and examining the definitions of \"change\" and its uses as a hypernym in other definitions.",
        "published": "2000-02-11T19:17:18Z",
        "link": "http://arxiv.org/abs/cs/0002007v1",
        "categories": [
            "cs.CL",
            "H.3.1; I.2.7; I.2.4"
        ]
    },
    {
        "title": "An Usage Measure Based on Psychophysical Relations",
        "authors": [
            "V. Kromer"
        ],
        "summary": "A new word usage measure is proposed. It is based on psychophysical relations and allows to reveal words by its degree of \"importance\" for making basic dictionaries of sublanguages.",
        "published": "2000-02-27T04:09:59Z",
        "link": "http://arxiv.org/abs/cs/0002017v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Prospects for in-depth story understanding by computer",
        "authors": [
            "Erik T. Mueller"
        ],
        "summary": "While much research on the hard problem of in-depth story understanding by computer was performed starting in the 1970s, interest shifted in the 1990s to information extraction and word sense disambiguation. Now that a degree of success has been achieved on these easier problems, I propose it is time to return to in-depth story understanding. In this paper I examine the shift away from story understanding, discuss some of the major problems in building a story understanding system, present some possible solutions involving a set of interacting understanding agents, and provide pointers to useful tools and resources for building story understanding systems.",
        "published": "2000-03-01T18:01:06Z",
        "link": "http://arxiv.org/abs/cs/0003003v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A database and lexicon of scripts for ThoughtTreasure",
        "authors": [
            "Erik T. Mueller"
        ],
        "summary": "Since scripts were proposed in the 1970's as an inferencing mechanism for AI and natural language processing programs, there have been few attempts to build a database of scripts. This paper describes a database and lexicon of scripts that has been added to the ThoughtTreasure commonsense platform. The database provides the following information about scripts: sequence of events, roles, props, entry conditions, results, goals, emotions, places, duration, frequency, and cost. English and French words and phrases are linked to script concepts.",
        "published": "2000-03-01T18:07:02Z",
        "link": "http://arxiv.org/abs/cs/0003004v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "I.2.7; I.2.4"
        ]
    },
    {
        "title": "Hypothetical revision and matter-of-fact supposition",
        "authors": [
            "Horacio Arlo-Costa"
        ],
        "summary": "The paper studies the notion of supposition encoded in non-Archimedean conditional probability (and revealed in the acceptance of the so-called indicative conditionals). The notion of qualitative change of view that thus arises is axiomatized and compared with standard notions like AGM and UPDATE. Applications in the following fields are discussed: (1) theory of games and decisions, (2) causal models, (3) non-monotonic logic.",
        "published": "2000-03-08T16:06:58Z",
        "link": "http://arxiv.org/abs/cs/0003022v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "I.2.4; I.2.6; I.2.7; I.2.11"
        ]
    },
    {
        "title": "TnT - A Statistical Part-of-Speech Tagger",
        "authors": [
            "Thorsten Brants"
        ],
        "summary": "Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora.",
        "published": "2000-03-13T09:55:08Z",
        "link": "http://arxiv.org/abs/cs/0003055v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Message Classification in the Call Center",
        "authors": [
            "Stephan Busemann",
            "Sven Schmeier",
            "Roman G. Arens"
        ],
        "summary": "Customer care in technical domains is increasingly based on e-mail communication, allowing for the reproduction of approved solutions. Identifying the customer's problem is often time-consuming, as the problem space changes if new products are launched. This paper describes a new approach to the classification of e-mail requests based on shallow text processing and machine learning techniques. It is implemented within an assistance system for call center agents that is used in a commercial setting.",
        "published": "2000-03-14T13:09:28Z",
        "link": "http://arxiv.org/abs/cs/0003060v1",
        "categories": [
            "cs.CL",
            "I.2.6; I.2.7; I.7.5"
        ]
    },
    {
        "title": "A Finite State and Data-Oriented Method for Grapheme to Phoneme   Conversion",
        "authors": [
            "Gosse Bouma"
        ],
        "summary": "A finite-state method, based on leftmost longest-match replacement, is presented for segmenting words into graphemes, and for converting graphemes into phonemes. A small set of hand-crafted conversion rules for Dutch achieves a phoneme accuracy of over 93%. The accuracy of the system is further improved by using transformation-based learning. The phoneme accuracy of the best system (using a large set of rule templates and a `lazy' variant of Brill's algoritm), trained on only 40K words, reaches 99% accuracy.",
        "published": "2000-03-23T11:29:15Z",
        "link": "http://arxiv.org/abs/cs/0003074v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Variable Word Rate N-grams",
        "authors": [
            "Yoshihiko Gotoh",
            "Steve Renals"
        ],
        "summary": "The rate of occurrence of words is not uniform but varies from document to document. Despite this observation, parameters for conventional n-gram language models are usually derived using the assumption of a constant word rate. In this paper we investigate the use of variable word rate assumption, modelled by a Poisson distribution or a continuous mixture of Poissons. We present an approach to estimating the relative frequencies of words or n-grams taking prior information of their occurrences into account. Discounting and smoothing schemes are also considered. Using the Broadcast News task, the approach demonstrates a reduction of perplexity up to 10%.",
        "published": "2000-03-29T16:35:58Z",
        "link": "http://arxiv.org/abs/cs/0003081v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Information Extraction from Broadcast News",
        "authors": [
            "Yoshihiko Gotoh",
            "Steve Renals"
        ],
        "summary": "This paper discusses the development of trainable statistical models for extracting content from television and radio news broadcasts. In particular we concentrate on statistical finite state models for identifying proper names and other named entities in broadcast speech. Two models are presented: the first represents name class information as a word attribute; the second represents both word-word and class-class transitions explicitly. A common n-gram based formulation is used for both models. The task of named entity identification is characterized by relatively sparse training data and issues related to smoothing are discussed. Experiments are reported using the DARPA/NIST Hub-4E evaluation for North American Broadcast News.",
        "published": "2000-03-30T16:52:50Z",
        "link": "http://arxiv.org/abs/cs/0003084v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Advances in domain independent linear text segmentation",
        "authors": [
            "Freddy Y. Y. Choi"
        ],
        "summary": "This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering.",
        "published": "2000-03-30T16:56:02Z",
        "link": "http://arxiv.org/abs/cs/0003083v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "How to Evaluate your Question Answering System Every Day and Still Get   Real Work Done",
        "authors": [
            "Eric Breck",
            "John D. Burger",
            "Lisa Ferro",
            "Lynette Hirschman",
            "David House",
            "Marc Light",
            "Inderjeet Mani"
        ],
        "summary": "In this paper, we report on Qaviar, an experimental automated evaluation system for question answering applications. The goal of our research was to find an automatically calculated measure that correlates well with human judges' assessment of answer correctness in the context of question answering tasks. Qaviar judges the response by computing recall against the stemmed content words in the human-generated answer key. It counts the answer correct if it exceeds agiven recall threshold. We determined that the answer correctness predicted by Qaviar agreed with the human 93% to 95% of the time. 41 question-answering systems were ranked by both Qaviar and human assessors, and these rankings correlated with a Kendall's Tau measure of 0.920, compared to a correlation of 0.956 between human assessors on the same data.",
        "published": "2000-04-17T19:29:51Z",
        "link": "http://arxiv.org/abs/cs/0004008v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "I.2.7; H.3.4"
        ]
    },
    {
        "title": "Looking at discourse in a corpus: The role of lexical cohesion",
        "authors": [
            "Tony Berber Sardinha"
        ],
        "summary": "This paper is aimed at reporting on the development and application of a computer model for discourse analysis through segmentation. Segmentation refers to the principled division of texts into contiguous constituents. Other studies have looked at the application of a number of models to the analysis of discourse by computer. The segmentation procedure developed for the present investigation is called LSM ('Link Set Median'). It was applied to three corpus of 300 texts from three different genres. The results obtained by application of the LSM procedure on the corpus were then compared to segmentation carried out at random. Statistical analyses suggested that LSM significantly outperformed random segmentation, thus indicating that the segmentation was meaningful.",
        "published": "2000-04-28T03:18:09Z",
        "link": "http://arxiv.org/abs/cs/0004016v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A Simple Approach to Building Ensembles of Naive Bayesian Classifiers   for Word Sense Disambiguation",
        "authors": [
            "Ted Pedersen"
        ],
        "summary": "This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co--occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disambiguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rivaling the best previously published results.",
        "published": "2000-05-07T00:15:59Z",
        "link": "http://arxiv.org/abs/cs/0005006v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Noun Phrase Recognition by System Combination",
        "authors": [
            "Erik F. Tjong Kim Sang"
        ],
        "summary": "The performance of machine learning algorithms can be improved by combining the output of different systems. In this paper we apply this idea to the recognition of noun phrases.We generate different classifiers by using different representations of the data. By combining the results with voting techniques described in (Van Halteren et.al. 1998) we manage to improve the best reported performances on standard data sets for base noun phrases and arbitrary noun phrases.",
        "published": "2000-05-10T11:58:12Z",
        "link": "http://arxiv.org/abs/cs/0005015v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Improving Testsuites via Instrumentation",
        "authors": [
            "Norbert Broeker"
        ],
        "summary": "This paper explores the usefulness of a technique from software engineering, namely code instrumentation, for the development of large-scale natural language grammars. Information about the usage of grammar rules in test sentences is used to detect untested rules, redundant test sentences, and likely causes of overgeneration. Results show that less than half of a large-coverage grammar for German is actually tested by two large testsuites, and that 10-30% of testing time is redundant. The methodology applied can be seen as a re-use of grammar writing knowledge for testsuite compilation.",
        "published": "2000-05-10T14:51:14Z",
        "link": "http://arxiv.org/abs/cs/0005016v1",
        "categories": [
            "cs.CL",
            "D.2.5"
        ]
    },
    {
        "title": "On the Scalability of the Answer Extraction System \"ExtrAns\"",
        "authors": [
            "Diego Moll'a Aliod",
            "Michael Hess"
        ],
        "summary": "This paper reports on the scalability of the answer extraction system ExtrAns. An answer extraction system locates the exact phrases in the documents that contain the explicit answers to the user queries. Answer extraction systems are therefore more convenient than document retrieval systems in situations where the user wants to find specific information in limited time.   ExtrAns performs answer extraction over UNIX manpages. It has been constructed by combining available linguistic resources and implementing only a few modules from scratch. A resolution procedure between the minimal logical form of the user query and the minimal logical forms of the manpage sentences finds the answers to the queries. These answers are displayed to the user, together with pointers to the respective manpages, and the exact phrases that contribute to the answer are highlighted.   This paper shows that the increase in response times is not a big issue when scaling the system up from 30 to 500 documents, and that the response times for 500 documents are still acceptable for a real-time answer extraction system.",
        "published": "2000-05-12T14:12:15Z",
        "link": "http://arxiv.org/abs/cs/0005019v1",
        "categories": [
            "cs.CL",
            "H.3.1; I.2.3; I.2.7"
        ]
    },
    {
        "title": "Centroid-based summarization of multiple documents: sentence extraction,   utility-based evaluation, and user studies",
        "authors": [
            "Dragomir R. Radev",
            "Hongyan Jing",
            "Malgorzata Budzikowska"
        ],
        "summary": "We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.",
        "published": "2000-05-12T17:24:06Z",
        "link": "http://arxiv.org/abs/cs/0005020v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DL",
            "cs.HC",
            "cs.IR",
            "H.3.1; H.3.4; H.3.7; H.5.2; I.2.7"
        ]
    },
    {
        "title": "Finite-State Reduplication in One-Level Prosodic Morphology",
        "authors": [
            "Markus Walther"
        ],
        "summary": "Reduplication, a central instance of prosodic morphology, is particularly challenging for state-of-the-art computational morphology, since it involves copying of some part of a phonological string. In this paper I advocate a finite-state method that combines enriched lexical representations via intersection to implement the copying. The proposal includes a resource-conscious variant of automata and can benefit from the existence of lazy algorithms. Finally, the implementation of a complex case from Koasati is presented.",
        "published": "2000-05-22T14:07:28Z",
        "link": "http://arxiv.org/abs/cs/0005025v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Ranking suspected answers to natural language questions using predictive   annotation",
        "authors": [
            "Dragomir R. Radev",
            "John Prager",
            "Valerie Samn"
        ],
        "summary": "In this paper, we describe a system to rank suspected answers to natural language questions. We process both corpus and query using a new technique, predictive annotation, which augments phrases in texts with labels anticipating their being targets of certain kinds of questions. Given a natural language question, an IR system returns a set of matching passages, which are then analyzed and ranked according to various criteria described in this paper. We provide an evaluation of the techniques based on results from the TREC Q&A evaluation in which our system participated.",
        "published": "2000-05-30T17:10:33Z",
        "link": "http://arxiv.org/abs/cs/0005029v1",
        "categories": [
            "cs.CL",
            "H.3.1;H.3.3;H.3.4;H.5.2;I.2.7"
        ]
    },
    {
        "title": "Exploiting Diversity in Natural Language Processing: Combining Parsers",
        "authors": [
            "John C. Henderson",
            "Eric Brill"
        ],
        "summary": "Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy. Two general approaches are presented and two combination techniques are described for each approach. Both parametric and non-parametric models are explored. The resulting parsers surpass the best previously published performance results for the Penn Treebank.",
        "published": "2000-06-01T18:42:24Z",
        "link": "http://arxiv.org/abs/cs/0006003v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Bagging and Boosting a Treebank Parser",
        "authors": [
            "John C. Henderson",
            "Eric Brill"
        ],
        "summary": "Bagging and boosting, two effective machine learning techniques, are applied to natural language parsing. Experiments using these techniques with a trainable statistical parser are described. The best resulting system provides roughly as large of a gain in F-measure as doubling the corpus size. Error analysis of the result of the boosting technique reveals some inconsistent annotations in the Penn Treebank, suggesting a semi-automatic method for finding inconsistent treebank annotations.",
        "published": "2000-06-05T18:04:51Z",
        "link": "http://arxiv.org/abs/cs/0006011v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Exploiting Diversity for Natural Language Parsing",
        "authors": [
            "John C. Henderson"
        ],
        "summary": "The popularity of applying machine learning methods to computational linguistics problems has produced a large supply of trainable natural language processing systems. Most problems of interest have an array of off-the-shelf products or downloadable code implementing solutions using various techniques. Where these solutions are developed independently, it is observed that their errors tend to be independently distributed. This thesis is concerned with approaches for capitalizing on this situation in a sample problem domain, Penn Treebank-style parsing.   The machine learning community provides techniques for combining outputs of classifiers, but parser output is more structured and interdependent than classifications. To address this discrepancy, two novel strategies for combining parsers are used: learning to control a switch between parsers and constructing a hybrid parse from multiple parsers' outputs.   Off-the-shelf parsers are not developed with an intention to perform well in a collaborative ensemble. Two techniques are presented for producing an ensemble of parsers that collaborate. All of the ensemble members are created using the same underlying parser induction algorithm, and the method for producing complementary parsers is only loosely constrained by that chosen algorithm.",
        "published": "2000-06-05T21:33:03Z",
        "link": "http://arxiv.org/abs/cs/0006012v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "An evaluation of Naive Bayesian anti-spam filtering",
        "authors": [
            "Ion Androutsopoulos",
            "John Koutsias",
            "Konstantinos V. Chandrinos",
            "George Paliouras",
            "Constantine D. Spyropoulos"
        ],
        "summary": "It has recently been argued that a Naive Bayesian classifier can be used to filter unsolicited bulk e-mail (\"spam\"). We conduct a thorough evaluation of this proposal on a corpus that we make publicly available, contributing towards standard benchmarks. At the same time we investigate the effect of attribute-set size, training-corpus size, lemmatization, and stop-lists on the filter's performance, issues that had not been previously explored. After introducing appropriate cost-sensitive evaluation measures, we reach the conclusion that additional safety nets are needed for the Naive Bayesian anti-spam filter to be viable in practice.",
        "published": "2000-06-07T11:10:50Z",
        "link": "http://arxiv.org/abs/cs/0006013v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "H.4.3; I.2.6; I.2.7; I.5.4; K.4.1"
        ]
    },
    {
        "title": "Turning Speech Into Scripts",
        "authors": [
            "Manny Rayner",
            "Beth Ann Hockey",
            "Frankie James"
        ],
        "summary": "We describe an architecture for implementing spoken natural language dialogue interfaces to semi-autonomous systems, in which the central idea is to transform the input speech signal through successive levels of representation corresponding roughly to linguistic knowledge, dialogue knowledge, and domain knowledge. The final representation is an executable program in a simple scripting language equivalent to a subset of Cshell. At each stage of the translation process, an input is transformed into an output, producing as a byproduct a \"meta-output\" which describes the nature of the transformation performed. We show how consistent use of the output/meta-output distinction permits a simple and perspicuous treatment of apparently diverse topics including resolution of pronouns, correction of user misconceptions, and optimization of scripts. The methods described have been concretely realized in a prototype speech interface to a simulation of the Personal Satellite Assistant.",
        "published": "2000-06-09T17:28:40Z",
        "link": "http://arxiv.org/abs/cs/0006017v1",
        "categories": [
            "cs.CL",
            "H.5.2; I.2.7"
        ]
    },
    {
        "title": "Accuracy, Coverage, and Speed: What Do They Mean to Users?",
        "authors": [
            "Frankie James",
            "Manny Rayner",
            "Beth Ann Hockey"
        ],
        "summary": "Speech is becoming increasingly popular as an interface modality, especially in hands- and eyes-busy situations where the use of a keyboard or mouse is difficult. However, despite the fact that many have hailed speech as being inherently usable (since everyone already knows how to talk), most users of speech input are left feeling disappointed by the quality of the interaction. Clearly, there is much work to be done on the design of usable spoken interfaces. We believe that there are two major problems in the design of speech interfaces, namely, (a) the people who are currently working on the design of speech interfaces are, for the most part, not interface designers and therefore do not have as much experience with usability issues as we in the CHI community do, and (b) speech, as an interface modality, has vastly different properties than other modalities, and therefore requires different usability measures.",
        "published": "2000-06-09T18:10:58Z",
        "link": "http://arxiv.org/abs/cs/0006018v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "H.5.2; I.2.7"
        ]
    },
    {
        "title": "A Comparison of the XTAG and CLE Grammars for English",
        "authors": [
            "Beth Ann Hockey",
            "Manny Rayner",
            "Frankie James"
        ],
        "summary": "When people develop something intended as a large broad-coverage grammar, they usually have a more specific goal in mind. Sometimes this goal is covering a corpus; sometimes the developers have theoretical ideas they wish to investigate; most often, work is driven by a combination of these two main types of goal. What tends to happen after a while is that the community of people working with the grammar starts thinking of some phenomena as ``central'', and makes serious efforts to deal with them; other phenomena are labelled ``marginal'', and ignored. Before long, the distinction between ``central'' and ``marginal'' becomes so ingrained that it is automatic, and people virtually stop thinking about the ``marginal'' phenomena. In practice, the only way to bring the marginal things back into focus is to look at what other people are doing and compare it with one's own work. In this paper, we will take two large grammars, XTAG and the CLE, and examine each of them from the other's point of view. We will find in both cases not only that important things are missing, but that the perspective offered by the other grammar suggests simple and practical ways of filling in the holes. It turns out that there is a pleasing symmetry to the picture. XTAG has a very good treatment of complement structure, which the CLE to some extent lacks; conversely, the CLE offers a powerful and general account of adjuncts, which the XTAG grammar does not fully duplicate. If we examine the way in which each grammar does the thing it is good at, we find that the relevant methods are quite easy to port to the other framework, and in fact only involve generalization and systematization of existing mechanisms.",
        "published": "2000-06-09T20:49:03Z",
        "link": "http://arxiv.org/abs/cs/0006020v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A Compact Architecture for Dialogue Management Based on Scripts and   Meta-Outputs",
        "authors": [
            "Manny Rayner",
            "Beth Ann Hockey",
            "Frankie James"
        ],
        "summary": "We describe an architecture for spoken dialogue interfaces to semi-autonomous systems that transforms speech signals through successive representations of linguistic, dialogue, and domain knowledge. Each step produces an output, and a meta-output describing the transformation, with an executable program in a simple scripting language as the final result. The output/meta-output distinction permits perspicuous treatment of diverse tasks such as resolving pronouns, correcting user misconceptions, and optimizing scripts.",
        "published": "2000-06-09T21:41:54Z",
        "link": "http://arxiv.org/abs/cs/0006019v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.5.2"
        ]
    },
    {
        "title": "Compiling Language Models from a Linguistically Motivated Unification   Grammar",
        "authors": [
            "Manny Rayner",
            "Beth Ann Hockey",
            "Frankie James",
            "Elizabeth O. Bratt",
            "Sharon Goldwater",
            "Mark Gawron"
        ],
        "summary": "Systems now exist which are able to compile unification grammars into language models that can be included in a speech recognizer, but it is so far unclear whether non-trivial linguistically principled grammars can be used for this purpose. We describe a series of experiments which investigate the question empirically, by incrementally constructing a grammar and discovering what problems emerge when successively larger versions are compiled into finite state graph representations and used as language models for a medium-vocabulary recognition task.",
        "published": "2000-06-09T22:03:10Z",
        "link": "http://arxiv.org/abs/cs/0006021v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Can Prosody Aid the Automatic Classification of Dialog Acts in   Conversational Speech?",
        "authors": [
            "E. Shriberg",
            "R. Bates",
            "A. Stolcke",
            "P. Taylor",
            "D. Jurafsky",
            "K. Ries",
            "N. Coccaro",
            "R. Martin",
            "M. Meteer",
            "C. Van Ess-Dykema"
        ],
        "summary": "Identifying whether an utterance is a statement, question, greeting, and so forth is integral to effective automatic understanding of natural dialog. Little is known, however, about how such dialog acts (DAs) can be automatically classified in truly natural conversation. This study asks whether current approaches, which use mainly word information, could be improved by adding prosodic information. The study is based on more than 1000 conversations from the Switchboard corpus. DAs were hand-annotated, and prosodic features (duration, pause, F0, energy, and speaking rate) were automatically extracted for each DA. In training, decision trees based on these features were inferred; trees were then applied to unseen test data to evaluate performance. Performance was evaluated for prosody models alone, and after combining the prosody models with word information -- either from true words or from the output of an automatic speech recognizer. For an overall classification task, as well as three subtasks, prosody made significant contributions to classification. Feature-specific analyses further revealed that although canonical features (such as F0 for questions) were important, less obvious features could compensate if canonical features were removed. Finally, in each task, integrating the prosodic model with a DA-specific statistical language model improved performance over that of the language model alone, especially for the case of recognized words. Results suggest that DAs are redundantly marked in natural conversation, and that a variety of automatically extractable prosodic features could aid dialog processing in speech applications.",
        "published": "2000-06-11T06:00:11Z",
        "link": "http://arxiv.org/abs/cs/0006024v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Dialogue Act Modeling for Automatic Tagging and Recognition of   Conversational Speech",
        "authors": [
            "A. Stolcke",
            "K. Ries",
            "N. Coccaro",
            "E. Shriberg",
            "R. Bates",
            "D. Jurafsky",
            "P. Taylor",
            "R. Martin",
            "C. Van Ess-Dykema",
            "M. Meteer"
        ],
        "summary": "We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech-act-like units such as Statement, Question, Backchannel, Agreement, Disagreement, and Apology. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence. The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act. We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy. Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We achieved good dialogue act labeling accuracy (65% based on errorful, automatically recognized words and prosody, and 71% based on word transcripts, compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction in word recognition error.",
        "published": "2000-06-11T06:06:10Z",
        "link": "http://arxiv.org/abs/cs/0006023v2",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Entropy-based Pruning of Backoff Language Models",
        "authors": [
            "A. Stolcke"
        ],
        "summary": "A criterion for pruning parameters from N-gram backoff language models is developed, based on the relative entropy between the original and the pruned model. It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models. The relative entropy measure can be expressed as a relative change in training set perplexity. This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold are removed from the model. Experiments show that a production-quality Hub4 LM can be reduced to 26% its original size without increasing recognition error. We also compare the approach to a heuristic pruning criterion by Seymore and Rosenfeld (1996), and show that their approach can be interpreted as an approximation to the relative entropy criterion. Experimentally, both approaches select similar sets of N-grams (about 85% overlap), with the exact relative entropy criterion giving marginally better performance.",
        "published": "2000-06-11T18:20:20Z",
        "link": "http://arxiv.org/abs/cs/0006025v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Verbal Interactions in Virtual Worlds",
        "authors": [
            "Pierre Nugues"
        ],
        "summary": "We first discuss respective advantages of language interaction in virtual worlds and of using 3D images in dialogue systems. Then, we describe an example of a verbal interaction system in virtual reality: Ulysse. Ulysse is a conversational agent that helps a user navigate in virtual worlds. It has been designed to be embedded in the representation of a participant of a virtual conference and it responds positively to motion orders. Ulysse navigates the user's viewpoint on his/her behalf in the virtual world. On tests we carried out, we discovered that users, novices as well as experienced ones have difficulties moving in a 3D environment. Agents such as Ulysse enable a user to carry out navigation motions that would have been impossible with classical interaction devices. From the whole Ulysse system, we have stripped off a skeleton architecture that we have ported to VRML, Java, and Prolog. We hope this skeleton helps the design of language applications in virtual worlds.",
        "published": "2000-06-13T09:50:03Z",
        "link": "http://arxiv.org/abs/cs/0006027v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "H.5.2; I.2.7"
        ]
    },
    {
        "title": "Trainable Methods for Surface Natural Language Generation",
        "authors": [
            "Adwait Ratnaparkhi"
        ],
        "summary": "We present three systems for surface natural language generation that are trainable from annotated corpora. The first two systems, called NLG1 and NLG2, require a corpus marked only with domain-specific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information. All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation. NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase. The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase. We present experiments in which we generate phrases to describe flights in the air travel domain.",
        "published": "2000-06-13T21:06:27Z",
        "link": "http://arxiv.org/abs/cs/0006028v1",
        "categories": [
            "cs.CL",
            "I.2.7; I.2.6"
        ]
    },
    {
        "title": "Estimation of English and non-English Language Use on the WWW",
        "authors": [
            "Gregory Grefenstette",
            "Julien Nioche"
        ],
        "summary": "The World Wide Web has grown so big, in such an anarchic fashion, that it is difficult to describe. One of the evident intrinsic characteristics of the World Wide Web is its multilinguality. Here, we present a technique for estimating the size of a language-specific corpus given the frequency of commonly occurring words in the corpus. We apply this technique to estimating the number of words available through Web browsers for given languages. Comparing data from 1996 to data from 1999 and 2000, we calculate the growth of a number of European languages on the Web. As expected, non-English languages are growing at a faster pace than English, though the position of English is still dominant.",
        "published": "2000-06-23T09:43:27Z",
        "link": "http://arxiv.org/abs/cs/0006032v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "J.5; H.5.2; I.2.7; H.3.5; H.5.3"
        ]
    },
    {
        "title": "Prosody-Based Automatic Segmentation of Speech into Sentences and Topics",
        "authors": [
            "E. Shriberg",
            "A. Stolcke",
            "D. Hakkani-Tur",
            "G. Tur"
        ],
        "summary": "A crucial step in processing speech audio data for information extraction, topic detection, or browsing/playback is to segment the input into sentence and topic units. Speech segmentation is challenging, since the cues typically present for segmenting text (headers, paragraphs, punctuation) are absent in spoken language. We investigate the use of prosody (information gleaned from the timing and melody of speech) for these tasks. Using decision tree and hidden Markov modeling techniques, we combine prosodic cues with word-based approaches, and evaluate performance on two speech corpora, Broadcast News and Switchboard. Results show that the prosodic model alone performs on par with, or better than, word-based statistical language models -- for both true and automatically recognized words in news speech. The prosodic model achieves comparable performance with significantly less training data, and requires no hand-labeling of prosodic events. Across tasks and corpora, we obtain a significant improvement over word-only models using a probabilistic combination of prosodic and lexical information. Inspection reveals that the prosodic models capture language-independent boundary indicators described in the literature. Finally, cue usage is task and corpus dependent. For example, pause and pitch features are highly informative for segmenting news speech, whereas pause, duration and word-based cues dominate for natural conversation.",
        "published": "2000-06-27T04:39:57Z",
        "link": "http://arxiv.org/abs/cs/0006036v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Approximation and Exactness in Finite State Optimality Theory",
        "authors": [
            "Dale Gerdemann",
            "Gertjan van Noord"
        ],
        "summary": "Previous work (Frank and Satta 1998; Karttunen, 1998) has shown that Optimality Theory with gradient constraints generally is not finite state. A new finite-state treatment of gradient constraints is presented which improves upon the approximation of Karttunen (1998). The method turns out to be exact, and very compact, for the syllabification analysis of Prince and Smolensky (1993).",
        "published": "2000-06-28T10:06:02Z",
        "link": "http://arxiv.org/abs/cs/0006038v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Using a Diathesis Model for Semantic Parsing",
        "authors": [
            "Jordi Atserias",
            "Irene Castellon",
            "Montse Civit",
            "German Rigau"
        ],
        "summary": "This paper presents a semantic parsing approach for unrestricted texts. Semantic parsing is one of the major bottlenecks of Natural Language Understanding (NLU) systems and usually requires building expensive resources not easily portable to other domains. Our approach obtains a case-role analysis, in which the semantic roles of the verb are identified. In order to cover all the possible syntactic realisations of a verb, our system combines their argument structure with a set of general semantic labelled diatheses models. Combining them, the system builds a set of syntactic-semantic patterns with their own role-case representation. Once the patterns are build, we use an approximate tree pattern-matching algorithm to identify the most reliable pattern for a sentence. The pattern matching is performed between the syntactic-semantic patterns and the feature-structure tree representing the morphological, syntactical and semantic information of the analysed sentence. For sentences assigned to the correct model, the semantic parsing system we are presenting identifies correctly more than 73% of possible semantic case-roles.",
        "published": "2000-06-29T07:44:16Z",
        "link": "http://arxiv.org/abs/cs/0006041v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7;I.5"
        ]
    },
    {
        "title": "Semantic Parsing based on Verbal Subcategorization",
        "authors": [
            "Jordi Atserias",
            "Irene Castellon",
            "Montse Civit",
            "German Rigau"
        ],
        "summary": "The aim of this work is to explore new methodologies on Semantic Parsing for unrestricted texts. Our approach follows the current trends in Information Extraction (IE) and is based on the application of a verbal subcategorization lexicon (LEXPIR) by means of complex pattern recognition techniques. LEXPIR is framed on the theoretical model of the verbal subcategorization developed in the Pirapides project.",
        "published": "2000-06-29T09:17:45Z",
        "link": "http://arxiv.org/abs/cs/0006042v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7;I.5"
        ]
    },
    {
        "title": "Finite-State Non-Concatenative Morphotactics",
        "authors": [
            "Kenneth R. Beesley",
            "Lauri Karttunen"
        ],
        "summary": "Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems. However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena. We describe a new technique for constructing finite-state transducers that involves reapplying the regular-expression compiler to its own output. Implemented in an algorithm called compile-replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation.",
        "published": "2000-06-30T13:22:33Z",
        "link": "http://arxiv.org/abs/cs/0006044v1",
        "categories": [
            "cs.CL",
            "A0;F1.1;J5"
        ]
    },
    {
        "title": "Incremental construction of minimal acyclic finite-state automata",
        "authors": [
            "Jan Daciuk",
            "Stoyan Mihov",
            "Bruce Watson",
            "Richard Watson"
        ],
        "summary": "In this paper, we describe a new method for constructing minimal, deterministic, acyclic finite-state automata from a set of strings. Traditional methods consist of two phases: the first to construct a trie, the second one to minimize it. Our approach is to construct a minimal automaton in a single phase by adding new strings one by one and minimizing the resulting automaton on-the-fly. We present a general algorithm as well as a specialization that relies upon the lexicographical ordering of the input strings.",
        "published": "2000-07-06T14:15:26Z",
        "link": "http://arxiv.org/abs/cs/0007009v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Boosting Applied to Word Sense Disambiguation",
        "authors": [
            "Gerard Escudero",
            "Lluis Marquez",
            "German Rigau"
        ],
        "summary": "In this paper Schapire and Singer's AdaBoost.MH boosting algorithm is applied to the Word Sense Disambiguation (WSD) problem. Initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses Naive Bayes and Exemplar-based approaches, which represent state-of-the-art accuracy on supervised WSD. In order to make boosting practical for a real learning domain of thousands of words, several ways of accelerating the algorithm by reducing the feature space are studied. The best variant, which we call LazyBoosting, is tested on the largest sense-tagged corpus available containing 192,800 examples of the 191 most frequent and ambiguous English words. Again, boosting compares favourably to the other benchmark algorithms.",
        "published": "2000-07-07T14:10:05Z",
        "link": "http://arxiv.org/abs/cs/0007010v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7;I.2.6"
        ]
    },
    {
        "title": "Naive Bayes and Exemplar-Based approaches to Word Sense Disambiguation   Revisited",
        "authors": [
            "Gerard Escudero",
            "Lluis Marquez",
            "German Rigau"
        ],
        "summary": "This paper describes an experimental comparison between two standard supervised learning methods, namely Naive Bayes and Exemplar-based classification, on the Word Sense Disambiguation (WSD) problem. The aim of the work is twofold. Firstly, it attempts to contribute to clarify some confusing information about the comparison between both methods appearing in the related literature. In doing so, several directions have been explored, including: testing several modifications of the basic learning algorithms and varying the feature space. Secondly, an improvement of both algorithms is proposed, in order to deal with large attribute sets. This modification, which basically consists in using only the positive information appearing in the examples, allows to improve greatly the efficiency of the methods, with no loss in accuracy. The experiments have been performed on the largest sense-tagged corpus available containing the most frequent and ambiguous English words. Results show that the Exemplar-based approach to WSD is generally superior to the Bayesian approach, especially when a specific metric for dealing with symbolic attributes is used.",
        "published": "2000-07-07T15:00:44Z",
        "link": "http://arxiv.org/abs/cs/0007011v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7;I.2.6"
        ]
    },
    {
        "title": "Using Learning-based Filters to Detect Rule-based Filtering Obsolescence",
        "authors": [
            "Francis Wolinski",
            "Frantz Vichot",
            "Mathieu Stricker"
        ],
        "summary": "For years, Caisse des Depots et Consignations has produced information filtering applications. To be operational, these applications require high filtering performances which are achieved by using rule-based filters. With this technique, an administrator has to tune a set of rules for each topic. However, filters become obsolescent over time. The decrease of their performances is due to diachronic polysemy of terms that involves a loss of precision and to diachronic polymorphism of concepts that involves a loss of recall.   To help the administrator to maintain his filters, we have developed a method which automatically detects filtering obsolescence. It consists in making a learning-based control filter using a set of documents which have already been categorised as relevant or not relevant by the rule-based filter. The idea is to supervise this filter by processing a differential comparison of its outcomes with those of the control one.   This method has many advantages. It is simple to implement since the training set used by the learning is supplied by the rule-based filter. Thus, both the making and the use of the control filter are fully automatic. With automatic detection of obsolescence, learning-based filtering finds a rich application which offers interesting prospects.",
        "published": "2000-07-07T15:13:09Z",
        "link": "http://arxiv.org/abs/cs/0007012v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "H.3.3; I.2.6"
        ]
    },
    {
        "title": "Applying Constraint Handling Rules to HPSG",
        "authors": [
            "Gerald Penn"
        ],
        "summary": "Constraint Handling Rules (CHR) have provided a realistic solution to an over-arching problem in many fields that deal with constraint logic programming: how to combine recursive functions or relations with constraints while avoiding non-termination problems. This paper focuses on some other benefits that CHR, specifically their implementation in SICStus Prolog, have provided to computational linguists working on grammar design tools. CHR rules are applied by means of a subsumption check and this check is made only when their variables are instantiated or bound. The former functionality is at best difficult to simulate using more primitive coroutining statements such as SICStus when/2, and the latter simply did not exist in any form before CHR.   For the sake of providing a case study in how these can be applied to grammar development, we consider the Attribute Logic Engine (ALE), a Prolog preprocessor for logic programming with typed feature structures, and its extension to a complete grammar development system for Head-driven Phrase Structure Grammar (HPSG), a popular constraint-based linguistic theory that uses typed feature structures. In this context, CHR can be used not only to extend the constraint language of feature structure descriptions to include relations in a declarative way, but also to provide support for constraints with complex antecedents and constraints on the co-occurrence of feature values that are necessary to interpret the type system of HPSG properly.",
        "published": "2000-07-07T19:09:25Z",
        "link": "http://arxiv.org/abs/cs/0007013v1",
        "categories": [
            "cs.CL",
            "cs.PL",
            "I.2.7; D.3.3"
        ]
    },
    {
        "title": "Two Steps Feature Selection and Neural Network Classification for the   TREC-8 Routing",
        "authors": [
            "Mathieu Stricker",
            "Frantz Vichot",
            "Gerard Dreyfus",
            "Francis Wolinski"
        ],
        "summary": "For the TREC-8 routing, one specific filter is built for each topic. Each filter is a classifier trained to recognize the documents that are relevant to the topic. When presented with a document, each classifier estimates the probability for the document to be relevant to the topic for which it has been trained. Since the procedure for building a filter is topic-independent, the system is fully automatic.   By making use of a sample of documents that have previously been evaluated as relevant or not relevant to a particular topic, a term selection is performed, and a neural network is trained. Each document is represented by a vector of frequencies of a list of selected terms. This list depends on the topic to be filtered; it is constructed in two steps. The first step defines the characteristic words used in the relevant documents of the corpus; the second one chooses, among the previous list, the most discriminant ones. The length of the vector is optimized automatically for each topic. At the end of the term selection, a vector of typically 25 words is defined for the topic, so that each document which has to be processed is represented by a vector of term frequencies.   This vector is subsequently input to a classifier that is trained from the same sample. After training, the classifier estimates for each document of a test set its probability of being relevant; for submission to TREC, the top 1000 documents are ranked in order of decreasing relevance.",
        "published": "2000-07-11T13:21:03Z",
        "link": "http://arxiv.org/abs/cs/0007016v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "H.3.3; K.3.2"
        ]
    },
    {
        "title": "Bootstrapping a Tagged Corpus through Combination of Existing   Heterogeneous Taggers",
        "authors": [
            "Jakub Zavrel",
            "Walter Daelemans"
        ],
        "summary": "This paper describes a new method, Combi-bootstrap, to exploit existing taggers and lexical resources for the annotation of corpora with new tagsets. Combi-bootstrap uses existing resources as features for a second level machine learning module, that is trained to make the mapping to the new tagset on a very small sample of annotated corpus material. Experiments show that Combi-bootstrap: i) can integrate a wide variety of existing resources, and ii) achieves much higher accuracy (up to 44.7 % error reduction) than both the best single tagger and an ensemble tagger constructed out of the same small training sample.",
        "published": "2000-07-13T12:46:00Z",
        "link": "http://arxiv.org/abs/cs/0007018v1",
        "categories": [
            "cs.CL",
            "I.2.7; I.2.6"
        ]
    },
    {
        "title": "ATLAS: A flexible and extensible architecture for linguistic annotation",
        "authors": [
            "Steven Bird",
            "David Day",
            "John Garofolo",
            "John Henderson",
            "Christophe Laprun",
            "Mark Liberman"
        ],
        "summary": "We describe a formal model for annotating linguistic artifacts, from which we derive an application programming interface (API) to a suite of tools for manipulating these annotations. The abstract logical model provides for a range of storage formats and promotes the reuse of tools that interact through this API. We focus first on ``Annotation Graphs,'' a graph model for annotations on linear signals (such as text and speech) indexed by intervals, for which efficient database storage and querying techniques are applicable. We note how a wide range of existing annotated corpora can be mapped to this annotation graph model. This model is then generalized to encompass a wider variety of linguistic ``signals,'' including both naturally occuring phenomena (as recorded in images, video, multi-modal interactions, etc.), as well as the derived resources that are increasingly important to the engineering of natural language processing systems (such as word lists, dictionaries, aligned bilingual corpora, etc.). We conclude with a review of the current efforts towards implementing key pieces of this architecture.",
        "published": "2000-07-13T18:26:38Z",
        "link": "http://arxiv.org/abs/cs/0007022v1",
        "categories": [
            "cs.CL",
            "E.2; H.2.1; H.3.3; H.3.4; H.3.7; I.2.7"
        ]
    },
    {
        "title": "Towards a query language for annotation graphs",
        "authors": [
            "Steven Bird",
            "Peter Buneman",
            "Wang-Chiew Tan"
        ],
        "summary": "The multidimensional, heterogeneous, and temporal nature of speech databases raises interesting challenges for representation and query. Recently, annotation graphs have been proposed as a general-purpose representational framework for speech databases. Typical queries on annotation graphs require path expressions similar to those used in semistructured query languages. However, the underlying model is rather different from the customary graph models for semistructured data: the graph is acyclic and unrooted, and both temporal and inclusion relationships are important. We develop a query language and describe optimization techniques for an underlying relational representation.",
        "published": "2000-07-13T18:40:15Z",
        "link": "http://arxiv.org/abs/cs/0007023v1",
        "categories": [
            "cs.CL",
            "cs.DB",
            "E.1; E.2; H.2.1; H.2.3; H.2.8; H.3.1; H.3.3; I.2.7"
        ]
    },
    {
        "title": "Many uses, many annotations for large speech corpora: Switchboard and   TDT as case studies",
        "authors": [
            "David Graff",
            "Steven Bird"
        ],
        "summary": "This paper discusses the challenges that arise when large speech corpora receive an ever-broadening range of diverse and distinct annotations. Two case studies of this process are presented: the Switchboard Corpus of telephone conversations and the TDT2 corpus of broadcast news. Switchboard has undergone two independent transcriptions and various types of additional annotation, all carried out as separate projects that were dispersed both geographically and chronologically. The TDT2 corpus has also received a variety of annotations, but all directly created or managed by a core group. In both cases, issues arise involving the propagation of repairs, consistency of references, and the ability to integrate annotations having different formats and levels of detail. We describe a general framework whereby these issues can be addressed successfully.",
        "published": "2000-07-13T18:51:48Z",
        "link": "http://arxiv.org/abs/cs/0007024v1",
        "categories": [
            "cs.CL",
            "E.2; H.2.5; I.2.7"
        ]
    },
    {
        "title": "Parameter-free Model of Rank Polysemantic Distribution",
        "authors": [
            "Victor Kromer"
        ],
        "summary": "A model of rank polysemantic distribution with a minimal number of fitting parameters is offered. In an ideal case a parameter-free description of the dependence on the basis of one or several immediate features of the distribution is possible.",
        "published": "2000-07-21T06:07:31Z",
        "link": "http://arxiv.org/abs/cs/0007031v2",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Mapping WordNets Using Structural Information",
        "authors": [
            "J. Daude",
            "L. Padro",
            "G. Rigau"
        ],
        "summary": "We present a robust approach for linking already existing lexical/semantic hierarchies. We used a constraint satisfaction algorithm (relaxation labeling) to select --among a set of candidates-- the node in a target taxonomy that bests matches each node in a source taxonomy. In particular, we use it to map the nominal part of WordNet 1.5 onto WordNet 1.6, with a very high precision and a very low remaining ambiguity.",
        "published": "2000-07-25T17:20:47Z",
        "link": "http://arxiv.org/abs/cs/0007035v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Language identification of controlled systems: Modelling, control and   anomaly detection",
        "authors": [
            "J. F. Martins",
            "J. A. Dente",
            "A. J. Pires",
            "R. Vilela Mendes"
        ],
        "summary": "Formal language techniques have been used in the past to study autonomous dynamical systems. However, for controlled systems, new features are needed to distinguish between information generated by the system and input control. We show how the modelling framework for controlled dynamical systems leads naturally to a formulation in terms of context-dependent grammars. A learning algorithm is proposed for on-line generation of the grammar productions, this formulation being then used for modelling, control and anomaly detection. Practical applications are described for electromechanical drives. Grammatical interpolation techniques yield accurate results and the pattern detection capabilities of the language-based formulation makes it a promising technique for the early detection of anomalies or faulty behaviour.",
        "published": "2000-07-25T17:40:00Z",
        "link": "http://arxiv.org/abs/cs/0007036v1",
        "categories": [
            "cs.CL",
            "I.2.8"
        ]
    },
    {
        "title": "Interfacing Constraint-Based Grammars and Generation Algorithms",
        "authors": [
            "Stephan Busemann"
        ],
        "summary": "Constraint-based grammars can, in principle, serve as the major linguistic knowledge source for both parsing and generation. Surface generation starts from input semantics representations that may vary across grammars. For many declarative grammars, the concept of derivation implicitly built in is that of parsing. They may thus not be interpretable by a generation algorithm. We show that linguistically plausible semantic analyses can cause severe problems for semantic-head-driven approaches for generation (SHDG). We use SeReal, a variant of SHDG and the DISCO grammar of German as our source of examples. We propose a new, general approach that explicitly accounts for the interface between the grammar and the generation algorithm by adding a control-oriented layer to the linguistic knowledge base that reorganizes the semantics in a way suitable for generation.",
        "published": "2000-08-07T16:06:15Z",
        "link": "http://arxiv.org/abs/cs/0008003v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Comparing two trainable grammatical relations finders",
        "authors": [
            "Alexander Yeh"
        ],
        "summary": "Grammatical relationships (GRs) form an important level of natural language processing, but different sets of GRs are useful for different purposes. Therefore, one may often only have time to obtain a small training corpus with the desired GR annotations. On such a small training corpus, we compare two systems. They use different learning techniques, but we find that this difference by itself only has a minor effect. A larger factor is that in English, a different GR length measure appears better suited for finding simple argument GRs than for finding modifier GRs. We also find that partitioning the data may help memory-based learning.",
        "published": "2000-08-08T22:42:51Z",
        "link": "http://arxiv.org/abs/cs/0008004v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "More accurate tests for the statistical significance of result   differences",
        "authors": [
            "Alexander Yeh"
        ],
        "summary": "Statistical significance testing of differences in values of metrics like recall, precision and balanced F-score is a necessary part of empirical natural language processing. Unfortunately, we find in a set of experiments that many commonly used tests often underestimate the significance and so are less likely to detect differences that exist between different techniques. This underestimation comes from an independence assumption that is often violated. We point out some useful tests that do not make this assumption, including computationally-intensive randomization tests.",
        "published": "2000-08-08T23:52:02Z",
        "link": "http://arxiv.org/abs/cs/0008005v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Tagger Evaluation Given Hierarchical Tag Sets",
        "authors": [
            "I. Dan Melamed",
            "Philip Resnik"
        ],
        "summary": "We present methods for evaluating human and automatic taggers that extend current practice in three ways. First, we show how to evaluate taggers that assign multiple tags to each test instance, even if they do not assign probabilities. Second, we show how to accommodate a common property of manually constructed ``gold standards'' that are typically used for objective evaluation, namely that there is often more than one correct answer. Third, we show how to measure performance when the set of possible tags is tree-structured in an IS-A hierarchy. To illustrate how our methods can be used to measure inter-annotator agreement, we show how to compute the kappa coefficient over hierarchical tag sets.",
        "published": "2000-08-10T03:21:33Z",
        "link": "http://arxiv.org/abs/cs/0008007v1",
        "categories": [
            "cs.CL",
            "G.3; I.2.7; J.5"
        ]
    },
    {
        "title": "Applying System Combination to Base Noun Phrase Identification",
        "authors": [
            "Erik F. Tjong Kim Sang",
            "Walter Daelemans",
            "Herve Dejean",
            "Rob Koeling",
            "Yuval Krymolowski",
            "Vasin Punyakanok",
            "Dan Roth"
        ],
        "summary": "We use seven machine learning algorithms for one task: identifying base noun phrases. The results have been processed by different system combination methods and all of these outperformed the best individual result. We have applied the seven learners with the best combinator, a majority vote of the top five systems, to a standard data set and managed to improve the best published result for this data set.",
        "published": "2000-08-17T13:13:42Z",
        "link": "http://arxiv.org/abs/cs/0008012v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Meta-Learning for Phonemic Annotation of Corpora",
        "authors": [
            "Veronique Hoste",
            "Walter Daelemans",
            "Erik Tjong Kim Sang",
            "Steven Gillis"
        ],
        "summary": "We apply rule induction, classifier combination and meta-learning (stacked classifiers) to the problem of bootstrapping high accuracy automatic annotation of corpora with pronunciation information. The task we address in this paper consists of generating phonemic representations reflecting the Flemish and Dutch pronunciations of a word on the basis of its orthographic representation (which in turn is based on the actual speech recordings). We compare several possible approaches to achieve the text-to-pronunciation mapping task: memory-based learning, transformation-based learning, rule induction, maximum entropy modeling, combination of classifiers in stacked learning, and stacking of meta-learners. We are interested both in optimal accuracy and in obtaining insight into the linguistic regularities involved. As far as accuracy is concerned, an already high accuracy level (93% for Celex and 86% for Fonilex at word level) for single classifiers is boosted significantly with additional error reductions of 31% and 38% respectively using combination of classifiers, and a further 5% using combination of meta-learners, bringing overall word level accuracy to 96% for the Dutch variant and 92% for the Flemish variant. We also show that the application of machine learning methods indeed leads to increased insight into the linguistic regularities determining the variation between the two pronunciation variants studied.",
        "published": "2000-08-18T12:06:36Z",
        "link": "http://arxiv.org/abs/cs/0008013v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Aspects of Pattern-Matching in Data-Oriented Parsing",
        "authors": [
            "Guy De Pauw"
        ],
        "summary": "Data-Oriented Parsing (dop) ranks among the best parsing schemes, pairing state-of-the art parsing accuracy to the psycholinguistic insight that larger chunks of syntactic structures are relevant grammatical and probabilistic units. Parsing with the dop-model, however, seems to involve a lot of CPU cycles and a considerable amount of double work, brought on by the concept of multiple derivations, which is necessary for probabilistic processing, but which is not convincingly related to a proper linguistic backbone. It is however possible to re-interpret the dop-model as a pattern-matching model, which tries to maximize the size of the substructures that construct the parse, rather than the probability of the parse. By emphasizing this memory-based aspect of the dop-model, it is possible to do away with multiple derivations, opening up possibilities for efficient Viterbi-style optimizations, while still retaining acceptable parsing accuracy through enhanced context-sensitivity.",
        "published": "2000-08-18T13:56:10Z",
        "link": "http://arxiv.org/abs/cs/0008014v1",
        "categories": [
            "cs.CL",
            "I.2.6;I.2.7;I.5.4"
        ]
    },
    {
        "title": "Temiar Reduplication in One-Level Prosodic Morphology",
        "authors": [
            "Markus Walther"
        ],
        "summary": "Temiar reduplication is a difficult piece of prosodic morphology. This paper presents the first computational analysis of Temiar reduplication, using the novel finite-state approach of One-Level Prosodic Morphology originally developed by Walther (1999b, 2000). After reviewing both the data and the basic tenets of One-level Prosodic Morphology, the analysis is laid out in some detail, using the notation of the FSA Utilities finite-state toolkit (van Noord 1997). One important discovery is that in this approach one can easily define a regular expression operator which ambiguously scans a string in the left- or rightward direction for a certain prosodic property. This yields an elegant account of base-length-dependent triggering of reduplication as found in Temiar.",
        "published": "2000-08-18T16:07:17Z",
        "link": "http://arxiv.org/abs/cs/0008015v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Processing Self Corrections in a speech to speech system",
        "authors": [
            "Joerg Spilker",
            "Martin Klarner",
            "Guenther Goerz"
        ],
        "summary": "Speech repairs occur often in spontaneous spoken dialogues. The ability to detect and correct those repairs is necessary for any spoken language system. We present a framework to detect and correct speech repairs where all relevant levels of information, i.e., acoustics, lexis, syntax and semantics can be integrated. The basic idea is to reduce the search space for repairs as soon as possible by cascading filters that involve more and more features. At first an acoustic module generates hypotheses about the existence of a repair. Second a stochastic model suggests a correction for every hypothesis. Well scored corrections are inserted as new paths in the word lattice. Finally a lattice parser decides on accepting the rep air.",
        "published": "2000-08-21T10:54:11Z",
        "link": "http://arxiv.org/abs/cs/0008016v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I 2.7"
        ]
    },
    {
        "title": "Efficient probabilistic top-down and left-corner parsing",
        "authors": [
            "Brian Roark",
            "Mark Johnson"
        ],
        "summary": "This paper examines efficient predictive broad-coverage parsing without dynamic programming. In contrast to bottom-up methods, depth-first top-down parsing produces partial parses that are fully connected trees spanning the entire left context, from which any kind of non-local dependency or partial semantic interpretation can in principle be read. We contrast two predictive parsing approaches, top-down and left-corner parsing, and find both to be viable. In addition, we find that enhancement with non-local information not only improves parser accuracy, but also substantially improves the search efficiency.",
        "published": "2000-08-21T19:27:18Z",
        "link": "http://arxiv.org/abs/cs/0008017v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "An Experimental Comparison of Naive Bayesian and Keyword-Based Anti-Spam   Filtering with Personal E-mail Messages",
        "authors": [
            "Ion Androutsopoulos",
            "John Koutsias",
            "Konstantinos V. Chandrinos",
            "Constantine D. Spyropoulos"
        ],
        "summary": "The growing problem of unsolicited bulk e-mail, also known as \"spam\", has generated a need for reliable anti-spam e-mail filters. Filters of this type have so far been based mostly on manually constructed keyword patterns. An alternative approach has recently been proposed, whereby a Naive Bayesian classifier is trained automatically to detect spam messages. We test this approach on a large collection of personal e-mail messages, which we make publicly available in \"encrypted\" form contributing towards standard benchmarks. We introduce appropriate cost-sensitive measures, investigating at the same time the effect of attribute-set size, training-corpus size, lemmatization, and stop lists, issues that have not been explored in previous experiments. Finally, the Naive Bayesian filter is compared, in terms of performance, to a filter that uses keyword patterns, and which is part of a widely used e-mail reader.",
        "published": "2000-08-22T11:20:14Z",
        "link": "http://arxiv.org/abs/cs/0008019v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.4.3; I.2.6; I.2.7; I.5.4; K.4.1"
        ]
    },
    {
        "title": "Explaining away ambiguity: Learning verb selectional preference with   Bayesian networks",
        "authors": [
            "Massimiliano Ciaramita",
            "Mark Johnson"
        ],
        "summary": "This paper presents a Bayesian model for unsupervised learning of verb selectional preferences. For each verb the model creates a Bayesian network whose architecture is determined by the lexical hierarchy of Wordnet and whose parameters are estimated from a list of verb-object pairs found from a corpus. ``Explaining away'', a well-known property of Bayesian networks, helps the model deal in a natural fashion with word sense ambiguity in the training data. On a word sense disambiguation test our model performed better than other state of the art systems for unsupervised learning of selectional preferences. Computational complexity problems, ways of improving this approach and methods for implementing ``explaining away'' in other graphical frameworks are discussed.",
        "published": "2000-08-22T15:01:21Z",
        "link": "http://arxiv.org/abs/cs/0008020v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Compact non-left-recursive grammars using the selective left-corner   transform and factoring",
        "authors": [
            "Mark Johnson",
            "Brian Roark"
        ],
        "summary": "The left-corner transform removes left-recursion from (probabilistic) context-free grammars and unification grammars, permitting simple top-down parsing techniques to be used. Unfortunately the grammars produced by the standard left-corner transform are usually much larger than the original. The selective left-corner transform described in this paper produces a transformed grammar which simulates left-corner recognition of a user-specified set of the original productions, and top-down recognition of the others. Combined with two factorizations, it produces non-left-recursive grammars that are not much larger than the original.",
        "published": "2000-08-22T15:16:22Z",
        "link": "http://arxiv.org/abs/cs/0008021v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A Learning Approach to Shallow Parsing",
        "authors": [
            "Marcia Muñoz",
            "Vasin Punyakanok",
            "Dan Roth",
            "Dav Zimak"
        ],
        "summary": "A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.",
        "published": "2000-08-22T21:37:50Z",
        "link": "http://arxiv.org/abs/cs/0008022v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Selectional Restrictions in HPSG",
        "authors": [
            "Ion Androutsopoulos",
            "Robert Dale"
        ],
        "summary": "Selectional restrictions are semantic sortal constraints imposed on the participants of linguistic constructions to capture contextually-dependent constraints on interpretation. Despite their limitations, selectional restrictions have proven very useful in natural language applications, where they have been used frequently in word sense disambiguation, syntactic disambiguation, and anaphora resolution. Given their practical value, we explore two methods to incorporate selectional restrictions in the HPSG theory, assuming that the reader is familiar with HPSG. The first method employs HPSG's Background feature and a constraint-satisfaction component pipe-lined after the parser. The second method uses subsorts of referential indices, and blocks readings that violate selectional restrictions during parsing. While theoretically less satisfactory, we have found the second method particularly useful in the development of practical systems.",
        "published": "2000-08-23T07:38:22Z",
        "link": "http://arxiv.org/abs/cs/0008023v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Estimation of Stochastic Attribute-Value Grammars using an Informative   Sample",
        "authors": [
            "Miles Osborne"
        ],
        "summary": "We argue that some of the computational complexity associated with estimation of stochastic attribute-value grammars can be reduced by training upon an informative subset of the full training set. Results using the parsed Wall Street Journal corpus show that in some circumstances, it is possible to obtain better estimation results using an informative sample than when training upon all the available material. Further experimentation demonstrates that with unlexicalised models, a Gaussian Prior can reduce overfitting. However, when models are lexicalised and contain overlapping features, overfitting does not seem to be a problem, and a Gaussian Prior makes minimal difference to performance. Our approach is applicable for situations when there are an infeasibly large number of parses in the training set, or else for when recovery of these parses from a packed representation is itself computationally expensive.",
        "published": "2000-08-23T12:38:08Z",
        "link": "http://arxiv.org/abs/cs/0008024v1",
        "categories": [
            "cs.CL",
            "I.2.6"
        ]
    },
    {
        "title": "Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon   construction",
        "authors": [
            "Brian Roark",
            "Eugene Charniak"
        ],
        "summary": "Generating semantic lexicons semi-automatically could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars. Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area. Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand. Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an ``enhancer'' of existing broad-coverage resources.",
        "published": "2000-08-24T13:28:25Z",
        "link": "http://arxiv.org/abs/cs/0008026v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Measuring efficiency in high-accuracy, broad-coverage statistical   parsing",
        "authors": [
            "Brian Roark",
            "Eugene Charniak"
        ],
        "summary": "Very little attention has been paid to the comparison of efficiency between high accuracy statistical parsers. This paper proposes one machine-independent metric that is general enough to allow comparisons across very different parsing architectures. This metric, which we call ``events considered'', measures the number of ``events'', however they are defined for a particular parser, for which a probability must be calculated, in order to find the parse. It is applicable to single-pass or multi-stage parsers. We discuss the advantages of the metric, and demonstrate its usefulness by using it to compare two parsers which differ in several fundamental ways.",
        "published": "2000-08-24T16:38:53Z",
        "link": "http://arxiv.org/abs/cs/0008027v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Estimators for Stochastic ``Unification-Based'' Grammars",
        "authors": [
            "Mark Johnson",
            "Stuart Geman",
            "Stephen Canon",
            "Zhiyi Chi",
            "Stefan Riezler"
        ],
        "summary": "Log-linear models provide a statistically sound framework for Stochastic ``Unification-Based'' Grammars (SUBGs) and stochastic versions of other kinds of grammars. We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical-Functional Grammar.",
        "published": "2000-08-25T17:23:07Z",
        "link": "http://arxiv.org/abs/cs/0008028v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Exploiting auxiliary distributions in stochastic unification-based   grammars",
        "authors": [
            "Mark Johnson",
            "Stefan Riezler"
        ],
        "summary": "This paper describes a method for estimating conditional probability distributions over the parses of ``unification-based'' grammars which can utilize auxiliary distributions that are estimated by other means. We show how this can be used to incorporate information about lexical selectional preferences gathered from other sources into Stochastic ``Unification-based'' Grammars (SUBGs). While we apply this estimator to a Stochastic Lexical-Functional Grammar, the method is general, and should be applicable to stochastic versions of HPSGs, categorial grammars and transformational grammars.",
        "published": "2000-08-25T17:31:53Z",
        "link": "http://arxiv.org/abs/cs/0008029v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Metonymy Interpretation Using X NO Y Examples",
        "authors": [
            "Masaki Murata",
            "Qing Ma",
            "Atsumu Yamamoto",
            "Hitoshi Isahara"
        ],
        "summary": "We developed on example-based method of metonymy interpretation. One advantages of this method is that a hand-built database of metonymy is not necessary because it instead uses examples in the form ``Noun X no Noun Y (Noun Y of Noun X).'' Another advantage is that we will be able to interpret newly-coined metonymic sentences by using a new corpus. We experimented with metonymy interpretation and obtained a precision rate of 66% when using this method.",
        "published": "2000-08-28T08:10:14Z",
        "link": "http://arxiv.org/abs/cs/0008030v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Bunsetsu Identification Using Category-Exclusive Rules",
        "authors": [
            "Masaki Murata",
            "Kiyotaka Uchimoto",
            "Qing Ma",
            "Hitoshi Isahara"
        ],
        "summary": "This paper describes two new bunsetsu identification methods using supervised learning. Since Japanese syntactic analysis is usually done after bunsetsu identification, bunsetsu identification is important for analyzing Japanese sentences. In experiments comparing the four previously available machine-learning methods (decision tree, maximum-entropy method, example-based approach and decision list) and two new methods using category-exclusive rules, the new method using the category-exclusive rules with the highest similarity performed best.",
        "published": "2000-08-28T08:17:18Z",
        "link": "http://arxiv.org/abs/cs/0008031v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Japanese Probabilistic Information Retrieval Using Location and Category   Information",
        "authors": [
            "Masaki Murata",
            "Qing Ma",
            "Kiyotaka Uchimoto",
            "Hiromi Ozaku",
            "Masao Utiyama",
            "Hitoshi Isahara"
        ],
        "summary": "Robertson's 2-poisson information retrieve model does not use location and category information. We constructed a framework using location and category information in a 2-poisson model. We submitted two systems based on this framework to the IREX contest, Japanese language information retrieval contest held in Japan in 1999. For precision in the A-judgement measure they scored 0.4926 and 0.4827, the highest values among the 15 teams and 22 systems that participated in the IREX contest. We describe our systems and the comparative experiments done when various parameters were changed. These experiments confirmed the effectiveness of using location and category information.",
        "published": "2000-08-28T08:27:22Z",
        "link": "http://arxiv.org/abs/cs/0008032v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Temporal Expressions in Japanese-to-English Machine Translation",
        "authors": [
            "Francis Bond",
            "Kentaro Ogura",
            "Hajime Uchino"
        ],
        "summary": "This paper describes in outline a method for translating Japanese temporal expressions into English. We argue that temporal expressions form a special subset of language that is best handled as a special module in machine translation. The paper deals with problems of lexical idiosyncrasy as well as the choice of articles and prepositions within temporal expressions. In addition temporal expressions are considered as parts of larger structures, and the question of whether to translate them as noun phrases or adverbials is addressed.",
        "published": "2000-08-28T19:51:32Z",
        "link": "http://arxiv.org/abs/cs/0008033v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Lexicalized Stochastic Modeling of Constraint-Based Grammars using   Log-Linear Measures and EM Training",
        "authors": [
            "Stefan Riezler",
            "Detlef Prescher",
            "Jonas Kuhn",
            "Mark Johnson"
        ],
        "summary": "We present a new approach to stochastic modeling of constraint-based grammars that is based on log-linear models and uses EM for estimation from unannotated data. The techniques are applied to an LFG grammar for German. Evaluation on an exact match task yields 86% precision for an ambiguity rate of 5.4, and 90% precision on a subcat frame match for an ambiguity rate of 25. Experimental comparison to training from a parsebank shows a 10% gain from EM training. Also, a new class-based grammar lexicalization is presented, showing a 10% gain over unlexicalized models.",
        "published": "2000-08-30T13:14:58Z",
        "link": "http://arxiv.org/abs/cs/0008034v1",
        "categories": [
            "cs.CL",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Using a Probabilistic Class-Based Lexicon for Lexical Ambiguity   Resolution",
        "authors": [
            "Detlef Prescher",
            "Stefan Riezler",
            "Mats Rooth"
        ],
        "summary": "This paper presents the use of probabilistic class-based lexica for disambiguation in target-word selection. Our method employs minimal but precise contextual information for disambiguation. That is, only information provided by the target-verb, enriched by the condensed information of a probabilistic class-based lexicon, is used. Induction of classes and fine-tuning to verbal arguments is done in an unsupervised manner by EM-based clustering techniques. The method shows promising results in an evaluation on real-world translations.",
        "published": "2000-08-30T13:24:06Z",
        "link": "http://arxiv.org/abs/cs/0008035v1",
        "categories": [
            "cs.CL",
            "I.2.6, I.2.7"
        ]
    },
    {
        "title": "Probabilistic Constraint Logic Programming. Formal Foundations of   Quantitative and Statistical Inference in Constraint-Based Natural Language   Processing",
        "authors": [
            "Stefan Riezler"
        ],
        "summary": "In this thesis, we present two approaches to a rigorous mathematical and algorithmic foundation of quantitative and statistical inference in constraint-based natural language processing. The first approach, called quantitative constraint logic programming, is conceptualized in a clear logical framework, and presents a sound and complete system of quantitative inference for definite clauses annotated with subjective weights. This approach combines a rigorous formal semantics for quantitative inference based on subjective weights with efficient weight-based pruning for constraint-based systems. The second approach, called probabilistic constraint logic programming, introduces a log-linear probability distribution on the proof trees of a constraint logic program and an algorithm for statistical inference of the parameters and properties of such probability models from incomplete, i.e., unparsed data. The possibility of defining arbitrary properties of proof trees as properties of the log-linear probability model and efficiently estimating appropriate parameter values for them permits the probabilistic modeling of arbitrary context-dependencies in constraint logic programs. The usefulness of these ideas is evaluated empirically in a small-scale experiment on finding the correct parses of a constraint-based grammar. In addition, we address the problem of computational intractability of the calculation of expectations in the inference task and present various techniques to approximately solve this task. Moreover, we present an approximate heuristic technique for searching for the most probable analysis in probabilistic constraint logic programs.",
        "published": "2000-08-30T13:57:19Z",
        "link": "http://arxiv.org/abs/cs/0008036v1",
        "categories": [
            "cs.CL",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Automatic Extraction of Subcategorization Frames for Czech",
        "authors": [
            "Anoop Sarkar",
            "Daniel Zeman"
        ],
        "summary": "We present some novel machine learning techniques for the identification of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we ar able to achieve 88% precision on unseen parsed text.",
        "published": "2000-09-08T15:48:53Z",
        "link": "http://arxiv.org/abs/cs/0009003v1",
        "categories": [
            "cs.CL",
            "I.2.7, G.3"
        ]
    },
    {
        "title": "Introduction to the CoNLL-2000 Shared Task: Chunking",
        "authors": [
            "Erik F. Tjong Kim Sang",
            "Sabine Buchholz"
        ],
        "summary": "We describe the CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words, so-called text chunking. We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance.",
        "published": "2000-09-18T12:08:54Z",
        "link": "http://arxiv.org/abs/cs/0009008v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Learning to Filter Spam E-Mail: A Comparison of a Naive Bayesian and a   Memory-Based Approach",
        "authors": [
            "Ion Androutsopoulos",
            "Georgios Paliouras",
            "Vangelis Karkaletsis",
            "Georgios Sakkis",
            "Constantine D. Spyropoulos",
            "Panagiotis Stamatopoulos"
        ],
        "summary": "We investigate the performance of two machine learning algorithms in the context of anti-spam filtering. The increasing volume of unsolicited bulk e-mail (spam) has generated a need for reliable anti-spam filters. Filters of this type have so far been based mostly on keyword patterns that are constructed by hand and perform poorly. The Naive Bayesian classifier has recently been suggested as an effective method to construct automatically anti-spam filters with superior performance. We investigate thoroughly the performance of the Naive Bayesian filter on a publicly available corpus, contributing towards standard benchmarks. At the same time, we compare the performance of the Naive Bayesian filter to an alternative memory-based learning approach, after introducing suitable cost-sensitive evaluation measures. Both methods achieve very accurate spam filtering, outperforming clearly the keyword-based filter of a widely used e-mail reader.",
        "published": "2000-09-18T14:05:13Z",
        "link": "http://arxiv.org/abs/cs/0009009v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.4.3; I.2.6; I.2.7; I.5.4; K.4.1"
        ]
    },
    {
        "title": "Anaphora Resolution in Japanese Sentences Using Surface Expressions and   Examples",
        "authors": [
            "Masaki Murata"
        ],
        "summary": "Anaphora resolution is one of the major problems in natural language processing. It is also one of the important tasks in machine translation and man/machine dialogue. We solve the problem by using surface expressions and examples. Surface expressions are the words in sentences which provide clues for anaphora resolution. Examples are linguistic data which are actually used in conversations and texts. The method using surface expressions and examples is a practical method. This thesis handles almost all kinds of anaphora: i. The referential property and number of a noun phrase ii. Noun phrase direct anaphora iii. Noun phrase indirect anaphora iv. Pronoun anaphora v. Verb phrase ellipsis",
        "published": "2000-09-19T00:44:47Z",
        "link": "http://arxiv.org/abs/cs/0009011v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Modeling Ambiguity in a Multi-Agent System",
        "authors": [
            "Christof Monz"
        ],
        "summary": "This paper investigates the formal pragmatics of ambiguous expressions by modeling ambiguity in a multi-agent system. Such a framework allows us to give a more refined notion of the kind of information that is conveyed by ambiguous expressions. We analyze how ambiguity affects the knowledge of the dialog participants and, especially, what they know about each other after an ambiguous sentence has been uttered. The agents communicate with each other by means of a TELL-function, whose application is constrained by an implementation of some of Grice's maxims. The information states of the multi-agent system itself are represented as a Kripke structures and TELL is an update function on those structures. This framework enables us to distinguish between the information conveyed by ambiguous sentences vs. the information conveyed by disjunctions, and between semantic ambiguity vs. perceived ambiguity.",
        "published": "2000-09-19T15:43:18Z",
        "link": "http://arxiv.org/abs/cs/0009012v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.MA",
            "F.4.1; I.2.7"
        ]
    },
    {
        "title": "Combining Linguistic and Spatial Information for Document Analysis",
        "authors": [
            "Marco Aiello",
            "Christof Monz",
            "Leon Todoran"
        ],
        "summary": "We present a framework to analyze color documents of complex layout. In addition, no assumption is made on the layout. Our framework combines in a content-driven bottom-up approach two different sources of information: textual and spatial. To analyze the text, shallow natural language processing tools, such as taggers and partial parsers, are used. To infer relations of the logical layout we resort to a qualitative spatial calculus closely related to Allen's calculus. We evaluate the system against documents from a color journal and present the results of extracting the reading order from the journal's pages. In this case, our analysis is successful as it extracts the intended reading order from the document.",
        "published": "2000-09-20T13:04:11Z",
        "link": "http://arxiv.org/abs/cs/0009014v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.3.5; H.3.6; H.3.7; I.2.7; I.7"
        ]
    },
    {
        "title": "A Tableaux Calculus for Ambiguous Quantification",
        "authors": [
            "Christof Monz",
            "Maarten de Rijke"
        ],
        "summary": "Coping with ambiguity has recently received a lot of attention in natural language processing. Most work focuses on the semantic representation of ambiguous expressions. In this paper we complement this work in two ways. First, we provide an entailment relation for a language with ambiguous expressions. Second, we give a sound and complete tableaux calculus for reasoning with statements involving ambiguous quantification. The calculus interleaves partial disambiguation steps with steps in a traditional deductive process, so as to minimize and postpone branching in the proof process, and thereby increases its efficiency.",
        "published": "2000-09-20T13:23:17Z",
        "link": "http://arxiv.org/abs/cs/0009015v1",
        "categories": [
            "cs.CL",
            "F.4.1 I.2.7"
        ]
    },
    {
        "title": "Contextual Inference in Computational Semantics",
        "authors": [
            "Christof Monz"
        ],
        "summary": "In this paper, an application of automated theorem proving techniques to computational semantics is considered. In order to compute the presuppositions of a natural language discourse, several inference tasks arise. Instead of treating these inferences independently of each other, we show how integrating techniques from formal approaches to context into deduction can help to compute presuppositions more efficiently. Contexts are represented as Discourse Representation Structures and the way they are nested is made explicit. In addition, a tableau calculus is present which keeps track of contextual information, and thereby allows to avoid carrying out redundant inference steps as it happens in approaches that neglect explicit nesting of contexts.",
        "published": "2000-09-20T13:41:06Z",
        "link": "http://arxiv.org/abs/cs/0009016v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "F.4.1; I.2.7"
        ]
    },
    {
        "title": "A Tableau Calculus for Pronoun Resolution",
        "authors": [
            "Christof Monz",
            "Maarten de Rijke"
        ],
        "summary": "We present a tableau calculus for reasoning in fragments of natural language. We focus on the problem of pronoun resolution and the way in which it complicates automated theorem proving for natural language processing. A method for explicitly manipulating contextual information during deduction is proposed, where pronouns are resolved against this context during deduction. As a result, pronoun resolution and deduction can be interleaved in such a way that pronouns are only resolved if this is licensed by a deduction rule; this helps us to avoid the combinatorial complexity of total pronoun disambiguation.",
        "published": "2000-09-21T14:49:19Z",
        "link": "http://arxiv.org/abs/cs/0009017v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "F.4.1; I.2.7"
        ]
    },
    {
        "title": "A Resolution Calculus for Dynamic Semantics",
        "authors": [
            "Christof Monz",
            "Maarten de Rijke"
        ],
        "summary": "This paper applies resolution theorem proving to natural language semantics. The aim is to circumvent the computational complexity triggered by natural language ambiguities like pronoun binding, by interleaving pronoun binding with resolution deduction. Therefore disambiguation is only applied to expression that actually occur during derivations.",
        "published": "2000-09-21T15:21:01Z",
        "link": "http://arxiv.org/abs/cs/0009018v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "F.4.1; I.2.7"
        ]
    },
    {
        "title": "Computing Presuppositions by Contextual Reasoning",
        "authors": [
            "Christof Monz"
        ],
        "summary": "This paper describes how automated deduction methods for natural language processing can be applied more efficiently by encoding context in a more elaborate way. Our work is based on formal approaches to context, and we provide a tableau calculus for contextual reasoning. This is explained by considering an example from the problem area of presupposition projection.",
        "published": "2000-09-21T15:32:17Z",
        "link": "http://arxiv.org/abs/cs/0009019v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "F.4.1; I.2.7"
        ]
    },
    {
        "title": "A Comparison between Supervised Learning Algorithms for Word Sense   Disambiguation",
        "authors": [
            "Gerard Escudero",
            "Lluis Marquez",
            "German Rigau"
        ],
        "summary": "This paper describes a set of comparative experiments, including cross-corpus evaluation, between five alternative algorithms for supervised Word Sense Disambiguation (WSD), namely Naive Bayes, Exemplar-based learning, SNoW, Decision Lists, and Boosting. Two main conclusions can be drawn: 1) The LazyBoosting algorithm outperforms the other four state-of-the-art algorithms in terms of accuracy and ability to tune to new domains; 2) The domain dependence of WSD systems seems very strong and suggests that some kind of adaptation or tuning is required for cross-corpus application.",
        "published": "2000-09-22T15:02:26Z",
        "link": "http://arxiv.org/abs/cs/0009022v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7;I.2.6"
        ]
    },
    {
        "title": "Parsing with the Shortest Derivation",
        "authors": [
            "Rens Bod"
        ],
        "summary": "Common wisdom has it that the bias of stochastic grammars in favor of shorter derivations of a sentence is harmful and should be redressed. We show that the common wisdom is wrong for stochastic grammars that use elementary trees instead of context-free rules, such as Stochastic Tree-Substitution Grammars used by Data-Oriented Parsing models. For such grammars a non-probabilistic metric based on the shortest derivation outperforms a probabilistic metric on the ATIS and OVIS corpora, while it obtains very competitive results on the Wall Street Journal corpus. This paper also contains the first published experiments with DOP on the Wall Street Journal.",
        "published": "2000-09-27T13:22:54Z",
        "link": "http://arxiv.org/abs/cs/0009025v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "An improved parser for data-oriented lexical-functional analysis",
        "authors": [
            "Rens Bod"
        ],
        "summary": "We present an LFG-DOP parser which uses fragments from LFG-annotated sentences to parse new sentences. Experiments with the Verbmobil and Homecentre corpora show that (1) Viterbi n best search performs about 100 times faster than Monte Carlo search while both achieve the same accuracy; (2) the DOP hypothesis which states that parse accuracy increases with increasing fragment size is confirmed for LFG-DOP; (3) LFG-DOP's relative frequency estimator performs worse than a discounted frequency estimator; and (4) LFG-DOP significantly outperforms Tree-DOP is evaluated on tree structures only.",
        "published": "2000-09-27T13:38:31Z",
        "link": "http://arxiv.org/abs/cs/0009026v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A Classification Approach to Word Prediction",
        "authors": [
            "Yair Even-Zohar",
            "Dan Roth"
        ],
        "summary": "The eventual goal of a language model is to accurately predict the value of a missing word given its context. We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context. This approach raises a few new questions that we address. First, in order to learn good word representations it is necessary to use an expressive representation of the context. We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction. Second, since the number of words ``competing'' for each prediction is large, there is a need to ``focus the attention'' on a smaller subset of these. We exhibit the contribution of a ``focus of attention'' mechanism to the performance of the word predictor. Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks.",
        "published": "2000-09-28T14:25:51Z",
        "link": "http://arxiv.org/abs/cs/0009027v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "I.2.6;I.2.7"
        ]
    },
    {
        "title": "Finding consensus in speech recognition: word error minimization and   other applications of confusion networks",
        "authors": [
            "L. Mangu",
            "E. Brill",
            "A. Stolcke"
        ],
        "summary": "We describe a new framework for distilling information from word lattices to improve the accuracy of speech recognition and obtain a more perspicuous representation of a set of alternative hypotheses. In the standard MAP decoding approach the recognizer outputs the string of words corresponding to the path with the highest posterior probability given the acoustics and a language model. However, even given optimal models, the MAP decoder does not necessarily minimize the commonly used performance metric, word error rate (WER). We describe a method for explicitly minimizing WER by extracting word hypotheses with the highest posterior probabilities from word lattices. We change the standard problem formulation by replacing global search over a large set of sentence hypotheses with local search over a small set of word candidates. In addition to improving the accuracy of the recognizer, our method produces a new representation of the set of candidate hypotheses that specifies the sequence of word-level confusions in a compact lattice format. We study the properties of confusion networks and examine their use for other tasks, such as lattice compression, word spotting, confidence annotation, and reevaluation of recognition hypotheses using higher-level knowledge sources.",
        "published": "2000-10-07T02:24:21Z",
        "link": "http://arxiv.org/abs/cs/0010012v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "On a cepstrum-based speech detector robust to white noise",
        "authors": [
            "Sergei Skorik",
            "Frederic Berthommier"
        ],
        "summary": "We study effects of additive white noise on the cepstral representation of speech signals. Distribution of each individual cepstrum coefficient of speech is shown to depend strongly on noise and to overlap significantly with the cepstrum distribution of noise. Based on these studies, we suggest a scalar quantity, V, equal to the sum of weighted cepstral coefficients, which is able to classify frames containing speech against noise-like frames. The distributions of V for speech and noise frames are reasonably well separated above SNR = 5 dB, demonstrating the feasibility of robust speech detector based on V.",
        "published": "2000-10-10T17:33:02Z",
        "link": "http://arxiv.org/abs/cs/0010014v1",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.HC",
            "I.2.7; I.2.1; I.2.10; H.5.5"
        ]
    },
    {
        "title": "Using existing systems to supplement small amounts of annotated   grammatical relations training data",
        "authors": [
            "Alexander Yeh"
        ],
        "summary": "Grammatical relationships (GRs) form an important level of natural language processing, but different sets of GRs are useful for different purposes. Therefore, one may often only have time to obtain a small training corpus with the desired GR annotations. To boost the performance from using such a small training corpus on a transformation rule learner, we use existing systems that find related types of annotations.",
        "published": "2000-10-11T22:30:09Z",
        "link": "http://arxiv.org/abs/cs/0010020v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Exploring automatic word sense disambiguation with decision lists and   the Web",
        "authors": [
            "Eneko Agirre",
            "David Martinez"
        ],
        "summary": "The most effective paradigm for word sense disambiguation, supervised learning, seems to be stuck because of the knowledge acquisition bottleneck. In this paper we take an in-depth study of the performance of decision lists on two publicly available corpora and an additional corpus automatically acquired from the Web, using the fine-grained highly polysemous senses in WordNet. Decision lists are shown a versatile state-of-the-art technique. The experiments reveal, among other facts, that SemCor can be an acceptable (0.7 precision for polysemous words) starting point for an all-words system. The results on the DSO corpus show that for some highly polysemous words 0.7 precision seems to be the current state-of-the-art limit. On the other hand, independently constructed hand-tagged corpora are not mutually useful, and a corpus automatically acquired from the Web is shown to fail.",
        "published": "2000-10-17T08:31:48Z",
        "link": "http://arxiv.org/abs/cs/0010024v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Extraction of semantic relations from a Basque monolingual dictionary   using Constraint Grammar",
        "authors": [
            "Eneko Agirre",
            "Olatz Ansa",
            "Xabier Arregi",
            "Xabier Artola",
            "Arantza Diaz de Ilarraza",
            "Mikel Lersundi",
            "David Martinez",
            "Kepa Sarasola",
            "Ruben Urizar"
        ],
        "summary": "This paper deals with the exploitation of dictionaries for the semi-automatic construction of lexicons and lexical knowledge bases. The final goal of our research is to enrich the Basque Lexical Database with semantic information such as senses, definitions, semantic relations, etc., extracted from a Basque monolingual dictionary. The work here presented focuses on the extraction of the semantic relations that best characterise the headword, that is, those of synonymy, antonymy, hypernymy, and other relations marked by specific relators and derivation. All nominal, verbal and adjectival entries were treated. Basque uses morphological inflection to mark case, and therefore semantic relations have to be inferred from suffixes rather than from prepositions. Our approach combines a morphological analyser and surface syntax parsing (based on Constraint Grammar), and has proven very successful for highly inflected languages such as Basque. Both the effort to write the rules and the actual processing time of the dictionary have been very low. At present we have extracted 42,533 relations, leaving only 2,943 (9%) definitions without any extracted relation. The error rate is extremely low, as only 2.2% of the extracted relations are wrong.",
        "published": "2000-10-17T09:05:27Z",
        "link": "http://arxiv.org/abs/cs/0010025v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Enriching very large ontologies using the WWW",
        "authors": [
            "Eneko Agirre",
            "Olatz Ansa",
            "Eduard Hovy",
            "David Martinez"
        ],
        "summary": "This paper explores the possibility to exploit text on the world wide web in order to enrich the concepts in existing ontologies. First, a method to retrieve documents from the WWW related to a concept is described. These document collections are used 1) to construct topic signatures (lists of topically related words) for each concept in WordNet, and 2) to build hierarchical clusters of the concepts (the word senses) that lexicalize a given word. The overall goal is to overcome two shortcomings of WordNet: the lack of topical links among concepts, and the proliferation of senses. Topic signatures are validated on a word sense disambiguation task with good results, which are improved when the hierarchical clusters are used.",
        "published": "2000-10-17T09:56:54Z",
        "link": "http://arxiv.org/abs/cs/0010026v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "One Sense per Collocation and Genre/Topic Variations",
        "authors": [
            "David Martinez",
            "Eneko Agirre"
        ],
        "summary": "This paper revisits the one sense per collocation hypothesis using fine-grained sense distinctions and two different corpora. We show that the hypothesis is weaker for fine-grained sense distinctions (70% vs. 99% reported earlier on 2-way ambiguities). We also show that one sense per collocation does hold across corpora, but that collocations vary from one corpus to the other, following genre and topic variations. This explains the low results when performing word sense disambiguation across corpora. In fact, we demonstrate that when two independent corpora share a related genre/topic, the word sense disambiguation results would be better. Future work on word sense disambiguation will have to take into account genre and topic as important parameters on their models.",
        "published": "2000-10-17T10:26:33Z",
        "link": "http://arxiv.org/abs/cs/0010027v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Reduction of Intermediate Alphabets in Finite-State Transducer Cascades",
        "authors": [
            "Andre Kempe"
        ],
        "summary": "This article describes an algorithm for reducing the intermediate alphabets in cascades of finite-state transducers (FSTs). Although the method modifies the component FSTs, there is no change in the overall relation described by the whole cascade. No additional information or special algorithm, that could decelerate the processing of input, is required at runtime. Two examples from Natural Language Processing are used to illustrate the effect of the algorithm on the sizes of the FSTs and their alphabets. With some FSTs the number of arcs and symbols shrank considerably.",
        "published": "2000-10-23T15:14:02Z",
        "link": "http://arxiv.org/abs/cs/0010030v1",
        "categories": [
            "cs.CL",
            "F.1.1; I.2.7"
        ]
    },
    {
        "title": "A Formal Framework for Linguistic Annotation (revised version)",
        "authors": [
            "Steven Bird",
            "Mark Liberman"
        ],
        "summary": "`Linguistic annotation' covers any descriptive or analytic notations applied to raw language data. The basic data may be in the form of time functions - audio, video and/or physiological recordings - or it may be textual. The added notations may include transcriptions of all sorts (from phonetic features to discourse structures), part-of-speech and sense tagging, syntactic analysis, `named entity' identification, co-reference annotation, and so on. While there are several ongoing efforts to provide formats and tools for such annotations and to publish annotated linguistic databases, the lack of widely accepted standards is becoming a critical problem. Proposed standards, to the extent they exist, have focused on file formats. This paper focuses instead on the logical structure of linguistic annotations. We survey a wide variety of existing annotation formats and demonstrate a common conceptual core, the annotation graph. This provides a formal framework for constructing, maintaining and searching linguistic annotations, while remaining consistent with many alternative data structures and file formats.",
        "published": "2000-10-26T17:42:30Z",
        "link": "http://arxiv.org/abs/cs/0010033v1",
        "categories": [
            "cs.CL",
            "cs.DB",
            "cs.DS",
            "A.1; E.2; H.2.1; H.3.3; H.3.7; I.2.7"
        ]
    },
    {
        "title": "Utilizing the World Wide Web as an Encyclopedia: Extracting Term   Descriptions from Semi-Structured Texts",
        "authors": [
            "Atsushi Fujii",
            "Tetsuya Ishikawa"
        ],
        "summary": "In this paper, we propose a method to extract descriptions of technical terms from Web pages in order to utilize the World Wide Web as an encyclopedia. We use linguistic patterns and HTML text structures to extract text fragments containing term descriptions. We also use a language model to discard extraneous descriptions, and a clustering method to summarize resultant descriptions. We show the effectiveness of our method by way of experiments.",
        "published": "2000-11-02T09:02:12Z",
        "link": "http://arxiv.org/abs/cs/0011001v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.3"
        ]
    },
    {
        "title": "A Novelty-based Evaluation Method for Information Retrieval",
        "authors": [
            "Atsushi Fujii",
            "Tetsuya Ishikawa"
        ],
        "summary": "In information retrieval research, precision and recall have long been used to evaluate IR systems. However, given that a number of retrieval systems resembling one another are already available to the public, it is valuable to retrieve novel relevant documents, i.e., documents that cannot be retrieved by those existing systems. In view of this problem, we propose an evaluation method that favors systems retrieving as many novel documents as possible. We also used our method to evaluate systems that participated in the IREX workshop.",
        "published": "2000-11-02T10:00:27Z",
        "link": "http://arxiv.org/abs/cs/0011002v1",
        "categories": [
            "cs.CL",
            "H.3.3"
        ]
    },
    {
        "title": "Applying Machine Translation to Two-Stage Cross-Language Information   Retrieval",
        "authors": [
            "Atsushi Fujii",
            "Tetsuya Ishikawa"
        ],
        "summary": "Cross-language information retrieval (CLIR), where queries and documents are in different languages, needs a translation of queries and/or documents, so as to standardize both of them into a common representation. For this purpose, the use of machine translation is an effective approach. However, computational cost is prohibitive in translating large-scale document collections. To resolve this problem, we propose a two-stage CLIR method. First, we translate a given query into the document language, and retrieve a limited number of foreign documents. Second, we machine translate only those documents into the user language, and re-rank them based on the translation result. We also show the effectiveness of our method by way of experiments using Japanese queries and English technical documents.",
        "published": "2000-11-02T10:32:08Z",
        "link": "http://arxiv.org/abs/cs/0011003v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.3"
        ]
    },
    {
        "title": "Tree-gram Parsing: Lexical Dependencies and Structural Relations",
        "authors": [
            "Khalil Sima'an"
        ],
        "summary": "This paper explores the kinds of probabilistic relations that are important in syntactic disambiguation. It proposes that two widely used kinds of relations, lexical dependencies and structural relations, have complementary disambiguation capabilities. It presents a new model based on structural relations, the Tree-gram model, and reports experiments showing that structural relations should benefit from enrichment by lexical dependencies.",
        "published": "2000-11-06T13:56:42Z",
        "link": "http://arxiv.org/abs/cs/0011007v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "I.2; K.3.2; J.5"
        ]
    },
    {
        "title": "Formal Properties of XML Grammars and Languages",
        "authors": [
            "Jean Berstel",
            "Luc Boasson"
        ],
        "summary": "XML documents are described by a document type definition (DTD). An XML-grammar is a formal grammar that captures the syntactic features of a DTD. We investigate properties of this family of grammars. We show that every XML-language basically has a unique XML-grammar. We give two characterizations of languages generated by XML-grammars, one is set-theoretic, the other is by a kind of saturation property. We investigate decidability problems and prove that some properties that are undecidable for general context-free languages become decidable for XML-languages. We also characterize those XML-grammars that generate regular XML-languages.",
        "published": "2000-11-07T10:09:15Z",
        "link": "http://arxiv.org/abs/cs/0011011v1",
        "categories": [
            "cs.DM",
            "cs.CL",
            "F.4.2;F.4.3;D.3.1"
        ]
    },
    {
        "title": "The Use of Instrumentation in Grammar Engineering",
        "authors": [
            "Norbert Broeker"
        ],
        "summary": "This paper explores the usefulness of a technique from software engineering, code instrumentation, for the development of large-scale natural language grammars. Information about the usage of grammar rules in test and corpus sentences is used to improve grammar and testsuite, as well as adapting a grammar to a specific genre. Results show that less than half of a large-coverage grammar for German is actually tested by two large testsuites, and that 10--30% of testing time is redundant. This methodology applied can be seen as a re-use of grammar writing knowledge for testsuite compilation.",
        "published": "2000-11-16T17:40:10Z",
        "link": "http://arxiv.org/abs/cs/0011020v2",
        "categories": [
            "cs.CL",
            "D.2.5"
        ]
    },
    {
        "title": "Retrieval from Captioned Image Databases Using Natural Language   Processing",
        "authors": [
            "David Elworthy"
        ],
        "summary": "It might appear that natural language processing should improve the accuracy of information retrieval systems, by making available a more detailed analysis of queries and documents. Although past results appear to show that this is not so, if the focus is shifted to short phrases rather than full documents, the situation becomes somewhat different. The ANVIL system uses a natural language technique to obtain high accuracy retrieval of images which have been annotated with a descriptive textual caption. The natural language techniques also allow additional contextual information to be derived from the relation between the query and the caption, which can help users to understand the overall collection of retrieval results. The techniques have been successfully used in a information retrieval system which forms both a testbed for research and the basis of a commercial system.",
        "published": "2000-11-20T15:36:09Z",
        "link": "http://arxiv.org/abs/cs/0011028v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3"
        ]
    },
    {
        "title": "Semantic interpretation of temporal information by abductive inference",
        "authors": [
            "Sven Verdoolaege",
            "Marc Denecker",
            "Ness Schelkens",
            "Danny De Schreye",
            "Frank Van Eynde"
        ],
        "summary": "Besides temporal information explicitly available in verbs and adjuncts, the temporal interpretation of a text also depends on general world knowledge and default assumptions. We will present a theory for describing the relation between, on the one hand, verbs, their tenses and adjuncts and, on the other, the eventualities and periods of time they represent and their relative temporal locations.   The theory is formulated in logic and is a practical implementation of the concepts described in Ness Schelkens et al. We will show how an abductive resolution procedure can be used on this representation to extract temporal information from texts.",
        "published": "2000-11-22T15:35:46Z",
        "link": "http://arxiv.org/abs/cs/0011034v1",
        "categories": [
            "cs.CL",
            "I.2.4; I.2.7"
        ]
    },
    {
        "title": "Abductive reasoning with temporal information",
        "authors": [
            "Sven Verdoolaege",
            "Marc Denecker",
            "Frank Van Eynde"
        ],
        "summary": "Texts in natural language contain a lot of temporal information, both explicit and implicit. Verbs and temporal adjuncts carry most of the explicit information, but for a full understanding general world knowledge and default assumptions have to be taken into account. We will present a theory for describing the relation between, on the one hand, verbs, their tenses and adjuncts and, on the other, the eventualities and periods of time they represent and their relative temporal locations, while allowing interaction with general world knowledge.   The theory is formulated in an extension of first order logic and is a practical implementation of the concepts described in Van Eynde 2001 and Schelkens et al. 2000. We will show how an abductive resolution procedure can be used on this representation to extract temporal information from texts. The theory presented here is an extension of that in Verdoolaege et al. 2000, adapted to VanEynde 2001, with a simplified and extended analysis of adjuncts and with more emphasis on how a model can be constructed.",
        "published": "2000-11-23T08:41:52Z",
        "link": "http://arxiv.org/abs/cs/0011035v1",
        "categories": [
            "cs.CL",
            "I.2.4; I.2.7"
        ]
    },
    {
        "title": "Do All Fragments Count?",
        "authors": [
            "Rens Bod"
        ],
        "summary": "We aim at finding the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street Journal treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank. We isolate a number of dependency relations which previous models neglect but which contribute to higher parse accuracy.",
        "published": "2000-11-24T23:51:21Z",
        "link": "http://arxiv.org/abs/cs/0011040v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Biologically Motivated Distributed Designs for Adaptive Knowledge   Management",
        "authors": [
            "Luis M. Rocha",
            "Johan Bollen"
        ],
        "summary": "We discuss how distributed designs that draw from biological network metaphors can largely improve the current state of information retrieval and knowledge management of distributed information systems. In particular, two adaptive recommendation systems named TalkMine and @ApWeb are discussed in more detail. TalkMine operates at the semantic level of keywords. It leads different databases to learn new and adapt existing keywords to the categories recognized by its communities of users using distributed algorithms. @ApWeb operates at the structural level of information resources, namely citation or hyperlink structure. It relies on collective behavior to adapt such structure to the expectations of users. TalkMine and @ApWeb are currently being implemented for the research library of the Los Alamos National Laboratory under the Active Recommendation Project. Together they define a biologically motivated information retrieval system, recommending simultaneously at the level of user knowledge categories expressed in keywords, and at the level of individual documents and their associations to other documents. Rather than passive information retrieval, with this system, users obtain an active, evolving interaction with information resources.",
        "published": "2000-02-16T18:22:32Z",
        "link": "http://arxiv.org/abs/cs/0002010v1",
        "categories": [
            "cs.IR",
            "H.3, H.1, H.2, I.2"
        ]
    },
    {
        "title": "Making news understandable to computers",
        "authors": [
            "Erik T. Mueller"
        ],
        "summary": "Computers and devices are largely unaware of events taking place in the world. This could be changed if news were made available in a computer-understandable form. In this paper we present XML documents called NewsForms that represent the key points of 17 types of news events. We discuss the benefits of computer-understandable news and present the NewsExtract program for converting text news stories into NewsForms.",
        "published": "2000-03-01T18:11:08Z",
        "link": "http://arxiv.org/abs/cs/0003001v1",
        "categories": [
            "cs.IR",
            "I.7.2"
        ]
    },
    {
        "title": "Automatic Classification of Text Databases through Query Probing",
        "authors": [
            "Panagiotis Ipeirotis",
            "Luis Gravano",
            "Mehran Sahami"
        ],
        "summary": "Many text databases on the web are \"hidden\" behind search interfaces, and their documents are only accessible through querying. Search engines typically ignore the contents of such search-only databases. Recently, Yahoo-like directories have started to manually organize these databases into categories that users can browse to find these valuable resources. We propose a novel strategy to automate the classification of search-only text databases. Our technique starts by training a rule-based document classifier, and then uses the classifier's rules to generate probing queries. The queries are sent to the text databases, which are then classified based on the number of matches that they produce for each query. We report some initial exploratory experiments that show that our approach is promising to automatically characterize the contents of text databases accessible on the web.",
        "published": "2000-03-09T04:01:22Z",
        "link": "http://arxiv.org/abs/cs/0003043v1",
        "categories": [
            "cs.DB",
            "cs.IR",
            "H.3"
        ]
    },
    {
        "title": "How to Evaluate your Question Answering System Every Day and Still Get   Real Work Done",
        "authors": [
            "Eric Breck",
            "John D. Burger",
            "Lisa Ferro",
            "Lynette Hirschman",
            "David House",
            "Marc Light",
            "Inderjeet Mani"
        ],
        "summary": "In this paper, we report on Qaviar, an experimental automated evaluation system for question answering applications. The goal of our research was to find an automatically calculated measure that correlates well with human judges' assessment of answer correctness in the context of question answering tasks. Qaviar judges the response by computing recall against the stemmed content words in the human-generated answer key. It counts the answer correct if it exceeds agiven recall threshold. We determined that the answer correctness predicted by Qaviar agreed with the human 93% to 95% of the time. 41 question-answering systems were ranked by both Qaviar and human assessors, and these rankings correlated with a Kendall's Tau measure of 0.920, compared to a correlation of 0.956 between human assessors on the same data.",
        "published": "2000-04-17T19:29:51Z",
        "link": "http://arxiv.org/abs/cs/0004008v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "I.2.7; H.3.4"
        ]
    },
    {
        "title": "Centroid-based summarization of multiple documents: sentence extraction,   utility-based evaluation, and user studies",
        "authors": [
            "Dragomir R. Radev",
            "Hongyan Jing",
            "Malgorzata Budzikowska"
        ],
        "summary": "We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.",
        "published": "2000-05-12T17:24:06Z",
        "link": "http://arxiv.org/abs/cs/0005020v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DL",
            "cs.HC",
            "cs.IR",
            "H.3.1; H.3.4; H.3.7; H.5.2; I.2.7"
        ]
    },
    {
        "title": "A One-Time Pad based Cipher for Data Protection in Distributed   Environments",
        "authors": [
            "Igor Sobrado"
        ],
        "summary": "A one-time pad (OTP) based cipher to insure both data protection and integrity when mobile code arrives to a remote host is presented. Data protection is required when a mobile agent could retrieve confidential information that would be encrypted in untrusted nodes of the network; in this case, information management could not rely on carrying an encryption key. Data integrity is a prerequisite because mobile code must be protected against malicious hosts that, by counterfeiting or removing collected data, could cover information to the server that has sent the agent. The algorithm described in this article seems to be simple enough, so as to be easily implemented. This scheme is based on a non-interactive protocol and allows a remote host to change its own data on-the-fly and, at the same time, protecting information against handling by other hosts.",
        "published": "2000-05-24T22:24:59Z",
        "link": "http://arxiv.org/abs/cs/0005026v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.IR",
            "cs.NI",
            "C.2.1; C.2.4; H.3; H.3.4"
        ]
    },
    {
        "title": "A method for command identification, using modified collision free   hashing with addition & rotation iterative hash functions (part 1)",
        "authors": [
            "Dimitrios Skraparlis"
        ],
        "summary": "This paper proposes a method for identification of a user`s fixed string set (which can be a command/instruction set for a terminal or microprocessor). This method is fast and has very small memory requirements, compared to a traditional full string storage and compare method. The user feeds characters into a microcontroller via a keyboard or another microprocessor sends commands and the microcontroller hashes the input in order to identify valid commands, ensuring no collisions between hashed valid strings, while applying further criteria to narrow collision between random and valid strings. The method proposed narrows the possibility of the latter kind of collision, achieving small code and memory-size utilization and very fast execution. Hashing is achieved using additive & rotating hash functions in an iterative form, which can be very easily implemented in simple microcontrollers and microprocessors. Such hash functions are presented and compared according to their efficiency for a given string/command set, using the program found in the appendix.",
        "published": "2000-05-27T01:08:07Z",
        "link": "http://arxiv.org/abs/cs/0005028v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "B.4.2; H.5.2"
        ]
    },
    {
        "title": "Using compression to identify acronyms in text",
        "authors": [
            "Stuart Yeates",
            "David Bainbridge",
            "Ian H. Witten"
        ],
        "summary": "Text mining is about looking for patterns in natural language text, and may be defined as the process of analyzing text to extract information from it for particular purposes. In previous work, we claimed that compression is a key technology for text mining, and backed this up with a study that showed how particular kinds of lexical tokens---names, dates, locations, etc.---can be identified and located in running text, using compression models to provide the leverage necessary to distinguish different token types (Witten et al., 1999)",
        "published": "2000-07-04T02:02:01Z",
        "link": "http://arxiv.org/abs/cs/0007003v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.7"
        ]
    },
    {
        "title": "Fuzzy data: XML may handle it",
        "authors": [
            "R. Schweiger",
            "S. Hoelzer",
            "J. Dudeck"
        ],
        "summary": "Data modeling is one of the most difficult tasks in application engineering. The engineer must be aware of the use cases and the required application services and at a certain point of time he has to fix the data model which forms the base for the application services. However, once the data model has been fixed it is difficult to consider changing needs. This might be a problem in specific domains, which are as dynamic as the healthcare domain. With fuzzy data we address all those data that are difficult to organize in a single database. In this paper we discuss a gradual and pragmatic approach that uses the XML technology to conquer more model flexibility. XML may provide the clue between unstructured text data and structured database solutions and shift the paradigm from \"organizing the data along a given model\" towards \"organizing the data along user requirements\".",
        "published": "2000-07-13T07:58:53Z",
        "link": "http://arxiv.org/abs/cs/0007017v1",
        "categories": [
            "cs.IR",
            "H3.2"
        ]
    },
    {
        "title": "Relevance as Deduction: A Logical View of Information Retrieval",
        "authors": [
            "Gianni Amati",
            "Konstantinos Georgatos"
        ],
        "summary": "The problem of Information Retrieval is, given a set of documents D and a query q, providing an algorithm for retrieving all documents in D relevant to q. However, retrieval should depend and be updated whenever the user is able to provide as an input a preferred set of relevant documents; this process is known as em relevance feedback. Recent work in IR has been paying great attention to models which employ a logical approach; the advantage being that one can have a simple computable characterization of retrieval on the basis of a pure logical analysis of retrieval. Most of the logical models make use of probabilities or similar belief functions in order to introduce the inductive component whereby uncertainty is treated. Their general paradigm is the following: em find the nature of conditional $d\\imp q$ and then define a probability on the top of it. We just reverse this point of view; first use the numerical information, frequencies or probabilities, then define your own logical consequence. More generally, we claim that retrieval is a form of deduction. We introduce a simple but powerful logical framework of relevance feedback, derived from the well founded area of nonmonotonic logic. This description can help us evaluate, describe and compare from a theoretical point of view previous approaches based on conditionals or probabilities.",
        "published": "2000-07-26T19:34:35Z",
        "link": "http://arxiv.org/abs/cs/0007041v1",
        "categories": [
            "cs.IR",
            "cs.LO",
            "I.2.3;H.3.0;H.3.3"
        ]
    },
    {
        "title": "Science User Scenarios for a Virtual Observatory Design Reference   Mission: Science Requirements for Data Mining",
        "authors": [
            "Kirk D. Borne"
        ],
        "summary": "The knowledge discovery potential of the new large astronomical databases is vast. When these are used in conjunction with the rich legacy data archives, the opportunities for scientific discovery multiply rapidly. A Virtual Observatory (VO) framework will enable transparent and efficient access, search, retrieval, and visualization of data across multiple data repositories, which are generally heterogeneous and distributed. Aspects of data mining that apply to a variety of science user scenarios with a VO are reviewed. The development of a VO should address the data mining needs of various astronomical research constituencies. By way of example, two user scenarios are presented which invoke applications and linkages of data across the catalog and image domains in order to address specific astrophysics research problems. These illustrate a subset of the desired capabilities and power of the VO, and as such they represent potential components of a VO Design Reference Mission.",
        "published": "2000-08-19T17:54:01Z",
        "link": "http://arxiv.org/abs/astro-ph/0008307v1",
        "categories": [
            "astro-ph",
            "cs.DB",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "An Experimental Comparison of Naive Bayesian and Keyword-Based Anti-Spam   Filtering with Personal E-mail Messages",
        "authors": [
            "Ion Androutsopoulos",
            "John Koutsias",
            "Konstantinos V. Chandrinos",
            "Constantine D. Spyropoulos"
        ],
        "summary": "The growing problem of unsolicited bulk e-mail, also known as \"spam\", has generated a need for reliable anti-spam e-mail filters. Filters of this type have so far been based mostly on manually constructed keyword patterns. An alternative approach has recently been proposed, whereby a Naive Bayesian classifier is trained automatically to detect spam messages. We test this approach on a large collection of personal e-mail messages, which we make publicly available in \"encrypted\" form contributing towards standard benchmarks. We introduce appropriate cost-sensitive measures, investigating at the same time the effect of attribute-set size, training-corpus size, lemmatization, and stop lists, issues that have not been explored in previous experiments. Finally, the Naive Bayesian filter is compared, in terms of performance, to a filter that uses keyword patterns, and which is part of a widely used e-mail reader.",
        "published": "2000-08-22T11:20:14Z",
        "link": "http://arxiv.org/abs/cs/0008019v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.4.3; I.2.6; I.2.7; I.5.4; K.4.1"
        ]
    },
    {
        "title": "Learning to Filter Spam E-Mail: A Comparison of a Naive Bayesian and a   Memory-Based Approach",
        "authors": [
            "Ion Androutsopoulos",
            "Georgios Paliouras",
            "Vangelis Karkaletsis",
            "Georgios Sakkis",
            "Constantine D. Spyropoulos",
            "Panagiotis Stamatopoulos"
        ],
        "summary": "We investigate the performance of two machine learning algorithms in the context of anti-spam filtering. The increasing volume of unsolicited bulk e-mail (spam) has generated a need for reliable anti-spam filters. Filters of this type have so far been based mostly on keyword patterns that are constructed by hand and perform poorly. The Naive Bayesian classifier has recently been suggested as an effective method to construct automatically anti-spam filters with superior performance. We investigate thoroughly the performance of the Naive Bayesian filter on a publicly available corpus, contributing towards standard benchmarks. At the same time, we compare the performance of the Naive Bayesian filter to an alternative memory-based learning approach, after introducing suitable cost-sensitive evaluation measures. Both methods achieve very accurate spam filtering, outperforming clearly the keyword-based filter of a widely used e-mail reader.",
        "published": "2000-09-18T14:05:13Z",
        "link": "http://arxiv.org/abs/cs/0009009v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.4.3; I.2.6; I.2.7; I.5.4; K.4.1"
        ]
    },
    {
        "title": "A Public-key based Information Management Model for Mobile Agents",
        "authors": [
            "Diego Rodriguez",
            "Igor Sobrado"
        ],
        "summary": "Mobile code based computing requires development of protection schemes that allow digital signature and encryption of data collected by the agents in untrusted hosts. These algorithms could not rely on carrying encryption keys if these keys could be stolen or used to counterfeit data by hostile hosts and agents. As a consequence, both information and keys must be protected in a way that only authorized hosts, that is the host that provides information and the server that has sent the mobile agent, could modify (by changing or removing) retrieved data. The data management model proposed in this work allows the information collected by the agents to be protected against handling by other hosts in the information network. It has been done by using standard public-key cryptography modified to support protection of data in distributed environments without requiring an interactive protocol with the host that has dropped the agent. Their significance stands on the fact that it is the first model that supports a full-featured protection of mobile agents allowing remote hosts to change its own information if required before agent returns to its originating server.",
        "published": "2000-10-09T20:17:35Z",
        "link": "http://arxiv.org/abs/cs/0010013v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.IR",
            "cs.NI",
            "C.2.1; C.2.4; H.3; H.3.4"
        ]
    },
    {
        "title": "Data Mining in Astronomical Databases",
        "authors": [
            "Kirk D. Borne"
        ],
        "summary": "A Virtual Observatory (VO) will enable transparent and efficient access, search, retrieval, and visualization of data across multiple data repositories, which are generally heterogeneous and distributed. Aspects of data mining that apply to a variety of science user scenarios with a VO are reviewed.",
        "published": "2000-10-27T21:46:42Z",
        "link": "http://arxiv.org/abs/astro-ph/0010583v2",
        "categories": [
            "astro-ph",
            "cs.DB",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Retrieval from Captioned Image Databases Using Natural Language   Processing",
        "authors": [
            "David Elworthy"
        ],
        "summary": "It might appear that natural language processing should improve the accuracy of information retrieval systems, by making available a more detailed analysis of queries and documents. Although past results appear to show that this is not so, if the focus is shifted to short phrases rather than full documents, the situation becomes somewhat different. The ANVIL system uses a natural language technique to obtain high accuracy retrieval of images which have been annotated with a descriptive textual caption. The natural language techniques also allow additional contextual information to be derived from the relation between the query and the caption, which can help users to understand the overall collection of retrieval results. The techniques have been successfully used in a information retrieval system which forms both a testbed for research and the basis of a commercial system.",
        "published": "2000-11-20T15:36:09Z",
        "link": "http://arxiv.org/abs/cs/0011028v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3"
        ]
    },
    {
        "title": "A Benchmark for Image Retrieval using Distributed Systems over the   Internet: BIRDS-I",
        "authors": [
            "Neil J. Gunther",
            "Giordano B. Beretta"
        ],
        "summary": "The performance of CBIR algorithms is usually measured on an isolated workstation. In a real-world environment the algorithms would only constitute a minor component among the many interacting components. The Internet dramati-cally changes many of the usual assumptions about measuring CBIR performance. Any CBIR benchmark should be designed from a networked systems standpoint. These benchmarks typically introduce communication overhead because the real systems they model are distributed applications. We present our implementation of a client/server benchmark called BIRDS-I to measure image retrieval performance over the Internet. It has been designed with the trend toward the use of small personalized wireless systems in mind. Web-based CBIR implies the use of heteroge-neous image sets, imposing certain constraints on how the images are organized and the type of performance metrics applicable. BIRDS-I only requires controlled human intervention for the compilation of the image collection and none for the generation of ground truth in the measurement of retrieval accuracy. Benchmark image collections need to be evolved incrementally toward the storage of millions of images and that scaleup can only be achieved through the use of computer-aided compilation. Finally, our scoring metric introduces a tightly optimized image-ranking window.",
        "published": "2000-12-22T23:38:37Z",
        "link": "http://arxiv.org/abs/cs/0012021v1",
        "categories": [
            "cs.IR",
            "cs.MM",
            "D.2.8;H.2.8;H.3.1;H.3.4;H.3.5"
        ]
    },
    {
        "title": "Why C++ is not very fit for GUI programming",
        "authors": [
            "Oleg Kiselyov"
        ],
        "summary": "With no intent of starting a holy war, this paper lists several annoying C++ birthmarks that the author has come across developing GUI class libraries. C++'s view of classes, instances and hierarchies appears tantalizingly close to GUI concepts of controls, widgets, window classes and subwindows. OO models of C++ and of a window system are however different. C++ was designed to be a \"static\" language with a lexical name scoping, static type checking and hierarchies defined at compile time. Screen objects on the other hand are inherently dynamic; they usually live well beyond the procedure/block that created them; the hierarchy of widgets is defined to a large extent by layout, visibility and event flow. Many GUI fundamentals such as dynamic and geometric hierarchies of windows and controls, broadcasting and percolation of events are not supported directly by C++ syntax or execution semantics (or supported as \"exceptions\" -- pun intended). Therefore these features have to be emulated in C++ GUI code. This leads to duplication of a graphical toolkit or a window manager functionality, code bloat, engaging in unsafe practices and forgoing of many strong C++ features (like scoping rules and compile-time type checking). This paper enumerates a few major C++/GUI sores and illustrates them on simple examples.",
        "published": "2000-01-06T21:31:29Z",
        "link": "http://arxiv.org/abs/cs/0001003v1",
        "categories": [
            "cs.PL",
            "D.3.3; D.1.5"
        ]
    },
    {
        "title": "Fractal Symbolic Analysis",
        "authors": [
            "Nikolay Mateev",
            "Vijay Menon",
            "Keshav Pingali"
        ],
        "summary": "Restructuring compilers use dependence analysis to prove that the meaning of a program is not changed by a transformation. A well-known limitation of dependence analysis is that it examines only the memory locations read and written by a statement, and does not assume any particular interpretation for the operations in that statement. Exploiting the semantics of these operations enables a wider set of transformations to be used, and is critical for optimizing important codes such as LU factorization with pivoting.   Symbolic execution of programs enables the exploitation of such semantic properties, but it is intractable for all but the simplest programs. In this paper, we propose a new form of symbolic analysis for use in restructuring compilers. Fractal symbolic analysis compares a program and its transformed version by repeatedly simplifying these programs until symbolic analysis becomes tractable, ensuring that equality of simplified programs is sufficient to guarantee equality of the original programs. We present a prototype implementation of fractal symbolic analysis, and show how it can be used to optimize the cache performance of LU factorization with pivoting.",
        "published": "2000-01-12T22:15:05Z",
        "link": "http://arxiv.org/abs/cs/0001009v1",
        "categories": [
            "cs.PL",
            "D.3.4"
        ]
    },
    {
        "title": "SLT-Resolution for the Well-Founded Semantics",
        "authors": [
            "Yi-Dong Shen",
            "Li-Yan Yuan",
            "Jia-Huai You"
        ],
        "summary": "Global SLS-resolution and SLG-resolution are two representative mechanisms for top-down evaluation of the well-founded semantics of general logic programs. Global SLS-resolution is linear for query evaluation but suffers from infinite loops and redundant computations. In contrast, SLG-resolution resolves infinite loops and redundant computations by means of tabling, but it is not linear. The principal disadvantage of a non-linear approach is that it cannot be implemented using a simple, efficient stack-based memory structure nor can it be easily extended to handle some strictly sequential operators such as cuts in Prolog.   In this paper, we present a linear tabling method, called SLT-resolution, for top-down evaluation of the well-founded semantics. SLT-resolution is a substantial extension of SLDNF-resolution with tabling. Its main features include: (1) It resolves infinite loops and redundant computations while preserving the linearity. (2) It is terminating, and sound and complete w.r.t. the well-founded semantics for programs with the bounded-term-size property with non-floundering queries. Its time complexity is comparable with SLG-resolution and polynomial for function-free logic programs. (3) Because of its linearity for query evaluation, SLT-resolution bridges the gap between the well-founded semantics and standard Prolog implementation techniques. It can be implemented by an extension to any existing Prolog abstract machines such as WAM or ATOAM.",
        "published": "2000-02-27T19:20:05Z",
        "link": "http://arxiv.org/abs/cs/0002016v3",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.3.1; F.4.1; I.2.3"
        ]
    },
    {
        "title": "TSIA: A Dataflow Model",
        "authors": [
            "Burkhard D. Steinmacher-Burow"
        ],
        "summary": "The Task System and Item Architecture (TSIA) is a model for transparent application execution. In many real-world projects, a TSIA provides a simple application with a transparent reliable, distributed, heterogeneous, adaptive, dynamic, real-time, parallel, secure or other execution. TSIA is suitable for many applications, not just for the simple applications served to date. This presentation shows that TSIA is a dataflow model - a long-standing model for transparent parallel execution. The advances to the dataflow model include a simple semantics, as well as support for input/output, for modifiable items and for other such effects.",
        "published": "2000-03-06T12:16:00Z",
        "link": "http://arxiv.org/abs/cs/0003010v1",
        "categories": [
            "cs.PL",
            "D.3.2;D.3.3"
        ]
    },
    {
        "title": "Reasoning with Higher-Order Abstract Syntax in a Logical Framework",
        "authors": [
            "Raymond C. McDowell",
            "Dale A. Miller"
        ],
        "summary": "Logical frameworks based on intuitionistic or linear logics with higher-type quantification have been successfully used to give high-level, modular, and formal specifications of many important judgments in the area of programming languages and inference systems. Given such specifications, it is natural to consider proving properties about the specified systems in the framework: for example, given the specification of evaluation for a functional programming language, prove that the language is deterministic or that evaluation preserves types. One challenge in developing a framework for such reasoning is that higher-order abstract syntax (HOAS), an elegant and declarative treatment of object-level abstraction and substitution, is difficult to treat in proofs involving induction. In this paper, we present a meta-logic that can be used to reason about judgments coded using HOAS; this meta-logic is an extension of a simple intuitionistic logic that admits higher-order quantification over simply typed lambda-terms (key ingredients for HOAS) as well as induction and a notion of definition. We explore the difficulties of formal meta-theoretic analysis of HOAS encodings by considering encodings of intuitionistic and linear logics, and formally derive the admissibility of cut for important subsets of these logics. We then propose an approach to avoid the apparent tradeoff between the benefits of higher-order abstract syntax and the ability to analyze the resulting encodings. We illustrate this approach through examples involving the simple functional and imperative programming languages PCF and PCF:=. We formally derive such properties as unicity of typing, subject reduction, determinacy of evaluation, and the equivalence of transition semantics and natural semantics presentations of evaluation.",
        "published": "2000-03-14T22:35:00Z",
        "link": "http://arxiv.org/abs/cs/0003062v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.1; F.3.1; D.2.4; F.4.1"
        ]
    },
    {
        "title": "A Polyvariant Binding-Time Analysis for Off-line Partial Deduction",
        "authors": [
            "Maurice Bruynooghe",
            "Michael Leuschel",
            "Konstantinos Sagonas"
        ],
        "summary": "We study the notion of binding-time analysis for logic programs. We formalise the unfolding aspect of an on-line partial deduction system as a Prolog program. Using abstract interpretation, we collect information about the run-time behaviour of the program. We use this information to make the control decisions about the unfolding at analysis time and to turn the on-line system into an off-line system. We report on some initial experiments.",
        "published": "2000-03-17T12:51:47Z",
        "link": "http://arxiv.org/abs/cs/0003068v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.3.0; D.1.6; F.3.1"
        ]
    },
    {
        "title": "The (Lazy) Functional Side of Logic Programming",
        "authors": [
            "S. Etalle",
            "J. Mountjoy"
        ],
        "summary": "The possibility of translating logic programs into functional ones has long been a subject of investigation. Common to the many approaches is that the original logic program, in order to be translated, needs to be well-moded and this has led to the common understanding that these programs can be considered to be the ``functional part'' of logic programs. As a consequence of this it has become widely accepted that ``complex'' logical variables, the possibility of a dynamic selection rule, and general properties of non-well-moded programs are exclusive features of logic programs. This is not quite true, as some of these features are naturally found in lazy functional languages. We readdress the old question of what features are exclusive to the logic programming paradigm by defining a simple translation applicable to a wider range of logic programs, and demonstrate that the current circumscription is unreasonably restrictive.",
        "published": "2000-03-20T09:52:20Z",
        "link": "http://arxiv.org/abs/cs/0003070v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.1; D.1.6; D.3.2; F.3.3"
        ]
    },
    {
        "title": "Constraint Programming viewed as Rule-based Programming",
        "authors": [
            "Krzysztof R. Apt",
            "Eric Monfroy"
        ],
        "summary": "We study here a natural situation when constraint programming can be entirely reduced to rule-based programming. To this end we explain first how one can compute on constraint satisfaction problems using rules represented by simple first-order formulas. Then we consider constraint satisfaction problems that are based on predefined, explicitly given constraints. To solve them we first derive rules from these explicitly given constraints and limit the computation process to a repeated application of these rules, combined with labeling.We consider here two types of rules. The first type, that we call equality rules, leads to a new notion of local consistency, called {\\em rule consistency} that turns out to be weaker than arc consistency for constraints of arbitrary arity (called hyper-arc consistency in \\cite{MS98b}). For Boolean constraints rule consistency coincides with the closure under the well-known propagation rules for Boolean constraints. The second type of rules, that we call membership rules, yields a rule-based characterization of arc consistency. To show feasibility of this rule-based approach to constraint programming we show how both types of rules can be automatically generated, as {\\tt CHR} rules of \\cite{fruhwirth-constraint-95}. This yields an implementation of this approach to programming by means of constraint logic programming. We illustrate the usefulness of this approach to constraint programming by discussing various examples, including Boolean constraints, two typical examples of many valued logics, constraints dealing with Waltz's language for describing polyhedral scenes, and Allen's qualitative approach to temporal logic.",
        "published": "2000-03-24T16:12:22Z",
        "link": "http://arxiv.org/abs/cs/0003076v2",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.3.3;I.2.2;I.2.3"
        ]
    },
    {
        "title": "Programming in Alma-0, or Imperative and Declarative Programming   Reconciled",
        "authors": [
            "Krzysztof R. Apt",
            "Andrea Schaerf"
        ],
        "summary": "In (Apt et al, TOPLAS 1998) we introduced the imperative programming language Alma-0 that supports declarative programming. In this paper we illustrate the hybrid programming style of Alma-0 by means of various examples that complement those presented in (Apt et al, TOPLAS 1998). The presented Alma-0 programs illustrate the versatility of the language and show that ``don't know'' nondeterminism can be naturally combined with assignment.",
        "published": "2000-04-05T16:04:26Z",
        "link": "http://arxiv.org/abs/cs/0004002v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL",
            "D.3.2;F.3.2;F.3.3;I.2.8;I.5.5"
        ]
    },
    {
        "title": "On Redundancy Elimination Tolerant Scheduling Rules",
        "authors": [
            "F. Ferrucci",
            "G. Pacini",
            "M. I. Sessa"
        ],
        "summary": "In (Ferrucci, Pacini and Sessa, 1995) an extended form of resolution, called Reduced SLD resolution (RSLD), is introduced. In essence, an RSLD derivation is an SLD derivation such that redundancy elimination from resolvents is performed after each rewriting step. It is intuitive that redundancy elimination may have positive effects on derivation process. However, undesiderable effects are also possible. In particular, as shown in this paper, program termination as well as completeness of loop checking mechanisms via a given selection rule may be lost. The study of such effects has led us to an analysis of selection rule basic concepts, so that we have found convenient to move the attention from rules of atom selection to rules of atom scheduling. A priority mechanism for atom scheduling is built, where a priority is assigned to each atom in a resolvent, and primary importance is given to the event of arrival of new atoms from the body of the applied clause at rewriting time. This new computational model proves able to address the study of redundancy elimination effects, giving at the same time interesting insights into general properties of selection rules. As a matter of fact, a class of scheduling rules, namely the specialisation independent ones, is defined in the paper by using not trivial semantic arguments. As a quite surprising result, specialisation independent scheduling rules turn out to coincide with a class of rules which have an immediate structural characterisation (named stack-queue rules). Then we prove that such scheduling rules are tolerant to redundancy elimination, in the sense that neither program termination nor completeness of equality loop check is lost passing from SLD to RSLD.",
        "published": "2000-04-17T13:34:17Z",
        "link": "http://arxiv.org/abs/cs/0004006v1",
        "categories": [
            "cs.PL",
            "D.1.6"
        ]
    },
    {
        "title": "Task Frames",
        "authors": [
            "Burkhard D. Steinmacher-Burow"
        ],
        "summary": "Forty years ago Dijkstra introduced the current conventional execution of routines. It places activation frames onto a stack. Each frame is the internal state of an executing routine. The resulting application execution is not easily helped by an external system. This presentation proposes an alternative execution of routines. It places task frames onto the stack. A task frame is the call of a routine to be executed. The feasibility of the alternative execution is demonstrated by a crude implementation. As described elsewhere, an application which executes in terms of tasks can be provided by an external system with a transparent reliable, distributed, heterogeneous, adaptive, dynamic, real-time, parallel, secure or other execution. By extending the crude implementation, this presentation outlines a simple transparent parallel execution.",
        "published": "2000-04-19T12:22:36Z",
        "link": "http://arxiv.org/abs/cs/0004011v1",
        "categories": [
            "cs.PL",
            "D.3.3;D.3.4"
        ]
    },
    {
        "title": "Application Software, Domain-Specific Languages, and Language Design   Assistants",
        "authors": [
            "Jan Heering"
        ],
        "summary": "While application software does the real work, domain-specific languages (DSLs) are tools to help produce it efficiently, and language design assistants in turn are meta-tools to help produce DSLs quickly. DSLs are already in wide use (HTML for web pages, Excel macros for spreadsheet applications, VHDL for hardware design, ...), but many more will be needed for both new as well as existing application domains. Language design assistants to help develop them currently exist only in the basic form of language development systems. After a quick look at domain-specific languages, and especially their relationship to application libraries, we survey existing language development systems and give an outline of future language design assistants.",
        "published": "2000-05-03T13:34:15Z",
        "link": "http://arxiv.org/abs/cs/0005002v1",
        "categories": [
            "cs.PL",
            "D.3"
        ]
    },
    {
        "title": "A Denotational Semantics for First-Order Logic",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "In Apt and Bezem [AB99] (see cs.LO/9811017) we provided a computational interpretation of first-order formulas over arbitrary interpretations. Here we complement this work by introducing a denotational semantics for first-order logic. Additionally, by allowing an assignment of a non-ground term to a variable we introduce in this framework logical variables.   The semantics combines a number of well-known ideas from the areas of semantics of imperative programming languages and logic programming. In the resulting computational view conjunction corresponds to sequential composition, disjunction to ``don't know'' nondeterminism, existential quantification to declaration of a local variable, and negation to the ``negation as finite failure'' rule. The soundness result shows correctness of the semantics with respect to the notion of truth. The proof resembles in some aspects the proof of the soundness of the SLDNF-resolution.",
        "published": "2000-05-08T12:23:07Z",
        "link": "http://arxiv.org/abs/cs/0005008v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "F.3.2; D.3.2"
        ]
    },
    {
        "title": "On Modular Termination Proofs of General Logic Programs",
        "authors": [
            "Annalisa Bossi",
            "Nicoletta Cocco",
            "Sandro Etalle",
            "Sabina Rossi"
        ],
        "summary": "We propose a modular method for proving termination of general logic programs (i.e., logic programs with negation). It is based on the notion of acceptable programs, but it allows us to prove termination in a truly modular way. We consider programs consisting of a hierarchy of modules and supply a general result for proving termination by dealing with each module separately. For programs which are in a certain sense well-behaved, namely well-moded or well-typed programs, we derive both a simple verification technique and an iterative proof method. Some examples show how our system allows for greatly simplified proofs.",
        "published": "2000-05-11T15:27:49Z",
        "link": "http://arxiv.org/abs/cs/0005018v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.2; D.3; F.3.1; F.3.2"
        ]
    },
    {
        "title": "C++ programming language for an abstract massively parallel SIMD   architecture",
        "authors": [
            "Alessandro Lonardo",
            "Emanuele Panizzi",
            "Benedetto Proietti"
        ],
        "summary": "The aim of this work is to define and implement an extended C++ language to support the SIMD programming paradigm. The C++ programming language has been extended to express all the potentiality of an abstract SIMD machine consisting of a central Control Processor and a N-dimensional toroidal array of Numeric Processors. Very few extensions have been added to the standard C++ with the goal of minimising the effort for the programmer in learning a new language and to keep very high the performance of the compiled code. The proposed language has been implemented as a porting of the GNU C++ Compiler on a SIMD supercomputer.",
        "published": "2000-05-19T10:19:51Z",
        "link": "http://arxiv.org/abs/cs/0005023v1",
        "categories": [
            "cs.PL",
            "D.3.3; D.2.1"
        ]
    },
    {
        "title": "Multimethods and separate static typechecking in a language with   C++-like object model",
        "authors": [
            "Emanuele Panizzi",
            "Bernardo Pastorelli"
        ],
        "summary": "The goal of this paper is the description and analysis of multimethod implementation in a new object-oriented, class-based programming language called OOLANG. The implementation of the multimethod typecheck and selection, deeply analyzed in the paper, is performed in two phases in order to allow static typechecking and separate compilation of modules. The first phase is performed at compile time, while the second is executed at link time and does not require the modules' source code. OOLANG has syntax similar to C++; the main differences are the absence of pointers and the realization of polymorphism through subsumption. It adopts the C++ object model and supports multiple inheritance as well as virtual base classes. For this reason, it has been necessary to define techniques for realigning argument and return value addresses when performing multimethod invocations.",
        "published": "2000-05-31T07:54:30Z",
        "link": "http://arxiv.org/abs/cs/0005033v1",
        "categories": [
            "cs.PL",
            "D.1.5; D.3.3; D.3.4"
        ]
    },
    {
        "title": "Verifying Termination and Error-Freedom of Logic Programs with block   Declarations",
        "authors": [
            "Jan-Georg Smaus",
            "Patricia M. Hill",
            "Andy King"
        ],
        "summary": "We present verification methods for logic programs with delay declarations. The verified properties are termination and freedom from errors related to built-ins. Concerning termination, we present two approaches. The first approach tries to eliminate the well-known problem of speculative output bindings. The second approach is based on identifying the predicates for which the textual position of an atom using this predicate is irrelevant with respect to termination. Three features are distinctive of this work: it allows for predicates to be used in several modes; it shows that block declarations, which are a very simple delay construct, are sufficient to ensure the desired properties; it takes the selection rule into account, assuming it to be as in most Prolog implementations. The methods can be used to verify existing programs and assist in writing new programs.",
        "published": "2000-06-23T14:27:09Z",
        "link": "http://arxiv.org/abs/cs/0006033v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.1; D.3.2"
        ]
    },
    {
        "title": "Type Classes and Constraint Handling Rules",
        "authors": [
            "Kevin Glynn",
            "Martin Sulzmann",
            "Peter J. Stuckey"
        ],
        "summary": "Type classes are an elegant extension to traditional, Hindley-Milner based typing systems. They are used in modern, typed languages such as Haskell to support controlled overloading of symbols. Haskell 98 supports only single-parameter and constructor type classes. Other extensions such as multi-parameter type classes are highly desired but are still not officially supported by Haskell. Subtle issues arise with extensions, which may lead to a loss of feasible type inference or ambiguous programs. A proper logical basis for type class systems seems to be missing. Such a basis would allow extensions to be characterised and studied rigorously. We propose to employ Constraint Handling Rules as a tool to study and develop type class systems in a uniform way.",
        "published": "2000-06-26T00:39:32Z",
        "link": "http://arxiv.org/abs/cs/0006034v1",
        "categories": [
            "cs.PL",
            "D.3.3; F.3.1"
        ]
    },
    {
        "title": "Constraint Exploration and Envelope of Simulation Trajectories",
        "authors": [
            "Oswaldo Teran",
            "Bruce Edmonds",
            "Steve Wallis"
        ],
        "summary": "The implicit theory that a simulation represents is precisely not in the individual choices but rather in the 'envelope' of possible trajectories - what is important is the shape of the whole envelope. Typically a huge amount of computation is required when experimenting with factors bearing on the dynamics of a simulation to tease out what affects the shape of this envelope. In this paper we present a methodology aimed at systematically exploring this envelope. We propose a method for searching for tendencies and proving their necessity relative to a range of parameterisations of the model and agents' choices, and to the logic of the simulation language. The exploration consists of a forward chaining generation of the trajectories associated to and constrained by such a range of parameterisations and choices. Additionally, we propose a computational procedure that helps implement this exploration by translating a Multi Agent System simulation into a constraint-based search over possible trajectories by 'compiling' the simulation rules into a more specific form, namely by partitioning the simulation rules using appropriate modularity in the simulation. An example of this procedure is exhibited.   Keywords: Constraint Search, Constraint Logic Programming, Proof, Emergence, Tendencies",
        "published": "2000-07-03T10:10:09Z",
        "link": "http://arxiv.org/abs/cs/0007001v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.LO",
            "D.3.3; F.3.1; F.4.1"
        ]
    },
    {
        "title": "Compiling Language Definitions: The ASF+SDF Compiler",
        "authors": [
            "M. G. J. van den Brand",
            "J. Heering",
            "P. Klint",
            "P. A. Olivier"
        ],
        "summary": "The ASF+SDF Meta-Environment is an interactive language development environment whose main application areas are definition of domain-specific languages, generation of program analysis and transformation tools, production of software renovation tools, and general specification and prototyping. It uses conditional rewrite rules to define the dynamic semantics and other tool-oriented aspects of languages, so the effectiveness of the generated tools is critically dependent on the quality of the rewrite rule implementation.   The ASF+SDF rewrite rule compiler generates C code, thus taking advantage of C's portability and the sophisticated optimization capabilities of current C compilers as well as avoiding potential abstract machine interface bottlenecks. It can handle large (10 000+ rule) language definitions and uses an efficient run-time storage scheme capable of handling large (1 000 000+ node) terms. Term storage uses maximal subterm sharing (hash-consing), which turns out to be more effective in the case of ASF+SDF than in Lisp or SML. Extensive benchmarking has shown the time and space performance of the generated code to be as good as or better than that of the best current rewrite rule and functional language compilers.",
        "published": "2000-07-06T11:38:42Z",
        "link": "http://arxiv.org/abs/cs/0007008v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.1; D.3.2; D.3.4; F.4.2"
        ]
    },
    {
        "title": "Applying Constraint Handling Rules to HPSG",
        "authors": [
            "Gerald Penn"
        ],
        "summary": "Constraint Handling Rules (CHR) have provided a realistic solution to an over-arching problem in many fields that deal with constraint logic programming: how to combine recursive functions or relations with constraints while avoiding non-termination problems. This paper focuses on some other benefits that CHR, specifically their implementation in SICStus Prolog, have provided to computational linguists working on grammar design tools. CHR rules are applied by means of a subsumption check and this check is made only when their variables are instantiated or bound. The former functionality is at best difficult to simulate using more primitive coroutining statements such as SICStus when/2, and the latter simply did not exist in any form before CHR.   For the sake of providing a case study in how these can be applied to grammar development, we consider the Attribute Logic Engine (ALE), a Prolog preprocessor for logic programming with typed feature structures, and its extension to a complete grammar development system for Head-driven Phrase Structure Grammar (HPSG), a popular constraint-based linguistic theory that uses typed feature structures. In this context, CHR can be used not only to extend the constraint language of feature structure descriptions to include relations in a declarative way, but also to provide support for constraints with complex antecedents and constraints on the co-occurrence of feature values that are necessary to interpret the type system of HPSG properly.",
        "published": "2000-07-07T19:09:25Z",
        "link": "http://arxiv.org/abs/cs/0007013v1",
        "categories": [
            "cs.CL",
            "cs.PL",
            "I.2.7; D.3.3"
        ]
    },
    {
        "title": "Polynomial-time Computation via Local Inference Relations",
        "authors": [
            "Robert Givan",
            "David McAllester"
        ],
        "summary": "We consider the concept of a local set of inference rules. A local rule set can be automatically transformed into a rule set for which bottom-up evaluation terminates in polynomial time. The local-rule-set transformation gives polynomial-time evaluation strategies for a large variety of rule sets that cannot be given terminating evaluation strategies by any other known automatic technique. This paper discusses three new results. First, it is shown that every polynomial-time predicate can be defined by an (unstratified) local rule set. Second, a new machine-recognizable subclass of the local rule sets is identified. Finally we show that locality, as a property of rule sets, is undecidable in general.",
        "published": "2000-07-13T17:19:43Z",
        "link": "http://arxiv.org/abs/cs/0007020v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL",
            "I.2.2; I.2.3; I.2.4; F.4.m"
        ]
    },
    {
        "title": "The Concurrent Language Aldwych",
        "authors": [
            "Matthew Huntbach"
        ],
        "summary": "Aldwych is proposed as the foundation of a general purpose language for parallel applications. It works on a rule-based principle, and has aspects variously of concurrent functional, logic and object-oriented languages, yet it forms an integrated whole. It is intended to be applicable both for small-scale parallel programming, and for large-scale open systems.",
        "published": "2000-09-29T14:24:39Z",
        "link": "http://arxiv.org/abs/cs/0009029v1",
        "categories": [
            "cs.PL",
            "D.3.3"
        ]
    },
    {
        "title": "From Syntactic Theories to Interpreters: A Specification Language and   Its Compilation",
        "authors": [
            "Yong Xiao",
            "Zena M. Ariola",
            "Michel Mauny"
        ],
        "summary": "Recent years have seen an increasing need of high-level specification languages and tools generating code from specifications. In this paper, we introduce a specification language, {\\splname}, which is tailored to the writing of syntactic theories of language semantics. More specifically, the language supports specifying primitive notions such as dynamic constraints, contexts, axioms, and inference rules. We also introduce a system which generates interpreters from {\\splname} specifications. A prototype system is implemented and has been tested on a number of examples, including a syntactic theory for Verilog.",
        "published": "2000-09-29T20:29:41Z",
        "link": "http://arxiv.org/abs/cs/0009030v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.1; D.1.2; D.3.1; F.3.1; F.3.2; I.2.2"
        ]
    },
    {
        "title": "The Light Lexicographic path Ordering",
        "authors": [
            "E. A. Cichon",
            "J-Y. Marion"
        ],
        "summary": "We introduce syntactic restrictions of the lexicographic path ordering to obtain the Light Lexicographic Path Ordering. We show that the light lexicographic path ordering leads to a characterisation of the functions computable in space bounded by a polynomial in the size of the inputs.",
        "published": "2000-10-03T15:38:46Z",
        "link": "http://arxiv.org/abs/cs/0010008v1",
        "categories": [
            "cs.PL",
            "cs.CC",
            "F.1.3; I.2.2"
        ]
    },
    {
        "title": "An Approach to the Implementation of Overlapping Rules in Standard ML",
        "authors": [
            "Riccardo Pucella"
        ],
        "summary": "We describe an approach to programming rule-based systems in Standard ML, with a focus on so-called overlapping rules, that is rules that can still be active when other rules are fired. Such rules are useful when implementing rule-based reactive systems, and to that effect we show a simple implementation of Loyall's Active Behavior Trees, used to control goal-directed agents in the Oz virtual environment. We discuss an implementation of our framework using a reactive library geared towards implementing those kind of systems.",
        "published": "2000-10-03T17:00:42Z",
        "link": "http://arxiv.org/abs/cs/0010009v1",
        "categories": [
            "cs.PL",
            "D.3.3"
        ]
    },
    {
        "title": "On Exponential-Time Completeness of the Circularity Problem for   Attribute Grammars",
        "authors": [
            "Pei-Chi Wu"
        ],
        "summary": "Attribute grammars (AGs) are a formal technique for defining semantics of programming languages. Existing complexity proofs on the circularity problem of AGs are based on automata theory, such as writing pushdown acceptor and alternating Turing machines. They reduced the acceptance problems of above automata, which are exponential-time (EXPTIME) complete, to the AG circularity problem. These proofs thus show that the circularity problem is EXPTIME-hard, at least as hard as the most difficult problems in EXPTIME. However, none has given a proof for the EXPTIME-completeness of the problem. This paper first presents an alternating Turing machine for the circularity problem. The alternating Turing machine requires polynomial space. Thus, the circularity problem is in EXPTIME and is then EXPTIME-complete.",
        "published": "2000-10-10T09:26:35Z",
        "link": "http://arxiv.org/abs/cs/0010015v1",
        "categories": [
            "cs.PL",
            "cs.CC",
            "D.3.1; D.3.4; F.2.2; F.4.2"
        ]
    },
    {
        "title": "Towards rule-based visual programming of generic visual systems",
        "authors": [
            "Berthold Hoffmann",
            "Mark Minas"
        ],
        "summary": "This paper illustrates how the diagram programming language DiaPlan can be used to program visual systems. DiaPlan is a visual rule-based language that is founded on the computational model of graph transformation. The language supports object-oriented programming since its graphs are hierarchically structured. Typing allows the shape of these graphs to be specified recursively in order to increase program security. Thanks to its genericity, DiaPlan allows to implement systems that represent and manipulate data in arbitrary diagram notations. The environment for the language exploits the diagram editor generator DiaGen for providing genericity, and for implementing its user interface and type checker.",
        "published": "2000-10-10T10:32:00Z",
        "link": "http://arxiv.org/abs/cs/0010016v1",
        "categories": [
            "cs.PL",
            "D.1.7; D.3.3"
        ]
    },
    {
        "title": "Sequence-Based Abstract Interpretation of Prolog",
        "authors": [
            "Baudouin Le Charlier",
            "Sabina Rossi",
            "Pascal Van Hentenryck"
        ],
        "summary": "Many abstract interpretation frameworks and analyses for Prolog have been proposed, which seek to extract information useful for program optimization. Although motivated by practical considerations, notably making Prolog competitive with imperative languages, such frameworks fail to capture some of the control structures of existing implementations of the language.   In this paper we propose a novel framework for the abstract interpretation of Prolog which handles the depth-first search rule and the cut operator. It relies on the notion of substitution sequence to model the result of the execution of a goal. The framework consists of (i) a denotational concrete semantics, (ii) a safe abstraction of the concrete semantics defined in terms of a class of post-fixpoints, and (iii) a generic abstract interpretation algorithm. We show that traditional abstract domains of substitutions may easily be adapted to the new framework, and provide experimental evidence of the effectiveness of our approach. We also show that previous work on determinacy analysis, that was not expressible by existing abstract interpretation frameworks, can be seen as an instance of our framework.",
        "published": "2000-10-19T11:00:12Z",
        "link": "http://arxiv.org/abs/cs/0010028v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.2; D.3; F.3.1; F.3.2"
        ]
    },
    {
        "title": "Static Analysis Techniques for Equational Logic Programming",
        "authors": [
            "Rakesh M. Verma"
        ],
        "summary": "An equational logic program is a set of directed equations or rules, which are used to compute in the obvious way (by replacing equals with ``simpler'' equals). We present static analysis techniques for efficient equational logic programming, some of which have been implemented in $LR^2$, a laboratory for developing and evaluating fast, efficient, and practical rewriting techniques. Two novel features of $LR^2$ are that non-left-linear rules are allowed in most contexts and it has a tabling option based on the congruence-closure based algorithm to compute normal forms. Although, the focus of this research is on the tabling approach some of the techniques are applicable to the untabled approach as well. Our presentation is in the context of $LR^2$, which is an interpreter, but some of the techniques apply to compilation as well.",
        "published": "2000-10-27T20:47:23Z",
        "link": "http://arxiv.org/abs/cs/0010034v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.2; D.3.2"
        ]
    },
    {
        "title": "Proceedings of the Fourth International Workshop on Automated Debugging   (AADEBUG 2000)",
        "authors": [
            "M. Ducasse"
        ],
        "summary": "Over the past decades automated debugging has seen major achievements. However, as debugging is by necessity attached to particular programming paradigms, the results are scattered. The aims of the workshop are to gather common themes and solutions across programming communities, and to cross-fertilize ideas. AADEBUG 2000 in Munich follows AADEBUG'93 in Linkoeping, Sweden; AADEBUG'95 in Saint Malo, France; AADEBUG'97 in Linkoeping, Sweden.",
        "published": "2000-10-30T15:54:03Z",
        "link": "http://arxiv.org/abs/cs/0010035v2",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Collecting Graphical Abstract Views of Mercury Program Executions",
        "authors": [
            "Erwan Jahier"
        ],
        "summary": "A program execution monitor is a program that collects and abstracts information about program executions. The \"collect\" operator is a high level, general purpose primitive which lets users implement their own monitors. \"Collect\" is built on top of the Mercury trace. In previous work, we have demonstrated how this operator can be used to efficiently collect various kinds of statistics about Mercury program executions. In this article we further demonstrate the expressive power and effectiveness of \"collect\" by providing more monitor examples. In particular, we show how to implement monitors that generate graphical abstractions of program executions such as proof trees, control flow graphs and dynamic call graphs. We show how those abstractions can be easily modified and adapted, since those monitors only require several dozens of lines of code. Those abstractions are intended to serve as front-ends of software visualization tools. Although \"collect\" is currently implemented on top of the Mercury trace, none of its underlying concepts depend of Mercury and it can be implemented on top of any tracer for any programming language.",
        "published": "2000-10-31T14:24:09Z",
        "link": "http://arxiv.org/abs/cs/0010038v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Non-intrusive on-the-fly data race detection using execution replay",
        "authors": [
            "Michiel Ronsse",
            "Koen De Bosschere"
        ],
        "summary": "This paper presents a practical solution for detecting data races in parallel programs. The solution consists of a combination of execution replay (RecPlay) with automatic on-the-fly data race detection. This combination enables us to perform the data race detection on an unaltered execution (almost no probe effect). Furthermore, the usage of multilevel bitmaps and snooped matrix clocks limits the amount of memory used. As the record phase of RecPlay is highly efficient, there is no need to switch it off, hereby eliminating the possibility of Heisenbugs because tracing can be left on all the time.",
        "published": "2000-11-06T13:03:11Z",
        "link": "http://arxiv.org/abs/cs/0011005v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Execution replay and debugging",
        "authors": [
            "Michiel Ronsse",
            "Koen De Bosschere",
            "Jacques Chassin de Kergommeaux"
        ],
        "summary": "As most parallel and distributed programs are internally non-deterministic -- consecutive runs with the same input might result in a different program flow -- vanilla cyclic debugging techniques as such are useless. In order to use cyclic debugging tools, we need a tool that records information about an execution so that it can be replayed for debugging. Because recording information interferes with the execution, we must limit the amount of information and keep the processing of the information fast. This paper contains a survey of existing execution replay techniques and tools.",
        "published": "2000-11-06T13:24:32Z",
        "link": "http://arxiv.org/abs/cs/0011006v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "A Lambda-Calculus with letrec, case, constructors and non-determinism",
        "authors": [
            "Manfred Schmidt-Schauß",
            "Michael Huber"
        ],
        "summary": "A non-deterministic call-by-need lambda-calculus \\calc with case, constructors, letrec and a (non-deterministic) erratic choice, based on rewriting rules is investigated. A standard reduction is defined as a variant of left-most outermost reduction. The semantics is defined by contextual equivalence of expressions instead of using $\\alpha\\beta(\\eta)$-equivalence. It is shown that several program transformations are correct, for example all (deterministic) rules of the calculus, and in addition the rules for garbage collection, removing indirections and unique copy.   This shows that the combination of a context lemma and a meta-rewriting on reductions using complete sets of commuting (forking, resp.) diagrams is a useful and successful method for providing a semantics of a functional programming language and proving correctness of program transformations.",
        "published": "2000-11-06T14:52:33Z",
        "link": "http://arxiv.org/abs/cs/0011008v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.SC",
            "F.4.1;D.3.2;I.2.2"
        ]
    },
    {
        "title": "Extension Language Automation of Embedded System Debugging",
        "authors": [
            "Dale Parson",
            "Bryan Schlieder",
            "Paul Beatty"
        ],
        "summary": "Embedded systems contain several layers of target processing abstraction. These layers include electronic circuit, binary machine code, mnemonic assembly code, and high-level procedural and object-oriented abstractions. Physical and temporal constraints and artifacts within physically embedded systems make it impossible for software engineers to operate at a single layer of processor abstraction. The Luxdbg embedded system debugger exposes these layers to debugger users, and it adds an additional layer, the extension language layer, that allows users to extend both the debugger and its target processor capabilities. Tcl is Luxdbg's extension language. Luxdbg users can apply Tcl to automate interactive debugging steps, to redirect and to interconnect target processor input-output facilities, to schedule multiple processor execution, to log and to react to target processing exceptions, and to automate target system testing. Inclusion of an extension language like Tcl in a debugger promises additional advantages for distributed debugging, where debuggers can pass extension language expressions across computer networks.",
        "published": "2000-11-06T23:16:22Z",
        "link": "http://arxiv.org/abs/cs/0011010v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Automatic Debugging Support for UML Designs",
        "authors": [
            "Johann Schumann"
        ],
        "summary": "Design of large software systems requires rigorous application of software engineering methods covering all phases of the software process. Debugging during the early design phases is extremely important, because late bug-fixes are expensive.   In this paper, we describe an approach which facilitates debugging of UML requirements and designs. The Unified Modeling Language (UML) is a set of notations for object-orient design of a software system. We have developed an algorithm which translates requirement specifications in the form of annotated sequence diagrams into structured statecharts. This algorithm detects conflicts between sequence diagrams and inconsistencies in the domain knowledge. After synthesizing statecharts from sequence diagrams, these statecharts usually are subject to manual modification and refinement. By using the ``backward'' direction of our synthesis algorithm, we are able to map modifications made to the statechart back into the requirements (sequence diagrams) and check for conflicts there. Fed back to the user conflicts detected by our algorithm are the basis for deductive-based debugging of requirements and domain theory in very early development stages. Our approach allows to generate explanations on why there is a conflict and which parts of the specifications are affected.",
        "published": "2000-11-13T23:30:33Z",
        "link": "http://arxiv.org/abs/cs/0011017v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "On-the-fly Query-Based Debugging with Examples",
        "authors": [
            "Raimondas Lencevicius"
        ],
        "summary": "Program errors are hard to find because of the cause-effect gap between the time when an error occurs and the time when the error becomes apparent to the programmer. Although debugging techniques such as conditional and data breakpoints help to find error causes in simple cases, they fail to effectively bridge the cause-effect gap in many situations. Query-based debuggers offer programmers an effective tool that provides instant error alert by continuously checking inter-object relationships while the debugged program is running. To enable the query-based debugger in the middle of program execution in a portable way, we propose efficient Java class file instrumentation and discuss alternative techniques. Although the on-the-fly debugger has a higher overhead than a dynamic query-based debugger, it offers additional interactive power and flexibility while maintaining complete portability. To speed up dynamic query evaluation, our debugger implemented in portable Java uses a combination of program instrumentation, load-time code generation, query optimization, and incremental reevaluation. This paper discusses on-the-fly debugging and demonstrates the query-based debugger application for debugging Java gas tank applet as well as SPECjvm98 suite applications.",
        "published": "2000-11-16T21:43:08Z",
        "link": "http://arxiv.org/abs/cs/0011021v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Apache web server execution tracing using Third Eye",
        "authors": [
            "Raimondas Lencevicius",
            "Alexander Ran",
            "Rahav Yairi"
        ],
        "summary": "Testing of modern software systems that integrate many components developed by different teams is a difficult task. Third Eye is a framework for tracing and validating software systems using application domain events. We use formal descriptions of the constraints between events to identify violations in execution traces. Third Eye is a flexible and modular framework that can be used in different products. We present the validation of the Apache Web Server access policy implementation. The results indicate that our tool is a helpful addition to software development infrastructure.",
        "published": "2000-11-16T22:13:47Z",
        "link": "http://arxiv.org/abs/cs/0011022v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Termination analysis of logic programs using acceptability with general   term orders",
        "authors": [
            "Alexander Serebrenik",
            "Danny De Schreye"
        ],
        "summary": "We present a new approach to termination analysis of logic programs. The essence of the approach is that we make use of general term-orderings (instead of level mappings), like it is done in transformational approaches to logic program termination analysis, but that we apply these orderings directly to the logic program and not to the term-rewrite system obtained through some transformation. We define some variants of acceptability, based on general term-orderings, and show how they are equivalent to LD-termination. We develop a demand driven, constraint-based approach to verify these acceptability-variants.   The advantage of the approach over standard acceptability is that in some cases, where complex level mappings are needed, fairly simple term-orderings may be easily generated. The advantage over transformational approaches is that it avoids the transformation step all together.",
        "published": "2000-11-17T12:45:13Z",
        "link": "http://arxiv.org/abs/cs/0011025v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.2.4"
        ]
    },
    {
        "title": "Extended Abstract - Model-Based Debugging of Java Programs",
        "authors": [
            "Cristinel Mateis",
            "Markus Stumptner",
            "Dominik Wieland",
            "Franz Wotawa"
        ],
        "summary": "Model-based reasoning is a central concept in current research into intelligent diagnostic systems. It is based on the assumption that sources of incorrect behavior in technical devices can be located and identified via the existence of a model describing the basic properties of components of a certain application domain. When actual data concerning the misbehavior of a system composed from such components is available, a domain-independent diagnosis engine can be used to infer which parts of the system contribute to the observed behavior. This paper describes the application of the model-based approach to the debugging of Java programs written in a subset of Java. We show how a simple dependency model can be derived from a program, demonstrate the use of the model for debugging and reducing the required user interactions, give a comparison of the functional dependency model with program slicing, and finally discuss some current research issues.",
        "published": "2000-11-20T14:48:38Z",
        "link": "http://arxiv.org/abs/cs/0011027v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Automatic Termination Analysis of Programs Containing Arithmetic   Predicates",
        "authors": [
            "Nachum Dershowitz",
            "Naomi Lindenstrauss",
            "Yehoshua Sagiv",
            "Alexander Serebrenik"
        ],
        "summary": "For logic programs with arithmetic predicates, showing termination is not easy, since the usual order for the integers is not well-founded. A new method, easily incorporated in the TermiLog system for automatic termination analysis, is presented for showing termination in this case.   The method consists of the following steps: First, a finite abstract domain for representing the range of integers is deduced automatically. Based on this abstraction, abstract interpretation is applied to the program. The result is a finite number of atoms abstracting answers to queries which are used to extend the technique of query-mapping pairs. For each query-mapping pair that is potentially non-terminating, a bounded (integer-valued) termination function is guessed. If traversing the pair decreases the value of the termination function, then termination is established. Simple functions often suffice for each query-mapping pair, and that gives our approach an edge over the classical approach of using a single termination function for all loops, which must inevitably be more complicated and harder to guess automatically. It is worth noting that the termination of McCarthy's 91 function can be shown automatically using our method.   In summary, the proposed approach is based on combining a finite abstraction of the integers with the technique of the query-mapping pairs, and is essentially capable of dividing a termination proof into several cases, such that a simple termination function suffices for each case. Consequently, the whole process of proving termination can be done automatically in the framework of TermiLog and similar systems.",
        "published": "2000-11-23T09:56:03Z",
        "link": "http://arxiv.org/abs/cs/0011036v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.2.4"
        ]
    },
    {
        "title": "Rewriting Calculus: Foundations and Applications",
        "authors": [
            "Horatiu Cirstea"
        ],
        "summary": "This thesis is devoted to the study of a calculus that describes the application of conditional rewriting rules and the obtained results at the same level of representation. We introduce the rewriting calculus, also called the rho-calculus, which generalizes the first order term rewriting and lambda-calculus, and makes possible the representation of the non-determinism. In our approach the abstraction operator as well as the application operator are objects of calculus. The result of a reduction in the rewriting calculus is either an empty set representing the application failure, or a singleton representing a deterministic result, or a set having several elements representing a not-deterministic choice of results.   In this thesis we concentrate on the properties of the rewriting calculus where a syntactic matching is used in order to bind the variables to their current values. We define evaluation strategies ensuring the confluence of the calculus and we show that these strategies become trivial for restrictions of the general rewriting calculus to simpler calculi like the lambda-calculus. The rewriting calculus is not terminating in the untyped case but the strong normalization is obtained for the simply typed calculus.   In the rewriting calculus extended with an operator allowing to test the application failure we define terms representing innermost and outermost normalizations with respect to a set of rewriting rules. By using these terms, we obtain a natural and concise description of the conditional rewriting. Finally, starting from the representation of the conditional rewriting rules, we show how the rewriting calculus can be used to give a semantics to ELAN, a language based on the application of rewriting rules controlled by strategies.",
        "published": "2000-11-28T15:01:07Z",
        "link": "http://arxiv.org/abs/cs/0011043v1",
        "categories": [
            "cs.SC",
            "cs.LO",
            "cs.PL",
            "I.1; D.1; D.3; F.4.0; F.4.1"
        ]
    },
    {
        "title": "Value Withdrawal Explanation in CSP",
        "authors": [
            "Gerard Ferrand",
            "Willy Lesaint",
            "Alexandre Tessier"
        ],
        "summary": "This work is devoted to constraint solving motivated by the debugging of constraint logic programs a la GNU-Prolog. The paper focuses only on the constraints. In this framework, constraint solving amounts to domain reduction. A computation is formalized by a chaotic iteration. The computed result is described as a closure. This model is well suited to the design of debugging notions and tools, for example failure explanations or error diagnosis. In this paper we detail an application of the model to an explanation of a value withdrawal in a domain. Some other works have already shown the interest of such a notion of explanation not only for failure analysis.",
        "published": "2000-12-11T13:04:18Z",
        "link": "http://arxiv.org/abs/cs/0012005v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Support for Debugging Automatically Parallelized Programs",
        "authors": [
            "Robert Hood",
            "Gabriele Jost"
        ],
        "summary": "We describe a system that simplifies the process of debugging programs produced by computer-aided parallelization tools. The system uses relative debugging techniques to compare serial and parallel executions in order to show where the computations begin to differ. If the original serial code is correct, errors due to parallelization will be isolated by the comparison.   One of the primary goals of the system is to minimize the effort required of the user. To that end, the debugging system uses information produced by the parallelization tool to drive the comparison process. In particular, the debugging system relies on the parallelization tool to provide information about where variables may have been modified and how arrays are distributed across multiple processes. User effort is also reduced through the use of dynamic instrumentation. This allows us to modify the program execution without changing the way the user builds the executable.   The use of dynamic instrumentation also permits us to compare the executions in a fine-grained fashion and only involve the debugger when a difference has been detected. This reduces the overhead of executing instrumentation.",
        "published": "2000-12-11T17:23:59Z",
        "link": "http://arxiv.org/abs/cs/0012006v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Kima - an Automated Error Correction System for Concurrent Logic   Programs",
        "authors": [
            "Yasuhiro Ajiro",
            "Kazunori Ueda"
        ],
        "summary": "We have implemented Kima, an automated error correction system for concurrent logic programs. Kima corrects near-misses such as wrong variable occurrences in the absence of explicit declarations of program properties. Strong moding/typing and constraint-based analysis are turning to play fundamental roles in debugging concurrent logic programs as well as in establishing the consistency of communication protocols and data types. Mode/type analysis of Moded Flat GHC is a constraint satisfaction problem with many simple mode/type constraints, and can be solved efficiently. We proposed a simple and efficient technique which, given a non-well-moded/typed program, diagnoses the ``reasons'' of inconsistency by finding minimal inconsistent subsets of mode/type constraints. Since each constraint keeps track of the symbol occurrence in the program, a minimal subset also tells possible sources of program errors. Kima realizes automated correction by replacing symbol occurrences around the possible sources and recalculating modes and types of the rewritten programs systematically. As long as bugs are near-misses, Kima proposes a rather small number of alternatives that include an intended program.",
        "published": "2000-12-13T08:48:06Z",
        "link": "http://arxiv.org/abs/cs/0012007v3",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "A General Framework for Automatic Termination Analysis of Logic Programs",
        "authors": [
            "Nachum Dershowitz",
            "Naomi Lindenstrauss",
            "Yehoshua Sagiv",
            "Alexander Serebrenik"
        ],
        "summary": "This paper describes a general framework for automatic termination analysis of logic programs, where we understand by ``termination'' the finitenes s of the LD-tree constructed for the program and a given query. A general property of mappings from a certain subset of the branches of an infinite LD-tree into a finite set is proved. From this result several termination theorems are derived, by using different finite sets. The first two are formulated for the predicate dependency and atom dependency graphs. Then a general result for the case of the query-mapping pairs relevant to a program is proved (cf. \\cite{Sagiv,Lindenstrauss:Sagiv}). The correctness of the {\\em TermiLog} system described in \\cite{Lindenstrauss:Sagiv:Serebrenik} follows from it. In this system it is not possible to prove termination for programs involving arithmetic predicates, since the usual order for the integers is not well-founded. A new method, which can be easily incorporated in {\\em TermiLog} or similar systems, is presented, which makes it possible to prove termination for programs involving arithmetic predicates. It is based on combining a finite abstraction of the integers with the technique of the query-mapping pairs, and is essentially capable of dividing a termination proof into several cases, such that a simple termination function suffices for each case. Finally several possible extensions are outlined.",
        "published": "2000-12-13T13:37:17Z",
        "link": "http://arxiv.org/abs/cs/0012008v1",
        "categories": [
            "cs.PL",
            "D.1.6"
        ]
    },
    {
        "title": "A brief overview of the MAD debugging activities",
        "authors": [
            "Dieter Kranzlmueller",
            "Christian Schaubschlaeger",
            "Jens Volkert"
        ],
        "summary": "Debugging parallel and distributed programs is a difficult activitiy due to the multiplicity of sequential bugs, the existence of malign effects like race conditions and deadlocks, and the huge amounts of data that have to be processed. These problems are addressed by the Monitoring And Debugging environment MAD, which offers debugging functionality based on a graphical representation of a program's execution. The target applications of MAD are parallel programs applying the standard Message-Passing Interface MPI, which is used extensively in the high-performance computing domain. The highlights of MAD are interactive inspection mechanisms including visualization of distributed arrays, the possibility to graphically place breakpoints, a mechanism for monitor overhead removal, and the evaluation of racing messages occuring due to nondeterminism in the code.",
        "published": "2000-12-16T20:32:00Z",
        "link": "http://arxiv.org/abs/cs/0012012v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Bezier Curves Intersection Using Relief Perspective",
        "authors": [
            "Radoslav Hlusek"
        ],
        "summary": "Presented paper describes the method for finding the intersection of class space rational Bezier curves. The problem curve/curve intersection belongs among basic geometric problems and the aim of this article is to describe the new technique to solve the problem using relief perspective and Bezier clipping.",
        "published": "2000-01-21T10:02:49Z",
        "link": "http://arxiv.org/abs/cs/0001017v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "F.2.2; I.3.5; J.6"
        ]
    },
    {
        "title": "PushPush is NP-hard in 2D",
        "authors": [
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Joseph O'Rourke"
        ],
        "summary": "We prove that a particular pushing-blocks puzzle is intractable in 2D, improving an earlier result that established intractability in 3D [OS99]. The puzzle, inspired by the game *PushPush*, consists of unit square blocks on an integer lattice. An agent may push blocks (but never pull them) in attempting to move between given start and goal positions. In the PushPush version, the agent can only push one block at a time, and moreover, each block, when pushed, slides the maximal extent of its free range. We prove this version is NP-hard in 2D by reduction from SAT.",
        "published": "2000-01-24T14:04:42Z",
        "link": "http://arxiv.org/abs/cs/0001019v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.m"
        ]
    },
    {
        "title": "Computational Geometry Column 38",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "Recent results on curve reconstruction are described.",
        "published": "2000-01-28T14:23:18Z",
        "link": "http://arxiv.org/abs/cs/0001025v1",
        "categories": [
            "cs.CG",
            "cs.CV",
            "F.2.2; I.5.3"
        ]
    },
    {
        "title": "Connectivity Compression for Irregular Quadrilateral Meshes",
        "authors": [
            "Davis King",
            "Jarek Rossignac",
            "Andrzej Szymczak"
        ],
        "summary": "Applications that require Internet access to remote 3D datasets are often limited by the storage costs of 3D models. Several compression methods are available to address these limits for objects represented by triangle meshes. Many CAD and VRML models, however, are represented as quadrilateral meshes or mixed triangle/quadrilateral meshes, and these models may also require compression. We present an algorithm for encoding the connectivity of such quadrilateral meshes, and we demonstrate that by preserving and exploiting the original quad structure, our approach achieves encodings 30 - 80% smaller than an approach based on randomly splitting quads into triangles. We present both a code with a proven worst-case cost of 3 bits per vertex (or 2.75 bits per vertex for meshes without valence-two vertices) and entropy-coding results for typical meshes ranging from 0.3 to 0.9 bits per vertex, depending on the regularity of the mesh. Our method may be implemented by a rule for a particular splitting of quads into triangles and by using the compression and decompression algorithms introduced in [Rossignac99] and [Rossignac&Szymczak99]. We also present extensions to the algorithm to compress meshes with holes and handles and meshes containing triangles and other polygons as well as quads.",
        "published": "2000-05-04T18:15:08Z",
        "link": "http://arxiv.org/abs/cs/0005005v1",
        "categories": [
            "cs.GR",
            "cs.CG",
            "cs.DS",
            "I.3.5; G2.2; E.4; J.6"
        ]
    },
    {
        "title": "On the Development of the Intersection of a Plane with a Polytope",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "Define a ``slice'' curve as the intersection of a plane with the surface of a polytope, i.e., a convex polyhedron in three dimensions. We prove that a slice curve develops on a plane without self-intersection. The key tool used is a generalization of Cauchy's arm lemma to permit nonconvex ``openings'' of a planar convex chain.",
        "published": "2000-06-26T14:56:46Z",
        "link": "http://arxiv.org/abs/cs/0006035v4",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Examples, Counterexamples, and Enumeration Results for Foldings and   Unfoldings between Polygons and Polytopes",
        "authors": [
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Anna Lubiw",
            "Joseph O'Rourke"
        ],
        "summary": "We investigate how to make the surface of a convex polyhedron (a polytope) by folding up a polygon and gluing its perimeter shut, and the reverse process of cutting open a polytope and unfolding it to a polygon. We explore basic enumeration questions in both directions: Given a polygon, how many foldings are there? Given a polytope, how many unfoldings are there to simple polygons? Throughout we give special attention to convex polygons, and to regular polygons. We show that every convex polygon folds to an infinite number of distinct polytopes, but that their number of combinatorially distinct gluings is polynomial. There are, however, simple polygons with an exponential number of distinct gluings.   In the reverse direction, we show that there are polytopes with an exponential number of distinct cuttings that lead to simple unfoldings. We establish necessary conditions for a polytope to have convex unfoldings, implying, for example, that among the Platonic solids, only the tetrahedron has a convex unfolding. We provide an inventory of the polytopes that may unfold to regular polygons, showing that, for n>6, there is essentially only one class of such polytopes.",
        "published": "2000-07-13T14:56:50Z",
        "link": "http://arxiv.org/abs/cs/0007019v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "PushPush and Push-1 are NP-hard in 2D",
        "authors": [
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Joseph O'Rourke"
        ],
        "summary": "We prove that two pushing-blocks puzzles are intractable in 2D. One of our constructions improves an earlier result that established intractability in 3D [OS99] for a puzzle inspired by the game PushPush. The second construction answers a question we raised in [DDO00] for a variant we call Push-1. Both puzzles consist of unit square blocks on an integer lattice; all blocks are movable. An agent may push blocks (but never pull them) in attempting to move between given start and goal positions. In the PushPush version, the agent can only push one block at a time, and moreover when a block is pushed it slides the maximal extent of its free range. In the Push-1 version, the agent can only push one block one square at a time, the minimal extent---one square. Both NP-hardness proofs are by reduction from SAT, and rely on a common construction.",
        "published": "2000-07-13T17:34:58Z",
        "link": "http://arxiv.org/abs/cs/0007021v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Computational Geometry Column 39",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "The resolution of a decades-old open problem is described: polygonal chains cannot lock in the plane.",
        "published": "2000-07-29T14:54:50Z",
        "link": "http://arxiv.org/abs/cs/0007042v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Flipturning polygons",
        "authors": [
            "Oswin Aichholzer",
            "Carmen Cortes",
            "Erik D. Demaine",
            "Vida Dujmovic",
            "Jeff Erickson",
            "Henk Meijer",
            "Mark Overmars",
            "Belen Palop",
            "Suneeta Ramaswami",
            "Godfried T. Toussaint"
        ],
        "summary": "A flipturn is an operation that transforms a nonconvex simple polygon into another simple polygon, by rotating a concavity 180 degrees around the midpoint of its bounding convex hull edge. Joss and Shannon proved in 1973 that a sequence of flipturns eventually transforms any simple polygon into a convex polygon. This paper describes several new results about such flipturn sequences. We show that any orthogonal polygon is convexified after at most n-5 arbitrary flipturns, or at most 5(n-4)/6 well-chosen flipturns, improving the previously best upper bound of (n-1)!/2. We also show that any simple polygon can be convexified by at most n^2-4n+1 flipturns, generalizing earlier results of Ahn et al. These bounds depend critically on how degenerate cases are handled; we carefully explore several possibilities. We describe how to maintain both a simple polygon and its convex hull in O(log^4 n) time per flipturn, using a data structure of size O(n). We show that although flipturn sequences for the same polygon can have very different lengths, the shape and position of the final convex polygon is the same for all sequences and can be computed in O(n log n) time. Finally, we demonstrate that finding the longest convexifying flipturn sequence of a simple polygon is NP-hard.",
        "published": "2000-08-16T04:32:21Z",
        "link": "http://arxiv.org/abs/cs/0008010v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "math.MG",
            "F.2.2; G.2"
        ]
    },
    {
        "title": "Pattern Matching for sets of segments",
        "authors": [
            "Alon Efrat",
            "Piotr Indyk",
            "Suresh Venkatasubramanian"
        ],
        "summary": "In this paper we present algorithms for a number of problems in geometric pattern matching where the input consist of a collections of segments in the plane. Our work consists of two main parts. In the first, we address problems and measures that relate to collections of orthogonal line segments in the plane. Such collections arise naturally from problems in mapping buildings and robot exploration.   We propose a new measure of segment similarity called a \\emph{coverage measure}, and present efficient algorithms for maximising this measure between sets of axis-parallel segments under translations. Our algorithms run in time $O(n^3\\polylog n)$ in the general case, and run in time $O(n^2\\polylog n)$ for the case when all segments are horizontal. In addition, we show that when restricted to translations that are only vertical, the Hausdorff distance between two sets of horizontal segments can be computed in time roughly $O(n^{3/2}{\\sl polylog}n)$. These algorithms form significant improvements over the general algorithm of Chew et al. that takes time $O(n^4 \\log^2 n)$. In the second part of this paper we address the problem of matching polygonal chains. We study the well known \\Frd, and present the first algorithm for computing the \\Frd under general translations. Our methods also yield algorithms for computing a generalization of the \\Fr distance, and we also present a simple approximation algorithm for the \\Frd that runs in time $O(n^2\\polylog n)$.",
        "published": "2000-09-19T20:09:39Z",
        "link": "http://arxiv.org/abs/cs/0009013v2",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Computing the Depth of a Flat",
        "authors": [
            "Marshall Bern",
            "David Eppstein"
        ],
        "summary": "We give algorithms for computing the regression depth of a k-flat for a set of n points in R^d. The running time is O(n^(d-2) + n log n) when 0 < k < d-1, faster than the best time bound for hyperplane regression or for data depth.",
        "published": "2000-09-25T19:28:49Z",
        "link": "http://arxiv.org/abs/cs/0009024v1",
        "categories": [
            "cs.CG",
            "F.2.2; g.3"
        ]
    },
    {
        "title": "Toward the Rectilinear Crossing Number of $K_n$: New Drawings, Upper   Bounds, and Asymptotics",
        "authors": [
            "Alex Brodsky",
            "Stephane Durocher",
            "Ellen Gethner"
        ],
        "summary": "Scheinerman and Wilf (1994) assert that `an important open problem in the study of graph embeddings is to determine the rectilinear crossing number of the complete graph K_n.' A rectilinear drawing of K_n is an arrangement of n vertices in the plane, every pair of which is connected by an edge that is a line segment. We assume that no three vertices are collinear, and that no three edges intersect in a point unless that point is an endpoint of all three. The rectilinear crossing number of K_n is the fewest number of edge crossings attainable over all rectilinear drawings of K_n.   For each n we construct a rectilinear drawing of K_n that has the fewest number of edge crossings and the best asymptotics known to date. Moreover, we give some alternative infinite families of drawings of K_n with good asymptotics. Finally, we mention some old and new open problems.",
        "published": "2000-09-28T16:33:06Z",
        "link": "http://arxiv.org/abs/cs/0009028v1",
        "categories": [
            "cs.DM",
            "cs.CG",
            "math.CO",
            "F.2.2;G.2.1;G.2.2"
        ]
    },
    {
        "title": "Internet Packet Filter Management and Rectangle Geometry",
        "authors": [
            "David Eppstein",
            "S. Muthukrishnan"
        ],
        "summary": "We consider rule sets for internet packet routing and filtering, where each rule consists of a range of source addresses, a range of destination addresses, a priority, and an action. A given packet should be handled by the action from the maximum priority rule that matches its source and destination. We describe new data structures for quickly finding the rule matching an incoming packet, in near-linear space, and a new algorithm for determining whether a rule set contains any conflicts, in time O(n^{3/2}).",
        "published": "2000-10-11T20:43:08Z",
        "link": "http://arxiv.org/abs/cs/0010018v1",
        "categories": [
            "cs.CG",
            "cs.NI",
            "F.2.2"
        ]
    },
    {
        "title": "Computational Geometry Column 40",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "It has recently been established by Below, De Loera, and Richter-Gebert that finding a minimum size (or even just a small) triangulation of a convex polyhedron is NP-complete. Their 3SAT-reduction proof is discussed.",
        "published": "2000-10-31T17:37:33Z",
        "link": "http://arxiv.org/abs/cs/0010039v1",
        "categories": [
            "cs.CG",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "When Can You Fold a Map?",
        "authors": [
            "Esther M. Arkin",
            "Michael A. Bender",
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Joseph S. B. Mitchell",
            "Saurabh Sethia",
            "Steven S. Skiena"
        ],
        "summary": "We explore the following problem: given a collection of creases on a piece of paper, each assigned a folding direction of mountain or valley, is there a flat folding by a sequence of simple folds? There are several models of simple folds; the simplest one-layer simple fold rotates a portion of paper about a crease in the paper by +-180 degrees. We first consider the analogous questions in one dimension lower -- bending a segment into a flat object -- which lead to interesting problems on strings. We develop efficient algorithms for the recognition of simply foldable 1D crease patterns, and reconstruction of a sequence of simple folds. Indeed, we prove that a 1D crease pattern is flat-foldable by any means precisely if it is by a sequence of one-layer simple folds.   Next we explore simple foldability in two dimensions, and find a surprising contrast: ``map'' folding and variants are polynomial, but slight generalizations are NP-complete. Specifically, we develop a linear-time algorithm for deciding foldability of an orthogonal crease pattern on a rectangular piece of paper, and prove that it is (weakly) NP-complete to decide foldability of (1) an orthogonal crease pattern on a orthogonal piece of paper, (2) a crease pattern of axis-parallel and diagonal (45-degree) creases on a square piece of paper, and (3) crease patterns without a mountain/valley assignment.",
        "published": "2000-11-20T08:55:49Z",
        "link": "http://arxiv.org/abs/cs/0011026v3",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Bezier Curves Intersection Using Relief Perspective",
        "authors": [
            "Radoslav Hlusek"
        ],
        "summary": "Presented paper describes the method for finding the intersection of class space rational Bezier curves. The problem curve/curve intersection belongs among basic geometric problems and the aim of this article is to describe the new technique to solve the problem using relief perspective and Bezier clipping.",
        "published": "2000-01-21T10:02:49Z",
        "link": "http://arxiv.org/abs/cs/0001017v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "F.2.2; I.3.5; J.6"
        ]
    },
    {
        "title": "Connectivity Compression for Irregular Quadrilateral Meshes",
        "authors": [
            "Davis King",
            "Jarek Rossignac",
            "Andrzej Szymczak"
        ],
        "summary": "Applications that require Internet access to remote 3D datasets are often limited by the storage costs of 3D models. Several compression methods are available to address these limits for objects represented by triangle meshes. Many CAD and VRML models, however, are represented as quadrilateral meshes or mixed triangle/quadrilateral meshes, and these models may also require compression. We present an algorithm for encoding the connectivity of such quadrilateral meshes, and we demonstrate that by preserving and exploiting the original quad structure, our approach achieves encodings 30 - 80% smaller than an approach based on randomly splitting quads into triangles. We present both a code with a proven worst-case cost of 3 bits per vertex (or 2.75 bits per vertex for meshes without valence-two vertices) and entropy-coding results for typical meshes ranging from 0.3 to 0.9 bits per vertex, depending on the regularity of the mesh. Our method may be implemented by a rule for a particular splitting of quads into triangles and by using the compression and decompression algorithms introduced in [Rossignac99] and [Rossignac&Szymczak99]. We also present extensions to the algorithm to compress meshes with holes and handles and meshes containing triangles and other polygons as well as quads.",
        "published": "2000-05-04T18:15:08Z",
        "link": "http://arxiv.org/abs/cs/0005005v1",
        "categories": [
            "cs.GR",
            "cs.CG",
            "cs.DS",
            "I.3.5; G2.2; E.4; J.6"
        ]
    },
    {
        "title": "Safe cooperative robot dynamics on graphs",
        "authors": [
            "Robert Ghrist",
            "Daniel Koditschek"
        ],
        "summary": "This paper initiates the use of vector fields to design, optimize, and implement reactive schedules for safe cooperative robot patterns on planar graphs. We consider Automated Guided Vehicles (AGV's) operating upon a predefined network of pathways. In contrast to the case of locally Euclidean configuration spaces, regularization of collisions is no longer a local procedure, and issues concerning the global topology of configuration spaces must be addressed. The focus of the present inquiry is the achievement of safe, efficient, cooperative patterns in the simplest nontrivial example (a pair of robots on a Y-network) by means of a state-event heirarchical controller.",
        "published": "2000-02-24T18:13:33Z",
        "link": "http://arxiv.org/abs/cs/0002014v1",
        "categories": [
            "cs.RO",
            "cs.AI",
            "I.2.9"
        ]
    },
    {
        "title": "Novelty Detection for Robot Neotaxis",
        "authors": [
            "Stephen Marsland",
            "Ulrich Nehmzow",
            "Jonathan Shapiro"
        ],
        "summary": "The ability of a robot to detect and respond to changes in its environment is potentially very useful, as it draws attention to new and potentially important features. We describe an algorithm for learning to filter out previously experienced stimuli to allow further concentration on novel features. The algorithm uses a model of habituation, a biological process which causes a decrement in response with repeated presentation. Experiments with a mobile robot are presented in which the robot detects the most novel stimulus and turns towards it (`neotaxis').",
        "published": "2000-06-02T11:32:17Z",
        "link": "http://arxiv.org/abs/cs/0006005v1",
        "categories": [
            "cs.RO",
            "cs.NE",
            "nlin.AO",
            "I.2.6"
        ]
    },
    {
        "title": "A Real-Time Novelty Detector for a Mobile Robot",
        "authors": [
            "Stephen Marsland",
            "Ulrich Nehmzow",
            "Jonathan Shapiro"
        ],
        "summary": "Recognising new or unusual features of an environment is an ability which is potentially very useful to a robot. This paper demonstrates an algorithm which achieves this task by learning an internal representation of `normality' from sonar scans taken as a robot explores the environment. This model of the environment is used to evaluate the novelty of each sonar scan presented to it with relation to the model. Stimuli which have not been seen before, and therefore have more novelty, are highlighted by the filter. The filter has the ability to forget about features which have been learned, so that stimuli which are seen only rarely recover their response over time. A number of robot experiments are presented which demonstrate the operation of the filter.",
        "published": "2000-06-02T12:00:15Z",
        "link": "http://arxiv.org/abs/cs/0006006v1",
        "categories": [
            "cs.RO",
            "cs.NE",
            "I.2.6"
        ]
    },
    {
        "title": "Novelty Detection on a Mobile Robot Using Habituation",
        "authors": [
            "Stephen Marsland",
            "Ulrich Nehmzow",
            "Jonathan Shapiro"
        ],
        "summary": "In this paper a novelty filter is introduced which allows a robot operating in an un structured environment to produce a self-organised model of its surroundings and to detect deviations from the learned model. The environment is perceived using the rob ot's 16 sonar sensors. The algorithm produces a novelty measure for each sensor scan relative to the model it has learned. This means that it highlights stimuli which h ave not been previously experienced. The novelty filter proposed uses a model of hab ituation. Habituation is a decrement in behavioural response when a stimulus is pre sented repeatedly. Robot experiments are presented which demonstrate the reliable o peration of the filter in a number of environments.",
        "published": "2000-06-02T12:33:13Z",
        "link": "http://arxiv.org/abs/cs/0006007v1",
        "categories": [
            "cs.RO",
            "cs.NE",
            "nlin.AO",
            "I.2.6"
        ]
    },
    {
        "title": "Multiagent Control of Self-reconfigurable Robots",
        "authors": [
            "Hristo Bojinov",
            "Arancha Casal",
            "Tad Hogg"
        ],
        "summary": "We demonstrate how multiagent systems provide useful control techniques for modular self-reconfigurable (metamorphic) robots. Such robots consist of many modules that can move relative to each other, thereby changing the overall shape of the robot to suit different tasks. Multiagent control is particularly well-suited for tasks involving uncertain and changing environments. We illustrate this approach through simulation experiments of Proteo, a metamorphic robot system currently under development.",
        "published": "2000-06-20T20:17:44Z",
        "link": "http://arxiv.org/abs/cs/0006030v2",
        "categories": [
            "cs.RO",
            "cs.DC",
            "cs.MA",
            "I.2.9; I.2.11; H.3.4"
        ]
    },
    {
        "title": "Design of an Electro-Hydraulic System Using Neuro-Fuzzy Techniques",
        "authors": [
            "P. J. Costa Branco",
            "J. A. Dente"
        ],
        "summary": "Increasing demands in performance and quality make drive systems fundamental parts in the progressive automation of industrial processes. Their conventional models become inappropriate and have limited scope if one requires a precise and fast performance. So, it is important to incorporate learning capabilities into drive systems in such a way that they improve their accuracy in realtime, becoming more autonomous agents with some degree of intelligence. To investigate this challenge, this chapter presents the development of a learning control system that uses neuro-fuzzy techniques in the design of a tracking controller to an experimental electro-hydraulic actuator. We begin the chapter by presenting the neuro-fuzzy modeling process of the actuator. This part surveys the learning algorithm, describes the laboratorial system, and presents the modeling steps as the choice of actuator representative variables, the acquisition of training and testing data sets, and the acquisition of the neuro-fuzzy inverse-model of the actuator. In the second part of the chapter, we use the extracted neuro-fuzzy model and its learning capabilities to design the actuator position controller based on the feedback-error-learning technique. Through a set of experimental results, we show the generalization properties of the controller, its learning capability in actualizing in realtime the initial neuro-fuzzy inverse-model, and its compensation action improving the electro-hydraulics tracking performance.",
        "published": "2000-09-30T11:47:42Z",
        "link": "http://arxiv.org/abs/cs/0010001v1",
        "categories": [
            "cs.RO",
            "cs.LG",
            "C.3; C.4; F.1.1; I.2.6; I.2.9; I.6.5; J.2, J.7"
        ]
    },
    {
        "title": "Torque Ripple Minimization in a Switched Reluctance Drive by Neuro-Fuzzy   Compensation",
        "authors": [
            "L. Henriques",
            "L. Rolim",
            "W. Suemitsu",
            "P. J. Costa Branco",
            "J. A. Dente"
        ],
        "summary": "Simple power electronic drive circuit and fault tolerance of converter are specific advantages of SRM drives, but excessive torque ripple has limited its use to special applications. It is well known that controlling the current shape adequately can minimize the torque ripple. This paper presents a new method for shaping the motor currents to minimize the torque ripple, using a neuro-fuzzy compensator. In the proposed method, a compensating signal is added to the output of a PI controller, in a current-regulated speed control loop. Numerical results are presented in this paper, with an analysis of the effects of changing the form of the membership function of the neuro-fuzzy compensator.",
        "published": "2000-09-30T15:31:16Z",
        "link": "http://arxiv.org/abs/cs/0010003v1",
        "categories": [
            "cs.RO",
            "cs.LG",
            "I.2.9; I.2.6"
        ]
    },
    {
        "title": "A Fuzzy Relational Identification Algorithm and Its Application to   Predict The Behaviour of a Motor Drive System",
        "authors": [
            "P. J. Costa Branco",
            "J. A. Dente"
        ],
        "summary": "Fuzzy relational identification builds a relational model describing systems behaviour by a nonlinear mapping between its variables. In this paper, we propose a new fuzzy relational algorithm based on simplified max-min relational equation. The algorithm presents an adaptation method applied to gravity-center of each fuzzy set based on error integral value between measured and predicted system output, and uses the concept of time-variant universe of discourses. The identification algorithm also includes a method to attenuate noise influence in extracted system relational model using a fuzzy filtering mechanism. The algorithm is applied to one-step forward prediction of a simulated and experimental motor drive system. The identified model has its input-output variables (stator-reference current and motor speed signal) treated as fuzzy sets, whereas the relations existing between them are described by means of a matrix R defining the relational model extracted by the algorithm. The results show the good potentialities of the algorithm in predict the behaviour of the system and attenuate through the fuzzy filtering method possible noise distortions in the relational model.",
        "published": "2000-09-30T15:42:55Z",
        "link": "http://arxiv.org/abs/cs/0010004v1",
        "categories": [
            "cs.RO",
            "cs.LG",
            "I.2.9; I.2.6"
        ]
    },
    {
        "title": "Computing and Comparing Semantics of Programs in Multi-valued Logics",
        "authors": [
            "Y. Loyer",
            "N. Spyratos",
            "D. Stamate"
        ],
        "summary": "The different semantics that can be assigned to a logic program correspond to different assumptions made concerning the atoms whose logical values cannot be inferred from the rules. Thus, the well founded semantics corresponds to the assumption that every such atom is false, while the Kripke-Kleene semantics corresponds to the assumption that every such atom is unknown. In this paper, we propose to unify and extend this assumption-based approach by introducing parameterized semantics for logic programs. The parameter holds the value that one assumes for all atoms whose logical values cannot be inferred from the rules. We work within multi-valued logic with bilattice structure, and we consider the class of logic programs defined by Fitting.   Following Fitting's approach, we define a simple operator that allows us to compute the parameterized semantics, and to compare and combine semantics obtained for different values of the parameter. The semantics proposed by Fitting corresponds to the value false. We also show that our approach captures and extends the usual semantics of conventional logic programs thereby unifying their computation.",
        "published": "2000-02-18T13:54:03Z",
        "link": "http://arxiv.org/abs/cs/0002013v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "H.0;I.2.3"
        ]
    },
    {
        "title": "Materialized View Selection and Maintenance Using Multi-Query   Optimization",
        "authors": [
            "Hoshi Mistry",
            "Prasan Roy",
            "Krithi Ramamritham",
            "S. Sudarshan"
        ],
        "summary": "Because the presence of views enhances query performance, materialized views are increasingly being supported by commercial database/data warehouse systems. Whenever the data warehouse is updated, the materialized views must also be updated. However, whereas the amount of data entering a warehouse, the query loads, and the need to obtain up-to-date responses are all increasing, the time window available for making the warehouse up-to-date is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.   In this paper, we show how to find an efficient plan for maintenance of a {\\em set} of views, by exploiting common subexpressions between different view maintenance expressions. These common subexpressions may be materialized temporarily during view maintenance. Our algorithms also choose subexpressions/indices to be materialized permanently (and maintained along with other materialized views), to speed up view maintenance. While there has been much work on view maintenance in the past, our novel contributions lie in exploiting a recently developed framework for multiquery optimization to efficiently find good view maintenance plans as above. In addition to faster view maintenance, our algorithms can also be used to efficiently select materialized views to speed up workloads containing queries.",
        "published": "2000-03-02T08:05:24Z",
        "link": "http://arxiv.org/abs/cs/0003006v1",
        "categories": [
            "cs.DB",
            "H.2.4;H.2.7"
        ]
    },
    {
        "title": "Don't Trash your Intermediate Results, Cache 'em",
        "authors": [
            "Prasan Roy",
            "Krithi Ramamritham",
            "S. Seshadri",
            "Pradeep Shenoy",
            "S. Sudarshan"
        ],
        "summary": "In data warehouse and data mart systems, queries often take a long time to execute due to their complex nature. Query response times can be greatly improved by caching final/intermediate results of previous queries, and using them to answer later queries. In this paper we describe a caching system called Exchequer which incorporates several novel features including optimization aware cache maintenance and the use of a cache aware optimizer. In contrast, in existing work, the module that makes cost-benefit decisions is part of the cache manager and works independent of the optimizer which essentially reconsiders these decisions while finding the best plan for a query. In our work, the optimizer takes the decisions for the cache manager. Furthermore, existing approaches are either restricted to cube (slice/point) queries, or cache just the query results. On the other hand, our work is extens ible and in fact presents a data-model independent framework and algorithm. Our experimental results attest to the efficacy of our cache management techniques and show that over a wide range of parameters (a) Exchequer's query response times are lower by more than 30% compared to the best performing competitor, and (b) Exchequer can deliver the same response time as its competitor with just one tenth of the cache size.",
        "published": "2000-03-02T08:15:21Z",
        "link": "http://arxiv.org/abs/cs/0003005v1",
        "categories": [
            "cs.DB",
            "H.2.4;H.2.7"
        ]
    },
    {
        "title": "Automatic Classification of Text Databases through Query Probing",
        "authors": [
            "Panagiotis Ipeirotis",
            "Luis Gravano",
            "Mehran Sahami"
        ],
        "summary": "Many text databases on the web are \"hidden\" behind search interfaces, and their documents are only accessible through querying. Search engines typically ignore the contents of such search-only databases. Recently, Yahoo-like directories have started to manually organize these databases into categories that users can browse to find these valuable resources. We propose a novel strategy to automate the classification of search-only text databases. Our technique starts by training a rule-based document classifier, and then uses the classifier's rules to generate probing queries. The queries are sent to the text databases, which are then classified based on the number of matches that they produce for each query. We report some initial exploratory experiments that show that our approach is promising to automatically characterize the contents of text databases accessible on the web.",
        "published": "2000-03-09T04:01:22Z",
        "link": "http://arxiv.org/abs/cs/0003043v1",
        "categories": [
            "cs.DB",
            "cs.IR",
            "H.3"
        ]
    },
    {
        "title": "Data storage issues in lattice QCD calculations",
        "authors": [
            "Craig McNeile"
        ],
        "summary": "I describe some of the data management issues in lattice Quantum Chromodynamics calculations. I focus on the experience of the UKQCD collaboration. I describe an attempt to use a relational database to store part of the data produced by a lattice QCD calculation.",
        "published": "2000-03-14T15:36:30Z",
        "link": "http://arxiv.org/abs/hep-lat/0003009v1",
        "categories": [
            "hep-lat",
            "cs.DB"
        ]
    },
    {
        "title": "Deciding first-order properties of locally tree-decomposable structures",
        "authors": [
            "Markus Frick",
            "Martin Grohe"
        ],
        "summary": "We introduce the concept of a class of graphs, or more generally, relational structures, being locally tree-decomposable. There are numerous examples of locally tree-decomposable classes, among them the class of planar graphs and all classes of bounded valence or of bounded tree-width. We also consider a slightly more general concept of a class of structures having bounded local tree-width.   We show that for each property P of structures that is definable in first-order logic and for each locally tree-decomposable class C of graphs, there is a linear time algorithm deciding whether a given structure A in C has property P. For classes C of bounded local tree-width, we show that for every k\\ge 1 there is an algorithm that solves the same problem in time O(n^{1+(1/k)}) (where n is the cardinality of the input structure).",
        "published": "2000-04-17T16:14:08Z",
        "link": "http://arxiv.org/abs/cs/0004007v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DB",
            "F.2.2; G.2.2; H.2.4; F.1.3"
        ]
    },
    {
        "title": "Design and Evaluation of Mechanisms for a Multicomputer Object Store",
        "authors": [
            "Lex Weaver"
        ],
        "summary": "Multicomputers have traditionally been viewed as powerful compute engines. It is from this perspective that they have been applied to various problems in order to achieve significant performance gains. There are many applications for which this compute intensive approach is only a partial solution. CAD, virtual reality, simulation, document management and analysis all require timely access to large amounts of data. This thesis investigates the use of the object store paradigm to harness the large distributed memories found on multicomputers. The design, implementation, and evaluation of a distributed object server on the Fujitsu AP1000 is described. The performance of the distributed object server under example applications, mainly physical simulation problems, is used to evaluate solutions to the problems of client space recovery, object migration, and coherence maintenance.   The distributed object server follows the client-server model, allows object replication, and uses binary semaphores as a concurrency control measure. Instrumentation of the server under these applications supports several conclusions: client space recovery should be dynamically controlled by the application, predictively prefetching object replicas yields benefits in restricted circumstances, object migration by storage unit (segment) is not generally suitable where there are many objects per storage unit, and binary semaphores are an expensive concurrency control measure in this environment.",
        "published": "2000-04-18T17:29:06Z",
        "link": "http://arxiv.org/abs/cs/0004010v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H.3.4; H.2.4"
        ]
    },
    {
        "title": "Towards a query language for annotation graphs",
        "authors": [
            "Steven Bird",
            "Peter Buneman",
            "Wang-Chiew Tan"
        ],
        "summary": "The multidimensional, heterogeneous, and temporal nature of speech databases raises interesting challenges for representation and query. Recently, annotation graphs have been proposed as a general-purpose representational framework for speech databases. Typical queries on annotation graphs require path expressions similar to those used in semistructured query languages. However, the underlying model is rather different from the customary graph models for semistructured data: the graph is acyclic and unrooted, and both temporal and inclusion relationships are important. We develop a query language and describe optimization techniques for an underlying relational representation.",
        "published": "2000-07-13T18:40:15Z",
        "link": "http://arxiv.org/abs/cs/0007023v1",
        "categories": [
            "cs.CL",
            "cs.DB",
            "E.1; E.2; H.2.1; H.2.3; H.2.8; H.3.1; H.3.3; I.2.7"
        ]
    },
    {
        "title": "Integrating E-Commerce and Data Mining: Architecture and Challenges",
        "authors": [
            "Suhail Ansari",
            "Ron Kohavi",
            "Llew Mason",
            "Zijian Zheng"
        ],
        "summary": "We show that the e-commerce domain can provide all the right ingredients for successful data mining and claim that it is a killer domain for data mining. We describe an integrated architecture, based on our expe-rience at Blue Martini Software, for supporting this integration. The architecture can dramatically reduce the pre-processing, cleaning, and data understanding effort often documented to take 80% of the time in knowledge discovery projects. We emphasize the need for data collection at the application server layer (not the web server) in order to support logging of data and metadata that is essential to the discovery process. We describe the data transformation bridges required from the transaction processing systems and customer event streams (e.g., clickstreams) to the data warehouse. We detail the mining workbench, which needs to provide multiple views of the data through reporting, data mining algorithms, visualization, and OLAP. We con-clude with a set of challenges.",
        "published": "2000-07-14T00:33:12Z",
        "link": "http://arxiv.org/abs/cs/0007026v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DB",
            "I.2.6;H.2.8"
        ]
    },
    {
        "title": "Managing Periodically Updated Data in Relational Databases: A Stochastic   Modeling Approach",
        "authors": [
            "Avigdor Gal",
            "Jonathan Eckstein"
        ],
        "summary": "Recent trends in information management involve the periodic transcription of data onto secondary devices in a networked environment, and the proper scheduling of these transcriptions is critical for efficient data management. To assist in the scheduling process, we are interested in modeling the reduction of consistency over time between a relation and its replica, termed obsolescence of data. The modeling is based on techniques from the field of stochastic processes, and provides several stochastic models for content evolution in the base relations of a database, taking referential integrity constraints into account. These models are general enough to accommodate most of the common scenarios in databases, including batch insertions and life spans both with and without memory. As an initial \"proof of concept\" of the applicability of our approach, we validate the insertion portion of our model framework via experiments with real data feeds. We also discuss a set of transcription protocols which make use of the proposed stochastic model.",
        "published": "2000-07-31T20:15:08Z",
        "link": "http://arxiv.org/abs/cs/0007044v2",
        "categories": [
            "cs.DB",
            "H.2.4"
        ]
    },
    {
        "title": "Data Mining to Measure and Improve the Success of Web Sites",
        "authors": [
            "Myra Spiliopoulou",
            "Carsten Pohle"
        ],
        "summary": "For many companies, competitiveness in e-commerce requires a successful presence on the web. Web sites are used to establish the company's image, to promote and sell goods and to provide customer support. The success of a web site affects and reflects directly the success of the company in the electronic market. In this study, we propose a methodology to improve the ``success'' of web sites, based on the exploitation of navigation pattern discovery. In particular, we present a theory, in which success is modelled on the basis of the navigation behaviour of the site's users. We then exploit WUM, a navigation pattern discovery miner, to study how the success of a site is reflected in the users' behaviour. With WUM we measure the success of a site's components and obtain concrete indications of how the site should be improved. We report on our first experiments with an online catalog, the success of which we have studied. Our mining analysis has shown very promising results, on the basis of which the site is currently undergoing concrete improvements.",
        "published": "2000-08-15T15:20:18Z",
        "link": "http://arxiv.org/abs/cs/0008009v1",
        "categories": [
            "cs.LG",
            "cs.DB",
            "I.2.6; H.2.8"
        ]
    },
    {
        "title": "Science User Scenarios for a Virtual Observatory Design Reference   Mission: Science Requirements for Data Mining",
        "authors": [
            "Kirk D. Borne"
        ],
        "summary": "The knowledge discovery potential of the new large astronomical databases is vast. When these are used in conjunction with the rich legacy data archives, the opportunities for scientific discovery multiply rapidly. A Virtual Observatory (VO) framework will enable transparent and efficient access, search, retrieval, and visualization of data across multiple data repositories, which are generally heterogeneous and distributed. Aspects of data mining that apply to a variety of science user scenarios with a VO are reviewed. The development of a VO should address the data mining needs of various astronomical research constituencies. By way of example, two user scenarios are presented which invoke applications and linkages of data across the catalog and image domains in order to address specific astrophysics research problems. These illustrate a subset of the desired capabilities and power of the VO, and as such they represent potential components of a VO Design Reference Mission.",
        "published": "2000-08-19T17:54:01Z",
        "link": "http://arxiv.org/abs/astro-ph/0008307v1",
        "categories": [
            "astro-ph",
            "cs.DB",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Applications of Data Mining to Electronic Commerce",
        "authors": [
            "Ron Kohavi",
            "Foster Provost"
        ],
        "summary": "Electronic commerce is emerging as the killer domain for data mining technology.   The following are five desiderata for success. Seldom are they they all present in one data mining application.   1. Data with rich descriptions. For example, wide customer records with many potentially useful fields allow data mining algorithms to search beyond obvious correlations.   2. A large volume of data. The large model spaces corresponding to rich data demand many training instances to build reliable models.   3. Controlled and reliable data collection. Manual data entry and integration from legacy systems both are notoriously problematic; fully automated collection is considerably better.   4. The ability to evaluate results. Substantial, demonstrable return on investment can be very convincing.   5. Ease of integration with existing processes. Even if pilot studies show potential benefit, deploying automated solutions to previously manual processes is rife with pitfalls. Building a system to take advantage of the mined knowledge can be a substantial undertaking. Furthermore, one often must deal with social and political issues involved in the automation of a previously manual business process.",
        "published": "2000-10-02T12:16:17Z",
        "link": "http://arxiv.org/abs/cs/0010006v1",
        "categories": [
            "cs.LG",
            "cs.DB",
            "I.2.6;H.2.8"
        ]
    },
    {
        "title": "A Formal Framework for Linguistic Annotation (revised version)",
        "authors": [
            "Steven Bird",
            "Mark Liberman"
        ],
        "summary": "`Linguistic annotation' covers any descriptive or analytic notations applied to raw language data. The basic data may be in the form of time functions - audio, video and/or physiological recordings - or it may be textual. The added notations may include transcriptions of all sorts (from phonetic features to discourse structures), part-of-speech and sense tagging, syntactic analysis, `named entity' identification, co-reference annotation, and so on. While there are several ongoing efforts to provide formats and tools for such annotations and to publish annotated linguistic databases, the lack of widely accepted standards is becoming a critical problem. Proposed standards, to the extent they exist, have focused on file formats. This paper focuses instead on the logical structure of linguistic annotations. We survey a wide variety of existing annotation formats and demonstrate a common conceptual core, the annotation graph. This provides a formal framework for constructing, maintaining and searching linguistic annotations, while remaining consistent with many alternative data structures and file formats.",
        "published": "2000-10-26T17:42:30Z",
        "link": "http://arxiv.org/abs/cs/0010033v1",
        "categories": [
            "cs.CL",
            "cs.DB",
            "cs.DS",
            "A.1; E.2; H.2.1; H.3.3; H.3.7; I.2.7"
        ]
    },
    {
        "title": "Data Mining in Astronomical Databases",
        "authors": [
            "Kirk D. Borne"
        ],
        "summary": "A Virtual Observatory (VO) will enable transparent and efficient access, search, retrieval, and visualization of data across multiple data repositories, which are generally heterogeneous and distributed. Aspects of data mining that apply to a variety of science user scenarios with a VO are reviewed.",
        "published": "2000-10-27T21:46:42Z",
        "link": "http://arxiv.org/abs/astro-ph/0010583v2",
        "categories": [
            "astro-ph",
            "cs.DB",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Algorithms for Rewriting Aggregate Queries Using Views",
        "authors": [
            "Sara Cohen",
            "Werner Nutt",
            "Alexander Serebrenik"
        ],
        "summary": "Queries involving aggregation are typical in database applications. One of the main ideas to optimize the execution of an aggregate query is to reuse results of previously answered queries. This leads to the problem of rewriting aggregate queries using views. Due to a lack of theory, algorithms for this problem were rather ad-hoc. They were sound, but were not proven to be complete.   Recently we have given syntactic characterizations for the equivalence of aggregate queries and applied them to decide when there exist rewritings. However, these decision procedures do not lend themselves immediately to an implementation. In this paper, we present practical algorithms for rewriting queries with $\\COUNT$ and $\\SUM$. Our algorithms are sound. They are also complete for important cases. Our techniques can be used to improve well-known procedures for rewriting non-aggregate queries. These procedures can then be adapted to obtain algorithms for rewriting queries with $\\MIN$ and $\\MAX$. The algorithms presented are a basis for realizing optimizers that rewrite queries using views.",
        "published": "2000-11-17T12:16:43Z",
        "link": "http://arxiv.org/abs/cs/0011024v1",
        "categories": [
            "cs.DB",
            "H.2.3"
        ]
    },
    {
        "title": "Web Mining Research: A Survey",
        "authors": [
            "Raymond Kosala",
            "Hendrik Blockeel"
        ],
        "summary": "With the huge amount of information available online, the World Wide Web is a fertile area for data mining research. The Web mining research is at the cross road of research from several research communities, such as database, information retrieval, and within AI, especially the sub-areas of machine learning and natural language processing. However, there is a lot of confusions when comparing research efforts from different point of views. In this paper, we survey the research in the area of Web mining, point out some confusions regarded the usage of the term Web mining and suggest three Web mining categories. Then we situate some of the research with respect to these three categories. We also explore the connection between the Web mining categories and the related agent paradigm. For the survey, we focus on representation issues, on the process, on the learning algorithm, and on the application of the recent works as the criteria. We conclude the paper with some research issues.",
        "published": "2000-11-22T09:41:53Z",
        "link": "http://arxiv.org/abs/cs/0011033v1",
        "categories": [
            "cs.LG",
            "cs.DB",
            "I.2.6; H.2.8"
        ]
    },
    {
        "title": "EquiX---A Search and Query Language for XML",
        "authors": [
            "Sara Cohen",
            "Yaron Kanza",
            "Yakov Kogan",
            "Werner Nutt",
            "Yehoshua Sagiv",
            "Alexander Serebrenik"
        ],
        "summary": "EquiX is a search language for XML that combines the power of querying with the simplicity of searching. Requirements for such languages are discussed and it is shown that EquiX meets the necessary criteria. Both a graphical abstract syntax and a formal concrete syntax are presented for EquiX queries. In addition, the semantics is defined and an evaluation algorithm is presented. The evaluation algorithm is polynomial under combined complexity.   EquiX combines pattern matching, quantification and logical expressions to query both the data and meta-data of XML documents. The result of a query in EquiX is a set of XML documents. A DTD describing the result documents is derived automatically from the query.",
        "published": "2000-11-27T09:15:08Z",
        "link": "http://arxiv.org/abs/cs/0011041v1",
        "categories": [
            "cs.DB",
            "H.2.3"
        ]
    },
    {
        "title": "A Theory of Universal Artificial Intelligence based on Algorithmic   Complexity",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameterless theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline for a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning, how the AIXI model can formally solve them. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXI-tl, which is still effectively more intelligent than any other time t and space l bounded agent. The computation time of AIXI-tl is of the order tx2^l. Other discussed topics are formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches.",
        "published": "2000-04-03T06:16:16Z",
        "link": "http://arxiv.org/abs/cs/0004001v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "I.2; F.1.3; E.4"
        ]
    },
    {
        "title": "An information-spectrum approach to joint source-channel coding",
        "authors": [
            "Te Sun Han"
        ],
        "summary": "Given a general source $\\sV=\\{V^n\\}\\noi$ with {\\em countably infinite} source alphabet and a general channel $\\sW=\\{W^n\\}\\noi$ with arbitrary {\\em abstract} channel input and output alphabets, we study the joint source-channel coding problem from the information-spectrum point of view. First, we generalize Feinstein's lemma (direct part) and Verd\\'u-Han's lemma (converse part) so as to be applicable to the general joint source-channel coding problem. Based on these lemmas, we establish a sufficient condition as well as a necessary condition for the source $\\sV$ to be reliably transmissible over the channel $\\sW$ with asymptotically vanishing probability of error. It is shown that our sufficient condition coincides with the sufficient condition derived by Vembu, Verd\\'u and Steinberg, whereas our necessary condition is much stronger than the necessary condition derived by them. Actually, our necessary condition coincide with our sufficient condition if we disregard some asymptotically vanishing terms appearing in those conditions. Also, it is shown that {\\em Separation Theorem} in the generalized sense always holds. In addition, we demonstrate a sufficient condition as well as a necessary condition for the $\\vep$-transmissibility ($0\\le \\vep <1$). Finally, the separation theorem of the traditional standard form is shown to hold for the class of sources and channels that satisfy the (semi-) strong converse property.",
        "published": "2000-05-06T09:41:32Z",
        "link": "http://arxiv.org/abs/math/0005058v2",
        "categories": [
            "math.PR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Connections between Linear Systems and Convolutional Codes",
        "authors": [
            "Joachim Rosenthal"
        ],
        "summary": "The article reviews different definitions for a convolutional code which can be found in the literature. The algebraic differences between the definitions are worked out in detail. It is shown that bi-infinite support systems are dual to finite-support systems under Pontryagin duality. In this duality the dual of a controllable system is observable and vice versa. Uncontrollability can occur only if there are bi-infinite support trajectories in the behavior, so finite and half-infinite-support systems must be controllable. Unobservability can occur only if there are finite support trajectories in the behavior, so bi-infinite and half-infinite-support systems must be observable. It is shown that the different definitions for convolutional codes are equivalent if one restricts attention to controllable and observable codes.",
        "published": "2000-05-30T15:39:44Z",
        "link": "http://arxiv.org/abs/math/0005281v1",
        "categories": [
            "math.OC",
            "cs.IT",
            "math.IT",
            "37B10, 93B25, 94B10"
        ]
    },
    {
        "title": "Algorithmic Statistics",
        "authors": [
            "Peter Gacs",
            "John Tromp",
            "Paul Vitanyi"
        ],
        "summary": "While Kolmogorov complexity is the accepted absolute measure of information content of an individual finite object, a similarly absolute notion is needed for the relation between an individual data sample and an individual model summarizing the information in the data, for example, a finite set (or probability distribution) where the data sample typically came from. The statistical theory based on such relations between individual objects can be called algorithmic statistics, in contrast to classical statistical theory that deals with relations between probabilistic ensembles. We develop the algorithmic theory of statistic, sufficient statistic, and minimal sufficient statistic. This theory is based on two-part codes consisting of the code for the statistic (the model summarizing the regularity, the meaningful information, in the data) and the model-to-data code. In contrast to the situation in probabilistic statistical theory, the algorithmic relation of (minimal) sufficiency is an absolute relation between the individual model and the individual data sample. We distinguish implicit and explicit descriptions of the models. We give characterizations of algorithmic (Kolmogorov) minimal sufficient statistic for all data samples for both description modes--in the explicit mode under some constraints. We also strengthen and elaborate earlier results on the ``Kolmogorov structure function'' and ``absolutely non-stochastic objects''--those rare objects for which the simplest models that summarize their relevant information (minimal sufficient statistics) are at least as complex as the objects themselves. We demonstrate a close relation between the probabilistic notions and the algorithmic ones.",
        "published": "2000-06-30T17:19:06Z",
        "link": "http://arxiv.org/abs/math/0006233v3",
        "categories": [
            "math.ST",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.PR",
            "physics.data-an",
            "stat.TH",
            "62B05, 62B10, 68Q32, 68Q30, 60AXX, 68T04"
        ]
    },
    {
        "title": "Critical Behavior in Lossy Source Coding",
        "authors": [
            "Amir Dembo",
            "Ioannis Kontoyiannis"
        ],
        "summary": "The following critical phenomenon was recently discovered. When a memoryless source is compressed using a variable-length fixed-distortion code, the fastest convergence rate of the (pointwise) compression ratio to the optimal $R(D)$ bits/symbol is either $O(\\sqrt{n})$ or $O(\\log n)$. We show it is always $O(\\sqrt{n})$, except for discrete, uniformly distributed sources.",
        "published": "2000-09-01T18:43:46Z",
        "link": "http://arxiv.org/abs/math/0009018v1",
        "categories": [
            "math.PR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Fields, towers of function fields meeting asymptotic bounds, and basis   constructions for algebraic-geometric codes",
        "authors": [
            "Vinay Deolalikar"
        ],
        "summary": "In this work, we use the notion of ``symmetry'' of functions for an extension $K/L$ of finite fields to produce extensions of a function field $F/K$ in which almost all places of degree one split completely. Then we introduce the notion of ``quasi-symmetry'' of functions for $K/L$, and demonstrate its use in producing extensions of $F/K$ in which all places of degree one split completely. Using these techniques, we are able to restrict the ramification either to one chosen rational place, or entirely to non-rational places. We then apply these methods to the related problem of building asymptotically good towers of function fields. We construct examples of towers of function fields in which all rational places split completely throughout the tower. We construct Abelian towers with this property also.   Furthermore, all of the above are done explicitly, ie., we give generators for the extensions, and equations that they satisfy.   We also construct an integral basis for a set of places in a tower of function fields meeting the Drinfeld-Vladut bound using the discriminant of the tower localized at each place. Thus we are able to obtain a basis for a collection of functions that contains the set of regular functions in this tower. Regular functions are of interest in the theory of error-correcting codes as they lead to an explicit description of the code associated to the tower by providing the code's generator matrix.",
        "published": "2000-10-30T21:44:32Z",
        "link": "http://arxiv.org/abs/math/0010307v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.IT",
            "14G05, 14H05, 14G50"
        ]
    },
    {
        "title": "Towards a Universal Theory of Artificial Intelligence based on   Algorithmic Probability and Sequential Decision Theory",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown distribution. We unify both theories and give strong arguments that the resulting universal AIXI model behaves optimal in any computable environment. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXI^tl, which is still superior to any other time t and space l bounded agent. The computation time of AIXI^tl is of the order t x 2^l.",
        "published": "2000-12-16T09:38:13Z",
        "link": "http://arxiv.org/abs/cs/0012011v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "I.2; I.2.3; I.2.6; I.2.8; F.1.3; F.2"
        ]
    },
    {
        "title": "Quantum error-correcting codes associated with graphs",
        "authors": [
            "D. Schlingemann",
            "R. F. Werner"
        ],
        "summary": "We present a construction scheme for quantum error correcting codes. The basic ingredients are a graph and a finite abelian group, from which the code can explicitly be obtained. We prove necessary and sufficient conditions for the graph such that the resulting code corrects a certain number of errors. This allows a simple verification of the 1-error correcting property of fivefold codes in any dimension. As new examples we construct a large class of codes saturating the singleton bound, as well as a tenfold code detecting 3 errors.",
        "published": "2000-12-20T16:20:53Z",
        "link": "http://arxiv.org/abs/quant-ph/0012111v1",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math-ph",
            "math.IT",
            "math.MP"
        ]
    },
    {
        "title": "Numerical Replication of Computer Simulations: Some Pitfalls and How To   Avoid Them",
        "authors": [
            "Theodore C. Belding"
        ],
        "summary": "A computer simulation, such as a genetic algorithm, that uses IEEE standard floating-point arithmetic may not produce exactly the same results in two different runs, even if it is rerun on the same computer with the same input and random number seeds. Researchers should not simply assume that the results from one run replicate those from another but should verify this by actually comparing the data. However, researchers who are aware of this pitfall can reliably replicate simulations, in practice. This paper discusses the problem and suggests solutions.",
        "published": "2000-01-26T03:08:08Z",
        "link": "http://arxiv.org/abs/nlin/0001057v1",
        "categories": [
            "nlin.AO",
            "cs.NE"
        ]
    },
    {
        "title": "Pattern Discovery and Computational Mechanics",
        "authors": [
            "Cosma Rohilla Shalizi",
            "James P. Crutchfield"
        ],
        "summary": "Computational mechanics is a method for discovering, describing and quantifying patterns, using tools from statistical physics. It constructs optimal, minimal models of stochastic processes and their underlying causal structures. These models tell us about the intrinsic computation embedded within a process---how it stores and transforms information. Here we summarize the mathematics of computational mechanics, especially recent optimality and uniqueness results. We also expound the principles and motivations underlying computational mechanics, emphasizing its connections to the minimum description length principle, PAC theory, and other aspects of machine learning.",
        "published": "2000-01-29T01:23:54Z",
        "link": "http://arxiv.org/abs/cs/0001027v1",
        "categories": [
            "cs.LG",
            "cs.NE",
            "I.2.6; F.1.3; G.3; H.1.1"
        ]
    },
    {
        "title": "Evolution of differentiated expression patterns in digital organisms",
        "authors": [
            "Charles Ofria",
            "Christoph Adami",
            "Travis C. Collier",
            "Grace K. Hsu"
        ],
        "summary": "We investigate the evolutionary processes behind the development and optimization of multiple threads of execution in digital organisms using the avida platform, a software package that implements Darwinian evolution on populations of self-replicating computer programs. The system is seeded with a linearly executed ancestor capable only of reproducing its own genome, whereas its underlying language has the capacity for multiple threads of execution (i.e., simultaneous expression of sections of the genome.) We witness the evolution to multi-threaded organisms and track the development of distinct expression patterns. Additionally, we examine both the evolvability of multi-threaded organisms and the level of thread differentiation as a function of environmental complexity, and find that differentiation is more pronounced in complex environments.",
        "published": "2000-02-29T23:51:21Z",
        "link": "http://arxiv.org/abs/physics/0002054v1",
        "categories": [
            "physics.bio-ph",
            "cs.NE",
            "q-bio.PE"
        ]
    },
    {
        "title": "Novelty Detection for Robot Neotaxis",
        "authors": [
            "Stephen Marsland",
            "Ulrich Nehmzow",
            "Jonathan Shapiro"
        ],
        "summary": "The ability of a robot to detect and respond to changes in its environment is potentially very useful, as it draws attention to new and potentially important features. We describe an algorithm for learning to filter out previously experienced stimuli to allow further concentration on novel features. The algorithm uses a model of habituation, a biological process which causes a decrement in response with repeated presentation. Experiments with a mobile robot are presented in which the robot detects the most novel stimulus and turns towards it (`neotaxis').",
        "published": "2000-06-02T11:32:17Z",
        "link": "http://arxiv.org/abs/cs/0006005v1",
        "categories": [
            "cs.RO",
            "cs.NE",
            "nlin.AO",
            "I.2.6"
        ]
    },
    {
        "title": "A Real-Time Novelty Detector for a Mobile Robot",
        "authors": [
            "Stephen Marsland",
            "Ulrich Nehmzow",
            "Jonathan Shapiro"
        ],
        "summary": "Recognising new or unusual features of an environment is an ability which is potentially very useful to a robot. This paper demonstrates an algorithm which achieves this task by learning an internal representation of `normality' from sonar scans taken as a robot explores the environment. This model of the environment is used to evaluate the novelty of each sonar scan presented to it with relation to the model. Stimuli which have not been seen before, and therefore have more novelty, are highlighted by the filter. The filter has the ability to forget about features which have been learned, so that stimuli which are seen only rarely recover their response over time. A number of robot experiments are presented which demonstrate the operation of the filter.",
        "published": "2000-06-02T12:00:15Z",
        "link": "http://arxiv.org/abs/cs/0006006v1",
        "categories": [
            "cs.RO",
            "cs.NE",
            "I.2.6"
        ]
    },
    {
        "title": "Novelty Detection on a Mobile Robot Using Habituation",
        "authors": [
            "Stephen Marsland",
            "Ulrich Nehmzow",
            "Jonathan Shapiro"
        ],
        "summary": "In this paper a novelty filter is introduced which allows a robot operating in an un structured environment to produce a self-organised model of its surroundings and to detect deviations from the learned model. The environment is perceived using the rob ot's 16 sonar sensors. The algorithm produces a novelty measure for each sensor scan relative to the model it has learned. This means that it highlights stimuli which h ave not been previously experienced. The novelty filter proposed uses a model of hab ituation. Habituation is a decrement in behavioural response when a stimulus is pre sented repeatedly. Robot experiments are presented which demonstrate the reliable o peration of the filter in a number of environments.",
        "published": "2000-06-02T12:33:13Z",
        "link": "http://arxiv.org/abs/cs/0006007v1",
        "categories": [
            "cs.RO",
            "cs.NE",
            "nlin.AO",
            "I.2.6"
        ]
    },
    {
        "title": "Orthogonal Least Squares Algorithm for the Approximation of a Map and   its Derivatives with a RBF Network",
        "authors": [
            "Carlo Drioli",
            "Davide Rocchesso"
        ],
        "summary": "Radial Basis Function Networks (RBFNs) are used primarily to solve curve-fitting problems and for non-linear system modeling. Several algorithms are known for the approximation of a non-linear curve from a sparse data set by means of RBFNs. However, there are no procedures that permit to define constrains on the derivatives of the curve. In this paper, the Orthogonal Least Squares algorithm for the identification of RBFNs is modified to provide the approximation of a non-linear 1-in 1-out map along with its derivatives, given a set of training data. The interest on the derivatives of non-linear functions concerns many identification and control tasks where the study of system stability and robustness is addressed. The effectiveness of the proposed algorithm is demonstrated by a study on the stability of a single loop feedback system.",
        "published": "2000-06-28T10:17:43Z",
        "link": "http://arxiv.org/abs/cs/0006039v1",
        "categories": [
            "cs.NE",
            "cs.SD",
            "I.2.6"
        ]
    },
    {
        "title": "Noise Effects in Fuzzy Modelling Systems",
        "authors": [
            "P. J. Costa Branco",
            "J. A. Dente"
        ],
        "summary": "Noise is source of ambiguity for fuzzy systems. Although being an important aspect, the effects of noise in fuzzy modeling have been little investigated. This paper presents a set of tests using three well-known fuzzy modeling algorithms. These evaluate perturbations in the extracted rule-bases caused by noise polluting the learning data, and the corresponding deformations in each learned functional relation. We present results to show: 1) how these fuzzy modeling systems deal with noise; 2) how the established fuzzy model structure influences noise sensitivity of each algorithm; and 3) whose characteristics of the learning algorithms are relevant to noise attenuation.",
        "published": "2000-09-30T14:37:23Z",
        "link": "http://arxiv.org/abs/cs/0010002v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "I.2.6; I.5.1; I.5.2"
        ]
    },
    {
        "title": "Optimization with Extremal Dynamics",
        "authors": [
            "S. Boettcher",
            "A. G. Percus"
        ],
        "summary": "We explore a new general-purpose heuristic for finding high-quality solutions to hard optimization problems. The method, called extremal optimization, is inspired by self-organized criticality, a concept introduced to describe emergent complexity in physical systems. Extremal optimization successively replaces extremely undesirable variables of a single sub-optimal solution with new, random ones. Large fluctuations ensue, that efficiently explore many local optima. With only one adjustable parameter, the heuristic's performance has proven competitive with more elaborate methods, especially near phase transitions which are believed to coincide with the hardest instances. We use extremal optimization to elucidate the phase transition in the 3-coloring problem, and we provide independent confirmation of previously reported extrapolations for the ground-state energy of +-J spin glasses in d=3 and 4.",
        "published": "2000-10-23T05:13:47Z",
        "link": "http://arxiv.org/abs/cond-mat/0010337v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.NE",
            "math.OC"
        ]
    },
    {
        "title": "Creativity and Delusions: A Neurocomputational Approach",
        "authors": [
            "Daniele Quintella Mendes",
            "Luis Alfredo Vidal de Carvalho"
        ],
        "summary": "Thinking is one of the most interesting mental processes. Its complexity is sometimes simplified and its different manifestations are classified into normal and abnormal, like the delusional and disorganized thought or the creative one. The boundaries between these facets of thinking are fuzzy causing difficulties in medical, academic, and philosophical discussions. Considering the dopaminergic signal-to-noise neuronal modulation in the central nervous system, and the existence of semantic maps in human brain, a self-organizing neural network model was developed to unify the different thought processes into a single neurocomputational substrate. Simulations were performed varying the dopaminergic modulation and observing the different patterns that emerged at the semantic map. Assuming that the thought process is the total pattern elicited at the output layer of the neural network, the model shows how the normal and abnormal thinking are generated and that there are no borders between their different manifestations. Actually, a continuum of different qualitative reasoning, ranging from delusion to disorganization of thought, and passing through the normal and the creative thinking, seems to be more plausible. The model is far from explaining the complexities of human thinking but, at least, it seems to be a good metaphorical and unifying view of the many facets of this phenomenon usually studied in separated settings.",
        "published": "2000-12-22T12:00:07Z",
        "link": "http://arxiv.org/abs/cs/0012020v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.5.1"
        ]
    },
    {
        "title": "Fully Sequential and Distributed Dynamic Algorithms for Minimum Spanning   Trees",
        "authors": [
            "Pradosh Kumar Mohapatra"
        ],
        "summary": "In this paper, we present a fully-dynamic distributed algorithm for maintaining a minimum spanning tree on general graphs with positive real edge weights. The goal of a dynamic MST algorithm is to update efficiently the minimum spanning tree after dynamic changes like edge weight changes, rather than having to recompute it from scatch each time. The first part of the paper surveys various algorithms available today both in sequential and distributed environments to solve static MST problem. We also present some of the efficient sequential algorithms for computing dynamic MST like the Frederickson's algorithm and Eppstein's sparsification technique. Lastly we present our new sequential and distributed algorithms for dynamic MST problem. To our knowledge, this is the first of the distributed algorithms for computing dynamic MSTs.",
        "published": "2000-02-08T17:51:50Z",
        "link": "http://arxiv.org/abs/cs/0002005v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "G.2.2;F.2.2"
        ]
    },
    {
        "title": "MOO: A Methodology for Online Optimization through Mining the Offline   Optimum",
        "authors": [
            "Jason W. H. Lee",
            "Y. C. Tay",
            "Anthony K. H. Tung"
        ],
        "summary": "Ports, warehouses and courier services have to decide online how an arriving task is to be served in order that cost is minimized (or profit maximized). These operators have a wealth of historical data on task assignments; can these data be mined for knowledge or rules that can help the decision-making?   MOO is a novel application of data mining to online optimization. The idea is to mine (logged) expert decisions or the offline optimum for rules that can be used for online decisions. It requires little knowledge about the task distribution and cost structure, and is applicable to a wide range of problems.   This paper presents a feasibility study of the methodology for the well-known k-server problem. Experiments with synthetic data show that optimization can be recast as classification of the optimum decisions; the resulting heuristic can achieve the optimum for strong request patterns, consistently outperforms other heuristics for weak patterns, and is robust despite changes in cost model.",
        "published": "2000-03-22T12:49:38Z",
        "link": "http://arxiv.org/abs/cs/0003072v1",
        "categories": [
            "cs.DS",
            "cs.LG",
            "F.2.2;H.2.8;F.1.2"
        ]
    },
    {
        "title": "About the finding of independent vertices of a graph",
        "authors": [
            "Anatoly D. Plotnikov"
        ],
        "summary": "We examine the Maximum Independent Set Problem in an undirected graph. The main result is that this problem can be considered as the solving the same problem in a subclass of the weighted normal twin-orthogonal graphs. The problem is formulated which is dual to the problem above. It is shown that, for trivial twin-orthogonal graphs, any of its maximal independent set is also maximum one.",
        "published": "2000-03-24T23:31:24Z",
        "link": "http://arxiv.org/abs/cs/0003078v1",
        "categories": [
            "cs.DS",
            "F.2.2;G.2.1;G.2.2"
        ]
    },
    {
        "title": "Deciding first-order properties of locally tree-decomposable structures",
        "authors": [
            "Markus Frick",
            "Martin Grohe"
        ],
        "summary": "We introduce the concept of a class of graphs, or more generally, relational structures, being locally tree-decomposable. There are numerous examples of locally tree-decomposable classes, among them the class of planar graphs and all classes of bounded valence or of bounded tree-width. We also consider a slightly more general concept of a class of structures having bounded local tree-width.   We show that for each property P of structures that is definable in first-order logic and for each locally tree-decomposable class C of graphs, there is a linear time algorithm deciding whether a given structure A in C has property P. For classes C of bounded local tree-width, we show that for every k\\ge 1 there is an algorithm that solves the same problem in time O(n^{1+(1/k)}) (where n is the cardinality of the input structure).",
        "published": "2000-04-17T16:14:08Z",
        "link": "http://arxiv.org/abs/cs/0004007v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DB",
            "F.2.2; G.2.2; H.2.4; F.1.3"
        ]
    },
    {
        "title": "Connectivity Compression for Irregular Quadrilateral Meshes",
        "authors": [
            "Davis King",
            "Jarek Rossignac",
            "Andrzej Szymczak"
        ],
        "summary": "Applications that require Internet access to remote 3D datasets are often limited by the storage costs of 3D models. Several compression methods are available to address these limits for objects represented by triangle meshes. Many CAD and VRML models, however, are represented as quadrilateral meshes or mixed triangle/quadrilateral meshes, and these models may also require compression. We present an algorithm for encoding the connectivity of such quadrilateral meshes, and we demonstrate that by preserving and exploiting the original quad structure, our approach achieves encodings 30 - 80% smaller than an approach based on randomly splitting quads into triangles. We present both a code with a proven worst-case cost of 3 bits per vertex (or 2.75 bits per vertex for meshes without valence-two vertices) and entropy-coding results for typical meshes ranging from 0.3 to 0.9 bits per vertex, depending on the regularity of the mesh. Our method may be implemented by a rule for a particular splitting of quads into triangles and by using the compression and decompression algorithms introduced in [Rossignac99] and [Rossignac&Szymczak99]. We also present extensions to the algorithm to compress meshes with holes and handles and meshes containing triangles and other polygons as well as quads.",
        "published": "2000-05-04T18:15:08Z",
        "link": "http://arxiv.org/abs/cs/0005005v1",
        "categories": [
            "cs.GR",
            "cs.CG",
            "cs.DS",
            "I.3.5; G2.2; E.4; J.6"
        ]
    },
    {
        "title": "PSPACE Reasoning for Graded Modal Logics",
        "authors": [
            "Stephan Tobies"
        ],
        "summary": "We present a PSPACE algorithm that decides satisfiability of the graded modal logic Gr(K_R)---a natural extension of propositional modal logic K_R by counting expressions---which plays an important role in the area of knowledge representation. The algorithm employs a tableaux approach and is the first known algorithm which meets the lower bound for the complexity of the problem. Thus, we exactly fix the complexity of the problem and refute an ExpTime-hardness conjecture. We extend the results to the logic Gr(K_(R \\cap I)), which augments Gr(K_R) with inverse relations and intersection of accessibility relations. This establishes a kind of ``theoretical benchmark'' that all algorithmic approaches can be measured against.",
        "published": "2000-05-08T14:51:58Z",
        "link": "http://arxiv.org/abs/cs/0005009v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "F.4.1"
        ]
    },
    {
        "title": "Smoothness and decay properties of the limiting Quicksort density   function",
        "authors": [
            "James Allen Fill",
            "Svante Janson"
        ],
        "summary": "Using Fourier analysis, we prove that the limiting distribution of the standardized random number of comparisons used by Quicksort to sort an array of n numbers has an everywhere positive and infinitely differentiable density f, and that each derivative f^{(k)} enjoys superpolynomial decay at plus and minus infinity. In particular, each f^{(k)} is bounded. Our method is sufficiently computational to prove, for example, that f is bounded by 16.",
        "published": "2000-05-23T19:24:59Z",
        "link": "http://arxiv.org/abs/math/0005235v1",
        "categories": [
            "math.PR",
            "cs.DS",
            "68W40 (primary), 68P10, 60E05, 60E10 (secondary)"
        ]
    },
    {
        "title": "A characterization of the set of fixed points of the Quicksort   transformation",
        "authors": [
            "James Allen Fill",
            "Svante Janson"
        ],
        "summary": "The limiting distribution \\mu of the normalized number of key comparisons required by the Quicksort sorting algorithm is known to be the unique fixed point of a certain distributional transformation T -- unique, that is, subject to the constraints of zero mean and finite variance. We show that a distribution is a fixed point of T if and only if it is the convolution of \\mu with a Cauchy distribution of arbitrary center and scale. In particular, therefore, \\mu is the unique fixed point of T having zero mean.",
        "published": "2000-05-23T20:02:57Z",
        "link": "http://arxiv.org/abs/math/0005236v1",
        "categories": [
            "math.PR",
            "cs.DS",
            "68W40 (primary), 60E05, 60E10, 68P10 (secondary)"
        ]
    },
    {
        "title": "Perfect simulation from the Quicksort limit distribution",
        "authors": [
            "Luc Devroye",
            "James Allen Fill",
            "Ralph Neininger"
        ],
        "summary": "The weak limit of the normalized number of comparisons needed by the Quicksort algorithm to sort n randomly permuted items is known to be determined implicitly by a distributional fixed-point equation. We give an algorithm for perfect random variate generation from this distribution.",
        "published": "2000-05-23T20:21:34Z",
        "link": "http://arxiv.org/abs/math/0005237v2",
        "categories": [
            "math.PR",
            "cs.DS",
            "65C10 (primary), 65C05, 68U20, 11K45 (secondary)"
        ]
    },
    {
        "title": "A Bayesian Reflection on Surfaces",
        "authors": [
            "David R. Wolf"
        ],
        "summary": "The topic of this paper is a novel Bayesian continuous-basis field representation and inference framework. Within this paper several problems are solved: The maximally informative inference of continuous-basis fields, that is where the basis for the field is itself a continuous object and not representable in a finite manner; the tradeoff between accuracy of representation in terms of information learned, and memory or storage capacity in bits; the approximation of probability distributions so that a maximal amount of information about the object being inferred is preserved; an information theoretic justification for multigrid methodology. The maximally informative field inference framework is described in full generality and denoted the Generalized Kalman Filter. The Generalized Kalman Filter allows the update of field knowledge from previous knowledge at any scale, and new data, to new knowledge at any other scale. An application example instance, the inference of continuous surfaces from measurements (for example, camera image data), is presented.",
        "published": "2000-05-26T20:24:48Z",
        "link": "http://arxiv.org/abs/cs/0005027v1",
        "categories": [
            "cs.CV",
            "cs.DS",
            "cs.LG",
            "math.PR",
            "nlin.AO",
            "physics.data-an",
            "G.3;I.2.4;I.2.6;I.2.10;I.4.1;I.4.4;I.4.5;I.4.10"
        ]
    },
    {
        "title": "Computational Complexity and Phase Transitions",
        "authors": [
            "Gabriel Istrate"
        ],
        "summary": "Phase transitions in combinatorial problems have recently been shown to be useful in locating \"hard\" instances of combinatorial problems. The connection between computational complexity and the existence of phase transitions has been addressed in Statistical Mechanics and Artificial Intelligence, but not studied rigorously.   We take a step in this direction by investigating the existence of sharp thresholds for the class of generalized satisfiability problems defined by Schaefer. In the case when all constraints are clauses we give a complete characterization of such problems that have a sharp threshold.   While NP-completeness does not imply (even in this restricted case) the existence of a sharp threshold, it \"almost implies\" this, since clausal generalized satisfiability problems that lack a sharp threshold are either   1. polynomial time solvable, or   2. predicted, with success probability lower bounded by some positive constant by across all the probability range, by a single, trivial procedure.",
        "published": "2000-05-31T04:05:58Z",
        "link": "http://arxiv.org/abs/cs/0005032v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Systematic Performance Evaluation of Multipoint Protocols",
        "authors": [
            "Ahmed Helmy",
            "Sandeep Gupta",
            "Deborah Estrin",
            "Alberto Cerpa",
            "Yan Yu"
        ],
        "summary": "The advent of multipoint (multicast-based) applications and the growth and complexity of the Internet has complicated network protocol design and evaluation.   In this paper, we present a method for automatic synthesis of worst and best case scenarios for multipoint protocol performance evaluation.   Our method uses a fault-oriented test generation (FOTG) algorithm for searching the protocol and system state space to synthesize these scenarios. The algorithm is based on a global finite state machine (FSM) model. We extend the algorithm with timing semantics to handle end-to-end delays and address performance criteria. We introduce the notion of a virtual LAN to represent delays of the underlying multicast distribution tree.   As a case study, we use our method to evaluate variants of the timer suppression mechanism, used in various multipoint protocols, with respect to two performance criteria: overhead of response messages and response time. Simulation results for reliable multicast protocols show that our method provides a scalable way for synthesizing worst-case scenarios automatically. We expect our method to serve as a model for applying systematic scenario generation to other multipoint protocols.",
        "published": "2000-06-17T18:16:39Z",
        "link": "http://arxiv.org/abs/cs/0006029v1",
        "categories": [
            "cs.NI",
            "cs.DS",
            "C.2.2; C.2.0"
        ]
    },
    {
        "title": "Correlation over Decomposed Signals: A Non-Linear Approach to Fast and   Effective Sequences Comparison",
        "authors": [
            "Luciano da Fontoura Costa"
        ],
        "summary": "A novel non-linear approach to fast and effective comparison of sequences is presented, compared to the traditional cross-correlation operator, and illustrated with respect to DNA sequences.",
        "published": "2000-06-28T18:34:14Z",
        "link": "http://arxiv.org/abs/cs/0006040v1",
        "categories": [
            "cs.CV",
            "cs.DS",
            "q-bio",
            "I.5.4; F.2.2; I.5.4; J.3"
        ]
    },
    {
        "title": "3-Coloring in Time O(1.3289^n)",
        "authors": [
            "Richard Beigel",
            "David Eppstein"
        ],
        "summary": "We consider worst case time bounds for NP-complete problems including 3-SAT, 3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a constraint satisfaction (CSP) formulation of these problems. 3-SAT is equivalent to (2,3)-CSP while the other problems above are special cases of (3,2)-CSP; there is also a natural duality transformation from (a,b)-CSP to (b,a)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the time bounds for solving the other problems listed above. Our techniques involve a mixture of Davis-Putnam-style backtracking with more sophisticated matching and network flow based ideas.",
        "published": "2000-06-30T22:04:04Z",
        "link": "http://arxiv.org/abs/cs/0006046v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Systematic Testing of Multicast Routing Protocols: Analysis of Forward   and Backward Search Techniques",
        "authors": [
            "Ahmed Helmy",
            "Deborah Estrin",
            "Sandeep Gupta"
        ],
        "summary": "In this paper, we present a new methodology for developing systematic and automatic test generation algorithms for multipoint protocols. These algorithms attempt to synthesize network topologies and sequences of events that stress the protocol's correctness or performance. This problem can be viewed as a domain-specific search problem that suffers from the state space explosion problem. One goal of this work is to circumvent the state space explosion problem utilizing knowledge of network and fault modeling, and multipoint protocols. The two approaches investigated in this study are based on forward and backward search techniques. We use an extended finite state machine (FSM) model of the protocol. The first algorithm uses forward search to perform reduced reachability analysis. Using domain-specific information for multicast routing over LANs, the algorithm complexity is reduced from exponential to polynomial in the number of routers. This approach, however, does not fully automate topology synthesis. The second algorithm, the fault-oriented test generation, uses backward search for topology synthesis and uses backtracking to generate event sequences instead of searching forward from initial states. Using these algorithms, we have conducted studies for correctness of the multicast routing protocol PIM. We propose to extend these algorithms to study end-to-end multipoint protocols using a virtual LAN that represents delays of the underlying multicast distribution tree.",
        "published": "2000-07-04T21:55:25Z",
        "link": "http://arxiv.org/abs/cs/0007005v1",
        "categories": [
            "cs.NI",
            "cs.DS",
            "C.2.2, C.2.0"
        ]
    },
    {
        "title": "DISCO: An object-oriented system for music composition and sound design",
        "authors": [
            "Hans G. Kaper",
            "Sever Tipei",
            "Jeff M. Wright"
        ],
        "summary": "This paper describes an object-oriented approach to music composition and sound design. The approach unifies the processes of music making and instrument building by using similar logic, objects, and procedures. The composition modules use an abstract representation of musical data, which can be easily mapped onto different synthesis languages or a traditionally notated score. An abstract base class is used to derive classes on different time scales. Objects can be related to act across time scales, as well as across an entire piece, and relationships between similar objects can replicate traditional music operations or introduce new ones. The DISCO (Digital Instrument for Sonification and Composition) system is an open-ended work in progress.",
        "published": "2000-07-05T18:20:51Z",
        "link": "http://arxiv.org/abs/cs/0007006v1",
        "categories": [
            "cs.SD",
            "cs.DS",
            "cs.SE",
            "H.5.5"
        ]
    },
    {
        "title": "Dimension-Dependent behavior in the satisfability of random k-Horn   formulae",
        "authors": [
            "Gabriel Istrate"
        ],
        "summary": "We determine the asymptotical satisfiability probability of a random at-most-k-Horn formula, via a probabilistic analysis of a simple version, called PUR, of positive unit resolution. We show that for k=k(n)->oo the problem can be ``reduced'' to the case k(n)=n, that was solved in cs.DS/9912001. On the other hand, in the case k= a constant the behavior of PUR is modeled by a simple queuing chain, leading to a closed-form solution when k=2. Our analysis predicts an ``easy-hard-easy'' pattern in this latter case. Under a rescaled parameter, the graphs of satisfaction probability corresponding to finite values of k converge to the one for the uniform case, a ``dimension-dependent behavior'' similar to the one found experimentally by Kirkpatrick and Selman (Science'94) for k-SAT. The phenomenon is qualitatively explained by a threshold property for the number of iterations of PUR makes on random satisfiable Horn formulas.",
        "published": "2000-07-18T21:50:04Z",
        "link": "http://arxiv.org/abs/cs/0007029v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Min-Max Fine Heaps",
        "authors": [
            "Suman Kumar Nath",
            "Rezaul Alam Chowdhury",
            "M. Kaykobad"
        ],
        "summary": "In this paper we present a new data structure for double ended priority queue, called min-max fine heap, which combines the techniques used in fine heap and traditional min-max heap. The standard operations on this proposed structure are also presented, and their analysis indicates that the new structure outperforms the traditional one.",
        "published": "2000-07-31T00:13:17Z",
        "link": "http://arxiv.org/abs/cs/0007043v1",
        "categories": [
            "cs.DS",
            "E.1"
        ]
    },
    {
        "title": "Structure of some sand pile model",
        "authors": [
            "M. Latapy",
            "R. Mantaci",
            "M. Morvan",
            "H. D. Phan"
        ],
        "summary": "SPM (Sand Pile Model) is a simple discrete dynamical system used in physics to represent granular objects. It is deeply related to integer partitions, and many other combinatorics problems, such as tilings or rewriting systems. The evolution of the system started with n stacked grains generates a lattice, denoted by SPM(n). We study here the structure of this lattice. We first explain how it can be constructed, by showing its strong self-similarity property. Then, we define SPM(infini), a natural extension of SPM when one starts with an infinite number of grains. Again, we give an efficient construction algorithm and a coding of this lattice using a self-similar tree. The two approaches give different recursive formulae for the cardinal of SPM(n), where no closed formula have ever been found.",
        "published": "2000-08-02T14:37:34Z",
        "link": "http://arxiv.org/abs/cs/0008002v1",
        "categories": [
            "cs.DM",
            "cond-mat.stat-mech",
            "cs.DS",
            "math.CO",
            "G.2.1; J.2"
        ]
    },
    {
        "title": "All Pairs Shortest Paths using Bridging Sets and Rectangular Matrix   Multiplication",
        "authors": [
            "Uri Zwick"
        ],
        "summary": "We present two new algorithms for solving the {\\em All Pairs Shortest Paths} (APSP) problem for weighted directed graphs. Both algorithms use fast matrix multiplication algorithms.   The first algorithm solves the APSP problem for weighted directed graphs in which the edge weights are integers of small absolute value in $\\Ot(n^{2+\\mu})$ time, where $\\mu$ satisfies the equation $\\omega(1,\\mu,1)=1+2\\mu$ and $\\omega(1,\\mu,1)$ is the exponent of the multiplication of an $n\\times n^\\mu$ matrix by an $n^\\mu \\times n$ matrix. Currently, the best available bounds on $\\omega(1,\\mu,1)$, obtained by Coppersmith, imply that $\\mu<0.575$. The running time of our algorithm is therefore $O(n^{2.575})$. Our algorithm improves on the $\\Ot(n^{(3+\\omega)/2})$ time algorithm, where $\\omega=\\omega(1,1,1)<2.376$ is the usual exponent of matrix multiplication, obtained by Alon, Galil and Margalit, whose running time is only known to be $O(n^{2.688})$.   The second algorithm solves the APSP problem {\\em almost} exactly for directed graphs with {\\em arbitrary} non-negative real weights. The algorithm runs in $\\Ot((n^\\omega/\\eps)\\log (W/\\eps))$ time, where $\\eps>0$ is an error parameter and W is the largest edge weight in the graph, after the edge weights are scaled so that the smallest non-zero edge weight in the graph is 1. It returns estimates of all the distances in the graph with a stretch of at most $1+\\eps$. Corresponding paths can also be found efficiently.",
        "published": "2000-08-16T20:39:45Z",
        "link": "http://arxiv.org/abs/cs/0008011v1",
        "categories": [
            "cs.DS",
            "F.2.2;G.2.2;G.3"
        ]
    },
    {
        "title": "Fast Approximation of Centrality",
        "authors": [
            "David Eppstein",
            "Joseph Wang"
        ],
        "summary": "Social studies researchers use graphs to model group activities in social networks. An important property in this context is the centrality of a vertex: the inverse of the average distance to each other vertex. We describe a randomized approximation algorithm for centrality in weighted graphs. For graphs exhibiting the small world phenomenon, our method estimates the centrality of all vertices with high probability within a (1+epsilon) factor in near-linear time.",
        "published": "2000-09-13T18:39:05Z",
        "link": "http://arxiv.org/abs/cs/0009005v1",
        "categories": [
            "cs.DS",
            "cond-mat.dis-nn",
            "cs.SI",
            "F.2.2"
        ]
    },
    {
        "title": "Improved Algorithms for 3-Coloring, 3-Edge-Coloring, and Constraint   Satisfaction",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We consider worst case time bounds for NP-complete problems including 3-SAT, 3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a constraint satisfaction (CSP) formulation of these problems; 3-SAT is equivalent to (2,3)-CSP while the other problems above are special cases of (3,2)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the time bounds for solving the other problems listed above. Our techniques involve a mixture of Davis-Putnam-style backtracking with more sophisticated matching and network flow based ideas.",
        "published": "2000-09-13T20:42:55Z",
        "link": "http://arxiv.org/abs/cs/0009006v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Computing Crossing Numbers in Quadratic Time",
        "authors": [
            "Martin Grohe"
        ],
        "summary": "We show that for every fixed non-negative integer k there is a quadratic time algorithm that decides whether a given graph has crossing number at most k and, if this is the case, computes a drawing of the graph in the plane with at most k crossings.",
        "published": "2000-09-18T14:48:25Z",
        "link": "http://arxiv.org/abs/cs/0009010v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2;G.2.2"
        ]
    },
    {
        "title": "Towards a Theory of Cache-Efficient Algorithms",
        "authors": [
            "Sandeep Sen",
            "Siddhartha Chatterjee",
            "Neeraj Dumir"
        ],
        "summary": "We describe a model that enables us to analyze the running time of an algorithm in a computer with a memory hierarchy with limited associativity, in terms of various cache parameters. Our model, an extension of Aggarwal and Vitter's I/O model, enables us to establish useful relationships between the cache complexity and the I/O complexity of computations. As a corollary, we obtain cache-optimal algorithms for some fundamental problems like sorting, FFT, and an important subclass of permutations in the single-level cache model. We also show that ignoring associativity concerns could lead to inferior performance, by analyzing the average-case cache behavior of mergesort. We further extend our model to multiple levels of cache with limited associativity and present optimal algorithms for matrix transpose and sorting. Our techniques may be used for systematic exploitation of the memory hierarchy starting from the algorithm design stage, and dealing with the hitherto unresolved problem of limited associativity.",
        "published": "2000-10-02T17:09:06Z",
        "link": "http://arxiv.org/abs/cs/0010007v1",
        "categories": [
            "cs.AR",
            "cs.DS",
            "B.3.2;C.0;F.1.1"
        ]
    },
    {
        "title": "Noise-Tolerant Learning, the Parity Problem, and the Statistical Query   Model",
        "authors": [
            "Avrim Blum",
            "Adam Kalai",
            "Hal Wasserman"
        ],
        "summary": "We describe a slightly sub-exponential time algorithm for learning parity functions in the presence of random classification noise. This results in a polynomial-time algorithm for the case of parity functions that depend on only the first O(log n log log n) bits of input. This is the first known instance of an efficient noise-tolerant algorithm for a concept class that is provably not learnable in the Statistical Query model of Kearns. Thus, we demonstrate that the set of problems learnable in the statistical query model is a strict subset of those problems learnable in the presence of noise in the PAC model.   In coding-theory terms, what we give is a poly(n)-time algorithm for decoding linear k by n codes in the presence of random noise for the case of k = c log n loglog n for some c > 0. (The case of k = O(log n) is trivial since one can just individually check each of the 2^k possible messages and choose the one that yields the closest codeword.)   A natural extension of the statistical query model is to allow queries about statistical properties that involve t-tuples of examples (as opposed to single examples). The second result of this paper is to show that any class of functions learnable (strongly or weakly) with t-wise queries for t = O(log n) is also weakly learnable with standard unary queries. Hence this natural extension to the statistical query model does not increase the set of weakly learnable functions.",
        "published": "2000-10-15T20:14:08Z",
        "link": "http://arxiv.org/abs/cs/0010022v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DS",
            "I.2.6"
        ]
    },
    {
        "title": "Oracle Complexity and Nontransitivity in Pattern Recognition",
        "authors": [
            "Vadim Bulitko"
        ],
        "summary": "Different mathematical models of recognition processes are known. In the present paper we consider a pattern recognition algorithm as an oracle computation on a Turing machine. Such point of view seems to be useful in pattern recognition as well as in recursion theory. Use of recursion theory in pattern recognition shows connection between a recognition algorithm comparison problem and complexity problems of oracle computation. That is because in many cases we can take into account only the number of sign computations or in other words volume of oracle information needed. Therefore, the problem of recognition algorithm preference can be formulated as a complexity optimization problem of oracle computation. Furthermore, introducing a certain \"natural\" preference relation on a set of recognizing algorithms, we discover it to be nontransitive. This relates to the well known nontransitivity paradox in probability theory.   Keywords: Pattern Recognition, Recursion Theory, Nontransitivity, Preference Relation",
        "published": "2000-10-16T07:42:23Z",
        "link": "http://arxiv.org/abs/cs/0010023v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "cs.CV",
            "cs.DS",
            "F.2.2; F.4.1; I.2.10; I.5.2"
        ]
    },
    {
        "title": "Opportunity Cost Algorithms for Combinatorial Auctions",
        "authors": [
            "Karhan Akcoglu",
            "James Aspnes",
            "Bhaskar DasGupta",
            "Ming-Yang Kao"
        ],
        "summary": "Two general algorithms based on opportunity costs are given for approximating a revenue-maximizing set of bids an auctioneer should accept, in a combinatorial auction in which each bidder offers a price for some subset of the available goods and the auctioneer can only accept non-intersecting bids. Since this problem is difficult even to approximate in general, the algorithms are most useful when the bids are restricted to be connected node subsets of an underlying object graph that represents which objects are relevant to each other. The approximation ratios of the algorithms depend on structural properties of this graph and are small constants for many interesting families of object graphs. The running times of the algorithms are linear in the size of the bid graph, which describes the conflicts between bids. Extensions of the algorithms allow for efficient processing of additional constraints, such as budget constraints that associate bids with particular bidders and limit how many bids from a particular bidder can be accepted.",
        "published": "2000-10-24T17:14:01Z",
        "link": "http://arxiv.org/abs/cs/0010031v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2.0; J.4"
        ]
    },
    {
        "title": "A Formal Framework for Linguistic Annotation (revised version)",
        "authors": [
            "Steven Bird",
            "Mark Liberman"
        ],
        "summary": "`Linguistic annotation' covers any descriptive or analytic notations applied to raw language data. The basic data may be in the form of time functions - audio, video and/or physiological recordings - or it may be textual. The added notations may include transcriptions of all sorts (from phonetic features to discourse structures), part-of-speech and sense tagging, syntactic analysis, `named entity' identification, co-reference annotation, and so on. While there are several ongoing efforts to provide formats and tools for such annotations and to publish annotated linguistic databases, the lack of widely accepted standards is becoming a critical problem. Proposed standards, to the extent they exist, have focused on file formats. This paper focuses instead on the logical structure of linguistic annotations. We survey a wide variety of existing annotation formats and demonstrate a common conceptual core, the annotation graph. This provides a formal framework for constructing, maintaining and searching linguistic annotations, while remaining consistent with many alternative data structures and file formats.",
        "published": "2000-10-26T17:42:30Z",
        "link": "http://arxiv.org/abs/cs/0010033v1",
        "categories": [
            "cs.CL",
            "cs.DB",
            "cs.DS",
            "A.1; E.2; H.2.1; H.3.3; H.3.7; I.2.7"
        ]
    },
    {
        "title": "Small Maximal Independent Sets and Faster Exact Graph Coloring",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We show that, for any n-vertex graph G and integer parameter k, there are at most 3^{4k-n}4^{n-3k} maximal independent sets I \\subset G with |I| <= k, and that all such sets can be listed in time O(3^{4k-n} 4^{n-3k}). These bounds are tight when n/4 <= k <= n/3. As a consequence, we show how to compute the exact chromatic number of a graph in time O((4/3 + 3^{4/3}/4)^n) ~= 2.4150^n, improving a previous O((1+3^{1/3})^n) ~= 2.4422^n algorithm of Lawler (1976).",
        "published": "2000-11-06T18:54:23Z",
        "link": "http://arxiv.org/abs/cs/0011009v1",
        "categories": [
            "cs.DS",
            "math.CO",
            "F.2.2"
        ]
    },
    {
        "title": "A Decomposition Theorem for Maximum Weight Bipartite Matchings",
        "authors": [
            "Ming-Yang Kao",
            "Tak-Wah Lam",
            "Wing-Kin Sung",
            "Hing-Fung Ting"
        ],
        "summary": "Let G be a bipartite graph with positive integer weights on the edges and without isolated nodes. Let n, N and W be the node count, the largest edge weight and the total weight of G. Let k(x,y) be log(x)/log(x^2/y). We present a new decomposition theorem for maximum weight bipartite matchings and use it to design an O(sqrt(n)W/k(n,W/N))-time algorithm for computing a maximum weight matching of G. This algorithm bridges a long-standing gap between the best known time complexity of computing a maximum weight matching and that of computing a maximum cardinality matching. Given G and a maximum weight matching of G, we can further compute the weight of a maximum weight matching of G-{u} for all nodes u in O(W) time.",
        "published": "2000-11-11T16:35:59Z",
        "link": "http://arxiv.org/abs/cs/0011015v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "E1; F2.2"
        ]
    },
    {
        "title": "Optimal Buy-and-Hold Strategies for Financial Markets with Bounded Daily   Returns",
        "authors": [
            "Gen-Huey Chen",
            "Ming-Yang Kao",
            "Yuh-Dauh Lyuu",
            "Hsing-Kuo Wong"
        ],
        "summary": "In the context of investment analysis, we formulate an abstract online computing problem called a planning game and develop general tools for solving such a game. We then use the tools to investigate a practical buy-and-hold trading problem faced by long-term investors in stocks. We obtain the unique optimal static online algorithm for the problem and determine its exact competitive ratio. We also compare this algorithm with the popular dollar averaging strategy using actual market data.",
        "published": "2000-11-14T16:04:27Z",
        "link": "http://arxiv.org/abs/cs/0011018v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2.2; I.1.2; J.4"
        ]
    },
    {
        "title": "Dancing links",
        "authors": [
            "Donald E. Knuth"
        ],
        "summary": "The author presents two tricks to accelerate depth-first search algorithms for a class of combinatorial puzzle problems, such as tiling a tray by a fixed set of polyominoes. The first trick is to implement each assumption of the search with reversible local operations on doubly linked lists. By this trick, every step of the search affects the data incrementally.   The second trick is to add a ghost square that represents the identity of each polyomino. Thus puts the rule that each polyomino be used once on the same footing as the rule that each square be covered once. The coding simplifies to a more abstract form which is equivalent to 0-1 integer programming. More significantly for the total computation time, the search can naturally switch between placing a fixed polyomino or covering a fixed square at different stages, according to a combined heuristic.   Finally the author reports excellent performance for his algorithm for some familiar puzzles. These include tiling a hexagon by 19 hexiamonds and the N queens problem for N up to 18.",
        "published": "2000-11-15T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/0011047v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A Moment of Perfect Clarity II: Consequences of Sparse Sets Hard for NP   with Respect to Weak Reductions",
        "authors": [
            "Christian Glasser",
            "Lane A. Hemaspaandra"
        ],
        "summary": "This paper discusses advances, due to the work of Cai, Naik, and Sivakumar and Glasser, in the complexity class collapses that follow if NP has sparse hard sets under reductions weaker than (full) truth-table reductions.",
        "published": "2000-11-16T17:12:20Z",
        "link": "http://arxiv.org/abs/cs/0011019v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.1.3; F.1.2"
        ]
    },
    {
        "title": "Optimal Bidding Algorithms Against Cheating in Multiple-Object Auctions",
        "authors": [
            "Ming-Yang Kao",
            "Junfeng Qi",
            "Lei Tan"
        ],
        "summary": "This paper studies some basic problems in a multiple-object auction model using methodologies from theoretical computer science. We are especially concerned with situations where an adversary bidder knows the bidding algorithms of all the other bidders. In the two-bidder case, we derive an optimal randomized bidding algorithm, by which the disadvantaged bidder can procure at least half of the auction objects despite the adversary's a priori knowledge of his algorithm. In the general $k$-bidder case, if the number of objects is a multiple of $k$, an optimal randomized bidding algorithm is found. If the $k-1$ disadvantaged bidders employ that same algorithm, each of them can obtain at least $1/k$ of the objects regardless of the bidding algorithm the adversary uses. These two algorithms are based on closed-form solutions to certain multivariate probability distributions. In situations where a closed-form solution cannot be obtained, we study a restricted class of bidding algorithms as an approximation to desired optimal algorithms.",
        "published": "2000-11-17T01:55:22Z",
        "link": "http://arxiv.org/abs/cs/0011023v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2.2; G.2.1; J.4"
        ]
    },
    {
        "title": "Provably Fast and Accurate Recovery of Evolutionary Trees through   Harmonic Greedy Triplets",
        "authors": [
            "Miklos Csuros",
            "Ming-Yang Kao"
        ],
        "summary": "We give a greedy learning algorithm for reconstructing an evolutionary tree based on a certain harmonic average on triplets of terminal taxa. After the pairwise distances between terminal taxa are estimated from sequence data, the algorithm runs in O(n^2) time using O(n) work space, where n is the number of terminal taxa. These time and space complexities are optimal in the sense that the size of an input distance matrix is n^2 and the size of an output tree is n. Moreover, in the Jukes-Cantor model of evolution, the algorithm recovers the correct tree topology with high probability using sample sequences of length polynomial in (1) n, (2) the logarithm of the error probability, and (3) the inverses of two small parameters.",
        "published": "2000-11-23T14:48:53Z",
        "link": "http://arxiv.org/abs/cs/0011038v1",
        "categories": [
            "cs.DS",
            "cs.LG",
            "E.1; F.2.2; G.2.1; G.2.2; G.2.3; G.3; G.4; J.3"
        ]
    },
    {
        "title": "Index Assignment for Multichannel Communication under Failure",
        "authors": [
            "Tanya Y. Berger-Wolf",
            "Edward M. Reingold"
        ],
        "summary": "We consider the problem of multiple description scalar quantizers and describing the achievable rate-distortion tuples in that setting. We formulate it as a combinatorial optimization problem of arranging numbers in a matrix to minimize the maximum difference between the largest and the smallest number in any row or column. We develop a technique for deriving lower bounds on the distortion at given channel rates. The approach is constructive, thus allowing an algorithm that gives a closely matching upper bound. For the case of two communication channels with equal rates, the bounds coincide, thus giving the precise lowest achievable distortion at fixed rates. The bounds are within a small constant for higher number of channels. To the best of our knowledge, this is the first result concerning systems with more than two communication channels. The problem is also equivalent to the bandwidth minimization problem of Hamming graphs.",
        "published": "2000-11-30T19:03:10Z",
        "link": "http://arxiv.org/abs/cs/0011045v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "H.1.1; F.2.2; G.2.2"
        ]
    },
    {
        "title": "Available Stabilizing Heaps",
        "authors": [
            "Ted Herman",
            "Toshimitsu Masuzawa"
        ],
        "summary": "This paper describes a heap construction that supports insert and delete operations in arbitrary (possibly illegitimate) states. After any sequence of at most O(m) heap operations, the heap state is guarantee to be legitimate, where m is the initial number of items in the heap. The response from each operation is consistent with its effect on the data structure, even for illegitimate states. The time complexity of each operation is O(lg K) where K is the capacity of the data structure; when the heap's state is legitimate the time complexity is O(lg n) for n equal to the number items in the heap.",
        "published": "2000-11-30T23:14:23Z",
        "link": "http://arxiv.org/abs/cs/0011046v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "C.2.4; D.1.3; D.3.3; E.2; C.4"
        ]
    },
    {
        "title": "Available and Stabilizing 2-3 Trees",
        "authors": [
            "Ted Herman",
            "Toshimitsu Masuzawa"
        ],
        "summary": "Transient faults corrupt the content and organization of data structures. A recovery technique dealing with such faults is stabilization, which guarantees, following some number of operations on the data structure, that content of the data structure is legitimate. Another notion of fault tolerance is availability, which is the property that operations continue to be applied during the period of recovery after a fault, and successful updates are not lost while the data structure stabilizes to a legitimate state. The available, stabilizing 2-3 tree supports find, insert, and delete operations, each with O(lg n) complexity when the tree's state is legitimate and contains n items. For an illegitimate state, these operations have O(lg K) complexity where K is the maximum capacity of the tree. Within O(t) operations, the state of the tree is guaranteed to be legitimate, where t is the number of nodes accessible via some path from the tree's root at the initial state. This paper resolves, for the first time, issues of dynamic allocation and pointer organization in a stabilizing data structure.",
        "published": "2000-12-01T15:55:50Z",
        "link": "http://arxiv.org/abs/cs/0012001v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "D.4.5; E.1; C.4"
        ]
    },
    {
        "title": "Random Shuffling to Reduce Disorder in Adaptive Sorting Scheme",
        "authors": [
            "Md. Enamul Karim",
            "Abdun Naser Mahmood"
        ],
        "summary": "In this paper we present a random shuffling scheme to apply with adaptive sorting algorithms. Adaptive sorting algorithms utilize the presortedness present in a given sequence. We have probabilistically increased the amount of presortedness present in a sequence by using a random shuffling technique that requires little computation. Theoretical analysis suggests that the proposed scheme can improve the performance of adaptive sorting. Experimental results show that it significantly reduces the amount of disorder present in a given sequence and improves the execution time of adaptive sorting algorithm as well.",
        "published": "2000-12-02T17:47:26Z",
        "link": "http://arxiv.org/abs/cs/0012002v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Hamilton Circuits in Graphs and Directed Graphs",
        "authors": [
            "Howard Kleiman"
        ],
        "summary": "We give polynomial-time algorithms for obtaining hamilton circuits in random graphs, G, and random directed graphs, D. If n is finite, we assume that G or D contains a hamilton circuit. If G is an arbitrary graph containing a hamilton circuit, we conjecture that Algorithm G always obtains a hamilton circuit in polynomial time.",
        "published": "2000-12-06T05:59:29Z",
        "link": "http://arxiv.org/abs/math/0012036v1",
        "categories": [
            "math.CO",
            "cs.DS"
        ]
    },
    {
        "title": "CoRR: A Computing Research Repository",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "Discusses how CoRR was set up and some policy issues involved with setting up such a repository.",
        "published": "2000-05-03T23:54:53Z",
        "link": "http://arxiv.org/abs/cs/0005003v1",
        "categories": [
            "cs.DL",
            "cs.CY",
            "H.3.7; K.4.0"
        ]
    },
    {
        "title": "A response to the commentaries on CoRR",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "This is a response to the commentaries on \"CoRR: A Computing Research Repository\".",
        "published": "2000-05-04T00:06:01Z",
        "link": "http://arxiv.org/abs/cs/0005004v1",
        "categories": [
            "cs.DL",
            "cs.CY",
            "H.3.7; K.4.0"
        ]
    },
    {
        "title": "Centroid-based summarization of multiple documents: sentence extraction,   utility-based evaluation, and user studies",
        "authors": [
            "Dragomir R. Radev",
            "Hongyan Jing",
            "Malgorzata Budzikowska"
        ],
        "summary": "We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.",
        "published": "2000-05-12T17:24:06Z",
        "link": "http://arxiv.org/abs/cs/0005020v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DL",
            "cs.HC",
            "cs.IR",
            "H.3.1; H.3.4; H.3.7; H.5.2; I.2.7"
        ]
    },
    {
        "title": "Using compression to identify acronyms in text",
        "authors": [
            "Stuart Yeates",
            "David Bainbridge",
            "Ian H. Witten"
        ],
        "summary": "Text mining is about looking for patterns in natural language text, and may be defined as the process of analyzing text to extract information from it for particular purposes. In previous work, we claimed that compression is a key technology for text mining, and backed this up with a study that showed how particular kinds of lexical tokens---names, dates, locations, etc.---can be identified and located in running text, using compression models to provide the leverage necessary to distinguish different token types (Witten et al., 1999)",
        "published": "2000-07-04T02:02:01Z",
        "link": "http://arxiv.org/abs/cs/0007003v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.7"
        ]
    },
    {
        "title": "Science User Scenarios for a Virtual Observatory Design Reference   Mission: Science Requirements for Data Mining",
        "authors": [
            "Kirk D. Borne"
        ],
        "summary": "The knowledge discovery potential of the new large astronomical databases is vast. When these are used in conjunction with the rich legacy data archives, the opportunities for scientific discovery multiply rapidly. A Virtual Observatory (VO) framework will enable transparent and efficient access, search, retrieval, and visualization of data across multiple data repositories, which are generally heterogeneous and distributed. Aspects of data mining that apply to a variety of science user scenarios with a VO are reviewed. The development of a VO should address the data mining needs of various astronomical research constituencies. By way of example, two user scenarios are presented which invoke applications and linkages of data across the catalog and image domains in order to address specific astrophysics research problems. These illustrate a subset of the desired capabilities and power of the VO, and as such they represent potential components of a VO Design Reference Mission.",
        "published": "2000-08-19T17:54:01Z",
        "link": "http://arxiv.org/abs/astro-ph/0008307v1",
        "categories": [
            "astro-ph",
            "cs.DB",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "A usage based analysis of CoRR",
        "authors": [
            "Les Carr",
            "Steve Hitchcock",
            "Wendy Hall",
            "Stevan Harnad"
        ],
        "summary": "Based on an empirical analysis of author usage of CoRR, and of its predecessor in the Los Alamos eprint archives, it is shown that CoRR has not yet been able to match the early growth of the Los Alamos physics archives. Some of the reasons are implicit in Halpern's paper, and we explore them further here. In particular we refer to the need to promote CoRR more effectively for its intended community - computer scientists in universities, industrial research labs and in government. We take up some points of detail on this new world of open archiving concerning central versus distributed self-archiving, publication, the restructuring of the journal publishers' niche, peer review and copyright.",
        "published": "2000-09-13T13:02:04Z",
        "link": "http://arxiv.org/abs/cs/0009004v1",
        "categories": [
            "cs.DL",
            "H.4.3"
        ]
    },
    {
        "title": "Combining Linguistic and Spatial Information for Document Analysis",
        "authors": [
            "Marco Aiello",
            "Christof Monz",
            "Leon Todoran"
        ],
        "summary": "We present a framework to analyze color documents of complex layout. In addition, no assumption is made on the layout. Our framework combines in a content-driven bottom-up approach two different sources of information: textual and spatial. To analyze the text, shallow natural language processing tools, such as taggers and partial parsers, are used. To infer relations of the logical layout we resort to a qualitative spatial calculus closely related to Allen's calculus. We evaluate the system against documents from a color journal and present the results of extracting the reading order from the journal's pages. In this case, our analysis is successful as it extracts the intended reading order from the document.",
        "published": "2000-09-20T13:04:11Z",
        "link": "http://arxiv.org/abs/cs/0009014v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.3.5; H.3.6; H.3.7; I.2.7; I.7"
        ]
    },
    {
        "title": "Data Mining in Astronomical Databases",
        "authors": [
            "Kirk D. Borne"
        ],
        "summary": "A Virtual Observatory (VO) will enable transparent and efficient access, search, retrieval, and visualization of data across multiple data repositories, which are generally heterogeneous and distributed. Aspects of data mining that apply to a variety of science user scenarios with a VO are reviewed.",
        "published": "2000-10-27T21:46:42Z",
        "link": "http://arxiv.org/abs/astro-ph/0010583v2",
        "categories": [
            "astro-ph",
            "cs.DB",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "DISCO: An object-oriented system for music composition and sound design",
        "authors": [
            "Hans G. Kaper",
            "Sever Tipei",
            "Jeff M. Wright"
        ],
        "summary": "This paper describes an object-oriented approach to music composition and sound design. The approach unifies the processes of music making and instrument building by using similar logic, objects, and procedures. The composition modules use an abstract representation of musical data, which can be easily mapped onto different synthesis languages or a traditionally notated score. An abstract base class is used to derive classes on different time scales. Objects can be related to act across time scales, as well as across an entire piece, and relationships between similar objects can replicate traditional music operations or introduce new ones. The DISCO (Digital Instrument for Sonification and Composition) system is an open-ended work in progress.",
        "published": "2000-07-05T18:20:51Z",
        "link": "http://arxiv.org/abs/cs/0007006v1",
        "categories": [
            "cs.SD",
            "cs.DS",
            "cs.SE",
            "H.5.5"
        ]
    },
    {
        "title": "Compiling Language Definitions: The ASF+SDF Compiler",
        "authors": [
            "M. G. J. van den Brand",
            "J. Heering",
            "P. Klint",
            "P. A. Olivier"
        ],
        "summary": "The ASF+SDF Meta-Environment is an interactive language development environment whose main application areas are definition of domain-specific languages, generation of program analysis and transformation tools, production of software renovation tools, and general specification and prototyping. It uses conditional rewrite rules to define the dynamic semantics and other tool-oriented aspects of languages, so the effectiveness of the generated tools is critically dependent on the quality of the rewrite rule implementation.   The ASF+SDF rewrite rule compiler generates C code, thus taking advantage of C's portability and the sophisticated optimization capabilities of current C compilers as well as avoiding potential abstract machine interface bottlenecks. It can handle large (10 000+ rule) language definitions and uses an efficient run-time storage scheme capable of handling large (1 000 000+ node) terms. Term storage uses maximal subterm sharing (hash-consing), which turns out to be more effective in the case of ASF+SDF than in Lisp or SML. Extensive benchmarking has shown the time and space performance of the generated code to be as good as or better than that of the best current rewrite rule and functional language compilers.",
        "published": "2000-07-06T11:38:42Z",
        "link": "http://arxiv.org/abs/cs/0007008v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.1; D.3.2; D.3.4; F.4.2"
        ]
    },
    {
        "title": "From Syntactic Theories to Interpreters: A Specification Language and   Its Compilation",
        "authors": [
            "Yong Xiao",
            "Zena M. Ariola",
            "Michel Mauny"
        ],
        "summary": "Recent years have seen an increasing need of high-level specification languages and tools generating code from specifications. In this paper, we introduce a specification language, {\\splname}, which is tailored to the writing of syntactic theories of language semantics. More specifically, the language supports specifying primitive notions such as dynamic constraints, contexts, axioms, and inference rules. We also introduce a system which generates interpreters from {\\splname} specifications. A prototype system is implemented and has been tested on a number of examples, including a syntactic theory for Verilog.",
        "published": "2000-09-29T20:29:41Z",
        "link": "http://arxiv.org/abs/cs/0009030v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.1; D.1.2; D.3.1; F.3.1; F.3.2; I.2.2"
        ]
    },
    {
        "title": "Proceedings of the Fourth International Workshop on Automated Debugging   (AADEBUG 2000)",
        "authors": [
            "M. Ducasse"
        ],
        "summary": "Over the past decades automated debugging has seen major achievements. However, as debugging is by necessity attached to particular programming paradigms, the results are scattered. The aims of the workshop are to gather common themes and solutions across programming communities, and to cross-fertilize ideas. AADEBUG 2000 in Munich follows AADEBUG'93 in Linkoeping, Sweden; AADEBUG'95 in Saint Malo, France; AADEBUG'97 in Linkoeping, Sweden.",
        "published": "2000-10-30T15:54:03Z",
        "link": "http://arxiv.org/abs/cs/0010035v2",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Collecting Graphical Abstract Views of Mercury Program Executions",
        "authors": [
            "Erwan Jahier"
        ],
        "summary": "A program execution monitor is a program that collects and abstracts information about program executions. The \"collect\" operator is a high level, general purpose primitive which lets users implement their own monitors. \"Collect\" is built on top of the Mercury trace. In previous work, we have demonstrated how this operator can be used to efficiently collect various kinds of statistics about Mercury program executions. In this article we further demonstrate the expressive power and effectiveness of \"collect\" by providing more monitor examples. In particular, we show how to implement monitors that generate graphical abstractions of program executions such as proof trees, control flow graphs and dynamic call graphs. We show how those abstractions can be easily modified and adapted, since those monitors only require several dozens of lines of code. Those abstractions are intended to serve as front-ends of software visualization tools. Although \"collect\" is currently implemented on top of the Mercury trace, none of its underlying concepts depend of Mercury and it can be implemented on top of any tracer for any programming language.",
        "published": "2000-10-31T14:24:09Z",
        "link": "http://arxiv.org/abs/cs/0010038v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Non-intrusive on-the-fly data race detection using execution replay",
        "authors": [
            "Michiel Ronsse",
            "Koen De Bosschere"
        ],
        "summary": "This paper presents a practical solution for detecting data races in parallel programs. The solution consists of a combination of execution replay (RecPlay) with automatic on-the-fly data race detection. This combination enables us to perform the data race detection on an unaltered execution (almost no probe effect). Furthermore, the usage of multilevel bitmaps and snooped matrix clocks limits the amount of memory used. As the record phase of RecPlay is highly efficient, there is no need to switch it off, hereby eliminating the possibility of Heisenbugs because tracing can be left on all the time.",
        "published": "2000-11-06T13:03:11Z",
        "link": "http://arxiv.org/abs/cs/0011005v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Execution replay and debugging",
        "authors": [
            "Michiel Ronsse",
            "Koen De Bosschere",
            "Jacques Chassin de Kergommeaux"
        ],
        "summary": "As most parallel and distributed programs are internally non-deterministic -- consecutive runs with the same input might result in a different program flow -- vanilla cyclic debugging techniques as such are useless. In order to use cyclic debugging tools, we need a tool that records information about an execution so that it can be replayed for debugging. Because recording information interferes with the execution, we must limit the amount of information and keep the processing of the information fast. This paper contains a survey of existing execution replay techniques and tools.",
        "published": "2000-11-06T13:24:32Z",
        "link": "http://arxiv.org/abs/cs/0011006v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Extension Language Automation of Embedded System Debugging",
        "authors": [
            "Dale Parson",
            "Bryan Schlieder",
            "Paul Beatty"
        ],
        "summary": "Embedded systems contain several layers of target processing abstraction. These layers include electronic circuit, binary machine code, mnemonic assembly code, and high-level procedural and object-oriented abstractions. Physical and temporal constraints and artifacts within physically embedded systems make it impossible for software engineers to operate at a single layer of processor abstraction. The Luxdbg embedded system debugger exposes these layers to debugger users, and it adds an additional layer, the extension language layer, that allows users to extend both the debugger and its target processor capabilities. Tcl is Luxdbg's extension language. Luxdbg users can apply Tcl to automate interactive debugging steps, to redirect and to interconnect target processor input-output facilities, to schedule multiple processor execution, to log and to react to target processing exceptions, and to automate target system testing. Inclusion of an extension language like Tcl in a debugger promises additional advantages for distributed debugging, where debuggers can pass extension language expressions across computer networks.",
        "published": "2000-11-06T23:16:22Z",
        "link": "http://arxiv.org/abs/cs/0011010v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Automatic Debugging Support for UML Designs",
        "authors": [
            "Johann Schumann"
        ],
        "summary": "Design of large software systems requires rigorous application of software engineering methods covering all phases of the software process. Debugging during the early design phases is extremely important, because late bug-fixes are expensive.   In this paper, we describe an approach which facilitates debugging of UML requirements and designs. The Unified Modeling Language (UML) is a set of notations for object-orient design of a software system. We have developed an algorithm which translates requirement specifications in the form of annotated sequence diagrams into structured statecharts. This algorithm detects conflicts between sequence diagrams and inconsistencies in the domain knowledge. After synthesizing statecharts from sequence diagrams, these statecharts usually are subject to manual modification and refinement. By using the ``backward'' direction of our synthesis algorithm, we are able to map modifications made to the statechart back into the requirements (sequence diagrams) and check for conflicts there. Fed back to the user conflicts detected by our algorithm are the basis for deductive-based debugging of requirements and domain theory in very early development stages. Our approach allows to generate explanations on why there is a conflict and which parts of the specifications are affected.",
        "published": "2000-11-13T23:30:33Z",
        "link": "http://arxiv.org/abs/cs/0011017v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "On-the-fly Query-Based Debugging with Examples",
        "authors": [
            "Raimondas Lencevicius"
        ],
        "summary": "Program errors are hard to find because of the cause-effect gap between the time when an error occurs and the time when the error becomes apparent to the programmer. Although debugging techniques such as conditional and data breakpoints help to find error causes in simple cases, they fail to effectively bridge the cause-effect gap in many situations. Query-based debuggers offer programmers an effective tool that provides instant error alert by continuously checking inter-object relationships while the debugged program is running. To enable the query-based debugger in the middle of program execution in a portable way, we propose efficient Java class file instrumentation and discuss alternative techniques. Although the on-the-fly debugger has a higher overhead than a dynamic query-based debugger, it offers additional interactive power and flexibility while maintaining complete portability. To speed up dynamic query evaluation, our debugger implemented in portable Java uses a combination of program instrumentation, load-time code generation, query optimization, and incremental reevaluation. This paper discusses on-the-fly debugging and demonstrates the query-based debugger application for debugging Java gas tank applet as well as SPECjvm98 suite applications.",
        "published": "2000-11-16T21:43:08Z",
        "link": "http://arxiv.org/abs/cs/0011021v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Apache web server execution tracing using Third Eye",
        "authors": [
            "Raimondas Lencevicius",
            "Alexander Ran",
            "Rahav Yairi"
        ],
        "summary": "Testing of modern software systems that integrate many components developed by different teams is a difficult task. Third Eye is a framework for tracing and validating software systems using application domain events. We use formal descriptions of the constraints between events to identify violations in execution traces. Third Eye is a flexible and modular framework that can be used in different products. We present the validation of the Apache Web Server access policy implementation. The results indicate that our tool is a helpful addition to software development infrastructure.",
        "published": "2000-11-16T22:13:47Z",
        "link": "http://arxiv.org/abs/cs/0011022v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Extended Abstract - Model-Based Debugging of Java Programs",
        "authors": [
            "Cristinel Mateis",
            "Markus Stumptner",
            "Dominik Wieland",
            "Franz Wotawa"
        ],
        "summary": "Model-based reasoning is a central concept in current research into intelligent diagnostic systems. It is based on the assumption that sources of incorrect behavior in technical devices can be located and identified via the existence of a model describing the basic properties of components of a certain application domain. When actual data concerning the misbehavior of a system composed from such components is available, a domain-independent diagnosis engine can be used to infer which parts of the system contribute to the observed behavior. This paper describes the application of the model-based approach to the debugging of Java programs written in a subset of Java. We show how a simple dependency model can be derived from a program, demonstrate the use of the model for debugging and reducing the required user interactions, give a comparison of the functional dependency model with program slicing, and finally discuss some current research issues.",
        "published": "2000-11-20T14:48:38Z",
        "link": "http://arxiv.org/abs/cs/0011027v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Systematic Debugging of Attribute Grammars",
        "authors": [
            "Yohei Ikezoe",
            "Akira Sasaki",
            "Yoshiki Ohshima",
            "Ken Wakita",
            "Masataka Sassa"
        ],
        "summary": "Although attribute grammars are commonly used for compiler construction, little investigation has been conducted on debugging attribute grammars. The paper proposes two types of systematic debugging methods, an algorithmic debugging and slice-based debugging, both tailored for attribute grammars. By means of query-based interaction with the developer, our debugging methods effectively narrow the potential bug space in the attribute grammar description and eventually identify the incorrect attribution rule. We have incorporated this technology in our visual debugging tool called Aki.",
        "published": "2000-11-21T08:09:10Z",
        "link": "http://arxiv.org/abs/cs/0011029v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Value Withdrawal Explanation in CSP",
        "authors": [
            "Gerard Ferrand",
            "Willy Lesaint",
            "Alexandre Tessier"
        ],
        "summary": "This work is devoted to constraint solving motivated by the debugging of constraint logic programs a la GNU-Prolog. The paper focuses only on the constraints. In this framework, constraint solving amounts to domain reduction. A computation is formalized by a chaotic iteration. The computed result is described as a closure. This model is well suited to the design of debugging notions and tools, for example failure explanations or error diagnosis. In this paper we detail an application of the model to an explanation of a value withdrawal in a domain. Some other works have already shown the interest of such a notion of explanation not only for failure analysis.",
        "published": "2000-12-11T13:04:18Z",
        "link": "http://arxiv.org/abs/cs/0012005v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Support for Debugging Automatically Parallelized Programs",
        "authors": [
            "Robert Hood",
            "Gabriele Jost"
        ],
        "summary": "We describe a system that simplifies the process of debugging programs produced by computer-aided parallelization tools. The system uses relative debugging techniques to compare serial and parallel executions in order to show where the computations begin to differ. If the original serial code is correct, errors due to parallelization will be isolated by the comparison.   One of the primary goals of the system is to minimize the effort required of the user. To that end, the debugging system uses information produced by the parallelization tool to drive the comparison process. In particular, the debugging system relies on the parallelization tool to provide information about where variables may have been modified and how arrays are distributed across multiple processes. User effort is also reduced through the use of dynamic instrumentation. This allows us to modify the program execution without changing the way the user builds the executable.   The use of dynamic instrumentation also permits us to compare the executions in a fine-grained fashion and only involve the debugger when a difference has been detected. This reduces the overhead of executing instrumentation.",
        "published": "2000-12-11T17:23:59Z",
        "link": "http://arxiv.org/abs/cs/0012006v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Kima - an Automated Error Correction System for Concurrent Logic   Programs",
        "authors": [
            "Yasuhiro Ajiro",
            "Kazunori Ueda"
        ],
        "summary": "We have implemented Kima, an automated error correction system for concurrent logic programs. Kima corrects near-misses such as wrong variable occurrences in the absence of explicit declarations of program properties. Strong moding/typing and constraint-based analysis are turning to play fundamental roles in debugging concurrent logic programs as well as in establishing the consistency of communication protocols and data types. Mode/type analysis of Moded Flat GHC is a constraint satisfaction problem with many simple mode/type constraints, and can be solved efficiently. We proposed a simple and efficient technique which, given a non-well-moded/typed program, diagnoses the ``reasons'' of inconsistency by finding minimal inconsistent subsets of mode/type constraints. Since each constraint keeps track of the symbol occurrence in the program, a minimal subset also tells possible sources of program errors. Kima realizes automated correction by replacing symbol occurrences around the possible sources and recalculating modes and types of the rewritten programs systematically. As long as bugs are near-misses, Kima proposes a rather small number of alternatives that include an intended program.",
        "published": "2000-12-13T08:48:06Z",
        "link": "http://arxiv.org/abs/cs/0012007v3",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Finding Failure Causes through Automated Testing",
        "authors": [
            "Holger Cleve",
            "Andreas Zeller"
        ],
        "summary": "A program fails. Under which circumstances does this failure occur? One single algorithm, the delta debugging algorithm, suffices to determine these failure-inducing circumstances. Delta debugging tests a program systematically and automatically to isolate failure-inducing circumstances such as the program input, changes to the program code, or executed statements.",
        "published": "2000-12-14T13:49:52Z",
        "link": "http://arxiv.org/abs/cs/0012009v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "A brief overview of the MAD debugging activities",
        "authors": [
            "Dieter Kranzlmueller",
            "Christian Schaubschlaeger",
            "Jens Volkert"
        ],
        "summary": "Debugging parallel and distributed programs is a difficult activitiy due to the multiplicity of sequential bugs, the existence of malign effects like race conditions and deadlocks, and the huge amounts of data that have to be processed. These problems are addressed by the Monitoring And Debugging environment MAD, which offers debugging functionality based on a graphical representation of a program's execution. The target applications of MAD are parallel programs applying the standard Message-Passing Interface MPI, which is used extensively in the high-performance computing domain. The highlights of MAD are interactive inspection mechanisms including visualization of distributed arrays, the possibility to graphically place breakpoints, a mechanism for monitor overhead removal, and the evaluation of racing messages occuring due to nondeterminism in the code.",
        "published": "2000-12-16T20:32:00Z",
        "link": "http://arxiv.org/abs/cs/0012012v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Slicing of Constraint Logic Programs",
        "authors": [
            "Gyongyi Szilagyi",
            "Tibor Gyimothy",
            "Jan Maluszynski"
        ],
        "summary": "Slicing is a program analysis technique originally developed for imperative languages. It facilitates understanding of data flow and debugging.   This paper discusses slicing of Constraint Logic Programs. Constraint Logic Programming (CLP) is an emerging software technology with a growing number of applications. Data flow in constraint programs is not explicit, and for this reason the concepts of slice and the slicing techniques of imperative languages are not directly applicable.   This paper formulates declarative notions of slice suitable for CLP. They provide a basis for defining slicing techniques (both dynamic and static) based on variable sharing. The techniques are further extended by using groundness information.   A prototype dynamic slicer of CLP programs implementing the presented ideas is briefly described together with the results of some slicing experiments.",
        "published": "2000-12-18T11:59:31Z",
        "link": "http://arxiv.org/abs/cs/0012014v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Performance and Scalability Models for a Hypergrowth e-Commerce Web Site",
        "authors": [
            "Neil J. Gunther"
        ],
        "summary": "The performance of successful Web-based e-commerce services has all the allure of a roller-coaster ride: accelerated fiscal growth combined with the ever-present danger of running out of server capacity. This chapter presents a case study based on the author's own capacity planning engagement with one of the hottest e-commerce Web sites in the world. Several spreadsheet techniques are presented for forecasting both short-term and long-term trends in the consumption of server capacity. Two new performance metrics are introduced for site planning and procurement: the effective demand, and the doubling period.",
        "published": "2000-12-26T22:42:39Z",
        "link": "http://arxiv.org/abs/cs/0012022v1",
        "categories": [
            "cs.PF",
            "cs.DC",
            "cs.SE",
            "C.4;D.2.8;D.4.8;H.3.5"
        ]
    },
    {
        "title": "High-resolution path-integral development of financial options",
        "authors": [
            "Lester Ingber"
        ],
        "summary": "The Black-Scholes theory of option pricing has been considered for many years as an important but very approximate zeroth-order description of actual market behavior. We generalize the functional form of the diffusion of these systems and also consider multi-factor models including stochastic volatility. Daily Eurodollar futures prices and implied volatilities are fit to determine exponents of functional behavior of diffusions using methods of global optimization, Adaptive Simulated Annealing (ASA), to generate tight fits across moving time windows of Eurodollar contracts. These short-time fitted distributions are then developed into long-time distributions using a robust non-Monte Carlo path-integral algorithm, PATHINT, to generate prices and derivatives commonly used by option traders.",
        "published": "2000-01-23T18:48:20Z",
        "link": "http://arxiv.org/abs/physics/0001048v1",
        "categories": [
            "physics.comp-ph",
            "cs.CE",
            "physics.data-an",
            "q-fin.PR"
        ]
    },
    {
        "title": "Adaptive simulated annealing (ASA): Lessons learned",
        "authors": [
            "Lester Ingber"
        ],
        "summary": "Adaptive simulated annealing (ASA) is a global optimization algorithm based on an associated proof that the parameter space can be sampled much more efficiently than by using other previous simulated annealing algorithms. The author's ASA code has been publicly available for over two years. During this time the author has volunteered to help people via e-mail, and the feedback obtained has been used to further develop the code. Some lessons learned, in particular some which are relevant to other simulated annealing algorithms, are described.",
        "published": "2000-01-23T20:36:49Z",
        "link": "http://arxiv.org/abs/cs/0001018v1",
        "categories": [
            "cs.MS",
            "cs.CE",
            "G.1.6"
        ]
    },
    {
        "title": "On The Closest String and Substring Problems",
        "authors": [
            "Ming Li",
            "Bin Ma",
            "Lusheng Wang"
        ],
        "summary": "The problem of finding a center string that is `close' to every given string arises and has many applications in computational biology and coding theory. This problem has two versions: the Closest String problem and the Closest Substring problem. Assume that we are given a set of strings ${\\cal S}=\\{s_1, s_2, ..., s_n\\}$ of strings, say, each of length $m$. The Closest String problem asks for the smallest $d$ and a string $s$ of length $m$ which is within Hamming distance $d$ to each $s_i\\in {\\cal S}$. This problem comes from coding theory when we are looking for a code not too far away from a given set of codes. The problem is NP-hard. Berman et al give a polynomial time algorithm for constant $d$. For super-logarithmic $d$, Ben-Dor et al give an efficient approximation algorithm using linear program relaxation technique. The best polynomial time approximation has ratio 4/3 for all $d$ given by Lanctot et al and Gasieniec et al. The Closest Substring problem looks for a string $t$ which is within Hamming distance $d$ away from a substring of each $s_i$. This problem only has a $2- \\frac{2}{2|\\Sigma|+1}$ approximation algorithm previously Lanctot et al and is much more elusive than the Closest String problem, but it has many applications in finding conserved regions, genetic drug target identification, and genetic probes in molecular biology. Whether there are efficient approximation algorithms for both problems are major open questions in this area. We present two polynomial time approxmation algorithms with approximation ratio $1+ \\epsilon$ for any small $\\epsilon$ to settle both questions.",
        "published": "2000-02-17T23:06:06Z",
        "link": "http://arxiv.org/abs/cs/0002012v1",
        "categories": [
            "cs.CE",
            "cs.CC",
            "F.2;J.3"
        ]
    },
    {
        "title": "From naive to sophisticated behavior in multiagents based financial   market models",
        "authors": [
            "Ricardo Mansilla"
        ],
        "summary": "We discuss the behavior of two magnitudes, physical complexity and mutual information function of the outcome of a model of heterogeneous, inductive rational agents inspired in the El Farol Bar problem and the Minority Game. The first is a measure rooted in Kolmogorov-Chaitin theory and the second one a measure related with information entropy of Shannon.   We make extensive computer simulations, as result of which, we propose an ansatz for physical complexity and establish the dependence of exponent of that ansatz from the parameters of the model. We discuss the accuracy of our results and the relationship with the behavior of mutual information function as a measure of time correlations of agents choice.",
        "published": "2000-02-21T20:09:19Z",
        "link": "http://arxiv.org/abs/cond-mat/0002331v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CE",
            "nlin.AO",
            "physics.data-an",
            "q-fin.TR"
        ]
    },
    {
        "title": "The dynamics of iterated transportation simulations",
        "authors": [
            "Kai Nagel",
            "Marcus Rickert",
            "Patrice M. Simon",
            "Martin Pieck"
        ],
        "summary": "Iterating between a router and a traffic micro-simulation is an increasibly accepted method for doing traffic assignment. This paper, after pointing out that the analytical theory of simulation-based assignment to-date is insufficient for some practical cases, presents results of simulation studies from a real world study. Specifically, we look into the issues of uniqueness, variability, and robustness and validation. Regarding uniqueness, despite some cautionary notes from a theoretical point of view, we find no indication of ``meta-stable'' states for the iterations. Variability however is considerable. By variability we mean the variation of the simulation of a given plan set by just changing the random seed. We show then results from three different micro-simulations under the same iteration scenario in order to test for the robustness of the results under different implementations. We find the results encouraging, also when comparing to reality and with a traditional assignment result.   Keywords: dynamic traffic assignment (DTA); traffic micro-simulation; TRANSIMS; large-scale simulations; urban planning",
        "published": "2000-02-23T01:42:08Z",
        "link": "http://arxiv.org/abs/nlin/0002040v1",
        "categories": [
            "nlin.AO",
            "cs.CE"
        ]
    },
    {
        "title": "Optimization of Trading Physics Models of Markets",
        "authors": [
            "Lester Ingber",
            "Radu Paul Mondescu"
        ],
        "summary": "We describe an end-to-end real-time S&P futures trading system. Inner-shell stochastic nonlinear dynamic models are developed, and Canonical Momenta Indicators (CMI) are derived from a fitted Lagrangian used by outer-shell trading models dependent on these indicators. Recursive and adaptive optimization using Adaptive Simulated Annealing (ASA) is used for fitting parameters shared across these shells of dynamic and trading models.",
        "published": "2000-07-22T02:23:07Z",
        "link": "http://arxiv.org/abs/physics/0007075v1",
        "categories": [
            "physics.comp-ph",
            "cond-mat.stat-mech",
            "cs.CE",
            "physics.data-an",
            "q-fin.ST"
        ]
    },
    {
        "title": "Fault Detection using Immune-Based Systems and Formal Language   Algorithms",
        "authors": [
            "J. F. Martins",
            "P. J. Costa Branco",
            "A. J. Pires",
            "J. A. Dente"
        ],
        "summary": "This paper describes two approaches for fault detection: an immune-based mechanism and a formal language algorithm. The first one is based on the feature of immune systems in distinguish any foreign cell from the body own cell. The formal language approach assumes the system as a linguistic source capable of generating a certain language, characterised by a grammar. Each algorithm has particular characteristics, which are analysed in the paper, namely in what cases they can be used with advantage. To test their practicality, both approaches were applied on the problem of fault detection in an induction motor.",
        "published": "2000-10-03T17:54:38Z",
        "link": "http://arxiv.org/abs/cs/0010010v1",
        "categories": [
            "cs.CE",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Towards Understanding the Predictability of Stock Markets from the   Perspective of Computational Complexity",
        "authors": [
            "James Aspnes",
            "David F. Fischer",
            "Michael J. Fischer",
            "Ming-Yang Kao",
            "Alok Kumar"
        ],
        "summary": "This paper initiates a study into the century-old issue of market predictability from the perspective of computational complexity. We develop a simple agent-based model for a stock market where the agents are traders equipped with simple trading strategies, and their trades together determine the stock prices. Computer simulations show that a basic case of this model is already capable of generating price graphs which are visually similar to the recent price movements of high tech stocks. In the general model, we prove that if there are a large number of traders but they employ a relatively small number of strategies, then there is a polynomial-time algorithm for predicting future price movements with high accuracy. On the other hand, if the number of strategies is large, market prediction becomes complete in two new computational complexity classes CPP and BCPP, which are between P^NP[O(log n)] and PP. These computational completeness results open up a novel possibility that the price graph of an actual stock could be sufficiently deterministic for various prediction goals but appear random to all polynomial-time prediction algorithms.",
        "published": "2000-10-14T14:01:17Z",
        "link": "http://arxiv.org/abs/cs/0010021v2",
        "categories": [
            "cs.CE",
            "cs.CC",
            "F.1.3;F.2.2;G.2.1;G.2.3;G.3;J.1"
        ]
    },
    {
        "title": "Hot-pressing process modeling for medium density fiberboard (MDF)",
        "authors": [
            "Noberto M. Nigro",
            "Mario A. Storti"
        ],
        "summary": "In this paper we present a numerical solution for the mathematical modeling of the hot-pressing process applied to medium density fiberboard. The model is based in the work of Humphrey[82], Humphrey and Bolton[89] and Carvalho and Costa[98], with some modifications and extensions in order to take into account mainly the convective effects on the phase change term and also a conservative numerical treatment of the resulting system of partial differential equations.",
        "published": "2000-10-17T17:04:00Z",
        "link": "http://arxiv.org/abs/math/0010173v4",
        "categories": [
            "math.NA",
            "cs.CE"
        ]
    },
    {
        "title": "Opportunity Cost Algorithms for Combinatorial Auctions",
        "authors": [
            "Karhan Akcoglu",
            "James Aspnes",
            "Bhaskar DasGupta",
            "Ming-Yang Kao"
        ],
        "summary": "Two general algorithms based on opportunity costs are given for approximating a revenue-maximizing set of bids an auctioneer should accept, in a combinatorial auction in which each bidder offers a price for some subset of the available goods and the auctioneer can only accept non-intersecting bids. Since this problem is difficult even to approximate in general, the algorithms are most useful when the bids are restricted to be connected node subsets of an underlying object graph that represents which objects are relevant to each other. The approximation ratios of the algorithms depend on structural properties of this graph and are small constants for many interesting families of object graphs. The running times of the algorithms are linear in the size of the bid graph, which describes the conflicts between bids. Extensions of the algorithms allow for efficient processing of additional constraints, such as budget constraints that associate bids with particular bidders and limit how many bids from a particular bidder can be accepted.",
        "published": "2000-10-24T17:14:01Z",
        "link": "http://arxiv.org/abs/cs/0010031v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2.0; J.4"
        ]
    },
    {
        "title": "Chip-level CMP Modeling and Smart Dummy for HDP and Conformal CVD Films",
        "authors": [
            "George Yong Liu",
            "Ray F. Zhang",
            "Kelvin Hsu",
            "Lawrence Camilletti"
        ],
        "summary": "Chip-level CMP modeling is investigated to obtain the post-CMP film profile thickness across a die from its design layout file and a few film deposition and CMP parameters. The work covers both HDP and conformal CVD film. The experimental CMP results agree well with the modeled results. Different algorithms for filling of dummy structure are compared. A smart algorithm for dummy filling is presented, which achieves maximal pattern-density uniformity and CMP planarity.",
        "published": "2000-11-09T20:39:25Z",
        "link": "http://arxiv.org/abs/cs/0011014v1",
        "categories": [
            "cs.CE",
            "B.7.2"
        ]
    },
    {
        "title": "Designing Proxies for Stock Market Indices is Computationally Hard",
        "authors": [
            "Ming-Yang Kao",
            "Stephen R. Tate"
        ],
        "summary": "In this paper, we study the problem of designing proxies (or portfolios) for various stock market indices based on historical data. We use four different methods for computing market indices, all of which are formulas used in actual stock market analysis. For each index, we consider three criteria for designing the proxy: the proxy must either track the market index, outperform the market index, or perform within a margin of error of the index while maintaining a low volatility. In eleven of the twelve cases (all combinations of four indices with three criteria except the problem of sacrificing return for less volatility using the price-relative index) we show that the problem is NP-hard, and hence most likely intractable.",
        "published": "2000-11-13T02:51:49Z",
        "link": "http://arxiv.org/abs/cs/0011016v1",
        "categories": [
            "cs.CE",
            "cs.CC",
            "F.2.2;G.2.3;J.4"
        ]
    },
    {
        "title": "Optimal Buy-and-Hold Strategies for Financial Markets with Bounded Daily   Returns",
        "authors": [
            "Gen-Huey Chen",
            "Ming-Yang Kao",
            "Yuh-Dauh Lyuu",
            "Hsing-Kuo Wong"
        ],
        "summary": "In the context of investment analysis, we formulate an abstract online computing problem called a planning game and develop general tools for solving such a game. We then use the tools to investigate a practical buy-and-hold trading problem faced by long-term investors in stocks. We obtain the unique optimal static online algorithm for the problem and determine its exact competitive ratio. We also compare this algorithm with the popular dollar averaging strategy using actual market data.",
        "published": "2000-11-14T16:04:27Z",
        "link": "http://arxiv.org/abs/cs/0011018v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2.2; I.1.2; J.4"
        ]
    },
    {
        "title": "Optimal Bidding Algorithms Against Cheating in Multiple-Object Auctions",
        "authors": [
            "Ming-Yang Kao",
            "Junfeng Qi",
            "Lei Tan"
        ],
        "summary": "This paper studies some basic problems in a multiple-object auction model using methodologies from theoretical computer science. We are especially concerned with situations where an adversary bidder knows the bidding algorithms of all the other bidders. In the two-bidder case, we derive an optimal randomized bidding algorithm, by which the disadvantaged bidder can procure at least half of the auction objects despite the adversary's a priori knowledge of his algorithm. In the general $k$-bidder case, if the number of objects is a multiple of $k$, an optimal randomized bidding algorithm is found. If the $k-1$ disadvantaged bidders employ that same algorithm, each of them can obtain at least $1/k$ of the objects regardless of the bidding algorithm the adversary uses. These two algorithms are based on closed-form solutions to certain multivariate probability distributions. In situations where a closed-form solution cannot be obtained, we study a restricted class of bidding algorithms as an approximation to desired optimal algorithms.",
        "published": "2000-11-17T01:55:22Z",
        "link": "http://arxiv.org/abs/cs/0011023v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2.2; G.2.1; J.4"
        ]
    },
    {
        "title": "Adaptive simulated annealing (ASA): Lessons learned",
        "authors": [
            "Lester Ingber"
        ],
        "summary": "Adaptive simulated annealing (ASA) is a global optimization algorithm based on an associated proof that the parameter space can be sampled much more efficiently than by using other previous simulated annealing algorithms. The author's ASA code has been publicly available for over two years. During this time the author has volunteered to help people via e-mail, and the feedback obtained has been used to further develop the code. Some lessons learned, in particular some which are relevant to other simulated annealing algorithms, are described.",
        "published": "2000-01-23T20:36:49Z",
        "link": "http://arxiv.org/abs/cs/0001018v1",
        "categories": [
            "cs.MS",
            "cs.CE",
            "G.1.6"
        ]
    },
    {
        "title": "Matrix Distributed Processing: A set of C++ Tools for implementing   generic lattice computations on parallel systems",
        "authors": [
            "Massimo Di Pierro"
        ],
        "summary": "We present a set of programming tools (classes and functions written in C++ and based on Message Passing Interface) for fast development of generic parallel (and non-parallel) lattice simulations. They are collectively called MDP 1.2.   These programming tools include classes and algorithms for matrices, random number generators, distributed lattices (with arbitrary topology), fields and parallel iterations. No previous knowledge of MPI is required in order to use them.   Some applications in electromagnetism, electronics, condensed matter and lattice QCD are presented.",
        "published": "2000-04-11T22:59:10Z",
        "link": "http://arxiv.org/abs/hep-lat/0004007v4",
        "categories": [
            "hep-lat",
            "cs.DC",
            "cs.MS",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Mathematical Software: Past, Present, and Future",
        "authors": [
            "Ronald F. Boisvert"
        ],
        "summary": "This paper provides some reflections on the field of mathematical software on the occasion of John Rice's 65th birthday. I describe some of the common themes of research in this field and recall some significant events in its evolution. Finally, I raise a number of issues that are of concern to future developments.",
        "published": "2000-04-13T13:43:48Z",
        "link": "http://arxiv.org/abs/cs/0004004v1",
        "categories": [
            "cs.MS",
            "G.4"
        ]
    },
    {
        "title": "Introduction to the GiNaC Framework for Symbolic Computation within the   C++ Programming Language",
        "authors": [
            "Christian Bauer",
            "Alexander Frink",
            "Richard Kreckel"
        ],
        "summary": "The traditional split-up into a low level language and a high level language in the design of computer algebra systems may become obsolete with the advent of more versatile computer languages. We describe GiNaC, a special-purpose system that deliberately denies the need for such a distinction. It is entirely written in C++ and the user can interact with it directly in that language. It was designed to provide efficient handling of multivariate polynomials, algebras and special functions that are needed for loop calculations in theoretical quantum field theory. It also bears some potential to become a more general purpose symbolic package.",
        "published": "2000-04-27T10:30:58Z",
        "link": "http://arxiv.org/abs/cs/0004015v2",
        "categories": [
            "cs.SC",
            "hep-ph",
            "physics.comp-ph",
            "I.1.1; I.1.3"
        ]
    },
    {
        "title": "A Lambda-Calculus with letrec, case, constructors and non-determinism",
        "authors": [
            "Manfred Schmidt-Schauß",
            "Michael Huber"
        ],
        "summary": "A non-deterministic call-by-need lambda-calculus \\calc with case, constructors, letrec and a (non-deterministic) erratic choice, based on rewriting rules is investigated. A standard reduction is defined as a variant of left-most outermost reduction. The semantics is defined by contextual equivalence of expressions instead of using $\\alpha\\beta(\\eta)$-equivalence. It is shown that several program transformations are correct, for example all (deterministic) rules of the calculus, and in addition the rules for garbage collection, removing indirections and unique copy.   This shows that the combination of a context lemma and a meta-rewriting on reductions using complete sets of commuting (forking, resp.) diagrams is a useful and successful method for providing a semantics of a functional programming language and proving correctness of program transformations.",
        "published": "2000-11-06T14:52:33Z",
        "link": "http://arxiv.org/abs/cs/0011008v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.SC",
            "F.4.1;D.3.2;I.2.2"
        ]
    },
    {
        "title": "Rewriting Calculus: Foundations and Applications",
        "authors": [
            "Horatiu Cirstea"
        ],
        "summary": "This thesis is devoted to the study of a calculus that describes the application of conditional rewriting rules and the obtained results at the same level of representation. We introduce the rewriting calculus, also called the rho-calculus, which generalizes the first order term rewriting and lambda-calculus, and makes possible the representation of the non-determinism. In our approach the abstraction operator as well as the application operator are objects of calculus. The result of a reduction in the rewriting calculus is either an empty set representing the application failure, or a singleton representing a deterministic result, or a set having several elements representing a not-deterministic choice of results.   In this thesis we concentrate on the properties of the rewriting calculus where a syntactic matching is used in order to bind the variables to their current values. We define evaluation strategies ensuring the confluence of the calculus and we show that these strategies become trivial for restrictions of the general rewriting calculus to simpler calculi like the lambda-calculus. The rewriting calculus is not terminating in the untyped case but the strong normalization is obtained for the simply typed calculus.   In the rewriting calculus extended with an operator allowing to test the application failure we define terms representing innermost and outermost normalizations with respect to a set of rewriting rules. By using these terms, we obtain a natural and concise description of the conditional rewriting. Finally, starting from the representation of the conditional rewriting rules, we show how the rewriting calculus can be used to give a semantics to ELAN, a language based on the application of rewriting rules controlled by strategies.",
        "published": "2000-11-28T15:01:07Z",
        "link": "http://arxiv.org/abs/cs/0011043v1",
        "categories": [
            "cs.SC",
            "cs.LO",
            "cs.PL",
            "I.1; D.1; D.3; F.4.0; F.4.1"
        ]
    },
    {
        "title": "Fully Sequential and Distributed Dynamic Algorithms for Minimum Spanning   Trees",
        "authors": [
            "Pradosh Kumar Mohapatra"
        ],
        "summary": "In this paper, we present a fully-dynamic distributed algorithm for maintaining a minimum spanning tree on general graphs with positive real edge weights. The goal of a dynamic MST algorithm is to update efficiently the minimum spanning tree after dynamic changes like edge weight changes, rather than having to recompute it from scatch each time. The first part of the paper surveys various algorithms available today both in sequential and distributed environments to solve static MST problem. We also present some of the efficient sequential algorithms for computing dynamic MST like the Frederickson's algorithm and Eppstein's sparsification technique. Lastly we present our new sequential and distributed algorithms for dynamic MST problem. To our knowledge, this is the first of the distributed algorithms for computing dynamic MSTs.",
        "published": "2000-02-08T17:51:50Z",
        "link": "http://arxiv.org/abs/cs/0002005v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "G.2.2;F.2.2"
        ]
    },
    {
        "title": "On Automata with Boundary",
        "authors": [
            "R. Gates",
            "P. Katis",
            "N. Sabadini",
            "R. F. C. Walters"
        ],
        "summary": "We present a theory of automata with boundary for designing, modelling and analysing distributed systems. Notions of behaviour, design and simulation appropriate to the theory are defined. The problem of model checking for deadlock detection is discussed, and an algorithm for state space reduction in exhaustive search, based on the theory presented here, is described. Three examples of the application of the theory are given, one in the course of the development of the ideas and two as illustrative examples of the use of the theory.",
        "published": "2000-02-16T02:45:39Z",
        "link": "http://arxiv.org/abs/cs/0002008v1",
        "categories": [
            "cs.DC",
            "D.2.2; D.2.4; F.1.1"
        ]
    },
    {
        "title": "A Problem-Specific Fault-Tolerance Mechanism for Asynchronous,   Distributed Systems",
        "authors": [
            "Adriana Iamnitchi",
            "Ian Foster"
        ],
        "summary": "The idle computers on a local area, campus area, or even wide area network represent a significant computational resource---one that is, however, also unreliable, heterogeneous, and opportunistic. This type of resource has been used effectively for embarrassingly parallel problems but not for more tightly coupled problems. We describe an algorithm that allows branch-and-bound problems to be solved in such environments. In designing this algorithm, we faced two challenges: (1) scalability, to effectively exploit the variably sized pools of resources available, and (2) fault tolerance, to ensure the reliability of services. We achieve scalability through a fully decentralized algorithm, by using a membership protocol for managing dynamically available resources. However, this fully decentralized design makes achieving reliability even more challenging. We guarantee fault tolerance in the sense that the loss of up to all but one resource will not affect the quality of the solution. For propagating information efficiently, we use epidemic communication for both the membership protocol and the fault-tolerance mechanism. We have developed a simulation framework that allows us to evaluate design alternatives. Results obtained in this framework suggest that our techniques can execute scalably and reliably.",
        "published": "2000-03-12T00:19:24Z",
        "link": "http://arxiv.org/abs/cs/0003054v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "A note on knowledge-based programs and specifications",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "Knowledge-based program are programs with explicit tests for knowledge. They have been used successfully in a number of applications. Sanders has pointed out what seem to be a counterintuitive property of knowledge-based programs. Roughly speaking, they do not satisfy a certain monotonicity property, while standard programs (ones without tests for knowledge) do. It is shown that there are two ways of defining the monotonicity property, which agree for standard programs. Knowledge-based programs satisfy the first, but do not satisfy the second. It is further argued by example that the fact that they do not satisfy the second is actually a feature, not a problem. Moreover, once we allow the more general class of knowledge-based specifications, standard programs do not satisfy the monotonicity property either.",
        "published": "2000-03-13T23:35:34Z",
        "link": "http://arxiv.org/abs/cs/0003058v1",
        "categories": [
            "cs.DC",
            "cs.LO",
            "D.1.3; F.3.1; D.2.1"
        ]
    },
    {
        "title": "Reliable Cellular Automata with Self-Organization",
        "authors": [
            "Peter Gacs"
        ],
        "summary": "In a probabilistic cellular automaton in which all local transitions have positive probability, the problem of keeping a bit of information indefinitely is nontrivial, even in an infinite automaton. Still, there is a solution in 2 dimensions, and this solution can be used to construct a simple 3-dimensional discrete-time universal fault-tolerant cellular automaton. This technique does not help much to solve the following problems: remembering a bit of information in 1 dimension; computing in dimensions lower than 3; computing in any dimension with non-synchronized transitions.   Our more complex technique organizes the cells in blocks that perform a reliable simulation of a second (generalized) cellular automaton. The cells of the latter automaton are also organized in blocks, simulating even more reliably a third automaton, etc. Since all this (a possibly infinite hierarchy) is organized in ``software'', it must be under repair all the time from damage caused by errors. A large part of the problem is essentially self-stabilization recovering from a mess of arbitrary size and content. The present paper constructs an asynchronous one-dimensional fault-tolerant cellular automaton, with the further feature of ``self-organization''. The latter means that the initial configuration does not have to encode an infinite hierarchy -- this will be built up over time.   This is a corrected and strengthened version of the journal paper of 2001.",
        "published": "2000-03-20T18:38:07Z",
        "link": "http://arxiv.org/abs/math/0003117v2",
        "categories": [
            "math.PR",
            "cs.DC",
            "60K35, 65Q80, 82C22, 37B15"
        ]
    },
    {
        "title": "Matrix Distributed Processing: A set of C++ Tools for implementing   generic lattice computations on parallel systems",
        "authors": [
            "Massimo Di Pierro"
        ],
        "summary": "We present a set of programming tools (classes and functions written in C++ and based on Message Passing Interface) for fast development of generic parallel (and non-parallel) lattice simulations. They are collectively called MDP 1.2.   These programming tools include classes and algorithms for matrices, random number generators, distributed lattices (with arbitrary topology), fields and parallel iterations. No previous knowledge of MPI is required in order to use them.   Some applications in electromagnetism, electronics, condensed matter and lattice QCD are presented.",
        "published": "2000-04-11T22:59:10Z",
        "link": "http://arxiv.org/abs/hep-lat/0004007v4",
        "categories": [
            "hep-lat",
            "cs.DC",
            "cs.MS",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Design and Evaluation of Mechanisms for a Multicomputer Object Store",
        "authors": [
            "Lex Weaver"
        ],
        "summary": "Multicomputers have traditionally been viewed as powerful compute engines. It is from this perspective that they have been applied to various problems in order to achieve significant performance gains. There are many applications for which this compute intensive approach is only a partial solution. CAD, virtual reality, simulation, document management and analysis all require timely access to large amounts of data. This thesis investigates the use of the object store paradigm to harness the large distributed memories found on multicomputers. The design, implementation, and evaluation of a distributed object server on the Fujitsu AP1000 is described. The performance of the distributed object server under example applications, mainly physical simulation problems, is used to evaluate solutions to the problems of client space recovery, object migration, and coherence maintenance.   The distributed object server follows the client-server model, allows object replication, and uses binary semaphores as a concurrency control measure. Instrumentation of the server under these applications supports several conclusions: client space recovery should be dynamically controlled by the application, predictively prefetching object replicas yields benefits in restricted circumstances, object migration by storage unit (segment) is not generally suitable where there are many objects per storage unit, and binary semaphores are an expensive concurrency control measure in this environment.",
        "published": "2000-04-18T17:29:06Z",
        "link": "http://arxiv.org/abs/cs/0004010v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H.3.4; H.2.4"
        ]
    },
    {
        "title": "Sorting Integers on the AP1000",
        "authors": [
            "Lex Weaver",
            "Andrew Lynes"
        ],
        "summary": "Sorting is one of the classic problems of computer science. Whilst well understood on sequential machines, the diversity of architectures amongst parallel systems means that algorithms do not perform uniformly on all platforms. This document describes the implementation of a radix based algorithm for sorting positive integers on a Fujitsu AP1000 Supercomputer, which was constructed as an entry in the Joint Symposium on Parallel Processing (JSPP) 1994 Parallel Software Contest (PSC94). Brief consideration is also given to a full radix sort conducted in parallel across the machine.",
        "published": "2000-04-23T04:32:18Z",
        "link": "http://arxiv.org/abs/cs/0004013v1",
        "categories": [
            "cs.DC",
            "C.1.4"
        ]
    },
    {
        "title": "Cluster Computing White Paper",
        "authors": [
            "Mark Baker"
        ],
        "summary": "Cluster computing is not a new area of computing. It is, however, evident that there is a growing interest in its usage in all areas where applications have traditionally used parallel or distributed computing platforms. The growing interest has been fuelled in part by the availability of powerful microprocessors and high-speed networks as off-the-shelf commodity components as well as in part by the rapidly maturing software components available to support high performance and high availability applications.   This White Paper has been broken down into eleven sections, each of which has been put together by academics and industrial researchers who are both experts in their fields and where willing to volunteer their time and effort to put together this White Paper. The status of this paper is draft and we are at the stage of publicizing its presence and making a Request For Comments (RFC).",
        "published": "2000-04-25T18:55:40Z",
        "link": "http://arxiv.org/abs/cs/0004014v2",
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.NI",
            "B.4.1 B.4.3 C.0 C.1.2 C.1.4 C.2 C.4 D.1.3 D.1.5 D.2.11 D.3.1 D.4.1\n  D.4.6"
        ]
    },
    {
        "title": "A One-Time Pad based Cipher for Data Protection in Distributed   Environments",
        "authors": [
            "Igor Sobrado"
        ],
        "summary": "A one-time pad (OTP) based cipher to insure both data protection and integrity when mobile code arrives to a remote host is presented. Data protection is required when a mobile agent could retrieve confidential information that would be encrypted in untrusted nodes of the network; in this case, information management could not rely on carrying an encryption key. Data integrity is a prerequisite because mobile code must be protected against malicious hosts that, by counterfeiting or removing collected data, could cover information to the server that has sent the agent. The algorithm described in this article seems to be simple enough, so as to be easily implemented. This scheme is based on a non-interactive protocol and allows a remote host to change its own data on-the-fly and, at the same time, protecting information against handling by other hosts.",
        "published": "2000-05-24T22:24:59Z",
        "link": "http://arxiv.org/abs/cs/0005026v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.IR",
            "cs.NI",
            "C.2.1; C.2.4; H.3; H.3.4"
        ]
    },
    {
        "title": "A Note on \"Optimal Static Load Balancing in Distributed Computer   Systems\"",
        "authors": [
            "S. A. Mondal"
        ],
        "summary": "The problem of minimizing mean response time of generic jobs submitted to a heterogenous distributed computer systems is considered in this paper. A static load balancing strategy, in which decision of redistribution of loads does not depend on the state of the system, is used for this purpose. The article is closely related to a previous article on the same topic. The present article points out number of inconsistencies in the previous article, provides a new formulation, and discusses the impact of new findings, based on the improved formulation, on the results of the previous article.",
        "published": "2000-06-02T03:14:07Z",
        "link": "http://arxiv.org/abs/cs/0006004v1",
        "categories": [
            "cs.DC",
            "C.4 Performance of Systems: modeling techniques; D.4.8 Perfomance:\n  modeling and prediction"
        ]
    },
    {
        "title": "Performing work efficiently in the presence of faults",
        "authors": [
            "Cynthia Dwork",
            "Joseph Y. Halpern",
            "O. Waarts"
        ],
        "summary": "We consider a system of t synchronous processes that communicate only by sending messages to one another, and that together must perform $n$ independent units of work. Processes may fail by crashing; we want to guarantee that in every execution of the protocol in which at least one process survives, all n units of work will be performed. We consider three parameters: the number of messages sent, the total number of units of work performed (including multiplicities), and time. We present three protocols for solving the problem. All three are work-optimal, doing O(n+t) work. The first has moderate costs in the remaining two parameters, sending O(t\\sqrt{t}) messages, and taking O(n+t) time. This protocol can be easily modified to run in any completely asynchronous system equipped with a failure detection mechanism. The second sends only O(t log{t}) messages, but its running time is large (exponential in n and t). The third is essentially time-optimal in the (usual) case in which there are no failures, and its time complexity degrades gracefully as the number of failures increases.",
        "published": "2000-06-02T18:35:55Z",
        "link": "http://arxiv.org/abs/cs/0006008v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Knowledge and common knowledge in a distributed environment",
        "authors": [
            "Joseph Y. Halpern",
            "Yoram Moses"
        ],
        "summary": "Reasoning about knowledge seems to play a fundamental role in distributed systems. Indeed, such reasoning is a central part of the informal intuitive arguments used in the design of distributed protocols. Communication in a distributed system can be viewed as the act of transforming the system's state of knowledge. This paper presents a general framework for formalizing and reasoning about knowledge in distributed systems. We argue that states of knowledge of groups of processors are useful concepts for the design and analysis of distributed protocols. In particular, distributed knowledge corresponds to knowledge that is ``distributed'' among the members of the group, while common knowledge corresponds to a fact being ``publicly known''. The relationship between common knowledge and a variety of desirable actions in a distributed system is illustrated. Furthermore, it is shown that, formally speaking, in practical systems common knowledge cannot be attained. A number of weaker variants of common knowledge that are attainable in many cases of interest are introduced and investigated.",
        "published": "2000-06-02T18:43:33Z",
        "link": "http://arxiv.org/abs/cs/0006009v1",
        "categories": [
            "cs.DC",
            "cs.AI",
            "C.2.2, C.2.4, D.2.4, I.2.4, F.3.1, F.3.1"
        ]
    },
    {
        "title": "The X-Files: Investigating Alien Performance in a Thin-client World",
        "authors": [
            "Neil J. Gunther"
        ],
        "summary": "Many scientific applications use the X11 window environment; an open source windows GUI standard employing a client/server architecture. X11 promotes: distributed computing, thin-client functionality, cheap desktop displays, compatibility with heterogeneous servers, remote services and administration, and greater maturity than newer web technologies. This paper details the author's investigations into close encounters with alien performance in X11-based seismic applications running on a 200-node cluster, backed by 2 TB of mass storage. End-users cited two significant UFOs (Unidentified Faulty Operations) i) long application launch times and ii) poor interactive response times. The paper is divided into three major sections describing Close Encounters of the 1st Kind: citings of UFO experiences, the 2nd Kind: recording evidence of a UFO, and the 3rd Kind: contact and analysis. UFOs do exist and this investigation presents a real case study for evaluating workload analysis and other diagnostic tools.",
        "published": "2000-06-08T18:08:19Z",
        "link": "http://arxiv.org/abs/cs/0006016v1",
        "categories": [
            "cs.PF",
            "cs.DC",
            "C.2.4;C.4;D.4.8;D.4.9;H.3.4;H.5.2;I.6.8"
        ]
    },
    {
        "title": "Multiagent Control of Self-reconfigurable Robots",
        "authors": [
            "Hristo Bojinov",
            "Arancha Casal",
            "Tad Hogg"
        ],
        "summary": "We demonstrate how multiagent systems provide useful control techniques for modular self-reconfigurable (metamorphic) robots. Such robots consist of many modules that can move relative to each other, thereby changing the overall shape of the robot to suit different tasks. Multiagent control is particularly well-suited for tasks involving uncertain and changing environments. We illustrate this approach through simulation experiments of Proteo, a metamorphic robot system currently under development.",
        "published": "2000-06-20T20:17:44Z",
        "link": "http://arxiv.org/abs/cs/0006030v2",
        "categories": [
            "cs.RO",
            "cs.DC",
            "cs.MA",
            "I.2.9; I.2.11; H.3.4"
        ]
    },
    {
        "title": "Phase Clocks for Transient Fault Repair",
        "authors": [
            "Ted Herman"
        ],
        "summary": "Phase clocks are synchronization tools that implement a form of logical time in distributed systems. For systems tolerating transient faults by self-repair of damaged data, phase clocks can enable reasoning about the progress of distributed repair procedures. This paper presents a phase clock algorithm suited to the model of transient memory faults in asynchronous systems with read/write registers. The algorithm is self-stabilizing and guarantees accuracy of phase clocks within O(k) time following an initial state that is k-faulty. Composition theorems show how the algorithm can be used for the timing of distributed procedures that repair system outputs.",
        "published": "2000-07-10T15:59:03Z",
        "link": "http://arxiv.org/abs/cs/0007015v1",
        "categories": [
            "cs.DC",
            "C.2.4; D.4.5"
        ]
    },
    {
        "title": "Cluster Computing: A High-Performance Contender",
        "authors": [
            "Mark Baker",
            "Rajkumar Buyya",
            "Dan Hyde"
        ],
        "summary": "When you first heard people speak of Piles of PCs, the first thing that came to mind may have been a cluttered computer room with processors, monitors, and snarls of cables all around. Collections of computers have undoubtedly become more sophisticated than in the early days of shared drives and modem connections. No matter what you call them, Clusters of Workstations (COW), Networks of Workstations (NOW), Workstation Clusters (WCs), Clusters of PCs (CoPs), clusters of computers are now filling the processing niche once occupied by more powerful stand-alone machines. This article discusses the need for cluster computing technology, Technologies, Components, and Applications, Supercluster Systems and Issues, The Need for a New Task Force, and Cluster Computing Educational Resources.",
        "published": "2000-09-22T10:39:30Z",
        "link": "http://arxiv.org/abs/cs/0009020v1",
        "categories": [
            "cs.DC",
            "cs.AR",
            "C0"
        ]
    },
    {
        "title": "Nimrod/G: An Architecture of a Resource Management and Scheduling System   in a Global Computational Grid",
        "authors": [
            "Rajkumar Buyya",
            "David Abramson",
            "Jon Giddy"
        ],
        "summary": "The availability of powerful microprocessors and high-speed networks as commodity components has enabled high performance computing on distributed systems (wide-area cluster computing). In this environment, as the resources are usually distributed geographically at various levels (department, enterprise, or worldwide) there is a great challenge in integrating, coordinating and presenting them as a single resource to the user; thus forming a computational grid. Another challenge comes from the distributed ownership of resources with each resource having its own access policy, cost, and mechanism.   The proposed Nimrod/G grid-enabled resource management and scheduling system builds on our earlier work on Nimrod and follows a modular and component-based architecture enabling extensibility, portability, ease of development, and interoperability of independently developed components. It uses the Globus toolkit services and can be easily extended to operate with any other emerging grid middleware services. It focuses on the management and scheduling of computations over dynamic resources scattered geographically across the Internet at department, enterprise, or global level with particular emphasis on developing scheduling schemes based on the concept of computational economy for a real test bed, namely, the Globus testbed (GUSTO).",
        "published": "2000-09-22T10:44:16Z",
        "link": "http://arxiv.org/abs/cs/0009021v1",
        "categories": [
            "cs.DC",
            "C0"
        ]
    },
    {
        "title": "A Public-key based Information Management Model for Mobile Agents",
        "authors": [
            "Diego Rodriguez",
            "Igor Sobrado"
        ],
        "summary": "Mobile code based computing requires development of protection schemes that allow digital signature and encryption of data collected by the agents in untrusted hosts. These algorithms could not rely on carrying encryption keys if these keys could be stolen or used to counterfeit data by hostile hosts and agents. As a consequence, both information and keys must be protected in a way that only authorized hosts, that is the host that provides information and the server that has sent the mobile agent, could modify (by changing or removing) retrieved data. The data management model proposed in this work allows the information collected by the agents to be protected against handling by other hosts in the information network. It has been done by using standard public-key cryptography modified to support protection of data in distributed environments without requiring an interactive protocol with the host that has dropped the agent. Their significance stands on the fact that it is the first model that supports a full-featured protection of mobile agents allowing remote hosts to change its own information if required before agent returns to its originating server.",
        "published": "2000-10-09T20:17:35Z",
        "link": "http://arxiv.org/abs/cs/0010013v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.IR",
            "cs.NI",
            "C.2.1; C.2.4; H.3; H.3.4"
        ]
    },
    {
        "title": "Available Stabilizing Heaps",
        "authors": [
            "Ted Herman",
            "Toshimitsu Masuzawa"
        ],
        "summary": "This paper describes a heap construction that supports insert and delete operations in arbitrary (possibly illegitimate) states. After any sequence of at most O(m) heap operations, the heap state is guarantee to be legitimate, where m is the initial number of items in the heap. The response from each operation is consistent with its effect on the data structure, even for illegitimate states. The time complexity of each operation is O(lg K) where K is the capacity of the data structure; when the heap's state is legitimate the time complexity is O(lg n) for n equal to the number items in the heap.",
        "published": "2000-11-30T23:14:23Z",
        "link": "http://arxiv.org/abs/cs/0011046v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "C.2.4; D.1.3; D.3.3; E.2; C.4"
        ]
    },
    {
        "title": "Available and Stabilizing 2-3 Trees",
        "authors": [
            "Ted Herman",
            "Toshimitsu Masuzawa"
        ],
        "summary": "Transient faults corrupt the content and organization of data structures. A recovery technique dealing with such faults is stabilization, which guarantees, following some number of operations on the data structure, that content of the data structure is legitimate. Another notion of fault tolerance is availability, which is the property that operations continue to be applied during the period of recovery after a fault, and successful updates are not lost while the data structure stabilizes to a legitimate state. The available, stabilizing 2-3 tree supports find, insert, and delete operations, each with O(lg n) complexity when the tree's state is legitimate and contains n items. For an illegitimate state, these operations have O(lg K) complexity where K is the maximum capacity of the tree. Within O(t) operations, the state of the tree is guaranteed to be legitimate, where t is the number of nodes accessible via some path from the tree's root at the initial state. This paper resolves, for the first time, issues of dynamic allocation and pointer organization in a stabilizing data structure.",
        "published": "2000-12-01T15:55:50Z",
        "link": "http://arxiv.org/abs/cs/0012001v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "D.4.5; E.1; C.4"
        ]
    },
    {
        "title": "Performance and Scalability Models for a Hypergrowth e-Commerce Web Site",
        "authors": [
            "Neil J. Gunther"
        ],
        "summary": "The performance of successful Web-based e-commerce services has all the allure of a roller-coaster ride: accelerated fiscal growth combined with the ever-present danger of running out of server capacity. This chapter presents a case study based on the author's own capacity planning engagement with one of the hottest e-commerce Web sites in the world. Several spreadsheet techniques are presented for forecasting both short-term and long-term trends in the consumption of server capacity. Two new performance metrics are introduced for site planning and procurement: the effective demand, and the doubling period.",
        "published": "2000-12-26T22:42:39Z",
        "link": "http://arxiv.org/abs/cs/0012022v1",
        "categories": [
            "cs.PF",
            "cs.DC",
            "cs.SE",
            "C.4;D.2.8;D.4.8;H.3.5"
        ]
    },
    {
        "title": "Byzantine Agreement with Faulty Majority using Bounded Broadcast",
        "authors": [
            "Jeffrey Considine",
            "Leonid A. Levin",
            "David Metcalf"
        ],
        "summary": "Byzantine Agreement introduced in [Pease, Shostak, Lamport, 80] is a widely used building block of reliable distributed protocols. It simulates broadcast despite the presence of faulty parties within the network, traditionally using only private unicast links. Under such conditions, Byzantine Agreement requires more than 2/3 of the parties to be compliant. [Fitzi, Maurer, 00], constructed a Byzantine Agreement protocol for any compliant majority based on an additional primitive allowing transmission to any two parties simultaneously. They proposed a problem of generalizing these results to wider channels and fewer compliant parties. We prove that 2f < kh condition is necessary and sufficient for implementing broadcast with h compliant and f faulty parties using k-cast channels.",
        "published": "2000-12-26T23:05:58Z",
        "link": "http://arxiv.org/abs/cs/0012024v4",
        "categories": [
            "cs.DC",
            "F.1.2"
        ]
    },
    {
        "title": "Centroid-based summarization of multiple documents: sentence extraction,   utility-based evaluation, and user studies",
        "authors": [
            "Dragomir R. Radev",
            "Hongyan Jing",
            "Malgorzata Budzikowska"
        ],
        "summary": "We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.",
        "published": "2000-05-12T17:24:06Z",
        "link": "http://arxiv.org/abs/cs/0005020v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DL",
            "cs.HC",
            "cs.IR",
            "H.3.1; H.3.4; H.3.7; H.5.2; I.2.7"
        ]
    },
    {
        "title": "A method for command identification, using modified collision free   hashing with addition & rotation iterative hash functions (part 1)",
        "authors": [
            "Dimitrios Skraparlis"
        ],
        "summary": "This paper proposes a method for identification of a user`s fixed string set (which can be a command/instruction set for a terminal or microprocessor). This method is fast and has very small memory requirements, compared to a traditional full string storage and compare method. The user feeds characters into a microcontroller via a keyboard or another microprocessor sends commands and the microcontroller hashes the input in order to identify valid commands, ensuring no collisions between hashed valid strings, while applying further criteria to narrow collision between random and valid strings. The method proposed narrows the possibility of the latter kind of collision, achieving small code and memory-size utilization and very fast execution. Hashing is achieved using additive & rotating hash functions in an iterative form, which can be very easily implemented in simple microcontrollers and microprocessors. Such hash functions are presented and compared according to their efficiency for a given string/command set, using the program found in the appendix.",
        "published": "2000-05-27T01:08:07Z",
        "link": "http://arxiv.org/abs/cs/0005028v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "B.4.2; H.5.2"
        ]
    },
    {
        "title": "Accuracy, Coverage, and Speed: What Do They Mean to Users?",
        "authors": [
            "Frankie James",
            "Manny Rayner",
            "Beth Ann Hockey"
        ],
        "summary": "Speech is becoming increasingly popular as an interface modality, especially in hands- and eyes-busy situations where the use of a keyboard or mouse is difficult. However, despite the fact that many have hailed speech as being inherently usable (since everyone already knows how to talk), most users of speech input are left feeling disappointed by the quality of the interaction. Clearly, there is much work to be done on the design of usable spoken interfaces. We believe that there are two major problems in the design of speech interfaces, namely, (a) the people who are currently working on the design of speech interfaces are, for the most part, not interface designers and therefore do not have as much experience with usability issues as we in the CHI community do, and (b) speech, as an interface modality, has vastly different properties than other modalities, and therefore requires different usability measures.",
        "published": "2000-06-09T18:10:58Z",
        "link": "http://arxiv.org/abs/cs/0006018v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "H.5.2; I.2.7"
        ]
    },
    {
        "title": "Verbal Interactions in Virtual Worlds",
        "authors": [
            "Pierre Nugues"
        ],
        "summary": "We first discuss respective advantages of language interaction in virtual worlds and of using 3D images in dialogue systems. Then, we describe an example of a verbal interaction system in virtual reality: Ulysse. Ulysse is a conversational agent that helps a user navigate in virtual worlds. It has been designed to be embedded in the representation of a participant of a virtual conference and it responds positively to motion orders. Ulysse navigates the user's viewpoint on his/her behalf in the virtual world. On tests we carried out, we discovered that users, novices as well as experienced ones have difficulties moving in a 3D environment. Agents such as Ulysse enable a user to carry out navigation motions that would have been impossible with classical interaction devices. From the whole Ulysse system, we have stripped off a skeleton architecture that we have ported to VRML, Java, and Prolog. We hope this skeleton helps the design of language applications in virtual worlds.",
        "published": "2000-06-13T09:50:03Z",
        "link": "http://arxiv.org/abs/cs/0006027v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "H.5.2; I.2.7"
        ]
    },
    {
        "title": "Estimation of English and non-English Language Use on the WWW",
        "authors": [
            "Gregory Grefenstette",
            "Julien Nioche"
        ],
        "summary": "The World Wide Web has grown so big, in such an anarchic fashion, that it is difficult to describe. One of the evident intrinsic characteristics of the World Wide Web is its multilinguality. Here, we present a technique for estimating the size of a language-specific corpus given the frequency of commonly occurring words in the corpus. We apply this technique to estimating the number of words available through Web browsers for given languages. Comparing data from 1996 to data from 1999 and 2000, we calculate the growth of a number of European languages on the Web. As expected, non-English languages are growing at a faster pace than English, though the position of English is still dominant.",
        "published": "2000-06-23T09:43:27Z",
        "link": "http://arxiv.org/abs/cs/0006032v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "J.5; H.5.2; I.2.7; H.3.5; H.5.3"
        ]
    },
    {
        "title": "Data sonification and sound visualization",
        "authors": [
            "Hans G. Kaper",
            "Sever Tipei",
            "Elizabeth Wiebel"
        ],
        "summary": "This article describes a collaborative project between researchers in the Mathematics and Computer Science Division at Argonne National Laboratory and the Computer Music Project of the University of Illinois at Urbana-Champaign. The project focuses on the use of sound for the exploration and analysis of complex data sets in scientific computing. The article addresses digital sound synthesis in the context of DIASS (Digital Instrument for Additive Sound Synthesis) and sound visualization in a virtual-reality environment by means of M4CAVE. It describes the procedures and preliminary results of some experiments in scientific sonification and sound visualization.",
        "published": "2000-07-05T21:26:48Z",
        "link": "http://arxiv.org/abs/cs/0007007v1",
        "categories": [
            "cs.SD",
            "cs.HC",
            "cs.MM",
            "H.5.5"
        ]
    },
    {
        "title": "On a cepstrum-based speech detector robust to white noise",
        "authors": [
            "Sergei Skorik",
            "Frederic Berthommier"
        ],
        "summary": "We study effects of additive white noise on the cepstral representation of speech signals. Distribution of each individual cepstrum coefficient of speech is shown to depend strongly on noise and to overlap significantly with the cepstrum distribution of noise. Based on these studies, we suggest a scalar quantity, V, equal to the sum of weighted cepstral coefficients, which is able to classify frames containing speech against noise-like frames. The distributions of V for speech and noise frames are reasonably well separated above SNR = 5 dB, demonstrating the feasibility of robust speech detector based on V.",
        "published": "2000-10-10T17:33:02Z",
        "link": "http://arxiv.org/abs/cs/0010014v1",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.HC",
            "I.2.7; I.2.1; I.2.10; H.5.5"
        ]
    },
    {
        "title": "Tree-gram Parsing: Lexical Dependencies and Structural Relations",
        "authors": [
            "Khalil Sima'an"
        ],
        "summary": "This paper explores the kinds of probabilistic relations that are important in syntactic disambiguation. It proposes that two widely used kinds of relations, lexical dependencies and structural relations, have complementary disambiguation capabilities. It presents a new model based on structural relations, the Tree-gram model, and reports experiments showing that structural relations should benefit from enrichment by lexical dependencies.",
        "published": "2000-11-06T13:56:42Z",
        "link": "http://arxiv.org/abs/cs/0011007v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "I.2; K.3.2; J.5"
        ]
    }
]