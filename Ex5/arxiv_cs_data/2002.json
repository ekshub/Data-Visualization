[
    {
        "title": "Sharpening Occam's Razor",
        "authors": [
            "Ming Li",
            "John Tromp",
            "Paul Vitanyi"
        ],
        "summary": "We provide a new representation-independent formulation of Occam's razor theorem, based on Kolmogorov complexity. This new formulation allows us to:   (i) Obtain better sample complexity than both length-based and VC-based versions of Occam's razor theorem, in many applications.   (ii) Achieve a sharper reverse of Occam's razor theorem than previous work.   Specifically, we weaken the assumptions made in an earlier publication, and extend the reverse to superpolynomial running times.",
        "published": "2002-01-08T16:44:10Z",
        "link": "http://arxiv.org/abs/cs/0201005v2",
        "categories": [
            "cs.LG",
            "cond-mat.dis-nn",
            "cs.AI",
            "cs.CC",
            "math.PR",
            "physics.data-an",
            "F.2, E.4, I.2"
        ]
    },
    {
        "title": "Computing Preferred Answer Sets by Meta-Interpretation in Answer Set   Programming",
        "authors": [
            "Thomas Eiter",
            "Wolfgang Faber",
            "Nicola Leone",
            "Gerald Pfeifer"
        ],
        "summary": "Most recently, Answer Set Programming (ASP) is attracting interest as a new paradigm for problem solving. An important aspect which needs to be supported is the handling of preferences between rules, for which several approaches have been presented. In this paper, we consider the problem of implementing preference handling approaches by means of meta-interpreters in Answer Set Programming. In particular, we consider the preferred answer set approaches by Brewka and Eiter, by Delgrande, Schaub and Tompits, and by Wang, Zhou and Lin. We present suitable meta-interpreters for these semantics using DLV, which is an efficient engine for ASP. Moreover, we also present a meta-interpreter for the weakly preferred answer set approach by Brewka and Eiter, which uses the weak constraint feature of DLV as a tool for expressing and solving an underlying optimization problem. We also consider advanced meta-interpreters, which make use of graph-based characterizations and often allow for more efficient computations. Our approach shows the suitability of ASP in general and of DLV in particular for fast prototyping. This can be fruitfully exploited for experimenting with new languages and knowledge-representation formalisms.",
        "published": "2002-01-16T20:00:46Z",
        "link": "http://arxiv.org/abs/cs/0201013v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Collusion in Unrepeated, First-Price Auctions with an Uncertain Number   of Participants",
        "authors": [
            "Kevin Leyton-Brown",
            "Moshe Tennenholtz",
            "Navin Bhat",
            "Yoav Shoham"
        ],
        "summary": "We consider the question of whether collusion among bidders (a \"bidding ring\") can be supported in equilibrium of unrepeated first-price auctions. Unlike previous work on the topic such as that by McAfee and McMillan [1992] and Marshall and Marx [2007], we do not assume that non-colluding agents have perfect knowledge about the number of colluding agents whose bids are suppressed by the bidding ring, and indeed even allow for the existence of multiple cartels. Furthermore, while we treat the association of bidders with bidding rings as exogenous, we allow bidders to make strategic decisions about whether to join bidding rings when invited. We identify a bidding ring protocol that results in an efficient allocation in Bayes{Nash equilibrium, under which non-colluding agents bid straightforwardly, and colluding agents join bidding rings when invited and truthfully declare their valuations to the ring center. We show that bidding rings benefit ring centers and all agents, both members and non-members of bidding rings, at the auctioneer's expense. The techniques we introduce in this paper may also be useful for reasoning about other problems in which agents have asymmetric information about a setting.",
        "published": "2002-01-19T00:12:50Z",
        "link": "http://arxiv.org/abs/cs/0201017v2",
        "categories": [
            "cs.GT",
            "cs.AI",
            "I.2.11; J.4"
        ]
    },
    {
        "title": "A Modal Logic Framework for Multi-agent Belief Fusion",
        "authors": [
            "Churn-Jung Liau"
        ],
        "summary": "This paper is aimed at providing a uniform framework for reasoning about beliefs of multiple agents and their fusion. In the first part of the paper, we develop logics for reasoning about cautiously merged beliefs of agents with different degrees of reliability. The logics are obtained by combining the multi-agent epistemic logic and multi-sources reasoning systems. Every ordering for the reliability of the agents is represented by a modal operator, so we can reason with the merged results under different situations. The fusion is cautious in the sense that if an agent's belief is in conflict with those of higher priorities, then his belief is completely discarded from the merged result. We consider two strategies for the cautious merging of beliefs. In the first one, if inconsistency occurs at some level, then all beliefs at the lower levels are discarded simultaneously, so it is called level cutting strategy. For the second one, only the level at which the inconsistency occurs is skipped, so it is called level skipping strategy. The formal semantics and axiomatic systems for these two strategies are presented. In the second part, we extend the logics both syntactically and semantically to cover some more sophisticated belief fusion and revision operators. While most existing approaches treat belief fusion operators as meta-level constructs, these operators are directly incorporated into our object logic language. Thus it is possible to reason not only with the merged results but also about the fusion process in our logics. The relationship of our extended logics with the conditional logics of belief revision is also discussed.",
        "published": "2002-01-23T02:43:01Z",
        "link": "http://arxiv.org/abs/cs/0201020v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4;F.4.1"
        ]
    },
    {
        "title": "A theory of experiment",
        "authors": [
            "Pierre Albarede"
        ],
        "summary": "This article aims at clarifying the language and practice of scientific experiment, mainly by hooking observability on calculability.",
        "published": "2002-01-23T19:53:08Z",
        "link": "http://arxiv.org/abs/cs/0201022v2",
        "categories": [
            "cs.AI",
            "I.2.4;I.2.6"
        ]
    },
    {
        "title": "The Deductive Database System LDL++",
        "authors": [
            "Faiz Arni",
            "KayLiang Ong",
            "Shalom Tsur",
            "Haixun Wang",
            "Carlo Zaniolo"
        ],
        "summary": "This paper describes the LDL++ system and the research advances that have enabled its design and development. We begin by discussing the new nonmonotonic and nondeterministic constructs that extend the functionality of the LDL++ language, while preserving its model-theoretic and fixpoint semantics. Then, we describe the execution model and the open architecture designed to support these new constructs and to facilitate the integration with existing DBMSs and applications. Finally, we describe the lessons learned by using LDL++ on various tested applications, such as middleware and datamining.",
        "published": "2002-02-01T05:00:24Z",
        "link": "http://arxiv.org/abs/cs/0202001v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "D.3.2"
        ]
    },
    {
        "title": "Semiclassical Neural Network",
        "authors": [
            "Fariel Shafee"
        ],
        "summary": "We have constructed a simple semiclassical model of neural network where neurons have quantum links with one another in a chosen way and affect one another in a fashion analogous to action potentials. We have examined the role of stochasticity introduced by the quantum potential and compare the system with the classical system of an integrate-and-fire model by Hopfield. Average periodicity and short term retentivity of input memory are noted.",
        "published": "2002-02-02T16:39:54Z",
        "link": "http://arxiv.org/abs/quant-ph/0202015v3",
        "categories": [
            "quant-ph",
            "cond-mat.dis-nn",
            "cs.AI",
            "q-bio"
        ]
    },
    {
        "title": "Neural Networks with c-NOT Gated Nodes",
        "authors": [
            "Fariel Shafee"
        ],
        "summary": "We try to design a quantum neural network with qubits instead of classical neurons with deterministic states, and also with quantum operators replacing teh classical action potentials. With our choice of gates interconnecting teh neural lattice, it appears that the state of the system behaves in ways reflecting both the strengths of coupling between neurons as well as initial conditions. We find that depending whether there is a threshold for emission from excited to ground state, the system shows either aperiodic oscillations or coherent ones with periodicity depending on the strength of coupling.",
        "published": "2002-02-02T18:05:45Z",
        "link": "http://arxiv.org/abs/quant-ph/0202016v2",
        "categories": [
            "quant-ph",
            "cond-mat.dis-nn",
            "cs.AI",
            "q-bio"
        ]
    },
    {
        "title": "A Qualitative Dynamical Modelling Approach to Capital Accumulation in   Unregulated Fisheries",
        "authors": [
            "K. Eisenack",
            "H. Welsch",
            "J. P. Kropp"
        ],
        "summary": "Capital accumulation has been a major issue in fisheries economics over the last two decades, whereby the interaction of the fish and capital stocks were of particular interest. Because bio-economic systems are intrinsically complex, previous efforts in this field have relied on a variety of simplifying assumptions. The model presented here relaxes some of these simplifications. Problems of tractability are surmounted by using the methodology of qualitative differential equations (QDE). The theory of QDEs takes into account that scientific knowledge about particular fisheries is usually limited, and facilitates an analysis of the global dynamics of systems with more than two ordinary differential equations. The model is able to trace the evolution of capital and fish stock in good agreement with observed patterns, and shows that over-capitalization is unavoidable in unregulated fisheries.",
        "published": "2002-02-05T17:50:56Z",
        "link": "http://arxiv.org/abs/cs/0202004v3",
        "categories": [
            "cs.AI",
            "cs.CE",
            "G.1.6; I.2.3; I.2.8; I.6.3; I.6.1; I.6.3"
        ]
    },
    {
        "title": "Steady State Resource Allocation Analysis of the Stochastic Diffusion   Search",
        "authors": [
            "Slawomir J. Nasuto",
            "Mark J. Bishop"
        ],
        "summary": "This article presents the long-term behaviour analysis of Stochastic Diffusion Search (SDS), a distributed agent-based system for best-fit pattern matching. SDS operates by allocating simple agents into different regions of the search space. Agents independently pose hypotheses about the presence of the pattern in the search space and its potential distortion. Assuming a compositional structure of hypotheses about pattern matching agents perform an inference on the basis of partial evidence from the hypothesised solution. Agents posing mutually consistent hypotheses about the pattern support each other and inhibit agents with inconsistent hypotheses. This results in the emergence of a stable agent population identifying the desired solution. Positive feedback via diffusion of information between the agents significantly contributes to the speed with which the solution population is formed. The formulation of the SDS model in terms of interacting Markov Chains enables its characterisation in terms of the allocation of agents, or computational resources. The analysis characterises the stationary probability distribution of the activity of agents, which leads to the characterisation of the solution population in terms of its similarity to the target pattern.",
        "published": "2002-02-07T14:11:18Z",
        "link": "http://arxiv.org/abs/cs/0202007v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.8; I.2.11; F.2.2; G.3"
        ]
    },
    {
        "title": "Logic program specialisation through partial deduction: Control issues",
        "authors": [
            "Michael Leuschel",
            "Maurice Bruynooghe"
        ],
        "summary": "Program specialisation aims at improving the overall performance of programs by performing source to source transformations. A common approach within functional and logic programming, known respectively as partial evaluation and partial deduction, is to exploit partial knowledge about the input. It is achieved through a well-automated application of parts of the Burstall-Darlington unfold/fold transformation framework. The main challenge in developing systems is to design automatic control that ensures correctness, efficiency, and termination. This survey and tutorial presents the main developments in controlling partial deduction over the past 10 years and analyses their respective merits and shortcomings. It ends with an assessment of current achievements and sketches some remaining research challenges.",
        "published": "2002-02-12T14:16:53Z",
        "link": "http://arxiv.org/abs/cs/0202012v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.1.6; D.1.2; I.2.2; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Linear Programming helps solving large multi-unit combinatorial auctions",
        "authors": [
            "Rica Gonen",
            "Daniel Lehmann"
        ],
        "summary": "Previous works suggested the use of Branch and Bound techniques for finding the optimal allocation in (multi-unit) combinatorial auctions. They remarked that Linear Programming could provide a good upper-bound to the optimal allocation, but they went on using lighter and less tight upper-bound heuristics, on the ground that LP was too time-consuming to be used repetitively to solve large combinatorial auctions. We present the results of extensive experiments solving large (multi-unit) combinatorial auctions generated according to distributions proposed by different researchers. Our surprising conclusion is that Linear Programming is worth using. Investing almost all of one's computing time in using LP to bound from above the value of the optimal solution in order to prune aggressively pays off. We present a way to save on the number of calls to the LP routine and experimental results comparing different heuristics for choosing the bid to be considered next. Those results show that the ordering based on the square root of the size of the bids that was shown to be theoretically optimal in a previous paper by the authors performs surprisingly better than others in practice. Choosing to deal first with the bid with largest coefficient (typically 1) in the optimal solution of the relaxed LP problem, is also a good choice. The gap between the lower bound provided by greedy heuristics and the upper bound provided by LP is typically small and pruning is therefore extensive. For most distributions, auctions of a few hundred goods among a few thousand bids can be solved in practice. All experiments were run on a PC under Matlab.",
        "published": "2002-02-15T12:09:14Z",
        "link": "http://arxiv.org/abs/cs/0202016v1",
        "categories": [
            "cs.GT",
            "cs.AI",
            "G.1.6;I.2.8"
        ]
    },
    {
        "title": "Nonmonotonic Logics and Semantics",
        "authors": [
            "Daniel Lehmann"
        ],
        "summary": "Tarski gave a general semantics for deductive reasoning: a formula a may be deduced from a set A of formulas iff a holds in all models in which each of the elements of A holds. A more liberal semantics has been considered: a formula a may be deduced from a set A of formulas iff a holds in all of the \"preferred\" models in which all the elements of A hold. Shoham proposed that the notion of \"preferred\" models be defined by a partial ordering on the models of the underlying language. A more general semantics is described in this paper, based on a set of natural properties of choice functions. This semantics is here shown to be equivalent to a semantics based on comparing the relative \"importance\" of sets of models, by what amounts to a qualitative probability measure. The consequence operations defined by the equivalent semantics are then characterized by a weakening of Tarski's properties in which the monotonicity requirement is replaced by three weaker conditions. Classical propositional connectives are characterized by natural introduction-elimination rules in a nonmonotonic setting. Even in the nonmonotonic setting, one obtains classical propositional logic, thus showing that monotonicity is not required to justify classical propositional connectives.",
        "published": "2002-02-15T12:49:11Z",
        "link": "http://arxiv.org/abs/cs/0202018v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "math.LO",
            "I.2.3"
        ]
    },
    {
        "title": "The Mysterious Optimality of Naive Bayes: Estimation of the Probability   in the System of \"Classifiers\"",
        "authors": [
            "Oleg Kupervasser",
            "Alexsander Vardy"
        ],
        "summary": "Bayes Classifiers are widely used currently for recognition, identification and knowledge discovery. The fields of application are, for example, image processing, medicine, chemistry (QSAR). But by mysterious way the Naive Bayes Classifier usually gives a very nice and good presentation of a recognition. It can not be improved considerably by more complex models of Bayes Classifier. We demonstrate here a very nice and simple proof of the Naive Bayes Classifier optimality, that can explain this interesting fact.The derivation in the current paper is based on arXiv:cs/0202020v1",
        "published": "2002-02-17T14:55:47Z",
        "link": "http://arxiv.org/abs/cs/0202020v3",
        "categories": [
            "cs.CV",
            "cs.AI",
            "E.5; E.4; E.2; H.1.1; F.1.1; F.1.3"
        ]
    },
    {
        "title": "Nonmonotonic Reasoning, Preferential Models and Cumulative Logics",
        "authors": [
            "Sarit Kraus",
            "Daniel Lehmann",
            "Menachem Magidor"
        ],
        "summary": "Many systems that exhibit nonmonotonic behavior have been described and studied already in the literature. The general notion of nonmonotonic reasoning, though, has almost always been described only negatively, by the property it does not enjoy, i.e. monotonicity. We study here general patterns of nonmonotonic reasoning and try to isolate properties that could help us map the field of nonmonotonic reasoning by reference to positive properties. We concentrate on a number of families of nonmonotonic consequence relations, defined in the style of Gentzen. Both proof-theoretic and semantic points of view are developed in parallel. The former point of view was pioneered by D. Gabbay, while the latter has been advocated by Y. Shoham in. Five such families are defined and characterized by representation theorems, relating the two points of view. One of the families of interest, that of preferential relations, turns out to have been studied by E. Adams. The \"preferential\" models proposed here are a much stronger tool than Adams' probabilistic semantics. The basic language used in this paper is that of propositional logic. The extension of our results to first order predicate calculi and the study of the computational complexity of the decision problems described in this paper will be treated in another paper.",
        "published": "2002-02-18T10:29:54Z",
        "link": "http://arxiv.org/abs/cs/0202021v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "What does a conditional knowledge base entail?",
        "authors": [
            "Daniel Lehmann",
            "Menachem Magidor"
        ],
        "summary": "This paper presents a logical approach to nonmonotonic reasoning based on the notion of a nonmonotonic consequence relation. A conditional knowledge base, consisting of a set of conditional assertions of the type \"if ... then ...\", represents the explicit defeasible knowledge an agent has about the way the world generally behaves. We look for a plausible definition of the set of all conditional assertions entailed by a conditional knowledge base. In a previous paper, S. Kraus and the authors defined and studied \"preferential\" consequence relations. They noticed that not all preferential relations could be considered as reasonable inference procedures. This paper studies a more restricted class of consequence relations, \"rational\" relations. It is argued that any reasonable nonmonotonic inference procedure should define a rational relation. It is shown that the rational relations are exactly those that may be represented by a \"ranked\" preferential model, or by a (non-standard) probabilistic model. The rational closure of a conditional knowledge base is defined and shown to provide an attractive answer to the question of the title. Global properties of this closure operation are proved: it is a cumulative operation. It is also computationally tractable. This paper assumes the underlying language is propositional.",
        "published": "2002-02-18T12:43:18Z",
        "link": "http://arxiv.org/abs/cs/0202022v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "A note on Darwiche and Pearl",
        "authors": [
            "Daniel Lehmann"
        ],
        "summary": "It is shown that Darwiche and Pearl's postulates imply an interesting property, not noticed by the authors.",
        "published": "2002-02-18T15:23:06Z",
        "link": "http://arxiv.org/abs/cs/0202024v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Distance Semantics for Belief Revision",
        "authors": [
            "Daniel Lehmann",
            "Menachem Magidor",
            "Karl Schlechta"
        ],
        "summary": "A vast and interesting family of natural semantics for belief revision is defined. Suppose one is given a distance d between any two models. One may then define the revision of a theory K by a formula a as the theory defined by the set of all those models of a that are closest, by d, to the set of models of K. This family is characterized by a set of rationality postulates that extends the AGM postulates. The new postulates describe properties of iterated revisions.",
        "published": "2002-02-18T15:36:46Z",
        "link": "http://arxiv.org/abs/cs/0202025v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Preferred History Semantics for Iterated Updates",
        "authors": [
            "Shai Berger",
            "Daniel Lehmann",
            "Karl Schlechta"
        ],
        "summary": "We give a semantics to iterated update by a preference relation on possible developments. An iterated update is a sequence of formulas, giving (incomplete) information about successive states of the world. A development is a sequence of models, describing a possible trajectory through time. We assume a principle of inertia and prefer those developments, which are compatible with the information, and avoid unnecessary changes. The logical properties of the updates defined in this way are considered, and a representation result is proved.",
        "published": "2002-02-18T15:53:56Z",
        "link": "http://arxiv.org/abs/cs/0202026v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Generalized Qualitative Probability: Savage revisited",
        "authors": [
            "Daniel Lehmann"
        ],
        "summary": "Preferences among acts are analyzed in the style of L. Savage, but as partially ordered. The rationality postulates considered are weaker than Savage's on three counts. The Sure Thing Principle is derived in this setting. The postulates are shown to lead to a characterization of generalized qualitative probability that includes and blends both traditional qualitative probability and the ranked structures used in logical approaches.",
        "published": "2002-02-20T13:48:16Z",
        "link": "http://arxiv.org/abs/cs/0202030v1",
        "categories": [
            "cs.GT",
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Nonmonotonic inference operations",
        "authors": [
            "Michael Freund",
            "Daniel Lehmann"
        ],
        "summary": "A. Tarski proposed the study of infinitary consequence operations as the central topic of mathematical logic. He considered monotonicity to be a property of all such operations. In this paper, we weaken the monotonicity requirement and consider more general operations, inference operations. These operations describe the nonmonotonic logics both humans and machines seem to be using when infering defeasible information from incomplete knowledge. We single out a number of interesting families of inference operations. This study of infinitary inference operations is inspired by the results of Kraus, Lehmann and Magidor on finitary nonmonotonic operations, but this paper is self-contained.",
        "published": "2002-02-20T14:19:42Z",
        "link": "http://arxiv.org/abs/cs/0202031v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Optimal Solutions for Multi-Unit Combinatorial Auctions: Branch and   Bound Heuristics",
        "authors": [
            "Rica Gonen",
            "Daniel Lehmann"
        ],
        "summary": "Finding optimal solutions for multi-unit combinatorial auctions is a hard problem and finding approximations to the optimal solution is also hard. We investigate the use of Branch-and-Bound techniques: they require both a way to bound from above the value of the best allocation and a good criterion to decide which bids are to be tried first. Different methods for efficiently bounding from above the value of the best allocation are considered. Theoretical original results characterize the best approximation ratio and the ordering criterion that provides it. We suggest to use this criterion.",
        "published": "2002-02-20T14:38:39Z",
        "link": "http://arxiv.org/abs/cs/0202032v1",
        "categories": [
            "cs.GT",
            "cs.AI",
            "G.1.6;I.2.8"
        ]
    },
    {
        "title": "The logical meaning of Expansion",
        "authors": [
            "Daniel Lehmann"
        ],
        "summary": "The Expansion property considered by researchers in Social Choice is shown to correspond to a logical property of nonmonotonic consequence relations that is the {\\em pure}, i.e., not involving connectives, version of a previously known weak rationality condition. The assumption that the union of two definable sets of models is definable is needed for the soundness part of the result.",
        "published": "2002-02-20T14:50:49Z",
        "link": "http://arxiv.org/abs/cs/0202033v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Covariance Plasticity and Regulated Criticality",
        "authors": [
            "Elie Bienenstock",
            "Daniel Lehmann"
        ],
        "summary": "We propose that a regulation mechanism based on Hebbian covariance plasticity may cause the brain to operate near criticality. We analyze the effect of such a regulation on the dynamics of a network with excitatory and inhibitory neurons and uniform connectivity within and across the two populations. We show that, under broad conditions, the system converges to a critical state lying at the common boundary of three regions in parameter space; these correspond to three modes of behavior: high activity, low activity, oscillation.",
        "published": "2002-02-20T17:12:03Z",
        "link": "http://arxiv.org/abs/cs/0202034v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "nlin.AO",
            "q-bio",
            "I.2.6"
        ]
    },
    {
        "title": "Another perspective on Default Reasoning",
        "authors": [
            "Daniel Lehmann"
        ],
        "summary": "The lexicographic closure of any given finite set D of normal defaults is defined. A conditional assertion \"if a then b\" is in this lexicographic closure if, given the defaults D and the fact a, one would conclude b. The lexicographic closure is essentially a rational extension of D, and of its rational closure, defined in a previous paper. It provides a logic of normal defaults that is different from the one proposed by R. Reiter and that is rich enough not to require the consideration of non-normal defaults. A large number of examples are provided to show that the lexicographic closure corresponds to the basic intuitions behind Reiter's logic of defaults.",
        "published": "2002-03-01T11:06:49Z",
        "link": "http://arxiv.org/abs/cs/0203002v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Deductive Nonmonotonic Inference Operations: Antitonic Representations",
        "authors": [
            "Yuri Kaluzhny",
            "Daniel Lehmann"
        ],
        "summary": "We provide a characterization of those nonmonotonic inference operations C for which C(X) may be described as the set of all logical consequences of X together with some set of additional assumptions S(X) that depends anti-monotonically on X (i.e., X is a subset of Y implies that S(Y) is a subset of S(X)). The operations represented are exactly characterized in terms of properties most of which have been studied in Freund-Lehmann(cs.AI/0202031). Similar characterizations of right-absorbing and cumulative operations are also provided. For cumulative operations, our results fit in closely with those of Freund. We then discuss extending finitary operations to infinitary operations in a canonical way and discuss co-compactness properties. Our results provide a satisfactory notion of pseudo-compactness, generalizing to deductive nonmonotonic operations the notion of compactness for monotonic operations. They also provide an alternative, more elegant and more general, proof of the existence of an infinitary deductive extension for any finitary deductive operation (Theorem 7.9 of Freund-Lehmann).",
        "published": "2002-03-01T11:20:59Z",
        "link": "http://arxiv.org/abs/cs/0203003v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Stereotypical Reasoning: Logical Properties",
        "authors": [
            "Daniel Lehmann"
        ],
        "summary": "Stereotypical reasoning assumes that the situation at hand is one of a kind and that it enjoys the properties generally associated with that kind of situation. It is one of the most basic forms of nonmonotonic reasoning. A formal model for stereotypical reasoning is proposed and the logical properties of this form of reasoning are studied. Stereotypical reasoning is shown to be cumulative under weak assumptions.",
        "published": "2002-03-04T08:57:54Z",
        "link": "http://arxiv.org/abs/cs/0203004v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "A Framework for Compiling Preferences in Logic Programs",
        "authors": [
            "J. P. Delgrande",
            "T. Schaub",
            "H. Tompits"
        ],
        "summary": "We introduce a methodology and framework for expressing general preference information in logic programming under the answer set semantics. An ordered logic program is an extended logic program in which rules are named by unique terms, and in which preferences among rules are given by a set of atoms of form s < t where s and t are names. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed program correspond with the preferred answer sets of the original program. Our approach allows the specification of dynamic orderings, in which preferences can appear arbitrarily within a program. Static orderings (in which preferences are external to a logic program) are a trivial restriction of the general dynamic case. First, we develop a specific approach to reasoning with preferences, wherein the preference ordering specifies the order in which rules are to be applied. We then demonstrate the wide range of applicability of our framework by showing how other approaches, among them that of Brewka and Eiter, can be captured within our framework. Since the result of each of these transformations is an extended logic program, we can make use of existing implementations, such as dlv and smodels. To this end, we have developed a publicly available compiler as a front-end for these programming systems.",
        "published": "2002-03-04T13:00:41Z",
        "link": "http://arxiv.org/abs/cs/0203005v2",
        "categories": [
            "cs.AI",
            "I.2.3; D.1.6"
        ]
    },
    {
        "title": "Entangled Quantum Networks",
        "authors": [
            "Fariel Shafee"
        ],
        "summary": "We present some results from simulation of a network of nodes connected by c-NOT gates with nearest neighbors. Though initially we begin with pure states of varying boundary conditions, the updating with time quickly involves a complicated entanglement involving all or most nodes. As a normal c-NOT gate, though unitary for a single pair of nodes, seems to be not so when used in a network in a naive way, we use a manifestly unitary form of the transition matrix with c?-NOT gates, which invert the phase as well as flipping the qubit. This leads to complete entanglement of the net, but with variable coefficients for the different components of the superposition. It is interesting to note that by a simple logical back projection the original input state can be recovered in most cases. We also prove that it is not possible for a sequence of unitary operators working on a net to make it move from an aperiodic regime to a periodic one, unlike some classical cases where phase-locking happens in course of evolution. However, we show that it is possible to introduce by hand periodic orbits to sets of initial states, which may be useful in forming dynamic pattern recognition systems.",
        "published": "2002-03-04T16:06:31Z",
        "link": "http://arxiv.org/abs/quant-ph/0203010v3",
        "categories": [
            "quant-ph",
            "cond-mat.dis-nn",
            "cs.AI"
        ]
    },
    {
        "title": "Two results for proiritized logic programming",
        "authors": [
            "Yan Zhang"
        ],
        "summary": "Prioritized default reasoning has illustrated its rich expressiveness and flexibility in knowledge representation and reasoning. However, many important aspects of prioritized default reasoning have yet to be thoroughly explored. In this paper, we investigate two properties of prioritized logic programs in the context of answer set semantics. Specifically, we reveal a close relationship between mutual defeasibility and uniqueness of the answer set for a prioritized logic program. We then explore how the splitting technique for extended logic programs can be extended to prioritized logic programs. We prove splitting theorems that can be used to simplify the evaluation of a prioritized logic program under certain conditions.",
        "published": "2002-03-05T00:28:04Z",
        "link": "http://arxiv.org/abs/cs/0203007v1",
        "categories": [
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Representing and Aggregating Conflicting Beliefs",
        "authors": [
            "Pedrito Maynard-Reid II",
            "Daniel Lehmann"
        ],
        "summary": "We consider the two-fold problem of representing collective beliefs and aggregating these beliefs. We propose modular, transitive relations for collective beliefs. They allow us to represent conflicting opinions and they have a clear semantics. We compare them with the quasi-transitive relations often used in Social Choice. Then, we describe a way to construct the belief state of an agent informed by a set of sources of varying degrees of reliability. This construction circumvents Arrow's Impossibility Theorem in a satisfactory manner. Finally, we give a simple set-theory-based operator for combining the information of multiple agents. We show that this operator satisfies the desirable invariants of idempotence, commutativity, and associativity, and, thus, is well-behaved when iterated, and we describe a computationally effective way of computing the resulting belief state.",
        "published": "2002-03-11T23:02:56Z",
        "link": "http://arxiv.org/abs/cs/0203013v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; I.2.11"
        ]
    },
    {
        "title": "NetNeg: A Connectionist-Agent Integrated System for Representing Musical   Knowledge",
        "authors": [
            "Claudia V. Goldman",
            "Dan Gang",
            "Jeffrey S. Rosenschein",
            "Daniel Lehmann"
        ],
        "summary": "The system presented here shows the feasibility of modeling the knowledge involved in a complex musical activity by integrating sub-symbolic and symbolic processes. This research focuses on the question of whether there is any advantage in integrating a neural network together with a distributed artificial intelligence approach within the music domain. The primary purpose of our work is to design a model that describes the different aspects a user might be interested in considering when involved in a musical activity. The approach we suggest in this work enables the musician to encode his knowledge, intuitions, and aesthetic taste into different modules. The system captures these aspects by computing and applying three distinct functions: rules, fuzzy concepts, and learning.   As a case study, we began experimenting with first species two-part counterpoint melodies. We have developed a hybrid system composed of a connectionist module and an agent-based module to combine the sub-symbolic and symbolic levels to achieve this task. The technique presented here to represent musical knowledge constitutes a new approach for composing polyphonic music.",
        "published": "2002-03-17T09:38:56Z",
        "link": "http://arxiv.org/abs/cs/0203021v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "I.2.6; J.5"
        ]
    },
    {
        "title": "The Algorithms of Updating Sequential Patterns",
        "authors": [
            "Qingguo Zheng",
            "Ke Xu",
            "Shilong Ma",
            "Weifeng Lv"
        ],
        "summary": "Because the data being mined in the temporal database will evolve with time, many researchers have focused on the incremental mining of frequent sequences in temporal database. In this paper, we propose an algorithm called IUS, using the frequent and negative border sequences in the original database for incremental sequence mining. To deal with the case where some data need to be updated from the original database, we present an algorithm called DUS to maintain sequential patterns in the updated database. We also define the negative border sequence threshold: Min_nbd_supp to control the number of sequences in the negative border.",
        "published": "2002-03-27T03:35:12Z",
        "link": "http://arxiv.org/abs/cs/0203027v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "When to Update the sequential patterns of stream data?",
        "authors": [
            "Qingguo Zheng",
            "Ke Xu",
            "Shilong Ma"
        ],
        "summary": "In this paper, we first define a difference measure between the old and new sequential patterns of stream data, which is proved to be a distance. Then we propose an experimental method, called TPD (Tradeoff between Performance and Difference), to decide when to update the sequential patterns of stream data by making a tradeoff between the performance of increasingly updating algorithms and the difference of sequential patterns. The experiments for the incremental updating algorithm IUS on two data sets show that generally, as the size of incremental windows grows, the values of the speedup and the values of the difference will decrease and increase respectively. It is also shown experimentally that the incremental ratio determined by the TPD method does not monotonically increase or decrease but changes in a range between 20 and 30 percentage for the IUS algorithm.",
        "published": "2002-03-27T06:31:58Z",
        "link": "http://arxiv.org/abs/cs/0203028v3",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "The tip-of-the-tongue phenomenon: Irrelevant neural network localization   or disruption of its interneuron links ?",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "On the base of recently proposed three-stage quantitative neural network model of the tip-of-the-tongue (TOT) phenomenon a possibility to occur of TOT states coursed by neural network interneuron links' disruption has been studied. Using a numerical example it was found that TOTs coursed by interneron links' disruption are in (1.5 + - 0.3)x1000 times less probable then those coursed by irrelevant (incomplete) neural network localization. It was shown that delayed TOT states' etiology cannot be related to neural network interneuron links' disruption.",
        "published": "2002-04-04T18:59:49Z",
        "link": "http://arxiv.org/abs/cs/0204008v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "q-bio.NC",
            "q-bio.QM",
            "H.3.3; I.2.7; J.4"
        ]
    },
    {
        "title": "Fast Hands-free Writing by Gaze Direction",
        "authors": [
            "David J. Ward",
            "David J. C. MacKay"
        ],
        "summary": "We describe a method for text entry based on inverse arithmetic coding that relies on gaze direction and which is faster and more accurate than using an on-screen keyboard.   These benefits are derived from two innovations: the writing task is matched to the capabilities of the eye, and a language model is used to make predictable words and phrases easier to write.",
        "published": "2002-04-12T15:07:46Z",
        "link": "http://arxiv.org/abs/cs/0204030v4",
        "categories": [
            "cs.HC",
            "cs.AI",
            "H.5.2;K.4.2;H.1.1;H.5.1;I.2.7"
        ]
    },
    {
        "title": "Belief Revision and Rational Inference",
        "authors": [
            "Michael Freund",
            "Daniel Lehmann"
        ],
        "summary": "The (extended) AGM postulates for belief revision seem to deal with the revision of a given theory K by an arbitrary formula, but not to constrain the revisions of two different theories by the same formula. A new postulate is proposed and compared with other similar postulates that have been proposed in the literature. The AGM revisions that satisfy this new postulate stand in one-to-one correspondence with the rational, consistency-preserving relations. This correspondence is described explicitly. Two viewpoints on iterative revisions are distinguished and discussed.",
        "published": "2002-04-14T09:22:42Z",
        "link": "http://arxiv.org/abs/cs/0204032v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Self-Optimizing and Pareto-Optimal Policies in General Environments   based on Bayes-Mixtures",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "The problem of making sequential decisions in unknown probabilistic environments is studied. In cycle $t$ action $y_t$ results in perception $x_t$ and reward $r_t$, where all quantities in general may depend on the complete history. The perception $x_t$ and reward $r_t$ are sampled from the (reactive) environmental probability distribution $\\mu$. This very general setting includes, but is not limited to, (partial observable, k-th order) Markov decision processes. Sequential decision theory tells us how to act in order to maximize the total expected reward, called value, if $\\mu$ is known. Reinforcement learning is usually used if $\\mu$ is unknown. In the Bayesian approach one defines a mixture distribution $\\xi$ as a weighted sum of distributions $\\nu\\in\\M$, where $\\M$ is any class of distributions including the true environment $\\mu$. We show that the Bayes-optimal policy $p^\\xi$ based on the mixture $\\xi$ is self-optimizing in the sense that the average value converges asymptotically for all $\\mu\\in\\M$ to the optimal value achieved by the (infeasible) Bayes-optimal policy $p^\\mu$ which knows $\\mu$ in advance. We show that the necessary condition that $\\M$ admits self-optimizing policies at all, is also sufficient. No other structural assumptions are made on $\\M$. As an example application, we discuss ergodic Markov decision processes, which allow for self-optimizing policies. Furthermore, we show that $p^\\xi$ is Pareto-optimal in the sense that there is no other policy yielding higher or equal value in {\\em all} environments $\\nu\\in\\M$ and a strictly higher value in at least one.",
        "published": "2002-04-17T10:46:00Z",
        "link": "http://arxiv.org/abs/cs/0204040v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "math.OC",
            "math.PR",
            "I.2"
        ]
    },
    {
        "title": "Learning from Scarce Experience",
        "authors": [
            "Leonid Peshkin",
            "Christian R. Shelton"
        ],
        "summary": "Searching the space of policies directly for the optimal policy has been one popular method for solving partially observable reinforcement learning problems. Typically, with each change of the target policy, its value is estimated from the results of following that very policy. This requires a large number of interactions with the environment as different polices are considered. We present a family of algorithms based on likelihood ratio estimation that use data gathered when executing one policy (or collection of policies) to estimate the value of a different policy. The algorithms combine estimation and optimization stages. The former utilizes experience to build a non-parametric representation of an optimized function. The latter performs optimization on this estimate. We show positive empirical results and provide the sample complexity bound.",
        "published": "2002-04-20T05:02:53Z",
        "link": "http://arxiv.org/abs/cs/0204043v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.NE",
            "cs.RO",
            "I.2; I.2.8; I.2.11; I.2.6; G.1.6"
        ]
    },
    {
        "title": "Robust Global Localization Using Clustered Particle Filtering",
        "authors": [
            "Javier Nicolas Sanchez",
            "Adam Milstein",
            "Evan Williamson"
        ],
        "summary": "Global mobile robot localization is the problem of determining a robot's pose in an environment, using sensor data, when the starting position is unknown. A family of probabilistic algorithms known as Monte Carlo Localization (MCL) is currently among the most popular methods for solving this problem. MCL algorithms represent a robot's belief by a set of weighted samples, which approximate the posterior probability of where the robot is located by using a Bayesian formulation of the localization problem. This article presents an extension to the MCL algorithm, which addresses its problems when localizing in highly symmetrical environments; a situation where MCL is often unable to correctly track equally probable poses for the robot. The problem arises from the fact that sample sets in MCL often become impoverished, when samples are generated according to their posterior likelihood. Our approach incorporates the idea of clusters of samples and modifies the proposal distribution considering the probability mass of those clusters. Experimental results are presented that show that this new extension to the MCL algorithm successfully localizes in symmetric environments where ordinary MCL often fails.",
        "published": "2002-04-21T01:21:22Z",
        "link": "http://arxiv.org/abs/cs/0204044v1",
        "categories": [
            "cs.RO",
            "cs.AI",
            "I.2.9"
        ]
    },
    {
        "title": "Sampling Strategies for Mining in Data-Scarce Domains",
        "authors": [
            "Naren Ramakrishnan",
            "Chris Bailey-Kellogg"
        ],
        "summary": "Data mining has traditionally focused on the task of drawing inferences from large datasets. However, many scientific and engineering domains, such as fluid dynamics and aircraft design, are characterized by scarce data, due to the expense and complexity of associated experiments and simulations. In such data-scarce domains, it is advantageous to focus the data collection effort on only those regions deemed most important to support a particular data mining objective. This paper describes a mechanism that interleaves bottom-up data mining, to uncover multi-level structures in spatial data, with top-down sampling, to clarify difficult decisions in the mining process. The mechanism exploits relevant physical properties, such as continuity, correspondence, and locality, in a unified framework. This leads to effective mining and sampling decisions that are explainable in terms of domain knowledge and data characteristics. This approach is demonstrated in two diverse applications -- mining pockets in spatial data, and qualitative determination of Jordan forms of matrices.",
        "published": "2002-04-22T19:41:24Z",
        "link": "http://arxiv.org/abs/cs/0204047v2",
        "categories": [
            "cs.CE",
            "cs.AI",
            "D.2.6; G.1.2; G.1.3; G.3; I.2.10; I.5; H.2.8"
        ]
    },
    {
        "title": "Qualitative Analysis of Correspondence for Experimental Algorithmics",
        "authors": [
            "Chris Bailey-Kellogg",
            "Naren Ramakrishnan"
        ],
        "summary": "Correspondence identifies relationships among objects via similarities among their components; it is ubiquitous in the analysis of spatial datasets, including images, weather maps, and computational simulations. This paper develops a novel multi-level mechanism for qualitative analysis of correspondence. Operators leverage domain knowledge to establish correspondence, evaluate implications for model selection, and leverage identified weaknesses to focus additional data collection. The utility of the mechanism is demonstrated in two applications from experimental algorithmics -- matrix spectral portrait analysis and graphical assessment of Jordan forms of matrices. Results show that the mechanism efficiently samples computational experiments and successfully uncovers high-level problem properties. It overcomes noise and data sparsity by leveraging domain knowledge to detect mutually reinforcing interpretations of spatial data.",
        "published": "2002-04-26T17:25:51Z",
        "link": "http://arxiv.org/abs/cs/0204053v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "D.2.6; G.1.2; G.1.3; G.3; I.2.10; I.5; H.2.8"
        ]
    },
    {
        "title": "Intelligent Search of Correlated Alarms for GSM Networks with   Model-based Constraints",
        "authors": [
            "Qingguo Zheng",
            "Ke Xu",
            "Weifeng Lv",
            "Shilong Ma"
        ],
        "summary": "In order to control the process of data mining and focus on the things of interest to us, many kinds of constraints have been added into the algorithms of data mining. However, discovering the correlated alarms in the alarm database needs deep domain constraints. Because the correlated alarms greatly depend on the logical and physical architecture of networks. Thus we use the network model as the constraints of algorithms, including Scope constraint, Inter-correlated constraint and Intra-correlated constraint, in our proposed algorithm called SMC (Search with Model-based Constraints). The experiments show that the SMC algorithm with Inter-correlated or Intra-correlated constraint is about two times faster than the algorithm with no constraints.",
        "published": "2002-04-29T08:03:53Z",
        "link": "http://arxiv.org/abs/cs/0204055v1",
        "categories": [
            "cs.NI",
            "cs.AI",
            "C.2.4"
        ]
    },
    {
        "title": "Computing stable models: worst-case performance estimates",
        "authors": [
            "Zbigniew Lonc",
            "Miroslaw Truszczynski"
        ],
        "summary": "We study algorithms for computing stable models of propositional logic programs and derive estimates on their worst-case performance that are asymptotically better than the trivial bound of O(m 2^n), where m is the size of an input program and n is the number of its atoms. For instance, for programs, whose clauses consist of at most two literals (counting the head) we design an algorithm to compute stable models that works in time O(m\\times 1.44225^n). We present similar results for several broader classes of programs, as well.",
        "published": "2002-05-11T20:27:20Z",
        "link": "http://arxiv.org/abs/cs/0205013v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6;I.2.4"
        ]
    },
    {
        "title": "Ultimate approximations in nonmonotonic knowledge representation systems",
        "authors": [
            "Marc Denecker",
            "Victor W. Marek",
            "Miroslaw Truszczynski"
        ],
        "summary": "We study fixpoints of operators on lattices. To this end we introduce the notion of an approximation of an operator. We order approximations by means of a precision ordering. We show that each lattice operator O has a unique most precise or ultimate approximation. We demonstrate that fixpoints of this ultimate approximation provide useful insights into fixpoints of the operator O.   We apply our theory to logic programming and introduce the ultimate Kripke-Kleene, well-founded and stable semantics. We show that the ultimate Kripke-Kleene and well-founded semantics are more precise then their standard counterparts We argue that ultimate semantics for logic programming have attractive epistemological properties and that, while in general they are computationally more complex than the standard semantics, for many classes of theories, their complexity is no worse.",
        "published": "2002-05-11T20:44:16Z",
        "link": "http://arxiv.org/abs/cs/0205014v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "From Alife Agents to a Kingdom of N Queens",
        "authors": [
            "Jing Han",
            "Jiming Liu",
            "Qingsheng Cai"
        ],
        "summary": "This paper presents a new approach to solving N-queen problems, which involves a model of distributed autonomous agents with artificial life (ALife) and a method of representing N-queen constraints in an agent environment. The distributed agents locally interact with their living environment, i.e., a chessboard, and execute their reactive behaviors by applying their behavioral rules for randomized motion, least-conflict position searching, and cooperating with other agents etc. The agent-based N-queen problem solving system evolves through selection and contest according to the rule of Survival of the Fittest, in which some agents will die or be eaten if their moving strategies are less efficient than others. The experimental results have shown that this system is capable of solving large-scale N-queen problems. This paper also provides a model of ALife agents for solving general CSPs.",
        "published": "2002-05-13T10:49:48Z",
        "link": "http://arxiv.org/abs/cs/0205016v1",
        "categories": [
            "cs.AI",
            "cs.DS",
            "cs.MA",
            "I.2.8;I.2.11;G.2.1"
        ]
    },
    {
        "title": "The Traits of the Personable",
        "authors": [
            "Naren Ramakrishnan"
        ],
        "summary": "Information personalization is fertile ground for application of AI techniques. In this article I relate personalization to the ability to capture partial information in an information-seeking interaction. The specific focus is on personalizing interactions at web sites. Using ideas from partial evaluation and explanation-based generalization, I present a modeling methodology for reasoning about personalization. This approach helps identify seven tiers of `personable traits' in web sites.",
        "published": "2002-05-14T19:25:07Z",
        "link": "http://arxiv.org/abs/cs/0205022v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "H.3.5; H.4.2; H.5.4; I.2.6; K.8"
        ]
    },
    {
        "title": "Contextualizing Concepts using a Mathematical Generalization of the   Quantum Formalism",
        "authors": [
            "Liane Gabora",
            "Diederik Aerts"
        ],
        "summary": "We outline the rationale and preliminary results of using the State Context Property (SCOP) formalism, originally developed as a generalization of quantum mechanics, to describe the contextual manner in which concepts are evoked, used, and combined to generate meaning. The quantum formalism was developed to cope with problems arising in the description of (1) the measurement process, and (2) the generation of new states with new properties when particles become entangled. Similar problems arising with concepts motivated the formal treatment introduced here. Concepts are viewed not as fixed representations, but entities existing in states of potentiality that require interaction with a context--a stimulus or another concept--to 'collapse' to an instantiated form (e.g. exemplar, prototype, or other possibly imaginary instance). The stimulus situation plays the role of the measurement in physics, acting as context that induces a change of the cognitive state from superposition state to collapsed state. The collapsed state is more likely to consist of a conjunction of concepts for associative than analytic thought because more stimulus or concept properties take part in the collapse. We provide two contextual measures of conceptual distance--one using collapse probabilities and the other weighted properties--and show how they can be applied to conjunctions using the pet fish problem",
        "published": "2002-05-26T16:38:39Z",
        "link": "http://arxiv.org/abs/quant-ph/0205161v2",
        "categories": [
            "quant-ph",
            "cs.AI",
            "q-bio.NC"
        ]
    },
    {
        "title": "A Spectrum of Applications of Automated Reasoning",
        "authors": [
            "Larry Wos"
        ],
        "summary": "The likelihood of an automated reasoning program being of substantial assistance for a wide spectrum of applications rests with the nature of the options and parameters it offers on which to base needed strategies and methodologies. This article focuses on such a spectrum, featuring W. McCune's program OTTER, discussing widely varied successes in answering open questions, and touching on some of the strategies and methodologies that played a key role. The applications include finding a first proof, discovering single axioms, locating improved axiom systems, and simplifying existing proofs. The last application is directly pertinent to the recently found (by R. Thiele) Hilbert's twenty-fourth problem--which is extremely amenable to attack with the appropriate automated reasoning program--a problem concerned with proof simplification. The methodologies include those for seeking shorter proofs and for finding proofs that avoid unwanted lemmas or classes of term, a specific option for seeking proofs with smaller equational or formula complexity, and a different option to address the variable richness of a proof. The type of proof one obtains with the use of OTTER is Hilbert-style axiomatic, including details that permit one sometimes to gain new insights. We include questions still open and challenges that merit consideration.",
        "published": "2002-05-30T21:20:52Z",
        "link": "http://arxiv.org/abs/cs/0205078v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; F.4.1"
        ]
    },
    {
        "title": "Connectives in Quantum and other Cumulative Logics",
        "authors": [
            "Daniel Lehmann"
        ],
        "summary": "Cumulative logics are studied in an abstract setting, i.e., without connectives, very much in the spirit of Makinson's early work. A powerful representation theorem characterizes those logics by choice functions that satisfy a weakening of Sen's property alpha, in the spirit of the author's \"Nonmonotonic Logics and Semantics\" (JLC). The representation results obtained are surprisingly smooth: in the completeness part the choice function may be defined on any set of worlds, not only definable sets and no definability-preservation property is required in the soundness part. For abstract cumulative logics, proper conjunction and negation may be defined. Contrary to the situation studied in \"Nonmonotonic Logics and Semantics\" no proper disjunction seems to be definable in general. The cumulative relations of KLM that satisfy some weakening of the consistency preservation property all define cumulative logics with a proper negation. Quantum Logics, as defined by Engesser and Gabbay are such cumulative logics but the negation defined by orthogonal complement does not provide a proper negation.",
        "published": "2002-05-31T10:09:47Z",
        "link": "http://arxiv.org/abs/cs/0205079v2",
        "categories": [
            "cs.AI",
            "math.LO",
            "I.2.3; F.4.1"
        ]
    },
    {
        "title": "Handling Defeasibilities in Action Domains",
        "authors": [
            "Yan Zhang"
        ],
        "summary": "Representing defeasibility is an important issue in common sense reasoning. In reasoning about action and change, this issue becomes more difficult because domain and action related defeasible information may conflict with general inertia rules. Furthermore, different types of defeasible information may also interfere with each other during the reasoning. In this paper, we develop a prioritized logic programming approach to handle defeasibilities in reasoning about action. In particular, we propose three action languages {\\cal AT}^{0}, {\\cal AT}^{1} and {\\cal AT}^{2} which handle three types of defeasibilities in action domains named defeasible constraints, defeasible observations and actions with defeasible and abnormal effects respectively. Each language with a higher superscript can be viewed as an extension of the language with a lower superscript. These action languages inherit the simple syntax of {\\cal A} language but their semantics is developed in terms of transition systems where transition functions are defined based on prioritized logic programs. By illustrating various examples, we show that our approach eventually provides a powerful mechanism to handle various defeasibilities in temporal prediction and postdiction. We also investigate semantic properties of these three action languages and characterize classes of action domains that present more desirable solutions in reasoning about action within the underlying action languages.",
        "published": "2002-06-03T06:20:21Z",
        "link": "http://arxiv.org/abs/cs/0206003v1",
        "categories": [
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Mining All Non-Derivable Frequent Itemsets",
        "authors": [
            "Toon Calders",
            "Bart Goethals"
        ],
        "summary": "Recent studies on frequent itemset mining algorithms resulted in significant performance improvements. However, if the minimal support threshold is set too low, or the data is highly correlated, the number of frequent itemsets itself can be prohibitively large. To overcome this problem, recently several proposals have been made to construct a concise representation of the frequent itemsets, instead of mining all frequent itemsets. The main goal of this paper is to identify redundancies in the set of all frequent itemsets and to exploit these redundancies in order to reduce the result of a mining operation. We present deduction rules to derive tight bounds on the support of candidate itemsets. We show how the deduction rules allow for constructing a minimal representation for all frequent itemsets. We also present connections between our proposal and recent proposals for concise representations and we give the results of experiments on real-life datasets that show the effectiveness of the deduction rules. In fact, the experiments even show that in many cases, first mining the concise representation, and then creating the frequent itemsets from this representation outperforms existing frequent set mining algorithms.",
        "published": "2002-06-03T14:13:51Z",
        "link": "http://arxiv.org/abs/cs/0206004v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "Robust Feature Selection by Mutual Information Distributions",
        "authors": [
            "Marco Zaffalon",
            "Marcus Hutter"
        ],
        "summary": "Mutual information is widely used in artificial intelligence, in a descriptive way, to measure the stochastic dependence of discrete random variables. In order to address questions such as the reliability of the empirical value, one must consider sample-to-population inferential approaches. This paper deals with the distribution of mutual information, as obtained in a Bayesian framework by a second-order Dirichlet prior distribution. The exact analytical expression for the mean and an analytical approximation of the variance are reported. Asymptotic approximations of the distribution are proposed. The results are applied to the problem of selecting features for incremental learning and classification of the naive Bayes classifier. A fast, newly defined method is shown to outperform the traditional approach based on empirical mutual information on a number of real data sets. Finally, a theoretical development is reported that allows one to efficiently extend the above methods to incomplete samples in an easy and effective way.",
        "published": "2002-06-03T16:00:55Z",
        "link": "http://arxiv.org/abs/cs/0206006v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2"
        ]
    },
    {
        "title": "Computer modeling of feelings and emotions: a quantitative neural   network model of the feeling-of-knowing",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "The first quantitative neural network model of feelings and emotions is proposed on the base of available data on their neuroscience and evolutionary biology nature, and on a neural network human memory model which admits distinct description of conscious and unconscious mental processes in a time dependent manner. As an example, proposed model is applied to quantitative description of the feeling of knowing.",
        "published": "2002-06-03T22:31:45Z",
        "link": "http://arxiv.org/abs/cs/0206008v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "q-bio.NC",
            "q-bio.QM",
            "I.2.0; J.4"
        ]
    },
    {
        "title": "The Prioritized Inductive Logic Programs",
        "authors": [
            "Shilong Ma",
            "Yuefei Sui",
            "Ke Xu"
        ],
        "summary": "The limit behavior of inductive logic programs has not been explored, but when considering incremental or online inductive learning algorithms which usually run ongoingly, such behavior of the programs should be taken into account. An example is given to show that some inductive learning algorithm may not be correct in the long run if the limit behavior is not considered. An inductive logic program is convergent if given an increasing sequence of example sets, the program produces a corresponding sequence of the Horn logic programs which has the set-theoretic limit, and is limit-correct if the limit of the produced sequence of the Horn logic programs is correct with respect to the limit of the sequence of the example sets. It is shown that the GOLEM system is not limit-correct. Finally, a limit-correct inductive logic system, called the prioritized GOLEM system, is proposed as a solution.",
        "published": "2002-06-10T16:02:36Z",
        "link": "http://arxiv.org/abs/cs/0206017v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.3; I.2.6"
        ]
    },
    {
        "title": "Relational Association Rules: getting WARMeR",
        "authors": [
            "Bart Goethals",
            "Jan Van den Bussche"
        ],
        "summary": "In recent years, the problem of association rule mining in transactional data has been well studied. We propose to extend the discovery of classical association rules to the discovery of association rules of conjunctive queries in arbitrary relational data, inspired by the WARMR algorithm, developed by Dehaspe and Toivonen, that discovers association rules over a limited set of conjunctive queries. Conjunctive query evaluation in relational databases is well understood, but still poses some great challenges when approached from a discovery viewpoint in which patterns are generated and evaluated with respect to some well defined search space and pruning operators.",
        "published": "2002-06-15T12:08:12Z",
        "link": "http://arxiv.org/abs/cs/0206023v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "Behaviour-based Knowledge Systems: An Epigenetic Path from Behaviour to   Knowledge",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "In this paper we expose the theoretical background underlying our current research. This consists in the development of behaviour-based knowledge systems, for closing the gaps between behaviour-based and knowledge-based systems, and also between the understandings of the phenomena they model. We expose the requirements and stages for developing behaviour-based knowledge systems and discuss their limits. We believe that these are necessary conditions for the development of higher order cognitive capacities, in artificial and natural cognitive systems.",
        "published": "2002-06-18T13:05:50Z",
        "link": "http://arxiv.org/abs/cs/0206027v1",
        "categories": [
            "cs.AI",
            "cs.AR",
            "cs.NE",
            "I.2.0, I.2.6"
        ]
    },
    {
        "title": "Knowledge management for enterprises (Wissensmanagement fuer   Unternehmen)",
        "authors": [
            "Wolfgang Eiden"
        ],
        "summary": "Although knowledge is one of the most valuable resource of enterprises and an important production and competition factor, this intellectual potential is often used (or maintained) only inadequate by the enterprises. Therefore, in a globalised and growing market the optimal usage of existing knowledge represents a key factor for enterprises of the future. Here, knowledge management systems should engage facilitating. Because geographically far distributed establishments cause, however, a distributed system, this paper should uncover the spectrum connected with it and present a possible basic approach which is based on ontologies and modern, platform independent technologies. Last but not least this attempt, as well as general questions of the knowledge management, are discussed.",
        "published": "2002-06-19T22:13:41Z",
        "link": "http://arxiv.org/abs/cs/0206028v2",
        "categories": [
            "cs.IR",
            "cs.AI",
            "H.3.0; I.2.4"
        ]
    },
    {
        "title": "Anticipatory Guidance of Plot",
        "authors": [
            "Jarmo Laaksolahti",
            "Magnus Boman"
        ],
        "summary": "An anticipatory system for guiding plot development in interactive narratives is described. The executable model is a finite automaton that provides the implemented system with a look-ahead. The identification of undesirable future states in the model is used to guide the player, in a transparent manner. In this way, too radical twists of the plot can be avoided. Since the player participates in the development of the plot, such guidance can have many forms, depending on the environment of the player, on the behavior of the other players, and on the means of player interaction. We present a design method for interactive narratives which produces designs suitable for the implementation of anticipatory mechanisms. Use of the method is illustrated by application to our interactive computer game Kaktus.",
        "published": "2002-06-26T09:17:13Z",
        "link": "http://arxiv.org/abs/cs/0206041v2",
        "categories": [
            "cs.AI",
            "I.2.11; I.6.3; I.6.5"
        ]
    },
    {
        "title": "Agent Programming with Declarative Goals",
        "authors": [
            "F. S. de Boer",
            "K. V. Hindriks",
            "W. van der Hoek",
            "J. -J. Ch. Meyer"
        ],
        "summary": "A long and lasting problem in agent research has been to close the gap between agent logics and agent programming frameworks. The main reason for this problem of establishing a link between agent logics and agent programming frameworks is identified and explained by the fact that agent programming frameworks have not incorporated the concept of a `declarative goal'. Instead, such frameworks have focused mainly on plans or `goals-to-do' instead of the end goals to be realised which are also called `goals-to-be'. In this paper, a new programming language called GOAL is introduced which incorporates such declarative goals. The notion of a `commitment strategy' - one of the main theoretical insights due to agent logics, which explains the relation between beliefs and goals - is used to construct a computational semantics for GOAL. Finally, a proof theory for proving properties of GOAL agents is introduced. Thus, we offer a complete theory of agent programming in the sense that our theory provides both for a programming framework and a programming logic for such agents. An example program is proven correct by using this programming logic.",
        "published": "2002-07-03T15:42:55Z",
        "link": "http://arxiv.org/abs/cs/0207008v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "F.3.1;F.3.2;I.2.5;I.2.4"
        ]
    },
    {
        "title": "Abduction, ASP and Open Logic Programs",
        "authors": [
            "Piero A. Bonatti"
        ],
        "summary": "Open logic programs and open entailment have been recently proposed as an abstract framework for the verification of incomplete specifications based upon normal logic programs and the stable model semantics. There are obvious analogies between open predicates and abducible predicates. However, despite superficial similarities, there are features of open programs that have no immediate counterpart in the framework of abduction and viceversa. Similarly, open programs cannot be immediately simulated with answer set programming (ASP). In this paper we start a thorough investigation of the relationships between open inference, abduction and ASP. We shall prove that open programs generalize the other two frameworks. The generalized framework suggests interesting extensions of abduction under the generalized stable model semantics. In some cases, we will be able to reduce open inference to abduction and ASP, thereby estimating its computational complexity. At the same time, the aforementioned reduction opens the way to new applications of abduction and ASP.",
        "published": "2002-07-07T09:55:00Z",
        "link": "http://arxiv.org/abs/cs/0207021v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Domain-Dependent Knowledge in Answer Set Planning",
        "authors": [
            "Tran Cao Son",
            "Chitta Baral",
            "Nam Tran",
            "Sheila McIlraith"
        ],
        "summary": "In this paper we consider three different kinds of domain-dependent control knowledge (temporal, procedural and HTN-based) that are useful in planning. Our approach is declarative and relies on the language of logic programming with answer set semantics (AnsProlog*). AnsProlog* is designed to plan without control knowledge. We show how temporal, procedural and HTN-based control knowledge can be incorporated into AnsProlog* by the modular addition of a small number of domain-dependent rules, without the need to modify the planner. We formally prove the correctness of our planner, both in the absence and presence of the control knowledge. Finally, we perform some initial experimentation that demonstrates the potential reduction in planning time that can be achieved when procedural domain knowledge is used to solve planning problems with large plan length.",
        "published": "2002-07-08T00:31:54Z",
        "link": "http://arxiv.org/abs/cs/0207023v2",
        "categories": [
            "cs.AI",
            "I.2.4; I.2.3; I.2.8"
        ]
    },
    {
        "title": "On Concise Encodings of Preferred Extensions",
        "authors": [
            "Paul E. Dunne"
        ],
        "summary": "Much work on argument systems has focussed on preferred extensions which define the maximal collectively defensible subsets. Identification and enumeration of these subsets is (under the usual assumptions) computationally demanding. We consider approaches to deciding if a subset S is a preferred extension which query a representations encoding all such extensions, so that the computational effort is invested once only (for the initial enumeration) rather than for each separate query.",
        "published": "2002-07-08T13:24:07Z",
        "link": "http://arxiv.org/abs/cs/0207024v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "F.2.m; I.2.3; I.2.m"
        ]
    },
    {
        "title": "\"Minimal defence\": a refinement of the preferred semantics for   argumentation frameworks",
        "authors": [
            "C. Cayrol",
            "S. Doutre",
            "M. -C. Lagasquie-Schiex",
            "J. Mengin"
        ],
        "summary": "Dung's abstract framework for argumentation enables a study of the interactions between arguments based solely on an ``attack'' binary relation on the set of arguments. Various ways to solve conflicts between contradictory pieces of information have been proposed in the context of argumentation, nonmonotonic reasoning or logic programming, and can be captured by appropriate semantics within Dung's framework. A common feature of these semantics is that one can always maximize in some sense the set of acceptable arguments. We propose in this paper to extend Dung's framework in order to allow for the representation of what we call ``restricted'' arguments: these arguments should only be used if absolutely necessary, that is, in order to support other arguments that would otherwise be defeated. We modify Dung's preferred semantics accordingly: a set of arguments becomes acceptable only if it contains a minimum of restricted arguments, for a maximum of unrestricted arguments.",
        "published": "2002-07-08T13:30:16Z",
        "link": "http://arxiv.org/abs/cs/0207025v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Two Representations for Iterative Non-prioritized Change",
        "authors": [
            "Alexander Bochman"
        ],
        "summary": "We address a general representation problem for belief change, and describe two interrelated representations for iterative non-prioritized change: a logical representation in terms of persistent epistemic states, and a constructive representation in terms of flocks of bases.",
        "published": "2002-07-09T12:32:45Z",
        "link": "http://arxiv.org/abs/cs/0207029v2",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Collective Argumentation",
        "authors": [
            "Alexander Bochman"
        ],
        "summary": "An extension of an abstract argumentation framework, called collective argumentation, is introduced in which the attack relation is defined directly among sets of arguments. The extension turns out to be suitable, in particular, for representing semantics of disjunctive logic programs. Two special kinds of collective argumentation are considered in which the opponents can share their arguments.",
        "published": "2002-07-09T12:42:24Z",
        "link": "http://arxiv.org/abs/cs/0207030v2",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Intuitions and the modelling of defeasible reasoning: some case studies",
        "authors": [
            "Henry Prakken"
        ],
        "summary": "The purpose of this paper is to address some criticisms recently raised by John Horty in two articles against the validity of two commonly accepted defeasible reasoning patterns, viz. reinstatement and floating conclusions. I shall argue that Horty's counterexamples, although they significantly raise our understanding of these reasoning patterns, do not show their invalidity. Some of them reflect patterns which, if made explicit in the formalisation, avoid the unwanted inference without having to give up the criticised inference principles. Other examples seem to involve hidden assumptions about the specific problem which, if made explicit, are nothing but extra information that defeat the defeasible inference. These considerations will be put in a wider perspective by reflecting on the nature of defeasible reasoning principles as principles of justified acceptance rather than `real' logical inference.",
        "published": "2002-07-09T14:16:18Z",
        "link": "http://arxiv.org/abs/cs/0207031v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Alternative Characterizations for Strong Equivalence of Logic Programs",
        "authors": [
            "Pedro Cabalar"
        ],
        "summary": "In this work we present additional results related to the property of strong equivalence of logic programs. This property asserts that two programs share the same set of stable models, even under the addition of new rules. As shown in a recent work by Lifschitz, Pearce and Valverde, strong equivalence can be simply reduced to equivalence in the logic of Here-and-There (HT). In this paper we provide two alternatives respectively based on classical logic and 3-valued logic. The former is applicable to general rules, but not for nested expressions, whereas the latter is applicable for nested expressions but, when moving to an unrestricted syntax, it generally yields different results from HT.",
        "published": "2002-07-09T15:05:32Z",
        "link": "http://arxiv.org/abs/cs/0207032v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Some logics of belief and disbelief",
        "authors": [
            "Samir Chopra",
            "Johannes Heidema",
            "Thomas Meyer"
        ],
        "summary": "The introduction of explicit notions of rejection, or disbelief, into logics for knowledge representation can be justified in a number of ways. Motivations range from the need for versions of negation weaker than classical negation, to the explicit recording of classic belief contraction operations in the area of belief change, and the additional levels of expressivity obtained from an extended version of belief change which includes disbelief contraction. In this paper we present four logics of disbelief which address some or all of these intuitions. Soundness and completeness results are supplied and the logics are compared with respect to applicability and utility.",
        "published": "2002-07-10T02:16:41Z",
        "link": "http://arxiv.org/abs/cs/0207037v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Iterated revision and the axiom of recovery: a unified treatment via   epistemic states",
        "authors": [
            "Samir Chopra",
            "Aditya Ghose",
            "Thomas Meyer"
        ],
        "summary": "The axiom of recovery, while capturing a central intuition regarding belief change, has been the source of much controversy. We argue briefly against putative counterexamples to the axiom--while agreeing that some of their insight deserves to be preserved--and present additional recovery-like axioms in a framework that uses epistemic states, which encode preferences, as the object of revisions. This provides a framework in which iterated revision becomes possible and makes explicit the connection between iterated belief change and the axiom of recovery. We provide a representation theorem that connects the semantic conditions that we impose on iterated revision and the additional syntactical properties mentioned. We also show some interesting similarities between our framework and that of Darwiche-Pearl. In particular, we show that the intuitions underlying the controversial (C2) postulate are captured by the recovery axiom and our recovery-like postulates (the latter can be seen as weakenings of (C2).",
        "published": "2002-07-10T02:31:19Z",
        "link": "http://arxiv.org/abs/cs/0207038v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Well-Founded Argumentation Semantics for Extended Logic Programming",
        "authors": [
            "Ralf Schweimeier",
            "Michael Schroeder"
        ],
        "summary": "This paper defines an argumentation semantics for extended logic programming and shows its equivalence to the well-founded semantics with explicit negation. We set up a general framework in which we extensively compare this semantics to other argumentation semantics, including those of Dung, and Prakken and Sartor. We present a general dialectical proof theory for these argumentation semantics.",
        "published": "2002-07-10T10:22:12Z",
        "link": "http://arxiv.org/abs/cs/0207040v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6.; F.3.2.; I.2.3.; I.2.4"
        ]
    },
    {
        "title": "Logic Programming with Ordered Disjunction",
        "authors": [
            "Gerhard Brewka"
        ],
        "summary": "Logic programs with ordered disjunction (LPODs) combine ideas underlying Qualitative Choice Logic (Brewka et al. KR 2002) and answer set programming. Logic programming under answer set semantics is extended with a new connective called ordered disjunction. The new connective allows us to represent alternative, ranked options for problem solutions in the heads of rules: A \\times B intuitively means: if possible A, but if A is not possible then at least B. The semantics of logic programs with ordered disjunction is based on a preference relation on answer sets. LPODs are useful for applications in design and configuration and can serve as a basis for qualitative decision making.",
        "published": "2002-07-11T11:03:34Z",
        "link": "http://arxiv.org/abs/cs/0207042v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Compilation of Propositional Weighted Bases",
        "authors": [
            "Adnan Darwiche",
            "Pierre Marquis"
        ],
        "summary": "In this paper, we investigate the extent to which knowledge compilation can be used to improve inference from propositional weighted bases. We present a general notion of compilation of a weighted base that is parametrized by any equivalence--preserving compilation function. Both negative and positive results are presented. On the one hand, complexity results are identified, showing that the inference problem from a compiled weighted base is as difficult as in the general case, when the prime implicates, Horn cover or renamable Horn cover classes are targeted. On the other hand, we show that the inference problem becomes tractable whenever DNNF-compilations are used and clausal queries are considered. Moreover, we show that the set of all preferred models of a DNNF-compilation of a weighted base can be computed in time polynomial in the output size. Finally, we sketch how our results can be used in model-based diagnosis in order to compute the most probable diagnoses of a system.",
        "published": "2002-07-11T16:11:40Z",
        "link": "http://arxiv.org/abs/cs/0207045v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "The Rise and Fall of the Church-Turing Thesis",
        "authors": [
            "Mark Burgin"
        ],
        "summary": "The essay consists of three parts. In the first part, it is explained how theory of algorithms and computations evaluates the contemporary situation with computers and global networks. In the second part, it is demonstrated what new perspectives this theory opens through its new direction that is called theory of super-recursive algorithms. These algorithms have much higher computing power than conventional algorithmic schemes. In the third part, we explicate how realization of what this theory suggests might influence life of people in future. It is demonstrated that now the theory is far ahead computing practice and practice has to catch up with the theory. We conclude with a comparison of different approaches to the development of information technology.",
        "published": "2002-07-12T02:51:45Z",
        "link": "http://arxiv.org/abs/cs/0207055v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "F.1.1; F.1.2; F.2.0; F.4.1; I.1.2; I.2.0"
        ]
    },
    {
        "title": "Modeling Complex Domains of Actions and Change",
        "authors": [
            "Antonis Kakas",
            "Loizos Michael"
        ],
        "summary": "This paper studies the problem of modeling complex domains of actions and change within high-level action description languages. We investigate two main issues of concern: (a) can we represent complex domains that capture together different problems such as ramifications, non-determinism and concurrency of actions, at a high-level, close to the given natural ontology of the problem domain and (b) what features of such a representation can affect, and how, its computational behaviour. The paper describes the main problems faced in this representation task and presents the results of an empirical study, carried out through a series of controlled experiments, to analyze the computational performance of reasoning in these representations. The experiments compare different representations obtained, for example, by changing the basic ontology of the domain or by varying the degree of use of indirect effect laws through domain constraints. This study has helped to expose the main sources of computational difficulty in the reasoning and suggest some methodological guidelines for representing complex domains. Although our work has been carried out within one particular high-level description language, we believe that the results, especially those that relate to the problems of representation, are independent of the specific modeling language.",
        "published": "2002-07-13T12:00:16Z",
        "link": "http://arxiv.org/abs/cs/0207056v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Value Based Argumentation Frameworks",
        "authors": [
            "T. Bench-Capon"
        ],
        "summary": "This paper introduces the notion of value-based argumentation frameworks, an extension of the standard argumentation frameworks proposed by Dung, which are able toshow how rational decision is possible in cases where arguments derive their force from the social values their acceptance would promote.",
        "published": "2002-07-15T11:30:16Z",
        "link": "http://arxiv.org/abs/cs/0207059v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Preferred well-founded semantics for logic programming by alternating   fixpoints: Preliminary report",
        "authors": [
            "Torsten Schaub",
            "Kewen Wang"
        ],
        "summary": "We analyze the problem of defining well-founded semantics for ordered logic programs within a general framework based on alternating fixpoint theory. We start by showing that generalizations of existing answer set approaches to preference are too weak in the setting of well-founded semantics. We then specify some informal yet intuitive criteria and propose a semantical framework for preference handling that is more suitable for defining well-founded semantics for ordered logic programs. The suitability of the new approach is convinced by the fact that many attractive properties are satisfied by our semantics. In particular, our semantics is still correct with respect to various existing answer sets semantics while it successfully overcomes the weakness of their generalization to well-founded semantics. Finally, we indicate how an existing preferred well-founded semantics can be captured within our semantical framework.",
        "published": "2002-07-15T13:30:24Z",
        "link": "http://arxiv.org/abs/cs/0207060v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Interpolation Theorems for Nonmonotonic Reasoning Systems",
        "authors": [
            "Eyal Amir"
        ],
        "summary": "Craig's interpolation theorem (Craig 1957) is an important theorem known for propositional logic and first-order logic. It says that if a logical formula $\\beta$ logically follows from a formula $\\alpha$, then there is a formula $\\gamma$, including only symbols that appear in both $\\alpha,\\beta$, such that $\\beta$ logically follows from $\\gamma$ and $\\gamma$ logically follows from $\\alpha$. Such theorems are important and useful for understanding those logics in which they hold as well as for speeding up reasoning with theories in those logics. In this paper we present interpolation theorems in this spirit for three nonmonotonic systems: circumscription, default logic and logic programs with the stable models semantics (a.k.a. answer set semantics). These results give us better understanding of those logics, especially in contrast to their nonmonotonic characteristics. They suggest that some \\emph{monotonicity} principle holds despite the failure of classic monotonicity for these logics. Also, they sometimes allow us to use methods for the decomposition of reasoning for these systems, possibly increasing their applicability and tractability. Finally, they allow us to build structured representations that use those logics.",
        "published": "2002-07-16T06:27:08Z",
        "link": "http://arxiv.org/abs/cs/0207064v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3;I.2.4;F.4.1"
        ]
    },
    {
        "title": "Embedding Default Logic in Propositional Argumentation Systems",
        "authors": [
            "Dritan Berzati",
            "Bernhard Anrig",
            "Juerg Kohlas"
        ],
        "summary": "In this paper we present a transformation of finite propositional default theories into so-called propositional argumentation systems. This transformation allows to characterize all notions of Reiter's default logic in the framework of argumentation systems. As a consequence, computing extensions, or determining wether a given formula belongs to one extension or all extensions can be answered without leaving the field of classical propositional logic. The transformation proposed is linear in the number of defaults.",
        "published": "2002-07-16T15:16:07Z",
        "link": "http://arxiv.org/abs/cs/0207065v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "On the existence and multiplicity of extensions in dialectical   argumentation",
        "authors": [
            "Bart Verheij"
        ],
        "summary": "In the present paper, the existence and multiplicity problems of extensions are addressed. The focus is on extension of the stable type. The main result of the paper is an elegant characterization of the existence and multiplicity of extensions in terms of the notion of dialectical justification, a close cousin of the notion of admissibility. The characterization is given in the context of the particular logic for dialectical argumentation DEFLOG. The results are of direct relevance for several well-established models of defeasible reasoning (like default logic, logic programming and argumentation frameworks), since elsewhere dialectical argumentation has been shown to have close formal connections with these models.",
        "published": "2002-07-17T12:09:45Z",
        "link": "http://arxiv.org/abs/cs/0207067v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "A Polynomial Translation of Logic Programs with Nested Expressions into   Disjunctive Logic Programs: Preliminary Report",
        "authors": [
            "David Pearce",
            "Vladimir Sarsakov",
            "Torsten Schaub",
            "Hans Tompits",
            "Stefan Woltran"
        ],
        "summary": "Nested logic programs have recently been introduced in order to allow for arbitrarily nested formulas in the heads and the bodies of logic program rules under the answer sets semantics. Nested expressions can be formed using conjunction, disjunction, as well as the negation as failure operator in an unrestricted fashion. This provides a very flexible and compact framework for knowledge representation and reasoning. Previous results show that nested logic programs can be transformed into standard (unnested) disjunctive logic programs in an elementary way, applying the negation as failure operator to body literals only. This is of great practical relevance since it allows us to evaluate nested logic programs by means of off-the-shelf disjunctive logic programming systems, like DLV. However, it turns out that this straightforward transformation results in an exponential blow-up in the worst-case, despite the fact that complexity results indicate that there is a polynomial translation among both formalisms. In this paper, we take up this challenge and provide a polynomial translation of logic programs with nested expressions into disjunctive logic programs. Moreover, we show that this translation is modular and (strongly) faithful. We have implemented both the straightforward as well as our advanced transformation; the resulting compiler serves as a front-end to DLV and is publicly available on the Web.",
        "published": "2002-07-19T12:17:42Z",
        "link": "http://arxiv.org/abs/cs/0207071v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4"
        ]
    },
    {
        "title": "Complexity of Nested Circumscription and Nested Abnormality Theories",
        "authors": [
            "Marco Cadoli",
            "Thomas Eiter",
            "Georg Gottlob"
        ],
        "summary": "The need for a circumscriptive formalism that allows for simple yet elegant modular problem representation has led Lifschitz (AIJ, 1995) to introduce nested abnormality theories (NATs) as a tool for modular knowledge representation, tailored for applying circumscription to minimize exceptional circumstances. Abstracting from this particular objective, we propose L_{CIRC}, which is an extension of generic propositional circumscription by allowing propositional combinations and nesting of circumscriptive theories. As shown, NATs are naturally embedded into this language, and are in fact of equal expressive capability. We then analyze the complexity of L_{CIRC} and NATs, and in particular the effect of nesting. The latter is found to be a source of complexity, which climbs the Polynomial Hierarchy as the nesting depth increases and reaches PSPACE-completeness in the general case. We also identify meaningful syntactic fragments of NATs which have lower complexity. In particular, we show that the generalization of Horn circumscription in the NAT framework remains CONP-complete, and that Horn NATs without fixed letters can be efficiently transformed into an equivalent Horn CNF, which implies polynomial solvability of principal reasoning tasks. Finally, we also study extensions of NATs and briefly address the complexity in the first-order case. Our results give insight into the ``cost'' of using L_{CIRC} (resp. NATs) as a host language for expressing other formalisms such as action theories, narratives, or spatial theories.",
        "published": "2002-07-20T19:59:21Z",
        "link": "http://arxiv.org/abs/cs/0207072v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LO",
            "I.2.3; I.2.4; F4.1; F.2.2"
        ]
    },
    {
        "title": "Reinforcing Reachable Routes",
        "authors": [
            "Srinidhi Varadarajan",
            "Naren Ramakrishnan"
        ],
        "summary": "This paper studies the evaluation of routing algorithms from the perspective of reachability routing, where the goal is to determine all paths between a sender and a receiver. Reachability routing is becoming relevant with the changing dynamics of the Internet and the emergence of low-bandwidth wireless/ad-hoc networks. We make the case for reinforcement learning as the framework of choice to realize reachability routing, within the confines of the current Internet infrastructure. The setting of the reinforcement learning problem offers several advantages, including loop resolution, multi-path forwarding capability, cost-sensitive routing, and minimizing state overhead, while maintaining the incremental spirit of current backbone routing algorithms. We identify research issues in reinforcement learning applied to the reachability routing problem to achieve a fluid and robust backbone routing framework. The paper is targeted toward practitioners seeking to implement a reachability routing algorithm.",
        "published": "2002-07-21T02:02:43Z",
        "link": "http://arxiv.org/abs/cs/0207073v1",
        "categories": [
            "cs.NI",
            "cs.AI",
            "C.2.2; I.2.6"
        ]
    },
    {
        "title": "Nonmonotonic Probabilistic Logics between Model-Theoretic Probabilistic   Logic and Probabilistic Logic under Coherence",
        "authors": [
            "Thomas Lukasiewicz"
        ],
        "summary": "Recently, it has been shown that probabilistic entailment under coherence is weaker than model-theoretic probabilistic entailment. Moreover, probabilistic entailment under coherence is a generalization of default entailment in System P. In this paper, we continue this line of research by presenting probabilistic generalizations of more sophisticated notions of classical default entailment that lie between model-theoretic probabilistic entailment and probabilistic entailment under coherence. That is, the new formalisms properly generalize their counterparts in classical default reasoning, they are weaker than model-theoretic probabilistic entailment, and they are stronger than probabilistic entailment under coherence. The new formalisms are useful especially for handling probabilistic inconsistencies related to conditioning on zero events. They can also be applied for probabilistic belief revision. More generally, in the same spirit as a similar previous paper, this paper sheds light on exciting new formalisms for probabilistic reasoning beyond the well-known standard ones.",
        "published": "2002-07-22T01:44:25Z",
        "link": "http://arxiv.org/abs/cs/0207075v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Evaluating Defaults",
        "authors": [
            "Henry E. Kyburg Jr.",
            "Choh Man Teng"
        ],
        "summary": "We seek to find normative criteria of adequacy for nonmonotonic logic similar to the criterion of validity for deductive logic. Rather than stipulating that the conclusion of an inference be true in all models in which the premises are true, we require that the conclusion of a nonmonotonic inference be true in ``almost all'' models of a certain sort in which the premises are true. This ``certain sort'' specification picks out the models that are relevant to the inference, taking into account factors such as specificity and vagueness, and previous inferences. The frequencies characterizing the relevant models reflect known frequencies in our actual world. The criteria of adequacy for a default inference can be extended by thresholding to criteria of adequacy for an extension. We show that this avoids the implausibilities that might otherwise result from the chaining of default inferences. The model proportions, when construed in terms of frequencies, provide a verifiable grounding of default rules, and can become the basis for generating default rules from statistics.",
        "published": "2002-07-24T23:05:29Z",
        "link": "http://arxiv.org/abs/cs/0207083v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "A Paraconsistent Higher Order Logic",
        "authors": [
            "Jørgen Villadsen"
        ],
        "summary": "Classical logic predicts that everything (thus nothing useful at all) follows from inconsistency. A paraconsistent logic is a logic where an inconsistency does not lead to such an explosion, and since in practice consistency is difficult to achieve there are many potential applications of paraconsistent logics in knowledge-based systems, logical semantics of natural language, etc. Higher order logics have the advantages of being expressive and with several automated theorem provers available. Also the type system can be helpful. We present a concise description of a paraconsistent higher order logic with countable infinite indeterminacy, where each basic formula can get its own indeterminate truth value (or as we prefer: truth code). The meaning of the logical operators is new and rather different from traditional many-valued logics as well as from logics based on bilattices. The adequacy of the logic is examined by a case study in the domain of medicine. Thus we try to build a bridge between the HOL and MVL communities. A sequent calculus is proposed based on recent work by Muskens.",
        "published": "2002-07-25T16:35:47Z",
        "link": "http://arxiv.org/abs/cs/0207088v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.4; I.2.1"
        ]
    },
    {
        "title": "Optimal Ordered Problem Solver",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "We present a novel, general, optimally fast, incremental way of searching for a universal algorithm that solves each task in a sequence of tasks. The Optimal Ordered Problem Solver (OOPS) continually organizes and exploits previously found solutions to earlier tasks, efficiently searching not only the space of domain-specific algorithms, but also the space of search algorithms. Essentially we extend the principles of optimal nonincremental universal search to build an incremental universal learner that is able to improve itself through experience. In illustrative experiments, our self-improver becomes the first general system that learns to solve all n disk Towers of Hanoi tasks (solution size 2^n-1) for n up to 30, profiting from previously solved, simpler tasks involving samples of a simple context free language.",
        "published": "2002-07-31T14:33:11Z",
        "link": "http://arxiv.org/abs/cs/0207097v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LG",
            "I.2.2;I.2.6;I.2.8"
        ]
    },
    {
        "title": "Soft Concurrent Constraint Programming",
        "authors": [
            "S. Bistarelli",
            "U. Montanari",
            "F. Rossi"
        ],
        "summary": "Soft constraints extend classical constraints to represent multiple consistency levels, and thus provide a way to express preferences, fuzziness, and uncertainty. While there are many soft constraint solving formalisms, even distributed ones, by now there seems to be no concurrent programming framework where soft constraints can be handled. In this paper we show how the classical concurrent constraint (cc) programming framework can work with soft constraints, and we also propose an extension of cc languages which can use soft constraints to prune and direct the search for a solution. We believe that this new programming paradigm, called soft cc (scc), can be also very useful in many web-related scenarios. In fact, the language level allows web agents to express their interaction and negotiation protocols, and also to post their requests in terms of preferences, and the underlying soft constraint solver can find an agreement among the agents even if their requests are incompatible.",
        "published": "2002-08-06T19:08:55Z",
        "link": "http://arxiv.org/abs/cs/0208008v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.1.3; D.3.1; D.3.2; D.3.3; F.3.2"
        ]
    },
    {
        "title": "Offline Specialisation in Prolog Using a Hand-Written Compiler Generator",
        "authors": [
            "Michael Leuschel",
            "Jesper Joergensen",
            "Wim Vanhoof",
            "Maurice Bruynooghe"
        ],
        "summary": "The so called ``cogen approach'' to program specialisation, writing a compiler generator instead of a specialiser, has been used with considerable success in partial evaluation of both functional and imperative languages. This paper demonstrates that the cogen approach is also applicable to the specialisation of logic programs (also called partial deduction) and leads to effective specialisers. Moreover, using good binding-time annotations, the speed-ups of the specialised programs are comparable to the speed-ups obtained with online specialisers. The paper first develops a generic approach to offline partial deduction and then a specific offline partial deduction method, leading to the offline system LIX for pure logic programs. While this is a usable specialiser by itself, it is used to develop the cogen system LOGEN. Given a program, a specification of what inputs will be static, and an annotation specifying which calls should be unfolded, LOGEN generates a specialised specialiser for the program at hand. Running this specialiser with particular values for the static inputs results in the specialised program. While this requires two steps instead of one, the efficiency of the specialisation process is improved in situations where the same program is specialised multiple times. The paper also presents and evaluates an automatic binding-time analysis that is able to derive the annotations. While the derived annotations are still suboptimal compared to hand-crafted ones, they enable non-expert users to use the LOGEN system in a fully automated way. Finally, LOGEN is extended so as to directly support a large part of Prolog's declarative and non-declarative features and so as to be able to perform so called mixline specialisations.",
        "published": "2002-08-07T11:14:53Z",
        "link": "http://arxiv.org/abs/cs/0208009v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.1.6; D.1.2; I.2.2; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Linking Makinson and Kraus-Lehmann-Magidor preferential entailments",
        "authors": [
            "Yves Moinard"
        ],
        "summary": "About ten years ago, various notions of preferential entailment have been introduced. The main reference is a paper by Kraus, Lehmann and Magidor (KLM), one of the main competitor being a more general version defined by Makinson (MAK). These two versions have already been compared, but it is time to revisit these comparisons. Here are our three main results: (1) These two notions are equivalent, provided that we restrict our attention, as done in KLM, to the cases where the entailment respects logical equivalence (on the left and on the right). (2) A serious simplification of the description of the fundamental cases in which MAK is equivalent to KLM, including a natural passage in both ways. (3) The two previous results are given for preferential entailments more general than considered in some of the original texts, but they apply also to the original definitions and, for this particular case also, the models can be simplified.",
        "published": "2002-08-08T17:08:46Z",
        "link": "http://arxiv.org/abs/cs/0208017v1",
        "categories": [
            "cs.AI",
            "I.2.3; F.4.1"
        ]
    },
    {
        "title": "Knowledge Representation",
        "authors": [
            "Mikalai Birukou"
        ],
        "summary": "This work analyses main features that should be present in knowledge representation. It suggests a model for representation and a way to implement this model in software. Representation takes care of both low-level sensor information and high-level concepts.",
        "published": "2002-08-12T22:34:47Z",
        "link": "http://arxiv.org/abs/cs/0208019v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Complete Axiomatizations for Reasoning About Knowledge and Time",
        "authors": [
            "Joseph Y. Halpern",
            "Ron van der Meyden",
            "Moshe Y. Vardi"
        ],
        "summary": "Sound and complete axiomatizations are provided for a number of different logics involving modalities for knowledge and time. These logics arise from different choices for various parameters. All the logics considered involve the discrete time linear temporal logic operators `next' and `until' and an operator for the knowledge of each of a number of agents. Both the single agent and multiple agent cases are studied: in some instances of the latter there is also an operator for the common knowledge of the group of all agents. Four different semantic properties of agents are considered: whether they have a unique initial state, whether they operate synchronously, whether they have perfect recall, and whether they learn. The property of no learning is essentially dual to perfect recall. Not all settings of these parameters lead to recursively axiomatizable logics, but sound and complete axiomatizations are presented for all the ones that do.",
        "published": "2002-08-20T22:55:24Z",
        "link": "http://arxiv.org/abs/cs/0208033v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1, I.2.4"
        ]
    },
    {
        "title": "Causes and Explanations: A Structural-Model Approach. Part II:   Explanations",
        "authors": [
            "Joseph Y. Halpern",
            "Judea Pearl"
        ],
        "summary": "We propose new definitions of (causal) explanation, using structural equations to model counterfactuals. The definition is based on the notion of actual cause, as defined and motivated in a companion paper. Essentially, an explanation is a fact that is not known for certain but, if found to be true, would constitute an actual cause of the fact to be explained, regardless of the agent's initial uncertainty. We show that the definition handles well a number of problematic examples from the literature.",
        "published": "2002-08-20T23:08:49Z",
        "link": "http://arxiv.org/abs/cs/0208034v3",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "The partition semantics of questions, syntactically",
        "authors": [
            "Chung-chieh Shan",
            "Balder D. ten Cate"
        ],
        "summary": "Groenendijk and Stokhof (1984, 1996; Groenendijk 1999) provide a logically attractive theory of the semantics of natural language questions, commonly referred to as the partition theory. Two central notions in this theory are entailment between questions and answerhood. For example, the question \"Who is going to the party?\" entails the question \"Is John going to the party?\", and \"John is going to the party\" counts as an answer to both. Groenendijk and Stokhof define these two notions in terms of partitions of a set of possible worlds.   We provide a syntactic characterization of entailment between questions and answerhood . We show that answers are, in some sense, exactly those formulas that are built up from instances of the question. This result lets us compare the partition theory with other approaches to interrogation -- both linguistic analyses, such as Hamblin's and Karttunen's semantics, and computational systems, such as Prolog. Our comparison separates a notion of answerhood into three aspects: equivalence (when two questions or answers are interchangeable), atomic answers (what instances of a question count as answers), and compound answers (how answers compose).",
        "published": "2002-09-04T20:11:14Z",
        "link": "http://arxiv.org/abs/cs/0209008v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LO",
            "F.4.1; I.2.3; I.2.7"
        ]
    },
    {
        "title": "Question answering: from partitions to Prolog",
        "authors": [
            "Balder D. ten Cate",
            "Chung-chieh Shan"
        ],
        "summary": "We implement Groenendijk and Stokhof's partition semantics of questions in a simple question answering algorithm. The algorithm is sound, complete, and based on tableau theorem proving. The algorithm relies on a syntactic characterization of answerhood: Any answer to a question is equivalent to some formula built up only from instances of the question. We prove this characterization by translating the logic of interrogation to classical predicate logic and applying Craig's interpolation theorem.",
        "published": "2002-09-04T20:46:59Z",
        "link": "http://arxiv.org/abs/cs/0209009v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LO",
            "F.4.1; I.2.3; I.2.7"
        ]
    },
    {
        "title": "Reasoning about Evolving Nonmonotonic Knowledge Bases",
        "authors": [
            "T. Eiter",
            "M. Fink",
            "G. Sabbatini",
            "H. Tompits"
        ],
        "summary": "Recently, several approaches to updating knowledge bases modeled as extended logic programs have been introduced, ranging from basic methods to incorporate (sequences of) sets of rules into a logic program, to more elaborate methods which use an update policy for specifying how updates must be incorporated. In this paper, we introduce a framework for reasoning about evolving knowledge bases, which are represented as extended logic programs and maintained by an update policy. We first describe a formal model which captures various update approaches, and we define a logical language for expressing properties of evolving knowledge bases. We then investigate semantical and computational properties of our framework, where we focus on properties of knowledge states with respect to the canonical reasoning task of whether a given formula holds on a given evolving knowledge base. In particular, we present finitary characterizations of the evolution for certain classes of framework instances, which can be exploited for obtaining decidability results. In more detail, we characterize the complexity of reasoning for some meaningful classes of evolving knowledge bases, ranging from polynomial to double exponential space complexity.",
        "published": "2002-09-16T19:23:19Z",
        "link": "http://arxiv.org/abs/cs/0209019v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4; F.4.1"
        ]
    },
    {
        "title": "A Comparison of Different Cognitive Paradigms Using Simple Animats in a   Virtual Laboratory, with Implications to the Notion of Cognition",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "In this thesis I present a virtual laboratory which implements five different models for controlling animats: a rule-based system, a behaviour-based system, a concept-based system, a neural network, and a Braitenberg architecture. Through different experiments, I compare the performance of the models and conclude that there is no \"best\" model, since different models are better for different things in different contexts.   The models I chose, although quite simple, represent different approaches for studying cognition. Using the results as an empirical philosophical aid,   I note that there is no \"best\" approach for studying cognition, since different approaches have all advantages and disadvantages, because they study different aspects of cognition from different contexts. This has implications for current debates on \"proper\" approaches for cognition: all approaches are a bit proper, but none will be \"proper enough\". I draw remarks on the notion of cognition abstracting from all the approaches used to study it, and propose a simple classification for different types of cognition.",
        "published": "2002-09-19T16:35:55Z",
        "link": "http://arxiv.org/abs/cs/0209022v2",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Extremal Optimization: an Evolutionary Local-Search Algorithm",
        "authors": [
            "Stefan Boettcher",
            "Allon G. Percus"
        ],
        "summary": "A recently introduced general-purpose heuristic for finding high-quality solutions for many hard optimization problems is reviewed. The method is inspired by recent progress in understanding far-from-equilibrium phenomena in terms of {\\em self-organized criticality,} a concept introduced to describe emergent complexity in physical systems. This method, called {\\em extremal optimization,} successively replaces the value of extremely undesirable variables in a sub-optimal solution with new, random ones. Large, avalanche-like fluctuations in the cost function self-organize from this dynamics, effectively scaling barriers to explore local optima in distant neighborhoods of the configuration space while eliminating the need to tune parameters. Drawing upon models used to simulate the dynamics of granular media, evolution, or geology, extremal optimization complements approximation methods inspired by equilibrium statistical physics, such as {\\em simulated annealing}. It may be but one example of applying new insights into {\\em non-equilibrium phenomena} systematically to hard optimization problems. This method is widely applicable and so far has proved competitive with -- and even superior to -- more elaborate general-purpose heuristics on testbeds of constrained optimization problems with up to $10^5$ variables, such as bipartitioning, coloring, and satisfiability. Analysis of a suitable model predicts the only free parameter of the method in accordance with all experimental results.",
        "published": "2002-09-26T14:16:15Z",
        "link": "http://arxiv.org/abs/cs/0209030v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "Revising Partially Ordered Beliefs",
        "authors": [
            "Salem Benferhat",
            "Sylvain Lagrue",
            "Odile Papini"
        ],
        "summary": "This paper deals with the revision of partially ordered beliefs. It proposes a semantic representation of epistemic states by partial pre-orders on interpretations and a syntactic representation by partially ordered belief bases. Two revision operations, the revision stemming from the history of observations and the possibilistic revision, defined when the epistemic state is represented by a total pre-order, are generalized, at a semantic level, to the case of a partial pre-order on interpretations, and at a syntactic level, to the case of a partially ordered belief base. The equivalence between the two representations is shown for the two revision operations.",
        "published": "2002-10-03T09:29:54Z",
        "link": "http://arxiv.org/abs/cs/0210004v1",
        "categories": [
            "cs.AI",
            "I.2.3;I.2.4"
        ]
    },
    {
        "title": "Compilability of Abduction",
        "authors": [
            "Paolo Liberatore",
            "Marco Schaerf"
        ],
        "summary": "Abduction is one of the most important forms of reasoning; it has been successfully applied to several practical problems such as diagnosis. In this paper we investigate whether the computational complexity of abduction can be reduced by an appropriate use of preprocessing. This is motivated by the fact that part of the data of the problem (namely, the set of all possible assumptions and the theory relating assumptions and manifestations) are often known before the rest of the problem. In this paper, we show some complexity results about abduction when compilation is allowed.",
        "published": "2002-10-09T17:17:27Z",
        "link": "http://arxiv.org/abs/cs/0210007v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "F.4.1; F.1.3"
        ]
    },
    {
        "title": "Geometric Aspects of Multiagent Systems",
        "authors": [
            "Timothy Porter"
        ],
        "summary": "Recent advances in Multiagent Systems (MAS) and Epistemic Logic within Distributed Systems Theory, have used various combinatorial structures that model both the geometry of the systems and the Kripke model structure of models for the logic. Examining one of the simpler versions of these models, interpreted systems, and the related Kripke semantics of the logic $S5_n$ (an epistemic logic with $n$-agents), the similarities with the geometric / homotopy theoretic structure of groupoid atlases is striking. These latter objects arise in problems within algebraic K-theory, an area of algebra linked to the study of decomposition and normal form theorems in linear algebra. They have a natural well structured notion of path and constructions of path objects, etc., that yield a rich homotopy theory.",
        "published": "2002-10-25T16:32:59Z",
        "link": "http://arxiv.org/abs/cs/0210023v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11 Distributed Artificial Intelligence"
        ]
    },
    {
        "title": "Encoding a Taxonomy of Web Attacks with Different-Length Vectors",
        "authors": [
            "Gonzalo Alvarez",
            "Slobodan Petrovic"
        ],
        "summary": "Web attacks, i.e. attacks exclusively using the HTTP protocol, are rapidly becoming one of the fundamental threats for information systems connected to the Internet. When the attacks suffered by web servers through the years are analyzed, it is observed that most of them are very similar, using a reduced number of attacking techniques. It is generally agreed that classification can help designers and programmers to better understand attacks and build more secure applications. As an effort in this direction, a new taxonomy of web attacks is proposed in this paper, with the objective of obtaining a practically useful reference framework for security applications. The use of the taxonomy is illustrated by means of multiplatform real world web attack examples. Along with this taxonomy, important features of each attack category are discussed. A suitable semantic-dependent web attack encoding scheme is defined that uses different-length vectors. Possible applications are described, which might benefit from this taxonomy and encoding scheme, such as intrusion detection systems and application firewalls.",
        "published": "2002-10-29T10:18:36Z",
        "link": "http://arxiv.org/abs/cs/0210026v1",
        "categories": [
            "cs.CR",
            "cs.AI",
            "H.3.3;H.3.5"
        ]
    },
    {
        "title": "A uniform approach to logic programming semantics",
        "authors": [
            "Pascal Hitzler",
            "Matthias Wendt"
        ],
        "summary": "Part of the theory of logic programming and nonmonotonic reasoning concerns the study of fixed-point semantics for these paradigms. Several different semantics have been proposed during the last two decades, and some have been more successful and acknowledged than others. The rationales behind those various semantics have been manifold, depending on one's point of view, which may be that of a programmer or inspired by commonsense reasoning, and consequently the constructions which lead to these semantics are technically very diverse, and the exact relationships between them have not yet been fully understood. In this paper, we present a conceptually new method, based on level mappings, which allows to provide uniform characterizations of different semantics for logic programs. We will display our approach by giving new and uniform characterizations of some of the major semantics, more particular of the least model semantics for definite programs, of the Fitting semantics, and of the well-founded semantics. A novel characterization of the weakly perfect model semantics will also be provided.",
        "published": "2002-10-29T11:37:31Z",
        "link": "http://arxiv.org/abs/cs/0210027v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; D.1.6; F.4.1"
        ]
    },
    {
        "title": "Intelligence and Cooperative Search by Coupled Local Minimizers",
        "authors": [
            "J. A. K. Suykens",
            "J. Vandewalle",
            "B. De Moor"
        ],
        "summary": "We show how coupling of local optimization processes can lead to better solutions than multi-start local optimization consisting of independent runs. This is achieved by minimizing the average energy cost of the ensemble, subject to synchronization constraints between the state vectors of the individual local minimizers. From an augmented Lagrangian which incorporates the synchronization constraints both as soft and hard constraints, a network is derived wherein the local minimizers interact and exchange information through the synchronization constraints. From the viewpoint of neural networks, the array can be considered as a Lagrange programming network for continuous optimization and as a cellular neural network (CNN). The penalty weights associated with the soft state synchronization constraints follow from the solution to a linear program. This expresses that the energy cost of the ensemble should maximally decrease. In this way successful local minimizers can implicitly impose their state to the others through a mechanism of master-slave dynamics resulting into a cooperative search mechanism. Improved information spreading within the ensemble is obtained by applying the concept of small-world networks. This work suggests, in an interdisciplinary context, the importance of information exchange and state synchronization within ensembles, towards issues as evolution, collective behaviour, optimality and intelligence.",
        "published": "2002-10-30T11:59:57Z",
        "link": "http://arxiv.org/abs/cs/0210030v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "cs.NE",
            "F.1.1"
        ]
    },
    {
        "title": "The DLV System for Knowledge Representation and Reasoning",
        "authors": [
            "Nicola Leone",
            "Gerald Pfeifer",
            "Wolfgang Faber",
            "Thomas Eiter",
            "Georg Gottlob",
            "Simona Perri",
            "Francesco Scarcello"
        ],
        "summary": "This paper presents the DLV system, which is widely considered the state-of-the-art implementation of disjunctive logic programming, and addresses several aspects. As for problem solving, we provide a formal definition of its kernel language, function-free disjunctive logic programs (also known as disjunctive datalog), extended by weak constraints, which are a powerful tool to express optimization problems. We then illustrate the usage of DLV as a tool for knowledge representation and reasoning, describing a new declarative programming methodology which allows one to encode complex problems (up to $\\Delta^P_3$-complete problems) in a declarative fashion. On the foundational side, we provide a detailed analysis of the computational complexity of the language of DLV, and by deriving new complexity results we chart a complete picture of the complexity of this language and important fragments thereof.   Furthermore, we illustrate the general architecture of the DLV system which has been influenced by these results. As for applications, we overview application front-ends which have been developed on top of DLV to solve specific knowledge representation tasks, and we briefly describe the main international projects investigating the potential of the system for industrial exploitation. Finally, we report about thorough experimentation and benchmarking, which has been carried out to assess the efficiency of the system. The experimental results confirm the solidity of DLV and highlight its potential for emerging application areas like knowledge management and information integration.",
        "published": "2002-11-04T15:18:04Z",
        "link": "http://arxiv.org/abs/cs/0211004v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.PL",
            "I.2.3; I.2.4; D.3.1"
        ]
    },
    {
        "title": "Maximing the Margin in the Input Space",
        "authors": [
            "Shotaro Akaho"
        ],
        "summary": "We propose a novel criterion for support vector machine learning: maximizing the margin in the input space, not in the feature (Hilbert) space. This criterion is a discriminative version of the principal curve proposed by Hastie et al. The criterion is appropriate in particular when the input space is already a well-designed feature space with rather small dimensionality. The definition of the margin is generalized in order to represent prior knowledge. The derived algorithm consists of two alternating steps to estimate the dual parameters. Firstly, the parameters are initialized by the original SVM. Then one set of parameters is updated by Newton-like procedure, and the other set is updated by solving a quadratic programming problem. The algorithm converges in a few steps to a local optimum under mild conditions and it preserves the sparsity of support vectors. Although the complexity to calculate temporal variables increases the complexity to solve the quadratic programming problem for each step does not change. It is also shown that the original SVM can be seen as a special case. We further derive a simplified algorithm which enables us to use the existing code for the original SVM.",
        "published": "2002-11-07T06:44:54Z",
        "link": "http://arxiv.org/abs/cs/0211006v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Can the whole brain be simpler than its \"parts\"?",
        "authors": [
            "Victor Eliashberg"
        ],
        "summary": "This is the first in a series of connected papers discussing the problem of a dynamically reconfigurable universal learning neurocomputer that could serve as a computational model for the whole human brain. The whole series is entitled \"The Brain Zero Project. My Brain as a Dynamically Reconfigurable Universal Learning Neurocomputer.\" (For more information visit the website www.brain0.com.) This introductory paper is concerned with general methodology. Its main goal is to explain why it is critically important for both neural modeling and cognitive modeling to pay much attention to the basic requirements of the whole brain as a complex computing system. The author argues that it can be easier to develop an adequate computational model for the whole \"unprogrammed\" (untrained) human brain than to find adequate formal representations of some nontrivial parts of brain's performance. (In the same way as, for example, it is easier to describe the behavior of a complex analytical function than the behavior of its real and/or imaginary part.) The \"curse of dimensionality\" that plagues purely phenomenological (\"brainless\") cognitive theories is a natural penalty for an attempt to represent insufficiently large parts of brain's performance in a state space of insufficiently high dimensionality. A \"partial\" modeler encounters \"Catch 22.\" An attempt to simplify a cognitive problem by artificially reducing its dimensionality makes the problem more difficult.",
        "published": "2002-11-09T17:16:18Z",
        "link": "http://arxiv.org/abs/cs/0211008v4",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Vanquishing the XCB Question: The Methodology Discovery of the Last   Shortest Single Axiom for the Equivalential Calculus",
        "authors": [
            "Larry Wos",
            "Dolph Ulrich",
            "Branden Fitelson"
        ],
        "summary": "With the inclusion of an effective methodology, this article answers in detail a question that, for a quarter of a century, remained open despite intense study by various researchers. Is the formula XCB = e(x,e(e(e(x,y),e(z,y)),z)) a single axiom for the classical equivalential calculus when the rules of inference consist of detachment (modus ponens) and substitution? Where the function e represents equivalence, this calculus can be axiomatized quite naturally with the formulas e(x,x), e(e(x,y),e(y,x)), and e(e(x,y),e(e(y,z),e(x,z))), which correspond to reflexivity, symmetry, and transitivity, respectively. (We note that e(x,x) is dependent on the other two axioms.) Heretofore, thirteen shortest single axioms for classical equivalence of length eleven had been discovered, and XCB was the only remaining formula of that length whose status was undetermined. To show that XCB is indeed such a single axiom, we focus on the rule of condensed detachment, a rule that captures detachment together with an appropriately general, but restricted, form of substitution. The proof we present in this paper consists of twenty-five applications of condensed detachment, completing with the deduction of transitivity followed by a deduction of symmetry. We also discuss some factors that may explain in part why XCB resisted relinquishing its treasure for so long. Our approach relied on diverse strategies applied by the automated reasoning program OTTER. Thus ends the search for shortest single axioms for the equivalential calculus.",
        "published": "2002-11-13T16:46:31Z",
        "link": "http://arxiv.org/abs/cs/0211014v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "XCB, the Last of the Shortest Single Axioms for the Classical   Equivalential Calculus",
        "authors": [
            "Larry Wos",
            "Dolph Ulrich",
            "Branden Fitelson"
        ],
        "summary": "It has long been an open question whether the formula XCB = EpEEEpqErqr is, with the rules of substitution and detachment, a single axiom for the classical equivalential calculus. This paper answers that question affirmatively, thus completing a search for all such eleven-symbol single axioms that began seventy years ago.",
        "published": "2002-11-13T20:23:11Z",
        "link": "http://arxiv.org/abs/cs/0211015v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Adaptive Development of Koncepts in Virtual Animats: Insights into the   Development of Knowledge",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "As a part of our effort for studying the evolution and development of cognition, we present results derived from synthetic experimentations in a virtual laboratory where animats develop koncepts adaptively and ground their meaning through action. We introduce the term \"koncept\" to avoid confusions and ambiguity derived from the wide use of the word \"concept\". We present the models which our animats use for abstracting koncepts from perceptions, plastically adapt koncepts, and associate koncepts with actions. On a more philosophical vein, we suggest that knowledge is a property of a cognitive system, not an element, and therefore observer-dependent.",
        "published": "2002-11-21T18:13:31Z",
        "link": "http://arxiv.org/abs/cs/0211027v1",
        "categories": [
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "Thinking Adaptive: Towards a Behaviours Virtual Laboratory",
        "authors": [
            "Carlos Gershenson",
            "Pedro Pablo Gonzalez",
            "Jose Negrete"
        ],
        "summary": "In this paper we name some of the advantages of virtual laboratories; and propose that a Behaviours Virtual Laboratory should be useful for both biologists and AI researchers, offering a new perspective for understanding adaptive behaviour. We present our development of a Behaviours Virtual Laboratory, which at this stage is focused in action selection, and show some experiments to illustrate the properties of our proposal, which can be accessed via Internet.",
        "published": "2002-11-21T18:35:23Z",
        "link": "http://arxiv.org/abs/cs/0211028v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "I.2.0, I.6.7"
        ]
    },
    {
        "title": "Redundancy in Logic I: CNF Propositional Formulae",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "A knowledge base is redundant if it contains parts that can be inferred from the rest of it. We study the problem of checking whether a CNF formula (a set of clauses) is redundant, that is, it contains clauses that can be derived from the other ones. Any CNF formula can be made irredundant by deleting some of its clauses: what results is an irredundant equivalent subset (I.E.S.) We study the complexity of some related problems: verification, checking existence of a I.E.S. with a given size, checking necessary and possible presence of clauses in I.E.S.'s, and uniqueness. We also consider the problem of redundancy with different definitions of equivalence.",
        "published": "2002-11-22T18:23:22Z",
        "link": "http://arxiv.org/abs/cs/0211031v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "I.2.4, F.1.3"
        ]
    },
    {
        "title": "Propositional satisfiability in declarative programming",
        "authors": [
            "Deborah East",
            "Miroslaw Truszczynski"
        ],
        "summary": "Answer-set programming (ASP) paradigm is a way of using logic to solve search problems. Given a search problem, to solve it one designs a theory in the logic so that models of this theory represent problem solutions. To compute a solution to a problem one needs to compute a model of the corresponding theory. Several answer-set programming formalisms have been developed on the basis of logic programming with the semantics of stable models. In this paper we show that also the logic of predicate calculus gives rise to effective implementations of the ASP paradigm, similar in spirit to logic programming with stable model semantics and with a similar scope of applicability. Specifically, we propose two logics based on predicate calculus as formalisms for encoding search problems. We show that the expressive power of these logics is given by the class NP-search. We demonstrate how to use them in programming and develop computational tools for model finding. In the case of one of the logics our techniques reduce the problem to that of propositional satisfiability and allow one to use off-the-shelf satisfiability solvers. The language of the other logic has more complex syntax and provides explicit means to model some high-level constraints. For theories in this logic, we designed our own solver that takes advantage of the expanded syntax. We present experimental results demonstrating computational effectiveness of the overall approach.",
        "published": "2002-11-25T13:15:58Z",
        "link": "http://arxiv.org/abs/cs/0211033v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3;D.1.6"
        ]
    },
    {
        "title": "Monadic Style Control Constructs for Inference Systems",
        "authors": [
            "Jean-Marie Chauvet"
        ],
        "summary": "Recent advances in programming languages study and design have established a standard way of grounding computational systems representation in category theory. These formal results led to a better understanding of issues of control and side-effects in functional and imperative languages. Another benefit is a better way of modelling computational effects in logical frameworks. With this analogy in mind, we embark on an investigation of inference systems based on considering inference behaviour as a form of computation. We delineate a categorical formalisation of control constructs in inference systems. This representation emphasises the parallel between the modular articulation of the categorical building blocks (triples) used to account for the inference architecture and the modular composition of cognitive processes.",
        "published": "2002-11-25T14:19:26Z",
        "link": "http://arxiv.org/abs/cs/0211035v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "68Q55"
        ]
    },
    {
        "title": "Dynamic Adjustment of the Motivation Degree in an Action Selection   Mechanism",
        "authors": [
            "Carlos Gershenson",
            "Pedro Pablo Gonzalez"
        ],
        "summary": "This paper presents a model for dynamic adjustment of the motivation degree, using a reinforcement learning approach, in an action selection mechanism previously developed by the authors. The learning takes place in the modification of a parameter of the model of combination of internal and external stimuli. Experiments that show the claimed properties are presented, using a VR simulation developed for such purposes. The importance of adaptation by learning in action selection is also discussed.",
        "published": "2002-11-27T10:35:50Z",
        "link": "http://arxiv.org/abs/cs/0211038v1",
        "categories": [
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "Action Selection Properties in a Software Simulated Agent",
        "authors": [
            "Carlos Gershenson Garcia",
            "Pedro Pablo Gonzalez Perez",
            "Jose Negrete Martinez"
        ],
        "summary": "This article analyses the properties of the Internal Behaviour network, an action selection mechanism previously proposed by the authors, with the aid of a simulation developed for such ends. A brief review of the Internal Behaviour network is followed by the explanation of the implementation of the simulation. Then, experiments are presented and discussed analysing the properties of the action selection in the proposed model.",
        "published": "2002-11-27T10:42:31Z",
        "link": "http://arxiv.org/abs/cs/0211039v1",
        "categories": [
            "cs.AI",
            "I.2.9"
        ]
    },
    {
        "title": "A Model for Combination of External and Internal Stimuli in the Action   Selection of an Autonomous Agent",
        "authors": [
            "Pedro Pablo Gonzalez Perez",
            "Jose Negrete Martinez",
            "Ariel Barreiro Garcia",
            "Carlos Gershenson Garcia"
        ],
        "summary": "This paper proposes a model for combination of external and internal stimuli for the action selection in an autonomous agent, based in an action selection mechanism previously proposed by the authors. This combination model includes additive and multiplicative elements, which allows to incorporate new properties, which enhance the action selection. A given parameter a, which is part of the proposed model, allows to regulate the degree of dependence of the observed external behaviour from the internal states of the entity.",
        "published": "2002-11-27T10:45:50Z",
        "link": "http://arxiv.org/abs/cs/0211040v1",
        "categories": [
            "cs.AI",
            "I.2.9"
        ]
    },
    {
        "title": "Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent   Space Alignment",
        "authors": [
            "Zhenyue Zhang",
            "Hongyuan Zha"
        ],
        "summary": "Nonlinear manifold learning from unorganized data points is a very challenging unsupervised learning and data visualization problem with a great variety of applications. In this paper we present a new algorithm for manifold learning and nonlinear dimension reduction. Based on a set of unorganized data points sampled with noise from the manifold, we represent the local geometry of the manifold using tangent spaces learned by fitting an affine subspace in a neighborhood of each data point. Those tangent spaces are aligned to give the internal global coordinates of the data points with respect to the underlying manifold by way of a partial eigendecomposition of the neighborhood connection matrix. We present a careful error analysis of our algorithm and show that the reconstruction errors are of second-order accuracy. We illustrate our algorithm using curves and surfaces both in   2D/3D and higher dimensional Euclidean spaces, and 64-by-64 pixel face images with various pose and lighting conditions. We also address several theoretical and algorithmic issues for further research and improvements.",
        "published": "2002-12-07T18:51:12Z",
        "link": "http://arxiv.org/abs/cs/0212008v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.5.1;I5.3"
        ]
    },
    {
        "title": "Searching for Plannable Domains can Speed up Reinforcement Learning",
        "authors": [
            "Istvan Szita",
            "Balint Takacs",
            "Andras Lorincz"
        ],
        "summary": "Reinforcement learning (RL) involves sequential decision making in uncertain environments. The aim of the decision-making agent is to maximize the benefit of acting in its environment over an extended period of time. Finding an optimal policy in RL may be very slow. To speed up learning, one often used solution is the integration of planning, for example, Sutton's Dyna algorithm, or various other methods using macro-actions.   Here we suggest to separate plannable, i.e., close to deterministic parts of the world, and focus planning efforts in this domain. A novel reinforcement learning method called plannable RL (pRL) is proposed here. pRL builds a simple model, which is used to search for macro actions. The simplicity of the model makes planning computationally inexpensive. It is shown that pRL finds an optimal policy, and that plannable macro actions found by pRL are near-optimal. In turn, it is unnecessary to try large numbers of macro actions, which enables fast learning. The utility of pRL is demonstrated by computer simulations.",
        "published": "2002-12-10T22:15:25Z",
        "link": "http://arxiv.org/abs/cs/0212025v1",
        "categories": [
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "Merging Locally Correct Knowledge Bases: A Preliminary Report",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "Belief integration methods are often aimed at deriving a single and consistent knowledge base that retains as much as possible of the knowledge bases to integrate. The rationale behind this approach is the minimal change principle: the result of the integration process should differ as less as possible from the knowledge bases to integrate. We show that this principle can be reformulated in terms of a more general model of belief revision, based on the assumption that inconsistency is due to the mistakes the knowledge bases contain. Current belief revision strategies are based on a specific kind of mistakes, which however does not include all possible ones. Some alternative possibilities are discussed.",
        "published": "2002-12-28T16:09:25Z",
        "link": "http://arxiv.org/abs/cs/0212053v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4, F.4.1"
        ]
    },
    {
        "title": "Sharpening Occam's Razor",
        "authors": [
            "Ming Li",
            "John Tromp",
            "Paul Vitanyi"
        ],
        "summary": "We provide a new representation-independent formulation of Occam's razor theorem, based on Kolmogorov complexity. This new formulation allows us to:   (i) Obtain better sample complexity than both length-based and VC-based versions of Occam's razor theorem, in many applications.   (ii) Achieve a sharper reverse of Occam's razor theorem than previous work.   Specifically, we weaken the assumptions made in an earlier publication, and extend the reverse to superpolynomial running times.",
        "published": "2002-01-08T16:44:10Z",
        "link": "http://arxiv.org/abs/cs/0201005v2",
        "categories": [
            "cs.LG",
            "cond-mat.dis-nn",
            "cs.AI",
            "cs.CC",
            "math.PR",
            "physics.data-an",
            "F.2, E.4, I.2"
        ]
    },
    {
        "title": "The performance of the batch learner algorithm",
        "authors": [
            "Igor Rivin"
        ],
        "summary": "We analyze completely the convergence speed of the \\emph{batch learning algorithm}, and compare its speed to that of the memoryless learning algorithm and of learning with memory. We show that the batch learning algorithm is never worse than the memoryless learning algorithm (at least asymptotically). Its performance \\emph{vis-a-vis} learning with full memory is less clearcut, and depends on certain probabilistic assumptions.",
        "published": "2002-01-14T18:38:55Z",
        "link": "http://arxiv.org/abs/cs/0201009v1",
        "categories": [
            "cs.LG",
            "cs.DM",
            "I2.6"
        ]
    },
    {
        "title": "The Dynamics of AdaBoost Weights Tells You What's Hard to Classify",
        "authors": [
            "Bruno Caprile",
            "Cesare Furlanello",
            "Stefano Merler"
        ],
        "summary": "The dynamical evolution of weights in the Adaboost algorithm contains useful information about the role that the associated data points play in the built of the Adaboost model. In particular, the dynamics induces a bipartition of the data set into two (easy/hard) classes. Easy points are ininfluential in the making of the model, while the varying relevance of hard points can be gauged in terms of an entropy value associated to their evolution. Smooth approximations of entropy highlight regions where classification is most uncertain. Promising results are obtained when methods proposed are applied in the Optimal Sampling framework.",
        "published": "2002-01-17T13:42:23Z",
        "link": "http://arxiv.org/abs/cs/0201014v1",
        "categories": [
            "cs.LG",
            "cs.DS",
            "I.5.1"
        ]
    },
    {
        "title": "Learning to Play Games in Extensive Form by Valuation",
        "authors": [
            "Philippe Jehiel",
            "Dov Samet"
        ],
        "summary": "A valuation for a player in a game in extensive form is an assignment of numeric values to the players moves. The valuation reflects the desirability moves. We assume a myopic player, who chooses a move with the highest valuation. Valuations can also be revised, and hopefully improved, after each play of the game. Here, a very simple valuation revision is considered, in which the moves made in a play are assigned the payoff obtained in the play. We show that by adopting such a learning process a player who has a winning strategy in a win-lose game can almost surely guarantee a win in a repeated game. When a player has more than two payoffs, a more elaborate learning procedure is required. We consider one that associates with each move the average payoff in the rounds in which this move was made. When all players adopt this learning procedure, with some perturbations, then, with probability 1, strategies that are close to subgame perfect equilibrium are played after some time. A single player who adopts this procedure can guarantee only her individually rational payoff.",
        "published": "2002-01-23T11:58:17Z",
        "link": "http://arxiv.org/abs/cs/0201021v1",
        "categories": [
            "cs.LG",
            "cs.GT",
            "I.2.1; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Extended Comment on Language Trees and Zipping",
        "authors": [
            "Joshua Goodman"
        ],
        "summary": "This is the extended version of a Comment submitted to Physical Review Letters. I first point out the inappropriateness of publishing a Letter unrelated to physics. Next, I give experimental results showing that the technique used in the Letter is 3 times worse and 17 times slower than a simple baseline. And finally, I review the literature, showing that the ideas of the Letter are not novel. I conclude by suggesting that Physical Review Letters should not publish Letters unrelated to physics.",
        "published": "2002-02-21T18:25:29Z",
        "link": "http://arxiv.org/abs/cond-mat/0202383v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CL",
            "cs.LG"
        ]
    },
    {
        "title": "On Learning by Exchanging Advice",
        "authors": [
            "L. Nunes",
            "E. Oliveira"
        ],
        "summary": "One of the main questions concerning learning in Multi-Agent Systems is: (How) can agents benefit from mutual interaction during the learning process?. This paper describes the study of an interactive advice-exchange mechanism as a possible way to improve agents' learning performance. The advice-exchange technique, discussed here, uses supervised learning (backpropagation), where reinforcement is not directly coming from the environment but is based on advice given by peers with better performance score (higher confidence), to enhance the performance of a heterogeneous group of Learning Agents (LAs). The LAs are facing similar problems, in an environment where only reinforcement information is available. Each LA applies a different, well known, learning technique: Random Walk (hill-climbing), Simulated Annealing, Evolutionary Algorithms and Q-Learning. The problem used for evaluation is a simplified traffic-control simulation. Initial results indicate that advice-exchange can improve learning speed, although bad advice and/or blind reliance can disturb the learning performance.",
        "published": "2002-03-07T10:16:25Z",
        "link": "http://arxiv.org/abs/cs/0203010v1",
        "categories": [
            "cs.LG",
            "cs.MA",
            "I.2.6; I.2.11"
        ]
    },
    {
        "title": "Capturing Knowledge of User Preferences: ontologies on recommender   systems",
        "authors": [
            "S. E. Middleton",
            "D. C. De Roure",
            "N. R. Shadbolt"
        ],
        "summary": "Tools for filtering the World Wide Web exist, but they are hampered by the difficulty of capturing user preferences in such a dynamic environment. We explore the acquisition of user profiles by unobtrusive monitoring of browsing behaviour and application of supervised machine-learning techniques coupled with an ontological representation to extract user preferences. A multi-class approach to paper classification is used, allowing the paper topic taxonomy to be utilised during profile construction. The Quickstep recommender system is presented and two empirical studies evaluate it in a real work setting, measuring the effectiveness of using a hierarchical topic ontology compared with an extendable flat list.",
        "published": "2002-03-08T15:58:23Z",
        "link": "http://arxiv.org/abs/cs/0203011v1",
        "categories": [
            "cs.LG",
            "cs.MA",
            "I.2.6;I.2.11"
        ]
    },
    {
        "title": "Interface agents: A review of the field",
        "authors": [
            "Stuart E. Middleton"
        ],
        "summary": "This paper reviews the origins of interface agents, discusses challenges that exist within the interface agent field and presents a survey of current attempts to find solutions to these challenges. A history of agent systems from their birth in the 1960's to the current day is described, along with the issues they try to address. A taxonomy of interface agent systems is presented, and today's agent systems categorized accordingly. Lastly, an analysis of the machine learning and user modelling techniques used by today's agents is presented.",
        "published": "2002-03-09T01:28:33Z",
        "link": "http://arxiv.org/abs/cs/0203012v1",
        "categories": [
            "cs.MA",
            "cs.LG",
            "I.2.11;I.2.6"
        ]
    },
    {
        "title": "Exploiting Synergy Between Ontologies and Recommender Systems",
        "authors": [
            "Stuart E. Middleton",
            "Harith Alani",
            "David C. De Roure"
        ],
        "summary": "Recommender systems learn about user preferences over time, automatically finding things of similar interest. This reduces the burden of creating explicit queries. Recommender systems do, however, suffer from cold-start problems where no initial information is available early on upon which to base recommendations. Semantic knowledge structures, such as ontologies, can provide valuable domain knowledge and user information. However, acquiring such knowledge and keeping it up to date is not a trivial task and user interests are particularly difficult to acquire and maintain. This paper investigates the synergy between a web-based research paper recommender system and an ontology containing information automatically extracted from departmental databases available on the web. The ontology is used to address the recommender systems cold-start problem. The recommender system addresses the ontology's interest-acquisition problem. An empirical evaluation of this approach is conducted and the performance of the integrated systems measured.",
        "published": "2002-04-08T10:56:26Z",
        "link": "http://arxiv.org/abs/cs/0204012v1",
        "categories": [
            "cs.LG",
            "cs.MA",
            "K.3.2;I.2.11"
        ]
    },
    {
        "title": "Self-Optimizing and Pareto-Optimal Policies in General Environments   based on Bayes-Mixtures",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "The problem of making sequential decisions in unknown probabilistic environments is studied. In cycle $t$ action $y_t$ results in perception $x_t$ and reward $r_t$, where all quantities in general may depend on the complete history. The perception $x_t$ and reward $r_t$ are sampled from the (reactive) environmental probability distribution $\\mu$. This very general setting includes, but is not limited to, (partial observable, k-th order) Markov decision processes. Sequential decision theory tells us how to act in order to maximize the total expected reward, called value, if $\\mu$ is known. Reinforcement learning is usually used if $\\mu$ is unknown. In the Bayesian approach one defines a mixture distribution $\\xi$ as a weighted sum of distributions $\\nu\\in\\M$, where $\\M$ is any class of distributions including the true environment $\\mu$. We show that the Bayes-optimal policy $p^\\xi$ based on the mixture $\\xi$ is self-optimizing in the sense that the average value converges asymptotically for all $\\mu\\in\\M$ to the optimal value achieved by the (infeasible) Bayes-optimal policy $p^\\mu$ which knows $\\mu$ in advance. We show that the necessary condition that $\\M$ admits self-optimizing policies at all, is also sufficient. No other structural assumptions are made on $\\M$. As an example application, we discuss ergodic Markov decision processes, which allow for self-optimizing policies. Furthermore, we show that $p^\\xi$ is Pareto-optimal in the sense that there is no other policy yielding higher or equal value in {\\em all} environments $\\nu\\in\\M$ and a strictly higher value in at least one.",
        "published": "2002-04-17T10:46:00Z",
        "link": "http://arxiv.org/abs/cs/0204040v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "math.OC",
            "math.PR",
            "I.2"
        ]
    },
    {
        "title": "Learning from Scarce Experience",
        "authors": [
            "Leonid Peshkin",
            "Christian R. Shelton"
        ],
        "summary": "Searching the space of policies directly for the optimal policy has been one popular method for solving partially observable reinforcement learning problems. Typically, with each change of the target policy, its value is estimated from the results of following that very policy. This requires a large number of interactions with the environment as different polices are considered. We present a family of algorithms based on likelihood ratio estimation that use data gathered when executing one policy (or collection of policies) to estimate the value of a different policy. The algorithms combine estimation and optimization stages. The former utilizes experience to build a non-parametric representation of an optimized function. The latter performs optimization on this estimate. We show positive empirical results and provide the sample complexity bound.",
        "published": "2002-04-20T05:02:53Z",
        "link": "http://arxiv.org/abs/cs/0204043v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.NE",
            "cs.RO",
            "I.2; I.2.8; I.2.11; I.2.6; G.1.6"
        ]
    },
    {
        "title": "Required sample size for learning sparse Bayesian networks with many   variables",
        "authors": [
            "Pawel Wocjan",
            "Dominik Janzing",
            "Thomas Beth"
        ],
        "summary": "Learning joint probability distributions on n random variables requires exponential sample size in the generic case. Here we consider the case that a temporal (or causal) order of the variables is known and that the (unknown) graph of causal dependencies has bounded in-degree Delta. Then the joint measure is uniquely determined by the probabilities of all (2 Delta+1)-tuples. Upper bounds on the sample size required for estimating their probabilities can be given in terms of the VC-dimension of the set of corresponding cylinder sets. The sample size grows less than linearly with n.",
        "published": "2002-04-26T14:33:29Z",
        "link": "http://arxiv.org/abs/cs/0204052v1",
        "categories": [
            "cs.LG",
            "math.PR",
            "I.2.6"
        ]
    },
    {
        "title": "Bootstrapping Structure into Language: Alignment-Based Learning",
        "authors": [
            "Menno M. van Zaanen"
        ],
        "summary": "This thesis introduces a new unsupervised learning framework, called Alignment-Based Learning, which is based on the alignment of sentences and Harris's (1951) notion of substitutability. Instances of the framework can be applied to an untagged, unstructured corpus of natural language sentences, resulting in a labelled, bracketed version of that corpus.   Firstly, the framework aligns all sentences in the corpus in pairs, resulting in a partition of the sentences consisting of parts of the sentences that are equal in both sentences and parts that are unequal. Unequal parts of sentences can be seen as being substitutable for each other, since substituting one unequal part for the other results in another valid sentence. The unequal parts of the sentences are thus considered to be possible (possibly overlapping) constituents, called hypotheses.   Secondly, the selection learning phase considers all hypotheses found by the alignment learning phase and selects the best of these. The hypotheses are selected based on the order in which they were found, or based on a probabilistic function.   The framework can be extended with a grammar extraction phase. This extended framework is called parseABL. Instead of returning a structured version of the unstructured input corpus, like the ABL system, this system also returns a stochastic context-free or tree substitution grammar.   Different instances of the framework have been tested on the English ATIS corpus, the Dutch OVIS corpus and the Wall Street Journal corpus. One of the interesting results, apart from the encouraging numerical results, is that all instances can (and do) learn recursive structures.",
        "published": "2002-05-16T12:35:00Z",
        "link": "http://arxiv.org/abs/cs/0205025v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Thumbs up? Sentiment Classification using Machine Learning Techniques",
        "authors": [
            "Bo Pang",
            "Lillian Lee",
            "Shivakumar Vaithyanathan"
        ],
        "summary": "We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.",
        "published": "2002-05-28T02:01:55Z",
        "link": "http://arxiv.org/abs/cs/0205070v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.7; I.2.6"
        ]
    },
    {
        "title": "Unsupervised Learning of Morphology without Morphemes",
        "authors": [
            "Sylvain Neuvel",
            "Sean A. Fulop"
        ],
        "summary": "The first morphological learner based upon the theory of Whole Word Morphology Ford et al. (1997) is outlined, and preliminary evaluation results are presented. The program, Whole Word Morphologizer, takes a POS-tagged lexicon as input, induces morphological relationships without attempting to discover or identify morphemes, and is then able to generate new words beyond the learning sample. The accuracy (precision) of the generated new words is as high as 80% using the pure Whole Word theory, and 92% after a post-hoc adjustment is added to the routine.",
        "published": "2002-05-29T17:48:48Z",
        "link": "http://arxiv.org/abs/cs/0205072v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "Robust Feature Selection by Mutual Information Distributions",
        "authors": [
            "Marco Zaffalon",
            "Marcus Hutter"
        ],
        "summary": "Mutual information is widely used in artificial intelligence, in a descriptive way, to measure the stochastic dependence of discrete random variables. In order to address questions such as the reliability of the empirical value, one must consider sample-to-population inferential approaches. This paper deals with the distribution of mutual information, as obtained in a Bayesian framework by a second-order Dirichlet prior distribution. The exact analytical expression for the mean and an analytical approximation of the variance are reported. Asymptotic approximations of the distribution are proposed. The results are applied to the problem of selecting features for incremental learning and classification of the naive Bayes classifier. A fast, newly defined method is shown to outperform the traditional approach based on empirical mutual information on a number of real data sets. Finally, a theoretical development is reported that allows one to efficiently extend the above methods to incomplete samples in an easy and effective way.",
        "published": "2002-06-03T16:00:55Z",
        "link": "http://arxiv.org/abs/cs/0206006v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2"
        ]
    },
    {
        "title": "The Prioritized Inductive Logic Programs",
        "authors": [
            "Shilong Ma",
            "Yuefei Sui",
            "Ke Xu"
        ],
        "summary": "The limit behavior of inductive logic programs has not been explored, but when considering incremental or online inductive learning algorithms which usually run ongoingly, such behavior of the programs should be taken into account. An example is given to show that some inductive learning algorithm may not be correct in the long run if the limit behavior is not considered. An inductive logic program is convergent if given an increasing sequence of example sets, the program produces a corresponding sequence of the Horn logic programs which has the set-theoretic limit, and is limit-correct if the limit of the produced sequence of the Horn logic programs is correct with respect to the limit of the sequence of the example sets. It is shown that the GOLEM system is not limit-correct. Finally, a limit-correct inductive logic system, called the prioritized GOLEM system, is proposed as a solution.",
        "published": "2002-06-10T16:02:36Z",
        "link": "http://arxiv.org/abs/cs/0206017v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.3; I.2.6"
        ]
    },
    {
        "title": "Optimal Ordered Problem Solver",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "We present a novel, general, optimally fast, incremental way of searching for a universal algorithm that solves each task in a sequence of tasks. The Optimal Ordered Problem Solver (OOPS) continually organizes and exploits previously found solutions to earlier tasks, efficiently searching not only the space of domain-specific algorithms, but also the space of search algorithms. Essentially we extend the principles of optimal nonincremental universal search to build an incremental universal learner that is able to improve itself through experience. In illustrative experiments, our self-improver becomes the first general system that learns to solve all n disk Towers of Hanoi tasks (solution size 2^n-1) for n up to 30, profiting from previously solved, simpler tasks involving samples of a simple context free language.",
        "published": "2002-07-31T14:33:11Z",
        "link": "http://arxiv.org/abs/cs/0207097v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LG",
            "I.2.2;I.2.6;I.2.8"
        ]
    },
    {
        "title": "An Algorithm for Pattern Discovery in Time Series",
        "authors": [
            "Cosma Rohilla Shalizi",
            "Kristina Lisa Shalizi",
            "James P. Crutchfield"
        ],
        "summary": "We present a new algorithm for discovering patterns in time series and other sequential data. We exhibit a reliable procedure for building the minimal set of hidden, Markovian states that is statistically capable of producing the behavior exhibited in the data -- the underlying process's causal states. Unlike conventional methods for fitting hidden Markov models (HMMs) to data, our algorithm makes no assumptions about the process's causal architecture (the number of hidden states and their transition structure), but rather infers it from the data. It starts with assumptions of minimal structure and introduces complexity only when the data demand it. Moreover, the causal states it infers have important predictive optimality properties that conventional HMM states lack. We introduce the algorithm, review the theory behind it, prove its asymptotic reliability, use large deviation theory to estimate its rate of convergence, and compare it to other algorithms which also construct HMMs from data. We also illustrate its behavior on an example process, and report selected numerical results from an implementation.",
        "published": "2002-10-29T00:33:26Z",
        "link": "http://arxiv.org/abs/cs/0210025v3",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2.6;H.1.1;E.4"
        ]
    },
    {
        "title": "Evaluation of the Performance of the Markov Blanket Bayesian Classifier   Algorithm",
        "authors": [
            "Michael G. Madden"
        ],
        "summary": "The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for construction of probabilistic classifiers. This paper presents an empirical comparison of the MBBC algorithm with three other Bayesian classifiers: Naive Bayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these are implemented using the K2 framework of Cooper and Herskovits. The classifiers are compared in terms of their performance (using simple accuracy measures and ROC curves) and speed, on a range of standard benchmark data sets. It is concluded that MBBC is competitive in terms of speed and accuracy with the other algorithms considered.",
        "published": "2002-11-01T18:09:56Z",
        "link": "http://arxiv.org/abs/cs/0211003v1",
        "categories": [
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "Maximing the Margin in the Input Space",
        "authors": [
            "Shotaro Akaho"
        ],
        "summary": "We propose a novel criterion for support vector machine learning: maximizing the margin in the input space, not in the feature (Hilbert) space. This criterion is a discriminative version of the principal curve proposed by Hastie et al. The criterion is appropriate in particular when the input space is already a well-designed feature space with rather small dimensionality. The definition of the margin is generalized in order to represent prior knowledge. The derived algorithm consists of two alternating steps to estimate the dual parameters. Firstly, the parameters are initialized by the original SVM. Then one set of parameters is updated by Newton-like procedure, and the other set is updated by solving a quadratic programming problem. The algorithm converges in a few steps to a local optimum under mild conditions and it preserves the sparsity of support vectors. Although the complexity to calculate temporal variables increases the complexity to solve the quadratic programming problem for each step does not change. It is also shown that the original SVM can be seen as a special case. We further derive a simplified algorithm which enables us to use the existing code for the original SVM.",
        "published": "2002-11-07T06:44:54Z",
        "link": "http://arxiv.org/abs/cs/0211006v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Approximating Incomplete Kernel Matrices by the em Algorithm",
        "authors": [
            "Koji Tsuda",
            "Shotaro Akaho",
            "Kiyoshi Asai"
        ],
        "summary": "In biological data, it is often the case that observed data are available only for a subset of samples. When a kernel matrix is derived from such data, we have to leave the entries for unavailable samples as missing. In this paper, we make use of a parametric model of kernel matrices, and estimate missing entries by fitting the model to existing entries. The parametric model is created as a set of spectral variants of a complete kernel matrix derived from another information source. For model fitting, we adopt the em algorithm based on the information geometry of positive definite matrices. We will report promising results on bacteria clustering experiments using two marker sequences: 16S and gyrB.",
        "published": "2002-11-07T07:21:58Z",
        "link": "http://arxiv.org/abs/cs/0211007v1",
        "categories": [
            "cs.LG",
            "I2.6; I5.2"
        ]
    },
    {
        "title": "Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent   Space Alignment",
        "authors": [
            "Zhenyue Zhang",
            "Hongyuan Zha"
        ],
        "summary": "Nonlinear manifold learning from unorganized data points is a very challenging unsupervised learning and data visualization problem with a great variety of applications. In this paper we present a new algorithm for manifold learning and nonlinear dimension reduction. Based on a set of unorganized data points sampled with noise from the manifold, we represent the local geometry of the manifold using tangent spaces learned by fitting an affine subspace in a neighborhood of each data point. Those tangent spaces are aligned to give the internal global coordinates of the data points with respect to the underlying manifold by way of a partial eigendecomposition of the neighborhood connection matrix. We present a careful error analysis of our algorithm and show that the reconstruction errors are of second-order accuracy. We illustrate our algorithm using curves and surfaces both in   2D/3D and higher dimensional Euclidean spaces, and 64-by-64 pixel face images with various pose and lighting conditions. We also address several theoretical and algorithmic issues for further research and improvements.",
        "published": "2002-12-07T18:51:12Z",
        "link": "http://arxiv.org/abs/cs/0212008v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.5.1;I5.3"
        ]
    },
    {
        "title": "Mining the Web for Lexical Knowledge to Improve Keyphrase Extraction:   Learning from Labeled and Unlabeled Data",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Keyphrases are useful for a variety of purposes, including summarizing, indexing, labeling, categorizing, clustering, highlighting, browsing, and searching. The task of automatic keyphrase extraction is to select keyphrases from within the text of a given document. Automatic keyphrase extraction makes it feasible to generate keyphrases for the huge number of documents that do not have manually assigned keyphrases. Good performance on this task has been obtained by approaching it as a supervised learning problem. An input document is treated as a set of candidate phrases that must be classified as either keyphrases or non-keyphrases. To classify a candidate phrase as a keyphrase, the most important features (attributes) appear to be the frequency and location of the candidate phrase in the document. Recent work has demonstrated that it is also useful to know the frequency of the candidate phrase as a manually assigned keyphrase for other documents in the same domain as the given document (e.g., the domain of computer science). Unfortunately, this keyphrase-frequency feature is domain-specific (the learning process must be repeated for each new domain) and training-intensive (good performance requires a relatively large number of training documents in the given domain, with manually assigned keyphrases). The aim of the work described here is to remove these limitations. In this paper, I introduce new features that are derived by mining lexical knowledge from a very large collection of unlabeled data, consisting of approximately 350 million Web pages without manually assigned keyphrases. I present experiments that show that the new features result in improved keyphrase extraction, although they are neither domain-specific nor training-intensive.",
        "published": "2002-12-08T18:52:33Z",
        "link": "http://arxiv.org/abs/cs/0212011v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Unsupervised Learning of Semantic Orientation from a   Hundred-Billion-Word Corpus",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman"
        ],
        "summary": "The evaluative character of a word is called its semantic orientation. A positive semantic orientation implies desirability (e.g., \"honest\", \"intrepid\") and a negative semantic orientation implies undesirability (e.g., \"disturbing\", \"superfluous\"). This paper introduces a simple algorithm for unsupervised learning of semantic orientation from extremely large corpora. The method involves issuing queries to a Web search engine and using pointwise mutual information to analyse the results. The algorithm is empirically evaluated using a training corpus of approximately one hundred billion words -- the subset of the Web that is indexed by the chosen search engine. Tested with 3,596 words (1,614 positive and 1,982 negative), the algorithm attains an accuracy of 80%. The 3,596 test words include adjectives, adverbs, nouns, and verbs. The accuracy is comparable with the results achieved by Hatzivassiloglou and McKeown (1997), using a complex four-stage supervised learning algorithm that is restricted to determining the semantic orientation of adjectives.",
        "published": "2002-12-08T19:06:08Z",
        "link": "http://arxiv.org/abs/cs/0212012v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Learning to Extract Keyphrases from Text",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Many academic journals ask their authors to provide a list of about five to fifteen key words, to appear on the first page of each article. Since these key words are often phrases of two or more words, we prefer to call them keyphrases. There is a surprisingly wide variety of tasks for which keyphrases are useful, as we discuss in this paper. Recent commercial software, such as Microsoft's Word 97 and Verity's Search 97, includes algorithms that automatically extract keyphrases from documents. In this paper, we approach the problem of automatically extracting keyphrases from text as a supervised learning task. We treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Our first set of experiments applies the C4.5 decision tree induction algorithm to this learning task. The second set of experiments applies the GenEx algorithm to the task. We developed the GenEx algorithm specifically for this task. The third set of experiments examines the performance of GenEx on the task of metadata generation, relative to the performance of Microsoft's Word 97. The fourth and final set of experiments investigates the performance of GenEx on the task of highlighting, relative to Verity's Search 97. The experimental results support the claim that a specialized learning algorithm (GenEx) can generate better keyphrases than a general-purpose learning algorithm (C4.5) and the non-learning algorithms that are used in commercial software (Word 97 and Search 97).",
        "published": "2002-12-08T19:27:56Z",
        "link": "http://arxiv.org/abs/cs/0212013v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Extraction of Keyphrases from Text: Evaluation of Four Algorithms",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This report presents an empirical evaluation of four algorithms for automatically extracting keywords and keyphrases from documents. The four algorithms are compared using five different collections of documents. For each document, we have a target set of keyphrases, which were generated by hand. The target keyphrases were generated for human readers; they were not tailored for any of the four keyphrase extraction algorithms. Each of the algorithms was evaluated by the degree to which the algorithm's keyphrases matched the manually generated keyphrases. The four algorithms were (1) the AutoSummarize feature in Microsoft's Word 97, (2) an algorithm based on Eric Brill's part-of-speech tagger, (3) the Summarize feature in Verity's Search 97, and (4) NRC's Extractor algorithm. For all five document collections, NRC's Extractor yields the best match with the manually generated keyphrases.",
        "published": "2002-12-08T19:40:42Z",
        "link": "http://arxiv.org/abs/cs/0212014v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Learning Algorithms for Keyphrase Extraction",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Many academic journals ask their authors to provide a list of about five to fifteen keywords, to appear on the first page of each article. Since these key words are often phrases of two or more words, we prefer to call them keyphrases. There is a wide variety of tasks for which keyphrases are useful, as we discuss in this paper. We approach the problem of automatically extracting keyphrases from text as a supervised learning task. We treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Our first set of experiments applies the C4.5 decision tree induction algorithm to this learning task. We evaluate the performance of nine different configurations of C4.5. The second set of experiments applies the GenEx algorithm to the task. We developed the GenEx algorithm specifically for automatically extracting keyphrases from text. The experimental results support the claim that a custom-designed algorithm (GenEx), incorporating specialized procedural domain knowledge, can generate better keyphrases than a generalpurpose algorithm (C4.5). Subjective human evaluation of the keyphrases generated by Extractor suggests that about 80% of the keyphrases are acceptable to human readers. This level of performance should be satisfactory for a wide variety of applications.",
        "published": "2002-12-10T15:30:56Z",
        "link": "http://arxiv.org/abs/cs/0212020v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "How to Shift Bias: Lessons from the Baldwin Effect",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "An inductive learning algorithm takes a set of data as input and generates a hypothesis as output. A set of data is typically consistent with an infinite number of hypotheses; therefore, there must be factors other than the data that determine the output of the learning algorithm. In machine learning, these other factors are called the bias of the learner. Classical learning algorithms have a fixed bias, implicit in their design. Recently developed learning algorithms dynamically adjust their bias as they search for a hypothesis. Algorithms that shift bias in this manner are not as well understood as classical algorithms. In this paper, we show that the Baldwin effect has implications for the design and analysis of bias shifting algorithms. The Baldwin effect was proposed in 1896, to explain how phenomena that might appear to require Lamarckian evolution (inheritance of acquired characteristics) can arise from purely Darwinian evolution. Hinton and Nowlan presented a computational model of the Baldwin effect in 1987. We explore a variation on their model, which we constructed explicitly to illustrate the lessons that the Baldwin effect has for research in bias shifting algorithms. The main lesson is that it appears that a good strategy for shift of bias in a learning algorithm is to begin with a weak bias and gradually shift to a strong bias.",
        "published": "2002-12-10T18:19:54Z",
        "link": "http://arxiv.org/abs/cs/0212023v1",
        "categories": [
            "cs.LG",
            "cs.NE",
            "I.2.6; I.2.8"
        ]
    },
    {
        "title": "Unsupervised Language Acquisition: Theory and Practice",
        "authors": [
            "Alexander Clark"
        ],
        "summary": "In this thesis I present various algorithms for the unsupervised machine learning of aspects of natural languages using a variety of statistical models. The scientific object of the work is to examine the validity of the so-called Argument from the Poverty of the Stimulus advanced in favour of the proposition that humans have language-specific innate knowledge. I start by examining an a priori argument based on Gold's theorem, that purports to prove that natural languages cannot be learned, and some formal issues related to the choice of statistical grammars rather than symbolic grammars. I present three novel algorithms for learning various parts of natural languages: first, an algorithm for the induction of syntactic categories from unlabelled text using distributional information, that can deal with ambiguous and rare words; secondly, a set of algorithms for learning morphological processes in a variety of languages, including languages such as Arabic with non-concatenative morphology; thirdly an algorithm for the unsupervised induction of a context-free grammar from tagged text. I carefully examine the interaction between the various components, and show how these algorithms can form the basis for a empiricist model of language acquisition. I therefore conclude that the Argument from the Poverty of the Stimulus is unsupported by the evidence.",
        "published": "2002-12-10T21:59:15Z",
        "link": "http://arxiv.org/abs/cs/0212024v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Technical Note: Bias and the Quantification of Stability",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Research on bias in machine learning algorithms has generally been concerned with the impact of bias on predictive accuracy. We believe that there are other factors that should also play a role in the evaluation of bias. One such factor is the stability of the algorithm; in other words, the repeatability of the results. If we obtain two sets of data from the same phenomenon, with the same underlying probability distribution, then we would like our learning algorithm to induce approximately the same concepts from both sets of data. This paper introduces a method for quantifying stability, based on a measure of the agreement between concepts. We also discuss the relationships among stability, predictive accuracy, and bias.",
        "published": "2002-12-11T15:50:41Z",
        "link": "http://arxiv.org/abs/cs/0212028v1",
        "categories": [
            "cs.LG",
            "cs.CV",
            "I.2.6; I.5.2"
        ]
    },
    {
        "title": "A Theory of Cross-Validation Error",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper presents a theory of error in cross-validation testing of algorithms for predicting real-valued attributes. The theory justifies the claim that predicting real-valued attributes requires balancing the conflicting demands of simplicity and accuracy. Furthermore, the theory indicates precisely how these conflicting demands must be balanced, in order to minimize cross-validation error. A general theory is presented, then it is developed in detail for linear regression and instance-based learning.",
        "published": "2002-12-11T16:08:36Z",
        "link": "http://arxiv.org/abs/cs/0212029v1",
        "categories": [
            "cs.LG",
            "cs.CV",
            "I.2.6; I.5.2"
        ]
    },
    {
        "title": "Theoretical Analyses of Cross-Validation Error and Voting in   Instance-Based Learning",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper begins with a general theory of error in cross-validation testing of algorithms for supervised learning from examples. It is assumed that the examples are described by attribute-value pairs, where the values are symbolic. Cross-validation requires a set of training examples and a set of testing examples. The value of the attribute that is to be predicted is known to the learner in the training set, but unknown in the testing set. The theory demonstrates that cross-validation error has two components: error on the training set (inaccuracy) and sensitivity to noise (instability). This general theory is then applied to voting in instance-based learning. Given an example in the testing set, a typical instance-based learning algorithm predicts the designated attribute by voting among the k nearest neighbors (the k most similar examples) to the testing example in the training set. Voting is intended to increase the stability (resistance to noise) of instance-based learning, but a theoretical analysis shows that there are circumstances in which voting can be destabilizing. The theory suggests ways to minimize cross-validation error, by insuring that voting is stable and does not adversely affect accuracy.",
        "published": "2002-12-11T17:36:00Z",
        "link": "http://arxiv.org/abs/cs/0212030v1",
        "categories": [
            "cs.LG",
            "cs.CV",
            "I.2.6; I.5.2"
        ]
    },
    {
        "title": "Contextual Normalization Applied to Aircraft Gas Turbine Engine   Diagnosis",
        "authors": [
            "Peter D. Turney",
            "Michael Halasz"
        ],
        "summary": "Diagnosing faults in aircraft gas turbine engines is a complex problem. It involves several tasks, including rapid and accurate interpretation of patterns in engine sensor data. We have investigated contextual normalization for the development of a software tool to help engine repair technicians with interpretation of sensor data. Contextual normalization is a new strategy for employing machine learning. It handles variation in data that is due to contextual factors, rather than the health of the engine. It does this by normalizing the data in a context-sensitive manner. This learning strategy was developed and tested using 242 observations of an aircraft gas turbine engine in a test cell, where each observation consists of roughly 12,000 numbers, gathered over a 12 second interval. There were eight classes of observations: seven deliberately implanted classes of faults and a healthy class. We compared two approaches to implementing our learning strategy: linear regression and instance-based learning. We have three main results. (1) For the given problem, instance-based learning works better than linear regression. (2) For this problem, contextual normalization works better than other common forms of normalization. (3) The algorithms described here can be the basis for a useful software tool for assisting technicians with the interpretation of sensor data.",
        "published": "2002-12-11T18:30:59Z",
        "link": "http://arxiv.org/abs/cs/0212031v1",
        "categories": [
            "cs.LG",
            "cs.CE",
            "cs.CV",
            "I.2.6; I.5.4; J.2"
        ]
    },
    {
        "title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised   Classification of Reviews",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \"subtle nuances\") and a negative semantic orientation when it has bad associations (e.g., \"very cavalier\"). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \"excellent\" minus the mutual information between the given phrase and the word \"poor\". A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.",
        "published": "2002-12-11T18:57:42Z",
        "link": "http://arxiv.org/abs/cs/0212032v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "I.2.6; I.2.7; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74%. PMI-IR is contrasted with Latent Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL questions. The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing).",
        "published": "2002-12-11T19:17:06Z",
        "link": "http://arxiv.org/abs/cs/0212033v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "I.2.6; I.2.7; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Types of Cost in Inductive Concept Learning",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Inductive concept learning is the task of learning to assign cases to a discrete set of classes. In real-world applications of concept learning, there are many different types of cost involved. The majority of the machine learning literature ignores all types of cost (unless accuracy is interpreted as a type of cost measure). A few papers have investigated the cost of misclassification errors. Very few papers have examined the many other types of cost. In this paper, we attempt to create a taxonomy of the different types of cost that are involved in inductive concept learning. This taxonomy may help to organize the literature on cost-sensitive learning. We hope that it will inspire researchers to investigate all types of cost in inductive concept learning in more depth.",
        "published": "2002-12-11T19:42:14Z",
        "link": "http://arxiv.org/abs/cs/0212034v1",
        "categories": [
            "cs.LG",
            "cs.CV",
            "I.2.6; I.5.2"
        ]
    },
    {
        "title": "Myths and Legends of the Baldwin Effect",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This position paper argues that the Baldwin effect is widely misunderstood by the evolutionary computation community. The misunderstandings appear to fall into two general categories. Firstly, it is commonly believed that the Baldwin effect is concerned with the synergy that results when there is an evolving population of learning individuals. This is only half of the story. The full story is more complicated and more interesting. The Baldwin effect is concerned with the costs and benefits of lifetime learning by individuals in an evolving population. Several researchers have focussed exclusively on the benefits, but there is much to be gained from attention to the costs. This paper explains the two sides of the story and enumerates ten of the costs and benefits of lifetime learning by individuals in an evolving population. Secondly, there is a cluster of misunderstandings about the relationship between the Baldwin effect and Lamarckian inheritance of acquired characteristics. The Baldwin effect is not Lamarckian. A Lamarckian algorithm is not better for most evolutionary computing problems than a Baldwinian algorithm. Finally, Lamarckian inheritance is not a better model of memetic (cultural) evolution than the Baldwin effect.",
        "published": "2002-12-11T21:34:18Z",
        "link": "http://arxiv.org/abs/cs/0212036v1",
        "categories": [
            "cs.LG",
            "cs.NE",
            "I.2.6; I.2.8"
        ]
    },
    {
        "title": "The Management of Context-Sensitive Features: A Review of Strategies",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "In this paper, we review five heuristic strategies for handling context-sensitive features in supervised machine learning from examples. We discuss two methods for recovering lost (implicit) contextual information. We mention some evidence that hybrid strategies can have a synergetic effect. We then show how the work of several machine learning researchers fits into this framework. While we do not claim that these strategies exhaust the possibilities, it appears that the framework includes all of the techniques that can be found in the published literature on contextsensitive learning.",
        "published": "2002-12-12T18:14:38Z",
        "link": "http://arxiv.org/abs/cs/0212037v1",
        "categories": [
            "cs.LG",
            "cs.CV",
            "I.2.6; I.5.2"
        ]
    },
    {
        "title": "The Identification of Context-Sensitive Features: A Formal Definition of   Context for Concept Learning",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "A large body of research in machine learning is concerned with supervised learning from examples. The examples are typically represented as vectors in a multi-dimensional feature space (also known as attribute-value descriptions). A teacher partitions a set of training examples into a finite number of classes. The task of the learning algorithm is to induce a concept from the training examples. In this paper, we formally distinguish three types of features: primary, contextual, and irrelevant features. We also formally define what it means for one feature to be context-sensitive to another feature. Context-sensitive features complicate the task of the learner and potentially impair the learner's performance. Our formal definitions make it possible for a learner to automatically identify context-sensitive features. After context-sensitive features have been identified, there are several strategies that the learner can employ for managing the features; however, a discussion of these strategies is outside of the scope of this paper. The formal definitions presented here correct a flaw in previously proposed definitions. We discuss the relationship between our work and a formal definition of relevance.",
        "published": "2002-12-12T18:29:02Z",
        "link": "http://arxiv.org/abs/cs/0212038v1",
        "categories": [
            "cs.LG",
            "cs.CV",
            "I.2.6; I.5.2"
        ]
    },
    {
        "title": "Low Size-Complexity Inductive Logic Programming: The East-West Challenge   Considered as a Problem in Cost-Sensitive Classification",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "The Inductive Logic Programming community has considered proof-complexity and model-complexity, but, until recently, size-complexity has received little attention. Recently a challenge was issued \"to the international computing community\" to discover low size-complexity Prolog programs for classifying trains. The challenge was based on a problem first proposed by Ryszard Michalski, 20 years ago. We interpreted the challenge as a problem in cost-sensitive classification and we applied a recently developed cost-sensitive classifier to the competition. Our algorithm was relatively successful (we won a prize). This paper presents our algorithm and analyzes the results of the competition.",
        "published": "2002-12-12T18:51:06Z",
        "link": "http://arxiv.org/abs/cs/0212039v1",
        "categories": [
            "cs.LG",
            "cs.NE",
            "I.2.6; I.2.8"
        ]
    },
    {
        "title": "Data Engineering for the Analysis of Semiconductor Manufacturing Data",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "We have analyzed manufacturing data from several different semiconductor manufacturing plants, using decision tree induction software called Q-YIELD. The software generates rules for predicting when a given product should be rejected. The rules are intended to help the process engineers improve the yield of the product, by helping them to discover the causes of rejection. Experience with Q-YIELD has taught us the importance of data engineering -- preprocessing the data to enable or facilitate decision tree induction. This paper discusses some of the data engineering problems we have encountered with semiconductor manufacturing data. The paper deals with two broad classes of problems: engineering the features in a feature vector representation and engineering the definition of the target concept (the classes). Manufacturing process data present special problems for feature engineering, since the data have multiple levels of granularity (detail, resolution). Engineering the target concept is important, due to our focus on understanding the past, as opposed to the more common focus in machine learning on predicting the future.",
        "published": "2002-12-12T19:11:11Z",
        "link": "http://arxiv.org/abs/cs/0212040v1",
        "categories": [
            "cs.LG",
            "cs.CE",
            "cs.CV",
            "I.2.6; I.5.2; I.5.4; J.2"
        ]
    },
    {
        "title": "Robust Classification with Context-Sensitive Features",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper addresses the problem of classifying observations when features are context-sensitive, especially when the testing set involves a context that is different from the training set. The paper begins with a precise definition of the problem, then general strategies are presented for enhancing the performance of classification algorithms on this type of problem. These strategies are tested on three domains. The first domain is the diagnosis of gas turbine engines. The problem is to diagnose a faulty engine in one context, such as warm weather, when the fault has previously been seen only in another context, such as cold weather. The second domain is speech recognition. The context is given by the identity of the speaker. The problem is to recognize words spoken by a new speaker, not represented in the training set. The third domain is medical prognosis. The problem is to predict whether a patient with hepatitis will live or die. The context is the age of the patient. For all three domains, exploiting context results in substantially more accurate classification.",
        "published": "2002-12-12T19:26:52Z",
        "link": "http://arxiv.org/abs/cs/0212041v1",
        "categories": [
            "cs.LG",
            "cs.CV",
            "I.2.6; I.5.2; I.5.4"
        ]
    },
    {
        "title": "Exploiting Context When Learning to Classify",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper addresses the problem of classifying observations when features are context-sensitive, specifically when the testing set involves a context that is different from the training set. The paper begins with a precise definition of the problem, then general strategies are presented for enhancing the performance of classification algorithms on this type of problem. These strategies are tested on two domains. The first domain is the diagnosis of gas turbine engines. The problem is to diagnose a faulty engine in one context, such as warm weather, when the fault has previously been seen only in another context, such as cold weather. The second domain is speech recognition. The problem is to recognize words spoken by a new speaker, not represented in the training set. For both domains, exploiting context results in substantially more accurate classification.",
        "published": "2002-12-12T19:40:50Z",
        "link": "http://arxiv.org/abs/cs/0212035v1",
        "categories": [
            "cs.LG",
            "cs.CV",
            "I.2.6; I.5.2; I.5.4"
        ]
    },
    {
        "title": "Lower Bounds for Matrix Product",
        "authors": [
            "Amir Shpilka"
        ],
        "summary": "We prove lower bounds on the number of product gates in bilinear and quadratic circuits that compute the product of two $n \\cross n$ matrices over finite fields. In particular we obtain the following results:   1. We show that the number of product gates in any bilinear (or quadratic) circuit that computes the product of two $n \\cross n$ matrices over $F_2$ is at least $3 n^2 - o(n^2)$.   2. We show that the number of product gates in any bilinear circuit that computes the product of two $n \\cross n$ matrices over $F_p$ is at least $(2.5 + \\frac{1.5}{p^3 -1})n^2 -o(n^2)$.   These results improve the former results of Bshouty '89 and Blaser '99 who proved lower bounds of $2.5 n^2 - o(n^2)$.",
        "published": "2002-01-02T09:50:57Z",
        "link": "http://arxiv.org/abs/cs/0201001v1",
        "categories": [
            "cs.CC",
            "F.1.1; F.2.0"
        ]
    },
    {
        "title": "A lower bound on the quantum query complexity of read-once functions",
        "authors": [
            "Howard Barnum",
            "Michael Saks"
        ],
        "summary": "We establish a lower bound of $\\Omega{(\\sqrt{n})}$ on the bounded-error quantum query complexity of read-once Boolean functions, providing evidence for the conjecture that $\\Omega(\\sqrt{D(f)})$ is a lower bound for all Boolean functions. Our technique extends a result of Ambainis, based on the idea that successful computation of a function requires ``decoherence'' of initially coherently superposed inputs in the query register, having different values of the function. The number of queries is bounded by comparing the required total amount of decoherence of a judiciously selected set of input-output pairs to an upper bound on the amount achievable in a single query step. We use an extension of this result to general weights on input pairs, and general superpositions of inputs.",
        "published": "2002-01-03T19:19:09Z",
        "link": "http://arxiv.org/abs/quant-ph/0201007v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Sharpening Occam's Razor",
        "authors": [
            "Ming Li",
            "John Tromp",
            "Paul Vitanyi"
        ],
        "summary": "We provide a new representation-independent formulation of Occam's razor theorem, based on Kolmogorov complexity. This new formulation allows us to:   (i) Obtain better sample complexity than both length-based and VC-based versions of Occam's razor theorem, in many applications.   (ii) Achieve a sharper reverse of Occam's razor theorem than previous work.   Specifically, we weaken the assumptions made in an earlier publication, and extend the reverse to superpolynomial running times.",
        "published": "2002-01-08T16:44:10Z",
        "link": "http://arxiv.org/abs/cs/0201005v2",
        "categories": [
            "cs.LG",
            "cond-mat.dis-nn",
            "cs.AI",
            "cs.CC",
            "math.PR",
            "physics.data-an",
            "F.2, E.4, I.2"
        ]
    },
    {
        "title": "On the Importance of Having an Identity or, is Consensus really   Universal?",
        "authors": [
            "Harry Buhrman",
            "Alessandro Panconesi",
            "Riccardo Silvestri",
            "Paul Vitanyi"
        ],
        "summary": "We show that Naming-- the existence of distinct IDs known to all-- is a hidden but necessary assumption of Herlihy's universality result for Consensus. We then show in a very precise sense that Naming is harder than Consensus and bring to the surface some important differences existing between popular shared memory models.",
        "published": "2002-01-09T18:49:27Z",
        "link": "http://arxiv.org/abs/cs/0201006v2",
        "categories": [
            "cs.DC",
            "cs.CC",
            "F.1.2; C.2.4; B.3.2;B.4.3;D.1.3;D.4.1;D.4.4"
        ]
    },
    {
        "title": "Equivalence and Isomorphism for Boolean Constraint Satisfaction",
        "authors": [
            "E. Boehler",
            "E. Hemaspaandra",
            "Steffen Reith",
            "Heribert Vollmer"
        ],
        "summary": "A Boolean constraint satisfaction instance is a conjunction of constraint applications, where the allowed constraints are drawn from a fixed set B of Boolean functions. We consider the problem of determining whether two given constraint satisfaction instances are equivalent and prove a Dichotomy Theorem by showing that for all sets C of allowed constraints, this problem is either polynomial-time solvable or coNP-complete, and we give a simple criterion to determine which case holds.   A more general problem addressed in this paper is the isomorphism problem, the problem of determining whether there exists a renaming of the variables that makes two given constraint satisfaction instances equivalent in the above sense. We prove that this problem is coNP-hard if the corresponding equivalence problem is coNP-hard, and polynomial-time many-one reducible to the graph isomorphism problem in all other cases.",
        "published": "2002-02-25T15:38:36Z",
        "link": "http://arxiv.org/abs/cs/0202036v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "Unpredictability of wave function's evolution in nonintegrable quantum   systems",
        "authors": [
            "I. B. Ivanov"
        ],
        "summary": "It is shown that evolution of wave functions in nonintegrable quantum systems is unpredictable for a long time T because of rapid growth of number of elementary computational operations $\\mathcal O(T)\\sim T^\\alpha$. On the other hand, the evolution of wave functions in integrable systems can be predicted by the fast algorithms $\\mathcal O(T)\\sim (log_2 T)^\\beta$ for logarithmically short time and thus there is an algorithmic \"compressibility\" of their dynamics. The difference between integrable and nonintegrable systems in our approach looks identically for classical and quantum systems. Therefore the minimal number of bit operations $\\mathcal O(T)$ needed to predict a state of system for time interval T can be used as universal sign of chaos.",
        "published": "2002-03-05T12:24:09Z",
        "link": "http://arxiv.org/abs/quant-ph/0203019v1",
        "categories": [
            "quant-ph",
            "cs.CC",
            "nlin.CD"
        ]
    },
    {
        "title": "Active Virtual Network Management Prediction: Complexity as a Framework   for Prediction, Optimization, and Assurance",
        "authors": [
            "Stephen F. Bush"
        ],
        "summary": "Research into active networking has provided the incentive to re-visit what has traditionally been classified as distinct properties and characteristics of information transfer such as protocol versus service; at a more fundamental level this paper considers the blending of computation and communication by means of complexity. The specific service examined in this paper is network self-prediction enabled by Active Virtual Network Management Prediction. Computation/communication is analyzed via Kolmogorov Complexity. The result is a mechanism to understand and improve the performance of active networking and Active Virtual Network Management Prediction in particular. The Active Virtual Network Management Prediction mechanism allows information, in various states of algorithmic and static form, to be transported in the service of prediction for network management. The results are generally applicable to algorithmic transmission of information. Kolmogorov Complexity is used and experimentally validated as a theory describing the relationship among algorithmic compression, complexity, and prediction accuracy within an active network. Finally, the paper concludes with a complexity-based framework for Information Assurance that attempts to take a holistic view of vulnerability analysis.",
        "published": "2002-03-11T23:09:16Z",
        "link": "http://arxiv.org/abs/cs/0203014v1",
        "categories": [
            "cs.CC",
            "cs.NI",
            "D.2.8;C.2.0;C.2.1;C.2.2;C.2.3"
        ]
    },
    {
        "title": "Dimension in Complexity Classes",
        "authors": [
            "Jack H. Lutz"
        ],
        "summary": "A theory of resource-bounded dimension is developed using gales, which are natural generalizations of martingales. When the resource bound \\Delta (a parameter of the theory) is unrestricted, the resulting dimension is precisely the classical Hausdorff dimension (sometimes called fractal dimension). Other choices of the parameter \\Delta yield internal dimension theories in E, E2, ESPACE, and other complexity classes, and in the class of all decidable problems. In general, if C is such a class, then every set X of languages has a dimension in C, which is a real number dim(X|C) in [0,1]. Along with the elements of this theory, two preliminary applications are presented:   1. For every real number \\alpha in (0,1/2), the set FREQ(<=\\alpha), consisting of all languages that asymptotically contain at most \\alpha of all strings, has dimension H(\\alpha) -- the binary entropy of \\alpha -- in E and in E2.   2. For every real number \\alpha in (0,1), the set SIZE(\\alpha* (2^n)/n), consisting of all languages decidable by Boolean circuits of at most \\alpha*(2^n)/n gates, has dimension \\alpha in ESPACE.",
        "published": "2002-03-12T20:16:27Z",
        "link": "http://arxiv.org/abs/cs/0203016v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "The Dimensions of Individual Strings and Sequences",
        "authors": [
            "Jack H. Lutz"
        ],
        "summary": "A constructive version of Hausdorff dimension is developed using constructive supergales, which are betting strategies that generalize the constructive supermartingales used in the theory of individual random sequences. This constructive dimension is used to assign every individual (infinite, binary) sequence S a dimension, which is a real number dim(S) in the interval [0,1]. Sequences that are random (in the sense of Martin-Lof) have dimension 1, while sequences that are decidable, \\Sigma^0_1, or \\Pi^0_1 have dimension 0. It is shown that for every \\Delta^0_2-computable real number \\alpha in [0,1] there is a \\Delta^0_2 sequence S such that \\dim(S) = \\alpha.   A discrete version of constructive dimension is also developed using termgales, which are supergale-like functions that bet on the terminations of (finite, binary) strings as well as on their successive bits. This discrete dimension is used to assign each individual string w a dimension, which is a nonnegative real number dim(w). The dimension of a sequence is shown to be the limit infimum of the dimensions of its prefixes.   The Kolmogorov complexity of a string is proven to be the product of its length and its dimension. This gives a new characterization of algorithmic information and a new proof of Mayordomo's recent theorem stating that the dimension of a sequence is the limit infimum of the average Kolmogorov complexity of its first n bits.   Every sequence that is random relative to any computable sequence of coin-toss biases that converge to a real number \\beta in (0,1) is shown to have dimension \\H(\\beta), the binary entropy of \\beta.",
        "published": "2002-03-12T20:28:19Z",
        "link": "http://arxiv.org/abs/cs/0203017v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Generic-case complexity, decision problems in group theory and random   walks",
        "authors": [
            "Ilya Kapovich",
            "Alexei Myasnikov",
            "Paul Schupp",
            "Vladimir Shpilrain"
        ],
        "summary": "We give a precise definition of ``generic-case complexity'' and show that for a very large class of finitely generated groups the classical decision problems of group theory - the word, conjugacy and membership problems - all have linear-time generic-case complexity. We prove such theorems by using the theory of random walks on regular graphs.",
        "published": "2002-03-22T18:25:42Z",
        "link": "http://arxiv.org/abs/math/0203239v3",
        "categories": [
            "math.GR",
            "cs.CC",
            "20F"
        ]
    },
    {
        "title": "Forbidden Information",
        "authors": [
            "Leonid A. Levin"
        ],
        "summary": "Goedel Incompleteness Theorem leaves open a way around it, vaguely perceived for a long time but not clearly identified. (Thus, Goedel believed informal arguments can answer any math question.) Closing this loophole does not seem obvious and involves Kolmogorov complexity. (This is unrelated to, well studied before, complexity quantifications of the usual Goedel effects.) I consider extensions U of the universal partial recursive predicate (or, say, Peano Arithmetic). I prove that any U either leaves an n-bit input (statement) unresolved or contains nearly all information about the n-bit prefix of any r.e. real r (which is n bits for some r). I argue that creating significant information about a SPECIFIC math sequence is impossible regardless of the methods used. Similar problems and answers apply to other unsolvability results for tasks allowing multiple solutions, e.g. non-recursive tilings.",
        "published": "2002-03-27T17:28:22Z",
        "link": "http://arxiv.org/abs/cs/0203029v21",
        "categories": [
            "cs.CC",
            "F.1"
        ]
    },
    {
        "title": "Quantum Optimization Problems",
        "authors": [
            "Tomoyuki Yamakami"
        ],
        "summary": "Krentel [J. Comput. System. Sci., 36, pp.490--509] presented a framework for an NP optimization problem that searches an optimal value among exponentially-many outcomes of polynomial-time computations. This paper expands his framework to a quantum optimization problem using polynomial-time quantum computations and introduces the notion of an ``universal'' quantum optimization problem similar to a classical ``complete'' optimization problem. We exhibit a canonical quantum optimization problem that is universal for the class of polynomial-time quantum optimization problems. We show in a certain relativized world that all quantum optimization problems cannot be approximated closely by quantum polynomial-time computations. We also study the complexity of quantum optimization problems in connection to well-known complexity classes.",
        "published": "2002-04-03T00:55:49Z",
        "link": "http://arxiv.org/abs/quant-ph/0204010v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "New Results on Monotone Dualization and Generating Hypergraph   Transversals",
        "authors": [
            "Thomas Eiter",
            "Georg Gottlob",
            "Kazuhisa Makino"
        ],
        "summary": "We consider the problem of dualizing a monotone CNF (equivalently, computing all minimal transversals of a hypergraph), whose associated decision problem is a prominent open problem in NP-completeness. We present a number of new polynomial time resp. output-polynomial time results for significant cases, which largely advance the tractability frontier and improve on previous results. Furthermore, we show that duality of two monotone CNFs can be disproved with limited nondeterminism. More precisely, this is feasible in polynomial time with O(chi(n) * log n) suitably guessed bits, where chi(n) is given by \\chi(n)^chi(n) = n; note that chi(n) = o(log n). This result sheds new light on the complexity of this important problem.",
        "published": "2002-04-04T19:23:49Z",
        "link": "http://arxiv.org/abs/cs/0204009v3",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2; F.1.3; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Computational problems for vector-valued quadratic forms",
        "authors": [
            "Francesco Bullo",
            "Jorge Cortes",
            "Andrew D. Lewis",
            "Sonia Martinez"
        ],
        "summary": "Given two real vector spaces $U$ and $V$, and a symmetric bilinear map $B: U\\times U\\to V$, let $Q_B$ be its associated quadratic map $Q_B$. The problems we consider are as follows: (i) are there necessary and sufficient conditions, checkable in polynomial-time, for determining when $Q_B$ is surjective?; (ii) if $Q_B$ is surjective, given $v\\in V$ is there a polynomial-time algorithm for finding a point $u\\in Q_B^{-1}(v)$?; (iii) are there necessary and sufficient conditions, checkable in polynomial-time, for determining when $B$ is indefinite? We present an alternative formulation of the problem of determining the image of a vector-valued quadratic form in terms of the unprojectivised Veronese surface. The relation of these questions with several interesting problems in Control Theory is illustrated.",
        "published": "2002-04-05T14:04:11Z",
        "link": "http://arxiv.org/abs/math/0204068v1",
        "categories": [
            "math.AG",
            "cs.CC",
            "math.OC",
            "11Exx; 14Pxx; 14Q99; 15A63"
        ]
    },
    {
        "title": "The Geometric Maximum Traveling Salesman Problem",
        "authors": [
            "Alexander Barvinok",
            "Sandor P. Fekete",
            "David S. Johnson",
            "Arie Tamir",
            "Gerhard J. Woeginger",
            "Russ Woodroofe"
        ],
        "summary": "We consider the traveling salesman problem when the cities are points in R^d for some fixed d and distances are computed according to geometric distances, determined by some norm. We show that for any polyhedral norm, the problem of finding a tour of maximum length can be solved in polynomial time. If arithmetic operations are assumed to take unit time, our algorithms run in time O(n^{f-2} log n), where f is the number of facets of the polyhedron determining the polyhedral norm. Thus for example we have O(n^2 log n) algorithms for the cases of points in the plane under the Rectilinear and Sup norms. This is in contrast to the fact that finding a minimum length tour in each case is NP-hard. Our approach can be extended to the more general case of quasi-norms with not necessarily symmetric unit ball, where we get a complexity of O(n^{2f-2} log n).   For the special case of two-dimensional metrics with f=4 (which includes the Rectilinear and Sup norms), we present a simple algorithm with O(n) running time. The algorithm does not use any indirect addressing, so its running time remains valid even in comparison based models in which sorting requires Omega(n \\log n) time. The basic mechanism of the algorithm provides some intuition on why polyhedral norms allow fast algorithms.   Complementing the results on simplicity for polyhedral norms, we prove that for the case of Euclidean distances in R^d for d>2, the Maximum TSP is NP-hard. This sheds new light on the well-studied difficulties of Euclidean distances.",
        "published": "2002-04-10T18:56:09Z",
        "link": "http://arxiv.org/abs/cs/0204024v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Kolmogorov's Structure Functions and Model Selection",
        "authors": [
            "Nikolai Vereshchagin",
            "Paul Vitanyi"
        ],
        "summary": "In 1974 Kolmogorov proposed a non-probabilistic approach to statistics and model selection. Let data be finite binary strings and models be finite sets of binary strings. Consider model classes consisting of models of given maximal (Kolmogorov) complexity. The ``structure function'' of the given data expresses the relation between the complexity level constraint on a model class and the least log-cardinality of a model in the class containing the data. We show that the structure function determines all stochastic properties of the data: for every constrained model class it determines the individual best-fitting model in the class irrespective of whether the ``true'' model is in the model class considered or not. In this setting, this happens {\\em with certainty}, rather than with high probability as is in the classical case. We precisely quantify the goodness-of-fit of an individual model with respect to individual data. We show that--within the obvious constraints--every graph is realized by the structure function of some data. We determine the (un)computability properties of the various functions contemplated and of the ``algorithmic minimal sufficient statistic.''",
        "published": "2002-04-16T15:58:58Z",
        "link": "http://arxiv.org/abs/cs/0204037v5",
        "categories": [
            "cs.CC",
            "math.PR",
            "physics.data-an",
            "E.5, E.4, E.2, H.1.1, F.1.1, F.1.3"
        ]
    },
    {
        "title": "PSPACE-Completeness of Sliding-Block Puzzles and Other Problems through   the Nondeterministic Constraint Logic Model of Computation",
        "authors": [
            "Robert A. Hearn",
            "Erik D. Demaine"
        ],
        "summary": "We present a nondeterministic model of computation based on reversing edge directions in weighted directed graphs with minimum in-flow constraints on vertices. Deciding whether this simple graph model can be manipulated in order to reverse the direction of a particular edge is shown to be PSPACE-complete by a reduction from Quantified Boolean Formulas. We prove this result in a variety of special cases including planar graphs and highly restricted vertex configurations, some of which correspond to a kind of passive constraint logic. Our framework is inspired by (and indeed a generalization of) the ``Generalized Rush Hour Logic'' developed by Flake and Baum.   We illustrate the importance of our model of computation by giving simple reductions to show that several motion-planning problems are PSPACE-hard. Our main result along these lines is that classic unrestricted sliding-block puzzles are PSPACE-hard, even if the pieces are restricted to be all dominoes (1x2 blocks) and the goal is simply to move a particular piece. No prior complexity results were known about these puzzles. This result can be seen as a strengthening of the existing result that the restricted Rush Hour puzzles are PSPACE-complete, of which we also give a simpler proof. Finally, we strengthen the existing result that the pushing-blocks puzzle Sokoban is PSPACE-complete, by showing that it is PSPACE-complete even if no barriers are allowed.",
        "published": "2002-05-04T19:04:59Z",
        "link": "http://arxiv.org/abs/cs/0205005v5",
        "categories": [
            "cs.CC",
            "cs.GT",
            "F.1; F.2.2"
        ]
    },
    {
        "title": "The asymptotic complexity of partial sorting -- How to learn large   posets by pairwise comparisons",
        "authors": [
            "Jobst Heitzig"
        ],
        "summary": "The expected number of pairwise comparisons needed to learn a partial order on n elements is shown to be at least n*n/4-o(n*n), and an algorithm is given that needs only n*n/4+o(n*n) comparisons on average. In addition, the optimal strategy for learning a poset with four elements is presented.",
        "published": "2002-05-06T12:41:56Z",
        "link": "http://arxiv.org/abs/math/0205049v1",
        "categories": [
            "math.CO",
            "cs.CC",
            "math.OC",
            "06A07; 11Y16"
        ]
    },
    {
        "title": "On-Line Paging against Adversarially Biased Random Inputs",
        "authors": [
            "Neal E. Young"
        ],
        "summary": "In evaluating an algorithm, worst-case analysis can be overly pessimistic. Average-case analysis can be overly optimistic. An intermediate approach is to show that an algorithm does well on a broad class of input distributions. Koutsoupias and Papadimitriou recently analyzed the least-recently-used (LRU) paging strategy in this manner, analyzing its performance on an input sequence generated by a so-called diffuse adversary -- one that must choose each request probabilitistically so that no page is chosen with probability more than some fixed epsilon>0. They showed that LRU achieves the optimal competitive ratio (for deterministic on-line algorithms), but they didn't determine the actual ratio.   In this paper we estimate the optimal ratios within roughly a factor of two for both deterministic strategies (e.g. least-recently-used and first-in-first-out) and randomized strategies. Around the threshold epsilon ~ 1/k (where k is the cache size), the optimal ratios are both Theta(ln k). Below the threshold the ratios tend rapidly to O(1). Above the threshold the ratio is unchanged for randomized strategies but tends rapidly to Theta(k) for deterministic ones.   We also give an alternate proof of the optimality of LRU.",
        "published": "2002-05-09T04:26:12Z",
        "link": "http://arxiv.org/abs/cs/0205007v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.0; F.1.3"
        ]
    },
    {
        "title": "Improved Bicriteria Existence Theorems for Scheduling",
        "authors": [
            "Javed Aslam",
            "April Rasala",
            "Cliff Stein",
            "Neal Young"
        ],
        "summary": "Two common objectives for evaluating a schedule are the makespan, or schedule length, and the average completion time. This short note gives improved bounds on the existence of schedules that simultaneously optimize both criteria. In particular, for any rho> 0, there exists a schedule of makespan at most 1+rho times the minimum, with average completion time at most (1-e)^rho times the minimum. The proof uses an infininite-dimensional linear program to generalize and strengthen a previous analysis by Cliff Stein and Joel Wein (1997).",
        "published": "2002-05-10T00:47:20Z",
        "link": "http://arxiv.org/abs/cs/0205008v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.0; F.1.3"
        ]
    },
    {
        "title": "Approximate Data Structures with Applications",
        "authors": [
            "Yossi Matias",
            "Jeff Vitter",
            "Neal Young"
        ],
        "summary": "This paper explores the notion of approximate data structures, which return approximately correct answers to queries, but run faster than their exact counterparts. The paper describes approximate variants of the van Emde Boas data structure, which support the same dynamic operations as the standard van Emde Boas data structure (min, max, successor, predecessor, and existence queries, as well as insertion and deletion), except that answers to queries are approximate. The variants support all operations in constant time provided the performance guarantee is 1+1/polylog(n), and in O(loglogn) time provided the performance guarantee is 1+1/polynomial(n), for n elements in the data structure.   Applications described include Prim's minimum-spanning-tree algorithm, Dijkstra's single-source shortest paths algorithm, and an on-line variant of Graham's convex hull algorithm. To obtain output which approximates the desired output with the performance guarantee tending to 1, Prim's algorithm requires only linear time, Dijkstra's algorithm requires O(mloglogn) time, and the on-line variant of Graham's algorithm requires constant amortized time per operation.",
        "published": "2002-05-10T01:12:18Z",
        "link": "http://arxiv.org/abs/cs/0205010v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.0; F.1.3"
        ]
    },
    {
        "title": "Polynomial-Time Approximation Scheme for Data Broadcast",
        "authors": [
            "Claire Kenyon",
            "Nicolas Schabanel",
            "Neal Young"
        ],
        "summary": "The data broadcast problem is to find a schedule for broadcasting a given set of messages over multiple channels. The goal is to minimize the cost of the broadcast plus the expected response time to clients who periodically and probabilistically tune in to wait for particular messages.   The problem models disseminating data to clients in asymmetric communication environments, where there is a much larger capacity from the information source to the clients than in the reverse direction. Examples include satellites, cable TV, internet broadcast, and mobile phones. Such environments favor the ``push-based'' model where the server broadcasts (pushes) its information on the communication medium and multiple clients simultaneously retrieve the specific information of individual interest.   This paper presents the first polynomial-time approximation scheme (PTAS) for data broadcast with O(1) channels and when each message has arbitrary probability, unit length and bounded cost. The best previous polynomial-time approximation algorithm for this case has a performance ratio of 9/8.",
        "published": "2002-05-10T02:23:47Z",
        "link": "http://arxiv.org/abs/cs/0205012v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.0, F.1.3"
        ]
    },
    {
        "title": "On Strongly Connected Digraphs with Bounded Cycle Length",
        "authors": [
            "Samir Khuller",
            "Balaji Raghavachari",
            "Neal Young"
        ],
        "summary": "The MEG (minimum equivalent graph) problem is, given a directed graph, to find a small subset of the edges that maintains all reachability relations between nodes. The problem is NP-hard. This paper gives a proof that, for graphs where each directed cycle has at most three edges, the MEG problem is equivalent to maximum bipartite matching, and therefore solvable in polynomial time. This leads to an improvement in the performance guarantee of the previously best approximation algorithm for the general problem in ``Approximating the Minimum Equivalent Digraph'' (1995).",
        "published": "2002-05-10T02:30:47Z",
        "link": "http://arxiv.org/abs/cs/0205011v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.0; F.1.3"
        ]
    },
    {
        "title": "Quantum Random Walks Hit Exponentially Faster",
        "authors": [
            "Julia Kempe"
        ],
        "summary": "We show that the hitting time of the discrete time quantum random walk on the n-bit hypercube from one corner to its opposite is polynomial in n. This gives the first exponential quantum-classical gap in the hitting time of discrete quantum random walks. We provide the framework for quantum hitting time and give two alternative definitions to set the ground for its study on general graphs. We then give an application to random routing.",
        "published": "2002-05-14T19:36:55Z",
        "link": "http://arxiv.org/abs/quant-ph/0205083v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Lecture Notes on Evasiveness of Graph Properties",
        "authors": [
            "Laszlo Lovasz",
            "Neal E. Young"
        ],
        "summary": "This report presents notes from the first eight lectures of the class Many Models of Complexity taught by Laszlo Lovasz at Princeton University in the fall of 1990. The topic is evasiveness of graph properties: given a graph property, how many edges of the graph an algorithm must check in the worst case before it knows whether the property holds.",
        "published": "2002-05-18T01:20:37Z",
        "link": "http://arxiv.org/abs/cs/0205031v1",
        "categories": [
            "cs.CC",
            "F.1.3, F.2.0"
        ]
    },
    {
        "title": "On-Line End-to-End Congestion Control",
        "authors": [
            "Naveen Garg",
            "Neal E. Young"
        ],
        "summary": "Congestion control in the current Internet is accomplished mainly by TCP/IP. To understand the macroscopic network behavior that results from TCP/IP and similar end-to-end protocols, one main analytic technique is to show that the the protocol maximizes some global objective function of the network traffic. Here we analyze a particular end-to-end, MIMD (multiplicative-increase, multiplicative-decrease) protocol. We show that if all users of the network use the protocol, and all connections last for at least logarithmically many rounds, then the total weighted throughput (value of all packets received) is near the maximum possible. Our analysis includes round-trip-times, and (in contrast to most previous analyses) gives explicit convergence rates, allows connections to start and stop, and allows capacities to change.",
        "published": "2002-05-18T01:54:56Z",
        "link": "http://arxiv.org/abs/cs/0205032v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.NI",
            "F.1.2, F.2.0; C.2.6; G.1.6"
        ]
    },
    {
        "title": "On-Line File Caching",
        "authors": [
            "Neal E. Young"
        ],
        "summary": "In the on-line file-caching problem problem, the input is a sequence of requests for files, given on-line (one at a time). Each file has a non-negative size and a non-negative retrieval cost. The problem is to decide which files to keep in a fixed-size cache so as to minimize the sum of the retrieval costs for files that are not in the cache when requested. The problem arises in web caching by browsers and by proxies. This paper describes a natural generalization of LRU called Landlord and gives an analysis showing that it has an optimal performance guarantee (among deterministic on-line algorithms).   The paper also gives an analysis of the algorithm in a so-called ``loosely'' competitive model, showing that on a ``typical'' cache size, either the performance guarantee is O(1) or the total retrieval cost is insignificant.",
        "published": "2002-05-18T02:04:59Z",
        "link": "http://arxiv.org/abs/cs/0205033v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.NI",
            "F.1.2, F.2.0, C.2.0"
        ]
    },
    {
        "title": "Simple Strategies for Large Zero-Sum Games with Applications to   Complexity Theory",
        "authors": [
            "Richard Lipton",
            "Neal E. Young"
        ],
        "summary": "Von Neumann's Min-Max Theorem guarantees that each player of a zero-sum matrix game has an optimal mixed strategy. This paper gives an elementary proof that each player has a near-optimal mixed strategy that chooses uniformly at random from a multiset of pure strategies of size logarithmic in the number of pure strategies available to the opponent.   For exponentially large games, for which even representing an optimal mixed strategy can require exponential space, it follows that there are near-optimal, linear-size strategies. These strategies are easy to play and serve as small witnesses to the approximate value of the game.   As a corollary, it follows that every language has small ``hard'' multisets of inputs certifying that small circuits can't decide the language. For example, if SAT does not have polynomial-size circuits, then, for each n and c, there is a set of n^(O(c)) Boolean formulae of size n such that no circuit of size n^c (or algorithm running in time n^c) classifies more than two-thirds of the formulae succesfully.",
        "published": "2002-05-18T03:41:40Z",
        "link": "http://arxiv.org/abs/cs/0205035v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.3; G.2.1"
        ]
    },
    {
        "title": "On the Number of Iterations for Dantzig-Wolfe Optimization and   Packing-Covering Approximation Algorithms",
        "authors": [
            "Phil Klein",
            "Neal E. Young"
        ],
        "summary": "We give a lower bound on the iteration complexity of a natural class of Lagrangean-relaxation algorithms for approximately solving packing/covering linear programs. We show that, given an input with $m$ random 0/1-constraints on $n$ variables, with high probability, any such algorithm requires $\\Omega(\\rho \\log(m)/\\epsilon^2)$ iterations to compute a $(1+\\epsilon)$-approximate solution, where $\\rho$ is the width of the input. The bound is tight for a range of the parameters $(m,n,\\rho,\\epsilon)$.   The algorithms in the class include Dantzig-Wolfe decomposition, Benders' decomposition, Lagrangean relaxation as developed by Held and Karp [1971] for lower-bounding TSP, and many others (e.g. by Plotkin, Shmoys, and Tardos [1988] and Grigoriadis and Khachiyan [1996]). To prove the bound, we use a discrepancy argument to show an analogous lower bound on the support size of $(1+\\epsilon)$-approximate mixed strategies for random two-player zero-sum 0/1-matrix games.",
        "published": "2002-05-19T14:43:19Z",
        "link": "http://arxiv.org/abs/cs/0205046v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.1; G.1.6"
        ]
    },
    {
        "title": "Parameterized Intractability of Motif Search Problems",
        "authors": [
            "Michael R. Fellows",
            "Jens Gramm",
            "Rolf Niedermeier"
        ],
        "summary": "We show that Closest Substring, one of the most important problems in the field of biological sequence analysis, is W[1]-hard when parameterized by the number k of input strings (and remains so, even over a binary alphabet). This problem is therefore unlikely to be solvable in time O(f(k)\\cdot n^{c}) for any function f of k and constant c independent of k. The problem can therefore be expected to be intractable, in any practical sense, for k>=3. Our result supports the intuition that Closest Substring is computationally much harder than the special case of Closest String, although both problems are NP-complete. We also prove W[1]-hardness for other parameterizations in the case of unbounded alphabet size. Our W[1]-hardness result for Closest Substring generalizes to Consensus Patterns, a problem of similar significance in computational biology.",
        "published": "2002-05-21T10:23:18Z",
        "link": "http://arxiv.org/abs/cs/0205056v1",
        "categories": [
            "cs.CC",
            "F.2;F.2.2;J.3"
        ]
    },
    {
        "title": "Adaptive Quantum Computation, Constant Depth Quantum Circuits and   Arthur-Merlin Games",
        "authors": [
            "Barbara M. Terhal",
            "David P. DiVincenzo"
        ],
        "summary": "We present evidence that there exist quantum computations that can be carried out in constant depth, using 2-qubit gates, that cannot be simulated classically with high accuracy. We prove that if one can simulate these circuits classically efficiently then the complexity class BQP is contained in AM.",
        "published": "2002-05-21T23:28:22Z",
        "link": "http://arxiv.org/abs/quant-ph/0205133v6",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Three complete deterministic polynomial algorithms for 3SAT",
        "authors": [
            "Charles Sauerbier"
        ],
        "summary": "Three algorithms are presented that determine the existence of satisfying assignments for 3SAT Boolean satisfiability expressions. One algorithm is presented for determining an instance of a satisfying assignment, where such exists. The algorithms are each deterministic and of polynomial complexity. The algorithms determining existence are complete as each produces a certificate of non-satisfiability, for instances where no satisfying assignment exists, and of satisfiability for such assignment does exist.",
        "published": "2002-05-24T21:09:09Z",
        "link": "http://arxiv.org/abs/cs/0205064v6",
        "categories": [
            "cs.CC",
            "F.2.2, F.1.1",
            "F.2.2; F.1.1"
        ]
    },
    {
        "title": "Complexity Results about Nash Equilibria",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "Noncooperative game theory provides a normative framework for analyzing strategic interactions. However, for the toolbox to be operational, the solutions it defines will have to be computed. In this paper, we provide a single reduction that 1) demonstrates NP-hardness of determining whether Nash equilibria with certain natural properties exist, and 2) demonstrates the #P-hardness of counting Nash equilibria (or connected sets of Nash equilibria). We also show that 3) determining whether a pure-strategy Bayes-Nash equilibrium exists is NP-hard, and that 4) determining whether a pure-strategy Nash equilibrium exists in a stochastic (Markov) game is PSPACE-hard even if the game is invisible (this remains NP-hard if the game is finite). All of our hardness results hold even if there are only two players and the game is symmetric.   Keywords: Nash equilibrium; game theory; computational complexity; noncooperative game theory; normal form game; stochastic game; Markov game; Bayes-Nash equilibrium; multiagent systems.",
        "published": "2002-05-28T23:32:37Z",
        "link": "http://arxiv.org/abs/cs/0205074v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Complexity of Mechanism Design",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "The aggregation of conflicting preferences is a central problem in multiagent systems. The key difficulty is that the agents may report their preferences insincerely. Mechanism design is the art of designing the rules of the game so that the agents are motivated to report their preferences truthfully and a (socially) desirable outcome is chosen. We propose an approach where a mechanism is automatically created for the preference aggregation setting at hand. This has several advantages, but the downside is that the mechanism design optimization problem needs to be solved anew each time. Focusing on settings where side payments are not possible, we show that the mechanism design problem is NP-complete for deterministic mechanisms. This holds both for dominant-strategy implementation and for Bayes-Nash implementation. We then show that if we allow randomized mechanisms, the mechanism design problem becomes tractable. In other words, the coordinator can tackle the computational complexity introduced by its uncertainty about the agents' preferences by making the agents face additional uncertainty. This comes at no loss, and in some cases at a gain, in the (social) objective.",
        "published": "2002-05-28T23:43:12Z",
        "link": "http://arxiv.org/abs/cs/0205075v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Vote Elicitation: Complexity and Strategy-Proofness",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "Preference elicitation is a central problem in AI, and has received significant attention in single-agent settings. It is also a key problem in multiagent systems, but has received little attention here so far. In this setting, the agents may have different preferences that often must be aggregated using voting. This leads to interesting issues because what, if any, information should be elicited from an agent depends on what other agents have revealed about their preferences so far.   In this paper we study effective elicitation, and its impediments, for the most common voting protocols. It turns out that in the Single Transferable Vote protocol, even knowing when to terminate elicitation is mathcal NP-complete, while this is easy for all the other protocols under study. Even for these protocols, determining how to elicit effectively is NP-complete, even with perfect suspicions about how the agents will vote. The exception is the Plurality protocol where such effective elicitation is easy.   We also show that elicitation introduces additional opportunities for strategic manipulation by the voters. We demonstrate how to curtail the space of elicitation schemes so that no such additional strategic issues arise.",
        "published": "2002-05-29T00:10:26Z",
        "link": "http://arxiv.org/abs/cs/0205073v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "The Fastest and Shortest Algorithm for All Well-Defined Problems",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "An algorithm $M$ is described that solves any well-defined problem $p$ as quickly as the fastest algorithm computing a solution to $p$, save for a factor of 5 and low-order additive terms. $M$ optimally distributes resources between the execution of provably correct $p$-solving programs and an enumeration of all proofs, including relevant proofs of program correctness and of time bounds on program runtimes. $M$ avoids Blum's speed-up theorem by ignoring programs without correctness proof. $M$ has broader applicability and can be faster than Levin's universal search, the fastest method for inverting functions save for a large multiplicative constant. An extension of Kolmogorov complexity and two novel natural measures of function complexity are used to show that the most efficient program computing some function $f$ is also among the shortest programs provably computing $f$.",
        "published": "2002-06-14T11:04:44Z",
        "link": "http://arxiv.org/abs/cs/0206022v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.2.3"
        ]
    },
    {
        "title": "On communication over an entanglement-assisted quantum channel",
        "authors": [
            "Ashwin Nayak",
            "Julia Salzman"
        ],
        "summary": "Shared entanglement is a resource available to parties communicating over a quantum channel, much akin to public coins in classical communication protocols. Whereas shared randomness does not help in the transmission of information, or significantly reduce the classical complexity of computing functions (as compared to private-coin protocols), shared entanglement leads to startling phenomena such as ``quantum teleportation'' and ``superdense coding.''   The problem of characterising the power of prior entanglement has puzzled many researchers. In this paper, we revisit the problem of transmitting classical bits over an entanglement-assisted quantum channel. We derive a new, optimal bound on the number of quantum bits required for this task, for any given probability of error. All known lower bounds in the setting of bounded error entanglement-assisted communication are based on sophisticated information theoretic arguments. In contrast, our result is derived from first principles, using a simple linear algebraic technique.",
        "published": "2002-06-19T03:10:26Z",
        "link": "http://arxiv.org/abs/quant-ph/0206122v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Average-case complexity and decision problems in group theory",
        "authors": [
            "Ilya Kapovich",
            "Alexei Myasnikov",
            "Paul Schupp",
            "Vladimir Shpilrain"
        ],
        "summary": "We investigate the average-case complexity of decision problems for finitely generated groups, in particular the word and membership problems. Using our recent results on ``generic-case complexity'' we show that if a finitely generated group $G$ has the word problem solvable in subexponential time and has a subgroup of finite index which possesses a non-elementary word-hyperbolic quotient group, then the average-case complexity of the word problem for $G$ is linear time, uniformly with respect to the collection of all length-invariant measures on $G$. For example, the result applies to all braid groups $B_n$.",
        "published": "2002-06-25T22:17:34Z",
        "link": "http://arxiv.org/abs/math/0206273v2",
        "categories": [
            "math.GR",
            "cs.CC",
            "math.GT",
            "20F36"
        ]
    },
    {
        "title": "Computational complexity arising from degree correlations in networks",
        "authors": [
            "Alexei Vazquez",
            "Martin Weigt"
        ],
        "summary": "We apply a Bethe-Peierls approach to statistical-mechanics models defined on random networks of arbitrary degree distribution and arbitrary correlations between the degrees of neighboring vertices. Using the NP-hard optimization problem of finding minimal vertex covers on these graphs, we show that such correlations may lead to a qualitatively different solution structure as compared to uncorrelated networks. This results in a higher complexity of the network in a computational sense: Simple heuristic algorithms fail to find a minimal vertex cover in the highly correlated case, whereas uncorrelated networks seem to be simple from the point of view of combinatorial optimization.",
        "published": "2002-07-01T14:11:35Z",
        "link": "http://arxiv.org/abs/cond-mat/0207035v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Computing Elementary Symmetric Polynomials with a Sublinear Number of   Multiplications",
        "authors": [
            "Vince Grolmusz"
        ],
        "summary": "Elementary symmetric polynomials $S_n^k$ are used as a benchmark for the bounded-depth arithmetic circuit model of computation. In this work we prove that $S_n^k$ modulo composite numbers $m=p_1p_2$ can be computed with much fewer multiplications than over any field, if the coefficients of monomials $x_{i_1}x_{i_2}... x_{i_k}$ are allowed to be 1 either mod $p_1$ or mod $p_2$ but not necessarily both. More exactly, we prove that for any constant $k$ such a representation of $S_n^k$ can be computed modulo $p_1p_2$ using only $\\exp(O(\\sqrt{\\log n}\\log\\log n))$ multiplications on the most restricted depth-3 arithmetic circuits, for $\\min({p_1,p_2})>k!$. Moreover, the number of multiplications remain sublinear while $k=O(\\log\\log n).$ In contrast, the well-known Graham-Pollack bound yields an $n-1$ lower bound for the number of multiplications even for the exact computation (not the representation) of $S_n^2$. Our results generalize for other non-prime power composite moduli as well. The proof uses the famous BBR-polynomial of Barrington, Beigel and Rudich.",
        "published": "2002-07-03T14:32:21Z",
        "link": "http://arxiv.org/abs/cs/0207009v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "F1.3, F2.1"
        ]
    },
    {
        "title": "On Concise Encodings of Preferred Extensions",
        "authors": [
            "Paul E. Dunne"
        ],
        "summary": "Much work on argument systems has focussed on preferred extensions which define the maximal collectively defensible subsets. Identification and enumeration of these subsets is (under the usual assumptions) computationally demanding. We consider approaches to deciding if a subset S is a preferred extension which query a representations encoding all such extensions, so that the computational effort is invested once only (for the initial enumeration) rather than for each separate query.",
        "published": "2002-07-08T13:24:07Z",
        "link": "http://arxiv.org/abs/cs/0207024v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "F.2.m; I.2.3; I.2.m"
        ]
    },
    {
        "title": "Permutation graphs, fast forward permutations, and sampling the cycle   structure of a permutation",
        "authors": [
            "Boaz Tsaban"
        ],
        "summary": "A permutation P on {1,..,N} is a_fast_forward_permutation_ if for each m the computational complexity of evaluating P^m(x)$ is small independently of m and x. Naor and Reingold constructed fast forward pseudorandom cycluses and involutions. By studying the evolution of permutation graphs, we prove that the number of queries needed to distinguish a random cyclus from a random permutation on {1,..,N} is Theta(N) if one does not use queries of the form P^m(x), but is only Theta(1) if one is allowed to make such queries.   We construct fast forward permutations which are indistinguishable from random permutations even when queries of the form P^m(x) are allowed. This is done by introducing an efficient method to sample the cycle structure of a random permutation, which in turn solves an open problem of Naor and Reingold.",
        "published": "2002-07-08T16:30:10Z",
        "link": "http://arxiv.org/abs/cs/0207027v6",
        "categories": [
            "cs.CR",
            "cs.CC",
            "math.CO",
            "math.PR",
            "F.2.2; G.2.2; G.3"
        ]
    },
    {
        "title": "The Rise and Fall of the Church-Turing Thesis",
        "authors": [
            "Mark Burgin"
        ],
        "summary": "The essay consists of three parts. In the first part, it is explained how theory of algorithms and computations evaluates the contemporary situation with computers and global networks. In the second part, it is demonstrated what new perspectives this theory opens through its new direction that is called theory of super-recursive algorithms. These algorithms have much higher computing power than conventional algorithmic schemes. In the third part, we explicate how realization of what this theory suggests might influence life of people in future. It is demonstrated that now the theory is far ahead computing practice and practice has to catch up with the theory. We conclude with a comparison of different approaches to the development of information technology.",
        "published": "2002-07-12T02:51:45Z",
        "link": "http://arxiv.org/abs/cs/0207055v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "F.1.1; F.1.2; F.2.0; F.4.1; I.1.2; I.2.0"
        ]
    },
    {
        "title": "Modulo Three Problem With A Cellular Automaton Solution",
        "authors": [
            "Hao Xu",
            "K. M. Lee",
            "H. F. Chau"
        ],
        "summary": "An important global property of a bit string is the number of ones in it. It has been found that the parity (odd or even) of this number can be found by a sequence of deterministic, translational invariant cellular automata with parallel update in succession for a total of O(N^2) time. In this paper, we discover a way to check if this number is divisible by three using the same kind of cellular automata in O(N^3) time. We also speculate that the method described here could be generalized to check if it is divisible by four and other positive integers.",
        "published": "2002-07-18T09:17:04Z",
        "link": "http://arxiv.org/abs/nlin/0207032v1",
        "categories": [
            "nlin.CG",
            "cs.CC"
        ]
    },
    {
        "title": "Complexity of Nested Circumscription and Nested Abnormality Theories",
        "authors": [
            "Marco Cadoli",
            "Thomas Eiter",
            "Georg Gottlob"
        ],
        "summary": "The need for a circumscriptive formalism that allows for simple yet elegant modular problem representation has led Lifschitz (AIJ, 1995) to introduce nested abnormality theories (NATs) as a tool for modular knowledge representation, tailored for applying circumscription to minimize exceptional circumstances. Abstracting from this particular objective, we propose L_{CIRC}, which is an extension of generic propositional circumscription by allowing propositional combinations and nesting of circumscriptive theories. As shown, NATs are naturally embedded into this language, and are in fact of equal expressive capability. We then analyze the complexity of L_{CIRC} and NATs, and in particular the effect of nesting. The latter is found to be a source of complexity, which climbs the Polynomial Hierarchy as the nesting depth increases and reaches PSPACE-completeness in the general case. We also identify meaningful syntactic fragments of NATs which have lower complexity. In particular, we show that the generalization of Horn circumscription in the NAT framework remains CONP-complete, and that Horn NATs without fixed letters can be efficiently transformed into an equivalent Horn CNF, which implies polynomial solvability of principal reasoning tasks. Finally, we also study extensions of NATs and briefly address the complexity in the first-order case. Our results give insight into the ``cost'' of using L_{CIRC} (resp. NATs) as a host language for expressing other formalisms such as action theories, narratives, or spatial theories.",
        "published": "2002-07-20T19:59:21Z",
        "link": "http://arxiv.org/abs/cs/0207072v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LO",
            "I.2.3; I.2.4; F4.1; F.2.2"
        ]
    },
    {
        "title": "Paraconsistent Reasoning via Quantified Boolean Formulas,I: Axiomatising   Signed Systems",
        "authors": [
            "Philippe Besnard",
            "Torsten Schaub",
            "Hans Tompits",
            "Stefan Woltran"
        ],
        "summary": "Signed systems were introduced as a general, syntax-independent framework for paraconsistent reasoning, that is, non-trivialised reasoning from inconsistent information. In this paper, we show how the family of corresponding paraconsistent consequence relations can be axiomatised by means of quantified Boolean formulas. This approach has several benefits. First, it furnishes an axiomatic specification of paraconsistent reasoning within the framework of signed systems. Second, this axiomatisation allows us to identify upper bounds for the complexity of the different signed consequence relations. We strengthen these upper bounds by providing strict complexity results for the considered reasoning tasks. Finally, we obtain an implementation of different forms of paraconsistent reasoning by appeal to the existing system QUIP.",
        "published": "2002-07-25T14:40:51Z",
        "link": "http://arxiv.org/abs/cs/0207084v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.1.3; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Non-Interactive Quantum Statistical and Perfect Zero-Knowledge",
        "authors": [
            "Hirotada Kobayashi"
        ],
        "summary": "This paper introduces quantum analogues of non-interactive perfect and statistical zero-knowledge proof systems. Similar to the classical cases, it is shown that sharing randomness or entanglement is necessary for non-trivial protocols of non-interactive quantum perfect and statistical zero-knowledge. It is also shown that, with sharing EPR pairs a priori, the class of languages having one-sided bounded error non-interactive quantum perfect zero-knowledge proof systems has a natural complete problem. Non-triviality of such a proof system is based on the fact proved in this paper that the Graph Non-Automorphism problem, which is not known in BQP, can be reduced to our complete problem. Our results may be the first non-trivial quantum zero-knowledge proofs secure even against dishonest quantum verifiers, since our protocols are non-interactive, and thus the zero-knowledge property does not depend on whether the verifier in the protocol is honest or not. A restricted version of our complete problem derives a natural complete problem for BQP.",
        "published": "2002-07-29T10:22:34Z",
        "link": "http://arxiv.org/abs/quant-ph/0207158v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Optimal Ordered Problem Solver",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "We present a novel, general, optimally fast, incremental way of searching for a universal algorithm that solves each task in a sequence of tasks. The Optimal Ordered Problem Solver (OOPS) continually organizes and exploits previously found solutions to earlier tasks, efficiently searching not only the space of domain-specific algorithms, but also the space of search algorithms. Essentially we extend the principles of optimal nonincremental universal search to build an incremental universal learner that is able to improve itself through experience. In illustrative experiments, our self-improver becomes the first general system that learns to solve all n disk Towers of Hanoi tasks (solution size 2^n-1) for n up to 30, profiting from previously solved, simpler tasks involving samples of a simple context free language.",
        "published": "2002-07-31T14:33:11Z",
        "link": "http://arxiv.org/abs/cs/0207097v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LG",
            "I.2.2;I.2.6;I.2.8"
        ]
    },
    {
        "title": "Classification of Random Boolean Networks",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "We provide the first classification of different types of Random Boolean Networks (RBNs). We study the differences of RBNs depending on the degree of synchronicity and determinism of their updating scheme. For doing so, we first define three new types of RBNs. We note some similarities and differences between different types of RBNs with the aid of a public software laboratory we developed. Particularly, we find that the point attractors are independent of the updating scheme, and that RBNs are more different depending on their determinism or non-determinism rather than depending on their synchronicity or asynchronicity. We also show a way of mapping non-synchronous deterministic RBNs into synchronous RBNs. Our results are important for justifying the use of specific types of RBNs for modelling natural phenomena.",
        "published": "2002-08-01T13:26:57Z",
        "link": "http://arxiv.org/abs/cs/0208001v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "math.DS",
            "nlin.CG",
            "F.1.1, B.6.1, C.1.3"
        ]
    },
    {
        "title": "Rectangle Size Bounds and Threshold Covers in Communication Complexity",
        "authors": [
            "Hartmut Klauck"
        ],
        "summary": "We investigate the power of the most important lower bound technique in randomized communication complexity, which is based on an evaluation of the maximal size of approximately monochromatic rectangles, minimized over all distributions on the inputs. While it is known that the 0-error version of this bound is polynomially tight for deterministic communication, nothing in this direction is known for constant error and randomized communication complexity. We first study a one-sided version of this bound and obtain that its value lies between the MA- and AM-complexities of the considered function. Hence the lower bound actually works for a (communication complexity) class between MA cap co-MA and AM cap co-AM. We also show that the MA-complexity of the disjointness problem is Omega(sqrt(n)). Following this we consider the conjecture that the lower bound method is polynomially tight for randomized communication complexity. First we disprove a distributional version of this conjecture. Then we give a combinatorial characterization of the value of the lower bound method, in which the optimization over all distributions is absent. This characterization is done by what we call a uniform threshold cover. We also study relaxations of this notion, namely approximate majority covers and majority covers, and compare these three notions in power, exhibiting exponential separations. Each of these covers captures a lower bound method previously used for randomized communication complexity.",
        "published": "2002-08-05T14:36:27Z",
        "link": "http://arxiv.org/abs/cs/0208006v1",
        "categories": [
            "cs.CC",
            "F.1.3;F.1.2.2"
        ]
    },
    {
        "title": "Quantum Circuits with Unbounded Fan-out",
        "authors": [
            "Peter Hoyer",
            "Robert Spalek"
        ],
        "summary": "We demonstrate that the unbounded fan-out gate is very powerful. Constant-depth polynomial-size quantum circuits with bounded fan-in and unbounded fan-out over a fixed basis (denoted by QNCf^0) can approximate with polynomially small error the following gates: parity, mod[q], And, Or, majority, threshold[t], exact[q], and Counting. Classically, we need logarithmic depth even if we can use unbounded fan-in gates. If we allow arbitrary one-qubit gates instead of a fixed basis, then these circuits can also be made exact in log-star depth. Sorting, arithmetical operations, phase estimation, and the quantum Fourier transform with arbitrary moduli can also be approximated in constant depth.",
        "published": "2002-08-07T09:43:10Z",
        "link": "http://arxiv.org/abs/quant-ph/0208043v4",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "A note on fractional derivative modeling of broadband   frequency-dependent absorption: Model III",
        "authors": [
            "W. Chen"
        ],
        "summary": "By far, the fractional derivative model is mainly related to the modelling of complicated solid viscoelastic material. In this study, we try to build the fractional derivative PDE model for broadband ultrasound propagation through human tissues.",
        "published": "2002-08-08T07:29:10Z",
        "link": "http://arxiv.org/abs/cs/0208016v1",
        "categories": [
            "cs.CE",
            "cs.CC",
            "G1.2, G1.8"
        ]
    },
    {
        "title": "Exponential Lower Bound for 2-Query Locally Decodable Codes via a   Quantum Argument",
        "authors": [
            "Iordanis Kerenidis",
            "Ronald de Wolf"
        ],
        "summary": "A locally decodable code encodes n-bit strings x in m-bit codewords C(x), in such a way that one can recover any bit x_i from a corrupted codeword by querying only a few bits of that word. We use a quantum argument to prove that LDCs with 2 classical queries need exponential length: m=2^{Omega(n)}. Previously this was known only for linear codes (Goldreich et al. 02). Our proof shows that a 2-query LDC can be decoded with only 1 quantum query, and then proves an exponential lower bound for such 1-query locally quantum-decodable codes. We also show that q quantum queries allow more succinct LDCs than the best known LDCs with q classical queries. Finally, we give new classical lower bounds and quantum upper bounds for the setting of private information retrieval. In particular, we exhibit a quantum 2-server PIR scheme with O(n^{3/10}) qubits of communication, improving upon the O(n^{1/3}) bits of communication of the best known classical 2-server PIR.",
        "published": "2002-08-09T19:50:18Z",
        "link": "http://arxiv.org/abs/quant-ph/0208062v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Does P = NP?",
        "authors": [
            "C. Sauerbier"
        ],
        "summary": "This paper considers the question of P = NP in context of the polynomial time SAT algorithm. It posits proposition dependent on existence of conjectured problem that even where the algorithm is shown to solve SAT in polynomial time it remains theoretically possible for there to yet exist a non-deterministically polynomial (NP) problem for which the algorithm does not provide a polynomial (P) time solution. The paper leaves open as subject of continuing research the question of existence of instance of conjectured problem.",
        "published": "2002-08-12T04:32:36Z",
        "link": "http://arxiv.org/abs/cs/0208018v2",
        "categories": [
            "cs.CC",
            "F.2.2;F.1.1"
        ]
    },
    {
        "title": "Mathematical basis for polySAT implication operator",
        "authors": [
            "Charles Sauerbier"
        ],
        "summary": "The mathematical basis motivating the \"implication operator\" of the polySAT algorithm and its function is examined. Such is not undertaken with onerous rigor of symbolic mathematics; a more intuitive visual appeal being employed to present some of the mathematical premises underlying function of the implication operator.",
        "published": "2002-08-18T04:31:39Z",
        "link": "http://arxiv.org/abs/cs/0208026v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.2.2;F.1.1"
        ]
    },
    {
        "title": "Coloring random graphs",
        "authors": [
            "R. Mulet",
            "A. Pagnani",
            "M. Weigt",
            "R. Zecchina"
        ],
        "summary": "We study the graph coloring problem over random graphs of finite average connectivity $c$. Given a number $q$ of available colors, we find that graphs with low connectivity admit almost always a proper coloring whereas graphs with high connectivity are uncolorable. Depending on $q$, we find the precise value of the critical average connectivity $c_q$. Moreover, we show that below $c_q$ there exist a clustering phase $c\\in [c_d,c_q]$ in which ground states spontaneously divide into an exponential number of clusters and where the proliferation of metastable states is responsible for the onset of complexity in local search algorithms.",
        "published": "2002-08-23T18:15:11Z",
        "link": "http://arxiv.org/abs/cond-mat/0208460v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "Perfectly Secure Message Transmission Revisited",
        "authors": [
            "Yvo Desmedt",
            "Yongge Wang"
        ],
        "summary": "Achieving secure communications in networks has been one of the most important problems in information technology. Dolev, Dwork, Waarts, and Yung have studied secure message transmission in one-way or two-way channels. They only consider the case when all channels are two-way or all channels are one-way. Goldreich, Goldwasser, and Linial, Franklin and Yung, Franklin and Wright, and Wang and Desmedt have studied secure communication and secure computation in multi-recipient (multicast) models. In a ``multicast channel'' (such as Ethernet), one processor can send the same message--simultaneously and privately--to a fixed subset of processors. In this paper, we shall study necessary and sufficient conditions for achieving secure communications against active adversaries in mixed one-way and two-way channels. We also discuss multicast channels and neighbor network channels.",
        "published": "2002-08-26T21:02:59Z",
        "link": "http://arxiv.org/abs/cs/0208041v1",
        "categories": [
            "cs.CR",
            "cs.CC",
            "C.2.2; E.1; E.3; E.4; F.1; F.2.3; H.1.1; H.2.0"
        ]
    },
    {
        "title": "Gales Suffice for Constructive Dimension",
        "authors": [
            "John M. Hitchcock"
        ],
        "summary": "Supergales, generalizations of supermartingales, have been used by Lutz (2002) to define the constructive dimensions of individual binary sequences. Here it is shown that gales, the corresponding generalizations of martingales, can be equivalently used to define constructive dimension.",
        "published": "2002-08-29T20:48:14Z",
        "link": "http://arxiv.org/abs/cs/0208043v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Gales and supergales are equivalent for defining constructive Hausdorff   dimension",
        "authors": [
            "Stephen A. Fenner"
        ],
        "summary": "We show that for a wide range of probability measures, constructive gales are interchangable with constructive supergales for defining constructive Hausdorff dimension, thus generalizing a previous independent result of Hitchcock (cs.CC/0208043) and partially answering an open question of Lutz (cs.CC/0203017).",
        "published": "2002-08-29T21:25:47Z",
        "link": "http://arxiv.org/abs/cs/0208044v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "A Novel Statistical Diagnosis of Clinical Data",
        "authors": [
            "Gene Kim",
            "MyungHo Kim"
        ],
        "summary": "In this paper, we present a diagnosis method of diseases from clinical data. The data are routine test such as urine test, hematology, chemistries etc. Though those tests have been done for people who check in medical institutes, how each item of the data interacts each other and which combination of them cause a disease are neither understood nor studied well. Here we attack the practically important problem by putting the data into mathematical setup and applying support vector machine. Finally we present simulation results for fatty liver, gastritis etc and discuss about their implications.",
        "published": "2002-09-02T03:52:48Z",
        "link": "http://arxiv.org/abs/cs/0209001v1",
        "categories": [
            "cs.CE",
            "cs.CC",
            "I.1.2; H.1.1; I.5.0"
        ]
    },
    {
        "title": "Quantum Lower Bound for Recursive Fourier Sampling",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "One of the earliest quantum algorithms was discovered by Bernstein and Vazirani, for a problem called Recursive Fourier Sampling. This paper shows that the Bernstein-Vazirani algorithm is not far from optimal. The moral is that the need to \"uncompute\" garbage can impose a fundamental limit on efficient quantum computation. The proof introduces a new parameter of Boolean functions called the \"nonparity coefficient,\" which might be of independent interest.",
        "published": "2002-09-09T05:41:34Z",
        "link": "http://arxiv.org/abs/quant-ph/0209060v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Does NP not equal P?",
        "authors": [
            "C. Sauerbier"
        ],
        "summary": "Stephen Cook posited SAT is NP-Complete in 1971. If SAT is NP-Complete then, as is generally accepted, any polynomial solution of it must also present a polynomial solution of all NP decision problems. It is here argued, however, that NP is not of necessity equivalent to P where it is shown that SAT is contained in P. This due to a paradox, of nature addressed by both Godel and Russell, in regards to the P-NP system in total.",
        "published": "2002-09-10T16:08:21Z",
        "link": "http://arxiv.org/abs/cs/0209015v2",
        "categories": [
            "cs.CC",
            "F.2.2;F.1.1"
        ]
    },
    {
        "title": "Probabilistic Reversible Automata and Quantum Automata",
        "authors": [
            "Marats Golovkins",
            "Maksim Kravtsev"
        ],
        "summary": "To study relationship between quantum finite automata and probabilistic finite automata, we introduce a notion of probabilistic reversible automata (PRA, or doubly stochastic automata). We find that there is a strong relationship between different possible models of PRA and corresponding models of quantum finite automata. We also propose a classification of reversible finite 1-way automata.",
        "published": "2002-09-16T16:26:02Z",
        "link": "http://arxiv.org/abs/cs/0209018v2",
        "categories": [
            "cs.CC",
            "cs.FL",
            "quant-ph",
            "F.1.1; F.4.3"
        ]
    },
    {
        "title": "Computers with closed timelike curves can solve hard problems",
        "authors": [
            "Todd A. Brun"
        ],
        "summary": "A computer which has access to a closed timelike curve, and can thereby send the results of calculations into its own past, can exploit this to solve difficult computational problems efficiently. I give a specific demonstration of this for the problem of factoring large numbers, and argue that a similar approach can solve NP-complete and PSPACE-complete problems. I discuss the potential impact of quantum effects on this result.",
        "published": "2002-09-18T18:52:39Z",
        "link": "http://arxiv.org/abs/gr-qc/0209061v1",
        "categories": [
            "gr-qc",
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "Complexity Results on DPLL and Resolution",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "DPLL and resolution are two popular methods for solving the problem of propositional satisfiability. Rather than algorithms, they are families of algorithms, as their behavior depend on some choices they face during execution: DPLL depends on the choice of the literal to branch on; resolution depends on the choice of the pair of clauses to resolve at each step. The complexity of making the optimal choice is analyzed in this paper. Extending previous results, we prove that choosing the optimal literal to branch on in DPLL is Delta[log]^2-hard, and becomes NP^PP-hard if branching is only allowed on a subset of variables. Optimal choice in regular resolution is both NP-hard and CoNP-hard. The problem of determining the size of the optimal proofs is also analyzed: it is CoNP-hard for DPLL, and Delta[log]^2-hard if a conjecture we make is true. This problem is CoNP-hard for regular resolution.",
        "published": "2002-09-27T14:12:48Z",
        "link": "http://arxiv.org/abs/cs/0209032v3",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.2.2; F.1.3; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Quantum Certificate Complexity",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "Given a Boolean function f, we study two natural generalizations of the certificate complexity C(f): the randomized certificate complexity RC(f) and the quantum certificate complexity QC(f). Using Ambainis' adversary method, we exactly characterize QC(f) as the square root of RC(f). We then use this result to prove the new relation R0(f) = O(Q2(f)^2 Q0(f) log n) for total f, where R0, Q2, and Q0 are zero-error randomized, bounded-error quantum, and zero-error quantum query complexities respectively. Finally we give asymptotic gaps between the measures, including a total f for which C(f) is superquadratic in QC(f), and a symmetric partial f for which QC(f) = O(1) yet Q2(f) = Omega(n/log n).",
        "published": "2002-10-02T22:03:37Z",
        "link": "http://arxiv.org/abs/quant-ph/0210020v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Compilability of Abduction",
        "authors": [
            "Paolo Liberatore",
            "Marco Schaerf"
        ],
        "summary": "Abduction is one of the most important forms of reasoning; it has been successfully applied to several practical problems such as diagnosis. In this paper we investigate whether the computational complexity of abduction can be reduced by an appropriate use of preprocessing. This is motivated by the fact that part of the data of the problem (namely, the set of all possible assumptions and the theory relating assumptions and manifestations) are often known before the rest of the problem. In this paper, we show some complexity results about abduction when compilation is allowed.",
        "published": "2002-10-09T17:17:27Z",
        "link": "http://arxiv.org/abs/cs/0210007v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "F.4.1; F.1.3"
        ]
    },
    {
        "title": "Cellular automata and communication complexity",
        "authors": [
            "Christoph Durr",
            "Ivan Rapaport",
            "Guillaume Theyssier"
        ],
        "summary": "The model of cellular automata is fascinating because very simple local rules can generate complex global behaviors. The relationship between local and global function is subject of many studies. We tackle this question by using results on communication complexity theory and, as a by-product, we provide (yet another) classification of cellular automata.",
        "published": "2002-10-11T16:29:19Z",
        "link": "http://arxiv.org/abs/cs/0210008v1",
        "categories": [
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "On the Cell-based Complexity of Recognition of Bounded Configurations by   Finite Dynamic Cellular Automata",
        "authors": [
            "Maxim Makatchev"
        ],
        "summary": "This paper studies complexity of recognition of classes of bounded configurations by a generalization of conventional cellular automata (CA) -- finite dynamic cellular automata (FDCA). Inspired by the CA-based models of biological and computer vision, this study attempts to derive the properties of a complexity measure and of the classes of input configurations that make it beneficial to realize the recognition via a two-layered automaton as compared to a one-layered automaton. A formalized model of an image pattern recognition task is utilized to demonstrate that the derived conditions can be satisfied for a non-empty set of practical problems.",
        "published": "2002-10-11T19:55:16Z",
        "link": "http://arxiv.org/abs/cs/0210009v1",
        "categories": [
            "cs.CC",
            "cs.CV",
            "F.1.3; F.2.2; F.2.3; I.4.3; I.5.1; I.5.4; I.5.5"
        ]
    },
    {
        "title": "A Note on Induction Schemas in Bounded Arithmetic",
        "authors": [
            "Aleksandar Ignjatovic"
        ],
        "summary": "As is well known, Buss' theory of bounded arithmetic $S^{1}_{2}$ proves $\\Sigma_{0}^{b}(\\Sigma_{1}^{b})-LIND$; however, we show that Allen's $D_{2}^{1}$ does not prove $\\Sigma_{0}^{b}(\\Sigma_{1}^{b})-LLIND$ unless $P = NC$. We also give some interesting alternative axiomatisations of $S^{1}_{2}$.",
        "published": "2002-10-14T05:07:03Z",
        "link": "http://arxiv.org/abs/cs/0210011v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Tetris is Hard, Even to Approximate",
        "authors": [
            "Erik D. Demaine",
            "Susan Hohenberger",
            "David Liben-Nowell"
        ],
        "summary": "In the popular computer game of Tetris, the player is given a sequence of tetromino pieces and must pack them into a rectangular gameboard initially occupied by a given configuration of filled squares; any completely filled row of the gameboard is cleared and all pieces above it drop by one row. We prove that in the offline version of Tetris, it is NP-complete to maximize the number of cleared rows, maximize the number of tetrises (quadruples of rows simultaneously filled and cleared), minimize the maximum height of an occupied square, or maximize the number of pieces placed before the game ends. We furthermore show the extreme inapproximability of the first and last of these objectives to within a factor of p^(1-epsilon), when given a sequence of p pieces, and the inapproximability of the third objective to within a factor of (2 - epsilon), for any epsilon>0. Our results hold under several variations on the rules of Tetris, including different models of rotation, limitations on player agility, and restricted piece sets.",
        "published": "2002-10-21T18:32:39Z",
        "link": "http://arxiv.org/abs/cs/0210020v1",
        "categories": [
            "cs.CC",
            "cs.CG",
            "cs.DM",
            "F.1.3; F.2.2; G.2.1; K.8.0"
        ]
    },
    {
        "title": "Quantum Zero-Error Algorithms Cannot be Composed",
        "authors": [
            "Harry Buhrman",
            "Ronald de Wolf"
        ],
        "summary": "We exhibit two black-box problems, both of which have an efficient quantum algorithm with zero-error, yet whose composition does not have an efficient quantum algorithm with zero-error. This shows that quantum zero-error algorithms cannot be composed. In oracle terms, we give a relativized world where ZQP^{ZQP}\\=ZQP, while classically we always have ZPP^{ZPP}=ZPP.",
        "published": "2002-11-06T18:18:54Z",
        "link": "http://arxiv.org/abs/quant-ph/0211029v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Phase Transitions and all that",
        "authors": [
            "Gabriel Istrate"
        ],
        "summary": "The paper (as posted originally) contains several errors. It has been subsequently split into two papers, the corrected (and accepted for publication) versions appear in the archive as papers cs.CC/0503082 and cs.DM/0503083.",
        "published": "2002-11-12T21:56:15Z",
        "link": "http://arxiv.org/abs/cs/0211012v2",
        "categories": [
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Effective Strong Dimension, Algorithmic Information, and Computational   Complexity",
        "authors": [
            "Krishna B. Athreya",
            "John M. Hitchcock",
            "Jack H. Lutz",
            "Elvira Mayordomo"
        ],
        "summary": "The two most important notions of fractal dimension are {\\it Hausdorff dimension}, developed by Hausdorff (1919), and {\\it packing dimension}, developed by Tricot (1982).   Lutz (2000) has recently proven a simple characterization of Hausdorff dimension in terms of {\\it gales}, which are betting strategies that generalize martingales. Imposing various computability and complexity constraints on these gales produces a spectrum of effective versions of Hausdorff dimension.   In this paper we show that packing dimension can also be characterized in terms of gales. Moreover, even though the usual definition of packing dimension is considerably more complex than that of Hausdorff dimension, our gale characterization of packing dimension is an exact dual of -- and every bit as simple as -- the gale characterization of Hausdorff dimension.   Effectivizing our gale characterization of packing dimension produces a variety of {\\it effective strong dimensions}, which are exact duals of the effective dimensions mentioned above.   We develop the basic properties of effective strong dimensions and prove a number of results relating them to fundamental aspects of randomness, Kolmogorov complexity, prediction, Boolean circuit-size complexity, polynomial-time degrees, and data compression.",
        "published": "2002-11-21T04:46:02Z",
        "link": "http://arxiv.org/abs/cs/0211025v3",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "How long is a Proof? - A short note",
        "authors": [
            "A. G. Yaneff"
        ],
        "summary": "Withdrawn. Silly notion and out of context.",
        "published": "2002-11-21T10:26:44Z",
        "link": "http://arxiv.org/abs/cs/0211026v2",
        "categories": [
            "cs.CC",
            "F2.0"
        ]
    },
    {
        "title": "Redundancy in Logic I: CNF Propositional Formulae",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "A knowledge base is redundant if it contains parts that can be inferred from the rest of it. We study the problem of checking whether a CNF formula (a set of clauses) is redundant, that is, it contains clauses that can be derived from the other ones. Any CNF formula can be made irredundant by deleting some of its clauses: what results is an irredundant equivalent subset (I.E.S.) We study the complexity of some related problems: verification, checking existence of a I.E.S. with a given size, checking necessary and possible presence of clauses in I.E.S.'s, and uniqueness. We also consider the problem of redundancy with different definitions of equivalence.",
        "published": "2002-11-22T18:23:22Z",
        "link": "http://arxiv.org/abs/cs/0211031v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "I.2.4, F.1.3"
        ]
    },
    {
        "title": "Solution Bounds for a Hypothetical Polynomial Time Aproximation   Algorithm for the TSP",
        "authors": [
            "A. G. Yaneff"
        ],
        "summary": "Bounds for the optimal tour length for a hypothetical TSP algorithm are derived.",
        "published": "2002-11-25T08:24:37Z",
        "link": "http://arxiv.org/abs/cs/0211032v2",
        "categories": [
            "cs.CC",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Quantum Time-Space Tradeoffs for Sorting",
        "authors": [
            "Hartmut Klauck"
        ],
        "summary": "We investigate the complexity of sorting in the model of sequential quantum circuits. While it is known that in general a quantum algorithm based on comparisons alone cannot outperform classical sorting algorithms by more than a constant factor in time complexity, this is wrong in a space bounded setting. We observe that for all storage bounds n/\\log n\\ge S\\ge \\log^3 n, one can devise a quantum algorithm that sorts n numbers (using comparisons only) in time T=O(n^{3/2}\\log^{3/2} n/\\sqrt S). We then show the following lower bound on the time-space tradeoff for sorting $n$ numbers from a polynomial size range in a general sorting algorithm (not necessarily based on comparisons): TS=\\Omega(n^{3/2}). Hence for small values of S the upper bound is almost tight. Classically the time-space tradeoff for sorting is TS=\\Theta(n^2).",
        "published": "2002-11-26T20:23:08Z",
        "link": "http://arxiv.org/abs/quant-ph/0211174v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Typical random 3-SAT formulae and the satisfiability threshold",
        "authors": [
            "Olivier Dubois",
            "Yacine Boufkhad",
            "Jacques Mandler"
        ],
        "summary": "We present a new structural (or syntatic) approach for estimating the satisfiability threshold of random 3-SAT formulae. We show its efficiency in obtaining a jump from the previous upper bounds, lowering them to 4.506. The method combines well with other techniques, and also applies to other problems, such as the 3-colourability of random graphs.",
        "published": "2002-11-26T20:50:03Z",
        "link": "http://arxiv.org/abs/cs/0211036v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "G.2.1"
        ]
    },
    {
        "title": "Comparing EQP and MOD_{p^k}P using Polynomial Degree Lower Bounds",
        "authors": [
            "M. de Graaf",
            "P. Valiant"
        ],
        "summary": "We show that an oracle A that contains either 1/4 or 3/4 of all strings of length n can be used to separate EQP from the counting classes MOD_{p^k}P. Our proof makes use of the degree of a representing polynomial over the finite field of size p^k. We show a linear lower bound on the degree of this polynomial.   We also show an upper bound of O(n^{1/log_p m}) on the degree over the ring of integers modulo m, whenever m is a squarefree composite with largest prime factor p.",
        "published": "2002-11-27T10:00:44Z",
        "link": "http://arxiv.org/abs/quant-ph/0211179v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Mathematical foundations of modern cryptography: computational   complexity perspective",
        "authors": [
            "Shafi Goldwasser"
        ],
        "summary": "Theoretical computer science has found fertile ground in many areas of mathematics. The approach has been to consider classical problems through the prism of computational complexity, where the number of basic computational steps taken to solve a problem is the crucial qualitative parameter. This new approach has led to a sequence of advances, in setting and solving new mathematical challenges as well as in harnessing discrete mathematics to the task of solving real-world problems.   In this talk, I will survey the development of modern cryptography -- the mathematics behind secret communications and protocols -- in this light. I will describe the complexity theoretic foundations underlying the cryptographic tasks of encryption, pseudo-randomness number generators and functions, zero knowledge interactive proofs, and multi-party secure protocols. I will attempt to highlight the paradigms and proof techniques which unify these foundations, and which have made their way into the mainstream of complexity theory.",
        "published": "2002-12-01T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/0212055v1",
        "categories": [
            "cs.CR",
            "cs.CC",
            "68Qxx, 11xx"
        ]
    },
    {
        "title": "On the Work of Madhu Sudan: the 2002 Nevalinna Prize Winner",
        "authors": [
            "Shafi Goldwasser"
        ],
        "summary": "Madhu Sudan's work spans many areas of computer science theory including computational complexity theory, the design of efficient algorithms, algorithmic coding theory, and the theory of program checking and correcting.   Two results of Sudan stand out in the impact they have had on the mathematics of computation. The first work shows a probabilistic characterization of the class NP -- those sets for which short and easily checkable proofs of membership exist, and demonstrates consequences of this characterization to classifying the complexity of approximation problems. The second work shows a polynomial time algorithm for list decoding the Reed Solomon error correcting codes.   This short note will be devoted to describing Sudan's work on probabilistically checkable proofs -- the so called {\\it PCP theorem} and its implications.",
        "published": "2002-12-01T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/0212056v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A Generalization of Chaitin's Halting Probability Ωand Halting   Self-Similar Sets",
        "authors": [
            "Kohtaro Tadaki"
        ],
        "summary": "We generalize the concept of randomness in an infinite binary sequence in order to characterize the degree of randomness by a real number D>0. Chaitin's halting probability \\Omega is generalized to \\Omega^D whose degree of randomness is precisely D. On the basis of this generalization, we consider the degree of randomness of each point in Euclidean space through its base-two expansion. It is then shown that the maximum value of such a degree of randomness provides the Hausdorff dimension of a self-similar set that is computable in a certain sense. The class of such self-similar sets includes familiar fractal sets such as the Cantor set, von Koch curve, and Sierpinski gasket. Knowledge of the property of \\Omega^D allows us to show that the self-similar subset of [0,1] defined by the halting set of a universal algorithm has a Hausdorff dimension of one.",
        "published": "2002-12-02T06:54:47Z",
        "link": "http://arxiv.org/abs/nlin/0212001v1",
        "categories": [
            "nlin.CD",
            "cs.CC"
        ]
    },
    {
        "title": "Traveling Salesmen in the Presence of Competition",
        "authors": [
            "Sandor P. Fekete",
            "Rudolf Fleischer",
            "Aviezri Fraenkel",
            "Matthias Schmitt"
        ],
        "summary": "We propose the ``Competing Salesmen Problem'' (CSP), a 2-player competitive version of the classical Traveling Salesman Problem. This problem arises when considering two competing salesmen instead of just one. The concern for a shortest tour is replaced by the necessity to reach any of the customers before the opponent does. In particular, we consider the situation where players take turns, moving along one edge at a time within a graph G=(V,E). The set of customers is given by a subset V_C V of the vertices. At any given time, both players know of their opponent's position. A player wins if he is able to reach a majority of the vertices in V_C before the opponent does. We prove that the CSP is PSPACE-complete, even if the graph is bipartite, and both players start at distance 2 from each other. We show that the starting player may lose the game, even if both players start from the same vertex. For bipartite graphs, we show that the starting player always can avoid a loss. We also show that the second player can avoid to lose by more than one customer, when play takes place on a graph that is a tree T, and V_C consists of leaves of T. For the case where T is a star and V_C consists of n leaves of T, we give a simple and fast strategy which is optimal for both players. If V_C consists not only of leaves, the situation is more involved.",
        "published": "2002-12-03T17:42:08Z",
        "link": "http://arxiv.org/abs/cs/0212001v1",
        "categories": [
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Survey propagation: an algorithm for satisfiability",
        "authors": [
            "A. Braunstein",
            "M. Mezard",
            "R. Zecchina"
        ],
        "summary": "We study the satisfiability of randomly generated formulas formed by $M$ clauses of exactly $K$ literals over $N$ Boolean variables. For a given value of $N$ the problem is known to be most difficult with $\\alpha=M/N$ close to the experimental threshold $\\alpha_c$ separating the region where almost all formulas are SAT from the region where all formulas are UNSAT. Recent results from a statistical physics analysis suggest that the difficulty is related to the existence of a clustering phenomenon of the solutions when $\\alpha$ is close to (but smaller than) $\\alpha_c$. We introduce a new type of message passing algorithm which allows to find efficiently a satisfiable assignment of the variables in the difficult region. This algorithm is iterative and composed of two main parts. The first is a message-passing procedure which generalizes the usual methods like Sum-Product or Belief Propagation: it passes messages that are surveys over clusters of the ordinary messages. The second part uses the detailed probabilistic information obtained from the surveys in order to fix variables and simplify the problem. Eventually, the simplified problem that remains is solved by a conventional heuristic.",
        "published": "2002-12-04T17:08:29Z",
        "link": "http://arxiv.org/abs/cs/0212002v4",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "G.3"
        ]
    },
    {
        "title": "On the survey-propagation equations for the random K-satisfiability   problem",
        "authors": [
            "Giorgio Parisi"
        ],
        "summary": "In this note we study the existence of a solution to the survey-propagation equations for the random K-satisfiability problem for a given instance. We conjecture that when the number of variables goes to infinity, the solution of these equations for a given instance can be approximated by the solution of the corresponding equations on an infinite tree. We conjecture (and we bring numerical evidence) that the survey-propagation equations on the infinite tree have an unique solution in the suitable range of parameters.",
        "published": "2002-12-07T22:01:27Z",
        "link": "http://arxiv.org/abs/cs/0212009v1",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "G.3, G.2.1"
        ]
    },
    {
        "title": "Complexity of the Exact Domatic Number Problem and of the Exact Conveyor   Flow Shop Problem",
        "authors": [
            "Tobias Riege",
            "Jörg Rothe"
        ],
        "summary": "We prove that the exact versions of the domatic number problem are complete for the levels of the boolean hierarchy over NP. The domatic number problem, which arises in the area of computer networks, is the problem of partitioning a given graph into a maximum number of disjoint dominating sets. This number is called the domatic number of the graph. We prove that the problem of determining whether or not the domatic number of a given graph is {\\em exactly} one of k given values is complete for the 2k-th level of the boolean hierarchy over NP. In particular, for k = 1, it is DP-complete to determine whether or not the domatic number of a given graph equals exactly a given integer. Note that DP is the second level of the boolean hierarchy over NP. We obtain similar results for the exact versions of generalized dominating set problems and of the conveyor flow shop problem. Our reductions apply Wagner's conditions sufficient to prove hardness for the levels of the boolean hierarchy over NP.",
        "published": "2002-12-09T19:46:02Z",
        "link": "http://arxiv.org/abs/cs/0212016v3",
        "categories": [
            "cs.CC",
            "F.2.2;F.1.3"
        ]
    },
    {
        "title": "Real numbers having ultimately periodic representations in abstract   numeration systems",
        "authors": [
            "P. Lecomte",
            "M. Rigo"
        ],
        "summary": "Using a genealogically ordered infinite regular language, we know how to represent an interval of R. Numbers having an ultimately periodic representation play a special role in classical numeration systems. The aim of this paper is to characterize the numbers having an ultimately periodic representation in generalized systems built on a regular language. The syntactical properties of these words are also investigated. Finally, we show the equivalence of the classical \"theta\"-expansions with our generalized representations in some special case related to a Pisot number \"theta\".",
        "published": "2002-12-10T12:46:25Z",
        "link": "http://arxiv.org/abs/cs/0212018v1",
        "categories": [
            "cs.CC",
            "cs.CL",
            "F.4.1; F.4.3"
        ]
    },
    {
        "title": "Upper bound by Kolmogorov complexity for the probability in computable   POVM measurement",
        "authors": [
            "Kohtaro Tadaki"
        ],
        "summary": "We apply algorithmic information theory to quantum mechanics in order to shed light on an algorithmic structure which inheres in quantum mechanics.   There are two equivalent ways to define the (classical) Kolmogorov complexity K(s) of a given classical finite binary string s. In the standard way, K(s) is defined as the length of the shortest input string for the universal self-delimiting Turing machine to output s. In the other way, we first introduce the so-called universal probability m, and then define K(s) as -log_2 m(s) without using the concept of program-size. We generalize the universal probability to a matrix-valued function, and identify this function with a POVM (positive operator-valued measure). On the basis of this identification, we study a computable POVM measurement with countable measurement outcomes performed upon a finite dimensional quantum system. We show that, up to a multiplicative constant, 2^{-K(s)} is the upper bound for the probability of each measurement outcome s in such a POVM measurement. In what follows, the upper bound 2^{-K(s)} is shown to be optimal in a certain sense.",
        "published": "2002-12-11T17:35:01Z",
        "link": "http://arxiv.org/abs/quant-ph/0212071v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Constraint Satisfaction by Survey Propagation",
        "authors": [
            "A. Braunstein",
            "M. Mezard",
            "M. Weigt",
            "R. Zecchina"
        ],
        "summary": "Survey Propagation is an algorithm designed for solving typical instances of random constraint satisfiability problems. It has been successfully tested on random 3-SAT and random $G(n,\\frac{c}{n})$ graph 3-coloring, in the hard region of the parameter space. Here we provide a generic formalism which applies to a wide class of discrete Constraint Satisfaction Problems.",
        "published": "2002-12-18T17:26:33Z",
        "link": "http://arxiv.org/abs/cond-mat/0212451v3",
        "categories": [
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "On local equilibrium equations for clustering states",
        "authors": [
            "Giorgio Parisi"
        ],
        "summary": "In this note we show that local equilibrium equations (the generalization of the TAP equations or of the belief propagation equations) do have solutions in the colorable phase of the coloring problem. The same results extend to other optimization problems where the solutions has cost zero (e.g. K-satisfiability). On a random graph the solutions of the local equilibrium equations are associated to clusters of configurations (clustering states). On a random graph the local equilibrium equations have solutions almost everywhere in the uncolored phase; in this case we have to introduce the concept quasi-solution of the local equilibrium equations.",
        "published": "2002-12-18T22:14:39Z",
        "link": "http://arxiv.org/abs/cs/0212047v2",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "cs.DS",
            "G.3, G.2.1"
        ]
    },
    {
        "title": "Computing Preferred Answer Sets by Meta-Interpretation in Answer Set   Programming",
        "authors": [
            "Thomas Eiter",
            "Wolfgang Faber",
            "Nicola Leone",
            "Gerald Pfeifer"
        ],
        "summary": "Most recently, Answer Set Programming (ASP) is attracting interest as a new paradigm for problem solving. An important aspect which needs to be supported is the handling of preferences between rules, for which several approaches have been presented. In this paper, we consider the problem of implementing preference handling approaches by means of meta-interpreters in Answer Set Programming. In particular, we consider the preferred answer set approaches by Brewka and Eiter, by Delgrande, Schaub and Tompits, and by Wang, Zhou and Lin. We present suitable meta-interpreters for these semantics using DLV, which is an efficient engine for ASP. Moreover, we also present a meta-interpreter for the weakly preferred answer set approach by Brewka and Eiter, which uses the weak constraint feature of DLV as a tool for expressing and solving an underlying optimization problem. We also consider advanced meta-interpreters, which make use of graph-based characterizations and often allow for more efficient computations. Our approach shows the suitability of ASP in general and of DLV in particular for fast prototyping. This can be fruitfully exploited for experimenting with new languages and knowledge-representation formalisms.",
        "published": "2002-01-16T20:00:46Z",
        "link": "http://arxiv.org/abs/cs/0201013v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6; I.2.3; I.2.4"
        ]
    },
    {
        "title": "A Modal Logic Framework for Multi-agent Belief Fusion",
        "authors": [
            "Churn-Jung Liau"
        ],
        "summary": "This paper is aimed at providing a uniform framework for reasoning about beliefs of multiple agents and their fusion. In the first part of the paper, we develop logics for reasoning about cautiously merged beliefs of agents with different degrees of reliability. The logics are obtained by combining the multi-agent epistemic logic and multi-sources reasoning systems. Every ordering for the reliability of the agents is represented by a modal operator, so we can reason with the merged results under different situations. The fusion is cautious in the sense that if an agent's belief is in conflict with those of higher priorities, then his belief is completely discarded from the merged result. We consider two strategies for the cautious merging of beliefs. In the first one, if inconsistency occurs at some level, then all beliefs at the lower levels are discarded simultaneously, so it is called level cutting strategy. For the second one, only the level at which the inconsistency occurs is skipped, so it is called level skipping strategy. The formal semantics and axiomatic systems for these two strategies are presented. In the second part, we extend the logics both syntactically and semantically to cover some more sophisticated belief fusion and revision operators. While most existing approaches treat belief fusion operators as meta-level constructs, these operators are directly incorporated into our object logic language. Thus it is possible to reason not only with the merged results but also about the fusion process in our logics. The relationship of our extended logics with the conditional logics of belief revision is also discussed.",
        "published": "2002-01-23T02:43:01Z",
        "link": "http://arxiv.org/abs/cs/0201020v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4;F.4.1"
        ]
    },
    {
        "title": "A Refinement Calculus for Logic Programs",
        "authors": [
            "Ian Hayes",
            "Robert Colvin",
            "David Hemer",
            "Paul Strooper",
            "Ray Nickson"
        ],
        "summary": "Existing refinement calculi provide frameworks for the stepwise development of imperative programs from specifications. This paper presents a refinement calculus for deriving logic programs. The calculus contains a wide-spectrum logic programming language, including executable constructs such as sequential conjunction, disjunction, and existential quantification, as well as specification constructs such as general predicates, assumptions and universal quantification. A declarative semantics is defined for this wide-spectrum language based on executions. Executions are partial functions from states to states, where a state is represented as a set of bindings. The semantics is used to define the meaning of programs and specifications, including parameters and recursion. To complete the calculus, a notion of correctness-preserving refinement over programs in the wide-spectrum language is defined and refinement laws for developing programs are introduced. The refinement calculus is illustrated using example derivations and prototype tool support is discussed.",
        "published": "2002-02-04T01:20:38Z",
        "link": "http://arxiv.org/abs/cs/0202002v1",
        "categories": [
            "cs.SE",
            "cs.LO",
            "F.3.1; D.1.6"
        ]
    },
    {
        "title": "Approximate Computation of Reach Sets in Hybrid Systems",
        "authors": [
            "D. Ravi",
            "R. K. Shyamasundar"
        ],
        "summary": "One of the most important problems in hybrid systems is the {\\em reachability problem}. The reachability problem has been shown to be undecidable even for a subclass of {\\em linear} hybrid systems. In view of this, the main focus in the area of hybrid systems has been to find {\\em effective} semi-decision procedures for this problem. Such an algorithmic approach involves finding methods of computation and representation of reach sets of the continuous variables within a discrete state of a hybrid system. In this paper, after presenting a brief introduction to hybrid systems and reachability problem, we propose a computational method for obtaining the reach sets of continuous variables in a hybrid system. In addition to this, we also describe a new algorithm to over-approximate with polyhedra the reach sets of the continuous variables with linear dynamics and polyhedral initial set. We illustrate these algorithms with typical interesting examples.",
        "published": "2002-02-07T12:50:13Z",
        "link": "http://arxiv.org/abs/cs/0202006v1",
        "categories": [
            "cs.LO",
            "cs.SE",
            "F3, D2.2"
        ]
    },
    {
        "title": "Nonmonotonic Logics and Semantics",
        "authors": [
            "Daniel Lehmann"
        ],
        "summary": "Tarski gave a general semantics for deductive reasoning: a formula a may be deduced from a set A of formulas iff a holds in all models in which each of the elements of A holds. A more liberal semantics has been considered: a formula a may be deduced from a set A of formulas iff a holds in all of the \"preferred\" models in which all the elements of A hold. Shoham proposed that the notion of \"preferred\" models be defined by a partial ordering on the models of the underlying language. A more general semantics is described in this paper, based on a set of natural properties of choice functions. This semantics is here shown to be equivalent to a semantics based on comparing the relative \"importance\" of sets of models, by what amounts to a qualitative probability measure. The consequence operations defined by the equivalent semantics are then characterized by a weakening of Tarski's properties in which the monotonicity requirement is replaced by three weaker conditions. Classical propositional connectives are characterized by natural introduction-elimination rules in a nonmonotonic setting. Even in the nonmonotonic setting, one obtains classical propositional logic, thus showing that monotonicity is not required to justify classical propositional connectives.",
        "published": "2002-02-15T12:49:11Z",
        "link": "http://arxiv.org/abs/cs/0202018v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "math.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Equivalence and Isomorphism for Boolean Constraint Satisfaction",
        "authors": [
            "E. Boehler",
            "E. Hemaspaandra",
            "Steffen Reith",
            "Heribert Vollmer"
        ],
        "summary": "A Boolean constraint satisfaction instance is a conjunction of constraint applications, where the allowed constraints are drawn from a fixed set B of Boolean functions. We consider the problem of determining whether two given constraint satisfaction instances are equivalent and prove a Dichotomy Theorem by showing that for all sets C of allowed constraints, this problem is either polynomial-time solvable or coNP-complete, and we give a simple criterion to determine which case holds.   A more general problem addressed in this paper is the isomorphism problem, the problem of determining whether there exists a renaming of the variables that makes two given constraint satisfaction instances equivalent in the above sense. We prove that this problem is coNP-hard if the corresponding equivalence problem is coNP-hard, and polynomial-time many-one reducible to the graph isomorphism problem in all other cases.",
        "published": "2002-02-25T15:38:36Z",
        "link": "http://arxiv.org/abs/cs/0202036v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "Composing Programs in a Rewriting Logic for Declarative Programming",
        "authors": [
            "Juan M. Molina",
            "Ernesto Pimentel"
        ],
        "summary": "Constructor-Based Conditional Rewriting Logic is a general framework for integrating first-order functional and logic programming which gives an algebraic semantics for non-deterministic functional-logic programs. In the context of this formalism, we introduce a simple notion of program module as an open program which can be extended together with several mechanisms to combine them. These mechanisms are based on a reduced set of operations. However, the high expressiveness of these operations enable us to model typical constructs for program modularization like hiding, export/import, genericity/instantiation, and inheritance in a simple way. We also deal with the semantic aspects of the proposal by introducing an immediate consequence operator, and studying several alternative semantics for a program module, based on this operator, in the line of logic programming: the operator itself, its least fixpoint (the least model of the module), the set of its pre-fixpoints (term models of the module), and some other variations in order to find a compositional and fully abstract semantics wrt the set of operations and a natural notion of observability.",
        "published": "2002-03-04T19:02:05Z",
        "link": "http://arxiv.org/abs/cs/0203006v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.2;D.3.3;F.3.2;F.3.3;F.4.1"
        ]
    },
    {
        "title": "SPINning Parallel Systems Software",
        "authors": [
            "O. S. Matlin",
            "E. Lusk",
            "W. McCune"
        ],
        "summary": "We describe our experiences in using SPIN to verify parts of the Multi Purpose Daemon (MPD) parallel process management system. MPD is a distributed collection of processes connected by Unix network sockets. MPD is dynamic: processes and connections among them are created and destroyed as MPD is initialized, runs user processes, recovers from faults, and terminates. This dynamic nature is easily expressible in the SPIN/PROMELA framework but poses performance and scalability challenges. We present here the results of expressing some of the parallel algorithms of MPD and executing both simulation and verification runs with SPIN.",
        "published": "2002-03-06T18:15:03Z",
        "link": "http://arxiv.org/abs/cs/0203009v1",
        "categories": [
            "cs.LO",
            "cs.DC",
            "F.3.1; D.1.3"
        ]
    },
    {
        "title": "Computing the Noncomputable",
        "authors": [
            "Tien D. Kieu"
        ],
        "summary": "We explore in the framework of Quantum Computation the notion of computability, which holds a central position in Mathematics and Theoretical Computer Science. A quantum algorithm that exploits the quantum adiabatic processes is considered for the Hilbert's tenth problem, which is equivalent to the Turing halting problem and known to be mathematically noncomputable. Generalised quantum algorithms are also considered for some other mathematical noncomputables in the same and of different noncomputability classes. The key element of all these algorithms is the measurability of both the values of physical observables and of the quantum-mechanical probability distributions for these values. It is argued that computability, and thus the limits of Mathematics, ought to be determined not solely by Mathematics itself but also by physical principles.",
        "published": "2002-03-07T12:02:40Z",
        "link": "http://arxiv.org/abs/quant-ph/0203034v4",
        "categories": [
            "quant-ph",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "Representing and Aggregating Conflicting Beliefs",
        "authors": [
            "Pedrito Maynard-Reid II",
            "Daniel Lehmann"
        ],
        "summary": "We consider the two-fold problem of representing collective beliefs and aggregating these beliefs. We propose modular, transitive relations for collective beliefs. They allow us to represent conflicting opinions and they have a clear semantics. We compare them with the quasi-transitive relations often used in Social Choice. Then, we describe a way to construct the belief state of an agent informed by a set of sources of varying degrees of reliability. This construction circumvents Arrow's Impossibility Theorem in a satisfactory manner. Finally, we give a simple set-theory-based operator for combining the information of multiple agents. We show that this operator satisfies the desirable invariants of idempotence, commutativity, and associativity, and, thus, is well-behaved when iterated, and we describe a computationally effective way of computing the resulting belief state.",
        "published": "2002-03-11T23:02:56Z",
        "link": "http://arxiv.org/abs/cs/0203013v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; I.2.11"
        ]
    },
    {
        "title": "Towards Experimental Nanosound Using Almost Disjoint Set Theory",
        "authors": [
            "Cameron L Jones"
        ],
        "summary": "Music composition using digital audio sequence editors is increasingly performed in a visual workspace where sound complexes are built from discrete sound objects, called gestures that are arranged in time and space to generate a continuous composition. The visual workspace, common to most industry standard audio loop sequencing software, is premised on the arrangement of gestures defined with geometric shape properties. Here, one aspect of fractal set theory was validated using audio-frequency sets to evaluate self-affine scaling behavior when new sound complexes are built through union and intersection operations on discrete musical gestures. Results showed that intersection of two sets revealed lower complexity compared with the union operator, meaning that the intersection of two sound gestures is an almost disjoint set, and in accord with formal logic. These results are also discussed with reference to fuzzy sets, cellular automata, nanotechnology and self-organization to further explore the link between sequenced notation and complexity.",
        "published": "2002-03-12T09:21:39Z",
        "link": "http://arxiv.org/abs/cs/0203015v1",
        "categories": [
            "cs.SD",
            "cs.LO",
            "H.5.5"
        ]
    },
    {
        "title": "Making Abstract Domains Condensing",
        "authors": [
            "R. Giacobazzi",
            "F. Ranzato",
            "F. Scozzari"
        ],
        "summary": "In this paper we show that reversible analysis of logic languages by abstract interpretation can be performed without loss of precision by systematically refining abstract domains. The idea is to include semantic structures into abstract domains in such a way that the refined abstract domain becomes rich enough to allow approximate bottom-up and top-down semantics to agree. These domains are known as condensing abstract domains. Substantially, an abstract domain is condensing if goal-driven and goal-independent analyses agree, namely no loss of precision is introduced by approximating queries in a goal-independent analysis. We prove that condensation is an abstract domain property and that the problem of making an abstract domain condensing boils down to the problem of making the domain complete with respect to unification. In a general abstract interpretation setting we show that when concrete domains and operations give rise to quantales, i.e. models of propositional linear logic, objects in a complete refined abstract domain can be explicitly characterized by linear logic-based formulations. This is the case for abstract domains for logic program analysis approximating computed answer substitutions where unification plays the role of multiplicative conjunction in a quantale of idempotent substitutions. Condensing abstract domains can therefore be systematically derived by minimally extending any, generally non-condensing domain, by a simple domain refinement operator.",
        "published": "2002-04-09T13:22:00Z",
        "link": "http://arxiv.org/abs/cs/0204016v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.3.1; D.3.2; F.3.2"
        ]
    },
    {
        "title": "A Dynamic Approach to Characterizing Termination of General Logic   Programs",
        "authors": [
            "Yi-Dong Shen",
            "Jia-Huai You",
            "Li-Yan Yuan",
            "Samuel S. P. Shen",
            "Qiang Yang"
        ],
        "summary": "We present a new characterization of termination of general logic programs. Most existing termination analysis approaches rely on some static information about the structure of the source code of a logic program, such as modes/types, norms/level mappings, models/interargument relations, and the like. We propose a dynamic approach which employs some key dynamic features of an infinite (generalized) SLDNF-derivation, such as repetition of selected subgoals and recursive increase in term size. We also introduce a new formulation of SLDNF-trees, called generalized SLDNF-trees. Generalized SLDNF-trees deal with negative subgoals in the same way as Prolog and exist for any general logic programs.",
        "published": "2002-04-12T18:01:32Z",
        "link": "http://arxiv.org/abs/cs/0204031v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6;D.1.2;F.4.1"
        ]
    },
    {
        "title": "Precongruence Formats for Decorated Trace Semantics",
        "authors": [
            "B. Bloom",
            "W. J. Fokkink",
            "R. J. van Glabbeek"
        ],
        "summary": "This paper explores the connection between semantic equivalences and preorders for concrete sequential processes, represented by means of labelled transition systems, and formats of transition system specifications using Plotkin's structural approach. For several preorders in the linear time - branching time spectrum a format is given, as general as possible, such that this preorder is a precongruence for all operators specifiable in that format. The formats are derived using the modal characterizations of the corresponding preorders.",
        "published": "2002-04-17T00:35:04Z",
        "link": "http://arxiv.org/abs/cs/0204039v1",
        "categories": [
            "cs.LO",
            "D.3.1; F.1.2; F.3.2"
        ]
    },
    {
        "title": "Some applications of logic to feasibility in higher types",
        "authors": [
            "Aleksandar Ignjatovic",
            "Arun Sharma"
        ],
        "summary": "In this paper we demonstrate that the class of basic feasible functionals has recursion theoretic properties which naturally generalize the corresponding properties of the class of feasible functions. We also improve the Kapron - Cook result on mashine representation of basic feasible functionals. Our proofs are based on essential applications of logic. We introduce a weak fragment of second order arithmetic with second order variables ranging over functions from N into N which suitably characterizes basic feasible functionals, and show that it is a useful tool for investigating the properties of basic feasible functionals. In particular, we provide an example how one can extract feasible \"programs\" from mathematical proofs which use non-feasible functionals (like second order polynomials).",
        "published": "2002-04-22T06:58:56Z",
        "link": "http://arxiv.org/abs/cs/0204045v1",
        "categories": [
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "The prospects for mathematical logic in the twenty-first century",
        "authors": [
            "Samuel R. Buss",
            "Alexander S. Kechris",
            "Anand Pillay",
            "Richard A. Shore"
        ],
        "summary": "The four authors present their speculations about the future developments of mathematical logic in the twenty-first century. The areas of recursion theory, proof theory and logic for computer science, model theory, and set theory are discussed independently.",
        "published": "2002-05-03T22:36:23Z",
        "link": "http://arxiv.org/abs/cs/0205003v1",
        "categories": [
            "cs.LO",
            "A.1;F.0;I.2.0"
        ]
    },
    {
        "title": "Computing stable models: worst-case performance estimates",
        "authors": [
            "Zbigniew Lonc",
            "Miroslaw Truszczynski"
        ],
        "summary": "We study algorithms for computing stable models of propositional logic programs and derive estimates on their worst-case performance that are asymptotically better than the trivial bound of O(m 2^n), where m is the size of an input program and n is the number of its atoms. For instance, for programs, whose clauses consist of at most two literals (counting the head) we design an algorithm to compute stable models that works in time O(m\\times 1.44225^n). We present similar results for several broader classes of programs, as well.",
        "published": "2002-05-11T20:27:20Z",
        "link": "http://arxiv.org/abs/cs/0205013v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6;I.2.4"
        ]
    },
    {
        "title": "A Spectrum of Applications of Automated Reasoning",
        "authors": [
            "Larry Wos"
        ],
        "summary": "The likelihood of an automated reasoning program being of substantial assistance for a wide spectrum of applications rests with the nature of the options and parameters it offers on which to base needed strategies and methodologies. This article focuses on such a spectrum, featuring W. McCune's program OTTER, discussing widely varied successes in answering open questions, and touching on some of the strategies and methodologies that played a key role. The applications include finding a first proof, discovering single axioms, locating improved axiom systems, and simplifying existing proofs. The last application is directly pertinent to the recently found (by R. Thiele) Hilbert's twenty-fourth problem--which is extremely amenable to attack with the appropriate automated reasoning program--a problem concerned with proof simplification. The methodologies include those for seeking shorter proofs and for finding proofs that avoid unwanted lemmas or classes of term, a specific option for seeking proofs with smaller equational or formula complexity, and a different option to address the variable richness of a proof. The type of proof one obtains with the use of OTTER is Hilbert-style axiomatic, including details that permit one sometimes to gain new insights. We include questions still open and challenges that merit consideration.",
        "published": "2002-05-30T21:20:52Z",
        "link": "http://arxiv.org/abs/cs/0205078v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; F.4.1"
        ]
    },
    {
        "title": "Characterization of Strongly Equivalent Logic Programs in Intermediate   Logics",
        "authors": [
            "Dick de Jongh",
            "Lex Hendriks"
        ],
        "summary": "The non-classical, nonmonotonic inference relation associated with the answer set semantics for logic programs gives rise to a relationship of 'strong equivalence' between logical programs that can be verified in 3-valued Goedel logic, G3, the strongest non-classical intermediate propositional logic (Lifschitz, Pearce and Valverde, 2001). In this paper we will show that KC (the logic obtained by adding axiom ~A v ~~A to intuitionistic logic), is the weakest intermediate logic for which strongly equivalent logic programs, in a language allowing negations, are logically equivalent.",
        "published": "2002-06-03T14:48:41Z",
        "link": "http://arxiv.org/abs/cs/0206005v1",
        "categories": [
            "cs.LO",
            "D.1.6"
        ]
    },
    {
        "title": "The Fastest and Shortest Algorithm for All Well-Defined Problems",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "An algorithm $M$ is described that solves any well-defined problem $p$ as quickly as the fastest algorithm computing a solution to $p$, save for a factor of 5 and low-order additive terms. $M$ optimally distributes resources between the execution of provably correct $p$-solving programs and an enumeration of all proofs, including relevant proofs of program correctness and of time bounds on program runtimes. $M$ avoids Blum's speed-up theorem by ignoring programs without correctness proof. $M$ has broader applicability and can be faster than Levin's universal search, the fastest method for inverting functions save for a large multiplicative constant. An extension of Kolmogorov complexity and two novel natural measures of function complexity are used to show that the most efficient program computing some function $f$ is also among the shortest programs provably computing $f$.",
        "published": "2002-06-14T11:04:44Z",
        "link": "http://arxiv.org/abs/cs/0206022v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.2.3"
        ]
    },
    {
        "title": "Sierpinski Gaskets for Logic Functions Representation",
        "authors": [
            "Denis V. Popel",
            "Anita Dani"
        ],
        "summary": "This paper introduces a new approach to represent logic functions in the form of Sierpinski Gaskets. The structure of the gasket allows to manipulate with the corresponding logic expression using recursive essence of fractals. Thus, the Sierpinski gasket's pattern has myriad useful properties which can enhance practical features of other graphic representations like decision diagrams. We have covered possible applications of Sierpinski gaskets in logic design and justified our assumptions in logic function minimization (both Boolean and multiple-valued cases). The experimental results on benchmarks with advances in the novel structure are considered as well.",
        "published": "2002-06-15T15:44:32Z",
        "link": "http://arxiv.org/abs/cs/0206024v1",
        "categories": [
            "cs.LO",
            "cs.DM",
            "B.6.3"
        ]
    },
    {
        "title": "Improving Web Database Access Using Decision Diagrams",
        "authors": [
            "Denis V. Popel",
            "Nawar Al-Hakeem"
        ],
        "summary": "In some areas of management and commerce, especially in Electronic commerce (E-commerce), that are accelerated by advances in Web technologies, it is essential to support the decision making process using formal methods. Among the problems of E-commerce applications: reducing the time of data access so that huge databases can be searched quickly; decreasing the cost of database design ... etc. We present the application of Decision Diagrams design using Information Theory approach to improve database access speeds. We show that such utilization provides systematic and visual ways of applying Decision Making methods to simplify complex Web engineering problems.",
        "published": "2002-07-04T04:09:50Z",
        "link": "http://arxiv.org/abs/cs/0207011v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "E.2"
        ]
    },
    {
        "title": "Intuitions and the modelling of defeasible reasoning: some case studies",
        "authors": [
            "Henry Prakken"
        ],
        "summary": "The purpose of this paper is to address some criticisms recently raised by John Horty in two articles against the validity of two commonly accepted defeasible reasoning patterns, viz. reinstatement and floating conclusions. I shall argue that Horty's counterexamples, although they significantly raise our understanding of these reasoning patterns, do not show their invalidity. Some of them reflect patterns which, if made explicit in the formalisation, avoid the unwanted inference without having to give up the criticised inference principles. Other examples seem to involve hidden assumptions about the specific problem which, if made explicit, are nothing but extra information that defeat the defeasible inference. These considerations will be put in a wider perspective by reflecting on the nature of defeasible reasoning principles as principles of justified acceptance rather than `real' logical inference.",
        "published": "2002-07-09T14:16:18Z",
        "link": "http://arxiv.org/abs/cs/0207031v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Alternative Characterizations for Strong Equivalence of Logic Programs",
        "authors": [
            "Pedro Cabalar"
        ],
        "summary": "In this work we present additional results related to the property of strong equivalence of logic programs. This property asserts that two programs share the same set of stable models, even under the addition of new rules. As shown in a recent work by Lifschitz, Pearce and Valverde, strong equivalence can be simply reduced to equivalence in the logic of Here-and-There (HT). In this paper we provide two alternatives respectively based on classical logic and 3-valued logic. The former is applicable to general rules, but not for nested expressions, whereas the latter is applicable for nested expressions but, when moving to an unrestricted syntax, it generally yields different results from HT.",
        "published": "2002-07-09T15:05:32Z",
        "link": "http://arxiv.org/abs/cs/0207032v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Some logics of belief and disbelief",
        "authors": [
            "Samir Chopra",
            "Johannes Heidema",
            "Thomas Meyer"
        ],
        "summary": "The introduction of explicit notions of rejection, or disbelief, into logics for knowledge representation can be justified in a number of ways. Motivations range from the need for versions of negation weaker than classical negation, to the explicit recording of classic belief contraction operations in the area of belief change, and the additional levels of expressivity obtained from an extended version of belief change which includes disbelief contraction. In this paper we present four logics of disbelief which address some or all of these intuitions. Soundness and completeness results are supplied and the logics are compared with respect to applicability and utility.",
        "published": "2002-07-10T02:16:41Z",
        "link": "http://arxiv.org/abs/cs/0207037v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Iterated revision and the axiom of recovery: a unified treatment via   epistemic states",
        "authors": [
            "Samir Chopra",
            "Aditya Ghose",
            "Thomas Meyer"
        ],
        "summary": "The axiom of recovery, while capturing a central intuition regarding belief change, has been the source of much controversy. We argue briefly against putative counterexamples to the axiom--while agreeing that some of their insight deserves to be preserved--and present additional recovery-like axioms in a framework that uses epistemic states, which encode preferences, as the object of revisions. This provides a framework in which iterated revision becomes possible and makes explicit the connection between iterated belief change and the axiom of recovery. We provide a representation theorem that connects the semantic conditions that we impose on iterated revision and the additional syntactical properties mentioned. We also show some interesting similarities between our framework and that of Darwiche-Pearl. In particular, we show that the intuitions underlying the controversial (C2) postulate are captured by the recovery axiom and our recovery-like postulates (the latter can be seen as weakenings of (C2).",
        "published": "2002-07-10T02:31:19Z",
        "link": "http://arxiv.org/abs/cs/0207038v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Well-Founded Argumentation Semantics for Extended Logic Programming",
        "authors": [
            "Ralf Schweimeier",
            "Michael Schroeder"
        ],
        "summary": "This paper defines an argumentation semantics for extended logic programming and shows its equivalence to the well-founded semantics with explicit negation. We set up a general framework in which we extensively compare this semantics to other argumentation semantics, including those of Dung, and Prakken and Sartor. We present a general dialectical proof theory for these argumentation semantics.",
        "published": "2002-07-10T10:22:12Z",
        "link": "http://arxiv.org/abs/cs/0207040v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6.; F.3.2.; I.2.3.; I.2.4"
        ]
    },
    {
        "title": "Physical Traces: Quantum vs. Classical Information Processing",
        "authors": [
            "Samson Abramsky",
            "Bob Coecke"
        ],
        "summary": "Within the Geometry of Interaction (GoI) paradigm, we present a setting that enables qualitative differences between classical and quantum processes to be explored. The key construction is the physical interpretation/realization of the traced monoidal categories of finite-dimensional vector spaces with tensor product as monoidal structure and of finite sets and relations with Cartesian product as monoidal structure, both of them providing a so-called wave-style GoI. The developments in this paper reveal that envisioning state update due to quantum measurement as a process provides a powerful tool for developing high-level approaches to quantum information processing.",
        "published": "2002-07-14T15:50:00Z",
        "link": "http://arxiv.org/abs/cs/0207057v2",
        "categories": [
            "cs.CG",
            "cs.LO",
            "math.CT",
            "quant-ph",
            "F.1, F.2"
        ]
    },
    {
        "title": "Interpolation Theorems for Nonmonotonic Reasoning Systems",
        "authors": [
            "Eyal Amir"
        ],
        "summary": "Craig's interpolation theorem (Craig 1957) is an important theorem known for propositional logic and first-order logic. It says that if a logical formula $\\beta$ logically follows from a formula $\\alpha$, then there is a formula $\\gamma$, including only symbols that appear in both $\\alpha,\\beta$, such that $\\beta$ logically follows from $\\gamma$ and $\\gamma$ logically follows from $\\alpha$. Such theorems are important and useful for understanding those logics in which they hold as well as for speeding up reasoning with theories in those logics. In this paper we present interpolation theorems in this spirit for three nonmonotonic systems: circumscription, default logic and logic programs with the stable models semantics (a.k.a. answer set semantics). These results give us better understanding of those logics, especially in contrast to their nonmonotonic characteristics. They suggest that some \\emph{monotonicity} principle holds despite the failure of classic monotonicity for these logics. Also, they sometimes allow us to use methods for the decomposition of reasoning for these systems, possibly increasing their applicability and tractability. Finally, they allow us to build structured representations that use those logics.",
        "published": "2002-07-16T06:27:08Z",
        "link": "http://arxiv.org/abs/cs/0207064v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3;I.2.4;F.4.1"
        ]
    },
    {
        "title": "Knuth-Bendix constraint solving is NP-complete",
        "authors": [
            "Konstantin Korovin",
            "Andrei Voronkov"
        ],
        "summary": "We show the NP-completeness of the existential theory of term algebras with the Knuth-Bendix order by giving a nondeterministic polynomial-time algorithm for solving Knuth-Bendix ordering constraints.",
        "published": "2002-07-17T18:34:45Z",
        "link": "http://arxiv.org/abs/cs/0207068v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "A Polynomial Translation of Logic Programs with Nested Expressions into   Disjunctive Logic Programs: Preliminary Report",
        "authors": [
            "David Pearce",
            "Vladimir Sarsakov",
            "Torsten Schaub",
            "Hans Tompits",
            "Stefan Woltran"
        ],
        "summary": "Nested logic programs have recently been introduced in order to allow for arbitrarily nested formulas in the heads and the bodies of logic program rules under the answer sets semantics. Nested expressions can be formed using conjunction, disjunction, as well as the negation as failure operator in an unrestricted fashion. This provides a very flexible and compact framework for knowledge representation and reasoning. Previous results show that nested logic programs can be transformed into standard (unnested) disjunctive logic programs in an elementary way, applying the negation as failure operator to body literals only. This is of great practical relevance since it allows us to evaluate nested logic programs by means of off-the-shelf disjunctive logic programming systems, like DLV. However, it turns out that this straightforward transformation results in an exponential blow-up in the worst-case, despite the fact that complexity results indicate that there is a polynomial translation among both formalisms. In this paper, we take up this challenge and provide a polynomial translation of logic programs with nested expressions into disjunctive logic programs. Moreover, we show that this translation is modular and (strongly) faithful. We have implemented both the straightforward as well as our advanced transformation; the resulting compiler serves as a front-end to DLV and is publicly available on the Web.",
        "published": "2002-07-19T12:17:42Z",
        "link": "http://arxiv.org/abs/cs/0207071v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4"
        ]
    },
    {
        "title": "Complexity of Nested Circumscription and Nested Abnormality Theories",
        "authors": [
            "Marco Cadoli",
            "Thomas Eiter",
            "Georg Gottlob"
        ],
        "summary": "The need for a circumscriptive formalism that allows for simple yet elegant modular problem representation has led Lifschitz (AIJ, 1995) to introduce nested abnormality theories (NATs) as a tool for modular knowledge representation, tailored for applying circumscription to minimize exceptional circumstances. Abstracting from this particular objective, we propose L_{CIRC}, which is an extension of generic propositional circumscription by allowing propositional combinations and nesting of circumscriptive theories. As shown, NATs are naturally embedded into this language, and are in fact of equal expressive capability. We then analyze the complexity of L_{CIRC} and NATs, and in particular the effect of nesting. The latter is found to be a source of complexity, which climbs the Polynomial Hierarchy as the nesting depth increases and reaches PSPACE-completeness in the general case. We also identify meaningful syntactic fragments of NATs which have lower complexity. In particular, we show that the generalization of Horn circumscription in the NAT framework remains CONP-complete, and that Horn NATs without fixed letters can be efficiently transformed into an equivalent Horn CNF, which implies polynomial solvability of principal reasoning tasks. Finally, we also study extensions of NATs and briefly address the complexity in the first-order case. Our results give insight into the ``cost'' of using L_{CIRC} (resp. NATs) as a host language for expressing other formalisms such as action theories, narratives, or spatial theories.",
        "published": "2002-07-20T19:59:21Z",
        "link": "http://arxiv.org/abs/cs/0207072v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LO",
            "I.2.3; I.2.4; F4.1; F.2.2"
        ]
    },
    {
        "title": "Paraconsistency of Interactive Computation",
        "authors": [
            "Dina Goldin",
            "Peter Wegner"
        ],
        "summary": "The goal of computational logic is to allow us to model computation as well as to reason about it. We argue that a computational logic must be able to model interactive computation. We show that first-order logic cannot model interactive computation due to the incompleteness of interaction. We show that interactive computation is necessarily paraconsistent, able to model both a fact and its negation, due to the role of the world (environment) in determining the course of the computation. We conclude that paraconsistency is a necessary property for a logic that can model interactive computation.",
        "published": "2002-07-21T06:19:23Z",
        "link": "http://arxiv.org/abs/cs/0207074v1",
        "categories": [
            "cs.LO",
            "F.4.1; F.1.2; I.2.0; I.2.11"
        ]
    },
    {
        "title": "Introducing Dynamic Behavior in Amalgamated Knowledge Bases",
        "authors": [
            "Elisa Bertino",
            "Barbara Catania",
            "Paolo Perlasca"
        ],
        "summary": "The problem of integrating knowledge from multiple and heterogeneous sources is a fundamental issue in current information systems. In order to cope with this problem, the concept of mediator has been introduced as a software component providing intermediate services, linking data resources and application programs, and making transparent the heterogeneity of the underlying systems. In designing a mediator architecture, we believe that an important aspect is the definition of a formal framework by which one is able to model integration according to a declarative style. To this purpose, the use of a logical approach seems very promising. Another important aspect is the ability to model both static integration aspects, concerning query execution, and dynamic ones, concerning data updates and their propagation among the various data sources. Unfortunately, as far as we know, no formal proposals for logically modeling mediator architectures both from a static and dynamic point of view have already been developed. In this paper, we extend the framework for amalgamated knowledge bases, presented by Subrahmanian, to deal with dynamic aspects. The language we propose is based on the Active U-Datalog language, and extends it with annotated logic and amalgamation concepts. We model the sources of information and the mediator (also called supervisor) as Active U-Datalog deductive databases, thus modeling queries, transactions, and active rules, interpreted according to the PARK semantics. By using active rules, the system can efficiently perform update propagation among different databases. The result is a logical environment, integrating active and deductive rules, to perform queries and update propagation in an heterogeneous mediated framework.",
        "published": "2002-07-22T07:50:01Z",
        "link": "http://arxiv.org/abs/cs/0207076v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "cs.LO",
            "D.1.6 Logic Programming; H.2.5 Heterogeneous Databases; F.4.1\n  Mathematical Logic, Logic and constraint programming"
        ]
    },
    {
        "title": "Paraconsistent Reasoning via Quantified Boolean Formulas,I: Axiomatising   Signed Systems",
        "authors": [
            "Philippe Besnard",
            "Torsten Schaub",
            "Hans Tompits",
            "Stefan Woltran"
        ],
        "summary": "Signed systems were introduced as a general, syntax-independent framework for paraconsistent reasoning, that is, non-trivialised reasoning from inconsistent information. In this paper, we show how the family of corresponding paraconsistent consequence relations can be axiomatised by means of quantified Boolean formulas. This approach has several benefits. First, it furnishes an axiomatic specification of paraconsistent reasoning within the framework of signed systems. Second, this axiomatisation allows us to identify upper bounds for the complexity of the different signed consequence relations. We strengthen these upper bounds by providing strict complexity results for the considered reasoning tasks. Finally, we obtain an implementation of different forms of paraconsistent reasoning by appeal to the existing system QUIP.",
        "published": "2002-07-25T14:40:51Z",
        "link": "http://arxiv.org/abs/cs/0207084v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.1.3; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Repairing Inconsistent Databases: A Model-Theoretic Approach and   Abductive Reasoning",
        "authors": [
            "Ofer Arieli",
            "Marc Denecker",
            "Bert Van Nuffelen",
            "Maurice Bruynooghe"
        ],
        "summary": "In this paper we consider two points of views to the problem of coherent integration of distributed data. First we give a pure model-theoretic analysis of the possible ways to `repair' a database. We do so by characterizing the possibilities to `recover' consistent data from an inconsistent database in terms of those models of the database that exhibit as minimal inconsistent information as reasonably possible. Then we introduce an abductive application to restore the consistency of a given database. This application is based on an abductive solver (A-system) that implements an SLDNFA-resolution procedure, and computes a list of data-facts that should be inserted to the database or retracted from it in order to keep the database consistent. The two approaches for coherent data integration are related by soundness and completeness results.",
        "published": "2002-07-25T15:13:18Z",
        "link": "http://arxiv.org/abs/cs/0207085v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "F.4.1; H.2.7; I.2.3"
        ]
    },
    {
        "title": "A Model-Theoretic Semantics for Defeasible Logic",
        "authors": [
            "Michael J. Maher"
        ],
        "summary": "Defeasible logic is an efficient logic for defeasible reasoning. It is defined through a proof theory and, until now, has had no model theory. In this paper a model-theoretic semantics is given for defeasible logic. The logic is sound and complete with respect to the semantics. We also briefly outline how this approach extends to a wide range of defeasible logics.",
        "published": "2002-07-25T15:39:01Z",
        "link": "http://arxiv.org/abs/cs/0207086v1",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Axiomatic Aspects of Default Inference",
        "authors": [
            "Guo-Qiang Zhang"
        ],
        "summary": "This paper studies axioms for nonmonotonic consequences from a semantics-based point of view, focusing on a class of mathematical structures for reasoning about partial information without a predefined syntax/logic. This structure is called a default structure. We study axioms for the nonmonotonic consequence relation derived from extensions as in Reiter's default logic, using skeptical reasoning, but extensions are now used for the construction of possible worlds in a default information structure.   In previous work we showed that skeptical reasoning arising from default-extensions obeys a well-behaved set of axioms including the axiom of cautious cut. We show here that, remarkably, the converse is also true: any consequence relation obeying this set of axioms can be represented as one constructed from skeptical reasoning. We provide representation theorems to relate axioms for nonmonotonic consequence relation and properties about extensions, and provide one-to-one correspondence between nonmonotonic systems which satisfies the law of cautious monotony and default structures with unique extensions. Our results give a theoretical justification for a set of basic rules governing the update of nonmonotonic knowledge bases, demonstrating the derivation of them from the more concrete and primitive construction of extensions. It is also striking to note that proofs of the representation theorems show that only shallow extensions are necessary, in the sense that the number of iterations needed to achieve an extension is at most three. All of these developments are made possible by taking a more liberal view of consistency: consistency is a user defined predicate, satisfying some basic properties.",
        "published": "2002-07-25T16:19:51Z",
        "link": "http://arxiv.org/abs/cs/0207087v1",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "A Paraconsistent Higher Order Logic",
        "authors": [
            "Jørgen Villadsen"
        ],
        "summary": "Classical logic predicts that everything (thus nothing useful at all) follows from inconsistency. A paraconsistent logic is a logic where an inconsistency does not lead to such an explosion, and since in practice consistency is difficult to achieve there are many potential applications of paraconsistent logics in knowledge-based systems, logical semantics of natural language, etc. Higher order logics have the advantages of being expressive and with several automated theorem provers available. Also the type system can be helpful. We present a concise description of a paraconsistent higher order logic with countable infinite indeterminacy, where each basic formula can get its own indeterminate truth value (or as we prefer: truth code). The meaning of the logical operators is new and rather different from traditional many-valued logics as well as from logics based on bilattices. The adequacy of the logic is examined by a case study in the domain of medicine. Thus we try to build a bridge between the HOL and MVL communities. A sequent calculus is proposed based on recent work by Muskens.",
        "published": "2002-07-25T16:35:47Z",
        "link": "http://arxiv.org/abs/cs/0207088v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.4; I.2.1"
        ]
    },
    {
        "title": "Defining Rough Sets by Extended Logic Programs",
        "authors": [
            "Jan Małuszyński",
            "Aida Vitória"
        ],
        "summary": "We show how definite extended logic programs can be used for defining and reasoning with rough sets. Moreover, a rough-set-specific query language is presented and an answering algorithm is outlined. Thus, we not only show a possible application of a paraconsistent logic to the field of rough sets as we also establish a link between rough set theory and logic programming, making possible transfer of expertise between both fields.",
        "published": "2002-07-25T16:52:46Z",
        "link": "http://arxiv.org/abs/cs/0207089v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.4.1; I.2.3; I.2.4; D.1.6"
        ]
    },
    {
        "title": "On a Partial Decision Method for Dynamic Proofs",
        "authors": [
            "Diderik Batens"
        ],
        "summary": "This paper concerns a goal directed proof procedure for the propositional fragment of the adaptive logic ACLuN1. At the propositional level, it forms an algorithm for final derivability. If extended to the predicative level, it provides a criterion for final derivability. This is essential in view of the absence of a positive test. The procedure may be generalized to all flat adaptive logics.",
        "published": "2002-07-25T17:14:16Z",
        "link": "http://arxiv.org/abs/cs/0207090v1",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "An Almost Classical Logic for Logic Programming and Nonmonotonic   Reasoning",
        "authors": [
            "François Bry"
        ],
        "summary": "The model theory of a first-order logic called N^4 is introduced. N^4 does not eliminate double negations, as classical logic does, but instead reduces fourfold negations. N^4 is very close to classical logic: N^4 has two truth values; implications in N^4 are material, like in classical logic; and negation distributes over compound formulas in N^4 as it does in classical logic. Results suggest that the semantics of normal logic programs is conveniently formalized in N^4: Classical logic Herbrand interpretations generalize straightforwardly to N^4; the classical minimal Herbrand model of a positive logic program coincides with its unique minimal N^4 Herbrand model; the stable models of a normal logic program and its so-called complete minimal N^4 Herbrand models coincide.",
        "published": "2002-07-25T17:40:14Z",
        "link": "http://arxiv.org/abs/cs/0207091v1",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.3; I.2.4; D.3.1"
        ]
    },
    {
        "title": "Eternity variables to prove simulation of specifications",
        "authors": [
            "Wim H. Hesselink"
        ],
        "summary": "Simulations of specifications are introduced as a unification and generalization of refinement mappings, history variables, forward simulations, prophecy variables, and backward simulations. A specification implements another specification if and only if there is a simulation from the first one to the second one that satisfies a certain condition. By adding stutterings, the formalism allows that the concrete behaviours take more (or possibly less) steps than the abstract ones.   Eternity variables are introduced as a more powerful alternative for prophecy variables and backward simulations. This formalism is semantically complete: every simulation that preserves quiescence is a composition of a forward simulation, an extension with eternity variables, and a refinement mapping. This result does not need finite invisible nondeterminism and machine closure as in the Abadi-Lamport Theorem. Internal continuity is weakened to preservation of quiescence.",
        "published": "2002-07-29T09:12:22Z",
        "link": "http://arxiv.org/abs/cs/0207095v4",
        "categories": [
            "cs.DC",
            "cs.LO",
            "F.1.1;F.3.1"
        ]
    },
    {
        "title": "Mathematical basis for polySAT implication operator",
        "authors": [
            "Charles Sauerbier"
        ],
        "summary": "The mathematical basis motivating the \"implication operator\" of the polySAT algorithm and its function is examined. Such is not undertaken with onerous rigor of symbolic mathematics; a more intuitive visual appeal being employed to present some of the mathematical premises underlying function of the implication operator.",
        "published": "2002-08-18T04:31:39Z",
        "link": "http://arxiv.org/abs/cs/0208026v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.2.2;F.1.1"
        ]
    },
    {
        "title": "A logical reconstruction of SPKI",
        "authors": [
            "Joseph Y. Halpern",
            "Ron van der Meyden"
        ],
        "summary": "SPKI/SDSI is a proposed public key infrastructure standard that incorporates the SDSI public key infrastructure. SDSI's key innovation was the use of local names. We previously introduced a Logic of Local Name Containment that has a clear semantics and was shown to completely characterize SDSI name resolution. Here we show how our earlier approach can be extended to deal with a number of key features of SPKI, including revocation, expiry dates, and tuple reduction. We show that these extensions add relatively little complexity to the logic. In particular, we do not need a nonmonotonic logic to capture revocation. We then use our semantics to examine SPKI's tuple reduction rules. Our analysis highlights places where SPKI's informal description of tuple reduction is somewhat vague, and shows that extra reduction rules are necessary in order to capture general information about binding and authorization.",
        "published": "2002-08-19T21:03:36Z",
        "link": "http://arxiv.org/abs/cs/0208028v1",
        "categories": [
            "cs.CR",
            "cs.LO",
            "D.4.6; F.4.1"
        ]
    },
    {
        "title": "First-order Logic as a Constraint Programming Language",
        "authors": [
            "K. R. Apt",
            "C. F. M. Vermeulen"
        ],
        "summary": "We provide a denotational semantics for first-order logic that captures the two-level view of the computation process typical for constraint programming. At one level we have the usual program execution. At the other level an automatic maintenance of the constraint store takes place. We prove that the resulting semantics is sound with respect to the truth definition. By instantiating it by specific forms of constraint management policies we obtain several sound evaluation policies of first-order formulas. This semantics can also be used a basis for sound implementation of constraint maintenance in presence of block declarations and conditionals.",
        "published": "2002-08-20T14:53:37Z",
        "link": "http://arxiv.org/abs/cs/0208032v2",
        "categories": [
            "cs.LO",
            "F3.2, D3.1"
        ]
    },
    {
        "title": "Complete Axiomatizations for Reasoning About Knowledge and Time",
        "authors": [
            "Joseph Y. Halpern",
            "Ron van der Meyden",
            "Moshe Y. Vardi"
        ],
        "summary": "Sound and complete axiomatizations are provided for a number of different logics involving modalities for knowledge and time. These logics arise from different choices for various parameters. All the logics considered involve the discrete time linear temporal logic operators `next' and `until' and an operator for the knowledge of each of a number of agents. Both the single agent and multiple agent cases are studied: in some instances of the latter there is also an operator for the common knowledge of the group of all agents. Four different semantic properties of agents are considered: whether they have a unique initial state, whether they operate synchronously, whether they have perfect recall, and whether they learn. The property of no learning is essentially dual to perfect recall. Not all settings of these parameters lead to recursively axiomatizable logics, but sound and complete axiomatizations are presented for all the ones that do.",
        "published": "2002-08-20T22:55:24Z",
        "link": "http://arxiv.org/abs/cs/0208033v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1, I.2.4"
        ]
    },
    {
        "title": "Proving correctness of Timed Concurrent Constraint Programs",
        "authors": [
            "F. S. de Boer",
            "M. Gabbrielli",
            "M. C. Meo"
        ],
        "summary": "A temporal logic is presented for reasoning about the correctness of timed concurrent constraint programs. The logic is based on modalities which allow one to specify what a process produces as a reaction to what its environment inputs. These modalities provide an assumption/commitment style of specification which allows a sound and complete compositional axiomatization of the reactive behavior of timed concurrent constraint programs.",
        "published": "2002-08-28T14:34:31Z",
        "link": "http://arxiv.org/abs/cs/0208042v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.1;D.3.1;D.3.2"
        ]
    },
    {
        "title": "The partition semantics of questions, syntactically",
        "authors": [
            "Chung-chieh Shan",
            "Balder D. ten Cate"
        ],
        "summary": "Groenendijk and Stokhof (1984, 1996; Groenendijk 1999) provide a logically attractive theory of the semantics of natural language questions, commonly referred to as the partition theory. Two central notions in this theory are entailment between questions and answerhood. For example, the question \"Who is going to the party?\" entails the question \"Is John going to the party?\", and \"John is going to the party\" counts as an answer to both. Groenendijk and Stokhof define these two notions in terms of partitions of a set of possible worlds.   We provide a syntactic characterization of entailment between questions and answerhood . We show that answers are, in some sense, exactly those formulas that are built up from instances of the question. This result lets us compare the partition theory with other approaches to interrogation -- both linguistic analyses, such as Hamblin's and Karttunen's semantics, and computational systems, such as Prolog. Our comparison separates a notion of answerhood into three aspects: equivalence (when two questions or answers are interchangeable), atomic answers (what instances of a question count as answers), and compound answers (how answers compose).",
        "published": "2002-09-04T20:11:14Z",
        "link": "http://arxiv.org/abs/cs/0209008v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LO",
            "F.4.1; I.2.3; I.2.7"
        ]
    },
    {
        "title": "Question answering: from partitions to Prolog",
        "authors": [
            "Balder D. ten Cate",
            "Chung-chieh Shan"
        ],
        "summary": "We implement Groenendijk and Stokhof's partition semantics of questions in a simple question answering algorithm. The algorithm is sound, complete, and based on tableau theorem proving. The algorithm relies on a syntactic characterization of answerhood: Any answer to a question is equivalent to some formula built up only from instances of the question. We prove this characterization by translating the logic of interrogation to classical predicate logic and applying Craig's interpolation theorem.",
        "published": "2002-09-04T20:46:59Z",
        "link": "http://arxiv.org/abs/cs/0209009v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LO",
            "F.4.1; I.2.3; I.2.7"
        ]
    },
    {
        "title": "Complexity Results on DPLL and Resolution",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "DPLL and resolution are two popular methods for solving the problem of propositional satisfiability. Rather than algorithms, they are families of algorithms, as their behavior depend on some choices they face during execution: DPLL depends on the choice of the literal to branch on; resolution depends on the choice of the pair of clauses to resolve at each step. The complexity of making the optimal choice is analyzed in this paper. Extending previous results, we prove that choosing the optimal literal to branch on in DPLL is Delta[log]^2-hard, and becomes NP^PP-hard if branching is only allowed on a subset of variables. Optimal choice in regular resolution is both NP-hard and CoNP-hard. The problem of determining the size of the optimal proofs is also analyzed: it is CoNP-hard for DPLL, and Delta[log]^2-hard if a conjecture we make is true. This problem is CoNP-hard for regular resolution.",
        "published": "2002-09-27T14:12:48Z",
        "link": "http://arxiv.org/abs/cs/0209032v3",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.2.2; F.1.3; F.4.1; I.2.3"
        ]
    },
    {
        "title": "A Note on Induction Schemas in Bounded Arithmetic",
        "authors": [
            "Aleksandar Ignjatovic"
        ],
        "summary": "As is well known, Buss' theory of bounded arithmetic $S^{1}_{2}$ proves $\\Sigma_{0}^{b}(\\Sigma_{1}^{b})-LIND$; however, we show that Allen's $D_{2}^{1}$ does not prove $\\Sigma_{0}^{b}(\\Sigma_{1}^{b})-LLIND$ unless $P = NC$. We also give some interesting alternative axiomatisations of $S^{1}_{2}$.",
        "published": "2002-10-14T05:07:03Z",
        "link": "http://arxiv.org/abs/cs/0210011v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "An Elementary Fragment of Second-Order Lambda Calculus",
        "authors": [
            "Klaus Aehlig",
            "Jan Johannsen"
        ],
        "summary": "A fragment of second-order lambda calculus (System F) is defined that characterizes the elementary recursive functions. Type quantification is restricted to be non-interleaved and stratified, i.e., the types are assigned levels, and a quantified variable can only be instantiated by a type of smaller level, with a slightly liberalized treatment of the level zero.",
        "published": "2002-10-25T10:31:39Z",
        "link": "http://arxiv.org/abs/cs/0210022v3",
        "categories": [
            "cs.LO",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "A uniform approach to logic programming semantics",
        "authors": [
            "Pascal Hitzler",
            "Matthias Wendt"
        ],
        "summary": "Part of the theory of logic programming and nonmonotonic reasoning concerns the study of fixed-point semantics for these paradigms. Several different semantics have been proposed during the last two decades, and some have been more successful and acknowledged than others. The rationales behind those various semantics have been manifold, depending on one's point of view, which may be that of a programmer or inspired by commonsense reasoning, and consequently the constructions which lead to these semantics are technically very diverse, and the exact relationships between them have not yet been fully understood. In this paper, we present a conceptually new method, based on level mappings, which allows to provide uniform characterizations of different semantics for logic programs. We will display our approach by giving new and uniform characterizations of some of the major semantics, more particular of the least model semantics for definite programs, of the Fitting semantics, and of the well-founded semantics. A novel characterization of the weakly perfect model semantics will also be provided.",
        "published": "2002-10-29T11:37:31Z",
        "link": "http://arxiv.org/abs/cs/0210027v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; D.1.6; F.4.1"
        ]
    },
    {
        "title": "Equivalences Among Aggregate Queries with Negation",
        "authors": [
            "Sara Cohen",
            "Werner Nutt",
            "Yehoshua Sagiv"
        ],
        "summary": "Query equivalence is investigated for disjunctive aggregate queries with negated subgoals, constants and comparisons. A full characterization of equivalence is given for the aggregation functions count, max, sum, prod, toptwo and parity. A related problem is that of determining, for a given natural number N, whether two given queries are equivalent over all databases with at most N constants. We call this problem bounded equivalence. A complete characterization of decidability of bounded equivalence is given. In particular, it is shown that this problem is decidable for all the above aggregation functions as well as for count distinct and average. For quasilinear queries (i.e., queries where predicates that occur positively are not repeated) it is shown that equivalence can be decided in polynomial time for the aggregation functions count, max, sum, parity, prod, toptwo and average. A similar result holds for count distinct provided that a few additional conditions hold. The results are couched in terms of abstract characteristics of aggregation functions, and new proof techniques are used. Finally, the results above also imply that equivalence, under bag-set semantics, is decidable for non-aggregate queries with negation.",
        "published": "2002-10-29T19:19:12Z",
        "link": "http://arxiv.org/abs/cs/0210028v2",
        "categories": [
            "cs.DB",
            "cs.LO",
            "F.4.1;H.2.3;H.2.4"
        ]
    },
    {
        "title": "Programming and Verifying Subgame Perfect Mechanisms",
        "authors": [
            "Marc Pauly"
        ],
        "summary": "An extension of the WHILE-language is developed for programming game-theoretic mechanisms involving multiple agents. Examples of such mechanisms include auctions, voting procedures, and negotiation protocols. A structured operational semantics is provided in terms of extensive games of almost perfect information. Hoare-style partial correctness assertions are proposed to reason about the correctness of these mechanisms, where correctness is interpreted as the existence of a subgame perfect equilibrium. Using an extensional approach to pre- and postconditions, we show that an extension of Hoare's original calculus is sound and complete for reasoning about subgame perfect equilibria in game-theoretic mechanisms.",
        "published": "2002-11-01T13:28:45Z",
        "link": "http://arxiv.org/abs/cs/0211002v2",
        "categories": [
            "cs.LO",
            "cs.GT",
            "F.3.1; F.3.2; F.4.1; I.2.11"
        ]
    },
    {
        "title": "The DLV System for Knowledge Representation and Reasoning",
        "authors": [
            "Nicola Leone",
            "Gerald Pfeifer",
            "Wolfgang Faber",
            "Thomas Eiter",
            "Georg Gottlob",
            "Simona Perri",
            "Francesco Scarcello"
        ],
        "summary": "This paper presents the DLV system, which is widely considered the state-of-the-art implementation of disjunctive logic programming, and addresses several aspects. As for problem solving, we provide a formal definition of its kernel language, function-free disjunctive logic programs (also known as disjunctive datalog), extended by weak constraints, which are a powerful tool to express optimization problems. We then illustrate the usage of DLV as a tool for knowledge representation and reasoning, describing a new declarative programming methodology which allows one to encode complex problems (up to $\\Delta^P_3$-complete problems) in a declarative fashion. On the foundational side, we provide a detailed analysis of the computational complexity of the language of DLV, and by deriving new complexity results we chart a complete picture of the complexity of this language and important fragments thereof.   Furthermore, we illustrate the general architecture of the DLV system which has been influenced by these results. As for applications, we overview application front-ends which have been developed on top of DLV to solve specific knowledge representation tasks, and we briefly describe the main international projects investigating the potential of the system for industrial exploitation. Finally, we report about thorough experimentation and benchmarking, which has been carried out to assess the efficiency of the system. The experimental results confirm the solidity of DLV and highlight its potential for emerging application areas like knowledge management and information integration.",
        "published": "2002-11-04T15:18:04Z",
        "link": "http://arxiv.org/abs/cs/0211004v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.PL",
            "I.2.3; I.2.4; D.3.1"
        ]
    },
    {
        "title": "Intersection Types and Lambda Theories",
        "authors": [
            "M. Dezani-Ciancaglini",
            "S. Lusin"
        ],
        "summary": "We illustrate the use of intersection types as a semantic tool for showing properties of the lattice of lambda theories. Relying on the notion of easy intersection type theory we successfully build a filter model in which the interpretation of an arbitrary simple easy term is any filter which can be described in an uniform way by a predicate. This allows us to prove the consistency of a well-know lambda theory: this consistency has interesting consequences on the algebraic structure of the lattice of lambda theories.",
        "published": "2002-11-12T21:33:33Z",
        "link": "http://arxiv.org/abs/cs/0211011v1",
        "categories": [
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "Vanquishing the XCB Question: The Methodology Discovery of the Last   Shortest Single Axiom for the Equivalential Calculus",
        "authors": [
            "Larry Wos",
            "Dolph Ulrich",
            "Branden Fitelson"
        ],
        "summary": "With the inclusion of an effective methodology, this article answers in detail a question that, for a quarter of a century, remained open despite intense study by various researchers. Is the formula XCB = e(x,e(e(e(x,y),e(z,y)),z)) a single axiom for the classical equivalential calculus when the rules of inference consist of detachment (modus ponens) and substitution? Where the function e represents equivalence, this calculus can be axiomatized quite naturally with the formulas e(x,x), e(e(x,y),e(y,x)), and e(e(x,y),e(e(y,z),e(x,z))), which correspond to reflexivity, symmetry, and transitivity, respectively. (We note that e(x,x) is dependent on the other two axioms.) Heretofore, thirteen shortest single axioms for classical equivalence of length eleven had been discovered, and XCB was the only remaining formula of that length whose status was undetermined. To show that XCB is indeed such a single axiom, we focus on the rule of condensed detachment, a rule that captures detachment together with an appropriately general, but restricted, form of substitution. The proof we present in this paper consists of twenty-five applications of condensed detachment, completing with the deduction of transitivity followed by a deduction of symmetry. We also discuss some factors that may explain in part why XCB resisted relinquishing its treasure for so long. Our approach relied on diverse strategies applied by the automated reasoning program OTTER. Thus ends the search for shortest single axioms for the equivalential calculus.",
        "published": "2002-11-13T16:46:31Z",
        "link": "http://arxiv.org/abs/cs/0211014v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "XCB, the Last of the Shortest Single Axioms for the Classical   Equivalential Calculus",
        "authors": [
            "Larry Wos",
            "Dolph Ulrich",
            "Branden Fitelson"
        ],
        "summary": "It has long been an open question whether the formula XCB = EpEEEpqErqr is, with the rules of substitution and detachment, a single axiom for the classical equivalential calculus. This paper answers that question affirmatively, thus completing a search for all such eleven-symbol single axioms that began seventy years ago.",
        "published": "2002-11-13T20:23:11Z",
        "link": "http://arxiv.org/abs/cs/0211015v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Efficient Solving of Quantified Inequality Constraints over the Real   Numbers",
        "authors": [
            "Stefan Ratschan"
        ],
        "summary": "Let a quantified inequality constraint over the reals be a formula in the first-order predicate language over the structure of the real numbers, where the allowed predicate symbols are $\\leq$ and $<$. Solving such constraints is an undecidable problem when allowing function symbols such $\\sin$ or $\\cos$. In the paper we give an algorithm that terminates with a solution for all, except for very special, pathological inputs. We ensure the practical efficiency of this algorithm by employing constraint programming techniques.",
        "published": "2002-11-14T11:13:30Z",
        "link": "http://arxiv.org/abs/cs/0211016v4",
        "categories": [
            "cs.LO",
            "cs.NA",
            "F.4.1; G.1.0; I.2.3"
        ]
    },
    {
        "title": "Sequent and Hypersequent Calculi for Abelian and Lukasiewicz Logics",
        "authors": [
            "G. Metcalfe",
            "N. Olivetti",
            "D. Gabbay"
        ],
        "summary": "We present two embeddings of infinite-valued Lukasiewicz logic L into Meyer and Slaney's abelian logic A, the logic of lattice-ordered abelian groups. We give new analytic proof systems for A and use the embeddings to derive corresponding systems for L. These include: hypersequent calculi for A and L and terminating versions of these calculi; labelled single sequent calculi for A and L of complexity co-NP; unlabelled single sequent calculi for A and L.",
        "published": "2002-11-18T12:08:17Z",
        "link": "http://arxiv.org/abs/cs/0211021v1",
        "categories": [
            "cs.LO",
            "F.4.1;I.2.3"
        ]
    },
    {
        "title": "Arithmetic, First-Order Logic, and Counting Quantifiers",
        "authors": [
            "Nicole Schweikardt"
        ],
        "summary": "This paper gives a thorough overview of what is known about first-order logic with counting quantifiers and with arithmetic predicates. As a main theorem we show that Presburger arithmetic is closed under unary counting quantifiers. Precisely, this means that for every first-order formula phi(y,z_1,...,z_k) over the signature {<,+} there is a first-order formula psi(x,z_1,...,z_k) which expresses over the structure <Nat,<,+> (respectively, over initial segments of this structure) that the variable x is interpreted exactly by the number of possible interpretations of the variable y for which the formula phi(y,z_1,...,z_k) is satisfied. Applying this theorem, we obtain an easy proof of Ruhl's result that reachability (and similarly, connectivity) in finite graphs is not expressible in first-order logic with unary counting quantifiers and addition. Furthermore, the above result on Presburger arithmetic helps to show the failure of a particular version of the Crane Beach conjecture.",
        "published": "2002-11-19T19:15:51Z",
        "link": "http://arxiv.org/abs/cs/0211022v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Propositional satisfiability in declarative programming",
        "authors": [
            "Deborah East",
            "Miroslaw Truszczynski"
        ],
        "summary": "Answer-set programming (ASP) paradigm is a way of using logic to solve search problems. Given a search problem, to solve it one designs a theory in the logic so that models of this theory represent problem solutions. To compute a solution to a problem one needs to compute a model of the corresponding theory. Several answer-set programming formalisms have been developed on the basis of logic programming with the semantics of stable models. In this paper we show that also the logic of predicate calculus gives rise to effective implementations of the ASP paradigm, similar in spirit to logic programming with stable model semantics and with a similar scope of applicability. Specifically, we propose two logics based on predicate calculus as formalisms for encoding search problems. We show that the expressive power of these logics is given by the class NP-search. We demonstrate how to use them in programming and develop computational tools for model finding. In the case of one of the logics our techniques reduce the problem to that of propositional satisfiability and allow one to use off-the-shelf satisfiability solvers. The language of the other logic has more complex syntax and provides explicit means to model some high-level constraints. For theories in this logic, we designed our own solver that takes advantage of the expanded syntax. We present experimental results demonstrating computational effectiveness of the overall approach.",
        "published": "2002-11-25T13:15:58Z",
        "link": "http://arxiv.org/abs/cs/0211033v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3;D.1.6"
        ]
    },
    {
        "title": "Database Repairs and Analytic Tableaux",
        "authors": [
            "Leopoldo Bertossi",
            "Camilla Schwind"
        ],
        "summary": "In this article, we characterize in terms of analytic tableaux the repairs of inconsistent relational databases, that is databases that do not satisfy a given set of integrity constraints. For this purpose we provide closing and opening criteria for branches in tableaux that are built for database instances and their integrity constraints. We use the tableaux based characterization as a basis for consistent query answering, that is for retrieving from the database answers to queries that are consistent wrt the integrity constraints.",
        "published": "2002-11-29T04:28:56Z",
        "link": "http://arxiv.org/abs/cs/0211042v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H2; F4; I2"
        ]
    },
    {
        "title": "Retractions of Types with Many Atoms",
        "authors": [
            "Laurent Regnier",
            "Pawel Urzyczyn"
        ],
        "summary": "We define a sound and complete proof system for affine beta-eta-retractions in simple types built over many atoms, and we state simple necessary conditions for arbitrary beta-eta-retractions in simple and polymorphic types.",
        "published": "2002-12-05T17:29:12Z",
        "link": "http://arxiv.org/abs/cs/0212005v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "A Generalization of the Lifting Lemma for Logic Programming",
        "authors": [
            "Etienne Payet",
            "Fred Mesnard"
        ],
        "summary": "Since the seminal work of J. A. Robinson on resolution, many lifting lemmas for simplifying proofs of completeness of resolution have been proposed in the literature. In the logic programming framework, they may also help to detect some infinite derivations while proving goals under the SLD-resolution. In this paper, we first generalize a version of the lifting lemma, by extending the relation \"is more general than\" so that it takes into account only some arguments of the atoms. The other arguments, which we call neutral arguments, are disregarded. Then we propose two syntactic conditions of increasing power for identifying neutral arguments from mere inspection of the text of a logic program.",
        "published": "2002-12-11T09:48:39Z",
        "link": "http://arxiv.org/abs/cs/0212026v1",
        "categories": [
            "cs.LO",
            "D.1.6"
        ]
    },
    {
        "title": "An Ehrenfeucht-Fraisse Game Approach to Collapse Results in Database   Theory",
        "authors": [
            "Nicole Schweikardt"
        ],
        "summary": "We present a new Ehrenfeucht-Fraisse game approach to collapse results in database theory and we show that, in principle, this approach suffices to prove every natural generic collapse result. Following this approach we can deal with certain infinite databases where previous, highly involved methods fail. We prove the natural generic collapse for Z-embeddable databases over any linearly ordered context structure with arbitrary monadic predicates, and for N-embeddable databases over the context structure (R,<,+,Mon_Q,Groups). Here, N, Z, R, denote the sets of natural numbers, integers, and real numbers, respectively. Groups is the collection of all subgroups of (R,+) that contain Z, and Mon_Q is the collection of all subsets of a particular infinite subset Q of N. Restricting the complexity of the formulas that may be used to formulate queries to Boolean combinations of purely existential first-order formulas, we even obtain the collapse for N-embeddable databases over any linearly ordered context structure with arbitrary predicates. Finally, we develop the notion of N-representable databases, which is a natural generalization of the classical notion of finitely representable databases. We show that natural generic collapse results for N-embeddable databases can be lifted to the larger class of N-representable databases. To obtain, in particular, the collapse result for (N,<,+,Mon_Q), we explicitly construct a winning strategy for the duplicator in the presence of the built-in addition relation +. This, as a side product, also leads to an Ehrenfeucht-Fraisse game proof of the theorem of Ginsburg and Spanier, stating that the spectra of FO(<,+)-sentences are semi-linear.",
        "published": "2002-12-20T14:34:37Z",
        "link": "http://arxiv.org/abs/cs/0212049v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "F.4.1; H.2.3"
        ]
    },
    {
        "title": "Merging Locally Correct Knowledge Bases: A Preliminary Report",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "Belief integration methods are often aimed at deriving a single and consistent knowledge base that retains as much as possible of the knowledge bases to integrate. The rationale behind this approach is the minimal change principle: the result of the integration process should differ as less as possible from the knowledge bases to integrate. We show that this principle can be reformulated in terms of a more general model of belief revision, based on the assumption that inconsistency is due to the mistakes the knowledge bases contain. Current belief revision strategies are based on a specific kind of mistakes, which however does not include all possible ones. Some alternative possibilities are discussed.",
        "published": "2002-12-28T16:09:25Z",
        "link": "http://arxiv.org/abs/cs/0212053v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4, F.4.1"
        ]
    },
    {
        "title": "A computer scientist looks at game theory",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "I consider issues in distributed computation that should be of relevance to game theory. In particular, I focus on (a) representing knowledge and uncertainty, (b) dealing with failures, and (c) specification of mechanisms.",
        "published": "2002-01-18T11:05:18Z",
        "link": "http://arxiv.org/abs/cs/0201016v1",
        "categories": [
            "cs.GT",
            "cs.DC",
            "cs.MA",
            "I.2.11, C.2.4"
        ]
    },
    {
        "title": "On Learning by Exchanging Advice",
        "authors": [
            "L. Nunes",
            "E. Oliveira"
        ],
        "summary": "One of the main questions concerning learning in Multi-Agent Systems is: (How) can agents benefit from mutual interaction during the learning process?. This paper describes the study of an interactive advice-exchange mechanism as a possible way to improve agents' learning performance. The advice-exchange technique, discussed here, uses supervised learning (backpropagation), where reinforcement is not directly coming from the environment but is based on advice given by peers with better performance score (higher confidence), to enhance the performance of a heterogeneous group of Learning Agents (LAs). The LAs are facing similar problems, in an environment where only reinforcement information is available. Each LA applies a different, well known, learning technique: Random Walk (hill-climbing), Simulated Annealing, Evolutionary Algorithms and Q-Learning. The problem used for evaluation is a simplified traffic-control simulation. Initial results indicate that advice-exchange can improve learning speed, although bad advice and/or blind reliance can disturb the learning performance.",
        "published": "2002-03-07T10:16:25Z",
        "link": "http://arxiv.org/abs/cs/0203010v1",
        "categories": [
            "cs.LG",
            "cs.MA",
            "I.2.6; I.2.11"
        ]
    },
    {
        "title": "Capturing Knowledge of User Preferences: ontologies on recommender   systems",
        "authors": [
            "S. E. Middleton",
            "D. C. De Roure",
            "N. R. Shadbolt"
        ],
        "summary": "Tools for filtering the World Wide Web exist, but they are hampered by the difficulty of capturing user preferences in such a dynamic environment. We explore the acquisition of user profiles by unobtrusive monitoring of browsing behaviour and application of supervised machine-learning techniques coupled with an ontological representation to extract user preferences. A multi-class approach to paper classification is used, allowing the paper topic taxonomy to be utilised during profile construction. The Quickstep recommender system is presented and two empirical studies evaluate it in a real work setting, measuring the effectiveness of using a hierarchical topic ontology compared with an extendable flat list.",
        "published": "2002-03-08T15:58:23Z",
        "link": "http://arxiv.org/abs/cs/0203011v1",
        "categories": [
            "cs.LG",
            "cs.MA",
            "I.2.6;I.2.11"
        ]
    },
    {
        "title": "Interface agents: A review of the field",
        "authors": [
            "Stuart E. Middleton"
        ],
        "summary": "This paper reviews the origins of interface agents, discusses challenges that exist within the interface agent field and presents a survey of current attempts to find solutions to these challenges. A history of agent systems from their birth in the 1960's to the current day is described, along with the issues they try to address. A taxonomy of interface agent systems is presented, and today's agent systems categorized accordingly. Lastly, an analysis of the machine learning and user modelling techniques used by today's agents is presented.",
        "published": "2002-03-09T01:28:33Z",
        "link": "http://arxiv.org/abs/cs/0203012v1",
        "categories": [
            "cs.MA",
            "cs.LG",
            "I.2.11;I.2.6"
        ]
    },
    {
        "title": "NetNeg: A Connectionist-Agent Integrated System for Representing Musical   Knowledge",
        "authors": [
            "Claudia V. Goldman",
            "Dan Gang",
            "Jeffrey S. Rosenschein",
            "Daniel Lehmann"
        ],
        "summary": "The system presented here shows the feasibility of modeling the knowledge involved in a complex musical activity by integrating sub-symbolic and symbolic processes. This research focuses on the question of whether there is any advantage in integrating a neural network together with a distributed artificial intelligence approach within the music domain. The primary purpose of our work is to design a model that describes the different aspects a user might be interested in considering when involved in a musical activity. The approach we suggest in this work enables the musician to encode his knowledge, intuitions, and aesthetic taste into different modules. The system captures these aspects by computing and applying three distinct functions: rules, fuzzy concepts, and learning.   As a case study, we began experimenting with first species two-part counterpoint melodies. We have developed a hybrid system composed of a connectionist module and an agent-based module to combine the sub-symbolic and symbolic levels to achieve this task. The technique presented here to represent musical knowledge constitutes a new approach for composing polyphonic music.",
        "published": "2002-03-17T09:38:56Z",
        "link": "http://arxiv.org/abs/cs/0203021v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "I.2.6; J.5"
        ]
    },
    {
        "title": "Exploiting Synergy Between Ontologies and Recommender Systems",
        "authors": [
            "Stuart E. Middleton",
            "Harith Alani",
            "David C. De Roure"
        ],
        "summary": "Recommender systems learn about user preferences over time, automatically finding things of similar interest. This reduces the burden of creating explicit queries. Recommender systems do, however, suffer from cold-start problems where no initial information is available early on upon which to base recommendations. Semantic knowledge structures, such as ontologies, can provide valuable domain knowledge and user information. However, acquiring such knowledge and keeping it up to date is not a trivial task and user interests are particularly difficult to acquire and maintain. This paper investigates the synergy between a web-based research paper recommender system and an ontology containing information automatically extracted from departmental databases available on the web. The ontology is used to address the recommender systems cold-start problem. The recommender system addresses the ontology's interest-acquisition problem. An empirical evaluation of this approach is conducted and the performance of the integrated systems measured.",
        "published": "2002-04-08T10:56:26Z",
        "link": "http://arxiv.org/abs/cs/0204012v1",
        "categories": [
            "cs.LG",
            "cs.MA",
            "K.3.2;I.2.11"
        ]
    },
    {
        "title": "From Alife Agents to a Kingdom of N Queens",
        "authors": [
            "Jing Han",
            "Jiming Liu",
            "Qingsheng Cai"
        ],
        "summary": "This paper presents a new approach to solving N-queen problems, which involves a model of distributed autonomous agents with artificial life (ALife) and a method of representing N-queen constraints in an agent environment. The distributed agents locally interact with their living environment, i.e., a chessboard, and execute their reactive behaviors by applying their behavioral rules for randomized motion, least-conflict position searching, and cooperating with other agents etc. The agent-based N-queen problem solving system evolves through selection and contest according to the rule of Survival of the Fittest, in which some agents will die or be eaten if their moving strategies are less efficient than others. The experimental results have shown that this system is capable of solving large-scale N-queen problems. This paper also provides a model of ALife agents for solving general CSPs.",
        "published": "2002-05-13T10:49:48Z",
        "link": "http://arxiv.org/abs/cs/0205016v1",
        "categories": [
            "cs.AI",
            "cs.DS",
            "cs.MA",
            "I.2.8;I.2.11;G.2.1"
        ]
    },
    {
        "title": "Effectiveness of Preference Elicitation in Combinatorial Auctions",
        "authors": [
            "Benoit Hudson",
            "Tuomas Sandholm"
        ],
        "summary": "Combinatorial auctions where agents can bid on bundles of items are desirable because they allow the agents to express complementarity and substitutability between the items. However, expressing one's preferences can require bidding on all bundles. Selective incremental preference elicitation by the auctioneer was recently proposed to address this problem (Conen & Sandholm 2001), but the idea was not evaluated. In this paper we show, experimentally and theoretically, that automated elicitation provides a drastic benefit. In all of the elicitation schemes under study, as the number of items for sale increases, the amount of information elicited is a vanishing fraction of the information collected in traditional ``direct revelation mechanisms'' where bidders reveal all their valuation information. Most of the elicitation schemes also maintain the benefit as the number of agents increases. We develop more effective elicitation policies for existing query types. We also present a new query type that takes the incremental nature of elicitation to a new level by allowing agents to give approximate answers that are refined only on an as-needed basis. In the process, we present methods for evaluating different types of elicitation policies.",
        "published": "2002-05-27T01:56:37Z",
        "link": "http://arxiv.org/abs/cs/0205066v1",
        "categories": [
            "cs.GT",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Complexity Results about Nash Equilibria",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "Noncooperative game theory provides a normative framework for analyzing strategic interactions. However, for the toolbox to be operational, the solutions it defines will have to be computed. In this paper, we provide a single reduction that 1) demonstrates NP-hardness of determining whether Nash equilibria with certain natural properties exist, and 2) demonstrates the #P-hardness of counting Nash equilibria (or connected sets of Nash equilibria). We also show that 3) determining whether a pure-strategy Bayes-Nash equilibrium exists is NP-hard, and that 4) determining whether a pure-strategy Nash equilibrium exists in a stochastic (Markov) game is PSPACE-hard even if the game is invisible (this remains NP-hard if the game is finite). All of our hardness results hold even if there are only two players and the game is symmetric.   Keywords: Nash equilibrium; game theory; computational complexity; noncooperative game theory; normal form game; stochastic game; Markov game; Bayes-Nash equilibrium; multiagent systems.",
        "published": "2002-05-28T23:32:37Z",
        "link": "http://arxiv.org/abs/cs/0205074v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Complexity of Mechanism Design",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "The aggregation of conflicting preferences is a central problem in multiagent systems. The key difficulty is that the agents may report their preferences insincerely. Mechanism design is the art of designing the rules of the game so that the agents are motivated to report their preferences truthfully and a (socially) desirable outcome is chosen. We propose an approach where a mechanism is automatically created for the preference aggregation setting at hand. This has several advantages, but the downside is that the mechanism design optimization problem needs to be solved anew each time. Focusing on settings where side payments are not possible, we show that the mechanism design problem is NP-complete for deterministic mechanisms. This holds both for dominant-strategy implementation and for Bayes-Nash implementation. We then show that if we allow randomized mechanisms, the mechanism design problem becomes tractable. In other words, the coordinator can tackle the computational complexity introduced by its uncertainty about the agents' preferences by making the agents face additional uncertainty. This comes at no loss, and in some cases at a gain, in the (social) objective.",
        "published": "2002-05-28T23:43:12Z",
        "link": "http://arxiv.org/abs/cs/0205075v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Vote Elicitation: Complexity and Strategy-Proofness",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "Preference elicitation is a central problem in AI, and has received significant attention in single-agent settings. It is also a key problem in multiagent systems, but has received little attention here so far. In this setting, the agents may have different preferences that often must be aggregated using voting. This leads to interesting issues because what, if any, information should be elicited from an agent depends on what other agents have revealed about their preferences so far.   In this paper we study effective elicitation, and its impediments, for the most common voting protocols. It turns out that in the Single Transferable Vote protocol, even knowing when to terminate elicitation is mathcal NP-complete, while this is easy for all the other protocols under study. Even for these protocols, determining how to elicit effectively is NP-complete, even with perfect suspicions about how the agents will vote. The exception is the Plurality protocol where such effective elicitation is easy.   We also show that elicitation introduces additional opportunities for strategic manipulation by the voters. We demonstrate how to curtail the space of elicitation schemes so that no such additional strategic issues arise.",
        "published": "2002-05-29T00:10:26Z",
        "link": "http://arxiv.org/abs/cs/0205073v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "What is a Joint Goal? Games with Beliefs and Defeasible Desires",
        "authors": [
            "Mehdi Dastani",
            "Leendert van der Torre"
        ],
        "summary": "In this paper we introduce a qualitative decision and game theory based on belief (B) and desire (D) rules. We show that a group of agents acts as if it is maximizing achieved joint goals.",
        "published": "2002-07-07T11:26:42Z",
        "link": "http://arxiv.org/abs/cs/0207022v1",
        "categories": [
            "cs.MA",
            "cs.GT",
            "I.2.4"
        ]
    },
    {
        "title": "Geometric Aspects of Multiagent Systems",
        "authors": [
            "Timothy Porter"
        ],
        "summary": "Recent advances in Multiagent Systems (MAS) and Epistemic Logic within Distributed Systems Theory, have used various combinatorial structures that model both the geometry of the systems and the Kripke model structure of models for the logic. Examining one of the simpler versions of these models, interpreted systems, and the related Kripke semantics of the logic $S5_n$ (an epistemic logic with $n$-agents), the similarities with the geometric / homotopy theoretic structure of groupoid atlases is striking. These latter objects arise in problems within algebraic K-theory, an area of algebra linked to the study of decomposition and normal form theorems in linear algebra. They have a natural well structured notion of path and constructions of path objects, etc., that yield a rich homotopy theory.",
        "published": "2002-10-25T16:32:59Z",
        "link": "http://arxiv.org/abs/cs/0210023v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11 Distributed Artificial Intelligence"
        ]
    },
    {
        "title": "Intelligence and Cooperative Search by Coupled Local Minimizers",
        "authors": [
            "J. A. K. Suykens",
            "J. Vandewalle",
            "B. De Moor"
        ],
        "summary": "We show how coupling of local optimization processes can lead to better solutions than multi-start local optimization consisting of independent runs. This is achieved by minimizing the average energy cost of the ensemble, subject to synchronization constraints between the state vectors of the individual local minimizers. From an augmented Lagrangian which incorporates the synchronization constraints both as soft and hard constraints, a network is derived wherein the local minimizers interact and exchange information through the synchronization constraints. From the viewpoint of neural networks, the array can be considered as a Lagrange programming network for continuous optimization and as a cellular neural network (CNN). The penalty weights associated with the soft state synchronization constraints follow from the solution to a linear program. This expresses that the energy cost of the ensemble should maximally decrease. In this way successful local minimizers can implicitly impose their state to the others through a mechanism of master-slave dynamics resulting into a cooperative search mechanism. Improved information spreading within the ensemble is obtained by applying the concept of small-world networks. This work suggests, in an interdisciplinary context, the importance of information exchange and state synchronization within ensembles, towards issues as evolution, collective behaviour, optimality and intelligence.",
        "published": "2002-10-30T11:59:57Z",
        "link": "http://arxiv.org/abs/cs/0210030v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "cs.NE",
            "F.1.1"
        ]
    },
    {
        "title": "Evolution and anti-evolution in a minimal stock market model",
        "authors": [
            "R. Rothenstein",
            "K. Pawelzik"
        ],
        "summary": "We present a novel microscopic stock market model consisting of a large number of random agents modeling traders in a market. Each agent is characterized by a set of parameters that serve to make iterated predictions of two successive returns. The future price is determined according to the offer and the demand of all agents. The system evolves by redistributing the capital among the agents in each trading cycle. Without noise the dynamics of this system is nearly regular and thereby fails to reproduce the stochastic return fluctuations observed in real markets. However, when in each cycle a small amount of noise is introduced we find the typical features of real financial time series like fat-tails of the return distribution and large temporal correlations in the volatility without significant correlations in the price returns. Introducing the noise by an evolutionary process leads to different scalings of the return distributions that depend on the definition of fitness. Because our realistic model has only very few parameters, and the results appear to be robust with respect to the noise level and the number of agents we expect that our framework may serve as new paradigm for modeling self generated return fluctuations in markets.",
        "published": "2002-11-07T23:15:48Z",
        "link": "http://arxiv.org/abs/nlin/0211010v2",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.MA",
            "q-fin.TR"
        ]
    },
    {
        "title": "A Spin Glass Model of Human Logic Systems",
        "authors": [
            "Fariel Shafee"
        ],
        "summary": "In this paper, we discuss different models for human logic systems and describe a game with nature. G\\\"odel`s incompleteness theorem is taken into account to construct a model of logical networks based on axioms obtained by symmetry breaking. These classical logic networks are then coupled using rules that depend on whether two networks contain axioms or anti-axioms. The social lattice of axiom based logic networks is then placed with the environment network in a game including entropy as a cost factor. The classical logical networks are then replaced with ``preference axioms'' to the role of fuzzy logic.",
        "published": "2002-11-08T16:40:30Z",
        "link": "http://arxiv.org/abs/nlin/0211013v1",
        "categories": [
            "nlin.AO",
            "cond-mat.dis-nn",
            "cs.MA"
        ]
    },
    {
        "title": "Thinking Adaptive: Towards a Behaviours Virtual Laboratory",
        "authors": [
            "Carlos Gershenson",
            "Pedro Pablo Gonzalez",
            "Jose Negrete"
        ],
        "summary": "In this paper we name some of the advantages of virtual laboratories; and propose that a Behaviours Virtual Laboratory should be useful for both biologists and AI researchers, offering a new perspective for understanding adaptive behaviour. We present our development of a Behaviours Virtual Laboratory, which at this stage is focused in action selection, and show some experiments to illustrate the properties of our proposal, which can be accessed via Internet.",
        "published": "2002-11-21T18:35:23Z",
        "link": "http://arxiv.org/abs/cs/0211028v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "I.2.0, I.6.7"
        ]
    },
    {
        "title": "Modelling intracellular signalling networks using behaviour-based   systems and the blackboard architecture",
        "authors": [
            "Pedro Pablo Gonzalez Perez",
            "Carlos Gershenson",
            "Maura Cardenas Garcia",
            "Jaime Lagunez Otero"
        ],
        "summary": "This paper proposes to model the intracellular signalling networks using a fusion of behaviour-based systems and the blackboard architecture. In virtue of this fusion, the model developed by us, which has been named Cellulat, allows to take account two essential aspects of the intracellular signalling networks: (1) the cognitive capabilities of certain types of networks' components and (2) the high level of spatial organization of these networks. A simple example of modelling of Ca2+ signalling pathways using Cellulat is presented here. An intracellular signalling virtual laboratory is being developed from Cellulat.",
        "published": "2002-11-22T12:10:55Z",
        "link": "http://arxiv.org/abs/cs/0211029v1",
        "categories": [
            "cs.MA",
            "q-bio.CB",
            "J.3"
        ]
    },
    {
        "title": "Integration of Computational Techniques for the Modelling of Signal   Transduction",
        "authors": [
            "Pedro Pablo Gonzalez Perez",
            "Maura Cardenas Garcia",
            "Carlos Gershenson",
            "Jaime Lagunez-Otero"
        ],
        "summary": "A cell can be seen as an adaptive autonomous agent or as a society of adaptive autonomous agents, where each can exhibit a particular behaviour depending on its cognitive capabilities. We present an intracellular signalling model obtained by integrating several computational techniques into an agent-based paradigm. Cellulat, the model, takes into account two essential aspects of the intracellular signalling networks: cognitive capacities and a spatial organization. Exemplifying the functionality of the system by modelling the EGFR signalling pathway, we discuss the methodology as well as the purposes of an intracellular signalling virtual laboratory, presently under development.",
        "published": "2002-11-22T12:18:11Z",
        "link": "http://arxiv.org/abs/cs/0211030v1",
        "categories": [
            "cs.MA",
            "q-bio.CB",
            "J.3"
        ]
    },
    {
        "title": "Method of Additional Structures on the Objects of a Monoidal Kleisli   Category as a Background for Information Transformers Theory",
        "authors": [
            "P. V. Golubtsov",
            "S. S. Moskaliuk"
        ],
        "summary": "Category theory provides a compact method of encoding mathematical structures in a uniform way, thereby enabling the use of general theorems on, for example, equivalence and universal constructions. In this article we develop the method of additional structures on the objects of a monoidal Kleisli category. It is proposed to consider any uniform class of information transformers (ITs) as a family of morphisms of a category that satisfy certain set of axioms. This makes it possible to study in a uniform way different types of ITs, e.g., statistical, multivalued, and fuzzy ITs. Proposed axioms define a category of ITs as a monoidal category that contains a subcategory (of deterministic ITs) with finite products. Besides, it is shown that many categories of ITs can be constructed as Kleisli categories with additional structures.",
        "published": "2002-11-27T18:47:30Z",
        "link": "http://arxiv.org/abs/math-ph/0211067v1",
        "categories": [
            "math-ph",
            "cs.MA",
            "math.CT",
            "math.MP"
        ]
    },
    {
        "title": "Incremental Construction of Compact Acyclic NFAs",
        "authors": [
            "Kyriakos N. Sgarbas",
            "Nikos D. Fakotakis",
            "George K. Kokkinakis"
        ],
        "summary": "This paper presents and analyzes an incremental algorithm for the construction of Acyclic Non-deterministic Finite-state Automata (NFA). Automata of this type are quite useful in computational linguistics, especially for storing lexicons. The proposed algorithm produces compact NFAs, i.e. NFAs that do not contain equivalent states. Unlike Deterministic Finite-state Automata (DFA), this property is not sufficient to ensure minimality, but still the resulting NFAs are considerably smaller than the minimal DFAs for the same languages.",
        "published": "2002-01-04T17:01:14Z",
        "link": "http://arxiv.org/abs/cs/0201002v1",
        "categories": [
            "cs.DS",
            "cs.CL",
            "E.1; F.2.2; H.3.1; I.2.7"
        ]
    },
    {
        "title": "Long-range fractal correlations in literary corpora",
        "authors": [
            "Marcelo A. Montemurro",
            "Pedro A. Pury"
        ],
        "summary": "In this paper we analyse the fractal structure of long human-language records by mapping large samples of texts onto time series. The particular mapping set up in this work is inspired on linguistic basis in the sense that is retains {\\em the word} as the fundamental unit of communication. The results confirm that beyond the short-range correlations resulting from syntactic rules acting at sentence level, long-range structures emerge in large written language samples that give rise to long-range correlations in the use of words.",
        "published": "2002-01-09T18:55:22Z",
        "link": "http://arxiv.org/abs/cond-mat/0201139v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CL",
            "nlin.AO"
        ]
    },
    {
        "title": "Using Tree Automata and Regular Expressions to Manipulate Hierarchically   Structured Data",
        "authors": [
            "Nikita Schmidt",
            "Ahmed Patel"
        ],
        "summary": "Information, stored or transmitted in digital form, is often structured. Individual data records are usually represented as hierarchies of their elements. Together, records form larger structures. Information processing applications have to take account of this structuring, which assigns different semantics to different data elements or records. Big variety of structural schemata in use today often requires much flexibility from applications--for example, to process information coming from different sources. To ensure application interoperability, translators are needed that can convert one structure into another. This paper puts forward a formal data model aimed at supporting hierarchical data processing in a simple and flexible way. The model is based on and extends results of two classical theories, studying finite string and tree automata. The concept of finite automata and regular languages is applied to the case of arbitrarily structured tree-like hierarchical data records, represented as \"structured strings.\" These automata are compared with classical string and tree automata; the model is shown to be a superset of the classical models. Regular grammars and expressions over structured strings are introduced. Regular expression matching and substitution has been widely used for efficient unstructured text processing; the model described here brings the power of this proven technique to applications that deal with information trees. A simple generic alternative is offered to replace today's specialised ad-hoc approaches. The model unifies structural and content transformations, providing applications with a single data type. An example scenario of how to build applications based on this theory is discussed. Further research directions are outlined.",
        "published": "2002-01-11T17:59:17Z",
        "link": "http://arxiv.org/abs/cs/0201008v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "E.1; F.4.3; I.7.2; H.3.3"
        ]
    },
    {
        "title": "Extended Comment on Language Trees and Zipping",
        "authors": [
            "Joshua Goodman"
        ],
        "summary": "This is the extended version of a Comment submitted to Physical Review Letters. I first point out the inappropriateness of publishing a Letter unrelated to physics. Next, I give experimental results showing that the technique used in the Letter is 3 times worse and 17 times slower than a simple baseline. And finally, I review the literature, showing that the ideas of the Letter are not novel. I conclude by suggesting that Physical Review Letters should not publish Letters unrelated to physics.",
        "published": "2002-02-21T18:25:29Z",
        "link": "http://arxiv.org/abs/cond-mat/0202383v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CL",
            "cs.LG"
        ]
    },
    {
        "title": "Entropy estimation of symbol sequences",
        "authors": [
            "Thomas Schürmann",
            "Peter Grassberger"
        ],
        "summary": "We discuss algorithms for estimating the Shannon entropy h of finite symbol sequences with long range correlations. In particular, we consider algorithms which estimate h from the code lengths produced by some compression algorithm. Our interest is in describing their convergence with sequence length, assuming no limits for the space and time complexities of the compression algorithms. A scaling law is proposed for extrapolation from finite sample lengths. This is applied to sequences of dynamical systems in non-trivial chaotic regimes, a 1-D cellular automaton, and to written English texts.",
        "published": "2002-03-21T11:24:56Z",
        "link": "http://arxiv.org/abs/cond-mat/0203436v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CL",
            "cs.IT",
            "math.IT",
            "physics.data-an",
            "stat.ML"
        ]
    },
    {
        "title": "Blind Normalization of Speech From Different Channels and Speakers",
        "authors": [
            "David N. Levin"
        ],
        "summary": "This paper describes representations of time-dependent signals that are invariant under any invertible time-independent transformation of the signal time series. Such a representation is created by rescaling the signal in a non-linear dynamic manner that is determined by recently encountered signal levels. This technique may make it possible to normalize signals that are related by channel-dependent and speaker-dependent transformations, without having to characterize the form of the signal transformations, which remain unknown. The technique is illustrated by applying it to the time-dependent spectra of speech that has been filtered to simulate the effects of different channels. The experimental results show that the rescaled speech representations are largely normalized (i.e., channel-independent), despite the channel-dependence of the raw (unrescaled) speech.",
        "published": "2002-04-02T21:14:40Z",
        "link": "http://arxiv.org/abs/cs/0204003v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "TableTrans, MultiTrans, InterTrans and TreeTrans: Diverse Tools Built on   the Annotation Graph Toolkit",
        "authors": [
            "Steven Bird",
            "Kazuaki Maeda",
            "Xiaoyi Ma",
            "Haejoong Lee",
            "Beth Randall",
            "Salim Zayat"
        ],
        "summary": "Four diverse tools built on the Annotation Graph Toolkit are described. Each tool associates linguistic codes and structures with time-series data. All are based on the same software library and tool architecture. TableTrans is for observational coding, using a spreadsheet whose rows are aligned to a signal. MultiTrans is for transcribing multi-party communicative interactions recorded using multi-channel signals. InterTrans is for creating interlinear text aligned to audio. TreeTrans is for creating and manipulating syntactic trees. This work demonstrates that the development of diverse tools and re-use of software components is greatly facilitated by a common high-level application programming interface for representing the data and managing input/output, together with a common architecture for managing the interaction of multiple components.",
        "published": "2002-04-03T17:18:49Z",
        "link": "http://arxiv.org/abs/cs/0204006v1",
        "categories": [
            "cs.CL",
            "cs.SD",
            "D.2.13; H.5.5; I.2.7"
        ]
    },
    {
        "title": "Models and Tools for Collaborative Annotation",
        "authors": [
            "Xiaoyi Ma",
            "Haejoong Lee",
            "Steven Bird",
            "Kazuaki Maeda"
        ],
        "summary": "The Annotation Graph Toolkit (AGTK) is a collection of software which facilitates development of linguistic annotation tools. AGTK provides a database interface which allows applications to use a database server for persistent storage. This paper discusses various modes of collaborative annotation and how they can be supported with tools built using AGTK and its database interface. We describe the relational database schema and API, and describe a version of the TableTrans tool which supports collaborative annotation. The remainder of the paper discusses a high-level query language for annotation graphs, along with optimizations, in support of expressive and efficient access to the annotations held on a large central server. The paper demonstrates that it is straightforward to support a variety of different levels of collaborative annotation with existing AGTK-based tools, with a minimum of additional programming effort.",
        "published": "2002-04-03T17:25:39Z",
        "link": "http://arxiv.org/abs/cs/0204004v1",
        "categories": [
            "cs.CL",
            "cs.SD",
            "H.2.4; H.5.3; H.5.5; I.2.7"
        ]
    },
    {
        "title": "Creating Annotation Tools with the Annotation Graph Toolkit",
        "authors": [
            "Kazuaki Maeda",
            "Steven Bird",
            "Xiaoyi Ma",
            "Haejoong Lee"
        ],
        "summary": "The Annotation Graph Toolkit is a collection of software supporting the development of annotation tools based on the annotation graph model. The toolkit includes application programming interfaces for manipulating annotation graph data and for importing data from other formats. There are interfaces for the scripting languages Tcl and Python, a database interface, specialized graphical user interfaces for a variety of annotation tasks, and several sample applications. This paper describes all the toolkit components for the benefit of would-be application developers.",
        "published": "2002-04-03T17:32:53Z",
        "link": "http://arxiv.org/abs/cs/0204005v1",
        "categories": [
            "cs.CL",
            "cs.SD",
            "D.2.13; H.5.5; I.2.7"
        ]
    },
    {
        "title": "An Integrated Framework for Treebanks and Multilayer Annotations",
        "authors": [
            "Scott Cotton",
            "Steven Bird"
        ],
        "summary": "Treebank formats and associated software tools are proliferating rapidly, with little consideration for interoperability. We survey a wide variety of treebank structures and operations, and show how they can be mapped onto the annotation graph model, and leading to an integrated framework encompassing tree and non-tree annotations alike. This development opens up new possibilities for managing and exploiting multilayer annotations.",
        "published": "2002-04-03T18:55:01Z",
        "link": "http://arxiv.org/abs/cs/0204007v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "The tip-of-the-tongue phenomenon: Irrelevant neural network localization   or disruption of its interneuron links ?",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "On the base of recently proposed three-stage quantitative neural network model of the tip-of-the-tongue (TOT) phenomenon a possibility to occur of TOT states coursed by neural network interneuron links' disruption has been studied. Using a numerical example it was found that TOTs coursed by interneron links' disruption are in (1.5 + - 0.3)x1000 times less probable then those coursed by irrelevant (incomplete) neural network localization. It was shown that delayed TOT states' etiology cannot be related to neural network interneuron links' disruption.",
        "published": "2002-04-04T18:59:49Z",
        "link": "http://arxiv.org/abs/cs/0204008v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "q-bio.NC",
            "q-bio.QM",
            "H.3.3; I.2.7; J.4"
        ]
    },
    {
        "title": "Seven Dimensions of Portability for Language Documentation and   Description",
        "authors": [
            "Steven Bird",
            "Gary Simons"
        ],
        "summary": "The process of documenting and describing the world's languages is undergoing radical transformation with the rapid uptake of new digital technologies for capture, storage, annotation and dissemination. However, uncritical adoption of new tools and technologies is leading to resources that are difficult to reuse and which are less portable than the conventional printed resources they replace. We begin by reviewing current uses of software tools and digital technologies for language documentation and description. This sheds light on how digital language documentation and description are created and managed, leading to an analysis of seven portability problems under the following headings: content, format, discovery, access, citation, preservation and rights. After characterizing each problem we provide a series of value statements, and this provides the framework for a broad range of best practice recommendations.",
        "published": "2002-04-10T13:52:19Z",
        "link": "http://arxiv.org/abs/cs/0204020v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.3.7; I.2.7; J.5"
        ]
    },
    {
        "title": "Annotation Graphs and Servers and Multi-Modal Resources: Infrastructure   for Interdisciplinary Education, Research and Development",
        "authors": [
            "Christopher Cieri",
            "Steven Bird"
        ],
        "summary": "Annotation graphs and annotation servers offer infrastructure to support the analysis of human language resources in the form of time-series data such as text, audio and video. This paper outlines areas of common need among empirical linguists and computational linguists. After reviewing examples of data and tools used or under development for each of several areas, it proposes a common framework for future tool development, data annotation and resource sharing based upon annotation graphs and servers.",
        "published": "2002-04-10T15:37:26Z",
        "link": "http://arxiv.org/abs/cs/0204022v1",
        "categories": [
            "cs.CL",
            "H.2.4; H.5.3; H.5.5; I.2.7"
        ]
    },
    {
        "title": "Computational Phonology",
        "authors": [
            "Steven Bird"
        ],
        "summary": "Phonology, as it is practiced, is deeply computational. Phonological analysis is data-intensive and the resulting models are nothing other than specialized data structures and algorithms. In the past, phonological computation - managing data and developing analyses - was done manually with pencil and paper. Increasingly, with the proliferation of affordable computers, IPA fonts and drawing software, phonologists are seeking to move their computation work online. Computational Phonology provides the theoretical and technological framework for this migration, building on methodologies and tools from computational linguistics. This piece consists of an apology for computational phonology, a history, and an overview of current research.",
        "published": "2002-04-10T15:49:24Z",
        "link": "http://arxiv.org/abs/cs/0204023v1",
        "categories": [
            "cs.CL",
            "I.2.7; J.5"
        ]
    },
    {
        "title": "Phonology",
        "authors": [
            "Steven Bird"
        ],
        "summary": "Phonology is the systematic study of the sounds used in language, their internal structure, and their composition into syllables, words and phrases. Computational phonology is the application of formal and computational techniques to the representation and processing of phonological information. This chapter will present the fundamentals of descriptive phonology along with a brief overview of computational phonology.",
        "published": "2002-04-11T11:22:43Z",
        "link": "http://arxiv.org/abs/cs/0204025v1",
        "categories": [
            "cs.CL",
            "I.2.7; J.5"
        ]
    },
    {
        "title": "Integrating selectional preferences in WordNet",
        "authors": [
            "Eneko Agirre",
            "David Martinez"
        ],
        "summary": "Selectional preference learning methods have usually focused on word-to-class relations, e.g., a verb selects as its subject a given nominal class. This paper extends previous statistical models to class-to-class preferences, and presents a model that learns selectional preferences for classes of verbs, together with an algorithm to integrate the learned preferences in WordNet. The theoretical motivation is twofold: different senses of a verb may have different preferences, and classes of verbs may share preferences. On the practical side, class-to-class selectional preferences can be learned from untagged corpora (the same as word-to-class), they provide selectional preferences for less frequent word senses via inheritance, and more important, they allow for easy integration in WordNet. The model is trained on subject-verb and object-verb relationships extracted from a small corpus disambiguated with WordNet senses. Examples are provided illustrating that the theoretical motivations are well founded, and showing that the approach is feasible. Experimental results on a word sense disambiguation task are also provided.",
        "published": "2002-04-11T16:41:28Z",
        "link": "http://arxiv.org/abs/cs/0204027v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Querying Databases of Annotated Speech",
        "authors": [
            "Steve Cassidy",
            "Steven Bird"
        ],
        "summary": "Annotated speech corpora are databases consisting of signal data along with time-aligned symbolic `transcriptions'. Such databases are typically multidimensional, heterogeneous and dynamic. These properties present a number of tough challenges for representation and query. The temporal nature of the data adds an additional layer of complexity. This paper presents and harmonises two independent efforts to model annotated speech databases, one at Macquarie University and one at the University of Pennsylvania. Various query languages are described, along with illustrative applications to a variety of analytical problems. The research reported here forms a part of several ongoing projects to develop platform-independent open-source tools for creating, browsing, searching, querying and transforming linguistic databases, and to disseminate large linguistic databases over the internet.",
        "published": "2002-04-11T16:43:57Z",
        "link": "http://arxiv.org/abs/cs/0204026v1",
        "categories": [
            "cs.CL",
            "cs.DB",
            "H.2.3; H.2.4; H.5.5; I.2.7; J.5"
        ]
    },
    {
        "title": "Decision Lists for English and Basque",
        "authors": [
            "Eneko Agirre",
            "David Martinez"
        ],
        "summary": "In this paper we describe the systems we developed for the English (lexical and all-words) and Basque tasks. They were all supervised systems based on Yarowsky's Decision Lists. We used Semcor for training in the English all-words task. We defined different feature sets for each language. For Basque, in order to extract all the information from the text, we defined features that have not been used before in the literature, using a morphological analyzer. We also implemented systems that selected automatically good features and were able to obtain a prefixed precision (85%) at the cost of coverage. The systems that used all the features were identified as BCU-ehu-dlist-all and the systems that selected some features as BCU-ehu-dlist-best.",
        "published": "2002-04-12T07:41:55Z",
        "link": "http://arxiv.org/abs/cs/0204028v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "The Basque task: did systems perform in the upperbound?",
        "authors": [
            "Eneko Agirre",
            "Elena Garcia",
            "Mikel Lersundi",
            "David Martinez",
            "Eli Pociello"
        ],
        "summary": "In this paper we describe the Senseval 2 Basque lexical-sample task. The task comprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from Euskal Hiztegia, the main Basque dictionary. Most examples were taken from the Egunkaria newspaper. The method used to hand-tag the examples produced low inter-tagger agreement (75%) before arbitration. The four competing systems attained results well above the most frequent baseline and the best system scored 75% precision at 100% coverage. The paper includes an analysis of the tagging procedure used, as well as the performance of the competing systems. In particular, we argue that inter-tagger agreement is not a real upperbound for the Basque WSD task.",
        "published": "2002-04-12T07:49:32Z",
        "link": "http://arxiv.org/abs/cs/0204029v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Memory-Based Shallow Parsing",
        "authors": [
            "Erik F. Tjong Kim Sang"
        ],
        "summary": "We present memory-based learning approaches to shallow parsing and apply these to five tasks: base noun phrase identification, arbitrary base phrase recognition, clause detection, noun phrase parsing and full parsing. We use feature selection techniques and system combination methods for improving the performance of the memory-based learner. Our approach is evaluated on standard data sets and the results are compared with that of other systems. This reveals that our approach works well for base phrase identification while its application towards recognizing embedded structures leaves some room for improvement.",
        "published": "2002-04-24T14:48:31Z",
        "link": "http://arxiv.org/abs/cs/0204049v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Unsupervised discovery of morphologically related words based on   orthographic and semantic similarity",
        "authors": [
            "Marco Baroni",
            "Johannes Matiasek",
            "Harald Trost"
        ],
        "summary": "We present an algorithm that takes an unannotated corpus as its input, and returns a ranked list of probable morphologically related pairs as its output. The algorithm tries to discover morphologically related pairs by looking for pairs that are both orthographically and semantically similar, where orthographic similarity is measured in terms of minimum edit distance, and semantic similarity is measured in terms of mutual information. The procedure does not rely on a morpheme concatenation model, nor on distributional properties of word substrings (such as affix frequency). Experiments with German and English input give encouraging results, both in terms of precision (proportion of good pairs found at various cutoff points of the ranked list), and in terms of a qualitative analysis of the types of morphological patterns discovered by the algorithm.",
        "published": "2002-05-08T14:39:19Z",
        "link": "http://arxiv.org/abs/cs/0205006v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Mostly-Unsupervised Statistical Segmentation of Japanese Kanji Sequences",
        "authors": [
            "Rie Kubota Ando",
            "Lillian Lee"
        ],
        "summary": "Given the lack of word delimiters in written Japanese, word segmentation is generally considered a crucial first step in processing Japanese texts. Typical Japanese segmentation algorithms rely either on a lexicon and syntactic analysis or on pre-segmented data; but these are labor-intensive, and the lexico-syntactic techniques are vulnerable to the unknown word problem. In contrast, we introduce a novel, more robust statistical method utilizing unsegmented training data. Despite its simplicity, the algorithm yields performance on long kanji sequences comparable to and sometimes surpassing that of state-of-the-art morphological analyzers over a variety of error metrics. The algorithm also outperforms another mostly-unsupervised statistical algorithm previously proposed for Chinese.   Additionally, we present a two-level annotation scheme for Japanese to incorporate multiple segmentation granularities, and introduce two novel evaluation metrics, both based on the notion of a compatible bracket, that can account for multiple granularities simultaneously.",
        "published": "2002-05-10T18:55:25Z",
        "link": "http://arxiv.org/abs/cs/0205009v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Ellogon: A New Text Engineering Platform",
        "authors": [
            "Georgios Petasis",
            "Vangelis Karkaletsis",
            "Georgios Paliouras",
            "Ion Androutsopoulos",
            "Constantine D. Spyropoulos"
        ],
        "summary": "This paper presents Ellogon, a multi-lingual, cross-platform, general-purpose text engineering environment. Ellogon was designed in order to aid both researchers in natural language processing, as well as companies that produce language engineering systems for the end-user. Ellogon provides a powerful TIPSTER-based infrastructure for managing, storing and exchanging textual data, embedding and managing text processing components as well as visualising textual data and their associated linguistic information. Among its key features are full Unicode support, an extensive multi-lingual graphical user interface, its modular architecture and the reduced hardware requirements.",
        "published": "2002-05-13T11:18:08Z",
        "link": "http://arxiv.org/abs/cs/0205017v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Bootstrapping Structure into Language: Alignment-Based Learning",
        "authors": [
            "Menno M. van Zaanen"
        ],
        "summary": "This thesis introduces a new unsupervised learning framework, called Alignment-Based Learning, which is based on the alignment of sentences and Harris's (1951) notion of substitutability. Instances of the framework can be applied to an untagged, unstructured corpus of natural language sentences, resulting in a labelled, bracketed version of that corpus.   Firstly, the framework aligns all sentences in the corpus in pairs, resulting in a partition of the sentences consisting of parts of the sentences that are equal in both sentences and parts that are unequal. Unequal parts of sentences can be seen as being substitutable for each other, since substituting one unequal part for the other results in another valid sentence. The unequal parts of the sentences are thus considered to be possible (possibly overlapping) constituents, called hypotheses.   Secondly, the selection learning phase considers all hypotheses found by the alignment learning phase and selects the best of these. The hypotheses are selected based on the order in which they were found, or based on a probabilistic function.   The framework can be extended with a grammar extraction phase. This extended framework is called parseABL. Instead of returning a structured version of the unstructured input corpus, like the ABL system, this system also returns a stochastic context-free or tree substitution grammar.   Different instances of the framework have been tested on the English ATIS corpus, the Dutch OVIS corpus and the Wall Street Journal corpus. One of the interesting results, apart from the encouraging numerical results, is that all instances can (and do) learn recursive structures.",
        "published": "2002-05-16T12:35:00Z",
        "link": "http://arxiv.org/abs/cs/0205025v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Monads for natural language semantics",
        "authors": [
            "Chung-chieh Shan"
        ],
        "summary": "Accounts of semantic phenomena often involve extending types of meanings and revising composition rules at the same time. The concept of monads allows many such accounts -- for intensionality, variable binding, quantification and focus -- to be stated uniformly and compositionally.",
        "published": "2002-05-17T08:24:56Z",
        "link": "http://arxiv.org/abs/cs/0205026v1",
        "categories": [
            "cs.CL",
            "cs.PL",
            "I.2.7; D.3.1; F.3.2"
        ]
    },
    {
        "title": "A variable-free dynamic semantics",
        "authors": [
            "Chung-chieh Shan"
        ],
        "summary": "I propose a variable-free treatment of dynamic semantics. By \"dynamic semantics\" I mean analyses of donkey sentences (\"Every farmer who owns a donkey beats it\") and other binding and anaphora phenomena in natural language where meanings of constituents are updates to information states, for instance as proposed by Groenendijk and Stokhof. By \"variable-free\" I mean denotational semantics in which functional combinators replace variable indices and assignment functions, for instance as advocated by Jacobson.   The new theory presented here achieves a compositional treatment of dynamic anaphora that does not involve assignment functions, and separates the combinatorics of variable-free semantics from the particular linguistic phenomena it treats. Integrating variable-free semantics and dynamic semantics gives rise to interactions that make new empirical predictions, for example \"donkey weak crossover\" effects.",
        "published": "2002-05-17T09:33:52Z",
        "link": "http://arxiv.org/abs/cs/0205027v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "NLTK: The Natural Language Toolkit",
        "authors": [
            "Edward Loper",
            "Steven Bird"
        ],
        "summary": "NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.",
        "published": "2002-05-17T12:51:00Z",
        "link": "http://arxiv.org/abs/cs/0205028v1",
        "categories": [
            "cs.CL",
            "D.2.6; I.2.7; J.5; K.3.2"
        ]
    },
    {
        "title": "Unsupervised Discovery of Morphemes",
        "authors": [
            "Mathias Creutz",
            "Krista Lagus"
        ],
        "summary": "We present two methods for unsupervised segmentation of words into morpheme-like units. The model utilized is especially suited for languages with a rich morphology, such as Finnish. The first method is based on the Minimum Description Length (MDL) principle and works online. In the second method, Maximum Likelihood (ML) optimization is used. The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis. Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current state-of-the-art system.",
        "published": "2002-05-21T14:37:22Z",
        "link": "http://arxiv.org/abs/cs/0205057v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Bootstrapping Lexical Choice via Multiple-Sequence Alignment",
        "authors": [
            "Regina Barzilay",
            "Lillian Lee"
        ],
        "summary": "An important component of any generation system is the mapping dictionary, a lexicon of elementary semantic expressions and corresponding natural language realizations. Typically, labor-intensive knowledge-based methods are used to construct the dictionary. We instead propose to acquire it automatically via a novel multiple-pass algorithm employing multiple-sequence alignment, a technique commonly used in bioinformatics. Crucially, our method leverages latent information contained in multi-parallel corpora -- datasets that supply several verbalizations of the corresponding semantics rather than just one.   We used our techniques to generate natural language versions of computer-generated mathematical proofs, with good results on both a per-component and overall-output basis. For example, in evaluations involving a dozen human judges, our system produced output whose readability and faithfulness to the semantic input rivaled that of a traditional generation system.",
        "published": "2002-05-25T21:32:09Z",
        "link": "http://arxiv.org/abs/cs/0205065v1",
        "categories": [
            "cs.CL",
            "1.2.7"
        ]
    },
    {
        "title": "Evaluating the Effectiveness of Ensembles of Decision Trees in   Disambiguating Senseval Lexical Samples",
        "authors": [
            "Ted Pedersen"
        ],
        "summary": "This paper presents an evaluation of an ensemble--based system that participated in the English and Spanish lexical sample tasks of Senseval-2. The system combines decision trees of unigrams, bigrams, and co--occurrences into a single classifier. The analysis is extended to include the Senseval-1 data.",
        "published": "2002-05-27T18:42:10Z",
        "link": "http://arxiv.org/abs/cs/0205067v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Assessing System Agreement and Instance Difficulty in the Lexical Sample   Tasks of Senseval-2",
        "authors": [
            "Ted Pedersen"
        ],
        "summary": "This paper presents a comparative evaluation among the systems that participated in the Spanish and English lexical sample tasks of Senseval-2. The focus is on pairwise comparisons among systems to assess the degree to which they agree, and on measuring the difficulty of the test instances included in these tasks.",
        "published": "2002-05-27T18:49:01Z",
        "link": "http://arxiv.org/abs/cs/0205068v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Machine Learning with Lexical Features: The Duluth Approach to   Senseval-2",
        "authors": [
            "Ted Pedersen"
        ],
        "summary": "This paper describes the sixteen Duluth entries in the Senseval-2 comparative exercise among word sense disambiguation systems. There were eight pairs of Duluth systems entered in the Spanish and English lexical sample tasks. These are all based on standard machine learning algorithms that induce classifiers from sense-tagged training text where the context in which ambiguous words occur are represented by simple lexical features. These are highly portable, robust methods that can serve as a foundation for more tailored approaches.",
        "published": "2002-05-27T18:57:11Z",
        "link": "http://arxiv.org/abs/cs/0205069v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Thumbs up? Sentiment Classification using Machine Learning Techniques",
        "authors": [
            "Bo Pang",
            "Lillian Lee",
            "Shivakumar Vaithyanathan"
        ],
        "summary": "We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.",
        "published": "2002-05-28T02:01:55Z",
        "link": "http://arxiv.org/abs/cs/0205070v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.7; I.2.6"
        ]
    },
    {
        "title": "Unsupervised Learning of Morphology without Morphemes",
        "authors": [
            "Sylvain Neuvel",
            "Sean A. Fulop"
        ],
        "summary": "The first morphological learner based upon the theory of Whole Word Morphology Ford et al. (1997) is outlined, and preliminary evaluation results are presented. The program, Whole Word Morphologizer, takes a POS-tagged lexicon as input, induces morphological relationships without attempting to discover or identify morphemes, and is then able to generate new words beyond the learning sample. The accuracy (precision) of the generated new words is as high as 80% using the pure Whole Word theory, and 92% after a post-hoc adjustment is added to the routine.",
        "published": "2002-05-29T17:48:48Z",
        "link": "http://arxiv.org/abs/cs/0205072v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "Using the Annotated Bibliography as a Resource for Indicative   Summarization",
        "authors": [
            "Min-Yen Kan",
            "Judith L. Klavans",
            "Kathleen R. McKeown"
        ],
        "summary": "We report on a language resource consisting of 2000 annotated bibliography entries, which is being analyzed as part of our research on indicative document summarization. We show how annotated bibliographies cover certain aspects of summarization that have not been well-covered by other summary corpora, and motivate why they constitute an important form to study for information retrieval. We detail our methodology for collecting the corpus, and overview our document feature markup that we introduced to facilitate summary analysis. We present the characteristics of the corpus, methods of collection, and show its use in finding the distribution of types of information included in indicative summaries and their relative ordering within the summaries.",
        "published": "2002-06-04T14:48:48Z",
        "link": "http://arxiv.org/abs/cs/0206007v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "I.2.7"
        ]
    },
    {
        "title": "A Method for Open-Vocabulary Speech-Driven Text Retrieval",
        "authors": [
            "Atsushi Fujii",
            "Katunobu Itou",
            "Tetsuya Ishikawa"
        ],
        "summary": "While recent retrieval techniques do not limit the number of index terms, out-of-vocabulary (OOV) words are crucial in speech recognition. Aiming at retrieving information with spoken queries, we fill the gap between speech recognition and text retrieval in terms of the vocabulary size. Given a spoken query, we generate a transcription and detect OOV words through speech recognition. We then correspond detected OOV words to terms indexed in a target collection to complete the transcription, and search the collection for documents relevant to the completed transcription. We show the effectiveness of our method by way of experiments.",
        "published": "2002-06-09T10:07:36Z",
        "link": "http://arxiv.org/abs/cs/0206014v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.3; H.5.1"
        ]
    },
    {
        "title": "Japanese/English Cross-Language Information Retrieval: Exploration of   Query Translation and Transliteration",
        "authors": [
            "Atsushi Fujii",
            "Tetsuya Ishikawa"
        ],
        "summary": "Cross-language information retrieval (CLIR), where queries and documents are in different languages, has of late become one of the major topics within the information retrieval community. This paper proposes a Japanese/English CLIR system, where we combine a query translation and retrieval modules. We currently target the retrieval of technical documents, and therefore the performance of our system is highly dependent on the quality of the translation of technical terms. However, the technical term translation is still problematic in that technical terms are often compound words, and thus new terms are progressively created by combining existing base words. In addition, Japanese often represents loanwords based on its special phonogram. Consequently, existing dictionaries find it difficult to achieve sufficient coverage. To counter the first problem, we produce a Japanese/English dictionary for base words, and translate compound words on a word-by-word basis. We also use a probabilistic method to resolve translation ambiguity. For the second problem, we use a transliteration method, which corresponds words unlisted in the base word dictionary to their phonetic equivalents in the target language. We evaluate our system using a test collection for CLIR, and show that both the compound word translation and transliteration methods improve the system performance.",
        "published": "2002-06-09T10:53:49Z",
        "link": "http://arxiv.org/abs/cs/0206015v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Interleaved semantic interpretation in environment-based parsing",
        "authors": [
            "William Schuler"
        ],
        "summary": "This paper extends a polynomial-time parsing algorithm that resolves structural ambiguity in input to a speech-based user interface by calculating and comparing the denotations of rival constituents, given some model of the interfaced application environment (Schuler 2001). The algorithm is extended to incorporate a full set of logical operators, including quantifiers and conjunctions, into this calculation without increasing the complexity of the overall algorithm beyond polynomial time, both in terms of the length of the input and the number of entities in the environment model.",
        "published": "2002-06-18T15:11:52Z",
        "link": "http://arxiv.org/abs/cs/0206026v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "I.2.7; H.2.5"
        ]
    },
    {
        "title": "A Probabilistic Method for Analyzing Japanese Anaphora Integrating Zero   Pronoun Detection and Resolution",
        "authors": [
            "Kazuhiro Seki",
            "Atsushi Fujii",
            "Tetsuya Ishikawa"
        ],
        "summary": "This paper proposes a method to analyze Japanese anaphora, in which zero pronouns (omitted obligatory cases) are used to refer to preceding entities (antecedents). Unlike the case of general coreference resolution, zero pronouns have to be detected prior to resolution because they are not expressed in discourse. Our method integrates two probability parameters to perform zero pronoun detection and resolution in a single framework. The first parameter quantifies the degree to which a given case is a zero pronoun. The second parameter quantifies the degree to which a given entity is the antecedent for a detected zero pronoun. To compute these parameters efficiently, we use corpora with/without annotations of anaphoric relations. We show the effectiveness of our method by way of experiments.",
        "published": "2002-06-20T07:13:59Z",
        "link": "http://arxiv.org/abs/cs/0206030v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Applying a Hybrid Query Translation Method to Japanese/English   Cross-Language Patent Retrieval",
        "authors": [
            "Masatoshi Fukui",
            "Shigeto Higuchi",
            "Youichi Nakatani",
            "Masao Tanaka",
            "Atsushi Fujii",
            "Tetsuya Ishikawa"
        ],
        "summary": "This paper applies an existing query translation method to cross-language patent retrieval. In our method, multiple dictionaries are used to derive all possible translations for an input query, and collocational statistics are used to resolve translation ambiguity. We used Japanese/English parallel patent abstracts to perform comparative experiments, where our method outperformed a simple dictionary-based query translation method, and achieved 76% of monolingual retrieval in terms of average precision.",
        "published": "2002-06-24T07:46:06Z",
        "link": "http://arxiv.org/abs/cs/0206034v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "PRIME: A System for Multi-lingual Patent Retrieval",
        "authors": [
            "Shigeto Higuchi",
            "Masatoshi Fukui",
            "Atsushi Fujii",
            "Tetsuya Ishikawa"
        ],
        "summary": "Given the growing number of patents filed in multiple countries, users are interested in retrieving patents across languages. We propose a multi-lingual patent retrieval system, which translates a user query into the target language, searches a multilingual database for patents relevant to the query, and improves the browsing efficiency by way of machine translation and clustering. Our system also extracts new translations from patent families consisting of comparable patents, to enhance the translation dictionary.",
        "published": "2002-06-24T08:00:45Z",
        "link": "http://arxiv.org/abs/cs/0206035v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Language Modeling for Multi-Domain Speech-Driven Text Retrieval",
        "authors": [
            "Katunobu Itou",
            "Atsushi Fujii",
            "Tetsuya Ishikawa"
        ],
        "summary": "We report experimental results associated with speech-driven text retrieval, which facilitates retrieving information in multiple domains with spoken queries. Since users speak contents related to a target collection, we produce language models used for speech recognition based on the target collection, so as to improve both the recognition and retrieval accuracy. Experiments using existing test collections combined with dictated queries showed the effectiveness of our method.",
        "published": "2002-06-24T08:38:36Z",
        "link": "http://arxiv.org/abs/cs/0206036v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.3; H.5.1"
        ]
    },
    {
        "title": "Speech-Driven Text Retrieval: Using Target IR Collections for   Statistical Language Model Adaptation in Speech Recognition",
        "authors": [
            "Atsushi Fujii",
            "Katunobu Itou",
            "Tetsuya Ishikawa"
        ],
        "summary": "Speech recognition has of late become a practical technology for real world applications. Aiming at speech-driven text retrieval, which facilitates retrieving information with spoken queries, we propose a method to integrate speech recognition and retrieval methods. Since users speak contents related to a target collection, we adapt statistical language models used for speech recognition based on the target collection, so as to improve both the recognition and retrieval accuracy. Experiments using existing test collections combined with dictated queries showed the effectiveness of our method.",
        "published": "2002-06-24T10:28:02Z",
        "link": "http://arxiv.org/abs/cs/0206037v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.3; H.5.1"
        ]
    },
    {
        "title": "Using eigenvectors of the bigram graph to infer morpheme identity",
        "authors": [
            "Mikhail Belkin",
            "John Goldsmith"
        ],
        "summary": "This paper describes the results of some experiments exploring statistical methods to infer syntactic behavior of words and morphemes from a raw corpus in an unsupervised fashion. It shares certain points in common with Brown et al (1992) and work that has grown out of that: it employs statistical techniques to analyze syntactic behavior based on what words occur adjacent to a given word. However, we use an eigenvector decomposition of a nearest-neighbor graph to produce a two-dimensional rendering of the words of a corpus in which words of the same syntactic category tend to form neighborhoods. We exploit this technique for extending the value of automatic learning of morphology. In particular, we look at the suffixes derived from a corpus by unsupervised learning of morphology, and we ask which of these suffixes have a consistent syntactic function (e.g., in English, -tion is primarily a mark of nouns, but -s marks both noun plurals and 3rd person present on verbs), and we determine that this method works well for this task.",
        "published": "2002-07-02T01:15:33Z",
        "link": "http://arxiv.org/abs/cs/0207002v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Analysis of Titles and Readers For Title Generation Centered on the   Readers",
        "authors": [
            "Yasuko Senda",
            "Yasusi Sinohara"
        ],
        "summary": "The title of a document has two roles, to give a compact summary and to lead the reader to read the document. Conventional title generation focuses on finding key expressions from the author's wording in the document to give a compact summary and pays little attention to the reader's interest. To make the title play its second role properly, it is indispensable to clarify the content (``what to say'') and wording (``how to say'') of titles that are effective to attract the target reader's interest. In this article, we first identify typical content and wording of titles aimed at general readers in a comparative study between titles of technical papers and headlines rewritten for newspapers. Next, we describe the results of a questionnaire survey on the effects of the content and wording of titles on the reader's interest. The survey of general and knowledgeable readers shows both common and different tendencies in interest.",
        "published": "2002-07-02T15:08:44Z",
        "link": "http://arxiv.org/abs/cs/0207003v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Efficient Deep Processing of Japanese",
        "authors": [
            "Melanie Siegel",
            "Emily M. Bender"
        ],
        "summary": "We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics. The grammar is created for use in real world applications, such that robustness and performance issues play an important role. It is connected to a POS tagging and word segmentation tool. This grammar is being developed in a multilingual context, requiring MRS structures that are easily comparable across languages.",
        "published": "2002-07-03T11:54:21Z",
        "link": "http://arxiv.org/abs/cs/0207005v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Question Answering over Unstructured Data without Domain Restrictions",
        "authors": [
            "Jochen L. Leidner"
        ],
        "summary": "Information needs are naturally represented as questions. Automatic Natural-Language Question Answering (NLQA) has only recently become a practical task on a larger scale and without domain constraints.   This paper gives a brief introduction to the field, its history and the impact of systematic evaluation competitions.   It is then demonstrated that an NLQA system for English can be built and evaluated in a very short time using off-the-shelf parsers and thesauri. The system is based on Robust Minimal Recursion Semantics (RMRS) and is portable with respect to the parser used as a frontend. It applies atomic term unification supported by question classification and WordNet lookup for semantic similarity matching of parsed question representation and free text.",
        "published": "2002-07-14T21:04:43Z",
        "link": "http://arxiv.org/abs/cs/0207058v2",
        "categories": [
            "cs.CL",
            "cs.IR",
            "I.2.7; H.3.1"
        ]
    },
    {
        "title": "A continuation semantics of interrogatives that accounts for Baker's   ambiguity",
        "authors": [
            "Chung-chieh Shan"
        ],
        "summary": "Wh-phrases in English can appear both raised and in-situ. However, only in-situ wh-phrases can take semantic scope beyond the immediately enclosing clause. I present a denotational semantics of interrogatives that naturally accounts for these two properties. It neither invokes movement or economy, nor posits lexical ambiguity between raised and in-situ occurrences of the same wh-phrase. My analysis is based on the concept of continuations. It uses a novel type system for higher-order continuations to handle wide-scope wh-phrases while remaining strictly compositional. This treatment sheds light on the combinatorics of interrogatives as well as other kinds of so-called A'-movement.",
        "published": "2002-07-18T23:27:01Z",
        "link": "http://arxiv.org/abs/cs/0207070v2",
        "categories": [
            "cs.CL",
            "cs.PL",
            "I.2.7"
        ]
    },
    {
        "title": "Using the DIFF Command for Natural Language Processing",
        "authors": [
            "Masaki Murata",
            "Hitoshi Isahara"
        ],
        "summary": "Diff is a software program that detects differences between two data sets and is useful in natural language processing. This paper shows several examples of the application of diff. They include the detection of differences between two different datasets, extraction of rewriting rules, merging of two different datasets, and the optimal matching of two different data sets. Since diff comes with any standard UNIX system, it is readily available and very easy to use. Our studies showed that diff is a practical tool for research into natural language processing.",
        "published": "2002-08-13T03:39:20Z",
        "link": "http://arxiv.org/abs/cs/0208020v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Evaluation of Coreference Rules on Complex Narrative Texts",
        "authors": [
            "Andrei Popescu-Belis",
            "Isabelle Robba"
        ],
        "summary": "This article studies the problem of assessing relevance to each of the rules of a reference resolution system. The reference solver described here stems from a formal model of reference and is integrated in a reference processing workbench. Evaluation of the reference resolution is essential, as it enables differential evaluation of individual rules. Numerical values of these measures are given, and discussed, for simple selection rules and other processing rules; such measures are then studied for numerical parameters.",
        "published": "2002-08-21T14:09:48Z",
        "link": "http://arxiv.org/abs/cs/0208035v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Three New Methods for Evaluating Reference Resolution",
        "authors": [
            "Andrei Popescu-Belis",
            "Isabelle Robba"
        ],
        "summary": "Reference resolution on extended texts (several thousand references) cannot be evaluated manually. An evaluation algorithm has been proposed for the MUC tests, using equivalence classes for the coreference relation. However, we show here that this algorithm is too indulgent, yielding good scores even for poor resolution strategies. We elaborate on the same formalism to propose two new evaluation algorithms, comparing them first with the MUC algorithm and giving then results on a variety of examples. A third algorithm using only distributional comparison of equivalence classes is finally described; it assesses the relative importance of the recall vs. precision errors.",
        "published": "2002-08-21T14:28:51Z",
        "link": "http://arxiv.org/abs/cs/0208036v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Cooperation between Pronoun and Reference Resolution for Unrestricted   Texts",
        "authors": [
            "Andrei Popescu-Belis",
            "Isabelle Robba"
        ],
        "summary": "Anaphora resolution is envisaged in this paper as part of the reference resolution process. A general open architecture is proposed, which can be particularized and configured in order to simulate some classic anaphora resolution methods. With the aim of improving pronoun resolution, the system takes advantage of elementary cues about characters of the text, which are represented through a particular data structure. In its most robust configuration, the system uses only a general lexicon, a local morpho-syntactic parser and a dictionary of synonyms. A short comparative corpus analysis shows that narrative texts are the most suitable for testing such a system.",
        "published": "2002-08-21T14:36:13Z",
        "link": "http://arxiv.org/abs/cs/0208037v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Reference Resolution Beyond Coreference: a Conceptual Frame and its   Application",
        "authors": [
            "Andrei Popescu-Belis",
            "Isabelle Robba",
            "Gerard Sabah"
        ],
        "summary": "A model for reference use in communication is proposed, from a representationist point of view. Both the sender and the receiver of a message handle representations of their common environment, including mental representations of objects. Reference resolution by a computer is viewed as the construction of object representations using referring expressions from the discourse, whereas often only coreference links between such expressions are looked for. Differences between these two approaches are discussed. The model has been implemented with elementary rules, and tested on complex narrative texts (hundreds to thousands of referring expressions). The results support the mental representations paradigm.",
        "published": "2002-08-21T14:43:18Z",
        "link": "http://arxiv.org/abs/cs/0208038v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A Chart-Parsing Algorithm for Efficient Semantic Analysis",
        "authors": [
            "Pascal Vaillant"
        ],
        "summary": "In some contexts, well-formed natural language cannot be expected as input to information or communication systems. In these contexts, the use of grammar-independent input (sequences of uninflected semantic units like e.g. language-independent icons) can be an answer to the users' needs. A semantic analysis can be performed, based on lexical semantic knowledge: it is equivalent to a dependency analysis with no syntactic or morphological clues. However, this requires that an intelligent system should be able to interpret this input with reasonable accuracy and in reasonable time. Here we propose a method allowing a purely semantic-based analysis of sequences of semantic units. It uses an algorithm inspired by the idea of ``chart parsing'' known in Natural Language Processing, which stores intermediate parsing results in order to bring the calculation time down. In comparison with using declarative logic programming - where the calculation time, left to a prolog engine, is hyperexponential -, this method brings the calculation time down to a polynomial time, where the order depends on the valency of the predicates.",
        "published": "2002-09-02T16:55:34Z",
        "link": "http://arxiv.org/abs/cs/0209002v1",
        "categories": [
            "cs.CL",
            "I.2.7; F.2.2"
        ]
    },
    {
        "title": "Rerendering Semantic Ontologies: Automatic Extensions to UMLS through   Corpus Analytics",
        "authors": [
            "J. Pustejovsky",
            "A. Rumshisky",
            "J. Castano"
        ],
        "summary": "In this paper, we discuss the utility and deficiencies of existing ontology resources for a number of language processing applications. We describe a technique for increasing the semantic type coverage of a specific ontology, the National Library of Medicine's UMLS, with the use of robust finite state methods used in conjunction with large-scale corpus analytics of the domain corpus. We call this technique \"semantic rerendering\" of the ontology. This research has been done in the context of Medstract, a joint Brandeis-Tufts effort aimed at developing tools for analyzing biomedical language (i.e., Medline), as well as creating targeted databases of bio-entities, biological relations, and pathway data for biological researchers. Motivating the current research is the need to have robust and reliable semantic typing of syntactic elements in the Medline corpus, in order to improve the overall performance of the information extraction applications mentioned above.",
        "published": "2002-09-03T05:28:56Z",
        "link": "http://arxiv.org/abs/cs/0209003v1",
        "categories": [
            "cs.CL",
            "I.2.7; J.3"
        ]
    },
    {
        "title": "The partition semantics of questions, syntactically",
        "authors": [
            "Chung-chieh Shan",
            "Balder D. ten Cate"
        ],
        "summary": "Groenendijk and Stokhof (1984, 1996; Groenendijk 1999) provide a logically attractive theory of the semantics of natural language questions, commonly referred to as the partition theory. Two central notions in this theory are entailment between questions and answerhood. For example, the question \"Who is going to the party?\" entails the question \"Is John going to the party?\", and \"John is going to the party\" counts as an answer to both. Groenendijk and Stokhof define these two notions in terms of partitions of a set of possible worlds.   We provide a syntactic characterization of entailment between questions and answerhood . We show that answers are, in some sense, exactly those formulas that are built up from instances of the question. This result lets us compare the partition theory with other approaches to interrogation -- both linguistic analyses, such as Hamblin's and Karttunen's semantics, and computational systems, such as Prolog. Our comparison separates a notion of answerhood into three aspects: equivalence (when two questions or answers are interchangeable), atomic answers (what instances of a question count as answers), and compound answers (how answers compose).",
        "published": "2002-09-04T20:11:14Z",
        "link": "http://arxiv.org/abs/cs/0209008v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LO",
            "F.4.1; I.2.3; I.2.7"
        ]
    },
    {
        "title": "Question answering: from partitions to Prolog",
        "authors": [
            "Balder D. ten Cate",
            "Chung-chieh Shan"
        ],
        "summary": "We implement Groenendijk and Stokhof's partition semantics of questions in a simple question answering algorithm. The algorithm is sound, complete, and based on tableau theorem proving. The algorithm relies on a syntactic characterization of answerhood: Any answer to a question is equivalent to some formula built up only from instances of the question. We prove this characterization by translating the logic of interrogation to classical predicate logic and applying Craig's interpolation theorem.",
        "published": "2002-09-04T20:46:59Z",
        "link": "http://arxiv.org/abs/cs/0209009v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LO",
            "F.4.1; I.2.3; I.2.7"
        ]
    },
    {
        "title": "Introduction to the CoNLL-2002 Shared Task: Language-Independent Named   Entity Recognition",
        "authors": [
            "Erik F. Tjong Kim Sang"
        ],
        "summary": "We describe the CoNLL-2002 shared task: language-independent named entity recognition. We give background information on the data sets and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.",
        "published": "2002-09-05T19:03:06Z",
        "link": "http://arxiv.org/abs/cs/0209010v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "An Algorithm for Pattern Discovery in Time Series",
        "authors": [
            "Cosma Rohilla Shalizi",
            "Kristina Lisa Shalizi",
            "James P. Crutchfield"
        ],
        "summary": "We present a new algorithm for discovering patterns in time series and other sequential data. We exhibit a reliable procedure for building the minimal set of hidden, Markovian states that is statistically capable of producing the behavior exhibited in the data -- the underlying process's causal states. Unlike conventional methods for fitting hidden Markov models (HMMs) to data, our algorithm makes no assumptions about the process's causal architecture (the number of hidden states and their transition structure), but rather infers it from the data. It starts with assumptions of minimal structure and introduces complexity only when the data demand it. Moreover, the causal states it infers have important predictive optimality properties that conventional HMM states lack. We introduce the algorithm, review the theory behind it, prove its asymptotic reliability, use large deviation theory to estimate its rate of convergence, and compare it to other algorithms which also construct HMMs from data. We also illustrate its behavior on an example process, and report selected numerical results from an implementation.",
        "published": "2002-10-29T00:33:26Z",
        "link": "http://arxiv.org/abs/cs/0210025v3",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2.6;H.1.1;E.4"
        ]
    },
    {
        "title": "Probabilistic Parsing Strategies",
        "authors": [
            "Mark-Jan Nederhof",
            "Giorgio Satta"
        ],
        "summary": "We present new results on the relation between purely symbolic context-free parsing strategies and their probabilistic counter-parts. Such parsing strategies are seen as constructions of push-down devices from grammars. We show that preservation of probability distribution is possible under two conditions, viz. the correct-prefix property and the property of strong predictiveness. These results generalize existing results in the literature that were obtained by considering parsing strategies in isolation. From our general results we also derive negative results on so-called generalized LR parsing.",
        "published": "2002-11-14T16:16:44Z",
        "link": "http://arxiv.org/abs/cs/0211017v1",
        "categories": [
            "cs.CL",
            "F.4.3; I.2.7"
        ]
    },
    {
        "title": "Answering Subcognitive Turing Test Questions: A Reply to French",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Robert French has argued that a disembodied computer is incapable of passing a Turing Test that includes subcognitive questions. Subcognitive questions are designed to probe the network of cultural and perceptual associations that humans naturally develop as we live, embodied and embedded in the world. In this paper, I show how it is possible for a disembodied computer to answer subcognitive questions appropriately, contrary to French's claim. My approach to answering subcognitive questions is to use statistical information extracted from a very large collection of text. In particular, I show how it is possible to answer a sample of subcognitive questions taken from French, by issuing queries to a search engine that indexes about 350 million Web pages. This simple algorithm may shed light on the nature of human (sub-) cognition, but the scope of this paper is limited to demonstrating that French is mistaken: a disembodied computer can answer subcognitive questions.",
        "published": "2002-12-09T13:09:10Z",
        "link": "http://arxiv.org/abs/cs/0212015v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Real numbers having ultimately periodic representations in abstract   numeration systems",
        "authors": [
            "P. Lecomte",
            "M. Rigo"
        ],
        "summary": "Using a genealogically ordered infinite regular language, we know how to represent an interval of R. Numbers having an ultimately periodic representation play a special role in classical numeration systems. The aim of this paper is to characterize the numbers having an ultimately periodic representation in generalized systems built on a regular language. The syntactical properties of these words are also investigated. Finally, we show the equivalence of the classical \"theta\"-expansions with our generalized representations in some special case related to a Pisot number \"theta\".",
        "published": "2002-12-10T12:46:25Z",
        "link": "http://arxiv.org/abs/cs/0212018v1",
        "categories": [
            "cs.CC",
            "cs.CL",
            "F.4.1; F.4.3"
        ]
    },
    {
        "title": "Learning Algorithms for Keyphrase Extraction",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Many academic journals ask their authors to provide a list of about five to fifteen keywords, to appear on the first page of each article. Since these key words are often phrases of two or more words, we prefer to call them keyphrases. There is a wide variety of tasks for which keyphrases are useful, as we discuss in this paper. We approach the problem of automatically extracting keyphrases from text as a supervised learning task. We treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Our first set of experiments applies the C4.5 decision tree induction algorithm to this learning task. We evaluate the performance of nine different configurations of C4.5. The second set of experiments applies the GenEx algorithm to the task. We developed the GenEx algorithm specifically for automatically extracting keyphrases from text. The experimental results support the claim that a custom-designed algorithm (GenEx), incorporating specialized procedural domain knowledge, can generate better keyphrases than a generalpurpose algorithm (C4.5). Subjective human evaluation of the keyphrases generated by Extractor suggests that about 80% of the keyphrases are acceptable to human readers. This level of performance should be satisfactory for a wide variety of applications.",
        "published": "2002-12-10T15:30:56Z",
        "link": "http://arxiv.org/abs/cs/0212020v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Unsupervised Language Acquisition: Theory and Practice",
        "authors": [
            "Alexander Clark"
        ],
        "summary": "In this thesis I present various algorithms for the unsupervised machine learning of aspects of natural languages using a variety of statistical models. The scientific object of the work is to examine the validity of the so-called Argument from the Poverty of the Stimulus advanced in favour of the proposition that humans have language-specific innate knowledge. I start by examining an a priori argument based on Gold's theorem, that purports to prove that natural languages cannot be learned, and some formal issues related to the choice of statistical grammars rather than symbolic grammars. I present three novel algorithms for learning various parts of natural languages: first, an algorithm for the induction of syntactic categories from unlabelled text using distributional information, that can deal with ambiguous and rare words; secondly, a set of algorithms for learning morphological processes in a variety of languages, including languages such as Arabic with non-concatenative morphology; thirdly an algorithm for the unsupervised induction of a context-free grammar from tagged text. I carefully examine the interaction between the various components, and show how these algorithms can form the basis for a empiricist model of language acquisition. I therefore conclude that the Argument from the Poverty of the Stimulus is unsupported by the evidence.",
        "published": "2002-12-10T21:59:15Z",
        "link": "http://arxiv.org/abs/cs/0212024v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised   Classification of Reviews",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \"subtle nuances\") and a negative semantic orientation when it has bad associations (e.g., \"very cavalier\"). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \"excellent\" minus the mutual information between the given phrase and the word \"poor\". A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.",
        "published": "2002-12-11T18:57:42Z",
        "link": "http://arxiv.org/abs/cs/0212032v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "I.2.6; I.2.7; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74%. PMI-IR is contrasted with Latent Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL questions. The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing).",
        "published": "2002-12-11T19:17:06Z",
        "link": "http://arxiv.org/abs/cs/0212033v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "I.2.6; I.2.7; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Hypernets -- Good (G)news for Gnutella",
        "authors": [
            "N. J. Gunther"
        ],
        "summary": "Criticism of Gnutella network scalability has rested on the bandwidth attributes of the original interconnection topology: a Cayley tree. Trees, in general, are known to have lower aggregate bandwidth than higher dimensional topologies e.g., hypercubes, meshes and tori. Gnutella was intended to support thousands to millions of peers. Studies of interconnection topologies in the literature, however, have focused on hardware implementations which are limited by cost to a few thousand nodes. Since the Gnutella network is virtual, hyper-topologies are relatively unfettered by such constraints. We present performance models for several plausible hyper-topologies and compare their query throughput up to millions of peers. The virtual hypercube and the virtual hypertorus are shown to offer near linear scalability subject to the number of peer TCP/IP connections that can be simultaneously kept open.",
        "published": "2002-02-16T21:46:14Z",
        "link": "http://arxiv.org/abs/cs/0202019v2",
        "categories": [
            "cs.PF",
            "cs.DC",
            "cs.IR",
            "cs.NI",
            "C.2.4; C.4; D.4.8"
        ]
    },
    {
        "title": "The structure of broad topics on the Web",
        "authors": [
            "Soumen Chakrabarti",
            "Mukul M. Joshi",
            "Kunal Punera",
            "David M. Pennock"
        ],
        "summary": "The Web graph is a giant social network whose properties have been measured and modeled extensively in recent years. Most such studies concentrate on the graph structure alone, and do not consider textual properties of the nodes. Consequently, Web communities have been characterized purely in terms of graph structure and not on page content. We propose that a topic taxonomy such as Yahoo! or the Open Directory provides a useful framework for understanding the structure of content-based clusters and communities. In particular, using a topic taxonomy and an automatic classifier, we can measure the background distribution of broad topics on the Web, and analyze the capability of recent random walk algorithms to draw samples which follow such distributions. In addition, we can measure the probability that a page about one broad topic will link to another broad topic. Extending this experiment, we can measure how quickly topic context is lost while walking randomly on the Web graph. Estimates of this topic mixing distance may explain why a global PageRank is still meaningful in the context of broad queries. In general, our measurements may prove valuable in the design of community-specific crawlers and link-based ranking systems.",
        "published": "2002-03-20T06:46:21Z",
        "link": "http://arxiv.org/abs/cs/0203024v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.5.4; H.5.3; H.1.0"
        ]
    },
    {
        "title": "Technology For Information Engineering (TIE): A New Way of Storing,   Retrieving and Analyzing Information",
        "authors": [
            "Jerzy Lewak"
        ],
        "summary": "The theoretical foundations of a new model and paradigm (called TIE) for data storage and access are introduced. Associations between data elements are stored in a single Matrix table, which is usually kept entirely in RAM for quick access. The model ties together a very intuitive \"guided\" GUI to the Matrix structure, allowing extremely easy complex searches through the data. Although it is an \"Associative Model\" in that it stores the data associations separately from the data itself, in contrast to other implementations of that model TIE guides the user to only the available information ensuring that every search is always fruitful. Very many diverse applications of the technology are reviewed.",
        "published": "2002-04-16T16:03:18Z",
        "link": "http://arxiv.org/abs/cs/0204038v1",
        "categories": [
            "cs.DB",
            "cs.IR",
            "E.0"
        ]
    },
    {
        "title": "Navigating the Small World Web by Textual Cues",
        "authors": [
            "Filippo Menczer"
        ],
        "summary": "Can a Web crawler efficiently locate an unknown relevant page? While this question is receiving much empirical attention due to its considerable commercial value in the search engine community [Cho98,Chakrabarti99,Menczer00,Menczer01], theoretical efforts to bound the performance of focused navigation have only exploited the link structure of the Web graph, neglecting other features [Kleinberg01,Adamic01,Kim02]. Here I investigate the connection between linkage and a content-induced topology of Web pages, suggesting that efficient paths can be discovered by decentralized navigation algorithms based on textual cues.",
        "published": "2002-04-26T22:45:30Z",
        "link": "http://arxiv.org/abs/cs/0204054v1",
        "categories": [
            "cs.IR",
            "cs.NI",
            "H.3.1; H.3.3; H.3.4; H.3.5"
        ]
    },
    {
        "title": "The Traits of the Personable",
        "authors": [
            "Naren Ramakrishnan"
        ],
        "summary": "Information personalization is fertile ground for application of AI techniques. In this article I relate personalization to the ability to capture partial information in an information-seeking interaction. The specific focus is on personalizing interactions at web sites. Using ideas from partial evaluation and explanation-based generalization, I present a modeling methodology for reasoning about personalization. This approach helps identify seven tiers of `personable traits' in web sites.",
        "published": "2002-05-14T19:25:07Z",
        "link": "http://arxiv.org/abs/cs/0205022v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "H.3.5; H.4.2; H.5.4; I.2.6; K.8"
        ]
    },
    {
        "title": "A Connection-Centric Survey of Recommender Systems Research",
        "authors": [
            "Saverio Perugini",
            "Marcos Andre Goncalves",
            "Edward A. Fox"
        ],
        "summary": "Recommender systems attempt to reduce information overload and retain customers by selecting a subset of items from a universal set based on user preferences. While research in recommender systems grew out of information retrieval and filtering, the topic has steadily advanced into a legitimate and challenging research area of its own. Recommender systems have traditionally been studied from a content-based filtering vs. collaborative design perspective. Recommendations, however, are not delivered within a vacuum, but rather cast within an informal community of users and social context. Therefore, ultimately all recommender systems make connections among people and thus should be surveyed from such a perspective. This viewpoint is under-emphasized in the recommender systems literature. We therefore take a connection-oriented viewpoint toward recommender systems research. We posit that recommendation has an inherently social element and is ultimately intended to connect people either directly as a result of explicit user modeling or indirectly through the discovery of relationships implicit in extant data. Thus, recommender systems are characterized by how they model users to bring people together: explicitly or implicitly. Finally, user modeling and the connection-centric viewpoint raise broadening and social issues--such as evaluation, targeting, and privacy and trust--which we also briefly address.",
        "published": "2002-05-22T08:36:32Z",
        "link": "http://arxiv.org/abs/cs/0205059v2",
        "categories": [
            "cs.IR",
            "cs.HC",
            "A.1;H.1.0;H.1.2;H.3.0;H.3.3;H.3.4;H.3.5;H.4.2;H.5.2;H.5.4"
        ]
    },
    {
        "title": "A Scalable Architecture for Harvest-Based Digital Libraries - The   ODU/Southampton Experiments",
        "authors": [
            "Xiaoming Liu",
            "Tim Brody",
            "Stevan Harnad",
            "Les Carr",
            "Kurt Maly",
            "Mohammad Zubair",
            "Michael L. Nelson"
        ],
        "summary": "This paper discusses the requirements of current and emerging applications based on the Open Archives Initiative (OAI) and emphasizes the need for a common infrastructure to support them. Inspired by HTTP proxy, cache, gateway and web service concepts, a design for a scalable and reliable infrastructure that aims at satisfying these requirements is presented. Moreover it is shown how various applications can exploit the services included in the proposed infrastructure. The paper concludes by discussing the current status of several prototype implementations.",
        "published": "2002-05-28T15:32:55Z",
        "link": "http://arxiv.org/abs/cs/0205071v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.7"
        ]
    },
    {
        "title": "Transforming the World Wide Web into a Complexity-Based Semantic Network",
        "authors": [
            "M. Marko",
            "M. A. Porter",
            "A. Probst",
            "C. Gershenson",
            "A. Das"
        ],
        "summary": "The aim of this paper is to introduce the idea of the Semantic Web to the Complexity community and set a basic ground for a project resulting in creation of Internet-based semantic network of Complexity-related information providers. Implementation of the Semantic Web technology would be of mutual benefit to both the participants and users and will confirm self-referencing power of the community to apply the products of its own research to itself. We first explain the logic of the transition and discuss important notions associated with the Semantic Web technology. We then present a brief outline of the project milestones.",
        "published": "2002-05-31T18:44:36Z",
        "link": "http://arxiv.org/abs/cs/0205080v2",
        "categories": [
            "cs.NI",
            "cs.IR",
            "I.2.4"
        ]
    },
    {
        "title": "Knowledge management for enterprises (Wissensmanagement fuer   Unternehmen)",
        "authors": [
            "Wolfgang Eiden"
        ],
        "summary": "Although knowledge is one of the most valuable resource of enterprises and an important production and competition factor, this intellectual potential is often used (or maintained) only inadequate by the enterprises. Therefore, in a globalised and growing market the optimal usage of existing knowledge represents a key factor for enterprises of the future. Here, knowledge management systems should engage facilitating. Because geographically far distributed establishments cause, however, a distributed system, this paper should uncover the spectrum connected with it and present a possible basic approach which is based on ontologies and modern, platform independent technologies. Last but not least this attempt, as well as general questions of the knowledge management, are discussed.",
        "published": "2002-06-19T22:13:41Z",
        "link": "http://arxiv.org/abs/cs/0206028v2",
        "categories": [
            "cs.IR",
            "cs.AI",
            "H.3.0; I.2.4"
        ]
    },
    {
        "title": "Question Answering over Unstructured Data without Domain Restrictions",
        "authors": [
            "Jochen L. Leidner"
        ],
        "summary": "Information needs are naturally represented as questions. Automatic Natural-Language Question Answering (NLQA) has only recently become a practical task on a larger scale and without domain constraints.   This paper gives a brief introduction to the field, its history and the impact of systematic evaluation competitions.   It is then demonstrated that an NLQA system for English can be built and evaluated in a very short time using off-the-shelf parsers and thesauri. The system is based on Robust Minimal Recursion Semantics (RMRS) and is portable with respect to the parser used as a frontend. It applies atomic term unification supported by question classification and WordNet lookup for semantic similarity matching of parsed question representation and free text.",
        "published": "2002-07-14T21:04:43Z",
        "link": "http://arxiv.org/abs/cs/0207058v2",
        "categories": [
            "cs.CL",
            "cs.IR",
            "I.2.7; H.3.1"
        ]
    },
    {
        "title": "Activities, Context and Ubiquitous Computing",
        "authors": [
            "Paul Prekop",
            "Mark Burnett"
        ],
        "summary": "Context and context-awareness provides computing environments with the ability to usefully adapt the services or information they provide. It is the ability to implicitly sense and automatically derive the user needs that separates context-aware applications from traditionally designed applications, and this makes them more attentive, responsive, and aware of their user's identity, and their user's environment. This paper argues that context-aware applications capable of supporting complex, cognitive activities can be built from a model of context called Activity-Centric Context. A conceptual model of Activity-Centric context is presented. The model is illustrated via a detailed example.",
        "published": "2002-09-19T06:53:51Z",
        "link": "http://arxiv.org/abs/cs/0209021v1",
        "categories": [
            "cs.IR",
            "F.M; H1; H4"
        ]
    },
    {
        "title": "Quantum Pattern Recognition",
        "authors": [
            "Carlo A. Trugenberger"
        ],
        "summary": "I review and expand the model of quantum associative memory that I have recently proposed. In this model binary patterns of n bits are stored in the quantum superposition of the appropriate subset of the computational basis of n qbits. Information can be retrieved by performing an input-dependent rotation of the memory quantum state within this subset and measuring the resulting state. The amplitudes of this rotated memory state are peaked on those stored patterns which are closest in Hamming distance to the input, resulting in a high probability of measuring a memory pattern very similar to it. The accuracy of pattern recall can be tuned by adjusting a parameter playing the role of an effective temperature. This model solves the well-known capacity shortage problem of classical associative memories, providing an exponential improvement in capacity. The price to pay is the probabilistic nature of information retrieval, a feature that, however, this model shares with our own brain.",
        "published": "2002-10-25T13:42:53Z",
        "link": "http://arxiv.org/abs/quant-ph/0210176v2",
        "categories": [
            "quant-ph",
            "cond-mat.dis-nn",
            "cs.IR",
            "nlin.AO",
            "q-bio.NC"
        ]
    },
    {
        "title": "An Approach to Automatic Indexing of Scientific Publications in High   Energy Physics for Database SPIRES HEP",
        "authors": [
            "A. V. Averin",
            "L. A. Vassilevskaya"
        ],
        "summary": "We introduce an approach to automatic indexing of e-prints based on a pattern-matching technique making extensive use of an Associative Patterns Dictionary (APD), developed by us. Entries in the APD consist of natural language phrases with the same semantic interpretation as a set of keywords from a controlled vocabulary. The method also allows to recognize within e-prints formulae written in TeX notations that might also appear as keywords. We present an automatic indexing system, AUTEX, which we have applied to keyword index e-prints in selected areas in high energy physics (HEP) making use of the DESY-HEPI thesaurus as a controlled vocabulary.",
        "published": "2002-11-28T17:33:19Z",
        "link": "http://arxiv.org/abs/cs/0211041v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.1; H.3.2; H.3.6; H.3.7"
        ]
    },
    {
        "title": "Mining the Web for Lexical Knowledge to Improve Keyphrase Extraction:   Learning from Labeled and Unlabeled Data",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Keyphrases are useful for a variety of purposes, including summarizing, indexing, labeling, categorizing, clustering, highlighting, browsing, and searching. The task of automatic keyphrase extraction is to select keyphrases from within the text of a given document. Automatic keyphrase extraction makes it feasible to generate keyphrases for the huge number of documents that do not have manually assigned keyphrases. Good performance on this task has been obtained by approaching it as a supervised learning problem. An input document is treated as a set of candidate phrases that must be classified as either keyphrases or non-keyphrases. To classify a candidate phrase as a keyphrase, the most important features (attributes) appear to be the frequency and location of the candidate phrase in the document. Recent work has demonstrated that it is also useful to know the frequency of the candidate phrase as a manually assigned keyphrase for other documents in the same domain as the given document (e.g., the domain of computer science). Unfortunately, this keyphrase-frequency feature is domain-specific (the learning process must be repeated for each new domain) and training-intensive (good performance requires a relatively large number of training documents in the given domain, with manually assigned keyphrases). The aim of the work described here is to remove these limitations. In this paper, I introduce new features that are derived by mining lexical knowledge from a very large collection of unlabeled data, consisting of approximately 350 million Web pages without manually assigned keyphrases. I present experiments that show that the new features result in improved keyphrase extraction, although they are neither domain-specific nor training-intensive.",
        "published": "2002-12-08T18:52:33Z",
        "link": "http://arxiv.org/abs/cs/0212011v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Unsupervised Learning of Semantic Orientation from a   Hundred-Billion-Word Corpus",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman"
        ],
        "summary": "The evaluative character of a word is called its semantic orientation. A positive semantic orientation implies desirability (e.g., \"honest\", \"intrepid\") and a negative semantic orientation implies undesirability (e.g., \"disturbing\", \"superfluous\"). This paper introduces a simple algorithm for unsupervised learning of semantic orientation from extremely large corpora. The method involves issuing queries to a Web search engine and using pointwise mutual information to analyse the results. The algorithm is empirically evaluated using a training corpus of approximately one hundred billion words -- the subset of the Web that is indexed by the chosen search engine. Tested with 3,596 words (1,614 positive and 1,982 negative), the algorithm attains an accuracy of 80%. The 3,596 test words include adjectives, adverbs, nouns, and verbs. The accuracy is comparable with the results achieved by Hatzivassiloglou and McKeown (1997), using a complex four-stage supervised learning algorithm that is restricted to determining the semantic orientation of adjectives.",
        "published": "2002-12-08T19:06:08Z",
        "link": "http://arxiv.org/abs/cs/0212012v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Learning to Extract Keyphrases from Text",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Many academic journals ask their authors to provide a list of about five to fifteen key words, to appear on the first page of each article. Since these key words are often phrases of two or more words, we prefer to call them keyphrases. There is a surprisingly wide variety of tasks for which keyphrases are useful, as we discuss in this paper. Recent commercial software, such as Microsoft's Word 97 and Verity's Search 97, includes algorithms that automatically extract keyphrases from documents. In this paper, we approach the problem of automatically extracting keyphrases from text as a supervised learning task. We treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Our first set of experiments applies the C4.5 decision tree induction algorithm to this learning task. The second set of experiments applies the GenEx algorithm to the task. We developed the GenEx algorithm specifically for this task. The third set of experiments examines the performance of GenEx on the task of metadata generation, relative to the performance of Microsoft's Word 97. The fourth and final set of experiments investigates the performance of GenEx on the task of highlighting, relative to Verity's Search 97. The experimental results support the claim that a specialized learning algorithm (GenEx) can generate better keyphrases than a general-purpose learning algorithm (C4.5) and the non-learning algorithms that are used in commercial software (Word 97 and Search 97).",
        "published": "2002-12-08T19:27:56Z",
        "link": "http://arxiv.org/abs/cs/0212013v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Extraction of Keyphrases from Text: Evaluation of Four Algorithms",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This report presents an empirical evaluation of four algorithms for automatically extracting keywords and keyphrases from documents. The four algorithms are compared using five different collections of documents. For each document, we have a target set of keyphrases, which were generated by hand. The target keyphrases were generated for human readers; they were not tailored for any of the four keyphrase extraction algorithms. Each of the algorithms was evaluated by the degree to which the algorithm's keyphrases matched the manually generated keyphrases. The four algorithms were (1) the AutoSummarize feature in Microsoft's Word 97, (2) an algorithm based on Eric Brill's part-of-speech tagger, (3) the Summarize feature in Verity's Search 97, and (4) NRC's Extractor algorithm. For all five document collections, NRC's Extractor yields the best match with the manually generated keyphrases.",
        "published": "2002-12-08T19:40:42Z",
        "link": "http://arxiv.org/abs/cs/0212014v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Learning Algorithms for Keyphrase Extraction",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Many academic journals ask their authors to provide a list of about five to fifteen keywords, to appear on the first page of each article. Since these key words are often phrases of two or more words, we prefer to call them keyphrases. There is a wide variety of tasks for which keyphrases are useful, as we discuss in this paper. We approach the problem of automatically extracting keyphrases from text as a supervised learning task. We treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Our first set of experiments applies the C4.5 decision tree induction algorithm to this learning task. We evaluate the performance of nine different configurations of C4.5. The second set of experiments applies the GenEx algorithm to the task. We developed the GenEx algorithm specifically for automatically extracting keyphrases from text. The experimental results support the claim that a custom-designed algorithm (GenEx), incorporating specialized procedural domain knowledge, can generate better keyphrases than a generalpurpose algorithm (C4.5). Subjective human evaluation of the keyphrases generated by Extractor suggests that about 80% of the keyphrases are acceptable to human readers. This level of performance should be satisfactory for a wide variety of applications.",
        "published": "2002-12-10T15:30:56Z",
        "link": "http://arxiv.org/abs/cs/0212020v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised   Classification of Reviews",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \"subtle nuances\") and a negative semantic orientation when it has bad associations (e.g., \"very cavalier\"). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \"excellent\" minus the mutual information between the given phrase and the word \"poor\". A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.",
        "published": "2002-12-11T18:57:42Z",
        "link": "http://arxiv.org/abs/cs/0212032v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "I.2.6; I.2.7; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74%. PMI-IR is contrasted with Latent Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL questions. The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing).",
        "published": "2002-12-11T19:17:06Z",
        "link": "http://arxiv.org/abs/cs/0212033v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "I.2.6; I.2.7; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Local Community Identification through User Access Patterns",
        "authors": [
            "Rodrigo B. Almeida",
            "Virgilio A. F. Almeida"
        ],
        "summary": "Community identification algorithms have been used to enhance the quality of the services perceived by its users. Although algorithms for community have a widespread use in the Web, their application to portals or specific subsets of the Web has not been much studied. In this paper, we propose a technique for local community identification that takes into account user access behavior derived from access logs of servers in the Web. The technique takes a departure from the existing community algorithms since it changes the focus of in terest, moving from authors to users. Our approach does not use relations imposed by authors (e.g. hyperlinks in the case of Web pages). It uses information derived from user accesses to a service in order to infer relationships. The communities identified are of great interest to content providers since they can be used to improve quality of their services. We also propose an evaluation methodology for analyzing the results obtained by the algorithm. We present two case studies based on actual data from two services: an online bookstore and an online radio. The case of the online radio is particularly relevant, because it emphasizes the contribution of the proposed algorithm to find out communities in an environment (i.e., streaming media service) without links, that represent the relations imposed by authors (e.g. hyperlinks in the case of Web pages).",
        "published": "2002-12-16T17:56:33Z",
        "link": "http://arxiv.org/abs/cs/0212045v1",
        "categories": [
            "cs.IR",
            "cs.HC",
            "I.5.3;H.1.2;J.4"
        ]
    },
    {
        "title": "A Quantum Computer Foundation for the Standard Model and SuperString   Theories",
        "authors": [
            "Stephen Blaha"
        ],
        "summary": "We show the Standard Model and SuperString Theories can be naturally based on a Quantum Computer foundation. The Standard Model of elementary particles can be viewed as defining a Quantum Computer Grammar and language. A Quantum Computer in a certain limit naturally forms a Superspace upon which Supersymmetry rotations can be defined - a Continuum Quantum Computer. Quantum high-level computer languages such as Quantum C and Quantum Assembly language are also discussed. In these new linguistic representations, particles become literally symbols or letters, and particle interactions become grammar rules. This view is NOT the same as the often-expressed view that Mathematics is the language of Physics. Some new developments relating to Quantum Computers and Quantum Turing Machines are also described.",
        "published": "2002-01-14T21:57:12Z",
        "link": "http://arxiv.org/abs/hep-th/0201092v1",
        "categories": [
            "hep-th",
            "cs.PL",
            "quant-ph"
        ]
    },
    {
        "title": "A Backward Analysis for Constraint Logic Programs",
        "authors": [
            "Andy King",
            "Lunjin Lu"
        ],
        "summary": "One recurring problem in program development is that of understanding how to re-use code developed by a third party. In the context of (constraint) logic programming, part of this problem reduces to figuring out how to query a program. If the logic program does not come with any documentation, then the programmer is forced to either experiment with queries in an ad hoc fashion or trace the control-flow of the program (backward) to infer the modes in which a predicate must be called so as to avoid an instantiation error. This paper presents an abstract interpretation scheme that automates the latter technique. The analysis presented in this paper can infer moding properties which if satisfied by the initial query, come with the guarantee that the program and query can never generate any moding or instantiation errors. Other applications of the analysis are discussed. The paper explains how abstract domains with certain computational properties (they condense) can be used to trace control-flow backward (right-to-left) to infer useful properties of initial queries. A correctness argument is presented and an implementation is reported.",
        "published": "2002-01-16T11:48:13Z",
        "link": "http://arxiv.org/abs/cs/0201011v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.6; F.3.2"
        ]
    },
    {
        "title": "Efficient Groundness Analysis in Prolog",
        "authors": [
            "Jacob M. Howe",
            "Andy King"
        ],
        "summary": "Boolean functions can be used to express the groundness of, and trace grounding dependencies between, program variables in (constraint) logic programs. In this paper, a variety of issues pertaining to the efficient Prolog implementation of groundness analysis are investigated, focusing on the domain of definite Boolean functions, Def. The systematic design of the representation of an abstract domain is discussed in relation to its impact on the algorithmic complexity of the domain operations; the most frequently called operations should be the most lightweight. This methodology is applied to Def, resulting in a new representation, together with new algorithms for its domain operations utilising previously unexploited properties of Def -- for instance, quadratic-time entailment checking. The iteration strategy driving the analysis is also discussed and a simple, but very effective, optimisation of induced magic is described. The analysis can be implemented straightforwardly in Prolog and the use of a non-ground representation results in an efficient, scalable tool which does not require widening to be invoked, even on the largest benchmarks. An extensive experimental evaluation is given",
        "published": "2002-01-16T12:02:33Z",
        "link": "http://arxiv.org/abs/cs/0201012v1",
        "categories": [
            "cs.PL",
            "D.1.6; F.3.2"
        ]
    },
    {
        "title": "Quantum Computers and Quantum Computer Languages: Quantum Assembly   Language and Quantum C Language",
        "authors": [
            "Stephen Blaha"
        ],
        "summary": "We show a representation of Quantum Computers defines Quantum Turing Machines with associated Quantum Grammars. We then create examples of Quantum Grammars. Lastly we develop an algebraic approach to high level Quantum Languages using Quantum Assembly language and Quantum C language as examples.",
        "published": "2002-01-18T15:08:05Z",
        "link": "http://arxiv.org/abs/quant-ph/0201082v1",
        "categories": [
            "quant-ph",
            "cs.PL"
        ]
    },
    {
        "title": "The Witness Properties and the Semantics of the Prolog Cut",
        "authors": [
            "James H. Andrews"
        ],
        "summary": "The semantics of the Prolog ``cut'' construct is explored in the context of some desirable properties of logic programming systems, referred to as the witness properties. The witness properties concern the operational consistency of responses to queries. A generalization of Prolog with negation as failure and cut is described, and shown not to have the witness properties. A restriction of the system is then described, which preserves the choice and first-solution behaviour of cut but allows the system to have the witness properties.   The notion of cut in the restricted system is more restricted than the Prolog hard cut, but retains the useful first-solution behaviour of hard cut, not retained by other proposed cuts such as the ``soft cut''. It is argued that the restricted system achieves a good compromise between the power and utility of the Prolog cut and the need for internal consistency in logic programming systems. The restricted system is given an abstract semantics, which depends on the witness properties; this semantics suggests that the restricted system has a deeper connection to logic than simply permitting some computations which are logical.   Parts of this paper appeared previously in a different form in the Proceedings of the 1995 International Logic Programming Symposium.",
        "published": "2002-01-31T17:47:45Z",
        "link": "http://arxiv.org/abs/cs/0201029v1",
        "categories": [
            "cs.PL",
            "D.3.1; D.3.3"
        ]
    },
    {
        "title": "Using parametric set constraints for locating errors in CLP programs",
        "authors": [
            "W. Drabent",
            "J. Maluszynski",
            "P. Pietrzak"
        ],
        "summary": "This paper introduces a framework of parametric descriptive directional types for constraint logic programming (CLP). It proposes a method for locating type errors in CLP programs and presents a prototype debugging tool. The main technique used is checking correctness of programs w.r.t. type specifications. The approach is based on a generalization of known methods for proving correctness of logic programs to the case of parametric specifications. Set-constraint techniques are used for formulating and checking verification conditions for (parametric) polymorphic type specifications. The specifications are expressed in a parametric extension of the formalism of term grammars. The soundness of the method is proved and the prototype debugging tool supporting the proposed approach is illustrated on examples.   The paper is a substantial extension of the previous work by the same authors concerning monomorphic directional types.",
        "published": "2002-02-11T11:50:37Z",
        "link": "http://arxiv.org/abs/cs/0202010v1",
        "categories": [
            "cs.PL",
            "D1.6; D2.5; F3.1"
        ]
    },
    {
        "title": "Logic program specialisation through partial deduction: Control issues",
        "authors": [
            "Michael Leuschel",
            "Maurice Bruynooghe"
        ],
        "summary": "Program specialisation aims at improving the overall performance of programs by performing source to source transformations. A common approach within functional and logic programming, known respectively as partial evaluation and partial deduction, is to exploit partial knowledge about the input. It is achieved through a well-automated application of parts of the Burstall-Darlington unfold/fold transformation framework. The main challenge in developing systems is to design automatic control that ensures correctness, efficiency, and termination. This survey and tutorial presents the main developments in controlling partial deduction over the past 10 years and analyses their respective merits and shortcomings. It ends with an assessment of current achievements and sketches some remaining research challenges.",
        "published": "2002-02-12T14:16:53Z",
        "link": "http://arxiv.org/abs/cs/0202012v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.1.6; D.1.2; I.2.2; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Towards Generic Refactoring",
        "authors": [
            "Ralf Laemmel"
        ],
        "summary": "We study program refactoring while considering the language or even the programming paradigm as a parameter. We use typed functional programs, namely Haskell programs, as the specification medium for a corresponding refactoring framework. In order to detach ourselves from language syntax, our specifications adhere to the following style. (I) As for primitive algorithms for program analysis and transformation, we employ generic function combinators supporting generic traversal and polymorphic functions refined by ad-hoc cases. (II) As for the language abstractions involved in refactorings, we design a dedicated multi-parameter class. This class can be instantiated for abstractions as present in various languages, e.g., Java, Prolog or Haskell.",
        "published": "2002-03-01T11:58:56Z",
        "link": "http://arxiv.org/abs/cs/0203001v1",
        "categories": [
            "cs.PL",
            "D.1.1; D.1.2; D.2.1; D.2.3; D.2.13; D.3.1; I.1.1; I.1.2; I.1.3"
        ]
    },
    {
        "title": "Composing Programs in a Rewriting Logic for Declarative Programming",
        "authors": [
            "Juan M. Molina",
            "Ernesto Pimentel"
        ],
        "summary": "Constructor-Based Conditional Rewriting Logic is a general framework for integrating first-order functional and logic programming which gives an algebraic semantics for non-deterministic functional-logic programs. In the context of this formalism, we introduce a simple notion of program module as an open program which can be extended together with several mechanisms to combine them. These mechanisms are based on a reduced set of operations. However, the high expressiveness of these operations enable us to model typical constructs for program modularization like hiding, export/import, genericity/instantiation, and inheritance in a simple way. We also deal with the semantic aspects of the proposal by introducing an immediate consequence operator, and studying several alternative semantics for a program module, based on this operator, in the line of logic programming: the operator itself, its least fixpoint (the least model of the module), the set of its pre-fixpoints (term models of the module), and some other variations in order to find a compositional and fully abstract semantics wrt the set of operations and a natural notion of observability.",
        "published": "2002-03-04T19:02:05Z",
        "link": "http://arxiv.org/abs/cs/0203006v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.2;D.3.3;F.3.2;F.3.3;F.4.1"
        ]
    },
    {
        "title": "Three Optimisations for Sharing",
        "authors": [
            "Jacob M. Howe",
            "Andy King"
        ],
        "summary": "In order to improve precision and efficiency sharing analysis should track both freeness and linearity. The abstract unification algorithms for these combined domains are suboptimal, hence there is scope for improving precision. This paper proposes three optimisations for tracing sharing in combination with freeness and linearity. A novel connection between equations and sharing abstractions is used to establish correctness of these optimisations even in the presence of rational trees. A method for pruning intermediate sharing abstractions to improve efficiency is also proposed. The optimisations are lightweight and therefore some, if not all, of these optimisations will be of interest to the implementor.",
        "published": "2002-03-18T13:22:30Z",
        "link": "http://arxiv.org/abs/cs/0203022v1",
        "categories": [
            "cs.PL",
            "D.1.6; f.3.2"
        ]
    },
    {
        "title": "The Sketch of a Polymorphic Symphony",
        "authors": [
            "Ralf Laemmel"
        ],
        "summary": "In previous work, we have introduced functional strategies, that is, first-class generic functions that can traverse into terms of any type while mixing uniform and type-specific behaviour. In the present paper, we give a detailed description of one particular Haskell-based model of functional strategies. This model is characterised as follows. Firstly, we employ first-class polymorphism as a form of second-order polymorphism as for the mere types of functional strategies. Secondly, we use an encoding scheme of run-time type case for mixing uniform and type-specific behaviour. Thirdly, we base all traversal on a fundamental combinator for folding over constructor applications.   Using this model, we capture common strategic traversal schemes in a highly parameterised style. We study two original forms of parameterisation. Firstly, we design parameters for the specific control-flow, data-flow and traversal characteristics of more concrete traversal schemes. Secondly, we use overloading to postpone commitment to a specific type scheme of traversal. The resulting portfolio of traversal schemes can be regarded as a challenging benchmark for setups for typed generic programming.   The way we develop the model and the suite of traversal schemes, it becomes clear that parameterised + typed strategic programming is best viewed as a potent combination of certain bits of parametric, intensional, polytypic, and ad-hoc polymorphism.",
        "published": "2002-04-08T14:12:21Z",
        "link": "http://arxiv.org/abs/cs/0204013v2",
        "categories": [
            "cs.PL",
            "D.1.1; D.3.3; I.1.3"
        ]
    },
    {
        "title": "Design Patterns for Functional Strategic Programming",
        "authors": [
            "Ralf Laemmel",
            "Joost Visser"
        ],
        "summary": "In previous work, we introduced the fundamentals and a supporting combinator library for \\emph{strategic programming}. This an idiom for generic programming based on the notion of a \\emph{functional strategy}: a first-class generic function that cannot only be applied to terms of any type, but which also allows generic traversal into subterms and can be customized with type-specific behaviour.   This paper seeks to provide practicing functional programmers with pragmatic guidance in crafting their own strategic programs. We present the fundamentals and the support from a user's perspective, and we initiate a catalogue of \\emph{strategy design patterns}. These design patterns aim at consolidating strategic programming expertise in accessible form.",
        "published": "2002-04-09T12:44:43Z",
        "link": "http://arxiv.org/abs/cs/0204015v1",
        "categories": [
            "cs.PL",
            "D.1.1; D.2.3; D.2.10"
        ]
    },
    {
        "title": "Making Abstract Domains Condensing",
        "authors": [
            "R. Giacobazzi",
            "F. Ranzato",
            "F. Scozzari"
        ],
        "summary": "In this paper we show that reversible analysis of logic languages by abstract interpretation can be performed without loss of precision by systematically refining abstract domains. The idea is to include semantic structures into abstract domains in such a way that the refined abstract domain becomes rich enough to allow approximate bottom-up and top-down semantics to agree. These domains are known as condensing abstract domains. Substantially, an abstract domain is condensing if goal-driven and goal-independent analyses agree, namely no loss of precision is introduced by approximating queries in a goal-independent analysis. We prove that condensation is an abstract domain property and that the problem of making an abstract domain condensing boils down to the problem of making the domain complete with respect to unification. In a general abstract interpretation setting we show that when concrete domains and operations give rise to quantales, i.e. models of propositional linear logic, objects in a complete refined abstract domain can be explicitly characterized by linear logic-based formulations. This is the case for abstract domains for logic program analysis approximating computed answer substitutions where unification plays the role of multiplicative conjunction in a quantale of idempotent substitutions. Condensing abstract domains can therefore be systematically derived by minimally extending any, generally non-condensing domain, by a simple domain refinement operator.",
        "published": "2002-04-09T13:22:00Z",
        "link": "http://arxiv.org/abs/cs/0204016v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.3.1; D.3.2; F.3.2"
        ]
    },
    {
        "title": "A Framework for Datatype Transformation",
        "authors": [
            "Jan Kort",
            "Ralf Laemmel"
        ],
        "summary": "We study one dimension in program evolution, namely the evolution of the datatype declarations in a program. To this end, a suite of basic transformation operators is designed. We cover structure-preserving refactorings, but also structure-extending and -reducing adaptations. Both the object programs that are subject to datatype transformations, and the meta programs that encode datatype transformations are functional programs.",
        "published": "2002-04-09T18:32:40Z",
        "link": "http://arxiv.org/abs/cs/0204018v3",
        "categories": [
            "cs.PL",
            "D.1.1; D.2.3; D.2.6; D.2.7; D.3.4"
        ]
    },
    {
        "title": "A Dynamic Approach to Characterizing Termination of General Logic   Programs",
        "authors": [
            "Yi-Dong Shen",
            "Jia-Huai You",
            "Li-Yan Yuan",
            "Samuel S. P. Shen",
            "Qiang Yang"
        ],
        "summary": "We present a new characterization of termination of general logic programs. Most existing termination analysis approaches rely on some static information about the structure of the source code of a logic program, such as modes/types, norms/level mappings, models/interargument relations, and the like. We propose a dynamic approach which employs some key dynamic features of an infinite (generalized) SLDNF-derivation, such as repetition of selected subgoals and recursive increase in term size. We also introduce a new formulation of SLDNF-trees, called generalized SLDNF-trees. Generalized SLDNF-trees deal with negative subgoals in the same way as Prolog and exist for any general logic programs.",
        "published": "2002-04-12T18:01:32Z",
        "link": "http://arxiv.org/abs/cs/0204031v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6;D.1.2;F.4.1"
        ]
    },
    {
        "title": "Typed Generic Traversal With Term Rewriting Strategies",
        "authors": [
            "Ralf Laemmel"
        ],
        "summary": "A typed model of strategic term rewriting is developed. The key innovation is that generic traversal is covered. To this end, we define a typed rewriting calculus S'_{gamma}. The calculus employs a many-sorted type system extended by designated generic strategy types gamma. We consider two generic strategy types, namely the types of type-preserving and type-unifying strategies. S'_{gamma} offers traversal combinators to construct traversals or schemes thereof from many-sorted and generic strategies. The traversal combinators model different forms of one-step traversal, that is, they process the immediate subterms of a given term without anticipating any scheme of recursion into terms. To inhabit generic types, we need to add a fundamental combinator to lift a many-sorted strategy $s$ to a generic type gamma. This step is called strategy extension. The semantics of the corresponding combinator states that s is only applied if the type of the term at hand fits, otherwise the extended strategy fails. This approach dictates that the semantics of strategy application must be type-dependent to a certain extent. Typed strategic term rewriting with coverage of generic term traversal is a simple but expressive model of generic programming. It has applications in program transformation and program analysis.",
        "published": "2002-05-14T10:18:42Z",
        "link": "http://arxiv.org/abs/cs/0205018v2",
        "categories": [
            "cs.PL",
            "D.1.1; D.1.2; D.3.1; D.3.3; F.4.2; I.1 .3; I.2.2"
        ]
    },
    {
        "title": "Monads for natural language semantics",
        "authors": [
            "Chung-chieh Shan"
        ],
        "summary": "Accounts of semantic phenomena often involve extending types of meanings and revising composition rules at the same time. The concept of monads allows many such accounts -- for intensionality, variable binding, quantification and focus -- to be stated uniformly and compositionally.",
        "published": "2002-05-17T08:24:56Z",
        "link": "http://arxiv.org/abs/cs/0205026v1",
        "categories": [
            "cs.CL",
            "cs.PL",
            "I.2.7; D.3.1; F.3.2"
        ]
    },
    {
        "title": "Three-Tiered Specification of Micro-Architectures",
        "authors": [
            "Vasu Alagar",
            "Ralf Laemmel"
        ],
        "summary": "A three-tiered specification approach is developed to formally specify collections of collaborating objects, say micro-architectures. (i) The structural properties to be maintained in the collaboration are specified in the lowest tier. (ii) The behaviour of the object methods in the classes is specified in the middle tier. (iii) The interaction of the objects in the micro-architecture is specified in the third tier. The specification approach is based on Larch and accompanying notations and tools. The approach enables the unambiguous and complete specification of reusable collections of collaborating objects. The layered, formal approach is compared to other approaches including the mainstream UML approach.",
        "published": "2002-05-19T14:46:34Z",
        "link": "http://arxiv.org/abs/cs/0205052v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.4; D.2.10; D.2.11; D.2.13"
        ]
    },
    {
        "title": "Agent Programming with Declarative Goals",
        "authors": [
            "F. S. de Boer",
            "K. V. Hindriks",
            "W. van der Hoek",
            "J. -J. Ch. Meyer"
        ],
        "summary": "A long and lasting problem in agent research has been to close the gap between agent logics and agent programming frameworks. The main reason for this problem of establishing a link between agent logics and agent programming frameworks is identified and explained by the fact that agent programming frameworks have not incorporated the concept of a `declarative goal'. Instead, such frameworks have focused mainly on plans or `goals-to-do' instead of the end goals to be realised which are also called `goals-to-be'. In this paper, a new programming language called GOAL is introduced which incorporates such declarative goals. The notion of a `commitment strategy' - one of the main theoretical insights due to agent logics, which explains the relation between beliefs and goals - is used to construct a computational semantics for GOAL. Finally, a proof theory for proving properties of GOAL agents is introduced. Thus, we offer a complete theory of agent programming in the sense that our theory provides both for a programming framework and a programming logic for such agents. An example program is proven correct by using this programming logic.",
        "published": "2002-07-03T15:42:55Z",
        "link": "http://arxiv.org/abs/cs/0207008v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "F.3.1;F.3.2;I.2.5;I.2.4"
        ]
    },
    {
        "title": "A continuation semantics of interrogatives that accounts for Baker's   ambiguity",
        "authors": [
            "Chung-chieh Shan"
        ],
        "summary": "Wh-phrases in English can appear both raised and in-situ. However, only in-situ wh-phrases can take semantic scope beyond the immediately enclosing clause. I present a denotational semantics of interrogatives that naturally accounts for these two properties. It neither invokes movement or economy, nor posits lexical ambiguity between raised and in-situ occurrences of the same wh-phrase. My analysis is based on the concept of continuations. It uses a novel type system for higher-order continuations to handle wide-scope wh-phrases while remaining strictly compositional. This treatment sheds light on the combinatorics of interrogatives as well as other kinds of so-called A'-movement.",
        "published": "2002-07-18T23:27:01Z",
        "link": "http://arxiv.org/abs/cs/0207070v2",
        "categories": [
            "cs.CL",
            "cs.PL",
            "I.2.7"
        ]
    },
    {
        "title": "Introducing Dynamic Behavior in Amalgamated Knowledge Bases",
        "authors": [
            "Elisa Bertino",
            "Barbara Catania",
            "Paolo Perlasca"
        ],
        "summary": "The problem of integrating knowledge from multiple and heterogeneous sources is a fundamental issue in current information systems. In order to cope with this problem, the concept of mediator has been introduced as a software component providing intermediate services, linking data resources and application programs, and making transparent the heterogeneity of the underlying systems. In designing a mediator architecture, we believe that an important aspect is the definition of a formal framework by which one is able to model integration according to a declarative style. To this purpose, the use of a logical approach seems very promising. Another important aspect is the ability to model both static integration aspects, concerning query execution, and dynamic ones, concerning data updates and their propagation among the various data sources. Unfortunately, as far as we know, no formal proposals for logically modeling mediator architectures both from a static and dynamic point of view have already been developed. In this paper, we extend the framework for amalgamated knowledge bases, presented by Subrahmanian, to deal with dynamic aspects. The language we propose is based on the Active U-Datalog language, and extends it with annotated logic and amalgamation concepts. We model the sources of information and the mediator (also called supervisor) as Active U-Datalog deductive databases, thus modeling queries, transactions, and active rules, interpreted according to the PARK semantics. By using active rules, the system can efficiently perform update propagation among different databases. The result is a logical environment, integrating active and deductive rules, to perform queries and update propagation in an heterogeneous mediated framework.",
        "published": "2002-07-22T07:50:01Z",
        "link": "http://arxiv.org/abs/cs/0207076v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "cs.LO",
            "D.1.6 Logic Programming; H.2.5 Heterogeneous Databases; F.4.1\n  Mathematical Logic, Logic and constraint programming"
        ]
    },
    {
        "title": "Defining Rough Sets by Extended Logic Programs",
        "authors": [
            "Jan Małuszyński",
            "Aida Vitória"
        ],
        "summary": "We show how definite extended logic programs can be used for defining and reasoning with rough sets. Moreover, a rough-set-specific query language is presented and an answering algorithm is outlined. Thus, we not only show a possible application of a paraconsistent logic to the field of rough sets as we also establish a link between rough set theory and logic programming, making possible transfer of expertise between both fields.",
        "published": "2002-07-25T16:52:46Z",
        "link": "http://arxiv.org/abs/cs/0207089v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.4.1; I.2.3; I.2.4; D.1.6"
        ]
    },
    {
        "title": "Soft Concurrent Constraint Programming",
        "authors": [
            "S. Bistarelli",
            "U. Montanari",
            "F. Rossi"
        ],
        "summary": "Soft constraints extend classical constraints to represent multiple consistency levels, and thus provide a way to express preferences, fuzziness, and uncertainty. While there are many soft constraint solving formalisms, even distributed ones, by now there seems to be no concurrent programming framework where soft constraints can be handled. In this paper we show how the classical concurrent constraint (cc) programming framework can work with soft constraints, and we also propose an extension of cc languages which can use soft constraints to prune and direct the search for a solution. We believe that this new programming paradigm, called soft cc (scc), can be also very useful in many web-related scenarios. In fact, the language level allows web agents to express their interaction and negotiation protocols, and also to post their requests in terms of preferences, and the underlying soft constraint solver can find an agreement among the agents even if their requests are incompatible.",
        "published": "2002-08-06T19:08:55Z",
        "link": "http://arxiv.org/abs/cs/0208008v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.1.3; D.3.1; D.3.2; D.3.3; F.3.2"
        ]
    },
    {
        "title": "Offline Specialisation in Prolog Using a Hand-Written Compiler Generator",
        "authors": [
            "Michael Leuschel",
            "Jesper Joergensen",
            "Wim Vanhoof",
            "Maurice Bruynooghe"
        ],
        "summary": "The so called ``cogen approach'' to program specialisation, writing a compiler generator instead of a specialiser, has been used with considerable success in partial evaluation of both functional and imperative languages. This paper demonstrates that the cogen approach is also applicable to the specialisation of logic programs (also called partial deduction) and leads to effective specialisers. Moreover, using good binding-time annotations, the speed-ups of the specialised programs are comparable to the speed-ups obtained with online specialisers. The paper first develops a generic approach to offline partial deduction and then a specific offline partial deduction method, leading to the offline system LIX for pure logic programs. While this is a usable specialiser by itself, it is used to develop the cogen system LOGEN. Given a program, a specification of what inputs will be static, and an annotation specifying which calls should be unfolded, LOGEN generates a specialised specialiser for the program at hand. Running this specialiser with particular values for the static inputs results in the specialised program. While this requires two steps instead of one, the efficiency of the specialisation process is improved in situations where the same program is specialised multiple times. The paper also presents and evaluates an automatic binding-time analysis that is able to derive the annotations. While the derived annotations are still suboptimal compared to hand-crafted ones, they enable non-expert users to use the LOGEN system in a fully automated way. Finally, LOGEN is extended so as to directly support a large part of Prolog's declarative and non-declarative features and so as to be able to perform so called mixline specialisations.",
        "published": "2002-08-07T11:14:53Z",
        "link": "http://arxiv.org/abs/cs/0208009v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.1.6; D.1.2; I.2.2; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Logic programming in the context of multiparadigm programming: the Oz   experience",
        "authors": [
            "Peter Van Roy",
            "Per Brand",
            "Denys Duchier",
            "Seif Haridi",
            "Martin Henz",
            "Christian Schulte"
        ],
        "summary": "Oz is a multiparadigm language that supports logic programming as one of its major paradigms. A multiparadigm language is designed to support different programming paradigms (logic, functional, constraint, object-oriented, sequential, concurrent, etc.) with equal ease. This article has two goals: to give a tutorial of logic programming in Oz and to show how logic programming fits naturally into the wider context of multiparadigm programming. Our experience shows that there are two classes of problems, which we call algorithmic and search problems, for which logic programming can help formulate practical solutions. Algorithmic problems have known efficient algorithms. Search problems do not have known efficient algorithms but can be solved with search. The Oz support for logic programming targets these two problem classes specifically, using the concepts needed for each. This is in contrast to the Prolog approach, which targets both classes with one set of concepts, which results in less than optimal support for each class. To explain the essential difference between algorithmic and search programs, we define the Oz execution model. This model subsumes both concurrent logic programming (committed-choice-style) and search-based logic programming (Prolog-style). Instead of Horn clause syntax, Oz has a simple, fully compositional, higher-order syntax that accommodates the abilities of the language. We conclude with lessons learned from this work, a brief history of Oz, and many entry points into the Oz literature.",
        "published": "2002-08-20T11:12:58Z",
        "link": "http://arxiv.org/abs/cs/0208029v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2; D.3.3; F.3.3"
        ]
    },
    {
        "title": "Proving correctness of Timed Concurrent Constraint Programs",
        "authors": [
            "F. S. de Boer",
            "M. Gabbrielli",
            "M. C. Meo"
        ],
        "summary": "A temporal logic is presented for reasoning about the correctness of timed concurrent constraint programs. The logic is based on modalities which allow one to specify what a process produces as a reaction to what its environment inputs. These modalities provide an assumption/commitment style of specification which allows a sound and complete compositional axiomatization of the reactive behavior of timed concurrent constraint programs.",
        "published": "2002-08-28T14:34:31Z",
        "link": "http://arxiv.org/abs/cs/0208042v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.1;D.3.1;D.3.2"
        ]
    },
    {
        "title": "The Weaves Reconfigurable Programming Framework",
        "authors": [
            "Srinidhi Varadarajan"
        ],
        "summary": "This research proposes a language independent intra-process framework for object based composition of unmodified code modules. Intuitively, the two major programming models, threads and processes, can be considered as extremes along a sharing axis. Multiple threads through a process share all global state, whereas instances of a process (or independent processes) share no global state. Weaves provide the generalized framework that allows arbitrary (selective) sharing of state between multiple control flows through a process. The Weaves framework supports multiple independent components in a single process, with flexible state sharing and scheduling, all of which is achieved without requiring any modification to existing code bases. Furthermore, the framework allows dynamic instantiation of code modules and control flows through them. In effect, weaves create intra-process modules (similar to objects in OOP) from code written in any language. The Weaves paradigm allows objects to be arbitrarily shared, it is a true superset of both processes as well as threads, with code sharing and fast context switching time similar to threads. Weaves does not require any special support from either the language or application code, practically any code can be weaved. Weaves also include support for fast automatic checkpointing and recovery with no application support. This paper presents the elements of the Weaves framework and results from our implementation that works by reverse-analyzing source-code independent ELF object files. The current implementation has been validated over Sweep3D, a benchmark for 3D discrete ordinates neutron transport [Koch et al., 1992], and a user-level port of the Linux 2.4 family kernel TCP/IP protocol stack.",
        "published": "2002-10-31T01:51:00Z",
        "link": "http://arxiv.org/abs/cs/0210031v1",
        "categories": [
            "cs.PL",
            "cs.OS",
            "D.2.11 D.2.12 D.1.3 D.3.2 D.3.4"
        ]
    },
    {
        "title": "The DLV System for Knowledge Representation and Reasoning",
        "authors": [
            "Nicola Leone",
            "Gerald Pfeifer",
            "Wolfgang Faber",
            "Thomas Eiter",
            "Georg Gottlob",
            "Simona Perri",
            "Francesco Scarcello"
        ],
        "summary": "This paper presents the DLV system, which is widely considered the state-of-the-art implementation of disjunctive logic programming, and addresses several aspects. As for problem solving, we provide a formal definition of its kernel language, function-free disjunctive logic programs (also known as disjunctive datalog), extended by weak constraints, which are a powerful tool to express optimization problems. We then illustrate the usage of DLV as a tool for knowledge representation and reasoning, describing a new declarative programming methodology which allows one to encode complex problems (up to $\\Delta^P_3$-complete problems) in a declarative fashion. On the foundational side, we provide a detailed analysis of the computational complexity of the language of DLV, and by deriving new complexity results we chart a complete picture of the complexity of this language and important fragments thereof.   Furthermore, we illustrate the general architecture of the DLV system which has been influenced by these results. As for applications, we overview application front-ends which have been developed on top of DLV to solve specific knowledge representation tasks, and we briefly describe the main international projects investigating the potential of the system for industrial exploitation. Finally, we report about thorough experimentation and benchmarking, which has been carried out to assess the efficiency of the system. The experimental results confirm the solidity of DLV and highlight its potential for emerging application areas like knowledge management and information integration.",
        "published": "2002-11-04T15:18:04Z",
        "link": "http://arxiv.org/abs/cs/0211004v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.PL",
            "I.2.3; I.2.4; D.3.1"
        ]
    },
    {
        "title": "Schedulers for Rule-based Constraint Programming",
        "authors": [
            "Krzysztof R. Apt",
            "Sebastian Brand"
        ],
        "summary": "We study here schedulers for a class of rules that naturally arise in the context of rule-based constraint programming. We systematically derive a scheduler for them from a generic iteration algorithm of Apt [2000]. We apply this study to so-called membership rules of Apt and Monfroy [2001]. This leads to an implementation that yields for these rules a considerably better performance than their execution as standard CHR rules.",
        "published": "2002-11-15T13:38:20Z",
        "link": "http://arxiv.org/abs/cs/0211019v1",
        "categories": [
            "cs.DS",
            "cs.PL",
            "I.2.2; I.2.3; D.3.3; D.3.4"
        ]
    },
    {
        "title": "Monadic Style Control Constructs for Inference Systems",
        "authors": [
            "Jean-Marie Chauvet"
        ],
        "summary": "Recent advances in programming languages study and design have established a standard way of grounding computational systems representation in category theory. These formal results led to a better understanding of issues of control and side-effects in functional and imperative languages. Another benefit is a better way of modelling computational effects in logical frameworks. With this analogy in mind, we embark on an investigation of inference systems based on considering inference behaviour as a form of computation. We delineate a categorical formalisation of control constructs in inference systems. This representation emphasises the parallel between the modular articulation of the categorical building blocks (triples) used to account for the inference architecture and the modular composition of cognitive processes.",
        "published": "2002-11-25T14:19:26Z",
        "link": "http://arxiv.org/abs/cs/0211035v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "68Q55"
        ]
    },
    {
        "title": "Ownership Confinement Ensures Representation Independence for   Object-Oriented Programs",
        "authors": [
            "Anindya Banerjee",
            "David A. Naumann"
        ],
        "summary": "Dedicated to the memory of Edsger W.Dijkstra.   Representation independence or relational parametricity formally characterizes the encapsulation provided by language constructs for data abstraction and justifies reasoning by simulation. Representation independence has been shown for a variety of languages and constructs but not for shared references to mutable state; indeed it fails in general for such languages. This paper formulates representation independence for classes, in an imperative, object-oriented language with pointers, subclassing and dynamic dispatch, class oriented visibility control, recursive types and methods, and a simple form of module. An instance of a class is considered to implement an abstraction using private fields and so-called representation objects. Encapsulation of representation objects is expressed by a restriction, called confinement, on aliasing. Representation independence is proved for programs satisfying the confinement condition. A static analysis is given for confinement that accepts common designs such as the observer and factory patterns. The formalization takes into account not only the usual interface between a client and a class that provides an abstraction but also the interface (often called ``protected'') between the class and its subclasses.",
        "published": "2002-12-04T23:11:22Z",
        "link": "http://arxiv.org/abs/cs/0212003v1",
        "categories": [
            "cs.PL",
            "D.3.3; F.3.1"
        ]
    },
    {
        "title": "Strategic polymorphism requires just two combinators!",
        "authors": [
            "Ralf Laemmel",
            "Joost Visser"
        ],
        "summary": "In previous work, we introduced the notion of functional strategies: first-class generic functions that can traverse terms of any type while mixing uniform and type-specific behaviour. Functional strategies transpose the notion of term rewriting strategies (with coverage of traversal) to the functional programming paradigm. Meanwhile, a number of Haskell-based models and combinator suites were proposed to support generic programming with functional strategies.   In the present paper, we provide a compact and matured reconstruction of functional strategies. We capture strategic polymorphism by just two primitive combinators. This is done without commitment to a specific functional language. We analyse the design space for implementational models of functional strategies. For completeness, we also provide an operational reference model for implementing functional strategies (in Haskell). We demonstrate the generality of our approach by reconstructing representative fragments of the Strafunski library for functional strategies.",
        "published": "2002-12-19T13:54:26Z",
        "link": "http://arxiv.org/abs/cs/0212048v1",
        "categories": [
            "cs.PL",
            "D.1.1; D.3.3; I.1.3"
        ]
    },
    {
        "title": "Long Proteins with Unique Optimal Foldings in the H-P Model",
        "authors": [
            "Oswin Aichholzer",
            "David Bremner",
            "Erik D. Demaine",
            "Henk Meijer",
            "Vera Sacristán",
            "Michael Soss"
        ],
        "summary": "It is widely accepted that (1) the natural or folded state of proteins is a global energy minimum, and (2) in most cases proteins fold to a unique state determined by their amino acid sequence. The H-P (hydrophobic-hydrophilic) model is a simple combinatorial model designed to answer qualitative questions about the protein folding process. In this paper we consider a problem suggested by Brian Hayes in 1998: what proteins in the two-dimensional H-P model have unique optimal (minimum energy) foldings? In particular, we prove that there are closed chains of monomers (amino acids) with this property for all (even) lengths; and that there are open monomer chains with this property for all lengths divisible by four.",
        "published": "2002-01-21T13:39:22Z",
        "link": "http://arxiv.org/abs/cs/0201018v1",
        "categories": [
            "cs.CG",
            "q-bio.BM",
            "G.2; I.3.5"
        ]
    },
    {
        "title": "Qualitative Visualization of Distance Information",
        "authors": [
            "Jobst Heitzig"
        ],
        "summary": "Different types of two- and three-dimensional representations of a finite metric space are studied that focus on the accurate representation of the linear order among the distances rather than their actual values. Lower and upper bounds for representability probabilities are produced by experiments including random generation, a rubber-band algorithm for accuracy optimization, and automatic proof generation. It is proved that both farthest neighbour representations and cluster tree representations always exist in the plane. Moreover, a measure of order accuracy is introduced, and some lower bound on the possible accuracy is proved using some clustering method and a result on maximal cuts in graphs.",
        "published": "2002-01-30T14:16:26Z",
        "link": "http://arxiv.org/abs/math/0201298v1",
        "categories": [
            "math.CO",
            "cs.CG"
        ]
    },
    {
        "title": "Small Strictly Convex Quadrilateral Meshes of Point Sets",
        "authors": [
            "David Bremner",
            "Ferran Hurtado",
            "Suneeta Ramaswami",
            "Vera Sacristan"
        ],
        "summary": "In this paper, we give upper and lower bounds on the number of Steiner points required to construct a strictly convex quadrilateral mesh for a planar point set. In particular, we show that $3{\\lfloor\\frac{n}{2}\\rfloor}$ internal Steiner points are always sufficient for a convex quadrilateral mesh of $n$ points in the plane. Furthermore, for any given $n\\geq 4$, there are point sets for which $\\lceil\\frac{n-3}{2}\\rceil-1$ Steiner points are necessary for a convex quadrilateral mesh.",
        "published": "2002-02-12T14:11:25Z",
        "link": "http://arxiv.org/abs/cs/0202011v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "The efficient generation of unstructured control volumes in 2D and 3D",
        "authors": [
            "Leszczynski Jacek",
            "Pluta Sebastian"
        ],
        "summary": "Many problems in engineering, chemistry and physics require the representation of solutions in complex geometries. In the paper we deal with a problem of unstructured mesh generation for the control volume method. We propose an algorithm which bases on the spheres generation in central points of the control volumes.",
        "published": "2002-02-26T16:32:12Z",
        "link": "http://arxiv.org/abs/cs/0202038v1",
        "categories": [
            "cs.CG",
            "cs.CE",
            "cs.NA",
            "math.NA",
            "physics.comp-ph",
            "65N20, 68Q20, 68Q22, 76M20"
        ]
    },
    {
        "title": "Computational Geometry Column 43",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "The concept of pointed pseudo-triangulations is defined and a few of its applications described.",
        "published": "2002-03-06T13:49:36Z",
        "link": "http://arxiv.org/abs/cs/0203008v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Sufficiently Fat Polyhedra are not 2-castable",
        "authors": [
            "David Bremner",
            "Alexander Golynski"
        ],
        "summary": "In this note we consider the problem of manufacturing a convex polyhedral object via casting. We consider a generalization of the sand casting process where the object is manufactured by gluing together two identical faces of parts cast with a single piece mold. In this model we show that the class of convex polyhedra which can be enclosed between two concentric spheres of the ratio of their radii less than 1.07 cannot be manufactured using only two cast parts.",
        "published": "2002-03-20T15:14:24Z",
        "link": "http://arxiv.org/abs/cs/0203025v1",
        "categories": [
            "cs.CG",
            "F.2.2; J.6"
        ]
    },
    {
        "title": "Conformal Geometry, Euclidean Space and Geometric Algebra",
        "authors": [
            "Chris Doran",
            "Anthony Lasenby",
            "Joan Lasenby"
        ],
        "summary": "Projective geometry provides the preferred framework for most implementations of Euclidean space in graphics applications. Translations and rotations are both linear transformations in projective geometry, which helps when it comes to programming complicated geometrical operations. But there is a fundamental weakness in this approach - the Euclidean distance between points is not handled in a straightforward manner. Here we discuss a solution to this problem, based on conformal geometry. The language of geometric algebra is best suited to exploiting this geometry, as it handles the interior and exterior products in a single, unified framework. A number of applications are discussed, including a compact formula for reflecting a line off a general spherical surface.",
        "published": "2002-03-22T14:33:34Z",
        "link": "http://arxiv.org/abs/cs/0203026v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "math.MG",
            "I.3.5;I.3.6"
        ]
    },
    {
        "title": "Coin-Moving Puzzles",
        "authors": [
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Helena A. Verrill"
        ],
        "summary": "We introduce a new family of one-player games, involving the movement of coins from one configuration to another. Moves are restricted so that a coin can be placed only in a position that is adjacent to at least two other coins. The goal of this paper is to specify exactly which of these games are solvable. By introducing the notion of a constant number of extra coins, we give tight theorems characterizing solvable puzzles on the square grid and equilateral-triangle grid. These existence results are supplemented by polynomial-time algorithms for finding a solution.",
        "published": "2002-03-31T01:02:12Z",
        "link": "http://arxiv.org/abs/cs/0204002v1",
        "categories": [
            "cs.DM",
            "cs.CG",
            "cs.GT",
            "G.2.1; G.2.2; F.2.2; I.3.5"
        ]
    },
    {
        "title": "Solitaire Clobber",
        "authors": [
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Rudolf Fleischer"
        ],
        "summary": "Clobber is a new two-player board game. In this paper, we introduce the one-player variant Solitaire Clobber where the goal is to remove as many stones as possible from the board by alternating white and black moves. We show that a checkerboard configuration on a single row (or single column) can be reduced to about n/4 stones. For boards with at least two rows and two columns, we show that a checkerboard configuration can be reduced to a single stone if and only if the number of stones is not a multiple of three, and otherwise it can be reduced to two stones. We also show that in general it is NP-complete to decide whether an arbitrary Clobber configuration can be reduced to a single stone.",
        "published": "2002-04-09T15:57:59Z",
        "link": "http://arxiv.org/abs/cs/0204017v2",
        "categories": [
            "cs.DM",
            "cs.CG",
            "cs.GT",
            "G.2.1; G.2.2; F.2.2; I.3.5"
        ]
    },
    {
        "title": "Preprocessing Chains for Fast Dihedral Rotations Is Hard or Even   Impossible",
        "authors": [
            "Michael Soss",
            "Jeff Erickson",
            "Mark Overmars"
        ],
        "summary": "We examine a computational geometric problem concerning the structure of polymers. We model a polymer as a polygonal chain in three dimensions. Each edge splits the polymer into two subchains, and a dihedral rotation rotates one of these chains rigidly about this edge. The problem is to determine, given a chain, an edge, and an angle of rotation, if the motion can be performed without causing the chain to self-intersect. An Omega(n log n) lower bound on the time complexity of this problem is known.   We prove that preprocessing a chain of n edges and answering n dihedral rotation queries is 3SUM-hard, giving strong evidence that solving n queries requires Omega(n^2) time in the worst case. For dynamic queries, which also modify the chain if the requested dihedral rotation is feasible, we show that answering n queries is by itself 3SUM-hard, suggesting that sublinear query time is impossible after any amount of preprocessing.",
        "published": "2002-04-19T21:03:35Z",
        "link": "http://arxiv.org/abs/cs/0204042v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Separating Thickness from Geometric Thickness",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We show that graph-theoretic thickness and geometric thickness are not asymptotically equivalent: for every t, there exists a graph with thickness three and geometric thickness >= t.",
        "published": "2002-04-20T00:51:53Z",
        "link": "http://arxiv.org/abs/math/0204252v2",
        "categories": [
            "math.CO",
            "cs.CG",
            "cs.DM",
            "05C10"
        ]
    },
    {
        "title": "Computing Homotopic Shortest Paths Efficiently",
        "authors": [
            "Alon Efrat",
            "Stephen G. Kobourov",
            "Anna Lubiw"
        ],
        "summary": "This paper addresses the problem of finding shortest paths homotopic to a given disjoint set of paths that wind amongst point obstacles in the plane. We present a faster algorithm than previously known.",
        "published": "2002-04-25T20:41:10Z",
        "link": "http://arxiv.org/abs/cs/0204050v1",
        "categories": [
            "cs.CG",
            "I.3.5;F.2.2"
        ]
    },
    {
        "title": "Instabilities of Robot Motion",
        "authors": [
            "Michael Farber"
        ],
        "summary": "Instabilities of robot motion are caused by topological reasons. In this paper we find a relation between the topological properties of a configuration space (the structure of its cohomology algebra) and the character of instabilities, which are unavoidable in any motion planning algorithm. More specifically, let $X$ denote the space of all admissible configurations of a mechanical system. A {\\it motion planner} is given by a splitting $X\\times X = F_1\\cup F_2\\cup ... \\cup F_k$ (where $F_1, ..., F_k$ are pairwise disjoint ENRs, see below) and by continuous maps $s_j: F_j \\to PX,$ such that $E\\circ s_j =1_{F_j}$. Here $PX$ denotes the space of all continuous paths in $X$ (admissible motions of the system) and $E: PX\\to X\\times X$ denotes the map which assigns to a path the pair of its initial -- end points. Any motion planner determines an algorithm of motion planning for the system. In this paper we apply methods of algebraic topology to study the minimal number of sets $F_j$ in any motion planner in $X$. We also introduce a new notion of {\\it order of instability} of a motion planner; it describes the number of essentially distinct motions which may occur as a result of small perturbations of the input data. We find the minimal order of instability, which may have motion planners on a given configuration space $X$. We study a number of specific problems: motion of a rigid body in $\\R^3$, a robot arm, motion in $\\R^3$ in the presence of obstacles, and others.",
        "published": "2002-05-12T11:23:21Z",
        "link": "http://arxiv.org/abs/cs/0205015v1",
        "categories": [
            "cs.RO",
            "cs.CG",
            "math.AT",
            "I.2.9; I.3.5"
        ]
    },
    {
        "title": "A quasi-RBF technique for numerical discretization of PDE's",
        "authors": [
            "W. Chen"
        ],
        "summary": "Atkinson developed a strategy which splits solution of a PDE system into homogeneous and particular solutions, where the former have to satisfy the boundary and governing equation, while the latter only need to satisfy the governing equation without concerning geometry. Since the particular solution can be solved irrespective of boundary shape, we can use a readily available fast Fourier or orthogonal polynomial technique O(NlogN) to evaluate it in a regular box or sphere surrounding physical domain. The distinction of this study is that we approximate homogeneous solution with nonsingular general solution RBF as in the boundary knot method. The collocation method using general solution RBF has very high accuracy and spectral convergent speed and is a simple, truly meshfree approach for any complicated geometry. More importantly, the use of nonsingular general solution avoids the controversial artificial boundary in the method of fundamental solution due to the singularity of fundamental solution.",
        "published": "2002-05-14T13:58:48Z",
        "link": "http://arxiv.org/abs/cs/0205020v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "Distance function wavelets - Part II: Extended results and conjectures",
        "authors": [
            "W. Chen"
        ],
        "summary": "Report II is concerned with the extended results of distance function wavelets (DFW). The fractional DFW transforms are first addressed relating to the fractal geometry and fractional derivative, and then, the discrete Helmholtz-Fourier transform is briefly presented. The Green second identity may be an alternative devise in developing the theoretical framework of the DFW transform and series. The kernel solutions of the Winkler plate equation and the Burger's equation are used to create the DFW transforms and series. Most interestingly, it is found that the translation invariant monomial solutions of the high-order Laplace equations can be used to make very simple harmonic polynomial DFW series. In most cases of this study, solid mathematical analysis is missing and results are obtained intuitively in the conjecture status.",
        "published": "2002-05-24T12:07:28Z",
        "link": "http://arxiv.org/abs/cs/0205063v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.9"
        ]
    },
    {
        "title": "Building Space-Time Meshes over Arbitrary Spatial Domains",
        "authors": [
            "Jeff Erickson",
            "Damrong Guoy",
            "John M. Sullivan",
            "Alper Üngör"
        ],
        "summary": "We present an algorithm to construct meshes suitable for space-time discontinuous Galerkin finite-element methods. Our method generalizes and improves the `Tent Pitcher' algorithm of \\\"Ung\\\"or and Sheffer. Given an arbitrary simplicially meshed domain M of any dimension and a time interval [0,T], our algorithm builds a simplicial mesh of the space-time domain Mx[0,T], in constant time per element. Our algorithm avoids the limitations of previous methods by carefully adapting the durations of space-time elements to the local quality and feature size of the underlying space mesh.",
        "published": "2002-06-01T23:20:51Z",
        "link": "http://arxiv.org/abs/cs/0206002v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Implementation and complexity of the watershed-from-markers algorithm   computed as a minimal cost forest",
        "authors": [
            "Petr Felkel",
            "Mario Bruckwschwaiger",
            "Rainer Wegenkittl"
        ],
        "summary": "The watershed algorithm belongs to classical algorithms in mathematical morphology. Lotufo et al. published a principle of the watershed computation by means of an Image Foresting Transform (IFT), which computes a shortest path forest from given markers. The algorithm itself was described for a 2D case (image) without a detailed discussion of its computation and memory demands for real datasets. As IFT cleverly solves the problem of plateaus and as it gives precise results when thin objects have to be segmented, it is obvious to use this algorithm for 3D datasets taking in mind the minimizing of a higher memory consumption for the 3D case without loosing low asymptotical time complexity of O(m+C) (and also the real computation speed). The main goal of this paper is an implementation of the IFT algorithm with a priority queue with buckets and careful tuning of this implementation to reach as minimal memory consumption as possible.   The paper presents five possible modifications and methods of implementation of the IFT algorithm. All presented implementations keep the time complexity of the standard priority queue with buckets but the best one minimizes the costly memory allocation and needs only 19-45% of memory for typical 3D medical imaging datasets. Memory saving was reached by an IFT algorithm simplification, which stores more elements in temporary structures but these elements are simpler and thus need less memory.   The best presented modification allows segmentation of large 3D medical datasets (up to 512x512x680 voxels) with 12-or 16-bits per voxel on currently available PC based workstations.",
        "published": "2002-06-04T13:08:24Z",
        "link": "http://arxiv.org/abs/cs/0206009v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "I.3.5 [Computational Geometry and Object Modeling]"
        ]
    },
    {
        "title": "Common transversals and tangents to two lines and two quadrics in P^3",
        "authors": [
            "Gábor Megyesi",
            "Frank Sottile",
            "Thorsten Theobald"
        ],
        "summary": "We solve the following geometric problem, which arises in several three-dimensional applications in computational geometry: For which arrangements of two lines and two spheres in R^3 are there infinitely many lines simultaneously transversal to the two lines and tangent to the two spheres?   We also treat a generalization of this problem to projective quadrics: Replacing the spheres in R^3 by quadrics in projective space P^3, and fixing the lines and one general quadric, we give the following complete geometric description of the set of (second) quadrics for which the 2 lines and 2 quadrics have infinitely many transversals and tangents: In the nine-dimensional projective space P^9 of quadrics, this is a curve of degree 24 consisting of 12 plane conics, a remarkably reducible variety.",
        "published": "2002-06-05T15:15:53Z",
        "link": "http://arxiv.org/abs/math/0206044v1",
        "categories": [
            "math.AG",
            "cs.CG",
            "math.AC",
            "13P10, 14N10, 14Q15, 51N20, 68U05"
        ]
    },
    {
        "title": "High-order fundamental and general solutions of convection-diffusion   equation and their applications with boundary particle method",
        "authors": [
            "W. Chen"
        ],
        "summary": "In this study, we presented the high-order fundamental solutions and general solutions of convection-diffusion equation. To demonstrate their efficacy, we applied the high-order general solutions to the boundary particle method (BPM) for the solution of some inhomogeneous convection-diffusion problems, where the BPM is a new truly boundary-only meshfree collocation method based on multiple reciprocity principle. For the sake of completeness, the BPM is also briefly described here.",
        "published": "2002-06-08T10:46:56Z",
        "link": "http://arxiv.org/abs/cs/0206013v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "Distance function wavelets - Part III: \"Exotic\" transforms and series",
        "authors": [
            "W. Chen"
        ],
        "summary": "Part III of the reports consists of various unconventional distance function wavelets (DFW). The dimension and the order of partial differential equation (PDE) are first used as a substitute of the scale parameter in the DFW transforms and series, especially with the space and time-space potential problems. It is noted that the recursive multiple reciprocity formulation is the DFW series. The Green second identity is used to avoid the singularity of the zero-order fundamental solution in creating the DFW series. The fundamental solutions of various composite PDEs are found very flexible and efficient to handle a borad range of problems. We also discuss the underlying connections between the crucial concepts of dimension, scale and the order of PDE through the analysis of dissipative acoustic wave propagation. The shape parameter of the potential problems is also employed as the \"scale parameter\" to create the non-orthogonal DFW. This paper also briefly discusses and conjectures the DFW correspondences of a variety of coordinate variable transforms and series. Practically important, the anisotropic and inhomogeneous DFW's are developed by using the geodesic distance variable. The DFW and the related basis functions are also used in making the kernel distance sigmoidal functions, which are potentially useful in the artificial neural network and machine learning. As or even worse than the preceding two reports, this study scarifies mathematical rigor and in turn unfetter imagination. Most results are intuitively obtained without rigorous analysis. Follow-up research is still under way. The paper is intended to inspire more research into this promising area.",
        "published": "2002-06-10T09:01:53Z",
        "link": "http://arxiv.org/abs/cs/0206016v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.9"
        ]
    },
    {
        "title": "On Simultaneous Graph Embedding",
        "authors": [
            "C. A. Duncan",
            "A. Efrat",
            "C. Erten",
            "S. Kobourov",
            "J. S. B. Mitchell"
        ],
        "summary": "We consider the problem of simultaneous embedding of planar graphs. There are two variants of this problem, one in which the mapping between the vertices of the two graphs is given and another where the mapping is not given. In particular, we show that without mapping, any number of outerplanar graphs can be embedded simultaneously on an $O(n)\\times O(n)$ grid, and an outerplanar and general planar graph can be embedded simultaneously on an $O(n^2)\\times O(n^3)$ grid. If the mapping is given, we show how to embed two paths on an $n \\times n$ grid, a caterpillar and a path on an $n \\times 2n$ grid, or two caterpillar graphs on an $O(n^2)\\times O(n^3)$ grid. We also show that 5 paths, or 3 caterpillars, or two general planar graphs cannot be simultaneously embedded given the mapping.",
        "published": "2002-06-11T23:53:35Z",
        "link": "http://arxiv.org/abs/cs/0206018v3",
        "categories": [
            "cs.CG",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Simultaneous Embedding of a Planar Graph and Its Dual on the Grid",
        "authors": [
            "C. Erten",
            "S. G. Kobourov"
        ],
        "summary": "Traditional representations of graphs and their duals suggest the requirement that the dual vertices be placed inside their corresponding primal faces, and the edges of the dual graph cross only their corresponding primal edges. We consider the problem of simultaneously embedding a planar graph and its dual into a small integer grid such that the edges are drawn as straight-line segments and the only crossings are between primal-dual pairs of edges. We provide a linear-time algorithm that simultaneously embeds a 3-connected planar graph and its dual on a (2n-2) by (2n-2) integer grid, where n is the total number of vertices in the graph and its dual. Furthermore our embedding algorithm satisfies the two natural requirements mentioned above.",
        "published": "2002-06-12T02:02:48Z",
        "link": "http://arxiv.org/abs/cs/0206019v1",
        "categories": [
            "cs.CG",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Optimally cutting a surface into a disk",
        "authors": [
            "Jeff Erickson",
            "Sariel Har-Peled"
        ],
        "summary": "We consider the problem of cutting a set of edges on a polyhedral manifold surface, possibly with boundary, to obtain a single topological disk, minimizing either the total number of cut edges or their total length. We show that this problem is NP-hard, even for manifolds without boundary and for punctured spheres. We also describe an algorithm with running time n^{O(g+k)}, where n is the combinatorial complexity, g is the genus, and k is the number of boundary components of the input surface. Finally, we describe a greedy algorithm that outputs a O(log^2 g)-approximation of the minimum cut graph in O(g^2 n log n) time.",
        "published": "2002-07-02T22:03:51Z",
        "link": "http://arxiv.org/abs/cs/0207004v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.GR",
            "F.2.2; I.3.5; G.2.m"
        ]
    },
    {
        "title": "Symmetric boundary knot method",
        "authors": [
            "W. Chen"
        ],
        "summary": "The boundary knot method (BKM) is a recent boundary-type radial basis function (RBF) collocation scheme for general PDEs. Like the method of fundamental solution (MFS), the RBF is employed to approximate the inhomogeneous terms via the dual reciprocity principle. Unlike the MFS, the method uses a nonsingular general solution instead of a singular fundamental solution to evaluate the homogeneous solution so as to circumvent the controversial artificial boundary outside the physical domain. The BKM is meshfree, superconvergent, integration free, very easy to learn and program. The original BKM, however, loses symmetricity in the presense of mixed boundary. In this study, by analogy with Hermite RBF interpolation, we developed a symmetric BKM scheme. The accuracy and efficiency of the symmetric BKM are also numerically validated in some 2D and 3D Helmholtz and diffusion reaction problems under complicated geometries.",
        "published": "2002-07-03T20:17:31Z",
        "link": "http://arxiv.org/abs/cs/0207010v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.9"
        ]
    },
    {
        "title": "New advances in dual reciprocity and boundary-only RBF methods",
        "authors": [
            "W. Chen",
            "M. Tanaka"
        ],
        "summary": "This paper made some significant advances in the dual reciprocity and boundary-only RBF techniques. The proposed boundary knot method (BKM) is different from the standard boundary element method in a number of important aspects. Namely, it is truly meshless, exponential convergence, integration-free (of course, no singular integration), boundary-only for general problems, and leads to symmetric matrix under certain conditions (able to be extended to general cases after further modified). The BKM also avoids the artificial boundary in the method of fundamental solution. An amazing finding is that the BKM can formulate linear modeling equations for nonlinear partial differential systems with linear boundary conditions. This merit makes it circumvent all perplexing issues in the iteration solution of nonlinear equations. On the other hand, by analogy with Green's second identity, this paper also presents a general solution RBF (GSR) methodology to construct efficient RBFs in the dual reciprocity and domain-type RBF collocation methods. The GSR approach first establishes an explicit relationship between the BEM and RBF itself on the ground of the weighted residual principle. This paper also discusses the RBF convergence and stability problems within the framework of integral equation theory.",
        "published": "2002-07-04T12:10:06Z",
        "link": "http://arxiv.org/abs/cs/0207015v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "Relationship between boundary integral equation and radial basis   function",
        "authors": [
            "W. Chen",
            "M. Tanaka"
        ],
        "summary": "This paper aims to survey our recent work relating to the radial basis function (RBF) from some new views of points. In the first part, we established the RBF on numerical integration analysis based on an intrinsic relationship between the Green's boundary integral representation and RBF. It is found that the kernel function of integral equation is important to create efficient RBF. The fundamental solution RBF (FS-RBF) was presented as a novel strategy constructing operator-dependent RBF. We proposed a conjecture formula featuring the dimension affect on error bound to show the independent-dimension merit of the RBF techniques. We also discussed wavelet RBF, localized RBF schemes, and the influence of node placement on the RBF solution accuracy. The centrosymmetric matrix structure of the RBF interpolation matrix under symmetric node placing is proved.   The second part of this paper is concerned with the boundary knot method (BKM), a new boundary-only, meshless, spectral convergent, integration-free RBF collocation technique. The BKM was tested to the Helmholtz, Laplace, linear and nonlinear convection-diffusion problems. In particular, we introduced the response knot-dependent nonsingular general solution to calculate varying-parameter and nonlinear steady convection-diffusion problems very efficiently. By comparing with the multiple dual reciprocity method, we discussed the completeness issue of the BKM.   Finally, the nonsingular solutions for some known differential operators were given in appendix. Also we expanded the RBF concepts by introducing time-space RBF for transient problems.",
        "published": "2002-07-04T12:15:11Z",
        "link": "http://arxiv.org/abs/cs/0207016v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.9"
        ]
    },
    {
        "title": "New Insights in Boundary-only and Domain-type RBF Methods",
        "authors": [
            "W. Chen",
            "M. Tanaka"
        ],
        "summary": "This paper has made some significant advances in the boundary-only and domain-type RBF techniques. The proposed boundary knot method (BKM) is different from the standard boundary element method in a number of important aspects. Namely, it is truly meshless, exponential convergence, integration-free (of course, no singular integration), boundary-only for general problems, and leads to symmetric matrix under certain conditions (able to be extended to general cases after further modified). The BKM also avoids the artificial boundary in the method of fundamental solution. An amazing finding is that the BKM can formulate linear modeling equations for nonlinear partial differential systems with linear boundary conditions. This merit makes it circumvent all perplexing issues in the iteration solution of nonlinear equations. On the other hand, by analogy with Green's second identity, we also presents a general solution RBF (GSR) methodology to construct efficient RBFs in the domain-type RBF collocation method and dual reciprocity method. The GSR approach first establishes an explicit relationship between the BEM and RBF itself on the ground of the potential theory. This paper also discusses some essential issues relating to the RBF computing, which include time-space RBFs, direct and indirect RBF schemes, finite RBF method, and the application of multipole and wavelet to the RBF solution of the PDEs.",
        "published": "2002-07-04T12:18:06Z",
        "link": "http://arxiv.org/abs/cs/0207017v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "Definitions of distance function in radial basis function approach",
        "authors": [
            "W. Chen"
        ],
        "summary": "Very few studies involve how to construct the efficient RBFs by means of problem features. Recently the present author presented general solution RBF (GS-RBF) methodology to create operator-dependent RBFs successfully [1]. On the other hand, the normal radial basis function (RBF) is defined via Euclidean space distance function or the geodesic distance [2]. This purpose of this note is to redefine distance function in conjunction with problem features, which include problem-dependent and time-space distance function.",
        "published": "2002-07-04T12:20:24Z",
        "link": "http://arxiv.org/abs/cs/0207018v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "Reducing the Computational Requirements of the Differential Quadrature   Method",
        "authors": [
            "W Chen",
            "Xinwei Wang",
            "Yongxi Yu"
        ],
        "summary": "This paper shows that the weighting coefficient matrices of the differential quadrature method (DQM) are centrosymmetric or skew-centrosymmetric if the grid spacings are symmetric irrespective of whether they are equal or unequal. A new skew centrosymmetric matrix is also discussed. The application of the properties of centrosymmetric and skew centrosymmetric matrix can reduce the computational effort of the DQM for calculations of the inverse, determinant, eigenvectors and eigenvalues by 75%. This computational advantage are also demonstrated via several numerical examples.",
        "published": "2002-07-09T19:53:42Z",
        "link": "http://arxiv.org/abs/cs/0207033v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "A Lyapunov Formulation for Efficient Solution of the Poisson and   Convection-Diffusion Equations by the Differential Quadrature Method",
        "authors": [
            "W. Chen",
            "Tingxiu Zhong"
        ],
        "summary": "Civan and Sliepcevich [1, 2] suggested that special matrix solver should be developed to further reduce the computing effort in applying the differential quadrature (DQ) method for the Poisson and convection-diffusion equations. Therefore, the purpose of the present communication is to introduce and apply the Lyapunov formulation which can be solved much more efficiently than the Gaussian elimination method. Civan and Sliepcevich [2] first presented DQ approximate formulas in polynomial form for partial derivatives in tow-dimensional variable domain. For simplifying formulation effort, Chen et al. [3] proposed the compact matrix form of these DQ approximate formulas. In this study, by using these matrix approximate formulas, the DQ formulations for the Poisson and convection-diffusion equations can be expressed as the Lyapunov algebraic matrix equation. The formulation effort is simplified, and a simple and explicit matrix formulation is obtained. A variety of fast algorithms in the solution of the Lyapunov equation [4-6] can be successfully applied in the DQ analysis of these two-dimensional problems, and, thus, the computing effort can be greatly reduced. Finally, we also point out that the present reduction technique can be easily extended to the three-dimensional cases.",
        "published": "2002-07-09T20:26:18Z",
        "link": "http://arxiv.org/abs/cs/0207035v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.2"
        ]
    },
    {
        "title": "Dual reciprocity BEM and dynamic programming filter for inverse   elastodynamic problems",
        "authors": [
            "Masataka Tanaka",
            "W Chen"
        ],
        "summary": "This paper presents the first coupling application of the dual reciprocity BEM (DRBEM) and dynamic programming filter to inverse elastodynamic problem. The DRBEM is the only BEM method, which does not require domain discretization for general linear and nonlinear dynamic problems. Since the size of numerical discretization system has a great effect on the computing effort of recursive or iterative calculations of inverse analysis, the intrinsic boundary-only merit of the DRBEM causes a considerable computational saving. On the other hand, the strengths of the dynamic programming filter lie in its mathematical simplicity, easy to program and great flexibility in the type, number and locations of measurements and unknown inputs. The combination of these two techniques is therefore very attractive for the solution of practical inverse problems. In this study, the spatial and temporal partial derivatives of the governing equation are respectively discretized first by the DRBEM and the precise integration method, and then, by using dynamic programming with regularization, dynamic load is estimated based on noisy measurements of velocity and displacement at very few locations. Numerical experiments involved with the periodic and Heaviside impact load are conducted to demonstrate the applicability, efficiency and simplicity of this strategy. The affect of noise level, regularization parameter, and measurement types on the estimation is also investigated.",
        "published": "2002-07-10T08:00:42Z",
        "link": "http://arxiv.org/abs/cs/0207039v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.2"
        ]
    },
    {
        "title": "RBF-based meshless boundary knot method and boundary particle method",
        "authors": [
            "W. Chen"
        ],
        "summary": "This paper is concerned with the two new boundary-type radial basis function collocation schemes, boundary knot method (BKM) and boundary particle method (BPM). The BKM is developed based on the dual reciprocity theorem, while the BPM employs the multiple reciprocity technique. Unlike the method of fundamental solution, the wto methods use the nonsingular general solutions instead of singular fundamental solution to circumvent the controversial artificial boundary outside physical domain. Compared with the boundary element method, both the BKM and BPM are meshfree, superconvergent, meshfree, integration free, symmetric, and mathematically simple collocation techniques for general PDEs. In particular, the BPM does not require any inner nodes for inhomogeneous problems. In this study, the accuracy and efficiency of the two methods are numerically demonstrated to some 2D, 3D Helmholtz and convection-diffusion problems under complicated geometries.",
        "published": "2002-07-10T20:31:29Z",
        "link": "http://arxiv.org/abs/cs/0207041v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "A meshless, integration-free, and boundary-only RBF technique",
        "authors": [
            "W. Chen",
            "M. Tanaka"
        ],
        "summary": "Based on the radial basis function (RBF), non-singular general solution and dual reciprocity method (DRM), this paper presents an inherently meshless, integration-free, boundary-only RBF collocation techniques for numerical solution of various partial differential equation systems. The basic ideas behind this methodology are very mathematically simple. In this study, the RBFs are employed to approximate the inhomogeneous terms via the DRM, while non-singular general solution leads to a boundary-only RBF formulation for homogenous solution. The present scheme is named as the boundary knot method (BKM) to differentiate it from the other numerical techniques. In particular, due to the use of nonsingular general solutions rather than singular fundamental solutions, the BKM is different from the method of fundamental solution in that the former does no require the artificial boundary and results in the symmetric system equations under certain conditions. The efficiency and utility of this new technique are validated through a number of typical numerical examples. Completeness concern of the BKM due to the only use of non-singular part of complete fundamental solution is also discussed.",
        "published": "2002-07-11T12:19:49Z",
        "link": "http://arxiv.org/abs/cs/0207043v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "Physical Traces: Quantum vs. Classical Information Processing",
        "authors": [
            "Samson Abramsky",
            "Bob Coecke"
        ],
        "summary": "Within the Geometry of Interaction (GoI) paradigm, we present a setting that enables qualitative differences between classical and quantum processes to be explored. The key construction is the physical interpretation/realization of the traced monoidal categories of finite-dimensional vector spaces with tensor product as monoidal structure and of finite sets and relations with Cartesian product as monoidal structure, both of them providing a so-called wave-style GoI. The developments in this paper reveal that envisioning state update due to quantum measurement as a process provides a powerful tool for developing high-level approaches to quantum information processing.",
        "published": "2002-07-14T15:50:00Z",
        "link": "http://arxiv.org/abs/cs/0207057v2",
        "categories": [
            "cs.CG",
            "cs.LO",
            "math.CT",
            "quant-ph",
            "F.1, F.2"
        ]
    },
    {
        "title": "Parallel Delaunay Refinement: Algorithms and Analyses",
        "authors": [
            "Dan A. Spielman",
            "Shang-hua Teng",
            "Alper Ungor"
        ],
        "summary": "In this paper, we analyze the complexity of natural parallelizations of Delaunay refinement methods for mesh generation. The parallelizations employ a simple strategy: at each iteration, they choose a set of ``independent'' points to insert into the domain, and then update the Delaunay triangulation. We show that such a set of independent points can be constructed efficiently in parallel and that the number of iterations needed is $O(\\log^2(L/s))$, where $L$ is the diameter of the domain, and $s$ is the smallest edge in the output mesh. In addition, we show that the insertion of each independent set of points can be realized sequentially by Ruppert's method in two dimensions and Shewchuk's in three dimensions. Therefore, our parallel Delaunay refinement methods provide the same element quality and mesh size guarantees as the sequential algorithms in both two and three dimensions. For quasi-uniform meshes, such as those produced by Chew's method, we show that the number of iterations can be reduced to $O(\\log(L/s))$. To the best of our knowledge, these are the first provably polylog$(L/s)$ parallel time Delaunay meshing algorithms that generate well-shaped meshes of size optimal to within a constant.",
        "published": "2002-07-15T20:20:34Z",
        "link": "http://arxiv.org/abs/cs/0207063v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Moebius-Invariant Natural Neighbor Interpolation",
        "authors": [
            "Marshall Bern",
            "David Eppstein"
        ],
        "summary": "We propose an interpolation method that is invariant under Moebius transformations; that is, interpolation followed by transformation gives the same result as transformation followed by interpolation. The method uses natural (Delaunay) neighbors, but weights neighbors according to angles formed by Delaunay circles.",
        "published": "2002-07-24T19:00:19Z",
        "link": "http://arxiv.org/abs/cs/0207081v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "A direct time-domain FEM modeling of broadband frequency-dependent   absorption with the presence of matrix fractional power: Model I",
        "authors": [
            "W Chen"
        ],
        "summary": "The frequency-dependent attenuation of broadband acoustics is often confronted in many different areas. However, the related time domain simulation is rarely found in literature due to enormous technical difficulty. The currently popular relaxation models with the presence of convolution operation require some material parameters which are not readily available. In this study, three reports are contributed to address broadband ultrasound frequency-dependent absorptions using the readily available empirical parameters. This report is the first in series concerned with developing a direct time domain FEM formulation. The next two reports are about the frequency decomposition model and the fractional derivative model.",
        "published": "2002-08-20T12:36:06Z",
        "link": "http://arxiv.org/abs/cs/0208030v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.9"
        ]
    },
    {
        "title": "A Survey and a New Competitive Method for the Planar min-# Problem",
        "authors": [
            "Lilian Buzer"
        ],
        "summary": "We survey most of the different types of approximation algorithms which minimize the number of output vertices. We present their main qualities and their inherent drawbacks.",
        "published": "2002-09-04T01:33:58Z",
        "link": "http://arxiv.org/abs/cs/0209007v2",
        "categories": [
            "cs.CG",
            "G.1.2; I.3.5; F.2.2"
        ]
    },
    {
        "title": "Remarks on d-Dimensional TSP Optimal Tour Length Behaviour",
        "authors": [
            "A. G. Yaneff"
        ],
        "summary": "The well-known $O(n^{1-1/d})$ behaviour of the optimal tour length for TSP in d-dimensional Cartesian space causes breaches of the triangle inequality. Other practical inadequacies of this model are discussed, including its use as basis for approximation of the TSP optimal tour length or bounds derivations, which I attempt to remedy.",
        "published": "2002-09-23T21:53:22Z",
        "link": "http://arxiv.org/abs/cs/0209027v1",
        "categories": [
            "cs.CG",
            "F.2.2., G.2.2"
        ]
    },
    {
        "title": "An Algorithmic Study of Manufacturing Paperclips and Other Folded   Structures",
        "authors": [
            "Esther M. Arkin",
            "Sandor P. Fekete",
            "Joseph S. B. Mitchell"
        ],
        "summary": "We study algorithmic aspects of bending wires and sheet metal into a specified structure. Problems of this type are closely related to the question of deciding whether a simple non-self-intersecting wire structure (a carpenter's ruler) can be straightened, a problem that was open for several years and has only recently been solved in the affirmative.   If we impose some of the constraints that are imposed by the manufacturing process, we obtain quite different results. In particular, we study the variant of the carpenter's ruler problem in which there is a restriction that only one joint can be modified at a time. For a linkage that does not self-intersect or self-touch, the recent results of Connelly et al. and Streinu imply that it can always be straightened, modifying one joint at a time. However, we show that for a linkage with even a single vertex degeneracy, it becomes NP-hard to decide if it can be straightened while altering only one joint at a time. If we add the restriction that each joint can be altered at most once, we show that the problem is NP-complete even without vertex degeneracies.   In the special case, arising in wire forming manufacturing, that each joint can be altered at most once, and must be done sequentially from one or both ends of the linkage, we give an efficient algorithm to determine if a linkage can be straightened.",
        "published": "2002-09-30T19:42:57Z",
        "link": "http://arxiv.org/abs/cs/0209034v1",
        "categories": [
            "cs.CG",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "On the Reflexivity of Point Sets",
        "authors": [
            "Esther M. Arkin",
            "Sandor P. Fekete",
            "Ferran Hurtado",
            "Joseph S. B. Mitchell",
            "Marc Noy",
            "Vera Sacristan",
            "Saurabh Sethia"
        ],
        "summary": "We introduce a new measure for planar point sets S that captures a combinatorial distance that S is from being a convex set: The reflexivity rho(S) of S is given by the smallest number of reflex vertices in a simple polygonalization of S. We prove various combinatorial bounds and provide efficient algorithms to compute reflexivity, both exactly (in special cases) and approximately (in general). Our study considers also some closely related quantities, such as the convex cover number kappa_c(S) of a planar point set, which is the smallest number of convex chains that cover S, and the convex partition number kappa_p(S), which is given by the smallest number of convex chains with pairwise-disjoint convex hulls that cover S. We have proved that it is NP-complete to determine the convex cover or the convex partition number and have given logarithmic-approximation algorithms for determining each.",
        "published": "2002-10-01T14:40:29Z",
        "link": "http://arxiv.org/abs/cs/0210003v2",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Criteria for Balance in Abelian Gain Graphs, with Applications to   Piecewise-Linear Geometry",
        "authors": [
            "Konstantin Rybnikov",
            "Thomas Zaslavsky"
        ],
        "summary": "A gain graph is a triple (G,h,H), where G is a connected graph with an arbitrary, but fixed, orientation of edges, H is a group, and h is a homomorphism from the free group on the edges of G to H. A gain graph is called balanced if the h-image of each closed walk on G is the identity.   Consider a gain graph with abelian gain group having no odd torsion. If there is a basis of the graph's binary cycle space each of whose members can be lifted to a closed walk whose gain is the identity, then the gain graph is balanced, provided that the graph is finite or the group has no nontrivial infinitely 2-divisible elements. We apply this theorem to deduce a result on the projective geometry of piecewise-linear realizations of cell-decompositions of manifolds.",
        "published": "2002-10-03T16:33:59Z",
        "link": "http://arxiv.org/abs/math/0210052v5",
        "categories": [
            "math.CO",
            "cs.CG",
            "cs.DM",
            "cs.DS",
            "math.AT",
            "Primary: 05C22, 52C25; Secondary: 05C38, 52C25, 52C22"
        ]
    },
    {
        "title": "Compact Floor-Planning via Orderly Spanning Trees",
        "authors": [
            "Chien-Chih Liao",
            "Hsueh-I Lu",
            "Hsu-Chun Yen"
        ],
        "summary": "Floor-planning is a fundamental step in VLSI chip design. Based upon the concept of orderly spanning trees, we present a simple O(n)-time algorithm to construct a floor-plan for any n-node plane triangulation. In comparison with previous floor-planning algorithms in the literature, our solution is not only simpler in the algorithm itself, but also produces floor-plans which require fewer module types. An equally important aspect of our new algorithm lies in its ability to fit the floor-plan area in a rectangle of size (n-1)x(2n+1)/3. Lower bounds on the worst-case area for floor-planning any plane triangulation are also provided in the paper.",
        "published": "2002-10-17T06:47:02Z",
        "link": "http://arxiv.org/abs/cs/0210016v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2; E.1; G.2.2; B.7.2"
        ]
    },
    {
        "title": "Tetris is Hard, Even to Approximate",
        "authors": [
            "Erik D. Demaine",
            "Susan Hohenberger",
            "David Liben-Nowell"
        ],
        "summary": "In the popular computer game of Tetris, the player is given a sequence of tetromino pieces and must pack them into a rectangular gameboard initially occupied by a given configuration of filled squares; any completely filled row of the gameboard is cleared and all pieces above it drop by one row. We prove that in the offline version of Tetris, it is NP-complete to maximize the number of cleared rows, maximize the number of tetrises (quadruples of rows simultaneously filled and cleared), minimize the maximum height of an occupied square, or maximize the number of pieces placed before the game ends. We furthermore show the extreme inapproximability of the first and last of these objectives to within a factor of p^(1-epsilon), when given a sequence of p pieces, and the inapproximability of the third objective to within a factor of (2 - epsilon), for any epsilon>0. Our results hold under several variations on the rules of Tetris, including different models of rotation, limitations on player agility, and restricted piece sets.",
        "published": "2002-10-21T18:32:39Z",
        "link": "http://arxiv.org/abs/cs/0210020v1",
        "categories": [
            "cs.CC",
            "cs.CG",
            "cs.DM",
            "F.1.3; F.2.2; G.2.1; K.8.0"
        ]
    },
    {
        "title": "Algebraic methods for computing smallest enclosing and circumscribing   cylinders of simplices",
        "authors": [
            "R. Brandenberg",
            "T. Theobald"
        ],
        "summary": "We provide an algebraic framework to compute smallest enclosing and smallest circumscribing cylinders of simplices in Euclidean space $\\E^n$. Explicitly, the computation of a smallest enclosing cylinder in $\\mathbb{E}^3$ is reduced to the computation of a smallest circumscribing cylinder. We improve existing polynomial formulations to compute the locally extreme circumscribing cylinders in $\\E^3$ and exhibit subclasses of simplices where the algebraic degrees can be further reduced. Moreover, we generalize these efficient formulations to the $n$-dimensional case and provide bounds on the number of local extrema. Using elementary invariant theory, we prove structural results on the direction vectors of any locally extreme circumscribing cylinder for regular simplices.",
        "published": "2002-11-21T15:57:04Z",
        "link": "http://arxiv.org/abs/math/0211344v1",
        "categories": [
            "math.OC",
            "cs.CG",
            "51N20; 52B55; 68U05; 68W30; 90C90"
        ]
    },
    {
        "title": "Toric ideals, real toric varieties, and the algebraic moment map",
        "authors": [
            "Frank Sottile"
        ],
        "summary": "This is a tutorial on some aspects of toric varieties related to their potential use in geometric modeling. We discuss projective toric varieties and their ideals, as well as real toric varieties and the algebraic moment map. In particular, we explain the relation between linear precision and the algebraic moment map. This builds on the introduction to toric varieties by David Cox: What is a Toric Variety? at http://www.cs.amherst.edu/~dac/lectures/tutorial.ps",
        "published": "2002-12-03T17:41:40Z",
        "link": "http://arxiv.org/abs/math/0212044v3",
        "categories": [
            "math.AG",
            "cs.CG",
            "14M25, 14Q99, 13P10, 68U05, 68U07; ACM I.3.5"
        ]
    },
    {
        "title": "Optimized Color Gamuts for Tiled Displays",
        "authors": [
            "Marshall Bern",
            "David Eppstein"
        ],
        "summary": "We consider the problem of finding a large color space that can be generated by all units in multi-projector tiled display systems. Viewing the problem geometrically as one of finding a large parallelepiped within the intersection of multiple parallelepipeds, and using colorimetric principles to define a volume-based objective function for comparing feasible solutions, we develop an algorithm for finding the optimal gamut in time O(n^3), where n denotes the number of projectors in the system. We also discuss more efficient quasiconvex programming algorithms for alternative objective functions based on maximizing the quality of the color space extrema.",
        "published": "2002-12-06T21:59:17Z",
        "link": "http://arxiv.org/abs/cs/0212007v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "F.2.2"
        ]
    },
    {
        "title": "Computing Conformal Structure of Surfaces",
        "authors": [
            "Xianfeng Gu",
            "Shing-Tung Yau"
        ],
        "summary": "This paper solves the problem of computing conformal structures of general 2-manifolds represented as triangle meshes. We compute conformal structures in the following way: first compute homology bases from simplicial complex structures, then construct dual cohomology bases and diffuse them to harmonic 1-forms. Next, we construct bases of holomorphic differentials. We then obtain period matrices by integrating holomorphic differentials along homology bases. We also study the global conformal mapping between genus zero surfaces and spheres, and between general meshes and planes. Our method of computing conformal structures can be applied to tackle fundamental problems in computer aid design and computer graphics, such as geometry classification and identification, and surface global parametrization.",
        "published": "2002-12-13T05:33:10Z",
        "link": "http://arxiv.org/abs/cs/0212043v1",
        "categories": [
            "cs.GR",
            "cs.CG",
            "I.3.5;F.2.2;G.2.m"
        ]
    },
    {
        "title": "Confluent Drawings: Visualizing Non-planar Diagrams in a Planar Way",
        "authors": [
            "Matthew Dickerson",
            "David Eppstein",
            "Michael T. Goodrich",
            "Jeremy Meng"
        ],
        "summary": "In this paper, we introduce a new approach for drawing diagrams that have applications in software visualization. Our approach is to use a technique we call confluent drawing for visualizing non-planar diagrams in a planar way. This approach allows us to draw, in a crossing-free manner, graphs--such as software interaction diagrams--that would normally have many crossings. The main idea of this approach is quite simple: we allow groups of edges to be merged together and drawn as \"tracks\" (similar to train tracks). Producing such confluent diagrams automatically from a graph with many crossings is quite challenging, however, so we offer two heuristic algorithms to test if a non-planar graph can be drawn efficiently in a confluent way. In addition, we identify several large classes of graphs that can be completely categorized as being either confluently drawable or confluently non-drawable.",
        "published": "2002-12-17T08:01:35Z",
        "link": "http://arxiv.org/abs/cs/0212046v1",
        "categories": [
            "cs.CG",
            "cs.SE",
            "D.2.2"
        ]
    },
    {
        "title": "Open Problems from CCCG 2002",
        "authors": [
            "Erik D. Demaine",
            "Joseph O'Rourke"
        ],
        "summary": "A list of the problems presented on August 12, 2002 at the open-problem session of the 14th Canadian Conference on Computational Geometry held in Lethbridge, Alberta, Canada.",
        "published": "2002-12-22T03:15:56Z",
        "link": "http://arxiv.org/abs/cs/0212050v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Improved Compact Visibility Representation of Planar Graph via   Schnyder's Realizer",
        "authors": [
            "Ching-Chi Lin",
            "Hsueh-I Lu",
            "I-Fan Sun"
        ],
        "summary": "Let $G$ be an $n$-node planar graph. In a visibility representation of $G$, each node of $G$ is represented by a horizontal line segment such that the line segments representing any two adjacent nodes of $G$ are vertically visible to each other. In the present paper we give the best known compact visibility representation of $G$. Given a canonical ordering of the triangulated $G$, our algorithm draws the graph incrementally in a greedy manner. We show that one of three canonical orderings obtained from Schnyder's realizer for the triangulated $G$ yields a visibility representation of $G$ no wider than $\\frac{22n-40}{15}$. Our easy-to-implement O(n)-time algorithm bypasses the complicated subroutines for four-connected components and four-block trees required by the best previously known algorithm of Kant. Our result provides a negative answer to Kant's open question about whether $\\frac{3n-6}{2}$ is a worst-case lower bound on the required width. Also, if $G$ has no degree-three (respectively, degree-five) internal node, then our visibility representation for $G$ is no wider than $\\frac{4n-9}{3}$ (respectively, $\\frac{4n-7}{3}$). Moreover, if $G$ is four-connected, then our visibility representation for $G$ is no wider than $n-1$, matching the best known result of Kant and He. As a by-product, we obtain a much simpler proof for a corollary of Wagner's Theorem on realizers, due to Bonichon, Sa\\\"{e}c, and Mosbah.",
        "published": "2002-12-29T12:41:47Z",
        "link": "http://arxiv.org/abs/cs/0212054v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2; B.7.2; E.1; G.2.2; I.3.6"
        ]
    },
    {
        "title": "Conformal Geometry, Euclidean Space and Geometric Algebra",
        "authors": [
            "Chris Doran",
            "Anthony Lasenby",
            "Joan Lasenby"
        ],
        "summary": "Projective geometry provides the preferred framework for most implementations of Euclidean space in graphics applications. Translations and rotations are both linear transformations in projective geometry, which helps when it comes to programming complicated geometrical operations. But there is a fundamental weakness in this approach - the Euclidean distance between points is not handled in a straightforward manner. Here we discuss a solution to this problem, based on conformal geometry. The language of geometric algebra is best suited to exploiting this geometry, as it handles the interior and exterior products in a single, unified framework. A number of applications are discussed, including a compact formula for reflecting a line off a general spherical surface.",
        "published": "2002-03-22T14:33:34Z",
        "link": "http://arxiv.org/abs/cs/0203026v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "math.MG",
            "I.3.5;I.3.6"
        ]
    },
    {
        "title": "Computer-Generated Photorealistic Hair",
        "authors": [
            "Alice J. Lin"
        ],
        "summary": "This paper presents an efficient method for generating and rendering photorealistic hair in two dimensional pictures. The method consists of three major steps. Simulating an artist drawing is used to design the rough hair shape. A convolution based filter is then used to generate photorealistic hair patches. A refine procedure is finally used to blend the boundaries of the patches with surrounding areas. This method can be used to create all types of photorealistic human hair (head hair, facial hair and body hair). It is also suitable for fur and grass generation. Applications of this method include: hairstyle designing/editing, damaged hair image restoration, human hair animation, virtual makeover of a human, and landscape creation.",
        "published": "2002-06-20T06:21:15Z",
        "link": "http://arxiv.org/abs/cs/0206029v1",
        "categories": [
            "cs.GR",
            "I.3.3"
        ]
    },
    {
        "title": "Optimally cutting a surface into a disk",
        "authors": [
            "Jeff Erickson",
            "Sariel Har-Peled"
        ],
        "summary": "We consider the problem of cutting a set of edges on a polyhedral manifold surface, possibly with boundary, to obtain a single topological disk, minimizing either the total number of cut edges or their total length. We show that this problem is NP-hard, even for manifolds without boundary and for punctured spheres. We also describe an algorithm with running time n^{O(g+k)}, where n is the combinatorial complexity, g is the genus, and k is the number of boundary components of the input surface. Finally, we describe a greedy algorithm that outputs a O(log^2 g)-approximation of the minimum cut graph in O(g^2 n log n) time.",
        "published": "2002-07-02T22:03:51Z",
        "link": "http://arxiv.org/abs/cs/0207004v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.GR",
            "F.2.2; I.3.5; G.2.m"
        ]
    },
    {
        "title": "User software for the next generation",
        "authors": [
            "T. G. Worlton",
            "A. Chatterjee",
            "J. P. Hammonds",
            "P. F. Peterson",
            "D. J. Mikkelson",
            "R. L. Mikkelson"
        ],
        "summary": "New generations of neutron scattering sources and instrumentation are providing challenges in data handling for user software. Time-of-Flight instruments used at pulsed sources typically produce hundreds or thousands of channels of data for each detector segment. New instruments are being designed with thousands to hundreds of thousands of detector segments. High intensity neutron sources make possible parametric studies and texture studies which further increase data handling requirements. The Integrated Spectral Analysis Workbench (ISAW) software developed at Argonne handles large numbers of spectra simultaneously while providing operations to reduce, sort, combine and export the data. It includes viewers to inspect the data in detail in real time. ISAW uses existing software components and packages where feasible and takes advantage of the excellent support for user interface design and network communication in Java. The included scripting language simplifies repetitive operations for analyzing many files related to a given experiment. Recent additions to ISAW include a contour view, a time-slice table view, routines for finding and fitting peaks in data, and support for data from other facilities using the NeXus format. In this paper, I give an overview of features and planned improvements of ISAW. Details of some of the improvements are covered in other presentations at this conference.",
        "published": "2002-10-19T01:27:45Z",
        "link": "http://arxiv.org/abs/cs/0210018v1",
        "categories": [
            "cs.GR",
            "cs.CE",
            "J2;I3.6;I3.3"
        ]
    },
    {
        "title": "Optimized Color Gamuts for Tiled Displays",
        "authors": [
            "Marshall Bern",
            "David Eppstein"
        ],
        "summary": "We consider the problem of finding a large color space that can be generated by all units in multi-projector tiled display systems. Viewing the problem geometrically as one of finding a large parallelepiped within the intersection of multiple parallelepipeds, and using colorimetric principles to define a volume-based objective function for comparing feasible solutions, we develop an algorithm for finding the optimal gamut in time O(n^3), where n denotes the number of projectors in the system. We also discuss more efficient quasiconvex programming algorithms for alternative objective functions based on maximizing the quality of the color space extrema.",
        "published": "2002-12-06T21:59:17Z",
        "link": "http://arxiv.org/abs/cs/0212007v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "F.2.2"
        ]
    },
    {
        "title": "Computing Conformal Structure of Surfaces",
        "authors": [
            "Xianfeng Gu",
            "Shing-Tung Yau"
        ],
        "summary": "This paper solves the problem of computing conformal structures of general 2-manifolds represented as triangle meshes. We compute conformal structures in the following way: first compute homology bases from simplicial complex structures, then construct dual cohomology bases and diffuse them to harmonic 1-forms. Next, we construct bases of holomorphic differentials. We then obtain period matrices by integrating holomorphic differentials along homology bases. We also study the global conformal mapping between genus zero surfaces and spheres, and between general meshes and planes. Our method of computing conformal structures can be applied to tackle fundamental problems in computer aid design and computer graphics, such as geometry classification and identification, and surface global parametrization.",
        "published": "2002-12-13T05:33:10Z",
        "link": "http://arxiv.org/abs/cs/0212043v1",
        "categories": [
            "cs.GR",
            "cs.CG",
            "I.3.5;F.2.2;G.2.m"
        ]
    },
    {
        "title": "Learning from Scarce Experience",
        "authors": [
            "Leonid Peshkin",
            "Christian R. Shelton"
        ],
        "summary": "Searching the space of policies directly for the optimal policy has been one popular method for solving partially observable reinforcement learning problems. Typically, with each change of the target policy, its value is estimated from the results of following that very policy. This requires a large number of interactions with the environment as different polices are considered. We present a family of algorithms based on likelihood ratio estimation that use data gathered when executing one policy (or collection of policies) to estimate the value of a different policy. The algorithms combine estimation and optimization stages. The former utilizes experience to build a non-parametric representation of an optimized function. The latter performs optimization on this estimate. We show positive empirical results and provide the sample complexity bound.",
        "published": "2002-04-20T05:02:53Z",
        "link": "http://arxiv.org/abs/cs/0204043v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.NE",
            "cs.RO",
            "I.2; I.2.8; I.2.11; I.2.6; G.1.6"
        ]
    },
    {
        "title": "Robust Global Localization Using Clustered Particle Filtering",
        "authors": [
            "Javier Nicolas Sanchez",
            "Adam Milstein",
            "Evan Williamson"
        ],
        "summary": "Global mobile robot localization is the problem of determining a robot's pose in an environment, using sensor data, when the starting position is unknown. A family of probabilistic algorithms known as Monte Carlo Localization (MCL) is currently among the most popular methods for solving this problem. MCL algorithms represent a robot's belief by a set of weighted samples, which approximate the posterior probability of where the robot is located by using a Bayesian formulation of the localization problem. This article presents an extension to the MCL algorithm, which addresses its problems when localizing in highly symmetrical environments; a situation where MCL is often unable to correctly track equally probable poses for the robot. The problem arises from the fact that sample sets in MCL often become impoverished, when samples are generated according to their posterior likelihood. Our approach incorporates the idea of clusters of samples and modifies the proposal distribution considering the probability mass of those clusters. Experimental results are presented that show that this new extension to the MCL algorithm successfully localizes in symmetric environments where ordinary MCL often fails.",
        "published": "2002-04-21T01:21:22Z",
        "link": "http://arxiv.org/abs/cs/0204044v1",
        "categories": [
            "cs.RO",
            "cs.AI",
            "I.2.9"
        ]
    },
    {
        "title": "Instabilities of Robot Motion",
        "authors": [
            "Michael Farber"
        ],
        "summary": "Instabilities of robot motion are caused by topological reasons. In this paper we find a relation between the topological properties of a configuration space (the structure of its cohomology algebra) and the character of instabilities, which are unavoidable in any motion planning algorithm. More specifically, let $X$ denote the space of all admissible configurations of a mechanical system. A {\\it motion planner} is given by a splitting $X\\times X = F_1\\cup F_2\\cup ... \\cup F_k$ (where $F_1, ..., F_k$ are pairwise disjoint ENRs, see below) and by continuous maps $s_j: F_j \\to PX,$ such that $E\\circ s_j =1_{F_j}$. Here $PX$ denotes the space of all continuous paths in $X$ (admissible motions of the system) and $E: PX\\to X\\times X$ denotes the map which assigns to a path the pair of its initial -- end points. Any motion planner determines an algorithm of motion planning for the system. In this paper we apply methods of algebraic topology to study the minimal number of sets $F_j$ in any motion planner in $X$. We also introduce a new notion of {\\it order of instability} of a motion planner; it describes the number of essentially distinct motions which may occur as a result of small perturbations of the input data. We find the minimal order of instability, which may have motion planners on a given configuration space $X$. We study a number of specific problems: motion of a rigid body in $\\R^3$, a robot arm, motion in $\\R^3$ in the presence of obstacles, and others.",
        "published": "2002-05-12T11:23:21Z",
        "link": "http://arxiv.org/abs/cs/0205015v1",
        "categories": [
            "cs.RO",
            "cs.CG",
            "math.AT",
            "I.2.9; I.3.5"
        ]
    },
    {
        "title": "Topological robotics: motion planning in projective spaces",
        "authors": [
            "Michael Farber",
            "Serge Tabachnikov",
            "Sergey Yuzvinsky"
        ],
        "summary": "We study an elementary problem of topological robotics: rotation of a line, which is fixed by a revolving joint at a base point: one wants to bring the line from its initial position to a final position by a continuous motion in the space. The final goal is to construct an algorithm which will perform this task once the initial and final positions are given.   Any such motion planning algorithm will have instabilities, which are caused by topological reasons. A general approach to study instabilities of robot motion was suggested recently by the first named author. With any path-connected topological space X one associates a number TC(X), called the topological complexity of X. This number is of fundamental importance for the motion planning problem: TC(X) determines character of instabilities which have all motion planning algorithms in X.   In the present paper we study the topological complexity of real projective spaces. In particular we compute TC(RP^n) for all n<24. Our main result is that (for n distinct from 1, 3, 7) the problem of calculating of TC(RP^n) is equivalent to finding the smallest k such that RP^n can be immersed into the Euclidean space R^{k-1}.",
        "published": "2002-10-02T09:13:38Z",
        "link": "http://arxiv.org/abs/math/0210018v1",
        "categories": [
            "math.AT",
            "cs.RO",
            "math.DG"
        ]
    },
    {
        "title": "Topological Robotics: Subspace Arrangements and Collision Free Motion   Planning",
        "authors": [
            "Michael Farber",
            "Sergey Yuzvinsky"
        ],
        "summary": "We study an elementary problem of the topological robotics: collective motion of a set of $n$ distinct particles which one has to move from an initial configuration to a final configuration, with the requirement that no collisions occur in the process of motion. The ultimate goal is to construct an algorithm which will perform this task once the initial and the final configurations are given. This reduces to a topological problem of finding the topological complexity TC(C_n(\\R^m)) of the configutation space C_n(\\R^m) of $n$ distinct ordered particles in \\R^m. We solve this problem for m=2 (the planar case) and for all odd m, including the case m=3 (particles in the three-dimensional space). We also study a more general motion planning problem in Euclidean space with a hyperplane arrangement as obstacle.",
        "published": "2002-10-08T06:39:32Z",
        "link": "http://arxiv.org/abs/math/0210115v1",
        "categories": [
            "math.AT",
            "cs.RO",
            "math.DG"
        ]
    },
    {
        "title": "Algorithms for Rapidly Dispersing Robot Swarms in Unknown Environments",
        "authors": [
            "Tien-Ruey Hsiang",
            "Esther M. Arkin",
            "Michael Bender",
            "Sandor P. Fekete",
            "Joseph S. B. Mitchell"
        ],
        "summary": "We develop and analyze algorithms for dispersing a swarm of primitive robots in an unknown environment, R. The primary objective is to minimize the makespan, that is, the time to fill the entire region. An environment is composed of pixels that form a connected subset of the integer grid.   There is at most one robot per pixel and robots move horizontally or vertically at unit speed. Robots enter R by means of k>=1 door pixels   Robots are primitive finite automata, only having local communication, local sensors, and a constant-sized memory.   We first give algorithms for the single-door case (i.e., k=1), analyzing the algorithms both theoretically and experimentally. We prove that our algorithms have optimal makespan 2A-1, where A is the area of R.   We next give an algorithm for the multi-door case (k>1), based on a wall-following version of the leader-follower strategy. We prove that our strategy is O(log(k+1))-competitive, and that this bound is tight for our strategy and other related strategies.",
        "published": "2002-12-10T16:36:50Z",
        "link": "http://arxiv.org/abs/cs/0212022v1",
        "categories": [
            "cs.RO",
            "I.2.9"
        ]
    },
    {
        "title": "Qualitative Study of a Robot Arm as a Hamiltonian System",
        "authors": [
            "G. A. Monerat",
            "E. V. Correa Silva",
            "A. G. Cyrino"
        ],
        "summary": "A double pendulum subject to external torques is used as a model to study the stability of a planar manipulator with two links and two rotational driven joints. The hamiltonian equations of motion and the fixed points (stationary solutions) in phase space are determined. Under suitable conditions, the presence of constant torques does not change the number of fixed points, and preserves the topology of orbits in their linear neighborhoods; two equivalent invariant manifolds are observed, each corresponding to a saddle-center fixed point.",
        "published": "2002-12-11T12:16:47Z",
        "link": "http://arxiv.org/abs/cs/0212027v1",
        "categories": [
            "cs.RO",
            "I.2.9"
        ]
    },
    {
        "title": "The Deductive Database System LDL++",
        "authors": [
            "Faiz Arni",
            "KayLiang Ong",
            "Shalom Tsur",
            "Haixun Wang",
            "Carlo Zaniolo"
        ],
        "summary": "This paper describes the LDL++ system and the research advances that have enabled its design and development. We begin by discussing the new nonmonotonic and nondeterministic constructs that extend the functionality of the LDL++ language, while preserving its model-theoretic and fixpoint semantics. Then, we describe the execution model and the open architecture designed to support these new constructs and to facilitate the integration with existing DBMSs and applications. Finally, we describe the lessons learned by using LDL++ on various tested applications, such as middleware and datamining.",
        "published": "2002-02-01T05:00:24Z",
        "link": "http://arxiv.org/abs/cs/0202001v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "D.3.2"
        ]
    },
    {
        "title": "Secure History Preservation Through Timeline Entanglement",
        "authors": [
            "Petros Maniatis",
            "Mary Baker"
        ],
        "summary": "A secure timeline is a tamper-evident historic record of the states through which a system goes throughout its operational history. Secure timelines can help us reason about the temporal ordering of system states in a provable manner. We extend secure timelines to encompass multiple, mutually distrustful services, using timeline entanglement. Timeline entanglement associates disparate timelines maintained at independent systems, by linking undeniably the past of one timeline to the future of another. Timeline entanglement is a sound method to map a time step in the history of one service onto the timeline of another, and helps clients of entangled services to get persistent temporal proofs for services rendered that survive the demise or non-cooperation of the originating service. In this paper we present the design and implementation of Timeweave, our service development framework for timeline entanglement based on two novel disk-based authenticated data structures. We evaluate Timeweave's performance characteristics and show that it can be efficiently deployed in a loosely-coupled distributed system of a few hundred services with overhead of roughly 2-8% of the processing resources of a PC-grade system.",
        "published": "2002-02-06T21:52:09Z",
        "link": "http://arxiv.org/abs/cs/0202005v1",
        "categories": [
            "cs.DC",
            "cs.CR",
            "cs.DB",
            "cs.DS",
            "D.4.6; K.6.5; E.2; C.2.4"
        ]
    },
    {
        "title": "The SDSS SkyServer: Public Access to the Sloan Digital Sky Server Data",
        "authors": [
            "Alexander S. Szalay",
            "Jim Gray",
            "Ani R. Thakar",
            "Peter Z. Kunszt",
            "Tanu Malik",
            "Jordan Raddick",
            "Christopher Stoughton",
            "Jan vandenBerg"
        ],
        "summary": "The SkyServer provides Internet access to the public Sloan Digi-tal Sky Survey (SDSS) data for both astronomers and for science education. This paper describes the SkyServer goals and archi-tecture. It also describes our experience operating the SkyServer on the Internet. The SDSS data is public and well-documented so it makes a good test platform for research on database algorithms and performance.",
        "published": "2002-02-12T23:16:36Z",
        "link": "http://arxiv.org/abs/cs/0202013v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "H.3.7; H.3.5;H.2; H.3; H.4; H.5"
        ]
    },
    {
        "title": "Data Mining the SDSS SkyServer Database",
        "authors": [
            "Jim Gray",
            "Alex S. Szalay",
            "Ani R. Thakar",
            "Peter Z. Kunszt",
            "Christopher Stoughton",
            "Don Slutz",
            "Jan vandenBerg"
        ],
        "summary": "An earlier paper (Szalay et. al. \"Designing and Mining MultiTerabyte Astronomy Archives: The Sloan Digital Sky Survey,\" ACM SIGMOD 2000) described the Sloan Digital Sky Survey's (SDSS) data management needs by defining twenty database queries and twelve data visualization tasks that a good data management system should support. We built a database and interfaces to support both the query load and also a website for ad-hoc access. This paper reports on the database design, describes the data loading pipeline, and reports on the query implementation and performance. The queries typically translated to a single SQL statement. Most queries run in less than 20 seconds, allowing scientists to interactively explore the database. This paper is an in-depth tour of those queries. Readers should first have studied the companion overview paper Szalay et. al. \"The SDSS SkyServer, Public Access to the Sloan Digital Sky Server Data\" ACM SIGMOND 2002.",
        "published": "2002-02-12T23:47:20Z",
        "link": "http://arxiv.org/abs/cs/0202014v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "H.2.8;H.3.3; H.3.5;h.3.7;H.4.2"
        ]
    },
    {
        "title": "Sprinkling Selections over Join DAGs for Efficient Query Optimization",
        "authors": [
            "Satyanarayana R Valluri",
            "Soujanya Vadapalli",
            "Kamalakar Karlapalem"
        ],
        "summary": "In optimizing queries, solutions based on AND/OR DAG can generate all possible join orderings and select placements before searching for optimal query execution strategy. But as the number of joins and selection conditions increase, the space and time complexity to generate optimal query plan increases exponentially. In this paper, we use join graph for a relational database schema to either pre-compute all possible join orderings that can be executed and store it as a join DAG or, extract joins in the queries to incrementally build a history join DAG as and when the queries are executed. The select conditions in the queries are appropriately placed in the retrieved join DAG (or, history join DAG) to generate optimal query execution strategy. We experimentally evaluate our query optimization technique on TPC-D/H query sets to show their effectiveness over AND/OR DAG query optimization strategy. Finally, we illustrate how our technique can be used for efficient multiple query optimization and selection of materialized views in data warehousing environments.",
        "published": "2002-02-21T05:23:51Z",
        "link": "http://arxiv.org/abs/cs/0202035v1",
        "categories": [
            "cs.DB",
            "H.2.4"
        ]
    },
    {
        "title": "Towards practical meta-querying",
        "authors": [
            "Jan Van den Bussche",
            "Stijn Vansummeren",
            "Gottfried Vossen"
        ],
        "summary": "We describe a meta-querying system for databases containing queries in addition to ordinary data. In the context of such databases, a meta-query is a query about queries. Representing stored queries in XML, and using the standard XML manipulation language XSLT as a sublanguage, we show that just a few features need to be added to SQL to turn it into a fully-fledged meta-query language. The good news is that these features can be directly supported by extensible database technology.",
        "published": "2002-02-25T19:35:12Z",
        "link": "http://arxiv.org/abs/cs/0202037v2",
        "categories": [
            "cs.DB",
            "H.2.3"
        ]
    },
    {
        "title": "The Algorithms of Updating Sequential Patterns",
        "authors": [
            "Qingguo Zheng",
            "Ke Xu",
            "Shilong Ma",
            "Weifeng Lv"
        ],
        "summary": "Because the data being mined in the temporal database will evolve with time, many researchers have focused on the incremental mining of frequent sequences in temporal database. In this paper, we propose an algorithm called IUS, using the frequent and negative border sequences in the original database for incremental sequence mining. To deal with the case where some data need to be updated from the original database, we present an algorithm called DUS to maintain sequential patterns in the updated database. We also define the negative border sequence threshold: Min_nbd_supp to control the number of sequences in the negative border.",
        "published": "2002-03-27T03:35:12Z",
        "link": "http://arxiv.org/abs/cs/0203027v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "When to Update the sequential patterns of stream data?",
        "authors": [
            "Qingguo Zheng",
            "Ke Xu",
            "Shilong Ma"
        ],
        "summary": "In this paper, we first define a difference measure between the old and new sequential patterns of stream data, which is proved to be a distance. Then we propose an experimental method, called TPD (Tradeoff between Performance and Difference), to decide when to update the sequential patterns of stream data by making a tradeoff between the performance of increasingly updating algorithms and the difference of sequential patterns. The experiments for the incremental updating algorithm IUS on two data sets show that generally, as the size of incremental windows grows, the values of the speedup and the values of the difference will decrease and increase respectively. It is also shown experimentally that the incremental ratio determined by the TPD method does not monotonically increase or decrease but changes in a range between 20 and 30 percentage for the IUS algorithm.",
        "published": "2002-03-27T06:31:58Z",
        "link": "http://arxiv.org/abs/cs/0203028v3",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "On the Computational Complexity of Consistent Query Answers",
        "authors": [
            "Jan Chomicki",
            "Jerzy Marcinkowski"
        ],
        "summary": "We consider here the problem of obtaining reliable, consistent information from inconsistent databases -- databases that do not have to satisfy given integrity constraints. We use the notion of consistent query answer -- a query answer which is true in every (minimal) repair of the database. We provide a complete classification of the computational complexity of consistent answers to first-order queries w.r.t. functional dependencies and denial constraints. We show how the complexity depends on the {\\em type} of the constraints considered, their {\\em number}, and the {\\em size} of the query. We obtain several new PTIME cases, using new algorithms.",
        "published": "2002-04-05T22:24:33Z",
        "link": "http://arxiv.org/abs/cs/0204010v1",
        "categories": [
            "cs.DB",
            "H.2.3; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Querying Databases of Annotated Speech",
        "authors": [
            "Steve Cassidy",
            "Steven Bird"
        ],
        "summary": "Annotated speech corpora are databases consisting of signal data along with time-aligned symbolic `transcriptions'. Such databases are typically multidimensional, heterogeneous and dynamic. These properties present a number of tough challenges for representation and query. The temporal nature of the data adds an additional layer of complexity. This paper presents and harmonises two independent efforts to model annotated speech databases, one at Macquarie University and one at the University of Pennsylvania. Various query languages are described, along with illustrative applications to a variety of analytical problems. The research reported here forms a part of several ongoing projects to develop platform-independent open-source tools for creating, browsing, searching, querying and transforming linguistic databases, and to disseminate large linguistic databases over the internet.",
        "published": "2002-04-11T16:43:57Z",
        "link": "http://arxiv.org/abs/cs/0204026v1",
        "categories": [
            "cs.CL",
            "cs.DB",
            "H.2.3; H.2.4; H.5.5; I.2.7; J.5"
        ]
    },
    {
        "title": "Technology For Information Engineering (TIE): A New Way of Storing,   Retrieving and Analyzing Information",
        "authors": [
            "Jerzy Lewak"
        ],
        "summary": "The theoretical foundations of a new model and paradigm (called TIE) for data storage and access are introduced. Associations between data elements are stored in a single Matrix table, which is usually kept entirely in RAM for quick access. The model ties together a very intuitive \"guided\" GUI to the Matrix structure, allowing extremely easy complex searches through the data. Although it is an \"Associative Model\" in that it stores the data associations separately from the data itself, in contrast to other implementations of that model TIE guides the user to only the available information ensuring that every search is always fruitful. Very many diverse applications of the technology are reviewed.",
        "published": "2002-04-16T16:03:18Z",
        "link": "http://arxiv.org/abs/cs/0204038v1",
        "categories": [
            "cs.DB",
            "cs.IR",
            "E.0"
        ]
    },
    {
        "title": "Optimal Aggregation Algorithms for Middleware",
        "authors": [
            "Ron Fagin",
            "Amnon Lotem",
            "Moni Naor"
        ],
        "summary": "Let D be a database of N objects where each object has m fields. The objects are given in m sorted lists (where the ith list is sorted according to the ith field). Our goal is to find the top k objects according to a monotone aggregation function t, while minimizing access to the lists. The problem arises in several contexts. In particular Fagin (JCSS 1999) considered it for the purpose of aggregating information in a multimedia database system.   We are interested in instance optimality, i.e. that our algorithm will be as good as any other (correct) algorithm on any instance. We provide and analyze several instance optimal algorithms for the task, with various access costs and models.",
        "published": "2002-04-22T18:32:44Z",
        "link": "http://arxiv.org/abs/cs/0204046v1",
        "categories": [
            "cs.DB",
            "cs.DS",
            "H.2.4; F.2.2"
        ]
    },
    {
        "title": "Optimizing Queries Using a Meta-level Database",
        "authors": [
            "Christoph Koch"
        ],
        "summary": "Graph simulation (using graph schemata or data guides) has been successfully proposed as a technique for adding structure to semistructured data. Design patterns for description (such as meta-classes and homomorphisms between schema layers), which are prominent in the object-oriented programming community, constitute a generalization of this graph simulation approach.   In this paper, we show description applicable to a wide range of data models that have some notion of object (-identity), and propose to turn it into a data model primitive much like, say, inheritance. We argue that such an extension fills a practical need in contemporary data management. Then, we present algebraic techniques for query optimization (using the notions of described and description queries). Finally, in the semistructured setting, we discuss the pruning of regular path queries (with nested conditions) using description meta-data. In this context, our notion of meta-data extends graph schemata and data guides by meta-level values, allowing to boost query performance and to reduce the redundancy of data.",
        "published": "2002-05-23T14:15:55Z",
        "link": "http://arxiv.org/abs/cs/0205060v1",
        "categories": [
            "cs.DB",
            "H.2.1, H.2.3, D.1.5, D.3.3"
        ]
    },
    {
        "title": "Mining All Non-Derivable Frequent Itemsets",
        "authors": [
            "Toon Calders",
            "Bart Goethals"
        ],
        "summary": "Recent studies on frequent itemset mining algorithms resulted in significant performance improvements. However, if the minimal support threshold is set too low, or the data is highly correlated, the number of frequent itemsets itself can be prohibitively large. To overcome this problem, recently several proposals have been made to construct a concise representation of the frequent itemsets, instead of mining all frequent itemsets. The main goal of this paper is to identify redundancies in the set of all frequent itemsets and to exploit these redundancies in order to reduce the result of a mining operation. We present deduction rules to derive tight bounds on the support of candidate itemsets. We show how the deduction rules allow for constructing a minimal representation for all frequent itemsets. We also present connections between our proposal and recent proposals for concise representations and we give the results of experiments on real-life datasets that show the effectiveness of the deduction rules. In fact, the experiments even show that in many cases, first mining the concise representation, and then creating the frequent itemsets from this representation outperforms existing frequent set mining algorithms.",
        "published": "2002-06-03T14:13:51Z",
        "link": "http://arxiv.org/abs/cs/0206004v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "Relational Association Rules: getting WARMeR",
        "authors": [
            "Bart Goethals",
            "Jan Van den Bussche"
        ],
        "summary": "In recent years, the problem of association rule mining in transactional data has been well studied. We propose to extend the discovery of classical association rules to the discovery of association rules of conjunctive queries in arbitrary relational data, inspired by the WARMR algorithm, developed by Dehaspe and Toivonen, that discovers association rules over a limited set of conjunctive queries. Conjunctive query evaluation in relational databases is well understood, but still poses some great challenges when approached from a discovery viewpoint in which patterns are generated and evaluated with respect to some well defined search space and pruning operators.",
        "published": "2002-06-15T12:08:12Z",
        "link": "http://arxiv.org/abs/cs/0206023v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "Improving Web Database Access Using Decision Diagrams",
        "authors": [
            "Denis V. Popel",
            "Nawar Al-Hakeem"
        ],
        "summary": "In some areas of management and commerce, especially in Electronic commerce (E-commerce), that are accelerated by advances in Web technologies, it is essential to support the decision making process using formal methods. Among the problems of E-commerce applications: reducing the time of data access so that huge databases can be searched quickly; decreasing the cost of database design ... etc. We present the application of Decision Diagrams design using Information Theory approach to improve database access speeds. We show that such utilization provides systematic and visual ways of applying Decision Making methods to simplify complex Web engineering problems.",
        "published": "2002-07-04T04:09:50Z",
        "link": "http://arxiv.org/abs/cs/0207011v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "E.2"
        ]
    },
    {
        "title": "Introducing Dynamic Behavior in Amalgamated Knowledge Bases",
        "authors": [
            "Elisa Bertino",
            "Barbara Catania",
            "Paolo Perlasca"
        ],
        "summary": "The problem of integrating knowledge from multiple and heterogeneous sources is a fundamental issue in current information systems. In order to cope with this problem, the concept of mediator has been introduced as a software component providing intermediate services, linking data resources and application programs, and making transparent the heterogeneity of the underlying systems. In designing a mediator architecture, we believe that an important aspect is the definition of a formal framework by which one is able to model integration according to a declarative style. To this purpose, the use of a logical approach seems very promising. Another important aspect is the ability to model both static integration aspects, concerning query execution, and dynamic ones, concerning data updates and their propagation among the various data sources. Unfortunately, as far as we know, no formal proposals for logically modeling mediator architectures both from a static and dynamic point of view have already been developed. In this paper, we extend the framework for amalgamated knowledge bases, presented by Subrahmanian, to deal with dynamic aspects. The language we propose is based on the Active U-Datalog language, and extends it with annotated logic and amalgamation concepts. We model the sources of information and the mediator (also called supervisor) as Active U-Datalog deductive databases, thus modeling queries, transactions, and active rules, interpreted according to the PARK semantics. By using active rules, the system can efficiently perform update propagation among different databases. The result is a logical environment, integrating active and deductive rules, to perform queries and update propagation in an heterogeneous mediated framework.",
        "published": "2002-07-22T07:50:01Z",
        "link": "http://arxiv.org/abs/cs/0207076v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "cs.LO",
            "D.1.6 Logic Programming; H.2.5 Heterogeneous Databases; F.4.1\n  Mathematical Logic, Logic and constraint programming"
        ]
    },
    {
        "title": "Repairing Inconsistent Databases: A Model-Theoretic Approach and   Abductive Reasoning",
        "authors": [
            "Ofer Arieli",
            "Marc Denecker",
            "Bert Van Nuffelen",
            "Maurice Bruynooghe"
        ],
        "summary": "In this paper we consider two points of views to the problem of coherent integration of distributed data. First we give a pure model-theoretic analysis of the possible ways to `repair' a database. We do so by characterizing the possibilities to `recover' consistent data from an inconsistent database in terms of those models of the database that exhibit as minimal inconsistent information as reasonably possible. Then we introduce an abductive application to restore the consistency of a given database. This application is based on an abductive solver (A-system) that implements an SLDNFA-resolution procedure, and computes a list of data-facts that should be inserted to the database or retracted from it in order to keep the database consistent. The two approaches for coherent data integration are related by soundness and completeness results.",
        "published": "2002-07-25T15:13:18Z",
        "link": "http://arxiv.org/abs/cs/0207085v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "F.4.1; H.2.7; I.2.3"
        ]
    },
    {
        "title": "Answer Sets for Consistent Query Answering in Inconsistent Databases",
        "authors": [
            "Marcelo Arenas",
            "Leopoldo Bertossi",
            "Jan Chomicki"
        ],
        "summary": "A relational database is inconsistent if it does not satisfy a given set of integrity constraints. Nevertheless, it is likely that most of the data in it is consistent with the constraints. In this paper we apply logic programming based on answer sets to the problem of retrieving consistent information from a possibly inconsistent database. Since consistent information persists from the original database to every of its minimal repairs, the approach is based on a specification of database repairs using disjunctive logic programs with exceptions, whose answer set semantics can be represented and computed by systems that implement stable model semantics. These programs allow us to declare persistence by defaults and repairing changes by exceptions. We concentrate mainly on logic programs for binary integrity constraints, among which we find most of the integrity constraints found in practice.",
        "published": "2002-07-26T21:18:50Z",
        "link": "http://arxiv.org/abs/cs/0207094v1",
        "categories": [
            "cs.DB",
            "H.2.3; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Preference Queries",
        "authors": [
            "Jan Chomicki"
        ],
        "summary": "The handling of user preferences is becoming an increasingly important issue in present-day information systems. Among others, preferences are used for information filtering and extraction to reduce the volume of data presented to the user. They are also used to keep track of user profiles and formulate policies to improve and automate decision making.   We propose here a simple, logical framework for formulating preferences as preference formulas. The framework does not impose any restrictions on the preference relations and allows arbitrary operation and predicate signatures in preference formulas. It also makes the composition of preference relations straightforward. We propose a simple, natural embedding of preference formulas into relational algebra (and SQL) through a single winnow operator parameterized by a preference formula. The embedding makes possible the formulation of complex preference queries, e.g., involving aggregation, by piggybacking on existing SQL constructs. It also leads in a natural way to the definition of further, preference-related concepts like ranking. Finally, we present general algebraic laws governing the winnow operator and its interaction with other relational algebra operators. The preconditions on the applicability of the laws are captured by logical formulas. The laws provide a formal foundation for the algebraic optimization of preference queries. We demonstrate the usefulness of our approach through numerous examples.",
        "published": "2002-07-27T00:46:50Z",
        "link": "http://arxiv.org/abs/cs/0207093v1",
        "categories": [
            "cs.DB",
            "H.2.3; F.4.1; I.2.3"
        ]
    },
    {
        "title": "TerraService.NET: An Introduction to Web Services",
        "authors": [
            "Tom Barclay",
            "Jim Gray",
            "Eric Strand",
            "Steve Ekblad",
            "Jeffrey Richter"
        ],
        "summary": "This article explores the design and construction of a geo-spatial Internet web service application from the host web site perspective and from the perspective of an application using the web service. The TerraService.NET web service was added to the popular TerraServer database and web site with no major structural changes to the database. The article discusses web service design, implementation, and deployment concepts and design guidelines. Web services enable applications that aggregate and interact with information and resources from Internet-scale distributed servers. The article presents the design of two USDA applications that interoperate with database and web service resources in Fort Collins Colorado and the TerraService web service located in Tukwila Washington.",
        "published": "2002-08-07T22:18:35Z",
        "link": "http://arxiv.org/abs/cs/0208010v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "H.2; H.3; H.4;H.5"
        ]
    },
    {
        "title": "Petabyte Scale Data Mining: Dream or Reality?",
        "authors": [
            "Alexander S. Szalay",
            "Jim Gray",
            "Jan vandenBerg"
        ],
        "summary": "Science is becoming very data intensive1. Today's astronomy datasets with tens of millions of galaxies already present substantial challenges for data mining. In less than 10 years the catalogs are expected to grow to billions of objects, and image archives will reach Petabytes. Imagine having a 100GB database in 1996, when disk scanning speeds were 30MB/s, and database tools were immature. Such a task today is trivial, almost manageable with a laptop. We think that the issue of a PB database will be very similar in six years. In this paper we scale our current experiments in data archiving and analysis on the Sloan Digital Sky Survey2,3 data six years into the future. We analyze these projections and look at the requirements of performing data mining on such data sets. We conclude that the task scales rather well: we could do the job today, although it would be expensive. There do not seem to be any show-stoppers that would prevent us from storing and using a Petabyte dataset six years from today.",
        "published": "2002-08-07T22:49:56Z",
        "link": "http://arxiv.org/abs/cs/0208013v1",
        "categories": [
            "cs.DB",
            "cs.CE",
            "H.2.8;J.2"
        ]
    },
    {
        "title": "Spatial Clustering of Galaxies in Large Datasets",
        "authors": [
            "Alexander S. Szalay",
            "Tamas Budavari",
            "Andrew Connolly",
            "Jim Gray",
            "Takahiko Matsubara",
            "Adrian Pope",
            "Istvan Szapudi"
        ],
        "summary": "Datasets with tens of millions of galaxies present new challenges for the analysis of spatial clustering. We have built a framework that integrates a database of object catalogs, tools for creating masks of bad regions, and a fast (NlogN) correlation code. This system has enabled unprecedented efficiency in carrying out the analysis of galaxy clustering in the SDSS catalog. A similar approach is used to compute the three-dimensional spatial clustering of galaxies on very large scales. We describe our strategy to estimate the effect of photometric errors using a database. We discuss our efforts as an early example of data-intensive science. While it would have been possible to get these results without the framework we describe, it will be infeasible to perform these computations on the future huge datasets without using this framework.",
        "published": "2002-08-07T23:06:40Z",
        "link": "http://arxiv.org/abs/cs/0208015v1",
        "categories": [
            "cs.DB",
            "cs.DS",
            "G.3;H.2.8; J.2"
        ]
    },
    {
        "title": "Equivalences Among Aggregate Queries with Negation",
        "authors": [
            "Sara Cohen",
            "Werner Nutt",
            "Yehoshua Sagiv"
        ],
        "summary": "Query equivalence is investigated for disjunctive aggregate queries with negated subgoals, constants and comparisons. A full characterization of equivalence is given for the aggregation functions count, max, sum, prod, toptwo and parity. A related problem is that of determining, for a given natural number N, whether two given queries are equivalent over all databases with at most N constants. We call this problem bounded equivalence. A complete characterization of decidability of bounded equivalence is given. In particular, it is shown that this problem is decidable for all the above aggregation functions as well as for count distinct and average. For quasilinear queries (i.e., queries where predicates that occur positively are not repeated) it is shown that equivalence can be decided in polynomial time for the aggregation functions count, max, sum, parity, prod, toptwo and average. A similar result holds for count distinct provided that a few additional conditions hold. The results are couched in terms of abstract characteristics of aggregation functions, and new proof techniques are used. Finally, the results above also imply that equivalence, under bag-set semantics, is decidable for non-aggregate queries with negation.",
        "published": "2002-10-29T19:19:12Z",
        "link": "http://arxiv.org/abs/cs/0210028v2",
        "categories": [
            "cs.DB",
            "cs.LO",
            "F.4.1;H.2.3;H.2.4"
        ]
    },
    {
        "title": "Monadic Datalog and the Expressive Power of Languages for Web   Information Extraction",
        "authors": [
            "Georg Gottlob",
            "Christoph Koch"
        ],
        "summary": "Research on information extraction from Web pages (wrapping) has seen much activity recently (particularly systems implementations), but little work has been done on formally studying the expressiveness of the formalisms proposed or on the theoretical foundations of wrapping. In this paper, we first study monadic datalog over trees as a wrapping language. We show that this simple language is equivalent to monadic second order logic (MSO) in its ability to specify wrappers. We believe that MSO has the right expressiveness required for Web information extraction and propose MSO as a yardstick for evaluating and comparing wrappers. Along the way, several other results on the complexity of query evaluation and query containment for monadic datalog over trees are established, and a simple normal form for this language is presented. Using the above results, we subsequently study the kernel fragment Elog$^-$ of the Elog wrapping language used in the Lixto system (a visual wrapper generator). Curiously, Elog$^-$ exactly captures MSO, yet is easier to use. Indeed, programs in this language can be entirely visually specified.",
        "published": "2002-11-15T20:01:54Z",
        "link": "http://arxiv.org/abs/cs/0211020v2",
        "categories": [
            "cs.DB",
            "F.1.1; F.4.1; F.4.3; H.2.3; I.7.2"
        ]
    },
    {
        "title": "SkyQuery: A WebService Approach to Federate Databases",
        "authors": [
            "Tanu Malik",
            "Alex S. Szalay",
            "Tamas Budavari",
            "Ani R. Thakar"
        ],
        "summary": "Traditional science searched for new objects and phenomena that led to discoveries. Tomorrow's science will combine together the large pool of information in scientific archives and make discoveries. Scienthists are currently keen to federate together the existing scientific databases. The major challenge in building a federation of these autonomous and heterogeneous databases is system integration. Ineffective integration will result in defunct federations and under utilized scientific data.   Astronomy, in particular, has many autonomous archives spread over the Internet. It is now seeking to federate these, with minimal effort, into a Virtual Observatory that will solve complex distributed computing tasks such as answering federated spatial join queries.   In this paper, we present SkyQuery, a successful prototype of an evolving federation of astronomy archives. It interoperates using the emerging Web services standard. We describe the SkyQuery architecture and show how it efficiently evaluates a probabilistic federated spatial join query.",
        "published": "2002-11-20T04:54:19Z",
        "link": "http://arxiv.org/abs/cs/0211023v1",
        "categories": [
            "cs.DB",
            "cs.CE",
            "J.2 ;H.3.5;H.2.8 ;H.2.5"
        ]
    },
    {
        "title": "Database Repairs and Analytic Tableaux",
        "authors": [
            "Leopoldo Bertossi",
            "Camilla Schwind"
        ],
        "summary": "In this article, we characterize in terms of analytic tableaux the repairs of inconsistent relational databases, that is databases that do not satisfy a given set of integrity constraints. For this purpose we provide closing and opening criteria for branches in tableaux that are built for database instances and their integrity constraints. We use the tableaux based characterization as a basis for consistent query answering, that is for retrieving from the database answers to queries that are consistent wrt the integrity constraints.",
        "published": "2002-11-29T04:28:56Z",
        "link": "http://arxiv.org/abs/cs/0211042v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H2; F4; I2"
        ]
    },
    {
        "title": "Minimal-Change Integrity Maintenance Using Tuple Deletions",
        "authors": [
            "Jan Chomicki",
            "Jerzy Marcinkowski"
        ],
        "summary": "We address the problem of minimal-change integrity maintenance in the context of integrity constraints in relational databases. We assume that integrity-restoration actions are limited to tuple deletions. We identify two basic computational issues: repair checking (is a database instance a repair of a given database?) and consistent query answers (is a tuple an answer to a given query in every repair of a given database?). We study the computational complexity of both problems, delineating the boundary between the tractable and the intractable. We consider denial constraints, general functional and inclusion dependencies, as well as key and foreign key constraints. Our results shed light on the computational feasibility of minimal-change integrity maintenance. The tractable cases should lead to practical implementations. The intractability results highlight the inherent limitations of any integrity enforcement mechanism, e.g., triggers or referential constraint actions, as a way of performing minimal-change integrity maintenance.",
        "published": "2002-12-05T16:23:35Z",
        "link": "http://arxiv.org/abs/cs/0212004v1",
        "categories": [
            "cs.DB",
            "H.2.3; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Use of openMosix for parallel I/O balancing on storage in Linux cluster",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this paper I present some experiences made in the matter of I/O for Linux Clustering. In particular is illustrated the use of the package openMosix, a balancer of workload for processes running in a cluster of nodes. I describe some tests for balancing the load of I/O storage massive processes in a cluster with four components. This work is been written for the proceedings of the workshop Linux cluster: the openMosix approach held at CINECA, Bologna, Italy, on 28 november 2002.",
        "published": "2002-12-06T12:57:50Z",
        "link": "http://arxiv.org/abs/cs/0212006v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.1.2; C.2.4; H.3.3"
        ]
    },
    {
        "title": "Classes of Spatiotemporal Objects and Their Closure Properties",
        "authors": [
            "Jan Chomicki",
            "Sofie Haesevoets",
            "Bart Kuijpers",
            "Peter Revesz"
        ],
        "summary": "We present a data model for spatio-temporal databases. In this model spatio-temporal data is represented as a finite union of objects described by means of a spatial reference object, a temporal object and a geometric transformation function that determines the change or movement of the reference object in time.   We define a number of practically relevant classes of spatio-temporal objects, and give complete results concerning closure under Boolean set operators for these classes. Since only few classes are closed under all set operators, we suggest an extension of the model, which leads to better closure properties, and therefore increased practical applicability. We also discuss a normal form for this extended data model.",
        "published": "2002-12-09T17:29:12Z",
        "link": "http://arxiv.org/abs/cs/0212017v1",
        "categories": [
            "cs.DB",
            "H.2.8"
        ]
    },
    {
        "title": "An Ehrenfeucht-Fraisse Game Approach to Collapse Results in Database   Theory",
        "authors": [
            "Nicole Schweikardt"
        ],
        "summary": "We present a new Ehrenfeucht-Fraisse game approach to collapse results in database theory and we show that, in principle, this approach suffices to prove every natural generic collapse result. Following this approach we can deal with certain infinite databases where previous, highly involved methods fail. We prove the natural generic collapse for Z-embeddable databases over any linearly ordered context structure with arbitrary monadic predicates, and for N-embeddable databases over the context structure (R,<,+,Mon_Q,Groups). Here, N, Z, R, denote the sets of natural numbers, integers, and real numbers, respectively. Groups is the collection of all subgroups of (R,+) that contain Z, and Mon_Q is the collection of all subsets of a particular infinite subset Q of N. Restricting the complexity of the formulas that may be used to formulate queries to Boolean combinations of purely existential first-order formulas, we even obtain the collapse for N-embeddable databases over any linearly ordered context structure with arbitrary predicates. Finally, we develop the notion of N-representable databases, which is a natural generalization of the classical notion of finitely representable databases. We show that natural generic collapse results for N-embeddable databases can be lifted to the larger class of N-representable databases. To obtain, in particular, the collapse result for (N,<,+,Mon_Q), we explicitly construct a winning strategy for the duplicator in the presence of the built-in addition relation +. This, as a side product, also leads to an Ehrenfeucht-Fraisse game proof of the theorem of Ginsburg and Spanier, stating that the spectra of FO(<,+)-sentences are semi-linear.",
        "published": "2002-12-20T14:34:37Z",
        "link": "http://arxiv.org/abs/cs/0212049v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "F.4.1; H.2.3"
        ]
    },
    {
        "title": "ExploitingWeb Service Semantics: Taxonomies vs. Ontologies",
        "authors": [
            "Asuman Dogac",
            "Gokce Laleci",
            "Yildiray Kabak",
            "Ibrahim Cingil"
        ],
        "summary": "Comprehensive semantic descriptions of Web services are essential to exploit them in their full potential, that is, discovering them dynamically, and enabling automated service negotiation, composition and monitoring. The semantic mechanisms currently available in service registries which are based on taxonomies fail to provide the means to achieve this. Although the terms taxonomy and ontology are sometimes used interchangably there is a critical difference. A taxonomy indicates only class/subclass relationship whereas an ontology describes a domain completely. The essential mechanisms that ontology languages provide include their formal specification (which allows them to be queried) and their ability to define properties of classes. Through properties very accurate descriptions of services can be defined and services can be related to other services or resources. In this paper, we discuss the advantages of describing service semantics through ontology languages and describe how to relate the semantics defined with the services advertised in service registries like UDDI and ebXML.",
        "published": "2002-12-23T10:26:36Z",
        "link": "http://arxiv.org/abs/cs/0212051v1",
        "categories": [
            "cs.DB",
            "D"
        ]
    },
    {
        "title": "Improving the Functionality of UDDI Registries through Web Service   Semantics",
        "authors": [
            "Asuman Dogac",
            "Ibrahim Cingil",
            "Gokce Laleci",
            "Yildiray Kabak"
        ],
        "summary": "In this paper we describe a framework for exploiting the semantics of Web services through UDDI registries. As a part of this framework, we extend the DAML-S upper ontology to describe the functionality we find essential for e-businesses. This functionality includes relating the services with electronic catalogs, describing the complementary services and finding services according to the properties of products or services. Once the semantics is defined, there is a need for a mechanism in the service registry to relate it with the service advertised. The ontology model developed is general enough to be used with any service registry. However when it comes to relating the semantics with services advertised, the capabilities provided by the registry effects how this is achieved. We demonstrate how to integrate the described service semantics to UDDI registries.",
        "published": "2002-12-25T15:15:16Z",
        "link": "http://arxiv.org/abs/cs/0212052v1",
        "categories": [
            "cs.DB",
            "D"
        ]
    },
    {
        "title": "On linear programming bounds for spherical codes and designs",
        "authors": [
            "Alex Samorodnitsky"
        ],
        "summary": "We investigate universal bounds on spherical codes and spherical designs that could be obtained using Delsarte's linear programming methods. We give a lower estimate for the LP upper bound on codes, and an upper estimate for the LP lower bound on designs. Specifically, when the distance of the code is fixed and the dimension goes to infinity, the LP upper bound on codes is at least as large as the average of the best known upper and lower bounds. When the dimension n of the design is fixed, and the strength k goes to infinity, the LP bound on designs turns out, in conjunction with known lower bounds, to be proportional to k^{n-1}.",
        "published": "2002-03-06T23:05:58Z",
        "link": "http://arxiv.org/abs/math/0203059v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "math.OC"
        ]
    },
    {
        "title": "The capacity of hybrid quantum memory",
        "authors": [
            "Greg Kuperberg"
        ],
        "summary": "The general stable quantum memory unit is a hybrid consisting of a classical digit with a quantum digit (qudit) assigned to each classical state. The shape of the memory is the vector of sizes of these qudits, which may differ. We determine when N copies of a quantum memory A embed in N(1+o(1)) copies of another quantum memory B. This relationship captures the notion that B is as at least as useful as A for all purposes in the bulk limit. We show that the embeddings exist if and only if for all p >= 1, the p-norm of the shape of A does not exceed the p-norm of the shape of B. The log of the p-norm of the shape of A can be interpreted as the maximum of S(\\rho) + H(\\rho)/p (quantum entropy plus discounted classical entropy) taken over all mixed states \\rho on A. We also establish a noiseless coding theorem that justifies these entropies. The noiseless coding theorem and the bulk embedding theorem together say that either A blindly bulk-encodes into B with perfect fidelity, or A admits a state that does not visibly bulk-encode into B with high fidelity.   In conclusion, the utility of a hybrid quantum memory is determined by its simultaneous capacity for classical and quantum entropy, which is not a finite list of numbers, but rather a convex region in the classical-quantum entropy plane.",
        "published": "2002-03-21T01:08:02Z",
        "link": "http://arxiv.org/abs/quant-ph/0203105v3",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math-ph",
            "math.IT",
            "math.MP",
            "math.OA"
        ]
    },
    {
        "title": "Entropy estimation of symbol sequences",
        "authors": [
            "Thomas Schürmann",
            "Peter Grassberger"
        ],
        "summary": "We discuss algorithms for estimating the Shannon entropy h of finite symbol sequences with long range correlations. In particular, we consider algorithms which estimate h from the code lengths produced by some compression algorithm. Our interest is in describing their convergence with sequence length, assuming no limits for the space and time complexities of the compression algorithms. A scaling law is proposed for extrapolation from finite sample lengths. This is applied to sequences of dynamical systems in non-trivial chaotic regimes, a 1-D cellular automaton, and to written English texts.",
        "published": "2002-03-21T11:24:56Z",
        "link": "http://arxiv.org/abs/cond-mat/0203436v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CL",
            "cs.IT",
            "math.IT",
            "physics.data-an",
            "stat.ML"
        ]
    },
    {
        "title": "A New Operation on Sequences: the Boustrouphedon Transform",
        "authors": [
            "Jessica Millar",
            "N. J. A. Sloane",
            "Neal E. Young"
        ],
        "summary": "A generalization of the Seidel-Entringer-Arnold method for calculating the alternating permutation numbers (or secant-tangent numbers) leads to a new operation on integer sequences, the Boustrophedon transform.",
        "published": "2002-05-20T16:47:41Z",
        "link": "http://arxiv.org/abs/math/0205218v3",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "05A15"
        ]
    },
    {
        "title": "The Lattice of N-Run Orthogonal Arrays",
        "authors": [
            "E. M. Rains",
            "N. J. A. Sloane",
            "John Stufken"
        ],
        "summary": "If the number of runs in a (mixed-level) orthogonal array of strength 2 is specified, what numbers of levels and factors are possible? The collection of possible sets of parameters for orthogonal arrays with N runs has a natural lattice structure, induced by the ``expansive replacement'' construction method. In particular the dual atoms in this lattice are the most important parameter sets, since any other parameter set for an N-run orthogonal array can be constructed from them. To get a sense for the number of dual atoms, and to begin to understand the lattice as a function of N, we investigate the height and the size of the lattice. It is shown that the height is at most [c(N-1)], where c= 1.4039... and that there is an infinite sequence of values of N for which this bound is attained. On the other hand, the number of nodes in the lattice is bounded above by a superpolynomial function of N (and superpolynomial growth does occur for certain sequences of values of N). Using a new construction based on ``mixed spreads'', all parameter sets with 64 runs are determined. Four of these 64-run orthogonal arrays appear to be new.",
        "published": "2002-05-28T17:35:10Z",
        "link": "http://arxiv.org/abs/math/0205299v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "05B15"
        ]
    },
    {
        "title": "Some Canonical Sequences of Integers",
        "authors": [
            "Mira Bernstein",
            "N. J. A. Sloane"
        ],
        "summary": "Extending earlier work of R. Donaghey and P. J. Cameron, we investigate some canonical \"eigen-sequences\" associated with transformations of integer sequences. Several known sequences appear in a new setting: for instance the sequences (such as 1, 3, 11, 49, 257, 1531, ...) studied by T. Tsuzuku, H. O. Foulkes and A. Kerber in connection with multiply transitive groups are eigen-sequences for the binomial transform. Many interesting new sequences also arise, such as 1, 1, 2, 26, 152, 1144, ..., which shifts one place left when transformed by the Stirling numbers of the second kind, and whose exponential generating function satisfies A'(x) = A(e^x -1) + 1.",
        "published": "2002-05-28T18:21:02Z",
        "link": "http://arxiv.org/abs/math/0205301v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "05Axx; 11Bxx"
        ]
    },
    {
        "title": "On Asymmetric Coverings and Covering Numbers",
        "authors": [
            "David Applegate",
            "E. M. Rains",
            "N. J. A. Sloane"
        ],
        "summary": "An asymmetric covering D(n,R) is a collection of special subsets S of an n-set such that every subset T of the n-set is contained in at least one special S with |S| - |T| <= R. In this paper we compute the smallest size of any D(n,1) for n <= 8. We also investigate ``continuous'' and ``banded'' versions of the problem. The latter involves the classical covering numbers C(n,k,k-1), and we determine the following new values: C(10,5,4) = 51, C(11,7,6,) =84, C(12,8,7) = 126, C(13,9,8)= 185 and C(14,10,9) = 259. We also find the number of nonisomorphic minimal covering designs in several cases.",
        "published": "2002-05-28T21:50:03Z",
        "link": "http://arxiv.org/abs/math/0205303v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "05Bxx, 94B60"
        ]
    },
    {
        "title": "Data compression limit for an information source of interacting qubits",
        "authors": [
            "Nilanjana Datta",
            "Yuri Suhov"
        ],
        "summary": "A system of interacting qubits can be viewed as a non-i.i.d quantum information source. A possible model of such a source is provided by a quantum spin system, in which spin-1/2 particles located at sites of a lattice interact with each other. We establish the limit for the compression of information from such a source and show that asymptotically it is given by the von Neumann entropy rate. Our result can be viewed as a quantum analog of Shannon's noiseless coding theorem for a class of non - i.i.d. quantum information sources.",
        "published": "2002-07-11T14:50:47Z",
        "link": "http://arxiv.org/abs/quant-ph/0207069v2",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math-ph",
            "math.IT",
            "math.MP"
        ]
    },
    {
        "title": "The Shannon-McMillan Theorem for Ergodic Quantum Lattice Systems",
        "authors": [
            "Igor Bjelakovic",
            "Tyll Krueger",
            "Rainer Siegmund-Schultze",
            "Arleta Szkola"
        ],
        "summary": "We formulate and prove a quantum Shannon-McMillan theorem. The theorem demonstrates the significance of the von Neumann entropy for translation invariant ergodic quantum spin systems on n-dimensional lattices: the entropy gives the logarithm of the essential number of eigenvectors of the system on large boxes. The one-dimensional case covers quantum information sources and is basic for coding theorems.",
        "published": "2002-07-15T14:50:22Z",
        "link": "http://arxiv.org/abs/math/0207121v3",
        "categories": [
            "math.DS",
            "cs.DS",
            "cs.IT",
            "math-ph",
            "math.IT",
            "math.MP",
            "math.OA",
            "quant-ph",
            "46L89; 68P30; 81Qxx"
        ]
    },
    {
        "title": "A Zador-Like Formula for Quantizers Based on Periodic Tilings",
        "authors": [
            "N. J. A. Sloane",
            "Vinay A. Vaishampayan"
        ],
        "summary": "We consider Zador's asymptotic formula for the distortion-rate function for a variable-rate vector quantizer in the high-rate case. This formula involves the differential entropy of the source, the rate of the quantizer in bits per sample, and a coefficient G which depends on the geometry of the quantizer but is independent of the source. We give an explicit formula for G in the case when the quantizing regions form a periodic tiling of n-dimensional space, in terms of the volumes and second moments of the Voronoi cells. As an application we show, extending earlier work of Kashyap and Neuhoff, that even a variable-rate three-dimensional quantizer based on the ``A15'' structure is still inferior to a quantizer based on the body-centered cubic lattice. We also determine the smallest covering radius of such a structure.",
        "published": "2002-07-17T17:46:39Z",
        "link": "http://arxiv.org/abs/math/0207146v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "11H31, 11H06, 52A99, 94A34"
        ]
    },
    {
        "title": "Quantizing Using Lattice Intersections",
        "authors": [
            "N. J. A. Sloane",
            "B. Beferull-Lozano"
        ],
        "summary": "The usual quantizer based on an n-dimensional lattice L maps a point x in R^n to a closest lattice point. Suppose L is the intersection of lattices L_1, ..., L_r. Then one may instead combine the information obtained by simultaneously quantizing x with respect to each of the L_i. This corresponds to decomposing R^n into a honeycomb of cells which are the intersections of the Voronoi cells for the L_i, and identifying the cell to which x belongs. This paper shows how to write several standard lattices (the face-centered and body-centered cubic lattices, the root lattices D_4, E_6*, E_8, the Coxeter-Todd, Barnes-Wall and Leech lattices, etc.) in a canonical way as intersections of a small number of simpler, decomposable, lattices. The cells of the honeycombs are given explicitly and the mean squared quantizing error calculated in the cases when the intersection lattice is the face-centered or body-centered cubic lattice or the lattice D_4.",
        "published": "2002-07-17T18:52:22Z",
        "link": "http://arxiv.org/abs/math/0207147v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "11H31, 11H06, 52A99, 94A34"
        ]
    },
    {
        "title": "A Simple Construction for the Barnes-Wall Lattices",
        "authors": [
            "G. Nebe",
            "E. M. Rains",
            "N. J. A. Sloane"
        ],
        "summary": "A certain family of orthogonal groups (called \"Clifford groups\" by G. E. Wall) has arisen in a variety of different contexts in recent years. These groups have a simple definition as the automorphism groups of certain generalized Barnes-Wall lattices. This leads to an especially simple construction for the usual Barnes-Wall lattices. This is based on the third author's talk at the Forney-Fest, M.I.T., March 2000, which in turn is based on our paper \"The Invariants of the Clifford Groups\", Designs, Codes, Crypt., 24 (2001), 99--121, to which the reader is referred for further details and proofs.",
        "published": "2002-07-22T01:47:23Z",
        "link": "http://arxiv.org/abs/math/0207186v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "11H06 (52C99)"
        ]
    },
    {
        "title": "On Single-Deletion-Correcting Codes",
        "authors": [
            "N. J. A. Sloane"
        ],
        "summary": "This paper gives a brief survey of binary single-deletion-correcting codes. The Varshamov-Tenengolts codes appear to be optimal, but many interesting unsolved problems remain. The connections with shift-register sequences also remain somewhat mysterious.",
        "published": "2002-07-22T20:30:20Z",
        "link": "http://arxiv.org/abs/math/0207197v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "94B60, 94A55"
        ]
    },
    {
        "title": "The Z_4-Linearity of Kerdock, Preparata, Goethals and Related Codes",
        "authors": [
            "A. Roger Hammons, Jr.",
            "P. Vijay Kumar",
            "A. R. Calderbank",
            "N. J. A. Sloane",
            "Patrick Solé"
        ],
        "summary": "Certain notorious nonlinear binary codes contain more codewords than any known linear code. These include the codes constructed by Nordstrom-Robinson, Kerdock, Preparata, Goethals, and Delsarte-Goethals. It is shown here that all these codes can be very simply constructed as binary images under the Gray map of linear codes over Z_4, the integers mod 4 (although this requires a slight modification of the Preparata and Goethals codes). The construction implies that all these binary codes are distance invariant. Duality in the Z_4 domain implies that the binary images have dual weight distributions. The Kerdock and \"Preparata\" codes are duals over Z_4 -- and the Nordstrom-Robinson code is self-dual -- which explains why their weight distributions are dual to each other. The Kerdock and \"Preparata\" codes are Z_4-analogues of first-order Reed-Muller and extended Hamming codes, respectively. All these codes are extended cyclic codes over Z_4, which greatly simplifies encoding and decoding. An algebraic hard-decision decoding algorithm is given for the \"Preparata\" code and a Hadamard-transform soft-decision decoding algorithm for the Kerdock code. Binary first- and second-order Reed-Muller codes are also linear over Z_4, but extended Hamming codes of length n >= 32 and the Golay code are not. Using Z_4-linearity, a new family of distance regular graphs are constructed on the cosets of the \"Preparata\" code.",
        "published": "2002-07-23T11:45:50Z",
        "link": "http://arxiv.org/abs/math/0207208v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "94B05 (94B60)"
        ]
    },
    {
        "title": "Interleaver Design for Turbo Codes",
        "authors": [
            "H. R. Sadjadpour",
            "N. J. A. Sloane",
            "M. Salehi",
            "G. Nebe"
        ],
        "summary": "The performance of a Turbo code with short block length depends critically on the interleaver design. There are two major criteria in the design of an interleaver: the distance spectrum of the code and the correlation between the information input data and the soft output of each decoder corresponding to its parity bits. This paper describes a new interleaver design for Turbo codes with short block length based on these two criteria. A deterministic interleaver suitable for Turbo codes is also described. Simulation results compare the new interleaver design to different existing interleavers.",
        "published": "2002-07-23T14:44:49Z",
        "link": "http://arxiv.org/abs/math/0207209v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "94B65 (94B60)"
        ]
    },
    {
        "title": "The Sphere-Packing Problem",
        "authors": [
            "N. J. A. Sloane"
        ],
        "summary": "A brief report on recent work on the sphere-packing problem.",
        "published": "2002-07-26T20:08:35Z",
        "link": "http://arxiv.org/abs/math/0207256v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "52C17 (11H31)"
        ]
    },
    {
        "title": "On Kissing Numbers in Dimensions 32 to 128",
        "authors": [
            "Yves Edel",
            "E. M. Rains",
            "N. J. A. Sloane"
        ],
        "summary": "An elementary construction using binary codes gives new record kissing numbers in dimensions from 32 to 128.",
        "published": "2002-07-30T21:51:04Z",
        "link": "http://arxiv.org/abs/math/0207291v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "94B75, 52C17 (05B40, 11H31, 94B75)"
        ]
    },
    {
        "title": "Self-Dual Codes",
        "authors": [
            "E. M. Rains",
            "N. J. A. Sloane"
        ],
        "summary": "Self-dual codes are important because many of the best codes known are of this type and they have a rich mathematical theory. Topics covered in this survey include codes over F_2, F_3, F_4, F_q, Z_4, Z_m, shadow codes, weight enumerators, Gleason-Pierce theorem, invariant theory, Gleason theorems, bounds, mass formulae, enumeration, extremal codes, open problems. There is a comprehensive bibliography.",
        "published": "2002-08-01T02:33:08Z",
        "link": "http://arxiv.org/abs/math/0208001v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "94-02 (94B60, 94B65)"
        ]
    },
    {
        "title": "Packing Planes in Four Dimensions and Other Mysteries",
        "authors": [
            "N. J. A. Sloane"
        ],
        "summary": "How should you choose a good set of (say) 48 planes in four dimensions? More generally, how do you find packings in Grassmannian spaces? In this article I give a brief introduction to the work that I have been doing on this problem in collaboration with A. R. Calderbank, J. H. Conway, R. H. Hardin, E. M. Rains and P. W. Shor. We have found many nice examples of specific packings (70 4-spaces in 8-space, for instance), several general constructions, and an embedding theorem which shows that a packing in Grassmannian space G(m,n) is a subset of a sphere in R^D, where D = (m+2)(m-1)/2, and leads to a proof that many of our packings are optimal. There are a number of interesting unsolved problems.",
        "published": "2002-08-02T17:12:53Z",
        "link": "http://arxiv.org/abs/math/0208017v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "51E15, 52C17 (51E23, 65Y25)"
        ]
    },
    {
        "title": "Toric codes over finite fields",
        "authors": [
            "David Joyner"
        ],
        "summary": "In this note, a class of error-correcting codes is associated to a toric variety associated to a fan defined over a finite field $\\fff_q$, analogous to the class of Goppa codes associated to a curve. For such a ``toric code'' satisfying certain additional conditions, we present an efficient decoding algorithm for the dual of a Goppa code. Many examples are given. For small $q$, many of these codes have parameters beating the Gilbert-Varshamov bound. In fact, using toric codes, we construct a $(n,k,d)=(49,11,28)$ code over $\\fff_8$, which is better than any other known code listed in Brouwer's on-line tables for that $n$ and $k$.",
        "published": "2002-08-21T17:42:16Z",
        "link": "http://arxiv.org/abs/math/0208155v2",
        "categories": [
            "math.AG",
            "cs.IT",
            "math.CO",
            "math.IT",
            "14M25;94B27"
        ]
    },
    {
        "title": "Uniformly distributed sequences of p-adic integers, II",
        "authors": [
            "Vladimir Anashin"
        ],
        "summary": "The paper describes ergodic (with respect to the Haar measure) functions in the class of all functions, which are defined on (and take values in) the ring of p-adic integers, and which satisfy (at least, locally) Lipschitz condition with coefficient 1. Equiprobable (in particular, measure-preserving) functions of this class are described also. In some cases (and especially for p=2) the descriptions are given by explicit formulae. Some of the results may be viewed as descriptions of ergodic isometric dynamical systems on p-adic unit disk. The study was motivated by the problem of pseudorandom number generation for computer simulation and cryptography. From this view the paper describes nonlinear congruential pseudorandom generators modulo M which produce stricly periodic uniformly distributed sequences modulo M with maximal possible period length (i.e., exactly M). Both the state change function and the output function of these generators could be, e.g., meromorphic functions (in particular, polynomials with rational, but not necessarily integer coefficients, or rational functions), or compositions of arithmetical operations (like addition, multiplication, exponentiation, raising to integer powers, including negative ones) with standard computer operations, such as bitwise logical operations (XOR, OR, AND, etc.). The linear complexity of the produced sequences is also studied.",
        "published": "2002-09-30T13:46:14Z",
        "link": "http://arxiv.org/abs/math/0209407v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.DS",
            "math.IT",
            "11K45 (Primary) 37B99; 94A60 (Secondary)"
        ]
    },
    {
        "title": "Representations of finite groups on Riemann-Roch spaces",
        "authors": [
            "David Joyner",
            "Will Traves"
        ],
        "summary": "We study the action of a finite group on the Riemann-Roch space of certain divisors on a curve. If $G$ is a finite subgroup of the automorphism group of a projective curve $X$ over an algebraically closed field and $D$ is a divisor on $X$ left stable by $G$ then we show the irreducible constituents of the natural representation of $G$ on the Riemann-Roch space $L(D)=L_X(D)$ are of dimension $\\leq d$, where $d$ is the size of the smallest $G$-orbit acting on $X$. We give an example to show that this is, in general, sharp (i.e., that dimension $d$ irreducible constituents can occur). Connections with coding theory, in particular to permutation decoding of AG codes, are discussed in the last section. Many examples are included.",
        "published": "2002-10-26T16:42:13Z",
        "link": "http://arxiv.org/abs/math/0210408v4",
        "categories": [
            "math.AG",
            "cs.IT",
            "math.GR",
            "math.IT",
            "14H37; 20C20; 94B27; 11T71; 05E20"
        ]
    },
    {
        "title": "On cyclic convolutional codes",
        "authors": [
            "H. Gluesing-Luerssen",
            "W. Schmale"
        ],
        "summary": "We investigate the notion of cyclicity for convolutional codes as it has been introduced by Piret and Roos in the seventies. Codes of this type are described as submodules of the module of all vector polynomials in one variable with some additional generalized cyclic structure but also as specific left ideals in a skew polynomial ring. Extending a result of Piret, we show in a purely algebraic setting that these ideals are always principal. This leads to the notion of a generator polynomial just like for cyclic block codes. Similarly a control polynomial can be introduced by considering the right annihilator ideal. An algorithmic procedure is developed which produces unique reduced generator and control polynomials. We also show how basic code properties and a minimal generator matrix can be read off from these objects. A close link between polynomial and vector description of the codes is provided by certain generalized circulant matrices.",
        "published": "2002-11-04T09:10:54Z",
        "link": "http://arxiv.org/abs/math/0211040v1",
        "categories": [
            "math.RA",
            "cs.IT",
            "math.CO",
            "math.IT",
            "94B10, 94B15, 16S36"
        ]
    },
    {
        "title": "On Near-MDS Elliptic Codes",
        "authors": [
            "Massimo Giulietti"
        ],
        "summary": "The main conjecture on maximum distance separable (MDS) codes states that, execpt for some special cases, the maximum length of a q-ary linear MDS code is q+1. This conjecture does not hold true for near maximum distance separable codes because of the existence of q-ary near MDS elliptic codes having length bigger than q+1. An interesting related question is whether a near MDS elliptic code can be extended to a longer near MDS code. Our results are some non-extendability results and an alternative and simpler construction for certain known near MDS elliptic codes.",
        "published": "2002-11-06T13:41:51Z",
        "link": "http://arxiv.org/abs/math/0211107v1",
        "categories": [
            "math.AG",
            "cs.IT",
            "math.CO",
            "math.IT",
            "51E22;14H52"
        ]
    },
    {
        "title": "On ASGS framework: general requirements and an example of implementation",
        "authors": [
            "Kamil Kulesza",
            "Zbigniew Kotulski"
        ],
        "summary": "In the paper we propose general framework for Automatic Secret Generation and Sharing (ASGS) that should be independent of underlying secret sharing scheme. ASGS allows to prevent the dealer from knowing the secret or even to eliminate him at all. Two situations are discussed. First concerns simultaneous generation and sharing of the random, prior nonexistent secret. Such a secret remains unknown until it is reconstructed. Next, we propose the framework for automatic sharing of a known secret. In this case the dealer does not know the secret and the secret owner does not know the shares. We present opportunities for joining ASGS with other extended capabilities, with special emphasize on PVSS and proactive secret sharing. Finally, we illustrate framework with practical implementation.   Keywords: cryptography, secret sharing, data security, extended capabilities, extended key verification protocol",
        "published": "2002-11-18T09:50:31Z",
        "link": "http://arxiv.org/abs/math/0211269v2",
        "categories": [
            "math.CO",
            "cs.CR",
            "cs.DM",
            "cs.IT",
            "math.IT",
            "D.4.6; E.4"
        ]
    },
    {
        "title": "A Goppa-like bound on the trellis state complexity of algebraic   geometric codes",
        "authors": [
            "Carlos Munuera",
            "Fernando Torres"
        ],
        "summary": "For a linear code $\\cC$ of length $n$ and dimension $k$, Wolf noticed that the trellis state complexity $s(\\cC)$ of $\\cC$ is upper bounded by $w(\\cC):=\\min(k,n-k)$. In this paper we point out some new lower bounds for $s(\\cC)$. In particular, if $\\cC$ is an Algebraic Geometric code, then $s(\\cC)\\geq w(\\cC)-(g-a)$, where $g$ is the genus of the underlying curve and $a$ is the abundance of the code.",
        "published": "2002-12-03T14:13:55Z",
        "link": "http://arxiv.org/abs/math/0212038v1",
        "categories": [
            "math.AG",
            "cs.IT",
            "math.IT",
            "94B05, 94B27, 14G50"
        ]
    },
    {
        "title": "Coverage control for mobile sensing networks",
        "authors": [
            "J. Cortes",
            "S. Martinez",
            "T. Karatas",
            "F. Bullo"
        ],
        "summary": "This paper presents control and coordination algorithms for groups of vehicles. The focus is on autonomous vehicle networks performing distributed sensing tasks where each vehicle plays the role of a mobile tunable sensor. The paper proposes gradient descent algorithms for a class of utility functions which encode optimal coverage and sensing policies. The resulting closed-loop behavior is adaptive, distributed, asynchronous, and verifiably correct.",
        "published": "2002-12-16T18:51:14Z",
        "link": "http://arxiv.org/abs/math/0212212v1",
        "categories": [
            "math.OC",
            "cs.IT",
            "math.IT",
            "68W15; 49N90; 93B52; 51M20; 70E60"
        ]
    },
    {
        "title": "Design of statistical quality control procedures using genetic   algorithms",
        "authors": [
            "Aristides T. Hatjimihail",
            "Theophanes T. Hatjimihail"
        ],
        "summary": "In general, we can not use algebraic or enumerative methods to optimize a quality control (QC) procedure so as to detect the critical random and systematic analytical errors with stated probabilities, while the probability for false rejection is minimum. Genetic algorithms (GAs) offer an alternative, as they do not require knowledge of the objective function to be optimized and search through large parameter spaces quickly. To explore the application of GAs in statistical QC, we have developed an interactive GAs based computer program that designs a novel near optimal QC procedure, given an analytical process. The program uses the deterministic crowding algorithm. An illustrative application of the program suggests that it has the potential to design QC procedures that are significantly better than 45 alternative ones that are used in the clinical laboratories.",
        "published": "2002-01-27T21:01:45Z",
        "link": "http://arxiv.org/abs/cs/0201024v2",
        "categories": [
            "cs.NE",
            "G.1.6"
        ]
    },
    {
        "title": "Steady State Resource Allocation Analysis of the Stochastic Diffusion   Search",
        "authors": [
            "Slawomir J. Nasuto",
            "Mark J. Bishop"
        ],
        "summary": "This article presents the long-term behaviour analysis of Stochastic Diffusion Search (SDS), a distributed agent-based system for best-fit pattern matching. SDS operates by allocating simple agents into different regions of the search space. Agents independently pose hypotheses about the presence of the pattern in the search space and its potential distortion. Assuming a compositional structure of hypotheses about pattern matching agents perform an inference on the basis of partial evidence from the hypothesised solution. Agents posing mutually consistent hypotheses about the pattern support each other and inhibit agents with inconsistent hypotheses. This results in the emergence of a stable agent population identifying the desired solution. Positive feedback via diffusion of information between the agents significantly contributes to the speed with which the solution population is formed. The formulation of the SDS model in terms of interacting Markov Chains enables its characterisation in terms of the allocation of agents, or computational resources. The analysis characterises the stationary probability distribution of the activity of agents, which leads to the characterisation of the solution population in terms of its similarity to the target pattern.",
        "published": "2002-02-07T14:11:18Z",
        "link": "http://arxiv.org/abs/cs/0202007v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.8; I.2.11; F.2.2; G.3"
        ]
    },
    {
        "title": "Non-negative sparse coding",
        "authors": [
            "Patrik O. Hoyer"
        ],
        "summary": "Non-negative sparse coding is a method for decomposing multivariate data into non-negative sparse components. In this paper we briefly describe the motivation behind this type of data representation and its relation to standard sparse coding and non-negative matrix factorization. We then give a simple yet efficient multiplicative algorithm for finding the optimal values of the hidden components. In addition, we show how the basis vectors can be learned from the observed data. Simulations demonstrate the effectiveness of the proposed method.",
        "published": "2002-02-11T11:04:08Z",
        "link": "http://arxiv.org/abs/cs/0202009v1",
        "categories": [
            "cs.NE",
            "cs.CV",
            "I.4.2"
        ]
    },
    {
        "title": "Threshold Disorder as a Source of Diverse and Complex Behavior in Random   Nets",
        "authors": [
            "Patrick C. McGuire",
            "Henrik Bohr",
            "John W. Clark",
            "Robert Haschke",
            "Chris Pershing",
            "Johann Rafelski"
        ],
        "summary": "We study the diversity of complex spatio-temporal patterns in the behavior of random synchronous asymmetric neural networks (RSANNs). Special attention is given to the impact of disordered threshold values on limit-cycle diversity and limit-cycle complexity in RSANNs which have `normal' thresholds by default. Surprisingly, RSANNs exhibit only a small repertoire of rather complex limit-cycle patterns when all parameters are fixed. This repertoire of complex patterns is also rather stable with respect to small parameter changes. These two unexpected results may generalize to the study of other complex systems. In order to reach beyond this seemingly-disabling `stable and small' aspect of the limit-cycle repertoire of RSANNs, we have found that if an RSANN has threshold disorder above a critical level, then there is a rapid increase of the size of the repertoire of patterns. The repertoire size initially follows a power-law function of the magnitude of the threshold disorder. As the disorder increases further, the limit-cycle patterns themselves become simpler until at a second critical level most of the limit cycles become simple fixed points. Nonetheless, for moderate changes in the threshold parameters, RSANNs are found to display specific features of behavior desired for rapidly-responding processing systems: accessibility to a large set of complex patterns.",
        "published": "2002-02-12T00:30:12Z",
        "link": "http://arxiv.org/abs/cond-mat/0202190v1",
        "categories": [
            "cond-mat.dis-nn",
            "cs.NE",
            "q-bio.NC"
        ]
    },
    {
        "title": "On model selection and the disability of neural networks to decompose   tasks",
        "authors": [
            "Marc Toussaint"
        ],
        "summary": "A neural network with fixed topology can be regarded as a parametrization of functions, which decides on the correlations between functional variations when parameters are adapted. We propose an analysis, based on a differential geometry point of view, that allows to calculate these correlations. In practise, this describes how one response is unlearned while another is trained. Concerning conventional feed-forward neural networks we find that they generically introduce strong correlations, are predisposed to forgetting, and inappropriate for task decomposition. Perspectives to solve these problems are discussed.",
        "published": "2002-02-19T12:39:23Z",
        "link": "http://arxiv.org/abs/nlin/0202038v1",
        "categories": [
            "nlin.AO",
            "cond-mat.dis-nn",
            "cs.NE"
        ]
    },
    {
        "title": "A neural model for multi-expert architectures",
        "authors": [
            "Marc Toussaint"
        ],
        "summary": "We present a generalization of conventional artificial neural networks that allows for a functional equivalence to multi-expert systems. The new model provides an architectural freedom going beyond existing multi-expert models and an integrative formalism to compare and combine various techniques of learning. (We consider gradient, EM, reinforcement, and unsupervised learning.) Its uniform representation aims at a simple genetic encoding and evolutionary structure optimization of multi-expert systems. This paper contains a detailed description of the model and learning rules, empirically validates its functionality, and discusses future perspectives.",
        "published": "2002-02-19T13:11:42Z",
        "link": "http://arxiv.org/abs/nlin/0202039v1",
        "categories": [
            "nlin.AO",
            "cond-mat.dis-nn",
            "cs.NE"
        ]
    },
    {
        "title": "Covariance Plasticity and Regulated Criticality",
        "authors": [
            "Elie Bienenstock",
            "Daniel Lehmann"
        ],
        "summary": "We propose that a regulation mechanism based on Hebbian covariance plasticity may cause the brain to operate near criticality. We analyze the effect of such a regulation on the dynamics of a network with excitatory and inhibitory neurons and uniform connectivity within and across the two populations. We show that, under broad conditions, the system converges to a critical state lying at the common boundary of three regions in parameter space; these correspond to three modes of behavior: high activity, low activity, oscillation.",
        "published": "2002-02-20T17:12:03Z",
        "link": "http://arxiv.org/abs/cs/0202034v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "nlin.AO",
            "q-bio",
            "I.2.6"
        ]
    },
    {
        "title": "Neutrality: A Necessity for Self-Adaptation",
        "authors": [
            "Marc Toussaint",
            "Christian Igel"
        ],
        "summary": "Self-adaptation is used in all main paradigms of evolutionary computation to increase efficiency. We claim that the basis of self-adaptation is the use of neutrality. In the absence of external control neutrality allows a variation of the search distribution without the risk of fitness loss.",
        "published": "2002-04-16T12:58:26Z",
        "link": "http://arxiv.org/abs/nlin/0204038v1",
        "categories": [
            "nlin.AO",
            "cs.NE",
            "q-bio"
        ]
    },
    {
        "title": "Trust Brokerage Systems for the Internet",
        "authors": [
            "Walter Eaves"
        ],
        "summary": "This thesis addresses the problem of providing trusted individuals with confidential information about other individuals, in particular, granting access to databases of personal records using the World-Wide Web. It proposes an access rights management system for distributed databases which aims to create and implement organisation structures based on the wishes of the owners and of demands of the users of the databases. The dissertation describes how current software components could be used to implement this system; it re-examines the theory of collective choice to develop mechanisms for generating hierarchies of authorities; it analyses organisational processes for stability and develops a means of measuring the similarity of their hierarchies.",
        "published": "2002-04-18T23:10:44Z",
        "link": "http://arxiv.org/abs/cs/0204041v1",
        "categories": [
            "cs.CR",
            "cs.GT",
            "cs.NE",
            "C.2.4; F.4.1; G.2.1; G.3; H.2.4; H.2.7; H.3.5; H.5.3; J.3; K.4.1;\n  K.6.5"
        ]
    },
    {
        "title": "Learning from Scarce Experience",
        "authors": [
            "Leonid Peshkin",
            "Christian R. Shelton"
        ],
        "summary": "Searching the space of policies directly for the optimal policy has been one popular method for solving partially observable reinforcement learning problems. Typically, with each change of the target policy, its value is estimated from the results of following that very policy. This requires a large number of interactions with the environment as different polices are considered. We present a family of algorithms based on likelihood ratio estimation that use data gathered when executing one policy (or collection of policies) to estimate the value of a different policy. The algorithms combine estimation and optimization stages. The former utilizes experience to build a non-parametric representation of an optimized function. The latter performs optimization on this estimate. We show positive empirical results and provide the sample complexity bound.",
        "published": "2002-04-20T05:02:53Z",
        "link": "http://arxiv.org/abs/cs/0204043v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.NE",
            "cs.RO",
            "I.2; I.2.8; I.2.11; I.2.6; G.1.6"
        ]
    },
    {
        "title": "Aging, double helix and small world property in genetic algorithms",
        "authors": [
            "Marek W. Gutowski"
        ],
        "summary": "Over a quarter of century after the invention of genetic algorithms and miriads of their modifications, as well as successful implementations, we are still lacking many essential details of thorough analysis of it's inner working. One of such fundamental questions is: how many generations do we need to solve the optimization problem? This paper tries to answer this question, albeit in a fuzzy way, making use of the double helix concept. As a byproduct we gain better understanding of the ways, in which the genetic algorithm may be fine tuned.",
        "published": "2002-05-23T15:34:57Z",
        "link": "http://arxiv.org/abs/cs/0205061v1",
        "categories": [
            "cs.NE",
            "cs.DS",
            "physics.data-an",
            "F.2.1; G.1.6; I.1.2"
        ]
    },
    {
        "title": "Neural Net Model for Featured Word Extraction",
        "authors": [
            "A. Das",
            "M. Marko",
            "A. Probst",
            "M. A. Porter",
            "C. Gershenson"
        ],
        "summary": "Search engines perform the task of retrieving information related to the user-supplied query words. This task has two parts; one is finding \"featured words\" which describe an article best and the other is finding a match among these words to user-defined search terms. There are two main independent approaches to achieve this task. The first one, using the concepts of semantics, has been implemented partially. For more details see another paper of Marko et al., 2002. The second approach is reported in this paper. It is a theoretical model based on using Neural Network (NN). Instead of using keywords or reading from the first few lines from papers/articles, the present model gives emphasis on extracting \"featured words\" from an article. Obviously we propose to exclude prepositions, articles and so on, that is, English words like \"of, the, are, so, therefore, \" etc. from such a list. A neural model is taken with its nodes pre-assigned energies. Whenever a match is found with featured words and userdefined search words, the node is fired and jumps to a higher energy. This firing continues until the model attains a steady energy level and total energy is now calculated. Clearly, higher match will generate higher energy; so on the basis of total energy, a ranking is done to the article indicating degree of relevance to the user's interest. Another important feature of the proposed model is incorporating a semantic module to refine the search words; like finding association among search words, etc. In this manner, information retrieval can be improved markedly.",
        "published": "2002-06-01T15:10:56Z",
        "link": "http://arxiv.org/abs/cs/0206001v1",
        "categories": [
            "cs.NE",
            "cs.NI",
            "I.2.6"
        ]
    },
    {
        "title": "Computer modeling of feelings and emotions: a quantitative neural   network model of the feeling-of-knowing",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "The first quantitative neural network model of feelings and emotions is proposed on the base of available data on their neuroscience and evolutionary biology nature, and on a neural network human memory model which admits distinct description of conscious and unconscious mental processes in a time dependent manner. As an example, proposed model is applied to quantitative description of the feeling of knowing.",
        "published": "2002-06-03T22:31:45Z",
        "link": "http://arxiv.org/abs/cs/0206008v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "q-bio.NC",
            "q-bio.QM",
            "I.2.0; J.4"
        ]
    },
    {
        "title": "Behaviour-based Knowledge Systems: An Epigenetic Path from Behaviour to   Knowledge",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "In this paper we expose the theoretical background underlying our current research. This consists in the development of behaviour-based knowledge systems, for closing the gaps between behaviour-based and knowledge-based systems, and also between the understandings of the phenomena they model. We expose the requirements and stages for developing behaviour-based knowledge systems and discuss their limits. We believe that these are necessary conditions for the development of higher order cognitive capacities, in artificial and natural cognitive systems.",
        "published": "2002-06-18T13:05:50Z",
        "link": "http://arxiv.org/abs/cs/0206027v1",
        "categories": [
            "cs.AI",
            "cs.AR",
            "cs.NE",
            "I.2.0, I.2.6"
        ]
    },
    {
        "title": "Winner-Relaxing Self-Organizing Maps",
        "authors": [
            "Jens Christian Claussen"
        ],
        "summary": "A new family of self-organizing maps, the Winner-Relaxing Kohonen Algorithm, is introduced as a generalization of a variant given by Kohonen in 1991. The magnification behaviour is calculated analytically. For the original variant a magnification exponent of 4/7 is derived; the generalized version allows to steer the magnification in the wide range from exponent 1/2 to 1 in the one-dimensional case, thus provides optimal mapping in the sense of information theory. The Winner Relaxing Algorithm requires minimal extra computations per learning step and is conveniently easy to implement.",
        "published": "2002-08-21T17:10:28Z",
        "link": "http://arxiv.org/abs/cond-mat/0208414v2",
        "categories": [
            "cond-mat.dis-nn",
            "cs.NE",
            "nlin.AO",
            "q-bio.NC"
        ]
    },
    {
        "title": "Extremal Optimization: an Evolutionary Local-Search Algorithm",
        "authors": [
            "Stefan Boettcher",
            "Allon G. Percus"
        ],
        "summary": "A recently introduced general-purpose heuristic for finding high-quality solutions for many hard optimization problems is reviewed. The method is inspired by recent progress in understanding far-from-equilibrium phenomena in terms of {\\em self-organized criticality,} a concept introduced to describe emergent complexity in physical systems. This method, called {\\em extremal optimization,} successively replaces the value of extremely undesirable variables in a sub-optimal solution with new, random ones. Large, avalanche-like fluctuations in the cost function self-organize from this dynamics, effectively scaling barriers to explore local optima in distant neighborhoods of the configuration space while eliminating the need to tune parameters. Drawing upon models used to simulate the dynamics of granular media, evolution, or geology, extremal optimization complements approximation methods inspired by equilibrium statistical physics, such as {\\em simulated annealing}. It may be but one example of applying new insights into {\\em non-equilibrium phenomena} systematically to hard optimization problems. This method is widely applicable and so far has proved competitive with -- and even superior to -- more elaborate general-purpose heuristics on testbeds of constrained optimization problems with up to $10^5$ variables, such as bipartitioning, coloring, and satisfiability. Analysis of a suitable model predicts the only free parameter of the method in accordance with all experimental results.",
        "published": "2002-09-26T14:16:15Z",
        "link": "http://arxiv.org/abs/cs/0209030v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "Selection of future events from a time series in relation to estimations   of forecasting uncertainty",
        "authors": [
            "Igor B. Konovalov"
        ],
        "summary": "A new general procedure for a priori selection of more predictable events from a time series of observed variable is proposed. The procedure is applicable to time series which contains different types of events that feature significantly different predictability, or, in other words, to heteroskedastic time series. A priori selection of future events in accordance to expected uncertainty of their forecasts may be helpful for making practical decisions. The procedure first implies creation of two neural network based forecasting models, one of which is aimed at prediction of conditional mean and other - conditional dispersion, and then elaboration of the rule for future event selection into groups of more and less predictable events. The method is demonstrated and tested by the example of the computer generated time series, and then applied to the real world time series, Dow Jones Industrial Average index.",
        "published": "2002-10-14T09:00:23Z",
        "link": "http://arxiv.org/abs/cs/0210012v1",
        "categories": [
            "cs.NE",
            "J.2; J.4"
        ]
    },
    {
        "title": "Intelligence and Cooperative Search by Coupled Local Minimizers",
        "authors": [
            "J. A. K. Suykens",
            "J. Vandewalle",
            "B. De Moor"
        ],
        "summary": "We show how coupling of local optimization processes can lead to better solutions than multi-start local optimization consisting of independent runs. This is achieved by minimizing the average energy cost of the ensemble, subject to synchronization constraints between the state vectors of the individual local minimizers. From an augmented Lagrangian which incorporates the synchronization constraints both as soft and hard constraints, a network is derived wherein the local minimizers interact and exchange information through the synchronization constraints. From the viewpoint of neural networks, the array can be considered as a Lagrange programming network for continuous optimization and as a cellular neural network (CNN). The penalty weights associated with the soft state synchronization constraints follow from the solution to a linear program. This expresses that the energy cost of the ensemble should maximally decrease. In this way successful local minimizers can implicitly impose their state to the others through a mechanism of master-slave dynamics resulting into a cooperative search mechanism. Improved information spreading within the ensemble is obtained by applying the concept of small-world networks. This work suggests, in an interdisciplinary context, the importance of information exchange and state synchronization within ensembles, towards issues as evolution, collective behaviour, optimality and intelligence.",
        "published": "2002-10-30T11:59:57Z",
        "link": "http://arxiv.org/abs/cs/0210030v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "cs.NE",
            "F.1.1"
        ]
    },
    {
        "title": "JohnnyVon: Self-Replicating Automata in Continuous Two-Dimensional Space",
        "authors": [
            "Arnold Smith",
            "Peter Turney",
            "Robert Ewaschuk"
        ],
        "summary": "JohnnyVon is an implementation of self-replicating automata in continuous two-dimensional space. Two types of particles drift about in a virtual liquid. The particles are automata with discrete internal states but continuous external relationships. Their internal states are governed by finite state machines but their external relationships are governed by a simulated physics that includes brownian motion, viscosity, and spring-like attractive and repulsive forces. The particles can be assembled into patterns that can encode arbitrary strings of bits. We demonstrate that, if an arbitrary \"seed\" pattern is put in a \"soup\" of separate individual particles, the pattern will replicate by assembling the individual particles into copies of itself. We also show that, given sufficient time, a soup of separate individual particles will eventually spontaneously form self-replicating patterns. We discuss the implications of JohnnyVon for research in nanotechnology, theoretical biology, and artificial life.",
        "published": "2002-12-08T00:26:49Z",
        "link": "http://arxiv.org/abs/cs/0212010v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "I.6.3; I.6.8; J.2; J.3"
        ]
    },
    {
        "title": "Thinking, Learning, and Autonomous Problem Solving",
        "authors": [
            "Joerg D. Becker"
        ],
        "summary": "Ever increasing computational power will require methods for automatic programming. We present an alternative to genetic programming, based on a general model of thinking and learning. The advantage is that evolution takes place in the space of constructs and can thus exploit the mathematical structures of this space. The model is formalized, and a macro language is presented which allows for a formal yet intuitive description of the problem under consideration. A prototype has been developed to implement the scheme in PERL. This method will lead to a concentration on the analysis of problems, to a more rapid prototyping, to the treatment of new problem classes, and to the investigation of philosophical problems. We see fields of application in nonlinear differential equations, pattern recognition, robotics, model building, and animated pictures.",
        "published": "2002-12-10T15:18:33Z",
        "link": "http://arxiv.org/abs/cs/0212019v1",
        "categories": [
            "cs.NE",
            "H.1.1; I.2.0; I.2.2"
        ]
    },
    {
        "title": "A Simple Model of Unbounded Evolutionary Versatility as a Largest-Scale   Trend in Organismal Evolution",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "The idea that there are any large-scale trends in the evolution of biological organisms is highly controversial. It is commonly believed, for example, that there is a large-scale trend in evolution towards increasing complexity, but empirical and theoretical arguments undermine this belief. Natural selection results in organisms that are well adapted to their local environments, but it is not clear how local adaptation can produce a global trend. In this paper, I present a simple computational model, in which local adaptation to a randomly changing environment results in a global trend towards increasing evolutionary versatility. In this model, for evolutionary versatility to increase without bound, the environment must be highly dynamic. The model also shows that unbounded evolutionary versatility implies an accelerating evolutionary pace. I believe that unbounded increase in evolutionary versatility is a large-scale trend in evolution. I discuss some of the testable predictions about organismal evolution that are suggested by the model.",
        "published": "2002-12-10T15:52:43Z",
        "link": "http://arxiv.org/abs/cs/0212021v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "q-bio.PE",
            "I.6.3; I.6.8; J.3"
        ]
    },
    {
        "title": "How to Shift Bias: Lessons from the Baldwin Effect",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "An inductive learning algorithm takes a set of data as input and generates a hypothesis as output. A set of data is typically consistent with an infinite number of hypotheses; therefore, there must be factors other than the data that determine the output of the learning algorithm. In machine learning, these other factors are called the bias of the learner. Classical learning algorithms have a fixed bias, implicit in their design. Recently developed learning algorithms dynamically adjust their bias as they search for a hypothesis. Algorithms that shift bias in this manner are not as well understood as classical algorithms. In this paper, we show that the Baldwin effect has implications for the design and analysis of bias shifting algorithms. The Baldwin effect was proposed in 1896, to explain how phenomena that might appear to require Lamarckian evolution (inheritance of acquired characteristics) can arise from purely Darwinian evolution. Hinton and Nowlan presented a computational model of the Baldwin effect in 1987. We explore a variation on their model, which we constructed explicitly to illustrate the lessons that the Baldwin effect has for research in bias shifting algorithms. The main lesson is that it appears that a good strategy for shift of bias in a learning algorithm is to begin with a weak bias and gradually shift to a strong bias.",
        "published": "2002-12-10T18:19:54Z",
        "link": "http://arxiv.org/abs/cs/0212023v1",
        "categories": [
            "cs.LG",
            "cs.NE",
            "I.2.6; I.2.8"
        ]
    },
    {
        "title": "The structure of evolutionary exploration: On crossover, buildings   blocks and Estimation-Of-Distribution Algorithms",
        "authors": [
            "Marc Toussaint"
        ],
        "summary": "The notion of building blocks can be related to the structure of the offspring probability distribution: loci of which variability is strongly correlated constitute a building block. We call this correlated exploration. With this background we analyze the structure of the offspring probability distribution, or exploration distribution, for a GA with mutation only, a crossover GA, and an Estimation-Of-Distribution Algorithm (EDA). The results allow a precise characterization of the structure of the crossover exploration distribution. Essentially, the crossover operator destroys mutual information between loci by transforming it into entropy; it does the inverse of correlated exploration. In contrast, the objective of EDAs is to model the mutual information between loci in the fitness distribution and thereby they induce correlated exploration.",
        "published": "2002-12-11T19:28:38Z",
        "link": "http://arxiv.org/abs/nlin/0212030v1",
        "categories": [
            "nlin.AO",
            "cs.NE",
            "q-bio"
        ]
    },
    {
        "title": "Myths and Legends of the Baldwin Effect",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This position paper argues that the Baldwin effect is widely misunderstood by the evolutionary computation community. The misunderstandings appear to fall into two general categories. Firstly, it is commonly believed that the Baldwin effect is concerned with the synergy that results when there is an evolving population of learning individuals. This is only half of the story. The full story is more complicated and more interesting. The Baldwin effect is concerned with the costs and benefits of lifetime learning by individuals in an evolving population. Several researchers have focussed exclusively on the benefits, but there is much to be gained from attention to the costs. This paper explains the two sides of the story and enumerates ten of the costs and benefits of lifetime learning by individuals in an evolving population. Secondly, there is a cluster of misunderstandings about the relationship between the Baldwin effect and Lamarckian inheritance of acquired characteristics. The Baldwin effect is not Lamarckian. A Lamarckian algorithm is not better for most evolutionary computing problems than a Baldwinian algorithm. Finally, Lamarckian inheritance is not a better model of memetic (cultural) evolution than the Baldwin effect.",
        "published": "2002-12-11T21:34:18Z",
        "link": "http://arxiv.org/abs/cs/0212036v1",
        "categories": [
            "cs.LG",
            "cs.NE",
            "I.2.6; I.2.8"
        ]
    },
    {
        "title": "Low Size-Complexity Inductive Logic Programming: The East-West Challenge   Considered as a Problem in Cost-Sensitive Classification",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "The Inductive Logic Programming community has considered proof-complexity and model-complexity, but, until recently, size-complexity has received little attention. Recently a challenge was issued \"to the international computing community\" to discover low size-complexity Prolog programs for classifying trains. The challenge was based on a problem first proposed by Ryszard Michalski, 20 years ago. We interpreted the challenge as a problem in cost-sensitive classification and we applied a recently developed cost-sensitive classifier to the competition. Our algorithm was relatively successful (we won a prize). This paper presents our algorithm and analyzes the results of the competition.",
        "published": "2002-12-12T18:51:06Z",
        "link": "http://arxiv.org/abs/cs/0212039v1",
        "categories": [
            "cs.LG",
            "cs.NE",
            "I.2.6; I.2.8"
        ]
    },
    {
        "title": "Increasing Evolvability Considered as a Large-Scale Trend in Evolution",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Evolvability is the capacity to evolve. This paper introduces a simple computational model of evolvability and demonstrates that, under certain conditions, evolvability can increase indefinitely, even when there is no direct selection for evolvability. The model shows that increasing evolvability implies an accelerating evolutionary pace. It is suggested that the conditions for indefinitely increasing evolvability are satisfied in biological and cultural evolution. We claim that increasing evolvability is a large-scale trend in evolution. This hypothesis leads to testable predictions about biological and cultural evolution.",
        "published": "2002-12-12T22:39:39Z",
        "link": "http://arxiv.org/abs/cs/0212042v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "q-bio.PE",
            "I.6.3; I.6.8; J.3"
        ]
    },
    {
        "title": "Incremental Construction of Compact Acyclic NFAs",
        "authors": [
            "Kyriakos N. Sgarbas",
            "Nikos D. Fakotakis",
            "George K. Kokkinakis"
        ],
        "summary": "This paper presents and analyzes an incremental algorithm for the construction of Acyclic Non-deterministic Finite-state Automata (NFA). Automata of this type are quite useful in computational linguistics, especially for storing lexicons. The proposed algorithm produces compact NFAs, i.e. NFAs that do not contain equivalent states. Unlike Deterministic Finite-state Automata (DFA), this property is not sufficient to ensure minimality, but still the resulting NFAs are considerably smaller than the minimal DFAs for the same languages.",
        "published": "2002-01-04T17:01:14Z",
        "link": "http://arxiv.org/abs/cs/0201002v1",
        "categories": [
            "cs.DS",
            "cs.CL",
            "E.1; F.2.2; H.3.1; I.2.7"
        ]
    },
    {
        "title": "Algorithm for generating orthogonal matrices with rational elements",
        "authors": [
            "Ruslan Sharipov"
        ],
        "summary": "Special orthogonal matrices with rational elements form the group SO(n,Q), where Q is the field of rational numbers. A theorem describing the structure of an arbitrary matrix from this group is proved. This theorem yields an algorithm for generating such matrices by means of random number routines.",
        "published": "2002-01-10T15:54:24Z",
        "link": "http://arxiv.org/abs/cs/0201007v2",
        "categories": [
            "cs.MS",
            "cs.DS",
            "G.4; K.3.1"
        ]
    },
    {
        "title": "Using Tree Automata and Regular Expressions to Manipulate Hierarchically   Structured Data",
        "authors": [
            "Nikita Schmidt",
            "Ahmed Patel"
        ],
        "summary": "Information, stored or transmitted in digital form, is often structured. Individual data records are usually represented as hierarchies of their elements. Together, records form larger structures. Information processing applications have to take account of this structuring, which assigns different semantics to different data elements or records. Big variety of structural schemata in use today often requires much flexibility from applications--for example, to process information coming from different sources. To ensure application interoperability, translators are needed that can convert one structure into another. This paper puts forward a formal data model aimed at supporting hierarchical data processing in a simple and flexible way. The model is based on and extends results of two classical theories, studying finite string and tree automata. The concept of finite automata and regular languages is applied to the case of arbitrarily structured tree-like hierarchical data records, represented as \"structured strings.\" These automata are compared with classical string and tree automata; the model is shown to be a superset of the classical models. Regular grammars and expressions over structured strings are introduced. Regular expression matching and substitution has been widely used for efficient unstructured text processing; the model described here brings the power of this proven technique to applications that deal with information trees. A simple generic alternative is offered to replace today's specialised ad-hoc approaches. The model unifies structural and content transformations, providing applications with a single data type. An example scenario of how to build applications based on this theory is discussed. Further research directions are outlined.",
        "published": "2002-01-11T17:59:17Z",
        "link": "http://arxiv.org/abs/cs/0201008v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "E.1; F.4.3; I.7.2; H.3.3"
        ]
    },
    {
        "title": "The Dynamics of AdaBoost Weights Tells You What's Hard to Classify",
        "authors": [
            "Bruno Caprile",
            "Cesare Furlanello",
            "Stefano Merler"
        ],
        "summary": "The dynamical evolution of weights in the Adaboost algorithm contains useful information about the role that the associated data points play in the built of the Adaboost model. In particular, the dynamics induces a bipartition of the data set into two (easy/hard) classes. Easy points are ininfluential in the making of the model, while the varying relevance of hard points can be gauged in terms of an entropy value associated to their evolution. Smooth approximations of entropy highlight regions where classification is most uncertain. Promising results are obtained when methods proposed are applied in the Optimal Sampling framework.",
        "published": "2002-01-17T13:42:23Z",
        "link": "http://arxiv.org/abs/cs/0201014v1",
        "categories": [
            "cs.LG",
            "cs.DS",
            "I.5.1"
        ]
    },
    {
        "title": "Secure History Preservation Through Timeline Entanglement",
        "authors": [
            "Petros Maniatis",
            "Mary Baker"
        ],
        "summary": "A secure timeline is a tamper-evident historic record of the states through which a system goes throughout its operational history. Secure timelines can help us reason about the temporal ordering of system states in a provable manner. We extend secure timelines to encompass multiple, mutually distrustful services, using timeline entanglement. Timeline entanglement associates disparate timelines maintained at independent systems, by linking undeniably the past of one timeline to the future of another. Timeline entanglement is a sound method to map a time step in the history of one service onto the timeline of another, and helps clients of entangled services to get persistent temporal proofs for services rendered that survive the demise or non-cooperation of the originating service. In this paper we present the design and implementation of Timeweave, our service development framework for timeline entanglement based on two novel disk-based authenticated data structures. We evaluate Timeweave's performance characteristics and show that it can be efficiently deployed in a loosely-coupled distributed system of a few hundred services with overhead of roughly 2-8% of the processing resources of a PC-grade system.",
        "published": "2002-02-06T21:52:09Z",
        "link": "http://arxiv.org/abs/cs/0202005v1",
        "categories": [
            "cs.DC",
            "cs.CR",
            "cs.DB",
            "cs.DS",
            "D.4.6; K.6.5; E.2; C.2.4"
        ]
    },
    {
        "title": "Generalized Cores",
        "authors": [
            "V. Batagelj",
            "M. Zaveršnik"
        ],
        "summary": "Cores are, besides connectivity components, one among few concepts that provides us with efficient decompositions of large graphs and networks.   In the paper a generalization of the notion of core of a graph based on vertex property function is presented. It is shown that for the local monotone vertex property functions the corresponding cores can be determined in $O(m \\max (\\Delta, \\log n))$ time.",
        "published": "2002-02-28T17:40:25Z",
        "link": "http://arxiv.org/abs/cs/0202039v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Improving Table Compression with Combinatorial Optimization",
        "authors": [
            "Adam L. Buchsbaum",
            "Glenn S. Fowler",
            "Raffaele Giancarlo"
        ],
        "summary": "We study the problem of compressing massive tables within the partition-training paradigm introduced by Buchsbaum et al. [SODA'00], in which a table is partitioned by an off-line training procedure into disjoint intervals of columns, each of which is compressed separately by a standard, on-line compressor like gzip. We provide a new theory that unifies previous experimental observations on partitioning and heuristic observations on column permutation, all of which are used to improve compression rates. Based on the theory, we devise the first on-line training algorithms for table compression, which can be applied to individual files, not just continuously operating sources; and also a new, off-line training algorithm, based on a link to the asymmetric traveling salesman problem, which improves on prior work by rearranging columns prior to partitioning. We demonstrate these results experimentally. On various test files, the on-line algorithms provide 35-55% improvement over gzip with negligible slowdown; the off-line reordering provides up to 20% further improvement over partitioning alone. We also show that a variation of the table compression problem is MAX-SNP hard.",
        "published": "2002-03-13T19:09:41Z",
        "link": "http://arxiv.org/abs/cs/0203018v1",
        "categories": [
            "cs.DS",
            "E.4; F.1.3; F.2.2; G.2.1; H.1.1; H.1.8; H.2.7"
        ]
    },
    {
        "title": "New Results on Monotone Dualization and Generating Hypergraph   Transversals",
        "authors": [
            "Thomas Eiter",
            "Georg Gottlob",
            "Kazuhisa Makino"
        ],
        "summary": "We consider the problem of dualizing a monotone CNF (equivalently, computing all minimal transversals of a hypergraph), whose associated decision problem is a prominent open problem in NP-completeness. We present a number of new polynomial time resp. output-polynomial time results for significant cases, which largely advance the tractability frontier and improve on previous results. Furthermore, we show that duality of two monotone CNFs can be disproved with limited nondeterminism. More precisely, this is feasible in polynomial time with O(chi(n) * log n) suitably guessed bits, where chi(n) is given by \\chi(n)^chi(n) = n; note that chi(n) = o(log n). This result sheds new light on the complexity of this important problem.",
        "published": "2002-04-04T19:23:49Z",
        "link": "http://arxiv.org/abs/cs/0204009v3",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2; F.1.3; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Fast Universalization of Investment Strategies with Provably Good   Relative Returns",
        "authors": [
            "Karhan Akcoglu",
            "Petros Drineas",
            "Ming-Yang Kao"
        ],
        "summary": "A universalization of a parameterized investment strategy is an online algorithm whose average daily performance approaches that of the strategy operating with the optimal parameters determined offline in hindsight. We present a general framework for universalizing investment strategies and discuss conditions under which investment strategies are universalizable. We present examples of common investment strategies that fit into our framework. The examples include both trading strategies that decide positions in individual stocks, and portfolio strategies that allocate wealth among multiple stocks. This work extends Cover's universal portfolio work. We also discuss the runtime efficiency of universalization algorithms. While a straightforward implementation of our algorithms runs in time exponential in the number of parameters, we show that the efficient universal portfolio computation technique of Kalai and Vempala involving the sampling of log-concave functions can be generalized to other classes of investment strategies.",
        "published": "2002-04-10T03:13:03Z",
        "link": "http://arxiv.org/abs/cs/0204019v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2; G.3; I.2.6"
        ]
    },
    {
        "title": "The Geometric Maximum Traveling Salesman Problem",
        "authors": [
            "Alexander Barvinok",
            "Sandor P. Fekete",
            "David S. Johnson",
            "Arie Tamir",
            "Gerhard J. Woeginger",
            "Russ Woodroofe"
        ],
        "summary": "We consider the traveling salesman problem when the cities are points in R^d for some fixed d and distances are computed according to geometric distances, determined by some norm. We show that for any polyhedral norm, the problem of finding a tour of maximum length can be solved in polynomial time. If arithmetic operations are assumed to take unit time, our algorithms run in time O(n^{f-2} log n), where f is the number of facets of the polyhedron determining the polyhedral norm. Thus for example we have O(n^2 log n) algorithms for the cases of points in the plane under the Rectilinear and Sup norms. This is in contrast to the fact that finding a minimum length tour in each case is NP-hard. Our approach can be extended to the more general case of quasi-norms with not necessarily symmetric unit ball, where we get a complexity of O(n^{2f-2} log n).   For the special case of two-dimensional metrics with f=4 (which includes the Rectilinear and Sup norms), we present a simple algorithm with O(n) running time. The algorithm does not use any indirect addressing, so its running time remains valid even in comparison based models in which sorting requires Omega(n \\log n) time. The basic mechanism of the algorithm provides some intuition on why polyhedral norms allow fast algorithms.   Complementing the results on simplicity for polyhedral norms, we prove that for the case of Euclidean distances in R^d for d>2, the Maximum TSP is NP-hard. This sheds new light on the well-studied difficulties of Euclidean distances.",
        "published": "2002-04-10T18:56:09Z",
        "link": "http://arxiv.org/abs/cs/0204024v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Randomized selection revisited",
        "authors": [
            "Krzysztof C. Kiwiel"
        ],
        "summary": "We show that several versions of Floyd and Rivest's algorithm Select for finding the $k$th smallest of $n$ elements require at most $n+\\min\\{k,n-k\\}+o(n)$ comparisons on average and with high probability. This rectifies the analysis of Floyd and Rivest, and extends it to the case of nondistinct elements. Our computational results confirm that Select may be the best algorithm in practice.",
        "published": "2002-04-15T04:48:21Z",
        "link": "http://arxiv.org/abs/cs/0204033v1",
        "categories": [
            "cs.DS",
            "F.2.2;G3"
        ]
    },
    {
        "title": "Optimal Aggregation Algorithms for Middleware",
        "authors": [
            "Ron Fagin",
            "Amnon Lotem",
            "Moni Naor"
        ],
        "summary": "Let D be a database of N objects where each object has m fields. The objects are given in m sorted lists (where the ith list is sorted according to the ith field). Our goal is to find the top k objects according to a monotone aggregation function t, while minimizing access to the lists. The problem arises in several contexts. In particular Fagin (JCSS 1999) considered it for the purpose of aggregating information in a multimedia database system.   We are interested in instance optimality, i.e. that our algorithm will be as good as any other (correct) algorithm on any instance. We provide and analyze several instance optimal algorithms for the task, with various access costs and models.",
        "published": "2002-04-22T18:32:44Z",
        "link": "http://arxiv.org/abs/cs/0204046v1",
        "categories": [
            "cs.DB",
            "cs.DS",
            "H.2.4; F.2.2"
        ]
    },
    {
        "title": "Phase Transition in a Random Fragmentation Problem with Applications to   Computer Science",
        "authors": [
            "David S. Dean",
            "Satya N. Majumdar"
        ],
        "summary": "We study a fragmentation problem where an initial object of size x is broken into m random pieces provided x>x_0 where x_0 is an atomic cut-off. Subsequently the fragmentation process continues for each of those daughter pieces whose sizes are bigger than x_0. The process stops when all the fragments have sizes smaller than x_0. We show that the fluctuation of the total number of splitting events, characterized by the variance, generically undergoes a nontrivial phase transition as one tunes the branching number m through a critical value m=m_c. For m<m_c, the fluctuations are Gaussian where as for m>m_c they are anomalously large and non-Gaussian. We apply this general result to analyze two different search algorithms in computer science.",
        "published": "2002-05-02T08:49:31Z",
        "link": "http://arxiv.org/abs/cond-mat/0205034v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.DS",
            "math.PR"
        ]
    },
    {
        "title": "On-Line Paging against Adversarially Biased Random Inputs",
        "authors": [
            "Neal E. Young"
        ],
        "summary": "In evaluating an algorithm, worst-case analysis can be overly pessimistic. Average-case analysis can be overly optimistic. An intermediate approach is to show that an algorithm does well on a broad class of input distributions. Koutsoupias and Papadimitriou recently analyzed the least-recently-used (LRU) paging strategy in this manner, analyzing its performance on an input sequence generated by a so-called diffuse adversary -- one that must choose each request probabilitistically so that no page is chosen with probability more than some fixed epsilon>0. They showed that LRU achieves the optimal competitive ratio (for deterministic on-line algorithms), but they didn't determine the actual ratio.   In this paper we estimate the optimal ratios within roughly a factor of two for both deterministic strategies (e.g. least-recently-used and first-in-first-out) and randomized strategies. Around the threshold epsilon ~ 1/k (where k is the cache size), the optimal ratios are both Theta(ln k). Below the threshold the ratios tend rapidly to O(1). Above the threshold the ratio is unchanged for randomized strategies but tends rapidly to Theta(k) for deterministic ones.   We also give an alternate proof of the optimality of LRU.",
        "published": "2002-05-09T04:26:12Z",
        "link": "http://arxiv.org/abs/cs/0205007v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.0; F.1.3"
        ]
    },
    {
        "title": "Improved Bicriteria Existence Theorems for Scheduling",
        "authors": [
            "Javed Aslam",
            "April Rasala",
            "Cliff Stein",
            "Neal Young"
        ],
        "summary": "Two common objectives for evaluating a schedule are the makespan, or schedule length, and the average completion time. This short note gives improved bounds on the existence of schedules that simultaneously optimize both criteria. In particular, for any rho> 0, there exists a schedule of makespan at most 1+rho times the minimum, with average completion time at most (1-e)^rho times the minimum. The proof uses an infininite-dimensional linear program to generalize and strengthen a previous analysis by Cliff Stein and Joel Wein (1997).",
        "published": "2002-05-10T00:47:20Z",
        "link": "http://arxiv.org/abs/cs/0205008v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.0; F.1.3"
        ]
    },
    {
        "title": "Approximate Data Structures with Applications",
        "authors": [
            "Yossi Matias",
            "Jeff Vitter",
            "Neal Young"
        ],
        "summary": "This paper explores the notion of approximate data structures, which return approximately correct answers to queries, but run faster than their exact counterparts. The paper describes approximate variants of the van Emde Boas data structure, which support the same dynamic operations as the standard van Emde Boas data structure (min, max, successor, predecessor, and existence queries, as well as insertion and deletion), except that answers to queries are approximate. The variants support all operations in constant time provided the performance guarantee is 1+1/polylog(n), and in O(loglogn) time provided the performance guarantee is 1+1/polynomial(n), for n elements in the data structure.   Applications described include Prim's minimum-spanning-tree algorithm, Dijkstra's single-source shortest paths algorithm, and an on-line variant of Graham's convex hull algorithm. To obtain output which approximates the desired output with the performance guarantee tending to 1, Prim's algorithm requires only linear time, Dijkstra's algorithm requires O(mloglogn) time, and the on-line variant of Graham's algorithm requires constant amortized time per operation.",
        "published": "2002-05-10T01:12:18Z",
        "link": "http://arxiv.org/abs/cs/0205010v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.0; F.1.3"
        ]
    },
    {
        "title": "Polynomial-Time Approximation Scheme for Data Broadcast",
        "authors": [
            "Claire Kenyon",
            "Nicolas Schabanel",
            "Neal Young"
        ],
        "summary": "The data broadcast problem is to find a schedule for broadcasting a given set of messages over multiple channels. The goal is to minimize the cost of the broadcast plus the expected response time to clients who periodically and probabilistically tune in to wait for particular messages.   The problem models disseminating data to clients in asymmetric communication environments, where there is a much larger capacity from the information source to the clients than in the reverse direction. Examples include satellites, cable TV, internet broadcast, and mobile phones. Such environments favor the ``push-based'' model where the server broadcasts (pushes) its information on the communication medium and multiple clients simultaneously retrieve the specific information of individual interest.   This paper presents the first polynomial-time approximation scheme (PTAS) for data broadcast with O(1) channels and when each message has arbitrary probability, unit length and bounded cost. The best previous polynomial-time approximation algorithm for this case has a performance ratio of 9/8.",
        "published": "2002-05-10T02:23:47Z",
        "link": "http://arxiv.org/abs/cs/0205012v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.0, F.1.3"
        ]
    },
    {
        "title": "On Strongly Connected Digraphs with Bounded Cycle Length",
        "authors": [
            "Samir Khuller",
            "Balaji Raghavachari",
            "Neal Young"
        ],
        "summary": "The MEG (minimum equivalent graph) problem is, given a directed graph, to find a small subset of the edges that maintains all reachability relations between nodes. The problem is NP-hard. This paper gives a proof that, for graphs where each directed cycle has at most three edges, the MEG problem is equivalent to maximum bipartite matching, and therefore solvable in polynomial time. This leads to an improvement in the performance guarantee of the previously best approximation algorithm for the general problem in ``Approximating the Minimum Equivalent Digraph'' (1995).",
        "published": "2002-05-10T02:30:47Z",
        "link": "http://arxiv.org/abs/cs/0205011v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.0; F.1.3"
        ]
    },
    {
        "title": "From Alife Agents to a Kingdom of N Queens",
        "authors": [
            "Jing Han",
            "Jiming Liu",
            "Qingsheng Cai"
        ],
        "summary": "This paper presents a new approach to solving N-queen problems, which involves a model of distributed autonomous agents with artificial life (ALife) and a method of representing N-queen constraints in an agent environment. The distributed agents locally interact with their living environment, i.e., a chessboard, and execute their reactive behaviors by applying their behavioral rules for randomized motion, least-conflict position searching, and cooperating with other agents etc. The agent-based N-queen problem solving system evolves through selection and contest according to the rule of Survival of the Fittest, in which some agents will die or be eaten if their moving strategies are less efficient than others. The experimental results have shown that this system is capable of solving large-scale N-queen problems. This paper also provides a model of ALife agents for solving general CSPs.",
        "published": "2002-05-13T10:49:48Z",
        "link": "http://arxiv.org/abs/cs/0205016v1",
        "categories": [
            "cs.AI",
            "cs.DS",
            "cs.MA",
            "I.2.8;I.2.11;G.2.1"
        ]
    },
    {
        "title": "A (non)static 0-order statistical model and its implementation for   compressing virtually uncompressible data",
        "authors": [
            "Evgueniy Vitchev"
        ],
        "summary": "We give an implementation of a statistical model, which can be successfully applied for compressing of a sequence of binary digits with behavior close to random.",
        "published": "2002-05-15T23:27:26Z",
        "link": "http://arxiv.org/abs/cs/0205024v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "E.4; G.3; H.1.1"
        ]
    },
    {
        "title": "Exact Solution of a Drop-push Model for Percolation",
        "authors": [
            "Satya N. Majumdar",
            "David S. Dean"
        ],
        "summary": "Motivated by a computer science algorithm known as `linear probing with hashing' we study a new type of percolation model whose basic features include a sequential `dropping' of particles on a substrate followed by their transport via a `pushing' mechanism. Our exact solution in one dimension shows that, unlike the ordinary random percolation model, the drop-push model has nontrivial spatial correlations generated by the dynamics itself. The critical exponents in the drop-push model are also different from that of the ordinary percolation. The relevance of our results to computer science is pointed out.",
        "published": "2002-05-16T06:40:58Z",
        "link": "http://arxiv.org/abs/cond-mat/0205336v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.DS",
            "math.PR"
        ]
    },
    {
        "title": "A Codebook Generation Algorithm for Document Image Compression",
        "authors": [
            "Qin Zhang",
            "John Danskin",
            "Neal Young"
        ],
        "summary": "Pattern-matching-based document-compression systems (e.g. for faxing) rely on finding a small set of patterns that can be used to represent all of the ink in the document. Finding an optimal set of patterns is NP-hard; previous compression schemes have resorted to heuristics. This paper describes an extension of the cross-entropy approach, used previously for measuring pattern similarity, to this problem. This approach reduces the problem to a k-medians problem, for which the paper gives a new algorithm with a provably good performance guarantee. In comparison to previous heuristics (First Fit, with and without generalized Lloyd's/k-means postprocessing steps), the new algorithm generates a better codebook, resulting in an overall improvement in compression performance of almost 17%.",
        "published": "2002-05-17T23:52:11Z",
        "link": "http://arxiv.org/abs/cs/0205029v1",
        "categories": [
            "cs.DS",
            "F.2.0, E.4, I.4.2"
        ]
    },
    {
        "title": "Approximation Algorithms for Covering/Packing Integer Programs",
        "authors": [
            "Stavros G. Kolliopoulos",
            "Neal E. Young"
        ],
        "summary": "Given matrices A and B and vectors a, b, c and d, all with non-negative entries, we consider the problem of computing min {c.x: x in Z^n_+, Ax > a, Bx < b, x < d}. We give a bicriteria-approximation algorithm that, given epsilon in (0, 1], finds a solution of cost O(ln(m)/epsilon^2) times optimal, meeting the covering constraints (Ax > a) and multiplicity constraints (x < d), and satisfying Bx < (1 + epsilon)b + beta, where beta is the vector of row sums beta_i = sum_j B_ij. Here m denotes the number of rows of A.   This gives an O(ln m)-approximation algorithm for CIP -- minimum-cost covering integer programs with multiplicity constraints, i.e., the special case when there are no packing constraints Bx < b. The previous best approximation ratio has been O(ln(max_j sum_i A_ij)) since 1982. CIP contains the set cover problem as a special case, so O(ln m)-approximation is the best possible unless P=NP.",
        "published": "2002-05-18T00:23:17Z",
        "link": "http://arxiv.org/abs/cs/0205030v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.1.6; G.2.1"
        ]
    },
    {
        "title": "On-Line End-to-End Congestion Control",
        "authors": [
            "Naveen Garg",
            "Neal E. Young"
        ],
        "summary": "Congestion control in the current Internet is accomplished mainly by TCP/IP. To understand the macroscopic network behavior that results from TCP/IP and similar end-to-end protocols, one main analytic technique is to show that the the protocol maximizes some global objective function of the network traffic. Here we analyze a particular end-to-end, MIMD (multiplicative-increase, multiplicative-decrease) protocol. We show that if all users of the network use the protocol, and all connections last for at least logarithmically many rounds, then the total weighted throughput (value of all packets received) is near the maximum possible. Our analysis includes round-trip-times, and (in contrast to most previous analyses) gives explicit convergence rates, allows connections to start and stop, and allows capacities to change.",
        "published": "2002-05-18T01:54:56Z",
        "link": "http://arxiv.org/abs/cs/0205032v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.NI",
            "F.1.2, F.2.0; C.2.6; G.1.6"
        ]
    },
    {
        "title": "On-Line File Caching",
        "authors": [
            "Neal E. Young"
        ],
        "summary": "In the on-line file-caching problem problem, the input is a sequence of requests for files, given on-line (one at a time). Each file has a non-negative size and a non-negative retrieval cost. The problem is to decide which files to keep in a fixed-size cache so as to minimize the sum of the retrieval costs for files that are not in the cache when requested. The problem arises in web caching by browsers and by proxies. This paper describes a natural generalization of LRU called Landlord and gives an analysis showing that it has an optimal performance guarantee (among deterministic on-line algorithms).   The paper also gives an analysis of the algorithm in a so-called ``loosely'' competitive model, showing that on a ``typical'' cache size, either the performance guarantee is O(1) or the total retrieval cost is insignificant.",
        "published": "2002-05-18T02:04:59Z",
        "link": "http://arxiv.org/abs/cs/0205033v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.NI",
            "F.1.2, F.2.0, C.2.0"
        ]
    },
    {
        "title": "Data-Collection for the Sloan Digital Sky Survey: a Network-Flow   Heuristic",
        "authors": [
            "Robert Lupton",
            "Miller Maley",
            "Neal Young"
        ],
        "summary": "The goal of the Sloan Digital Sky Survey is ``to map in detail one-quarter of the entire sky, determining the positions and absolute brightnesses of more than 100 million celestial objects''. The survey will be performed by taking ``snapshots'' through a large telescope. Each snapshot can capture up to 600 objects from a small circle of the sky. This paper describes the design and implementation of the algorithm that is being used to determine the snapshots so as to minimize their number. The problem is NP-hard in general; the algorithm described is a heuristic, based on Lagriangian-relaxation and min-cost network flow. It gets within 5-15% of a naive lower bound, whereas using a ``uniform'' cover only gets within 25-35%.",
        "published": "2002-05-18T03:29:33Z",
        "link": "http://arxiv.org/abs/cs/0205034v1",
        "categories": [
            "cs.DS",
            "cs.CE",
            "F.2.1; G.1.6; J.2"
        ]
    },
    {
        "title": "Randomized Rounding without Solving the Linear Program",
        "authors": [
            "Neal E. Young"
        ],
        "summary": "Randomized rounding is a standard method, based on the probabilistic method, for designing combinatorial approximation algorithms. In Raghavan's seminal paper introducing the method (1988), he writes: \"The time taken to solve the linear program relaxations of the integer programs dominates the net running time theoretically (and, most likely, in practice as well).\"   This paper explores how this bottleneck can be avoided for randomized rounding algorithms for packing and covering problems (linear programs, or mixed integer linear programs, having no negative coefficients). The resulting algorithms are greedy algorithms, and are faster and simpler to implement than standard randomized-rounding algorithms.   This approach can also be used to understand Lagrangian-relaxation algorithms for packing/covering linear programs: such algorithms can be viewed as as (derandomized) randomized-rounding schemes.",
        "published": "2002-05-18T04:05:19Z",
        "link": "http://arxiv.org/abs/cs/0205036v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.1.3; G.1.6; G.2.1"
        ]
    },
    {
        "title": "A Primal-Dual Parallel Approximation Technique Applied to Weighted Set   and Vertex Cover",
        "authors": [
            "Samir Khuller",
            "Uzi Vishkin",
            "Neal Young"
        ],
        "summary": "The paper describes a simple deterministic parallel/distributed (2+epsilon)-approximation algorithm for the minimum-weight vertex-cover problem and its dual (edge/element packing).",
        "published": "2002-05-18T04:22:29Z",
        "link": "http://arxiv.org/abs/cs/0205037v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.0; G.1.6; C.2.4"
        ]
    },
    {
        "title": "Competitive Paging Algorithms",
        "authors": [
            "Amos Fiat",
            "Richard Karp",
            "Mike Luby",
            "Lyle McGeoch",
            "Daniel Sleator",
            "Neal E. Young"
        ],
        "summary": "The paging problem is that of deciding which pages to keep in a memory of k pages in order to minimize the number of page faults. This paper introduces the marking algorithm, a simple randomized on-line algorithm for the paging problem, and gives a proof that its performance guarantee (competitive ratio) is O(log k). In contrast, no deterministic on-line algorithm can have a performance guarantee better than k.",
        "published": "2002-05-18T13:49:31Z",
        "link": "http://arxiv.org/abs/cs/0205038v1",
        "categories": [
            "cs.DS",
            "cs.NI",
            "F.2.0; F.1.2; C.0"
        ]
    },
    {
        "title": "Sequential and Parallel Algorithms for Mixed Packing and Covering",
        "authors": [
            "Neal E. Young"
        ],
        "summary": "Mixed packing and covering problems are problems that can be formulated as linear programs using only non-negative coefficients. Examples include multicommodity network flow, the Held-Karp lower bound on TSP, fractional relaxations of set cover, bin-packing, knapsack, scheduling problems, minimum-weight triangulation, etc. This paper gives approximation algorithms for the general class of problems. The sequential algorithm is a simple greedy algorithm that can be implemented to find an epsilon-approximate solution in O(epsilon^-2 log m) linear-time iterations. The parallel algorithm does comparable work but finishes in polylogarithmic time.   The results generalize previous work on pure packing and covering (the special case when the constraints are all \"less-than\" or all \"greater-than\") by Michael Luby and Noam Nisan (1993) and Naveen Garg and Jochen Konemann (1998).",
        "published": "2002-05-18T15:12:41Z",
        "link": "http://arxiv.org/abs/cs/0205039v1",
        "categories": [
            "cs.DS",
            "F.2.1, G.1.6"
        ]
    },
    {
        "title": "Faster Parametric Shortest Path and Minimum Balance Algorithms",
        "authors": [
            "Neal Young",
            "Robert Tarjan",
            "James Orlin"
        ],
        "summary": "The parametric shortest path problem is to find the shortest paths in graph where the edge costs are of the form w_ij+lambda where each w_ij is constant and lambda is a parameter that varies. The problem is to find shortest path trees for every possible value of lambda.   The minimum-balance problem is to find a ``weighting'' of the vertices so that adjusting the edge costs by the vertex weights yields a graph in which, for every cut, the minimum weight of any edge crossing the cut in one direction equals the minimum weight of any edge crossing the cut in the other direction.   The paper presents fast algorithms for both problems. The algorithms run in O(nm+n^2 log n) time. The paper also describes empirical studies of the algorithms on random graphs, suggesting that the expected time for finding a minimum-mean cycle (an important special case of both problems) is O(n log(n) + m).",
        "published": "2002-05-18T15:48:56Z",
        "link": "http://arxiv.org/abs/cs/0205041v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2; G.1.6"
        ]
    },
    {
        "title": "Approximating the Minimum Equivalent Digraph",
        "authors": [
            "Samir Khuller",
            "Balaji Raghavachari",
            "Neal E. Young"
        ],
        "summary": "The MEG (minimum equivalent graph) problem is, given a directed graph, to find a small subset of the edges that maintains all reachability relations between nodes. The problem is NP-hard. This paper gives an approximation algorithm with performance guarantee of pi^2/6 ~ 1.64. The algorithm and its analysis are based on the simple idea of contracting long cycles. (This result is strengthened slightly in ``On strongly connected digraphs with bounded cycle length'' (1996).) The analysis applies directly to 2-Exchange, a simple ``local improvement'' algorithm, showing that its performance guarantee is 1.75.",
        "published": "2002-05-18T15:59:14Z",
        "link": "http://arxiv.org/abs/cs/0205040v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Orienting Graphs to Optimize Reachability",
        "authors": [
            "S. L. Hakimi",
            "E. Schmeichel",
            "Neal E. Young"
        ],
        "summary": "The paper focuses on two problems: (i) how to orient the edges of an undirected graph in order to maximize the number of ordered vertex pairs (x,y) such that there is a directed path from x to y, and (ii) how to orient the edges so as to minimize the number of such pairs. The paper describes a quadratic-time algorithm for the first problem, and a proof that the second problem is NP-hard to approximate within some constant 1+epsilon > 1. The latter proof also shows that the second problem is equivalent to ``comparability graph completion''; neither problem was previously known to be NP-hard.",
        "published": "2002-05-18T16:12:53Z",
        "link": "http://arxiv.org/abs/cs/0205042v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Low-Degree Spanning Trees of Small Weight",
        "authors": [
            "Samir Khuller",
            "Balaji Raghavachari",
            "Neal E. Young"
        ],
        "summary": "The degree-d spanning tree problem asks for a minimum-weight spanning tree in which the degree of each vertex is at most d. When d=2 the problem is TSP, and in this case, the well-known Christofides algorithm provides a 1.5-approximation algorithm (assuming the edge weights satisfy the triangle inequality).   In 1984, Christos Papadimitriou and Umesh Vazirani posed the challenge of finding an algorithm with performance guarantee less than 2 for Euclidean graphs (points in R^n) and d > 2. This paper gives the first answer to that challenge, presenting an algorithm to compute a degree-3 spanning tree of cost at most 5/3 times the MST. For points in the plane, the ratio improves to 3/2 and the algorithm can also find a degree-4 spanning tree of cost at most 5/4 times the MST.",
        "published": "2002-05-18T16:37:28Z",
        "link": "http://arxiv.org/abs/cs/0205043v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Balancing Minimum Spanning and Shortest Path Trees",
        "authors": [
            "Samir Khuller",
            "Balaji Raghavachari",
            "Neal E. Young"
        ],
        "summary": "This paper give a simple linear-time algorithm that, given a weighted digraph, finds a spanning tree that simultaneously approximates a shortest-path tree and a minimum spanning tree. The algorithm provides a continuous trade-off: given the two trees and epsilon > 0, the algorithm returns a spanning tree in which the distance between any vertex and the root of the shortest-path tree is at most 1+epsilon times the shortest-path distance, and yet the total weight of the tree is at most 1+2/epsilon times the weight of a minimum spanning tree. This is the best tradeoff possible. The paper also describes a fast parallel implementation.",
        "published": "2002-05-18T18:08:46Z",
        "link": "http://arxiv.org/abs/cs/0205045v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "The K-Server Dual and Loose Competitiveness for Paging",
        "authors": [
            "Neal E. Young"
        ],
        "summary": "This paper has two results. The first is based on the surprising observation that the well-known ``least-recently-used'' paging algorithm and the ``balance'' algorithm for weighted caching are linear-programming primal-dual algorithms. This observation leads to a strategy (called ``Greedy-Dual'') that generalizes them both and has an optimal performance guarantee for weighted caching.   For the second result, the paper presents empirical studies of paging algorithms, documenting that in practice, on ``typical'' cache sizes and sequences, the performance of paging strategies are much better than their worst-case analyses in the standard model suggest. The paper then presents theoretical results that support and explain this. For example: on any input sequence, with almost all cache sizes, either the performance guarantee of least-recently-used is O(log k) or the fault rate (in an absolute sense) is insignificant.   Both of these results are strengthened and generalized in``On-line File Caching'' (1998).",
        "published": "2002-05-18T18:16:13Z",
        "link": "http://arxiv.org/abs/cs/0205044v1",
        "categories": [
            "cs.DS",
            "cs.NI",
            "F.1.2; F.2.0; C.2.0"
        ]
    },
    {
        "title": "K-Medians, Facility Location, and the Chernoff-Wald Bound",
        "authors": [
            "Neal E. Young"
        ],
        "summary": "The paper gives approximation algorithms for the k-medians and facility-location problems (both NP-hard). For k-medians, the algorithm returns a solution using at most ln(n+n/epsilon)k medians and having cost at most (1+epsilon) times the cost of the best solution that uses at most k medians. Here epsilon > 0 is an input to the algorithm. In comparison, the best previous algorithm (Jyh-Han Lin and Jeff Vitter, 1992) had a (1+1/epsilon)ln(n) term instead of the ln(n+n/epsilon) term in the performance guarantee. For facility location, the algorithm returns a solution of cost at most d+ln(n) k, provided there exists a solution of cost d+k where d is the assignment cost and k is the facility cost. In comparison, the best previous algorithm (Dorit Hochbaum, 1982) returned a solution of cost at most ln(n)(d+k). For both problems, the algorithms currently provide the best performance guarantee known for the general (non-metric) problems.   The paper also introduces a new probabilistic bound (called \"Chernoff-Wald bound\") for bounding the expectation of the maximum of a collection of sums of random variables, when each sum contains a random number of terms. The bound is used to analyze the randomized rounding scheme that underlies the algorithms.",
        "published": "2002-05-18T18:53:27Z",
        "link": "http://arxiv.org/abs/cs/0205047v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.1; G.1.6; G.2.2; G.3"
        ]
    },
    {
        "title": "Huffman Coding with Letter Costs: A Linear-Time Approximation Scheme",
        "authors": [
            "Mordecai Golin",
            "Claire Mathieu",
            "Neal E. Young"
        ],
        "summary": "We give a polynomial-time approximation scheme for the generalization of Huffman Coding in which codeword letters have non-uniform costs (as in Morse code, where the dash is twice as long as the dot). The algorithm computes a (1+epsilon)-approximate solution in time O(n + f(epsilon) log^3 n), where n is the input size.",
        "published": "2002-05-18T18:57:04Z",
        "link": "http://arxiv.org/abs/cs/0205048v2",
        "categories": [
            "cs.DS",
            "F.2.0; E.4; I.4.2"
        ]
    },
    {
        "title": "Prefix Codes: Equiprobable Words, Unequal Letter Costs",
        "authors": [
            "Mordecai Golin",
            "Neal E. Young"
        ],
        "summary": "Describes a near-linear-time algorithm for a variant of Huffman coding, in which the letters may have non-uniform lengths (as in Morse code), but with the restriction that each word to be encoded has equal probability. [See also ``Huffman Coding with Unequal Letter Costs'' (2002).]",
        "published": "2002-05-18T19:05:55Z",
        "link": "http://arxiv.org/abs/cs/0205049v1",
        "categories": [
            "cs.DS",
            "F.2.0; E.4; I.4.2"
        ]
    },
    {
        "title": "A Network-Flow Technique for Finding Low-Weight Bounded-Degree Spanning   Trees",
        "authors": [
            "S. Fekete",
            "S. Khuller",
            "M. Klemmstein",
            "B. Raghavachari",
            "Neal E. Young"
        ],
        "summary": "The problem considered is the following. Given a graph with edge weights satisfying the triangle inequality, and a degree bound for each vertex, compute a low-weight spanning tree such that the degree of each vertex is at most its specified bound. The problem is NP-hard (it generalizes Traveling Salesman (TSP)). This paper describes a network-flow heuristic for modifying a given tree T to meet the constraints. Choosing T to be a minimum spanning tree (MST) yields approximation algorithms with performance guarantee less than 2 for the problem on geometric graphs with L_p-norms. The paper also describes a Euclidean graph whose minimum TSP costs twice the MST, disproving a conjecture made in ``Low-Degree Spanning Trees of Small Weight'' (1996).",
        "published": "2002-05-18T19:25:25Z",
        "link": "http://arxiv.org/abs/cs/0205050v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Rounding Algorithms for a Geometric Embedding of Minimum Multiway Cut",
        "authors": [
            "David Karger",
            "Phil Klein",
            "Cliff Stein",
            "Mikkel Thorup",
            "Neal E. Young"
        ],
        "summary": "The multiway-cut problem is, given a weighted graph and k >= 2 terminal nodes, to find a minimum-weight set of edges whose removal separates all the terminals. The problem is NP-hard, and even NP-hard to approximate within 1+delta for some small delta > 0.   Calinescu, Karloff, and Rabani (1998) gave an algorithm with performance guarantee 3/2-1/k, based on a geometric relaxation of the problem. In this paper, we give improved randomized rounding schemes for their relaxation, yielding a 12/11-approximation algorithm for k=3 and a 1.3438-approximation algorithm in general.   Our approach hinges on the observation that the problem of designing a randomized rounding scheme for a geometric relaxation is itself a linear programming problem. The paper explores computational solutions to this problem, and gives a proof that for a general class of geometric relaxations, there are always randomized rounding schemes that match the integrality gap.",
        "published": "2002-05-19T14:40:18Z",
        "link": "http://arxiv.org/abs/cs/0205051v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.0; G.1.6; G.2.2"
        ]
    },
    {
        "title": "On the Number of Iterations for Dantzig-Wolfe Optimization and   Packing-Covering Approximation Algorithms",
        "authors": [
            "Phil Klein",
            "Neal E. Young"
        ],
        "summary": "We give a lower bound on the iteration complexity of a natural class of Lagrangean-relaxation algorithms for approximately solving packing/covering linear programs. We show that, given an input with $m$ random 0/1-constraints on $n$ variables, with high probability, any such algorithm requires $\\Omega(\\rho \\log(m)/\\epsilon^2)$ iterations to compute a $(1+\\epsilon)$-approximate solution, where $\\rho$ is the width of the input. The bound is tight for a range of the parameters $(m,n,\\rho,\\epsilon)$.   The algorithms in the class include Dantzig-Wolfe decomposition, Benders' decomposition, Lagrangean relaxation as developed by Held and Karp [1971] for lower-bounding TSP, and many others (e.g. by Plotkin, Shmoys, and Tardos [1988] and Grigoriadis and Khachiyan [1996]). To prove the bound, we use a discrepancy argument to show an analogous lower bound on the support size of $(1+\\epsilon)$-approximate mixed strategies for random two-player zero-sum 0/1-matrix games.",
        "published": "2002-05-19T14:43:19Z",
        "link": "http://arxiv.org/abs/cs/0205046v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.1; G.1.6"
        ]
    },
    {
        "title": "Aging, double helix and small world property in genetic algorithms",
        "authors": [
            "Marek W. Gutowski"
        ],
        "summary": "Over a quarter of century after the invention of genetic algorithms and miriads of their modifications, as well as successful implementations, we are still lacking many essential details of thorough analysis of it's inner working. One of such fundamental questions is: how many generations do we need to solve the optimization problem? This paper tries to answer this question, albeit in a fuzzy way, making use of the double helix concept. As a byproduct we gain better understanding of the ways, in which the genetic algorithm may be fine tuned.",
        "published": "2002-05-23T15:34:57Z",
        "link": "http://arxiv.org/abs/cs/0205061v1",
        "categories": [
            "cs.NE",
            "cs.DS",
            "physics.data-an",
            "F.2.1; G.1.6; I.1.2"
        ]
    },
    {
        "title": "Designing Multi-Commodity Flow Trees",
        "authors": [
            "Samir Khuller",
            "Balaji Raghavachari",
            "Neal E. Young"
        ],
        "summary": "The traditional multi-commodity flow problem assumes a given flow network in which multiple commodities are to be maximally routed in response to given demands. This paper considers the multi-commodity flow network-design problem: given a set of multi-commodity flow demands, find a network subject to certain constraints such that the commodities can be maximally routed.   This paper focuses on the case when the network is required to be a tree. The main result is an approximation algorithm for the case when the tree is required to be of constant degree. The algorithm reduces the problem to the minimum-weight balanced-separator problem; the performance guarantee of the algorithm is within a factor of 4 of the performance guarantee of the balanced-separator procedure. If Leighton and Rao's balanced-separator procedure is used, the performance guarantee is O(log n). This improves the O(log^2 n) approximation factor that is trivial to obtain by a direct application of the balanced-separator method.",
        "published": "2002-05-30T12:34:24Z",
        "link": "http://arxiv.org/abs/cs/0205077v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Implementation and complexity of the watershed-from-markers algorithm   computed as a minimal cost forest",
        "authors": [
            "Petr Felkel",
            "Mario Bruckwschwaiger",
            "Rainer Wegenkittl"
        ],
        "summary": "The watershed algorithm belongs to classical algorithms in mathematical morphology. Lotufo et al. published a principle of the watershed computation by means of an Image Foresting Transform (IFT), which computes a shortest path forest from given markers. The algorithm itself was described for a 2D case (image) without a detailed discussion of its computation and memory demands for real datasets. As IFT cleverly solves the problem of plateaus and as it gives precise results when thin objects have to be segmented, it is obvious to use this algorithm for 3D datasets taking in mind the minimizing of a higher memory consumption for the 3D case without loosing low asymptotical time complexity of O(m+C) (and also the real computation speed). The main goal of this paper is an implementation of the IFT algorithm with a priority queue with buckets and careful tuning of this implementation to reach as minimal memory consumption as possible.   The paper presents five possible modifications and methods of implementation of the IFT algorithm. All presented implementations keep the time complexity of the standard priority queue with buckets but the best one minimizes the costly memory allocation and needs only 19-45% of memory for typical 3D medical imaging datasets. Memory saving was reached by an IFT algorithm simplification, which stores more elements in temporary structures but these elements are simpler and thus need less memory.   The best presented modification allows segmentation of large 3D medical datasets (up to 512x512x680 voxels) with 12-or 16-bits per voxel on currently available PC based workstations.",
        "published": "2002-06-04T13:08:24Z",
        "link": "http://arxiv.org/abs/cs/0206009v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "I.3.5 [Computational Geometry and Object Modeling]"
        ]
    },
    {
        "title": "Fast Deterministic Consensus in a Noisy Environment",
        "authors": [
            "James Aspnes"
        ],
        "summary": "It is well known that the consensus problem cannot be solved deterministically in an asynchronous environment, but that randomized solutions are possible. We propose a new model, called noisy scheduling, in which an adversarial schedule is perturbed randomly, and show that in this model randomness in the environment can substitute for randomness in the algorithm. In particular, we show that a simplified, deterministic version of Chandra's wait-free shared-memory consensus algorithm (PODC, 1996, pp. 166-175) solves consensus in time at most logarithmic in the number of active processes. The proof of termination is based on showing that a race between independent delayed renewal processes produces a winner quickly. In addition, we show that the protocol finishes in constant time using quantum and priority-based scheduling on a uniprocessor, suggesting that it is robust against the choice of model over a wide range.",
        "published": "2002-06-07T18:50:04Z",
        "link": "http://arxiv.org/abs/cs/0206012v2",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2"
        ]
    },
    {
        "title": "Algorithms for Media",
        "authors": [
            "David Eppstein",
            "Jean-Claude Falmagne"
        ],
        "summary": "Falmagne recently introduced the concept of a medium, a combinatorial object encompassing hyperplane arrangements, topological orderings, acyclic orientations, and many other familiar structures. We find efficient solutions for several algorithmic problems on media: finding short reset sequences, shortest paths, testing whether a medium has a closed orientation, and listing the states of a medium given a black-box description.",
        "published": "2002-06-24T06:50:52Z",
        "link": "http://arxiv.org/abs/cs/0206033v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Optimally cutting a surface into a disk",
        "authors": [
            "Jeff Erickson",
            "Sariel Har-Peled"
        ],
        "summary": "We consider the problem of cutting a set of edges on a polyhedral manifold surface, possibly with boundary, to obtain a single topological disk, minimizing either the total number of cut edges or their total length. We show that this problem is NP-hard, even for manifolds without boundary and for punctured spheres. We also describe an algorithm with running time n^{O(g+k)}, where n is the combinatorial complexity, g is the genus, and k is the number of boundary components of the input surface. Finally, we describe a greedy algorithm that outputs a O(log^2 g)-approximation of the minimum cut graph in O(g^2 n log n) time.",
        "published": "2002-07-02T22:03:51Z",
        "link": "http://arxiv.org/abs/cs/0207004v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.GR",
            "F.2.2; I.3.5; G.2.m"
        ]
    },
    {
        "title": "Computing Elementary Symmetric Polynomials with a Sublinear Number of   Multiplications",
        "authors": [
            "Vince Grolmusz"
        ],
        "summary": "Elementary symmetric polynomials $S_n^k$ are used as a benchmark for the bounded-depth arithmetic circuit model of computation. In this work we prove that $S_n^k$ modulo composite numbers $m=p_1p_2$ can be computed with much fewer multiplications than over any field, if the coefficients of monomials $x_{i_1}x_{i_2}... x_{i_k}$ are allowed to be 1 either mod $p_1$ or mod $p_2$ but not necessarily both. More exactly, we prove that for any constant $k$ such a representation of $S_n^k$ can be computed modulo $p_1p_2$ using only $\\exp(O(\\sqrt{\\log n}\\log\\log n))$ multiplications on the most restricted depth-3 arithmetic circuits, for $\\min({p_1,p_2})>k!$. Moreover, the number of multiplications remain sublinear while $k=O(\\log\\log n).$ In contrast, the well-known Graham-Pollack bound yields an $n-1$ lower bound for the number of multiplications even for the exact computation (not the representation) of $S_n^2$. Our results generalize for other non-prime power composite moduli as well. The proof uses the famous BBR-polynomial of Barrington, Beigel and Rudich.",
        "published": "2002-07-03T14:32:21Z",
        "link": "http://arxiv.org/abs/cs/0207009v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "F1.3, F2.1"
        ]
    },
    {
        "title": "A Compact Graph Model of Handwritten Images: Integration into   Authentification and Recognition",
        "authors": [
            "Denis V. Popel"
        ],
        "summary": "A novel algorithm for creating a mathematical model of curved shapes is introduced. The core of the algorithm is based on building a graph representation of the contoured image, which occupies less storage space than produced by raster compression techniques. Different advanced applications of the mathematical model are discussed: recognition of handwritten characters and verification of handwritten text and signatures for authentification purposes. Reducing the storage requirements due to the efficient mathematical model results in faster retrieval and processing times. The experimental outcomes in compression of contoured images and recognition of handwritten numerals are given.",
        "published": "2002-07-04T16:02:41Z",
        "link": "http://arxiv.org/abs/cs/0207013v1",
        "categories": [
            "cs.HC",
            "cs.DS",
            "G.2.2"
        ]
    },
    {
        "title": "On Concise Encodings of Preferred Extensions",
        "authors": [
            "Paul E. Dunne"
        ],
        "summary": "Much work on argument systems has focussed on preferred extensions which define the maximal collectively defensible subsets. Identification and enumeration of these subsets is (under the usual assumptions) computationally demanding. We consider approaches to deciding if a subset S is a preferred extension which query a representations encoding all such extensions, so that the computational effort is invested once only (for the initial enumeration) rather than for each separate query.",
        "published": "2002-07-08T13:24:07Z",
        "link": "http://arxiv.org/abs/cs/0207024v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "F.2.m; I.2.3; I.2.m"
        ]
    },
    {
        "title": "Linear-Time Algorithms for Computing Maximum-Density Sequence Segments   with Bioinformatics Applications",
        "authors": [
            "Michael H. Goldwasser",
            "Ming-Yang Kao",
            "Hsueh-I Lu"
        ],
        "summary": "We study an abstract optimization problem arising from biomolecular sequence analysis. For a sequence A of pairs (a_i,w_i) for i = 1,..,n and w_i>0, a segment A(i,j) is a consecutive subsequence of A starting with index i and ending with index j. The width of A(i,j) is w(i,j) = sum_{i <= k <= j} w_k, and the density is (sum_{i<= k <= j} a_k)/ w(i,j). The maximum-density segment problem takes A and two values L and U as input and asks for a segment of A with the largest possible density among those of width at least L and at most U. When U is unbounded, we provide a relatively simple, O(n)-time algorithm, improving upon the O(n \\log L)-time algorithm by Lin, Jiang and Chao. When both L and U are specified, there are no previous nontrivial results. We solve the problem in O(n) time if w_i=1 for all i, and more generally in O(n+n\\log(U-L+1)) time when w_i>=1 for all i.",
        "published": "2002-07-08T14:45:49Z",
        "link": "http://arxiv.org/abs/cs/0207026v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; J.3; G.2"
        ]
    },
    {
        "title": "Greedy Facility Location Algorithms Analyzed using Dual Fitting with   Factor-Revealing LP",
        "authors": [
            "Kamal Jain",
            "Mohammad Mahdian",
            "Evangelos Markakis",
            "Amin Saberi",
            "Vijay V. Vazirani"
        ],
        "summary": "In this paper, we will formalize the method of dual fitting and the idea of factor-revealing LP. This combination is used to design and analyze two greedy algorithms for the metric uncapacitated facility location problem. Their approximation factors are 1.861 and 1.61, with running times of O(mlog m) and O(n^3), respectively, where n is the total number of vertices and m is the number of edges in the underlying complete bipartite graph between cities and facilities. The algorithms are used to improve recent results for several variants of the problem.",
        "published": "2002-07-09T00:29:12Z",
        "link": "http://arxiv.org/abs/cs/0207028v1",
        "categories": [
            "cs.DS",
            "cs.GT",
            "F.2.2;G.2.1;G.2.2"
        ]
    },
    {
        "title": "The Shannon-McMillan Theorem for Ergodic Quantum Lattice Systems",
        "authors": [
            "Igor Bjelakovic",
            "Tyll Krueger",
            "Rainer Siegmund-Schultze",
            "Arleta Szkola"
        ],
        "summary": "We formulate and prove a quantum Shannon-McMillan theorem. The theorem demonstrates the significance of the von Neumann entropy for translation invariant ergodic quantum spin systems on n-dimensional lattices: the entropy gives the logarithm of the essential number of eigenvectors of the system on large boxes. The one-dimensional case covers quantum information sources and is basic for coding theorems.",
        "published": "2002-07-15T14:50:22Z",
        "link": "http://arxiv.org/abs/math/0207121v3",
        "categories": [
            "math.DS",
            "cs.DS",
            "cs.IT",
            "math-ph",
            "math.IT",
            "math.MP",
            "math.OA",
            "quant-ph",
            "46L89; 68P30; 81Qxx"
        ]
    },
    {
        "title": "Linear-Time Pointer-Machine Algorithms for Path-Evaluation Problems on   Trees and Graphs",
        "authors": [
            "Adam L. Buchsbaum",
            "Loukas Georgiadis",
            "Haim Kaplan",
            "Anne Rogers",
            "Robert E. Tarjan",
            "Jeffery R. Westbrook"
        ],
        "summary": "We present algorithms that run in linear time on pointer machines for a collection of problems, each of which either directly or indirectly requires the evaluation of a function defined on paths in a tree. These problems previously had linear-time algorithms but only for random-access machines (RAMs); the best pointer-machine algorithms were super-linear by an inverse-Ackermann-function factor. Our algorithms are also simpler, in some cases substantially, than the previous linear-time RAM algorithms. Our improvements come primarily from three new ideas: a refined analysis of path compression that gives a linear bound if the compressions favor certain nodes, a pointer-based radix sort as a replacement for table-based methods, and a more careful partitioning of a tree into easily managed parts. Our algorithms compute nearest common ancestors off-line, verify and construct minimum spanning trees, do interval analysis on a flowgraph, find the dominators of a flowgraph, and build the component tree of a weighted tree.",
        "published": "2002-07-15T18:47:57Z",
        "link": "http://arxiv.org/abs/cs/0207061v2",
        "categories": [
            "cs.DS",
            "D.3.4; E.1; F.1.1; F.2.2; G.2.2"
        ]
    },
    {
        "title": "Polynomial Time Data Reduction for Dominating Set",
        "authors": [
            "Jochen Alber",
            "Michael R. Fellows",
            "Rolf Niedermeier"
        ],
        "summary": "Dealing with the NP-complete Dominating Set problem on undirected graphs, we demonstrate the power of data reduction by preprocessing from a theoretical as well as a practical side. In particular, we prove that Dominating Set restricted to planar graphs has a so-called problem kernel of linear size, achieved by two simple and easy to implement reduction rules. Moreover, having implemented our reduction rules, first experiments indicate the impressive practical potential of these rules. Thus, this work seems to open up a new and prospective way how to cope with one of the most important problems in graph theory and combinatorial optimization.",
        "published": "2002-07-16T17:58:48Z",
        "link": "http://arxiv.org/abs/cs/0207066v1",
        "categories": [
            "cs.DS",
            "F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Libra: An Economy driven Job Scheduling System for Clusters",
        "authors": [
            "Jahanzeb Sherwani",
            "Nosheen Ali",
            "Nausheen Lotia",
            "Zahra Hayat",
            "Rajkumar Buyya"
        ],
        "summary": "Clusters of computers have emerged as mainstream parallel and distributed platforms for high-performance, high-throughput and high-availability computing. To enable effective resource management on clusters, numerous cluster managements systems and schedulers have been designed. However, their focus has essentially been on maximizing CPU performance, but not on improving the value of utility delivered to the user and quality of services. This paper presents a new computational economy driven scheduling system called Libra, which has been designed to support allocation of resources based on the users? quality of service (QoS) requirements. It is intended to work as an add-on to the existing queuing and resource management system. The first version has been implemented as a plugin scheduler to the PBS (Portable Batch System) system. The scheduler offers market-based economy driven service for managing batch jobs on clusters by scheduling CPU time according to user utility as determined by their budget and deadline rather than system performance considerations. The Libra scheduler ensures that both these constraints are met within an O(n) run-time. The Libra scheduler has been simulated using the GridSim toolkit to carry out a detailed performance analysis. Results show that the deadline and budget based proportional resource allocation strategy improves the utility of the system and user satisfaction as compared to system-centric scheduling strategies.",
        "published": "2002-07-22T11:35:33Z",
        "link": "http://arxiv.org/abs/cs/0207077v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "* C.1.4"
        ]
    },
    {
        "title": "Randomized Approximation Schemes for Cuts and Flows in Capacitated   Graphs",
        "authors": [
            "Andras Benczur",
            "David R. Karger"
        ],
        "summary": "We improve on random sampling techniques for approximately solving problems that involve cuts and flows in graphs. We give a near-linear-time construction that transforms any graph on n vertices into an O(n\\log n)-edge graph on the same vertices whose cuts have approximately the same value as the original graph's. In this new graph, for example, we can run the O(m^{3/2})-time maximum flow algorithm of Goldberg and Rao to find an s--t minimum cut in O(n^{3/2}) time. This corresponds to a (1+epsilon)-times minimum s--t cut in the original graph. In a similar way, we can approximate a sparsest cut to within O(log n) in O(n^2) time using a previous O(mn)-time algorithm. A related approach leads to a randomized divide and conquer algorithm producing an approximately maximum flow in O(m sqrt{n}) time.",
        "published": "2002-07-23T04:27:35Z",
        "link": "http://arxiv.org/abs/cs/0207078v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.1;G.2.2"
        ]
    },
    {
        "title": "Dynamic Generators of Topologically Embedded Graphs",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We provide a data structure for maintaining an embedding of a graph on a surface (represented combinatorially by a permutation of edges around each vertex) and computing generators of the fundamental group of the surface, in amortized time O(log n + log g(log log g)^3) per update on a surface of genus g; we can also test orientability of the surface in the same time, and maintain the minimum and maximum spanning tree of the graph in time O(log n + log^4 g) per update. Our data structure allows edge insertion and deletion as well as the dual operations; these operations may implicitly change the genus of the embedding surface. We apply similar ideas to improve the constant factor in a separator theorem for low-genus graphs, and to find in linear time a tree-decomposition of low-genus low-diameter graphs.",
        "published": "2002-07-24T19:56:17Z",
        "link": "http://arxiv.org/abs/cs/0207082v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Detecting Race Conditions in Parallel Programs that Use Semaphores",
        "authors": [
            "Philip N. Klein",
            "Hsueh-I Lu",
            "Rob H. B. Netzer"
        ],
        "summary": "We address the problem of detecting race conditions in programs that use semaphores for synchronization. Netzer and Miller showed that it is NP-complete to detect race conditions in programs that use many semaphores. We show in this paper that it remains NP-complete even if only two semaphores are used in the parallel programs.   For the tractable case, i.e., using only one semaphore, we give two algorithms for detecting race conditions from the trace of executing a parallel program on p processors, where n semaphore operations are executed. The first algorithm determines in O(n) time whether a race condition exists between any two given operations. The second algorithm runs in O(np log n) time and outputs a compact representation from which one can determine in O(1) time whether a race condition exists between any two given operations. The second algorithm is near-optimal in that the running time is only O(log n) times the time required simply to write down the output.",
        "published": "2002-08-03T17:20:58Z",
        "link": "http://arxiv.org/abs/cs/0208004v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2; G.2.2; D.1.3; D.4.1; E.1"
        ]
    },
    {
        "title": "Spatial Clustering of Galaxies in Large Datasets",
        "authors": [
            "Alexander S. Szalay",
            "Tamas Budavari",
            "Andrew Connolly",
            "Jim Gray",
            "Takahiko Matsubara",
            "Adrian Pope",
            "Istvan Szapudi"
        ],
        "summary": "Datasets with tens of millions of galaxies present new challenges for the analysis of spatial clustering. We have built a framework that integrates a database of object catalogs, tools for creating masks of bad regions, and a fast (NlogN) correlation code. This system has enabled unprecedented efficiency in carrying out the analysis of galaxy clustering in the SDSS catalog. A similar approach is used to compute the three-dimensional spatial clustering of galaxies on very large scales. We describe our strategy to estimate the effect of photometric errors using a database. We discuss our efforts as an early example of data-intensive science. While it would have been possible to get these results without the framework we describe, it will be infeasible to perform these computations on the future huge datasets without using this framework.",
        "published": "2002-08-07T23:06:40Z",
        "link": "http://arxiv.org/abs/cs/0208015v1",
        "categories": [
            "cs.DB",
            "cs.DS",
            "G.3;H.2.8; J.2"
        ]
    },
    {
        "title": "Randomized protocols for asynchronous consensus",
        "authors": [
            "James Aspnes"
        ],
        "summary": "The famous Fischer, Lynch, and Paterson impossibility proof shows that it is impossible to solve the consensus problem in a natural model of an asynchronous distributed system if even a single process can fail. Since its publication, two decades of work on fault-tolerant asynchronous consensus algorithms have evaded this impossibility result by using extended models that provide (a) randomization, (b) additional timing assumptions, (c) failure detectors, or (d) stronger synchronization mechanisms than are available in the basic model. Concentrating on the first of these approaches, we illustrate the history and structure of randomized asynchronous consensus protocols by giving detailed descriptions of several such protocols.",
        "published": "2002-09-06T15:36:58Z",
        "link": "http://arxiv.org/abs/cs/0209014v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2; F.1.2"
        ]
    },
    {
        "title": "Sorting with a forklift",
        "authors": [
            "M. H. Albert",
            "M. D. Atkinson"
        ],
        "summary": "A fork stack is a generalised stack which allows pushes and pops of several items at a time. We consider the problem of determining which input streams can be sorted using a single forkstack, or dually, which permutations of a fixed input stream can be produced using a single forkstack. An algorithm is given to solve the sorting problem and the minimal unsortable sequences are found. The results are extended to fork stacks where there are bounds on how many items can be pushed and popped at one time. In this context we also establish how to enumerate the collection of sortable sequences.",
        "published": "2002-09-10T22:05:28Z",
        "link": "http://arxiv.org/abs/cs/0209016v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "math.CO",
            "G.2.1"
        ]
    },
    {
        "title": "Cycle and Circle Tests of Balance in Gain Graphs: Forbidden Minors and   Their Groups",
        "authors": [
            "Konstantin Rybnikov",
            "Thomas Zaslavsky"
        ],
        "summary": "We examine two criteria for balance of a gain graph, one based on binary cycles and one on circles. The graphs for which each criterion is valid depend on the set of allowed gain groups. The binary cycle test is invalid, except for forests, if any possible gain group has an element of odd order. Assuming all groups are allowed, or all abelian groups, or merely the cyclic group of order 3, we characterize, both constructively and by forbidden minors, the graphs for which the circle test is valid. It turns out that these three classes of groups have the same set of forbidden minors. The exact reason for the importance of the ternary cyclic group is not clear.",
        "published": "2002-09-24T04:24:53Z",
        "link": "http://arxiv.org/abs/math/0209316v3",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.DS",
            "05C22 (Primary) 05C38 (Secondary)"
        ]
    },
    {
        "title": "Preemptive Scheduling of Equal-Length Jobs to Maximize Weighted   Throughput",
        "authors": [
            "Philippe Baptiste",
            "Marek Chrobak",
            "Christoph Durr",
            "Wojciech Jawor",
            "Nodari Vakhania"
        ],
        "summary": "We study the problem of computing a preemptive schedule of equal-length jobs with given release times, deadlines and weights. Our goal is to maximize the weighted throughput, which is the total weight of completed jobs. In Graham's notation this problem is described as (1 | r_j;p_j=p;pmtn | sum w_j U_j). We provide an O(n^4)-time algorithm for this problem, improving the previous bound of O(n^{10}) by Baptiste.",
        "published": "2002-09-30T10:33:13Z",
        "link": "http://arxiv.org/abs/cs/0209033v2",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "On the Reflexivity of Point Sets",
        "authors": [
            "Esther M. Arkin",
            "Sandor P. Fekete",
            "Ferran Hurtado",
            "Joseph S. B. Mitchell",
            "Marc Noy",
            "Vera Sacristan",
            "Saurabh Sethia"
        ],
        "summary": "We introduce a new measure for planar point sets S that captures a combinatorial distance that S is from being a convex set: The reflexivity rho(S) of S is given by the smallest number of reflex vertices in a simple polygonalization of S. We prove various combinatorial bounds and provide efficient algorithms to compute reflexivity, both exactly (in special cases) and approximately (in general). Our study considers also some closely related quantities, such as the convex cover number kappa_c(S) of a planar point set, which is the smallest number of convex chains that cover S, and the convex partition number kappa_p(S), which is given by the smallest number of convex chains with pairwise-disjoint convex hulls that cover S. We have proved that it is NP-complete to determine the convex cover or the convex partition number and have given logarithmic-approximation algorithms for determining each.",
        "published": "2002-10-01T14:40:29Z",
        "link": "http://arxiv.org/abs/cs/0210003v2",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Criteria for Balance in Abelian Gain Graphs, with Applications to   Piecewise-Linear Geometry",
        "authors": [
            "Konstantin Rybnikov",
            "Thomas Zaslavsky"
        ],
        "summary": "A gain graph is a triple (G,h,H), where G is a connected graph with an arbitrary, but fixed, orientation of edges, H is a group, and h is a homomorphism from the free group on the edges of G to H. A gain graph is called balanced if the h-image of each closed walk on G is the identity.   Consider a gain graph with abelian gain group having no odd torsion. If there is a basis of the graph's binary cycle space each of whose members can be lifted to a closed walk whose gain is the identity, then the gain graph is balanced, provided that the graph is finite or the group has no nontrivial infinitely 2-divisible elements. We apply this theorem to deduce a result on the projective geometry of piecewise-linear realizations of cell-decompositions of manifolds.",
        "published": "2002-10-03T16:33:59Z",
        "link": "http://arxiv.org/abs/math/0210052v5",
        "categories": [
            "math.CO",
            "cs.CG",
            "cs.DM",
            "cs.DS",
            "math.AT",
            "Primary: 05C22, 52C25; Secondary: 05C38, 52C25, 52C22"
        ]
    },
    {
        "title": "Dynamic Ordered Sets with Exponential Search Trees",
        "authors": [
            "Arne Andersson",
            "Mikkel Thorup"
        ],
        "summary": "We introduce exponential search trees as a novel technique for converting static polynomial space search structures for ordered sets into fully-dynamic linear space data structures.   This leads to an optimal bound of O(sqrt(log n/loglog n)) for searching and updating a dynamic set of n integer keys in linear space. Here searching an integer y means finding the maximum key in the set which is smaller than or equal to y. This problem is equivalent to the standard text book problem of maintaining an ordered set (see, e.g., Cormen, Leiserson, Rivest, and Stein: Introduction to Algorithms, 2nd ed., MIT Press, 2001).   The best previous deterministic linear space bound was O(log n/loglog n) due Fredman and Willard from STOC 1990. No better deterministic search bound was known using polynomial space.   We also get the following worst-case linear space trade-offs between the number n, the word length w, and the maximal key U < 2^w: O(min{loglog n+log n/log w, (loglog n)(loglog U)/(logloglog U)}). These trade-offs are, however, not likely to be optimal.   Our results are generalized to finger searching and string searching, providing optimal results for both in terms of n.",
        "published": "2002-10-09T04:49:56Z",
        "link": "http://arxiv.org/abs/cs/0210006v2",
        "categories": [
            "cs.DS",
            "E.1;F.2.2;G.2.2"
        ]
    },
    {
        "title": "A Quantum Random Walk Search Algorithm",
        "authors": [
            "Neil Shenvi",
            "Julia Kempe",
            "K. Birgitta Whaley"
        ],
        "summary": "Quantum random walks on graphs have been shown to display many interesting properties, including exponentially fast hitting times when compared with their classical counterparts. However, it is still unclear how to use these novel properties to gain an algorithmic speed-up over classical algorithms. In this paper, we present a quantum search algorithm based on the quantum random walk architecture that provides such a speed-up. It will be shown that this algorithm performs an oracle search on a database of $N$ items with $O(\\sqrt{N})$ calls to the oracle, yielding a speed-up similar to other quantum search algorithms. It appears that the quantum random walk formulation has considerable flexibility, presenting interesting opportunities for development of other, possibly novel quantum algorithms.",
        "published": "2002-10-10T00:06:30Z",
        "link": "http://arxiv.org/abs/quant-ph/0210064v1",
        "categories": [
            "quant-ph",
            "cs.DS"
        ]
    },
    {
        "title": "On the Sum-of-Squares Algorithm for Bin Packing",
        "authors": [
            "Janos Csirik",
            "David S. Johnson",
            "Claire Kenyon",
            "James B. Orlin",
            "Peter W. Shor",
            "Richard R. Weber"
        ],
        "summary": "In this paper we present a theoretical analysis of the deterministic on-line {\\em Sum of Squares} algorithm ($SS$) for bin packing introduced and studied experimentally in \\cite{CJK99}, along with several new variants. $SS$ is applicable to any instance of bin packing in which the bin capacity $B$ and item sizes $s(a)$ are integral (or can be scaled to be so), and runs in time $O(nB)$. It performs remarkably well from an average case point of view: For any discrete distribution in which the optimal expected waste is sublinear, $SS$ also has sublinear expected waste. For any discrete distribution where the optimal expected waste is bounded, $SS$ has expected waste at most $O(\\log n)$. In addition, we discuss several interesting variants on $SS$, including a randomized $O(nB\\log B)$-time on-line algorithm $SS^*$, based on $SS$, whose expected behavior is essentially optimal for all discrete distributions. Algorithm $SS^*$ also depends on a new linear-programming-based pseudopolynomial-time algorithm for solving the NP-hard problem of determining, given a discrete distribution $F$, just what is the growth rate for the optimal expected waste. This article is a greatly expanded version of the conference paper \\cite{sumsq2000}.",
        "published": "2002-10-14T17:17:28Z",
        "link": "http://arxiv.org/abs/cs/0210013v2",
        "categories": [
            "cs.DS",
            "F.2.2 G.3"
        ]
    },
    {
        "title": "Compact Floor-Planning via Orderly Spanning Trees",
        "authors": [
            "Chien-Chih Liao",
            "Hsueh-I Lu",
            "Hsu-Chun Yen"
        ],
        "summary": "Floor-planning is a fundamental step in VLSI chip design. Based upon the concept of orderly spanning trees, we present a simple O(n)-time algorithm to construct a floor-plan for any n-node plane triangulation. In comparison with previous floor-planning algorithms in the literature, our solution is not only simpler in the algorithm itself, but also produces floor-plans which require fewer module types. An equally important aspect of our new algorithm lies in its ability to fit the floor-plan area in a rectangle of size (n-1)x(2n+1)/3. Lower bounds on the worst-case area for floor-planning any plane triangulation are also provided in the paper.",
        "published": "2002-10-17T06:47:02Z",
        "link": "http://arxiv.org/abs/cs/0210016v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2; E.1; G.2.2; B.7.2"
        ]
    },
    {
        "title": "The Lazy Bureaucrat Scheduling Problem",
        "authors": [
            "Esther M. Arkin",
            "Michael A. Bender",
            "Joseph S. B. Mitchell",
            "Steven S. Skiena"
        ],
        "summary": "We introduce a new class of scheduling problems in which the optimization is performed by the worker (single ``machine'') who performs the tasks. A typical worker's objective is to minimize the amount of work he does (he is ``lazy''), or more generally, to schedule as inefficiently (in some sense) as possible. The worker is subject to the constraint that he must be busy when there is work that he can do; we make this notion precise both in the preemptive and nonpreemptive settings. The resulting class of ``perverse'' scheduling problems, which we denote ``Lazy Bureaucrat Problems,'' gives rise to a rich set of new questions that explore the distinction between maximization and minimization in computing optimal schedules.",
        "published": "2002-10-26T21:59:10Z",
        "link": "http://arxiv.org/abs/cs/0210024v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "Fast and Simple Computation of All Longest Common Subsequences",
        "authors": [
            "Ronald I. Greenberg"
        ],
        "summary": "This paper shows that a simple algorithm produces the {\\em all-prefixes-LCSs-graph} in $O(mn)$ time for two input sequences of size $m$ and $n$. Given any prefix $p$ of the first input sequence and any prefix $q$ of the second input sequence, all longest common subsequences (LCSs) of $p$ and $q$ can be generated in time proportional to the output size, once the all-prefixes-LCSs-graph has been constructed. The problem can be solved in the context of generating all the distinct character strings that represent an LCS or in the context of generating all ways of embedding an LCS in the two input strings.",
        "published": "2002-11-01T06:23:00Z",
        "link": "http://arxiv.org/abs/cs/0211001v2",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Improved Phylogeny Comparisons: Non-Shared Edges Nearest Neighbor   Interchanges, and Subtree Transfers",
        "authors": [
            "Wing-Kai Hon",
            "Ming-Yang Kao",
            "Tak-Wah Lam",
            "Wing-Kin Sung",
            "Siu-Ming Yiu"
        ],
        "summary": "The number of the non-shared edges of two phylogenies is a basic measure of the dissimilarity between the phylogenies. The non-shared edges are also the building block for approximating a more sophisticated metric called the nearest neighbor interchange (NNI) distance. In this paper, we give the first subquadratic-time algorithm for finding the non-shared edges, which are then used to speed up the existing approximating algorithm for the NNI distance from $O(n^2)$ time to $O(n \\log n)$ time. Another popular distance metric for phylogenies is the subtree transfer (STT) distance. Previous work on computing the STT distance considered degree-3 trees only. We give an approximation algorithm for the STT distance for degree-$d$ trees with arbitrary $d$ and with generalized STT operations.",
        "published": "2002-11-11T12:02:30Z",
        "link": "http://arxiv.org/abs/cs/0211009v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Efficient Tree Layout in a Multilevel Memory Hierarchy",
        "authors": [
            "Stephen Alstrup",
            "Michael A. Bender",
            "Erik D. Demaine",
            "Martin Farach-Colton",
            "Theis Rauhe",
            "Mikkel Thorup"
        ],
        "summary": "We consider the problem of laying out a tree with fixed parent/child structure in hierarchical memory. The goal is to minimize the expected number of block transfers performed during a search along a root-to-leaf path, subject to a given probability distribution on the leaves. This problem was previously considered by Gil and Itai, who developed optimal but slow algorithms when the block-transfer size B is known. We present faster but approximate algorithms for the same problem; the fastest such algorithm runs in linear time and produces a solution that is within an additive constant of optimal.   In addition, we show how to extend any approximately optimal algorithm to the cache-oblivious setting in which the block-transfer size is unknown to the algorithm. The query performance of the cache-oblivious layout is within a constant factor of the query performance of the optimal known-block-size layout. Computing the cache-oblivious layout requires only logarithmically many calls to the layout algorithm for known block size; in particular, the cache-oblivious layout can be computed in O(N lg N) time, where N is the number of nodes.   Finally, we analyze two greedy strategies, and show that they have a performance ratio between Omega(lg B / lg lg B) and O(lg B) when compared to the optimal layout.",
        "published": "2002-11-12T03:32:02Z",
        "link": "http://arxiv.org/abs/cs/0211010v2",
        "categories": [
            "cs.DS",
            "E.1; F.2.2"
        ]
    },
    {
        "title": "Algorithmic scalability in globally constrained conservative parallel   discrete event simulations of asynchronous systems",
        "authors": [
            "A. Kolakowska",
            "M. A. Novotny",
            "G. Korniss"
        ],
        "summary": "We consider parallel simulations for asynchronous systems employing L processing elements that are arranged on a ring. Processors communicate only among the nearest neighbors and advance their local simulated time only if it is guaranteed that this does not violate causality. In simulations with no constraints, in the infinite L-limit the utilization scales (Korniss et al, PRL 84, 2000); but, the width of the virtual time horizon diverges (i.e., the measurement phase of the algorithm does not scale). In this work, we introduce a moving window global constraint, which modifies the algorithm so that the measurement phase scales as well. We present results of systematic studies in which the system size (i.e., L and the volume load per processor) as well as the constraint are varied. The constraint eliminates the extreme fluctuations in the virtual time horizon, provides a bound on its width, and controls the average progress rate. The width of the window constraint can serve as a tuning parameter that, for a given volume load per processor, could be adjusted to optimize the utilization so as to maximize the efficiency. This result may find numerous applications in modeling the evolution of general spatially extended short-range interacting systems with asynchronous dynamics, including dynamic Monte Carlo studies.",
        "published": "2002-11-12T23:35:12Z",
        "link": "http://arxiv.org/abs/cs/0211013v1",
        "categories": [
            "cs.DC",
            "cond-mat.stat-mech",
            "cs.DS",
            "physics.comp-ph",
            "D.1.3; F.1.1; F.2.0; G.3"
        ]
    },
    {
        "title": "Indexing schemes for similarity search: an illustrated paradigm",
        "authors": [
            "Vladimir Pestov",
            "Aleksandar Stojmirovic"
        ],
        "summary": "We suggest a variation of the Hellerstein--Koutsoupias--Papadimitriou indexability model for datasets equipped with a similarity measure, with the aim of better understanding the structure of indexing schemes for similarity-based search and the geometry of similarity workloads. This in particular provides a unified approach to a great variety of schemes used to index into metric spaces and facilitates their transfer to more general similarity measures such as quasi-metrics. We discuss links between performance of indexing schemes and high-dimensional geometry. The concepts and results are illustrated on a very large concrete dataset of peptide fragments equipped with a biologically significant similarity measure.",
        "published": "2002-11-14T19:10:16Z",
        "link": "http://arxiv.org/abs/cs/0211018v2",
        "categories": [
            "cs.DS",
            "H.3.3"
        ]
    },
    {
        "title": "Schedulers for Rule-based Constraint Programming",
        "authors": [
            "Krzysztof R. Apt",
            "Sebastian Brand"
        ],
        "summary": "We study here schedulers for a class of rules that naturally arise in the context of rule-based constraint programming. We systematically derive a scheduler for them from a generic iteration algorithm of Apt [2000]. We apply this study to so-called membership rules of Apt and Monfroy [2001]. This leads to an implementation that yields for these rules a considerably better performance than their execution as standard CHR rules.",
        "published": "2002-11-15T13:38:20Z",
        "link": "http://arxiv.org/abs/cs/0211019v1",
        "categories": [
            "cs.DS",
            "cs.PL",
            "I.2.2; I.2.3; D.3.3; D.3.4"
        ]
    },
    {
        "title": "On graph coloring check-digit method",
        "authors": [
            "Kamil Kulesza",
            "Zbigniew Kotulski"
        ],
        "summary": "We show a method how to convert any graph into the binary number and vice versa. We derive upper bound for maximum number of graphs, that, have fixed number of vertices and can be colored with n colors (n is any given number). Proof for the result is outlined. Next, graph coloring based check-digit scheme is proposed. We use quantitative result derived, to show, that feasibility of the proposed scheme increases with size of the number which digits are checked, and overall probability of digits errors.",
        "published": "2002-11-20T12:41:13Z",
        "link": "http://arxiv.org/abs/math/0211317v1",
        "categories": [
            "math.CO",
            "cs.CR",
            "cs.DM",
            "cs.DS",
            "D.4.6; E.4"
        ]
    },
    {
        "title": "Solving a \"Hard\" Problem to Approximate an \"Easy\" One: Heuristics for   Maximum Matchings and Maximum Traveling Salesman Problems",
        "authors": [
            "Sandor P. Fekete",
            "Henk Meijer",
            "Andre Rohe",
            "Walter Tietze"
        ],
        "summary": "We consider geometric instances of the Maximum Weighted Matching Problem (MWMP) and the Maximum Traveling Salesman Problem (MTSP) with up to 3,000,000 vertices. Making use of a geometric duality relationship between MWMP, MTSP, and the Fermat-Weber-Problem (FWP), we develop a heuristic approach that yields in near-linear time solutions as well as upper bounds. Using various computational tools, we get solutions within considerably less than 1% of the optimum.   An interesting feature of our approach is that, even though an FWP is hard to compute in theory and Edmonds' algorithm for maximum weighted matching yields a polynomial solution for the MWMP, the practical behavior is just the opposite, and we can solve the FWP with high accuracy in order to find a good heuristic solution for the MWMP.",
        "published": "2002-12-16T09:39:16Z",
        "link": "http://arxiv.org/abs/cs/0212044v1",
        "categories": [
            "cs.DS",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "On local equilibrium equations for clustering states",
        "authors": [
            "Giorgio Parisi"
        ],
        "summary": "In this note we show that local equilibrium equations (the generalization of the TAP equations or of the belief propagation equations) do have solutions in the colorable phase of the coloring problem. The same results extend to other optimization problems where the solutions has cost zero (e.g. K-satisfiability). On a random graph the solutions of the local equilibrium equations are associated to clusters of configurations (clustering states). On a random graph the local equilibrium equations have solutions almost everywhere in the uncolored phase; in this case we have to introduce the concept quasi-solution of the local equilibrium equations.",
        "published": "2002-12-18T22:14:39Z",
        "link": "http://arxiv.org/abs/cs/0212047v2",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "cs.DS",
            "G.3, G.2.1"
        ]
    },
    {
        "title": "Improved Compact Visibility Representation of Planar Graph via   Schnyder's Realizer",
        "authors": [
            "Ching-Chi Lin",
            "Hsueh-I Lu",
            "I-Fan Sun"
        ],
        "summary": "Let $G$ be an $n$-node planar graph. In a visibility representation of $G$, each node of $G$ is represented by a horizontal line segment such that the line segments representing any two adjacent nodes of $G$ are vertically visible to each other. In the present paper we give the best known compact visibility representation of $G$. Given a canonical ordering of the triangulated $G$, our algorithm draws the graph incrementally in a greedy manner. We show that one of three canonical orderings obtained from Schnyder's realizer for the triangulated $G$ yields a visibility representation of $G$ no wider than $\\frac{22n-40}{15}$. Our easy-to-implement O(n)-time algorithm bypasses the complicated subroutines for four-connected components and four-block trees required by the best previously known algorithm of Kant. Our result provides a negative answer to Kant's open question about whether $\\frac{3n-6}{2}$ is a worst-case lower bound on the required width. Also, if $G$ has no degree-three (respectively, degree-five) internal node, then our visibility representation for $G$ is no wider than $\\frac{4n-9}{3}$ (respectively, $\\frac{4n-7}{3}$). Moreover, if $G$ is four-connected, then our visibility representation for $G$ is no wider than $n-1$, matching the best known result of Kant and He. As a by-product, we obtain a much simpler proof for a corollary of Wagner's Theorem on realizers, due to Bonichon, Sa\\\"{e}c, and Mosbah.",
        "published": "2002-12-29T12:41:47Z",
        "link": "http://arxiv.org/abs/cs/0212054v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2; B.7.2; E.1; G.2.2; I.3.6"
        ]
    },
    {
        "title": "Core Services in the Architecture of the National Digital Library for   Science Education (NSDL)",
        "authors": [
            "Carl Lagoze",
            "William Arms",
            "Stoney Gan",
            "Diane Hillmann",
            "Christopher Ingram",
            "Dean Krafft",
            "Richard Marisa",
            "Jon Phipps",
            "John Saylor",
            "Carol Terrizzi",
            "Walter Hoehn",
            "David Millman",
            "James Allan",
            "Sergio Guzman-Lara",
            "Tom Kalt"
        ],
        "summary": "We describe the core components of the architecture for the (NSDL) National Science, Mathematics, Engineering, and Technology Education Digital Library. Over time the NSDL will include heterogeneous users, content, and services. To accommodate this, a design for a technical and organization infrastructure has been formulated based on the notion of a spectrum of interoperability. This paper describes the first phase of the interoperability infrastructure including the metadata repository, search and discovery services, rights management services, and user interface portal facilities.",
        "published": "2002-01-29T17:51:15Z",
        "link": "http://arxiv.org/abs/cs/0201025v1",
        "categories": [
            "cs.DL",
            "D.2.12"
        ]
    },
    {
        "title": "Components of an NSDL Architecture: Technical Scope and Functional Model",
        "authors": [
            "David Fulker",
            "Greg Janee"
        ],
        "summary": "We describe work leading toward specification of a technical architecture for the National Science, Mathematics, Engineering, and Technology Education Digital Library (NSDL). This includes a technical scope and a functional model, with some elaboration on the particularly rich set of library services that NSDL is expected eventually to encompass.",
        "published": "2002-01-30T01:14:35Z",
        "link": "http://arxiv.org/abs/cs/0201027v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "The SDSS SkyServer: Public Access to the Sloan Digital Sky Server Data",
        "authors": [
            "Alexander S. Szalay",
            "Jim Gray",
            "Ani R. Thakar",
            "Peter Z. Kunszt",
            "Tanu Malik",
            "Jordan Raddick",
            "Christopher Stoughton",
            "Jan vandenBerg"
        ],
        "summary": "The SkyServer provides Internet access to the public Sloan Digi-tal Sky Survey (SDSS) data for both astronomers and for science education. This paper describes the SkyServer goals and archi-tecture. It also describes our experience operating the SkyServer on the Internet. The SDSS data is public and well-documented so it makes a good test platform for research on database algorithms and performance.",
        "published": "2002-02-12T23:16:36Z",
        "link": "http://arxiv.org/abs/cs/0202013v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "H.3.7; H.3.5;H.2; H.3; H.4; H.5"
        ]
    },
    {
        "title": "Data Mining the SDSS SkyServer Database",
        "authors": [
            "Jim Gray",
            "Alex S. Szalay",
            "Ani R. Thakar",
            "Peter Z. Kunszt",
            "Christopher Stoughton",
            "Don Slutz",
            "Jan vandenBerg"
        ],
        "summary": "An earlier paper (Szalay et. al. \"Designing and Mining MultiTerabyte Astronomy Archives: The Sloan Digital Sky Survey,\" ACM SIGMOD 2000) described the Sloan Digital Sky Survey's (SDSS) data management needs by defining twenty database queries and twelve data visualization tasks that a good data management system should support. We built a database and interfaces to support both the query load and also a website for ad-hoc access. This paper reports on the database design, describes the data loading pipeline, and reports on the query implementation and performance. The queries typically translated to a single SQL statement. Most queries run in less than 20 seconds, allowing scientists to interactively explore the database. This paper is an in-depth tour of those queries. Readers should first have studied the companion overview paper Szalay et. al. \"The SDSS SkyServer, Public Access to the Sloan Digital Sky Server Data\" ACM SIGMOND 2002.",
        "published": "2002-02-12T23:47:20Z",
        "link": "http://arxiv.org/abs/cs/0202014v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "H.2.8;H.3.3; H.3.5;h.3.7;H.4.2"
        ]
    },
    {
        "title": "The structure of broad topics on the Web",
        "authors": [
            "Soumen Chakrabarti",
            "Mukul M. Joshi",
            "Kunal Punera",
            "David M. Pennock"
        ],
        "summary": "The Web graph is a giant social network whose properties have been measured and modeled extensively in recent years. Most such studies concentrate on the graph structure alone, and do not consider textual properties of the nodes. Consequently, Web communities have been characterized purely in terms of graph structure and not on page content. We propose that a topic taxonomy such as Yahoo! or the Open Directory provides a useful framework for understanding the structure of content-based clusters and communities. In particular, using a topic taxonomy and an automatic classifier, we can measure the background distribution of broad topics on the Web, and analyze the capability of recent random walk algorithms to draw samples which follow such distributions. In addition, we can measure the probability that a page about one broad topic will link to another broad topic. Extending this experiment, we can measure how quickly topic context is lost while walking randomly on the Web graph. Estimates of this topic mixing distance may explain why a global PageRank is still meaningful in the context of broad queries. In general, our measurements may prove valuable in the design of community-specific crawlers and link-based ranking systems.",
        "published": "2002-03-20T06:46:21Z",
        "link": "http://arxiv.org/abs/cs/0203024v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.5.4; H.5.3; H.1.0"
        ]
    },
    {
        "title": "Seven Dimensions of Portability for Language Documentation and   Description",
        "authors": [
            "Steven Bird",
            "Gary Simons"
        ],
        "summary": "The process of documenting and describing the world's languages is undergoing radical transformation with the rapid uptake of new digital technologies for capture, storage, annotation and dissemination. However, uncritical adoption of new tools and technologies is leading to resources that are difficult to reuse and which are less portable than the conventional printed resources they replace. We begin by reviewing current uses of software tools and digital technologies for language documentation and description. This sheds light on how digital language documentation and description are created and managed, leading to an analysis of seven portability problems under the following headings: content, format, discovery, access, citation, preservation and rights. After characterizing each problem we provide a series of value statements, and this provides the framework for a broad range of best practice recommendations.",
        "published": "2002-04-10T13:52:19Z",
        "link": "http://arxiv.org/abs/cs/0204020v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.3.7; I.2.7; J.5"
        ]
    },
    {
        "title": "A Scalable Architecture for Harvest-Based Digital Libraries - The   ODU/Southampton Experiments",
        "authors": [
            "Xiaoming Liu",
            "Tim Brody",
            "Stevan Harnad",
            "Les Carr",
            "Kurt Maly",
            "Mohammad Zubair",
            "Michael L. Nelson"
        ],
        "summary": "This paper discusses the requirements of current and emerging applications based on the Open Archives Initiative (OAI) and emphasizes the need for a common infrastructure to support them. Inspired by HTTP proxy, cache, gateway and web service concepts, a design for a scalable and reliable infrastructure that aims at satisfying these requirements is presented. Moreover it is shown how various applications can exploit the services included in the proposed infrastructure. The paper concludes by discussing the current status of several prototype implementations.",
        "published": "2002-05-28T15:32:55Z",
        "link": "http://arxiv.org/abs/cs/0205071v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.7"
        ]
    },
    {
        "title": "Using the Annotated Bibliography as a Resource for Indicative   Summarization",
        "authors": [
            "Min-Yen Kan",
            "Judith L. Klavans",
            "Kathleen R. McKeown"
        ],
        "summary": "We report on a language resource consisting of 2000 annotated bibliography entries, which is being analyzed as part of our research on indicative document summarization. We show how annotated bibliographies cover certain aspects of summarization that have not been well-covered by other summary corpora, and motivate why they constitute an important form to study for information retrieval. We detail our methodology for collecting the corpus, and overview our document feature markup that we introduced to facilitate summary analysis. We present the characteristics of the corpus, methods of collection, and show its use in finding the distribution of types of information included in indicative summaries and their relative ordering within the summaries.",
        "published": "2002-06-04T14:48:48Z",
        "link": "http://arxiv.org/abs/cs/0206007v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "I.2.7"
        ]
    },
    {
        "title": "TerraService.NET: An Introduction to Web Services",
        "authors": [
            "Tom Barclay",
            "Jim Gray",
            "Eric Strand",
            "Steve Ekblad",
            "Jeffrey Richter"
        ],
        "summary": "This article explores the design and construction of a geo-spatial Internet web service application from the host web site perspective and from the perspective of an application using the web service. The TerraService.NET web service was added to the popular TerraServer database and web site with no major structural changes to the database. The article discusses web service design, implementation, and deployment concepts and design guidelines. Web services enable applications that aggregate and interact with information and resources from Internet-scale distributed servers. The article presents the design of two USDA applications that interoperate with database and web service resources in Fort Collins Colorado and the TerraService web service located in Tukwila Washington.",
        "published": "2002-08-07T22:18:35Z",
        "link": "http://arxiv.org/abs/cs/0208010v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "H.2; H.3; H.4;H.5"
        ]
    },
    {
        "title": "Online Scientific Data Curation, Publication, and Archiving",
        "authors": [
            "Jim Gray",
            "Alexander S. Szalay",
            "Ani R. Thakar",
            "Christopher Stoughton",
            "Jan vandenBerg"
        ],
        "summary": "Science projects are data publishers. The scale and complexity of current and future science data changes the nature of the publication process. Publication is becoming a major project component. At a minimum, a project must preserve the ephemeral data it gathers. Derived data can be reconstructed from metadata, but metadata is ephemeral. Longer term, a project should expect some archive to preserve the data. We observe that pub-lished scientific data needs to be available forever ? this gives rise to the data pyramid of versions and to data inflation where the derived data volumes explode. As an example, this article describes the Sloan Digital Sky Survey (SDSS) strategies for data publication, data access, curation, and preservation.",
        "published": "2002-08-07T22:42:31Z",
        "link": "http://arxiv.org/abs/cs/0208012v1",
        "categories": [
            "cs.DL",
            "H.3.7;I.7.4;J.2;J.3;J.7"
        ]
    },
    {
        "title": "Web Services for the Virtual Observatory",
        "authors": [
            "Alexander S. Szalay",
            "Tamas Budavari",
            "Tanu Malika",
            "Jim Gray",
            "Ani Thakara"
        ],
        "summary": "Web Services form a new, emerging paradigm to handle distributed access to resources over the Internet. There are platform independent standards (SOAP, WSDL), which make the developers? task considerably easier. This article discusses how web services could be used in the context of the Virtual Observatory. We envisage a multi-layer architecture, with interoperating services. A well-designed lower layer consisting of simple, standard services implemented by most data providers will go a long way towards establishing a modular architecture. More complex applications can be built upon this core layer. We present two prototype applications, the SdssCutout and the SkyQuery as examples of this layered architecture.",
        "published": "2002-08-07T22:58:37Z",
        "link": "http://arxiv.org/abs/cs/0208014v1",
        "categories": [
            "cs.DC",
            "cs.DL",
            "C.23.1;C.2.4;D.2.12;D.4.7;H.2;H.3;H.4;J.2;J.3"
        ]
    },
    {
        "title": "A Virtual Library of Technical Publications",
        "authors": [
            "Elizabeth Anderson",
            "Robert Atkinson",
            "Elizabeth Buckley-Geer",
            "Cynthia Crego",
            "Lisa Giacchetti",
            "Stephen Hanson",
            "David Ritchie",
            "Jean Slisz",
            "Sara Tompson",
            "Stephen Wolbers"
        ],
        "summary": "Through a collaborative effort, the Fermilab Information Resources Department and Computing Division have created a \"virtual library\" of technical publications that provides public access to electronic full-text documents. This paper will discuss the vision, planning and milestones of the project, as well as the hardware, software and interdepartmental cooperation components.",
        "published": "2002-08-23T19:14:32Z",
        "link": "http://arxiv.org/abs/cs/0208039v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Reconciling MPEG-7 and MPEG-21 Semantics through a Common Event-Aware   Metadata Model",
        "authors": [
            "Jane Hunter"
        ],
        "summary": "The \"event\" concept appears repeatedly when developing metadata models for the description and management of multimedia content. During the typical life cycle of multimedia content, events occur at many different levels - from the events which happen during content creation (directing, acting, camera panning and zooming) to the events which happen to the physical form (acquisition, relocation, damage of film or video) to the digital conversion, reformatting, editing and repackaging events, to the events which are depicted in the actual content (political, news, sporting) to the usage, ownership and copyright agreement events and even the metadata attribution events. Support is required within both MPEG-7 and MPEG-21 for the clear and unambiguous description of all of these event types which may occur at widely different levels of nesting and granularity. In this paper we first describe an event-aware model (the ABC model) which is capable of modeling and yet clearly differentiating between all of these, often recursive and overlapping events. We then illustrate how this model can be used as the foundation to facilitate semantic interoperability between MPEG-7 and MPEG-21. By expressing the semantics of both MPEG-7 and MPEG-21 metadata terms in RDF Schema (and some DAML+OIL extensions) and attaching the MPEG-7 and MPEG-21 class and property hierarchies to the appropriate top-level classes and properties of the ABC model, we are essentially able to define a single distributed machine-understandable ontology, which will enable interoperability of data and services across the entire multimedia content delivery chain.",
        "published": "2002-10-22T02:16:57Z",
        "link": "http://arxiv.org/abs/cs/0210021v1",
        "categories": [
            "cs.MM",
            "cs.DL",
            "H.2.1;H.3.1;H.3.7;H.5.1"
        ]
    },
    {
        "title": "Integration and interoperability accessing electronic information   resources in science and technology: the proposal of Brazilian Digital   Library",
        "authors": [
            "Carlos H. Marcondes",
            "Luis Fernando Sayao"
        ],
        "summary": "This paper describes technological and methodological options to achieve interoperability in accessing electronic information resources, available in Internet, in the scope of Brazilian Digital Library in Science and Technology Project - BDL, developed by Brazilian Institute for Scientific and Technical Information - IBICT. It stresses the impact of the Web in the publishing and communication processes in science and technology and also in the information systems and libraries. The work points out the two major objectives of the BDL Project: facilitates electronic publishing of different full text materials such as theses, journal articles, conference papers,grey literature - by Brazilian scientific community, so amplifying their nationally and internationally visibility; and achieving, through a unified gateway, thus avoiding a user to navigate and query across different information resources individually. The work explains technological options and standards that will assure interoperability in this context.",
        "published": "2002-10-29T23:02:21Z",
        "link": "http://arxiv.org/abs/cs/0210029v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "An Approach to Automatic Indexing of Scientific Publications in High   Energy Physics for Database SPIRES HEP",
        "authors": [
            "A. V. Averin",
            "L. A. Vassilevskaya"
        ],
        "summary": "We introduce an approach to automatic indexing of e-prints based on a pattern-matching technique making extensive use of an Associative Patterns Dictionary (APD), developed by us. Entries in the APD consist of natural language phrases with the same semantic interpretation as a set of keywords from a controlled vocabulary. The method also allows to recognize within e-prints formulae written in TeX notations that might also appear as keywords. We present an automatic indexing system, AUTEX, which we have applied to keyword index e-prints in selected areas in high energy physics (HEP) making use of the DESY-HEPI thesaurus as a controlled vocabulary.",
        "published": "2002-11-28T17:33:19Z",
        "link": "http://arxiv.org/abs/cs/0211041v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.1; H.3.2; H.3.6; H.3.7"
        ]
    },
    {
        "title": "A Backward Analysis for Constraint Logic Programs",
        "authors": [
            "Andy King",
            "Lunjin Lu"
        ],
        "summary": "One recurring problem in program development is that of understanding how to re-use code developed by a third party. In the context of (constraint) logic programming, part of this problem reduces to figuring out how to query a program. If the logic program does not come with any documentation, then the programmer is forced to either experiment with queries in an ad hoc fashion or trace the control-flow of the program (backward) to infer the modes in which a predicate must be called so as to avoid an instantiation error. This paper presents an abstract interpretation scheme that automates the latter technique. The analysis presented in this paper can infer moding properties which if satisfied by the initial query, come with the guarantee that the program and query can never generate any moding or instantiation errors. Other applications of the analysis are discussed. The paper explains how abstract domains with certain computational properties (they condense) can be used to trace control-flow backward (right-to-left) to infer useful properties of initial queries. A correctness argument is presented and an implementation is reported.",
        "published": "2002-01-16T11:48:13Z",
        "link": "http://arxiv.org/abs/cs/0201011v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.6; F.3.2"
        ]
    },
    {
        "title": "Model-Based Software Engineering and Ada: Synergy for the Development of   Safety-Critical Systems",
        "authors": [
            "Andree Blotz",
            "Franz Huber",
            "Heiko Loetzbeyer",
            "Alexander Pretschner",
            "Oscar Slotosch",
            "Hans-Peter Zaengerl"
        ],
        "summary": "In this paper we outline a software development process for safety-critical systems that aims at combining some of the specific strengths of model-based development with those of programming language based development using safety-critical subsets of Ada. Model-based software development and model-based test case generation techniques are combined with code generation techniques and tools providing a transition from model to code both for a system itself and for its test cases. This allows developers to combine domain-oriented, model-based techniques with source code based validation techniques, as required for conformity with standards for the development of safety-critical software, such as the avionics standard RTCA/DO-178B. We introduce the AutoFocus and Validator modeling and validation toolset and sketch its usage for modeling, test case generation, and code generation in a combined approach, which is further illustrated by a simplified leading edge aerospace model with built-in fault tolerance.",
        "published": "2002-01-26T11:32:31Z",
        "link": "http://arxiv.org/abs/cs/0201023v1",
        "categories": [
            "cs.SE",
            "D.2, K.6.3"
        ]
    },
    {
        "title": "Software Validation using Power Profiles",
        "authors": [
            "Raimondas Lencevicius",
            "Edu Metz",
            "Alexander Ran"
        ],
        "summary": "The validation of modern software systems incorporates both functional and quality requirements. This paper proposes a validation approach for software quality requirement - its power consumption. This approach validates whether the software produces the desired results with a minimum expenditure of energy. We present energy requirements and an approach for their validation using a power consumption model, test-case specification, software traces, and power measurements. Three different approaches for power data gathering are described. The power consumption of mobile phone applications is obtained and matched against the power consumption model.",
        "published": "2002-01-30T22:28:52Z",
        "link": "http://arxiv.org/abs/cs/0201028v1",
        "categories": [
            "cs.SE",
            "D.2.4"
        ]
    },
    {
        "title": "A Refinement Calculus for Logic Programs",
        "authors": [
            "Ian Hayes",
            "Robert Colvin",
            "David Hemer",
            "Paul Strooper",
            "Ray Nickson"
        ],
        "summary": "Existing refinement calculi provide frameworks for the stepwise development of imperative programs from specifications. This paper presents a refinement calculus for deriving logic programs. The calculus contains a wide-spectrum logic programming language, including executable constructs such as sequential conjunction, disjunction, and existential quantification, as well as specification constructs such as general predicates, assumptions and universal quantification. A declarative semantics is defined for this wide-spectrum language based on executions. Executions are partial functions from states to states, where a state is represented as a set of bindings. The semantics is used to define the meaning of programs and specifications, including parameters and recursion. To complete the calculus, a notion of correctness-preserving refinement over programs in the wide-spectrum language is defined and refinement laws for developing programs are introduced. The refinement calculus is illustrated using example derivations and prototype tool support is discussed.",
        "published": "2002-02-04T01:20:38Z",
        "link": "http://arxiv.org/abs/cs/0202002v1",
        "categories": [
            "cs.SE",
            "cs.LO",
            "F.3.1; D.1.6"
        ]
    },
    {
        "title": "Approximate Computation of Reach Sets in Hybrid Systems",
        "authors": [
            "D. Ravi",
            "R. K. Shyamasundar"
        ],
        "summary": "One of the most important problems in hybrid systems is the {\\em reachability problem}. The reachability problem has been shown to be undecidable even for a subclass of {\\em linear} hybrid systems. In view of this, the main focus in the area of hybrid systems has been to find {\\em effective} semi-decision procedures for this problem. Such an algorithmic approach involves finding methods of computation and representation of reach sets of the continuous variables within a discrete state of a hybrid system. In this paper, after presenting a brief introduction to hybrid systems and reachability problem, we propose a computational method for obtaining the reach sets of continuous variables in a hybrid system. In addition to this, we also describe a new algorithm to over-approximate with polyhedra the reach sets of the continuous variables with linear dynamics and polyhedral initial set. We illustrate these algorithms with typical interesting examples.",
        "published": "2002-02-07T12:50:13Z",
        "link": "http://arxiv.org/abs/cs/0202006v1",
        "categories": [
            "cs.LO",
            "cs.SE",
            "F3, D2.2"
        ]
    },
    {
        "title": "BSML: A Binding Schema Markup Language for Data Interchange in Problem   Solving Environments (PSEs)",
        "authors": [
            "Alex Verstak",
            "Naren Ramakrishnan",
            "Layne T. Watson",
            "Jian He",
            "Clifford A. Shaffer",
            "Kyung Kyoon Bae",
            "Jing Jiang",
            "William H. Tranter",
            "Theodore S. Rappaport"
        ],
        "summary": "We describe a binding schema markup language (BSML) for describing data interchange between scientific codes. Such a facility is an important constituent of scientific problem solving environments (PSEs). BSML is designed to integrate with a PSE or application composition system that views model specification and execution as a problem of managing semistructured data. The data interchange problem is addressed by three techniques for processing semistructured data: validation, binding, and conversion. We present BSML and describe its application to a PSE for wireless communications system design.",
        "published": "2002-02-18T16:01:03Z",
        "link": "http://arxiv.org/abs/cs/0202027v1",
        "categories": [
            "cs.CE",
            "cs.SE",
            "D.2.6; I.2.4"
        ]
    },
    {
        "title": "An Assessment of the Consistency for Software Measurement Methods",
        "authors": [
            "R. Asensio Monge",
            "F. Sanchis Marco",
            "F. Torre Cervigon"
        ],
        "summary": "Consistency, defined as the requirement that a series of measurements of the same project carried out by different raters using the same method should produce similar results, is one of the most important aspects to be taken into account in the measurement methods of the software. In spite of this, there is a widespread view that many measurement methods introduce an undesirable amount of subjectivity in the measurement process. This perception has made several organizations develop revisions of the standard methods whose main aim is to improve their consistency by introducing some suitable modifications of those aspects which are believed to introduce a greater degree of subjectivity.Each revision of a method must be empirically evaluated to determine to what extent is the aim of improving its consistency achieved. In this article we will define an homogeneous statistic intended to describe the consistency level of a method, and we will develop the statistical analysis which should be carried out in order to conclude whether or not a measurement method is more consistent than other one.",
        "published": "2002-04-09T09:33:44Z",
        "link": "http://arxiv.org/abs/cs/0204014v1",
        "categories": [
            "cs.SE",
            "D.4.8"
        ]
    },
    {
        "title": "Monitoring and Debugging Concurrent and Distributed Object-Oriented   Systems",
        "authors": [
            "Joseph R. Kiniry"
        ],
        "summary": "A major part of debugging, testing, and analyzing a complex software system is understanding what is happening within the system at run-time. Some developers advocate running within a debugger to better understand the system at this level. Others embed logging statements, even in the form of hard-coded calls to print functions, throughout the code. These techniques are all general, rough forms of what we call system monitoring, and, while they have limited usefulness in simple, sequential systems, they are nearly useless in complex, concurrent ones. We propose a set of new mechanisms, collectively known as a monitoring system, for understanding such complex systems, and we describe an example implementation of such a system, called IDebug, for the Java programming language.",
        "published": "2002-04-15T23:33:24Z",
        "link": "http://arxiv.org/abs/cs/0204034v1",
        "categories": [
            "cs.SE",
            "D.2; D.2.5"
        ]
    },
    {
        "title": "Semantic Properties for Lightweight Specification in Knowledgeable   Development Environments",
        "authors": [
            "Joseph R. Kiniry"
        ],
        "summary": "Semantic properties are domain-specific specification constructs used to augment an existing language with richer semantics. These properties are taken advantage of in system analysis, design, implementation, testing, and maintenance through the use of documentation and source-code transformation tools. Semantic properties are themselves specified at two levels: loosely with precise natural language, and formally within the problem domain. The refinement relationships between these specification levels, as well as between a semantic property's use and its realization in program code via tools, is specified with a new formal method for reuse called kind theory.",
        "published": "2002-04-15T23:40:52Z",
        "link": "http://arxiv.org/abs/cs/0204035v1",
        "categories": [
            "cs.SE",
            "D.1.0; D.2; D.3.1; D.3.2; D.3.4; F.3.1; F.4.1; F.4.3"
        ]
    },
    {
        "title": "Semantic Component Composition",
        "authors": [
            "Joseph R. Kiniry"
        ],
        "summary": "Building complex software systems necessitates the use of component-based architectures. In theory, of the set of components needed for a design, only some small portion of them are \"custom\"; the rest are reused or refactored existing pieces of software. Unfortunately, this is an idealized situation. Just because two components should work together does not mean that they will work together.   The \"glue\" that holds components together is not just technology. The contracts that bind complex systems together implicitly define more than their explicit type. These \"conceptual contracts\" describe essential aspects of extra-system semantics: e.g., object models, type systems, data representation, interface action semantics, legal and contractual obligations, and more.   Designers and developers spend inordinate amounts of time technologically duct-taping systems to fulfill these conceptual contracts because system-wide semantics have not been rigorously characterized or codified. This paper describes a formal characterization of the problem and discusses an initial implementation of the resulting theoretical system.",
        "published": "2002-04-15T23:50:42Z",
        "link": "http://arxiv.org/abs/cs/0204036v1",
        "categories": [
            "cs.SE",
            "D.1.0; D.2; D.3.1; F.3.1; F.4.1; F.4.3"
        ]
    },
    {
        "title": "Three-Tiered Specification of Micro-Architectures",
        "authors": [
            "Vasu Alagar",
            "Ralf Laemmel"
        ],
        "summary": "A three-tiered specification approach is developed to formally specify collections of collaborating objects, say micro-architectures. (i) The structural properties to be maintained in the collaboration are specified in the lowest tier. (ii) The behaviour of the object methods in the classes is specified in the middle tier. (iii) The interaction of the objects in the micro-architecture is specified in the third tier. The specification approach is based on Larch and accompanying notations and tools. The approach enables the unambiguous and complete specification of reusable collections of collaborating objects. The layered, formal approach is compared to other approaches including the mainstream UML approach.",
        "published": "2002-05-19T14:46:34Z",
        "link": "http://arxiv.org/abs/cs/0205052v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.4; D.2.10; D.2.11; D.2.13"
        ]
    },
    {
        "title": "The analysis of the IFPUG method sensitivity",
        "authors": [
            "R. Asensio Monge",
            "F. Sanchis Marco",
            "F. Torre Cervigon"
        ],
        "summary": "J. Albrecht`s Function Point Analysis (FPA) is a method to determine the functional size of software products. An organization called International Function Point Users Group (IPFUG), considers the FPA as a standard in the software functional size measurement. The Albrechts method is followed by IPFUG method which includes some modifications in order to improve it. A limitation of the method refers to the fact that FPA is not sensitive enough to differentiate the functional size in small enhancements. That affects the productivity analysis, where the software product functional size is required. To provide more power to the functional size measurement, A. Abran, M. Maya and H. Nguyeckim have proposed some modifications to improve it. The IPFUG v 4.1 method which includes these modifications is named IFPUG v 4.1 extended. In this work we set the conditions to delimiting granular from non granular functions and we calculate the static calibration and sensitivity graphs for the measurements of a set of projects with a high percentage of granular functions, all of then measured with the IFPUG v 4.1 method and the IFPUG v 4.1 extended. Finally, we introduce a statistic analysis in order to determine whether significant differences exist between both methods or not.",
        "published": "2002-06-14T09:24:53Z",
        "link": "http://arxiv.org/abs/cs/0206021v1",
        "categories": [
            "cs.SE",
            "D.4.8"
        ]
    },
    {
        "title": "COINS: a constraint-based interactive solving system",
        "authors": [
            "Samir Ouis",
            "Narendra Jussien",
            "Patrice Boizumault"
        ],
        "summary": "This paper describes the COINS (COnstraint-based INteractive Solving) system: a conflict-based constraint solver. It helps understanding inconsistencies, simulates constraint additions and/or retractions (without any propagation), determines if a given constraint belongs to a conflict and provides diagnosis tools (e.g. why variable v cannot take value val). COINS also uses user-friendly representation of conflicts and explanations.",
        "published": "2002-07-11T16:25:20Z",
        "link": "http://arxiv.org/abs/cs/0207046v2",
        "categories": [
            "cs.SE",
            "D.1.6; D.2.5; D.2.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Tracing and Explaining Execution of CLP(FD) Programs",
        "authors": [
            "Magnus Agren",
            "Tamas Szeredi",
            "Nicolas Beldiceanu",
            "Mats Carlsson"
        ],
        "summary": "Previous work in the area of tracing CLP(FD) programs mainly focuses on providing information about control of execution and domain modification. In this paper, we present a trace structure that provides information about additional important aspects. We incorporate explanations in the trace structure, i.e. reasons for why certain solver actions occur. Furthermore, we come up with a format for describing the execution of the filtering algorithms of global constraints. Some new ideas about the design of the trace are also presented. For example, we have modeled our trace as a nested block structure in order to achieve a hierarchical view. Also, new ways about how to represent and identify different entities such as constraints and domain variables are presented.",
        "published": "2002-07-11T16:43:30Z",
        "link": "http://arxiv.org/abs/cs/0207047v2",
        "categories": [
            "cs.SE",
            "D.1.6; D.2.5; D.2.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "More Precise Yet Efficient Type Inference for Logic Programs",
        "authors": [
            "Claudio Vaucheret",
            "Francisco Bueno"
        ],
        "summary": "Type analyses of logic programs which aim at inferring the types of the program being analyzed are presented in a unified abstract interpretation-based framework. This covers most classical abstract interpretation-based type analyzers for logic programs, built on either top-down or bottom-up interpretation of the program. In this setting, we discuss the widening operator, arguably a crucial one. We present a new widening which is more precise than those previously proposed. Practical results with our analysis domain are also presented, showing that it also allows for efficient analysis.",
        "published": "2002-07-11T16:50:45Z",
        "link": "http://arxiv.org/abs/cs/0207049v2",
        "categories": [
            "cs.SE",
            "D.1.6; D.2.5; D.2.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Value withdrawal explanations: a theoretical tool for programming   environments",
        "authors": [
            "Willy Lesaint"
        ],
        "summary": "Constraint logic programming combines declarativity and efficiency thanks to constraint solvers implemented for specific domains. Value withdrawal explanations have been efficiently used in several constraints programming environments but there does not exist any formalization of them. This paper is an attempt to fill this lack. Furthermore, we hope that this theoretical tool could help to validate some programming environments. A value withdrawal explanation is a tree describing the withdrawal of a value during a domain reduction by local consistency notions and labeling. Domain reduction is formalized by a search tree using two kinds of operators: operators for local consistency notions and operators for labeling. These operators are defined by sets of rules. Proof trees are built with respect to these rules. For each removed value, there exists such a proof tree which is the withdrawal explanation of this value.",
        "published": "2002-07-11T16:52:49Z",
        "link": "http://arxiv.org/abs/cs/0207050v2",
        "categories": [
            "cs.SE",
            "D.1.6; D.2.5; D.2.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Declarative program development in Prolog with GUPU",
        "authors": [
            "Ulrich Neumerkel",
            "Stefan Kral"
        ],
        "summary": "We present GUPU, a side-effect free environment specialized for programming courses. It seamlessly guides and supports students during all phases of program development, covering specification, implementation, and program debugging. GUPU features several innovations in this area. The specification phase is supported by reference implementations augmented with diagnostic facilities. During implementation, immediate feedback from test cases and from visualization tools helps the programmer's program understanding. A set of slicing techniques narrows down programming errors. The whole process is guided by a marking system.",
        "published": "2002-07-11T16:57:59Z",
        "link": "http://arxiv.org/abs/cs/0207044v3",
        "categories": [
            "cs.SE",
            "D.1.6; D.2.5; D.2.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Exporting Prolog source code",
        "authors": [
            "Nicos Angelopoulos"
        ],
        "summary": "In this paper we present a simple source code configuration tool. ExLibris operates on libraries and can be used to extract from local libraries all code relevant to a particular project. Our approach is not designed to address problems arising in code production lines, but rather, to support the needs of individual or small teams of researchers who wish to communicate their Prolog programs. In the process, we also wish to accommodate and encourage the writing of reusable code. Moreover, we support and propose ways of dealing with issues arising in the development of code that can be run on a variety of like-minded Prolog systems. With consideration to these aims we have made the following decisions: (i) support file-based source development, (ii) require minimal program transformation, (iii) target simplicity of usage, and (iv) introduce minimum number of new primitives.",
        "published": "2002-07-11T17:17:17Z",
        "link": "http://arxiv.org/abs/cs/0207051v2",
        "categories": [
            "cs.SE",
            "D.1.6; D.2.5; D.2.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "CLPGUI: a generic graphical user interface for constraint logic   programming over finite domains",
        "authors": [
            "Francois Fages"
        ],
        "summary": "CLPGUI is a graphical user interface for visualizing and interacting with constraint logic programs over finite domains. In CLPGUI, the user can control the execution of a CLP program through several views of constraints, of finite domain variables and of the search tree. CLPGUI is intended to be used both for teaching purposes, and for debugging and improving complex programs of realworld scale. It is based on a client-server architecture for connecting the CLP process to a Java-based GUI process. Communication by message passing provides an open architecture which facilitates the reuse of graphical components and the porting to different constraint programming systems. Arbitrary constraints and goals can be posted incrementally from the GUI. We propose several dynamic 2D and 3D visualizations of the search tree and of the evolution of finite domain variables. We argue that the 3D representation of search trees proposed in this paper provides the most appropriate visualization of large search trees. We describe the current implementation of the annotations and of the interactive execution model in GNU-Prolog, and report some evaluation results.",
        "published": "2002-07-11T18:05:59Z",
        "link": "http://arxiv.org/abs/cs/0207048v2",
        "categories": [
            "cs.SE",
            "D.1.6; D.2.5; D.2.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Enhancing Usefulness of Declarative Programming Frameworks through   Complete Integration",
        "authors": [
            "Goran Falkman",
            "Olof Torgersson"
        ],
        "summary": "The Gisela framework for declarative programming was developed with the specific aim of providing a tool that would be useful for knowledge representation and reasoning within real-world applications. To achieve this, a complete integration into an object-oriented application development environment was used. The framework and methodology developed provide two alternative application programming interfaces (APIs): Programming using objects or programming using a traditional equational declarative style. In addition to providing complete integration, Gisela also allows extensions and modifications due to the general computation model and well-defined APIs. We give a brief overview of the declarative model underlying Gisela and we present the methodology proposed for building applications together with some real examples.",
        "published": "2002-07-12T01:17:13Z",
        "link": "http://arxiv.org/abs/cs/0207054v1",
        "categories": [
            "cs.SE",
            "D.1.6; D.2.5; D.2.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "An Architecture for Making Object-Oriented Systems Available from Prolog",
        "authors": [
            "Jan Wielemaker",
            "Anjo Anjewierden"
        ],
        "summary": "It is next to impossible to develop real-life applications in just pure Prolog. With XPCE we realised a mechanism for integrating Prolog with an external object-oriented system that turns this OO system into a natural extension to Prolog. We describe the design and how it can be applied to other external OO systems.",
        "published": "2002-07-12T01:22:20Z",
        "link": "http://arxiv.org/abs/cs/0207053v1",
        "categories": [
            "cs.SE",
            "D.1.6; D.2.5; D.2.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Proceedings of the 12th International Workshop on Logic Programming   Environments",
        "authors": [
            "Alexandre Tessier"
        ],
        "summary": "The twelfth Workshop on Logic Programming Environments, WLPE 2002, is one in a series of international workshops held in the topic area. The workshops facilitate the exchange ideas and results among researchers and system developers on all aspects of environments for logic programming. Relevant topics for these workshops include user interfaces, human engineering, execution visualization, development tools, providing for new paradigms, and interfacing to language system tools and external systems. This twelfth workshop held in Copenhaguen. It follows the successful eleventh Workshop on Logic Programming Environments held in Cyprus in December, 2001.   WLPE 2002 features ten presentations. The presentations involve, in some way, constraint logic programming, object-oriented programming and abstract interpretation. Topics areas addressed include tools for software development, execution visualization, software maintenance, instructional aids.   This workshop was a post-conference workshop at ICLP 2002.   Alexandre Tessier, Program Chair, WLPE 2002, June 2002.",
        "published": "2002-07-12T01:23:51Z",
        "link": "http://arxiv.org/abs/cs/0207052v1",
        "categories": [
            "cs.SE",
            "D.1.6; D.2.5; D.2.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Confluent Drawings: Visualizing Non-planar Diagrams in a Planar Way",
        "authors": [
            "Matthew Dickerson",
            "David Eppstein",
            "Michael T. Goodrich",
            "Jeremy Meng"
        ],
        "summary": "In this paper, we introduce a new approach for drawing diagrams that have applications in software visualization. Our approach is to use a technique we call confluent drawing for visualizing non-planar diagrams in a planar way. This approach allows us to draw, in a crossing-free manner, graphs--such as software interaction diagrams--that would normally have many crossings. The main idea of this approach is quite simple: we allow groups of edges to be merged together and drawn as \"tracks\" (similar to train tracks). Producing such confluent diagrams automatically from a graph with many crossings is quite challenging, however, so we offer two heuristic algorithms to test if a non-planar graph can be drawn efficiently in a confluent way. In addition, we identify several large classes of graphs that can be completely categorized as being either confluently drawable or confluently non-drawable.",
        "published": "2002-12-17T08:01:35Z",
        "link": "http://arxiv.org/abs/cs/0212046v1",
        "categories": [
            "cs.CG",
            "cs.SE",
            "D.2.2"
        ]
    },
    {
        "title": "An Empirical Model for Volatility of Returns and Option Pricing",
        "authors": [
            "Joseph L. McCauley",
            "Gemunu H. Gunaratne"
        ],
        "summary": "In a seminal paper in 1973, Black and Scholes argued how expected distributions of stock prices can be used to price options. Their model assumed a directed random motion for the returns and consequently a lognormal distribution of asset prices after a finite time. We point out two problems with their formulation. First, we show that the option valuation is not uniquely determined; in particular, stratergies based on the delta-hedge and CAMP (Capital Asset Pricing Model) are shown to provide different valuations of an option. Second, asset returns are known not to be Gaussian distributed. Empirically, distributions of returns are seen to be much better approximated by an exponential distribution. This exponential distribution of asset prices can be used to develop a new pricing model for options that is shown to provide valuations that agree very well with those used by traders. We show how the Fokker-Planck formulation of fluctuations (i.e., the dynamics of the distribution) can be modified to provide an exponential distribution for returns. We also show how a singular volatility can be used to go smoothly from exponential to Gaussian returns and thereby illustrate why exponential returns cannot be reached perturbatively starting from Gaussian ones, and explain how the theory of 'stochastic volatility' can be obtained from our model by making a bad approximation. Finally, we show how to calculate put and call prices for a stretched exponential density.",
        "published": "2002-01-29T18:03:39Z",
        "link": "http://arxiv.org/abs/cs/0201026v1",
        "categories": [
            "cs.CE",
            "G.3; J.1"
        ]
    },
    {
        "title": "A Qualitative Dynamical Modelling Approach to Capital Accumulation in   Unregulated Fisheries",
        "authors": [
            "K. Eisenack",
            "H. Welsch",
            "J. P. Kropp"
        ],
        "summary": "Capital accumulation has been a major issue in fisheries economics over the last two decades, whereby the interaction of the fish and capital stocks were of particular interest. Because bio-economic systems are intrinsically complex, previous efforts in this field have relied on a variety of simplifying assumptions. The model presented here relaxes some of these simplifications. Problems of tractability are surmounted by using the methodology of qualitative differential equations (QDE). The theory of QDEs takes into account that scientific knowledge about particular fisheries is usually limited, and facilitates an analysis of the global dynamics of systems with more than two ordinary differential equations. The model is able to trace the evolution of capital and fish stock in good agreement with observed patterns, and shows that over-capitalization is unavoidable in unregulated fisheries.",
        "published": "2002-02-05T17:50:56Z",
        "link": "http://arxiv.org/abs/cs/0202004v3",
        "categories": [
            "cs.AI",
            "cs.CE",
            "G.1.6; I.2.3; I.2.8; I.6.3; I.6.1; I.6.3"
        ]
    },
    {
        "title": "BSML: A Binding Schema Markup Language for Data Interchange in Problem   Solving Environments (PSEs)",
        "authors": [
            "Alex Verstak",
            "Naren Ramakrishnan",
            "Layne T. Watson",
            "Jian He",
            "Clifford A. Shaffer",
            "Kyung Kyoon Bae",
            "Jing Jiang",
            "William H. Tranter",
            "Theodore S. Rappaport"
        ],
        "summary": "We describe a binding schema markup language (BSML) for describing data interchange between scientific codes. Such a facility is an important constituent of scientific problem solving environments (PSEs). BSML is designed to integrate with a PSE or application composition system that views model specification and execution as a problem of managing semistructured data. The data interchange problem is addressed by three techniques for processing semistructured data: validation, binding, and conversion. We present BSML and describe its application to a PSE for wireless communications system design.",
        "published": "2002-02-18T16:01:03Z",
        "link": "http://arxiv.org/abs/cs/0202027v1",
        "categories": [
            "cs.CE",
            "cs.SE",
            "D.2.6; I.2.4"
        ]
    },
    {
        "title": "A numerical method for solution of ordinary differential equations of   fractional order",
        "authors": [
            "Leszczynski Jacek",
            "Ciesielski Mariusz"
        ],
        "summary": "In this paper we propose an algorithm for the numerical solution of arbitrary differential equations of fractional order. The algorithm is obtained by using the following decomposition of the differential equation into a system of differential equation of integer order connected with inverse forms of Abel-integral equations. The algorithm is used for solution of the linear and non-linear equations.",
        "published": "2002-02-26T15:51:23Z",
        "link": "http://arxiv.org/abs/math/0202276v1",
        "categories": [
            "math.NA",
            "cs.CE",
            "physics.comp-ph",
            "26A33, 45J05, 65D20, 65L05, 65L70"
        ]
    },
    {
        "title": "The efficient generation of unstructured control volumes in 2D and 3D",
        "authors": [
            "Leszczynski Jacek",
            "Pluta Sebastian"
        ],
        "summary": "Many problems in engineering, chemistry and physics require the representation of solutions in complex geometries. In the paper we deal with a problem of unstructured mesh generation for the control volume method. We propose an algorithm which bases on the spheres generation in central points of the control volumes.",
        "published": "2002-02-26T16:32:12Z",
        "link": "http://arxiv.org/abs/cs/0202038v1",
        "categories": [
            "cs.CG",
            "cs.CE",
            "cs.NA",
            "math.NA",
            "physics.comp-ph",
            "65N20, 68Q20, 68Q22, 76M20"
        ]
    },
    {
        "title": "Agent trade servers in financial exchange systems",
        "authors": [
            "David Lyback",
            "Magnus Boman"
        ],
        "summary": "New services based on the best-effort paradigm could complement the current deterministic services of an electronic financial exchange. Four crucial aspects of such systems would benefit from a hybrid stance: proper use of processing resources, bandwidth management, fault tolerance, and exception handling. We argue that a more refined view on Quality-of-Service control for exchange systems, in which the principal ambition of upholding a fair and orderly marketplace is left uncompromised, would benefit all interested parties.",
        "published": "2002-03-19T10:05:58Z",
        "link": "http://arxiv.org/abs/cs/0203023v1",
        "categories": [
            "cs.CE",
            "I.2.11; J.4; K.4.4"
        ]
    },
    {
        "title": "Anticorrelations and subdiffusion in financial systems",
        "authors": [
            "Kestutis Staliunas"
        ],
        "summary": "Statistical dynamics of financial systems is investigated, based on a model of a randomly coupled equation system driven by a stochastic Langevin force. Anticorrelations of price returns, and subdiffusion of prices is found from the model, and and compared with those calculated from historical $/EURO exchange rates.",
        "published": "2002-03-28T12:29:24Z",
        "link": "http://arxiv.org/abs/cond-mat/0203591v1",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CE",
            "q-fin.ST"
        ]
    },
    {
        "title": "Fast Universalization of Investment Strategies with Provably Good   Relative Returns",
        "authors": [
            "Karhan Akcoglu",
            "Petros Drineas",
            "Ming-Yang Kao"
        ],
        "summary": "A universalization of a parameterized investment strategy is an online algorithm whose average daily performance approaches that of the strategy operating with the optimal parameters determined offline in hindsight. We present a general framework for universalizing investment strategies and discuss conditions under which investment strategies are universalizable. We present examples of common investment strategies that fit into our framework. The examples include both trading strategies that decide positions in individual stocks, and portfolio strategies that allocate wealth among multiple stocks. This work extends Cover's universal portfolio work. We also discuss the runtime efficiency of universalization algorithms. While a straightforward implementation of our algorithms runs in time exponential in the number of parameters, we show that the efficient universal portfolio computation technique of Kalai and Vempala involving the sampling of log-concave functions can be generalized to other classes of investment strategies.",
        "published": "2002-04-10T03:13:03Z",
        "link": "http://arxiv.org/abs/cs/0204019v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2; G.3; I.2.6"
        ]
    },
    {
        "title": "Sampling Strategies for Mining in Data-Scarce Domains",
        "authors": [
            "Naren Ramakrishnan",
            "Chris Bailey-Kellogg"
        ],
        "summary": "Data mining has traditionally focused on the task of drawing inferences from large datasets. However, many scientific and engineering domains, such as fluid dynamics and aircraft design, are characterized by scarce data, due to the expense and complexity of associated experiments and simulations. In such data-scarce domains, it is advantageous to focus the data collection effort on only those regions deemed most important to support a particular data mining objective. This paper describes a mechanism that interleaves bottom-up data mining, to uncover multi-level structures in spatial data, with top-down sampling, to clarify difficult decisions in the mining process. The mechanism exploits relevant physical properties, such as continuity, correspondence, and locality, in a unified framework. This leads to effective mining and sampling decisions that are explainable in terms of domain knowledge and data characteristics. This approach is demonstrated in two diverse applications -- mining pockets in spatial data, and qualitative determination of Jordan forms of matrices.",
        "published": "2002-04-22T19:41:24Z",
        "link": "http://arxiv.org/abs/cs/0204047v2",
        "categories": [
            "cs.CE",
            "cs.AI",
            "D.2.6; G.1.2; G.1.3; G.3; I.2.10; I.5; H.2.8"
        ]
    },
    {
        "title": "Parrondo Strategies for Artificial Traders",
        "authors": [
            "Magnus Boman",
            "Stefan Johansson",
            "David Lyback"
        ],
        "summary": "On markets with receding prices, artificial noise traders may consider alternatives to buy-and-hold. By simulating variations of the Parrondo strategy, using real data from the Swedish stock market, we produce first indications of a buy-low-sell-random Parrondo variation outperforming buy-and-hold. Subject to our assumptions, buy-low-sell-random also outperforms the traditional value and trend investor strategies. We measure the success of the Parrondo variations not only through their performance compared to other kinds of strategies, but also relative to varying levels of perfect information, received through messages within a multi-agent system of artificial traders.",
        "published": "2002-04-26T12:20:08Z",
        "link": "http://arxiv.org/abs/cs/0204051v1",
        "categories": [
            "cs.CE",
            "I.2.11; J.4; K.4.4"
        ]
    },
    {
        "title": "Qualitative Analysis of Correspondence for Experimental Algorithmics",
        "authors": [
            "Chris Bailey-Kellogg",
            "Naren Ramakrishnan"
        ],
        "summary": "Correspondence identifies relationships among objects via similarities among their components; it is ubiquitous in the analysis of spatial datasets, including images, weather maps, and computational simulations. This paper develops a novel multi-level mechanism for qualitative analysis of correspondence. Operators leverage domain knowledge to establish correspondence, evaluate implications for model selection, and leverage identified weaknesses to focus additional data collection. The utility of the mechanism is demonstrated in two applications from experimental algorithmics -- matrix spectral portrait analysis and graphical assessment of Jordan forms of matrices. Results show that the mechanism efficiently samples computational experiments and successfully uncovers high-level problem properties. It overcomes noise and data sparsity by leveraging domain knowledge to detect mutually reinforcing interpretations of spatial data.",
        "published": "2002-04-26T17:25:51Z",
        "link": "http://arxiv.org/abs/cs/0204053v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "D.2.6; G.1.2; G.1.3; G.3; I.2.10; I.5; H.2.8"
        ]
    },
    {
        "title": "Trading Agents for Roaming Users",
        "authors": [
            "Magnus Boman",
            "Markus Bylund",
            "Fredrik Espinoza",
            "Mats Danielson",
            "David Lyback"
        ],
        "summary": "Some roaming users need services to manipulate autonomous processes. Trading agents running on agent trade servers are used as a case in point. We present a solution that provides the agent owners with means to upkeeping their desktop environment, and maintaining their agent trade server processes, via a briefcase service.",
        "published": "2002-04-29T12:20:11Z",
        "link": "http://arxiv.org/abs/cs/0204056v1",
        "categories": [
            "cs.CE",
            "I.2.11; K.8"
        ]
    },
    {
        "title": "Distance function wavelets - Part I: Helmholtz and convection-diffusion   transforms and series",
        "authors": [
            "W. Chen"
        ],
        "summary": "This report aims to present my research updates on distance function wavelets (DFW) based on the fundamental solutions and the general solutions of the Helmholtz, modified Helmholtz, and convection-diffusion equations, which include the isotropic Helmholtz-Fourier (HF) transform and series, the Helmholtz-Laplace (HL) transform, and the anisotropic convection-diffusion wavelets and ridgelets. The latter is set to handle discontinuous and track data problems. The edge effect of the HF series is addressed. Alternative existence conditions for the DFW transforms are proposed and discussed. To simplify and streamline the expression of the HF and HL transforms, a new dimension-dependent function notation is introduced. The HF series is also used to evaluate the analytical solutions of linear diffusion problems of arbitrary dimensionality and geometry. The weakness of this report is lacking of rigorous mathematical analysis due to the author's limited mathematical knowledge.",
        "published": "2002-05-14T13:43:47Z",
        "link": "http://arxiv.org/abs/cs/0205019v1",
        "categories": [
            "cs.CE",
            "cs.NA",
            "G.1"
        ]
    },
    {
        "title": "A quasi-RBF technique for numerical discretization of PDE's",
        "authors": [
            "W. Chen"
        ],
        "summary": "Atkinson developed a strategy which splits solution of a PDE system into homogeneous and particular solutions, where the former have to satisfy the boundary and governing equation, while the latter only need to satisfy the governing equation without concerning geometry. Since the particular solution can be solved irrespective of boundary shape, we can use a readily available fast Fourier or orthogonal polynomial technique O(NlogN) to evaluate it in a regular box or sphere surrounding physical domain. The distinction of this study is that we approximate homogeneous solution with nonsingular general solution RBF as in the boundary knot method. The collocation method using general solution RBF has very high accuracy and spectral convergent speed and is a simple, truly meshfree approach for any complicated geometry. More importantly, the use of nonsingular general solution avoids the controversial artificial boundary in the method of fundamental solution due to the singularity of fundamental solution.",
        "published": "2002-05-14T13:58:48Z",
        "link": "http://arxiv.org/abs/cs/0205020v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "Data-Collection for the Sloan Digital Sky Survey: a Network-Flow   Heuristic",
        "authors": [
            "Robert Lupton",
            "Miller Maley",
            "Neal Young"
        ],
        "summary": "The goal of the Sloan Digital Sky Survey is ``to map in detail one-quarter of the entire sky, determining the positions and absolute brightnesses of more than 100 million celestial objects''. The survey will be performed by taking ``snapshots'' through a large telescope. Each snapshot can capture up to 600 objects from a small circle of the sky. This paper describes the design and implementation of the algorithm that is being used to determine the snapshots so as to minimize their number. The problem is NP-hard in general; the algorithm described is a heuristic, based on Lagriangian-relaxation and min-cost network flow. It gets within 5-15% of a naive lower bound, whereas using a ``uniform'' cover only gets within 25-35%.",
        "published": "2002-05-18T03:29:33Z",
        "link": "http://arxiv.org/abs/cs/0205034v1",
        "categories": [
            "cs.DS",
            "cs.CE",
            "F.2.1; G.1.6; J.2"
        ]
    },
    {
        "title": "Distance function wavelets - Part II: Extended results and conjectures",
        "authors": [
            "W. Chen"
        ],
        "summary": "Report II is concerned with the extended results of distance function wavelets (DFW). The fractional DFW transforms are first addressed relating to the fractal geometry and fractional derivative, and then, the discrete Helmholtz-Fourier transform is briefly presented. The Green second identity may be an alternative devise in developing the theoretical framework of the DFW transform and series. The kernel solutions of the Winkler plate equation and the Burger's equation are used to create the DFW transforms and series. Most interestingly, it is found that the translation invariant monomial solutions of the high-order Laplace equations can be used to make very simple harmonic polynomial DFW series. In most cases of this study, solid mathematical analysis is missing and results are obtained intuitively in the conjecture status.",
        "published": "2002-05-24T12:07:28Z",
        "link": "http://arxiv.org/abs/cs/0205063v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.9"
        ]
    },
    {
        "title": "High-order fundamental and general solutions of convection-diffusion   equation and their applications with boundary particle method",
        "authors": [
            "W. Chen"
        ],
        "summary": "In this study, we presented the high-order fundamental solutions and general solutions of convection-diffusion equation. To demonstrate their efficacy, we applied the high-order general solutions to the boundary particle method (BPM) for the solution of some inhomogeneous convection-diffusion problems, where the BPM is a new truly boundary-only meshfree collocation method based on multiple reciprocity principle. For the sake of completeness, the BPM is also briefly described here.",
        "published": "2002-06-08T10:46:56Z",
        "link": "http://arxiv.org/abs/cs/0206013v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "Distance function wavelets - Part III: \"Exotic\" transforms and series",
        "authors": [
            "W. Chen"
        ],
        "summary": "Part III of the reports consists of various unconventional distance function wavelets (DFW). The dimension and the order of partial differential equation (PDE) are first used as a substitute of the scale parameter in the DFW transforms and series, especially with the space and time-space potential problems. It is noted that the recursive multiple reciprocity formulation is the DFW series. The Green second identity is used to avoid the singularity of the zero-order fundamental solution in creating the DFW series. The fundamental solutions of various composite PDEs are found very flexible and efficient to handle a borad range of problems. We also discuss the underlying connections between the crucial concepts of dimension, scale and the order of PDE through the analysis of dissipative acoustic wave propagation. The shape parameter of the potential problems is also employed as the \"scale parameter\" to create the non-orthogonal DFW. This paper also briefly discusses and conjectures the DFW correspondences of a variety of coordinate variable transforms and series. Practically important, the anisotropic and inhomogeneous DFW's are developed by using the geodesic distance variable. The DFW and the related basis functions are also used in making the kernel distance sigmoidal functions, which are potentially useful in the artificial neural network and machine learning. As or even worse than the preceding two reports, this study scarifies mathematical rigor and in turn unfetter imagination. Most results are intuitively obtained without rigorous analysis. Follow-up research is still under way. The paper is intended to inspire more research into this promising area.",
        "published": "2002-06-10T09:01:53Z",
        "link": "http://arxiv.org/abs/cs/0206016v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.9"
        ]
    },
    {
        "title": "Hidden Markov model segmentation of hydrological and enviromental time   series",
        "authors": [
            "Ath. Kehagias"
        ],
        "summary": "Motivated by Hubert's segmentation procedure we discuss the application of hidden Markov models (HMM) to the segmentation of hydrological and enviromental time series. We use a HMM algorithm which segments time series of several hundred terms in a few seconds and is computationally feasible for even longer time series. The segmentation algorithm computes the Maximum Likelihood segmentation by use of an expectation / maximization iteration. We rigorously prove algorithm convergence and use numerical experiments, involving temperature and river discharge time series, to show that the algorithm usually converges to the globally optimal segmentation. The relation of the proposed algorithm to Hubert's segmentation procedure is also discussed.",
        "published": "2002-06-25T09:12:16Z",
        "link": "http://arxiv.org/abs/cs/0206039v1",
        "categories": [
            "cs.CE",
            "cs.NA",
            "math.NA",
            "nlin.CD",
            "physics.data-an",
            "G.3; I.5"
        ]
    },
    {
        "title": "National Infrastructure Contingencies: Survey of Wireless Technology   Support",
        "authors": [
            "Ronald M. Fussell"
        ],
        "summary": "In modern society, the flow of information has become the lifeblood of commerce and social interaction. This movement of data supports most aspects of the United States economy in particular, as well as, serving as the vehicle upon which governmental agencies react to social conditions. In addition, it is understood that the continuance of efficient and reliable data communications during times of national or regional disaster remains a priority in the United States. The coordination of emergency response and area revitalization / rehabilitation efforts between local, state, and federal emergency response is increasingly necessary as agencies strive to work more seamlessly between the affected organizations. Additionally, international support is often made available to react to such adverse conditions as wildfire suppression scenarios and therefore require the efficient management of workforce and associated logistics support.   It is through the examination of the issues related to un-tethered data transmission during infrastructure contingencies that responders may best tailor a unified approach to the rapid recovery after disasters occur.",
        "published": "2002-07-01T18:09:44Z",
        "link": "http://arxiv.org/abs/cs/0207001v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "C.2.5"
        ]
    },
    {
        "title": "Symmetric boundary knot method",
        "authors": [
            "W. Chen"
        ],
        "summary": "The boundary knot method (BKM) is a recent boundary-type radial basis function (RBF) collocation scheme for general PDEs. Like the method of fundamental solution (MFS), the RBF is employed to approximate the inhomogeneous terms via the dual reciprocity principle. Unlike the MFS, the method uses a nonsingular general solution instead of a singular fundamental solution to evaluate the homogeneous solution so as to circumvent the controversial artificial boundary outside the physical domain. The BKM is meshfree, superconvergent, integration free, very easy to learn and program. The original BKM, however, loses symmetricity in the presense of mixed boundary. In this study, by analogy with Hermite RBF interpolation, we developed a symmetric BKM scheme. The accuracy and efficiency of the symmetric BKM are also numerically validated in some 2D and 3D Helmholtz and diffusion reaction problems under complicated geometries.",
        "published": "2002-07-03T20:17:31Z",
        "link": "http://arxiv.org/abs/cs/0207010v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.9"
        ]
    },
    {
        "title": "New advances in dual reciprocity and boundary-only RBF methods",
        "authors": [
            "W. Chen",
            "M. Tanaka"
        ],
        "summary": "This paper made some significant advances in the dual reciprocity and boundary-only RBF techniques. The proposed boundary knot method (BKM) is different from the standard boundary element method in a number of important aspects. Namely, it is truly meshless, exponential convergence, integration-free (of course, no singular integration), boundary-only for general problems, and leads to symmetric matrix under certain conditions (able to be extended to general cases after further modified). The BKM also avoids the artificial boundary in the method of fundamental solution. An amazing finding is that the BKM can formulate linear modeling equations for nonlinear partial differential systems with linear boundary conditions. This merit makes it circumvent all perplexing issues in the iteration solution of nonlinear equations. On the other hand, by analogy with Green's second identity, this paper also presents a general solution RBF (GSR) methodology to construct efficient RBFs in the dual reciprocity and domain-type RBF collocation methods. The GSR approach first establishes an explicit relationship between the BEM and RBF itself on the ground of the weighted residual principle. This paper also discusses the RBF convergence and stability problems within the framework of integral equation theory.",
        "published": "2002-07-04T12:10:06Z",
        "link": "http://arxiv.org/abs/cs/0207015v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "Relationship between boundary integral equation and radial basis   function",
        "authors": [
            "W. Chen",
            "M. Tanaka"
        ],
        "summary": "This paper aims to survey our recent work relating to the radial basis function (RBF) from some new views of points. In the first part, we established the RBF on numerical integration analysis based on an intrinsic relationship between the Green's boundary integral representation and RBF. It is found that the kernel function of integral equation is important to create efficient RBF. The fundamental solution RBF (FS-RBF) was presented as a novel strategy constructing operator-dependent RBF. We proposed a conjecture formula featuring the dimension affect on error bound to show the independent-dimension merit of the RBF techniques. We also discussed wavelet RBF, localized RBF schemes, and the influence of node placement on the RBF solution accuracy. The centrosymmetric matrix structure of the RBF interpolation matrix under symmetric node placing is proved.   The second part of this paper is concerned with the boundary knot method (BKM), a new boundary-only, meshless, spectral convergent, integration-free RBF collocation technique. The BKM was tested to the Helmholtz, Laplace, linear and nonlinear convection-diffusion problems. In particular, we introduced the response knot-dependent nonsingular general solution to calculate varying-parameter and nonlinear steady convection-diffusion problems very efficiently. By comparing with the multiple dual reciprocity method, we discussed the completeness issue of the BKM.   Finally, the nonsingular solutions for some known differential operators were given in appendix. Also we expanded the RBF concepts by introducing time-space RBF for transient problems.",
        "published": "2002-07-04T12:15:11Z",
        "link": "http://arxiv.org/abs/cs/0207016v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.9"
        ]
    },
    {
        "title": "New Insights in Boundary-only and Domain-type RBF Methods",
        "authors": [
            "W. Chen",
            "M. Tanaka"
        ],
        "summary": "This paper has made some significant advances in the boundary-only and domain-type RBF techniques. The proposed boundary knot method (BKM) is different from the standard boundary element method in a number of important aspects. Namely, it is truly meshless, exponential convergence, integration-free (of course, no singular integration), boundary-only for general problems, and leads to symmetric matrix under certain conditions (able to be extended to general cases after further modified). The BKM also avoids the artificial boundary in the method of fundamental solution. An amazing finding is that the BKM can formulate linear modeling equations for nonlinear partial differential systems with linear boundary conditions. This merit makes it circumvent all perplexing issues in the iteration solution of nonlinear equations. On the other hand, by analogy with Green's second identity, we also presents a general solution RBF (GSR) methodology to construct efficient RBFs in the domain-type RBF collocation method and dual reciprocity method. The GSR approach first establishes an explicit relationship between the BEM and RBF itself on the ground of the potential theory. This paper also discusses some essential issues relating to the RBF computing, which include time-space RBFs, direct and indirect RBF schemes, finite RBF method, and the application of multipole and wavelet to the RBF solution of the PDEs.",
        "published": "2002-07-04T12:18:06Z",
        "link": "http://arxiv.org/abs/cs/0207017v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "Definitions of distance function in radial basis function approach",
        "authors": [
            "W. Chen"
        ],
        "summary": "Very few studies involve how to construct the efficient RBFs by means of problem features. Recently the present author presented general solution RBF (GS-RBF) methodology to create operator-dependent RBFs successfully [1]. On the other hand, the normal radial basis function (RBF) is defined via Euclidean space distance function or the geodesic distance [2]. This purpose of this note is to redefine distance function in conjunction with problem features, which include problem-dependent and time-space distance function.",
        "published": "2002-07-04T12:20:24Z",
        "link": "http://arxiv.org/abs/cs/0207018v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "Reducing the Computational Requirements of the Differential Quadrature   Method",
        "authors": [
            "W Chen",
            "Xinwei Wang",
            "Yongxi Yu"
        ],
        "summary": "This paper shows that the weighting coefficient matrices of the differential quadrature method (DQM) are centrosymmetric or skew-centrosymmetric if the grid spacings are symmetric irrespective of whether they are equal or unequal. A new skew centrosymmetric matrix is also discussed. The application of the properties of centrosymmetric and skew centrosymmetric matrix can reduce the computational effort of the DQM for calculations of the inverse, determinant, eigenvectors and eigenvalues by 75%. This computational advantage are also demonstrated via several numerical examples.",
        "published": "2002-07-09T19:53:42Z",
        "link": "http://arxiv.org/abs/cs/0207033v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "A Lyapunov Formulation for Efficient Solution of the Poisson and   Convection-Diffusion Equations by the Differential Quadrature Method",
        "authors": [
            "W. Chen",
            "Tingxiu Zhong"
        ],
        "summary": "Civan and Sliepcevich [1, 2] suggested that special matrix solver should be developed to further reduce the computing effort in applying the differential quadrature (DQ) method for the Poisson and convection-diffusion equations. Therefore, the purpose of the present communication is to introduce and apply the Lyapunov formulation which can be solved much more efficiently than the Gaussian elimination method. Civan and Sliepcevich [2] first presented DQ approximate formulas in polynomial form for partial derivatives in tow-dimensional variable domain. For simplifying formulation effort, Chen et al. [3] proposed the compact matrix form of these DQ approximate formulas. In this study, by using these matrix approximate formulas, the DQ formulations for the Poisson and convection-diffusion equations can be expressed as the Lyapunov algebraic matrix equation. The formulation effort is simplified, and a simple and explicit matrix formulation is obtained. A variety of fast algorithms in the solution of the Lyapunov equation [4-6] can be successfully applied in the DQ analysis of these two-dimensional problems, and, thus, the computing effort can be greatly reduced. Finally, we also point out that the present reduction technique can be easily extended to the three-dimensional cases.",
        "published": "2002-07-09T20:26:18Z",
        "link": "http://arxiv.org/abs/cs/0207035v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.2"
        ]
    },
    {
        "title": "Dual reciprocity BEM and dynamic programming filter for inverse   elastodynamic problems",
        "authors": [
            "Masataka Tanaka",
            "W Chen"
        ],
        "summary": "This paper presents the first coupling application of the dual reciprocity BEM (DRBEM) and dynamic programming filter to inverse elastodynamic problem. The DRBEM is the only BEM method, which does not require domain discretization for general linear and nonlinear dynamic problems. Since the size of numerical discretization system has a great effect on the computing effort of recursive or iterative calculations of inverse analysis, the intrinsic boundary-only merit of the DRBEM causes a considerable computational saving. On the other hand, the strengths of the dynamic programming filter lie in its mathematical simplicity, easy to program and great flexibility in the type, number and locations of measurements and unknown inputs. The combination of these two techniques is therefore very attractive for the solution of practical inverse problems. In this study, the spatial and temporal partial derivatives of the governing equation are respectively discretized first by the DRBEM and the precise integration method, and then, by using dynamic programming with regularization, dynamic load is estimated based on noisy measurements of velocity and displacement at very few locations. Numerical experiments involved with the periodic and Heaviside impact load are conducted to demonstrate the applicability, efficiency and simplicity of this strategy. The affect of noise level, regularization parameter, and measurement types on the estimation is also investigated.",
        "published": "2002-07-10T08:00:42Z",
        "link": "http://arxiv.org/abs/cs/0207039v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.2"
        ]
    },
    {
        "title": "RBF-based meshless boundary knot method and boundary particle method",
        "authors": [
            "W. Chen"
        ],
        "summary": "This paper is concerned with the two new boundary-type radial basis function collocation schemes, boundary knot method (BKM) and boundary particle method (BPM). The BKM is developed based on the dual reciprocity theorem, while the BPM employs the multiple reciprocity technique. Unlike the method of fundamental solution, the wto methods use the nonsingular general solutions instead of singular fundamental solution to circumvent the controversial artificial boundary outside physical domain. Compared with the boundary element method, both the BKM and BPM are meshfree, superconvergent, meshfree, integration free, symmetric, and mathematically simple collocation techniques for general PDEs. In particular, the BPM does not require any inner nodes for inhomogeneous problems. In this study, the accuracy and efficiency of the two methods are numerically demonstrated to some 2D, 3D Helmholtz and convection-diffusion problems under complicated geometries.",
        "published": "2002-07-10T20:31:29Z",
        "link": "http://arxiv.org/abs/cs/0207041v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "A meshless, integration-free, and boundary-only RBF technique",
        "authors": [
            "W. Chen",
            "M. Tanaka"
        ],
        "summary": "Based on the radial basis function (RBF), non-singular general solution and dual reciprocity method (DRM), this paper presents an inherently meshless, integration-free, boundary-only RBF collocation techniques for numerical solution of various partial differential equation systems. The basic ideas behind this methodology are very mathematically simple. In this study, the RBFs are employed to approximate the inhomogeneous terms via the DRM, while non-singular general solution leads to a boundary-only RBF formulation for homogenous solution. The present scheme is named as the boundary knot method (BKM) to differentiate it from the other numerical techniques. In particular, due to the use of nonsingular general solutions rather than singular fundamental solutions, the BKM is different from the method of fundamental solution in that the former does no require the artificial boundary and results in the symmetric system equations under certain conditions. The efficiency and utility of this new technique are validated through a number of typical numerical examples. Completeness concern of the BKM due to the only use of non-singular part of complete fundamental solution is also discussed.",
        "published": "2002-07-11T12:19:49Z",
        "link": "http://arxiv.org/abs/cs/0207043v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "Some addenda on distance function wavelets",
        "authors": [
            "W. Chen"
        ],
        "summary": "This report will add some supplements to the recently finished report series on the distance function wavelets (DFW). First, we define the general distance in terms of the Riesz potential, and then, the distance function Abel wavelets are derived via the fractional integral and Laplacian. Second, the DFW Weyl transform is found to be a shifted Laplace potential DFW. The DFW Radon transform is also presented. Third, we present a conjecture on truncation error formula of the multiple reciprocity Laplace DFW series and discuss its error distributions in terms of node density distributions. Forth, we point out that the Hermite distance function interpolation can be used to replace overlapping in the domain decomposition in order to produce sparse matrix. Fifth, the shape parameter is explained as a virtual extra axis contribution in terms of the MQ-type Possion kernel. The report is concluded with some remarks on a range of other issues.",
        "published": "2002-07-15T19:58:27Z",
        "link": "http://arxiv.org/abs/cs/0207062v1",
        "categories": [
            "cs.NA",
            "cs.CE",
            "G1.8, G1.2"
        ]
    },
    {
        "title": "Petabyte Scale Data Mining: Dream or Reality?",
        "authors": [
            "Alexander S. Szalay",
            "Jim Gray",
            "Jan vandenBerg"
        ],
        "summary": "Science is becoming very data intensive1. Today's astronomy datasets with tens of millions of galaxies already present substantial challenges for data mining. In less than 10 years the catalogs are expected to grow to billions of objects, and image archives will reach Petabytes. Imagine having a 100GB database in 1996, when disk scanning speeds were 30MB/s, and database tools were immature. Such a task today is trivial, almost manageable with a laptop. We think that the issue of a PB database will be very similar in six years. In this paper we scale our current experiments in data archiving and analysis on the Sloan Digital Sky Survey2,3 data six years into the future. We analyze these projections and look at the requirements of performing data mining on such data sets. We conclude that the task scales rather well: we could do the job today, although it would be expensive. There do not seem to be any show-stoppers that would prevent us from storing and using a Petabyte dataset six years from today.",
        "published": "2002-08-07T22:49:56Z",
        "link": "http://arxiv.org/abs/cs/0208013v1",
        "categories": [
            "cs.DB",
            "cs.CE",
            "H.2.8;J.2"
        ]
    },
    {
        "title": "A note on fractional derivative modeling of broadband   frequency-dependent absorption: Model III",
        "authors": [
            "W. Chen"
        ],
        "summary": "By far, the fractional derivative model is mainly related to the modelling of complicated solid viscoelastic material. In this study, we try to build the fractional derivative PDE model for broadband ultrasound propagation through human tissues.",
        "published": "2002-08-08T07:29:10Z",
        "link": "http://arxiv.org/abs/cs/0208016v1",
        "categories": [
            "cs.CE",
            "cs.CC",
            "G1.2, G1.8"
        ]
    },
    {
        "title": "Symbolic Methodology in Numeric Data Mining: Relational Techniques for   Financial Applications",
        "authors": [
            "B. Kovalerchuk",
            "E. Vityaev",
            "H. Yusupov"
        ],
        "summary": "Currently statistical and artificial neural network methods dominate in financial data mining. Alternative relational (symbolic) data mining methods have shown their effectiveness in robotics, drug design and other applications. Traditionally symbolic methods prevail in the areas with significant non-numeric (symbolic) knowledge, such as relative location in robot navigation. At first glance, stock market forecast looks as a pure numeric area irrelevant to symbolic methods. One of our major goals is to show that financial time series can benefit significantly from relational data mining based on symbolic methods. The paper overviews relational data mining methodology and develops this techniques for financial data mining.",
        "published": "2002-08-15T03:45:36Z",
        "link": "http://arxiv.org/abs/cs/0208022v1",
        "categories": [
            "cs.CE",
            "I.2.6"
        ]
    },
    {
        "title": "A direct time-domain FEM modeling of broadband frequency-dependent   absorption with the presence of matrix fractional power: Model I",
        "authors": [
            "W Chen"
        ],
        "summary": "The frequency-dependent attenuation of broadband acoustics is often confronted in many different areas. However, the related time domain simulation is rarely found in literature due to enormous technical difficulty. The currently popular relaxation models with the presence of convolution operation require some material parameters which are not readily available. In this study, three reports are contributed to address broadband ultrasound frequency-dependent absorptions using the readily available empirical parameters. This report is the first in series concerned with developing a direct time domain FEM formulation. The next two reports are about the frequency decomposition model and the fractional derivative model.",
        "published": "2002-08-20T12:36:06Z",
        "link": "http://arxiv.org/abs/cs/0208030v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G1.8, G1.9"
        ]
    },
    {
        "title": "Using Hierarchical Data Mining to Characterize Performance of Wireless   System Configurations",
        "authors": [
            "Alex Verstak",
            "Naren Ramakrishnan",
            "Kyung Kyoon Bae",
            "William H. Tranter",
            "Layne T. Watson",
            "Jian He",
            "Clifford A. Shaffer",
            "Theodore S. Rappaport"
        ],
        "summary": "This paper presents a statistical framework for assessing wireless systems performance using hierarchical data mining techniques. We consider WCDMA (wideband code division multiple access) systems with two-branch STTD (space time transmit diversity) and 1/2 rate convolutional coding (forward error correction codes). Monte Carlo simulation estimates the bit error probability (BEP) of the system across a wide range of signal-to-noise ratios (SNRs). A performance database of simulation runs is collected over a targeted space of system configurations. This database is then mined to obtain regions of the configuration space that exhibit acceptable average performance. The shape of the mined regions illustrates the joint influence of configuration parameters on system performance. The role of data mining in this application is to provide explainable and statistically valid design conclusions. The research issue is to define statistically meaningful aggregation of data in a manner that permits efficient and effective data mining algorithms. We achieve a good compromise between these goals and help establish the applicability of data mining for characterizing wireless systems performance.",
        "published": "2002-08-25T16:28:33Z",
        "link": "http://arxiv.org/abs/cs/0208040v1",
        "categories": [
            "cs.CE",
            "I.6.4"
        ]
    },
    {
        "title": "A Novel Statistical Diagnosis of Clinical Data",
        "authors": [
            "Gene Kim",
            "MyungHo Kim"
        ],
        "summary": "In this paper, we present a diagnosis method of diseases from clinical data. The data are routine test such as urine test, hematology, chemistries etc. Though those tests have been done for people who check in medical institutes, how each item of the data interacts each other and which combination of them cause a disease are neither understood nor studied well. Here we attack the practically important problem by putting the data into mathematical setup and applying support vector machine. Finally we present simulation results for fatty liver, gastritis etc and discuss about their implications.",
        "published": "2002-09-02T03:52:48Z",
        "link": "http://arxiv.org/abs/cs/0209001v1",
        "categories": [
            "cs.CE",
            "cs.CC",
            "I.1.2; H.1.1; I.5.0"
        ]
    },
    {
        "title": "A new definition of the fractional Laplacian",
        "authors": [
            "W. Chen"
        ],
        "summary": "It is noted that the standard definition of the fractional Laplacian leads to a hyper-singular convolution integral and is also obscure about how to implement the boundary conditions. This purpose of this note is to introduce a new definition of the fractional Laplacian to overcome these major drawbacks.",
        "published": "2002-09-18T12:45:43Z",
        "link": "http://arxiv.org/abs/cs/0209020v1",
        "categories": [
            "cs.NA",
            "cs.CE",
            "G.1.8; G.1.9"
        ]
    },
    {
        "title": "The calculation of a normal force between multiparticle contacts using   fractional operators",
        "authors": [
            "Leszczynski Jacek"
        ],
        "summary": "This paper deals with the complex problem of how to simulate multiparticle contacts. The collision process is responsible for the transfer and dissipation of energy in granular media. A novel model of the interaction force between particles has been proposed and tested. Such model allows us to simulate multiparticle collisions and granular cohesion dynamics.",
        "published": "2002-09-23T18:52:56Z",
        "link": "http://arxiv.org/abs/physics/0209085v1",
        "categories": [
            "physics.comp-ph",
            "cs.CE",
            "cs.NA",
            "math.NA",
            "physics.class-ph",
            "physics.geo-ph"
        ]
    },
    {
        "title": "Positive time fractional derivative",
        "authors": [
            "W Chen"
        ],
        "summary": "In mathematical modeling of the non-squared frequency-dependent diffusions, also known as the anomalous diffusions, it is desirable to have a positive real Fourier transform for the time derivative of arbitrary fractional or odd integer order. The Fourier transform of the fractional time derivative in the Riemann-Liouville and Caputo senses, however, involves a complex power function of the fractional order. In this study, a positive time derivative of fractional or odd integer order is introduced to respect the positivity in modeling the anomalous diffusions.",
        "published": "2002-10-07T19:28:50Z",
        "link": "http://arxiv.org/abs/cs/0210005v1",
        "categories": [
            "cs.CE",
            "G.1.8; G.1.9"
        ]
    },
    {
        "title": "Simple Model for the Dynamics of Correlations in the Evolution of   Economic Entities Under Varying Economic Conditions",
        "authors": [
            "Marcel Ausloos",
            "Paulette Clippe",
            "Andrzej Pekalski"
        ],
        "summary": "From some observations on economic behaviors, in particular changing economic conditions with time and space, we develop a very simple model for the evolution of economic entities within a geographical type of framework. We raise a few questions and attempt to investigate whether some of them can be tackled by our model. Several cases of interest are reported. It is found that the model even in its simple forms can lead to a large variety of situations, including: delocalization and cycles, but also pre-chaotic behavior.",
        "published": "2002-10-18T11:35:49Z",
        "link": "http://arxiv.org/abs/nlin/0210041v1",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.CE",
            "physics.soc-ph"
        ]
    },
    {
        "title": "User software for the next generation",
        "authors": [
            "T. G. Worlton",
            "A. Chatterjee",
            "J. P. Hammonds",
            "P. F. Peterson",
            "D. J. Mikkelson",
            "R. L. Mikkelson"
        ],
        "summary": "New generations of neutron scattering sources and instrumentation are providing challenges in data handling for user software. Time-of-Flight instruments used at pulsed sources typically produce hundreds or thousands of channels of data for each detector segment. New instruments are being designed with thousands to hundreds of thousands of detector segments. High intensity neutron sources make possible parametric studies and texture studies which further increase data handling requirements. The Integrated Spectral Analysis Workbench (ISAW) software developed at Argonne handles large numbers of spectra simultaneously while providing operations to reduce, sort, combine and export the data. It includes viewers to inspect the data in detail in real time. ISAW uses existing software components and packages where feasible and takes advantage of the excellent support for user interface design and network communication in Java. The included scripting language simplifies repetitive operations for analyzing many files related to a given experiment. Recent additions to ISAW include a contour view, a time-slice table view, routines for finding and fitting peaks in data, and support for data from other facilities using the NeXus format. In this paper, I give an overview of features and planned improvements of ISAW. Details of some of the improvements are covered in other presentations at this conference.",
        "published": "2002-10-19T01:27:45Z",
        "link": "http://arxiv.org/abs/cs/0210018v1",
        "categories": [
            "cs.GR",
            "cs.CE",
            "J2;I3.6;I3.3"
        ]
    },
    {
        "title": "Exploring the cooperative regimes in a model of agents without memory or   \"tags\": indirect reciprocity vs. selfish incentives",
        "authors": [
            "H. Fort"
        ],
        "summary": "The self-organization in cooperative regimes in a simple mean-field version of a model based on \"selfish\" agents which play the Prisoner's Dilemma (PD) game is studied. The agents have no memory and use strategies not based on direct reciprocity nor 'tags'. Two variables are assigned to each agent $i$ at time $t$, measuring its capital $C(i;t)$ and its probability of cooperation $p(i;t)$. At each time step $t$ a pair of agents interact by playing the PD game. These 2 agents update their probability of cooperation $p(i)$ as follows: they compare the profits they made in this interaction $\\delta C(i;t)$ with an estimator $\\epsilon(i;t)$ and, if $\\delta C(i;t) \\ge \\epsilon(i;t)$, agent $i$ increases its $p(i;t)$ while if $\\delta C(i;t) < \\epsilon(i;t)$ the agent decreases $p(i;t)$. The 4!=24 different cases produced by permuting the four Prisoner's Dilemma canonical payoffs 3, 0, 1, and 5 - corresponding,respectively, to $R$ (reward), $S$ (sucker's payoff), $T$ (temptation to defect) and $P$ (punishment) - are analyzed. It turns out that for all these 24 possibilities, after a transient,the system self-organizes into a stationary state with average equilibrium probability of cooperation $\\bar{p}_\\infty$ = constant $ > 0$.Depending on the payoff matrix, there are different equilibrium states characterized by their average probability of cooperation and average equilibrium per-capita-income ($\\bar{p}_\\infty,\\bar{\\delta C}_\\infty$).",
        "published": "2002-11-15T17:48:45Z",
        "link": "http://arxiv.org/abs/nlin/0211024v1",
        "categories": [
            "nlin.AO",
            "cond-mat",
            "cs.CE",
            "hep-lat",
            "nlin.CG",
            "physics.soc-ph"
        ]
    },
    {
        "title": "SkyQuery: A WebService Approach to Federate Databases",
        "authors": [
            "Tanu Malik",
            "Alex S. Szalay",
            "Tamas Budavari",
            "Ani R. Thakar"
        ],
        "summary": "Traditional science searched for new objects and phenomena that led to discoveries. Tomorrow's science will combine together the large pool of information in scientific archives and make discoveries. Scienthists are currently keen to federate together the existing scientific databases. The major challenge in building a federation of these autonomous and heterogeneous databases is system integration. Ineffective integration will result in defunct federations and under utilized scientific data.   Astronomy, in particular, has many autonomous archives spread over the Internet. It is now seeking to federate these, with minimal effort, into a Virtual Observatory that will solve complex distributed computing tasks such as answering federated spatial join queries.   In this paper, we present SkyQuery, a successful prototype of an evolving federation of astronomy archives. It interoperates using the emerging Web services standard. We describe the SkyQuery architecture and show how it efficiently evaluates a probabilistic federated spatial join query.",
        "published": "2002-11-20T04:54:19Z",
        "link": "http://arxiv.org/abs/cs/0211023v1",
        "categories": [
            "cs.DB",
            "cs.CE",
            "J.2 ;H.3.5;H.2.8 ;H.2.5"
        ]
    },
    {
        "title": "JohnnyVon: Self-Replicating Automata in Continuous Two-Dimensional Space",
        "authors": [
            "Arnold Smith",
            "Peter Turney",
            "Robert Ewaschuk"
        ],
        "summary": "JohnnyVon is an implementation of self-replicating automata in continuous two-dimensional space. Two types of particles drift about in a virtual liquid. The particles are automata with discrete internal states but continuous external relationships. Their internal states are governed by finite state machines but their external relationships are governed by a simulated physics that includes brownian motion, viscosity, and spring-like attractive and repulsive forces. The particles can be assembled into patterns that can encode arbitrary strings of bits. We demonstrate that, if an arbitrary \"seed\" pattern is put in a \"soup\" of separate individual particles, the pattern will replicate by assembling the individual particles into copies of itself. We also show that, given sufficient time, a soup of separate individual particles will eventually spontaneously form self-replicating patterns. We discuss the implications of JohnnyVon for research in nanotechnology, theoretical biology, and artificial life.",
        "published": "2002-12-08T00:26:49Z",
        "link": "http://arxiv.org/abs/cs/0212010v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "I.6.3; I.6.8; J.2; J.3"
        ]
    },
    {
        "title": "A Simple Model of Unbounded Evolutionary Versatility as a Largest-Scale   Trend in Organismal Evolution",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "The idea that there are any large-scale trends in the evolution of biological organisms is highly controversial. It is commonly believed, for example, that there is a large-scale trend in evolution towards increasing complexity, but empirical and theoretical arguments undermine this belief. Natural selection results in organisms that are well adapted to their local environments, but it is not clear how local adaptation can produce a global trend. In this paper, I present a simple computational model, in which local adaptation to a randomly changing environment results in a global trend towards increasing evolutionary versatility. In this model, for evolutionary versatility to increase without bound, the environment must be highly dynamic. The model also shows that unbounded evolutionary versatility implies an accelerating evolutionary pace. I believe that unbounded increase in evolutionary versatility is a large-scale trend in evolution. I discuss some of the testable predictions about organismal evolution that are suggested by the model.",
        "published": "2002-12-10T15:52:43Z",
        "link": "http://arxiv.org/abs/cs/0212021v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "q-bio.PE",
            "I.6.3; I.6.8; J.3"
        ]
    },
    {
        "title": "Contextual Normalization Applied to Aircraft Gas Turbine Engine   Diagnosis",
        "authors": [
            "Peter D. Turney",
            "Michael Halasz"
        ],
        "summary": "Diagnosing faults in aircraft gas turbine engines is a complex problem. It involves several tasks, including rapid and accurate interpretation of patterns in engine sensor data. We have investigated contextual normalization for the development of a software tool to help engine repair technicians with interpretation of sensor data. Contextual normalization is a new strategy for employing machine learning. It handles variation in data that is due to contextual factors, rather than the health of the engine. It does this by normalizing the data in a context-sensitive manner. This learning strategy was developed and tested using 242 observations of an aircraft gas turbine engine in a test cell, where each observation consists of roughly 12,000 numbers, gathered over a 12 second interval. There were eight classes of observations: seven deliberately implanted classes of faults and a healthy class. We compared two approaches to implementing our learning strategy: linear regression and instance-based learning. We have three main results. (1) For the given problem, instance-based learning works better than linear regression. (2) For this problem, contextual normalization works better than other common forms of normalization. (3) The algorithms described here can be the basis for a useful software tool for assisting technicians with the interpretation of sensor data.",
        "published": "2002-12-11T18:30:59Z",
        "link": "http://arxiv.org/abs/cs/0212031v1",
        "categories": [
            "cs.LG",
            "cs.CE",
            "cs.CV",
            "I.2.6; I.5.4; J.2"
        ]
    },
    {
        "title": "Data Engineering for the Analysis of Semiconductor Manufacturing Data",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "We have analyzed manufacturing data from several different semiconductor manufacturing plants, using decision tree induction software called Q-YIELD. The software generates rules for predicting when a given product should be rejected. The rules are intended to help the process engineers improve the yield of the product, by helping them to discover the causes of rejection. Experience with Q-YIELD has taught us the importance of data engineering -- preprocessing the data to enable or facilitate decision tree induction. This paper discusses some of the data engineering problems we have encountered with semiconductor manufacturing data. The paper deals with two broad classes of problems: engineering the features in a feature vector representation and engineering the definition of the target concept (the classes). Manufacturing process data present special problems for feature engineering, since the data have multiple levels of granularity (detail, resolution). Engineering the target concept is important, due to our focus on understanding the past, as opposed to the more common focus in machine learning on predicting the future.",
        "published": "2002-12-12T19:11:11Z",
        "link": "http://arxiv.org/abs/cs/0212040v1",
        "categories": [
            "cs.LG",
            "cs.CE",
            "cs.CV",
            "I.2.6; I.5.2; I.5.4; J.2"
        ]
    },
    {
        "title": "Increasing Evolvability Considered as a Large-Scale Trend in Evolution",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Evolvability is the capacity to evolve. This paper introduces a simple computational model of evolvability and demonstrates that, under certain conditions, evolvability can increase indefinitely, even when there is no direct selection for evolvability. The model shows that increasing evolvability implies an accelerating evolutionary pace. It is suggested that the conditions for indefinitely increasing evolvability are satisfied in biological and cultural evolution. We claim that increasing evolvability is a large-scale trend in evolution. This hypothesis leads to testable predictions about biological and cultural evolution.",
        "published": "2002-12-12T22:39:39Z",
        "link": "http://arxiv.org/abs/cs/0212042v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "q-bio.PE",
            "I.6.3; I.6.8; J.3"
        ]
    },
    {
        "title": "Algorithm for generating orthogonal matrices with rational elements",
        "authors": [
            "Ruslan Sharipov"
        ],
        "summary": "Special orthogonal matrices with rational elements form the group SO(n,Q), where Q is the field of rational numbers. A theorem describing the structure of an arbitrary matrix from this group is proved. This theorem yields an algorithm for generating such matrices by means of random number routines.",
        "published": "2002-01-10T15:54:24Z",
        "link": "http://arxiv.org/abs/cs/0201007v2",
        "categories": [
            "cs.MS",
            "cs.DS",
            "G.4; K.3.1"
        ]
    },
    {
        "title": "Introducing LambdaTensor1.0 - A package for explicit symbolic and   numeric Lie algebra and Lie group calculations",
        "authors": [
            "Thomas Fischbacher"
        ],
        "summary": "Due to the occurrence of large exceptional Lie groups in supergravity, calculations involving explicit Lie algebra and Lie group element manipulations easily become very complicated and hence also error-prone if done by hand. Research on the extremal structure of maximal gauged supergravity theories in various dimensions sparked the development of a library for efficient abstract multilinear algebra calculations involving sparse and non-sparse higher-rank tensors, which is presented here.",
        "published": "2002-08-29T16:37:45Z",
        "link": "http://arxiv.org/abs/hep-th/0208218v2",
        "categories": [
            "hep-th",
            "cs.MS",
            "math-ph",
            "math.MP"
        ]
    },
    {
        "title": "Symbolic Expansion of Transcendental Functions",
        "authors": [
            "Stefan Weinzierl"
        ],
        "summary": "Higher transcendental function occur frequently in the calculation of Feynman integrals in quantum field theory. Their expansion in a small parameter is a non-trivial task. We report on a computer program which allows the systematic expansion of certain classes of functions. The algorithms are based on the Hopf algebra of nested sums. The program is written in C++ and uses the GiNaC library.",
        "published": "2002-01-04T09:55:42Z",
        "link": "http://arxiv.org/abs/math-ph/0201011v2",
        "categories": [
            "math-ph",
            "cs.SC",
            "hep-ph",
            "math.MP"
        ]
    },
    {
        "title": "Performance Comparison of Function Evaluation Methods",
        "authors": [
            "Leo Liberti"
        ],
        "summary": "We perform a comparison of the performance and efficiency of four different function evaluation methods: black-box functions, binary trees, $n$-ary trees and string parsing. The test consists in evaluating 8 different functions of two variables $x,y$ over 5000 floating point values of the pair $(x,y)$. The outcome of the test indicates that the $n$-ary tree representation of algebraic expressions is the fastest method, closely followed by black-box function method, then by binary trees and lastly by string parsing.",
        "published": "2002-06-06T09:10:39Z",
        "link": "http://arxiv.org/abs/cs/0206010v2",
        "categories": [
            "cs.SC",
            "cs.NA",
            "I.1.3"
        ]
    },
    {
        "title": "A correct proof of the heuristic GCD algorithm",
        "authors": [
            "Bernard Parisse"
        ],
        "summary": "In this note, we fill a gap in the proof of the heuristic GCD in the multivariate case made by Char, Geddes and Gonnet (JSC 1989) and give some additionnal information on this method.",
        "published": "2002-06-21T06:49:37Z",
        "link": "http://arxiv.org/abs/cs/0206032v1",
        "categories": [
            "cs.SC",
            "G.4"
        ]
    },
    {
        "title": "Orthonormal RBF wavelet and ridgelet-like series and transforms for   high-dimensional problems",
        "authors": [
            "W. Chen"
        ],
        "summary": "This paper developed a systematic strategy establishing RBF on the wavelet analysis, which includes continuous and discrete RBF orthonormal wavelet transforms respectively in terms of singular fundamental solutions and nonsingular general solutions of differential operators. In particular, the harmonic Bessel RBF transforms were presented for high-dimensional data processing. It was also found that the kernel functions of convection-diffusion operator are feasible to construct some stable ridgelet-like RBF transforms. We presented time-space RBF transforms based on non-singular solution and fundamental solution of time-dependent differential operators. The present methodology was further extended to analysis of some known RBFs such as the MQ, Gaussian and pre-wavelet kernel RBFs.",
        "published": "2002-07-03T15:34:39Z",
        "link": "http://arxiv.org/abs/cs/0207006v1",
        "categories": [
            "cs.SC",
            "G1.2, G1.8"
        ]
    },
    {
        "title": "A Note on the DQ Analysis of Anisotropic Plates",
        "authors": [
            "W Chen",
            "Weixing He",
            "Tingxiu Zhong"
        ],
        "summary": "Recently, Bert, Wang and Striz [1, 2] applied the differential quadrature (DQ) and harmonic differential quadrature (HDQ) methods to analyze static and dynamic behaviors of anisotropic plates. Their studies showed that the methods were conceptually simple and computationally efficient in comparison to other numerical techniques. Based on some recent work by the present author [3, 4], the purpose of this note is to further simplify the formulation effort and improve computing efficiency in applying the DQ and HDQ methods for these cases.",
        "published": "2002-07-09T20:04:28Z",
        "link": "http://arxiv.org/abs/cs/0207034v1",
        "categories": [
            "cs.SC",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "The Hilbert Zonotope and a Polynomial Time Algorithm for Universal   Grobner Bases",
        "authors": [
            "Eric Babson",
            "Shmuel Onn",
            "Rekha Thomas"
        ],
        "summary": "We provide a polynomial time algorithm for computing the universal Gr\\\"obner basis of any polynomial ideal having a finite set of common zeros in fixed number of variables. One ingredient of our algorithm is an effective construction of the state polyhedron of any member of the Hilbert scheme Hilb^d_n of n-long d-variate ideals, enabled by introducing the Hilbert zonotope H^d_n and showing that it simultaneously refines all state polyhedra of ideals on Hilb^d_n.",
        "published": "2002-07-16T20:55:36Z",
        "link": "http://arxiv.org/abs/math/0207135v1",
        "categories": [
            "math.CO",
            "cs.SC",
            "math.AG"
        ]
    },
    {
        "title": "Parameterized Type Definitions in Mathematica: Methods and Advantages",
        "authors": [
            "Alina Andreica"
        ],
        "summary": "The theme of symbolic computation in algebraic categories has become of utmost importance in the last decade since it enables the automatic modeling of modern algebra theories. On this theoretical background, the present paper reveals the utility of the parameterized categorical approach by deriving a multivariate polynomial category (over various coefficient domains), which is used by our Mathematica implementation of Buchberger's algorithms for determining the Groebner basis. These implementations are designed according to domain and category parameterization principles underlining their advantages: operation protection, inheritance, generality, easy extendibility. In particular, such an extension of Mathematica, a widely used symbolic computation system, with a new type system has a certain practical importance. The approach we propose for Mathematica is inspired from D. Gruntz and M. Monagan's work in Gauss, for Maple.",
        "published": "2002-08-20T13:19:42Z",
        "link": "http://arxiv.org/abs/cs/0208031v1",
        "categories": [
            "cs.SC",
            "I.1.4, I.1.2"
        ]
    },
    {
        "title": "Gravity, torsion, Dirac field and computer algebra using MAPLE and   REDUCE",
        "authors": [
            "D. N. Vulcanov"
        ],
        "summary": "The article presents computer algebra procedures and routines applied to the study of the Dirac field on curved spacetimes. The main part of the procedures is devoted to the construction of Pauli and Dirac matrices algebra on an anholonomic orthonormal reference frame. Then these procedures are used to compute the Dirac equation on curved spacetimes in a sequence of special dedicated routines. A comparative review of such procedures obtained for two computer algebra platforms (REDUCE + EXCALC and MAPLE + GRTensorII) is carried out. Applications for the calculus of Dirac equation on specific examples of spacetimes with or without torsion are pointed out.",
        "published": "2002-09-25T13:34:17Z",
        "link": "http://arxiv.org/abs/gr-qc/0209096v2",
        "categories": [
            "gr-qc",
            "cs.SC",
            "hep-th",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Ideal decompositions and computation of tensor normal forms",
        "authors": [
            "B. Fiedler"
        ],
        "summary": "Symmetry properties of r-times covariant tensors T can be described by certain linear subspaces W of the group ring K[S_r] of a symmetric group S_r. If for a class of tensors T such a W is known, the elements of the orthogonal subspace W^{\\bot} of W within the dual space of K[S_r] yield linear identities needed for a treatment of the term combination problem for the coordinates of the T. We give the structure of these W for every situation which appears in symbolic tensor calculations by computer. Characterizing idempotents of such W can be determined by means of an ideal decomposition algorithm which works in every semisimple ring up to an isomorphism. Furthermore, we use tools such as the Littlewood-Richardson rule, plethysms and discrete Fourier transforms for S_r to increase the efficience of calculations. All described methods were implemented in a Mathematica package called PERMS.",
        "published": "2002-11-09T18:47:36Z",
        "link": "http://arxiv.org/abs/math/0211156v1",
        "categories": [
            "math.CO",
            "cs.SC",
            "math.DG",
            "16D60; 15A72; 05E10; 16D70; 16S50; 05-04"
        ]
    },
    {
        "title": "Determination of the structure of algebraic curvature tensors by means   of Young symmetrizers",
        "authors": [
            "B. Fiedler"
        ],
        "summary": "For a positive definite fundamental tensor all known examples of Osserman algebraic curvature tensors have a typical structure. They can be produced from a metric tensor and a finite set of skew-symmetric matrices which fulfil Clifford commutation relations. We show by means of Young symmetrizers and a theorem of S. A. Fulling, R. C. King, B. G. Wybourne and C. J. Cummins that every algebraic curvature tensor has a structure which is very similar to that of the above Osserman curvature tensors. We verify our results by means of the Littlewood-Richardson rule and plethysms. For certain symbolic calculations we used the Mathematica packages MathTensor, Ricci and PERMS.",
        "published": "2002-12-19T20:26:42Z",
        "link": "http://arxiv.org/abs/math/0212278v2",
        "categories": [
            "math.CO",
            "cs.SC",
            "math.DG",
            "53B20, 15A72, 05E10, 16D60, 05-04"
        ]
    },
    {
        "title": "On the Importance of Having an Identity or, is Consensus really   Universal?",
        "authors": [
            "Harry Buhrman",
            "Alessandro Panconesi",
            "Riccardo Silvestri",
            "Paul Vitanyi"
        ],
        "summary": "We show that Naming-- the existence of distinct IDs known to all-- is a hidden but necessary assumption of Herlihy's universality result for Consensus. We then show in a very precise sense that Naming is harder than Consensus and bring to the surface some important differences existing between popular shared memory models.",
        "published": "2002-01-09T18:49:27Z",
        "link": "http://arxiv.org/abs/cs/0201006v2",
        "categories": [
            "cs.DC",
            "cs.CC",
            "F.1.2; C.2.4; B.3.2;B.4.3;D.1.3;D.4.1;D.4.4"
        ]
    },
    {
        "title": "A computer scientist looks at game theory",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "I consider issues in distributed computation that should be of relevance to game theory. In particular, I focus on (a) representing knowledge and uncertainty, (b) dealing with failures, and (c) specification of mechanisms.",
        "published": "2002-01-18T11:05:18Z",
        "link": "http://arxiv.org/abs/cs/0201016v1",
        "categories": [
            "cs.GT",
            "cs.DC",
            "cs.MA",
            "I.2.11, C.2.4"
        ]
    },
    {
        "title": "Simple Optimal Wait-free Multireader Registers",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "Multireader shared registers are basic objects used as communication medium in asynchronous concurrent computation. We propose a surprisingly simple and natural scheme to obtain several wait-free constructions of bounded 1-writer multireader registers from atomic 1-writer 1-reader registers, that is easier to prove correct than any previous construction. Our main construction is the first symmetric pure timestamp one that is optimal with respect to the worst-case local use of control bits; the other one is optimal with respect to global use of control bits; both are optimal in time.",
        "published": "2002-02-04T17:29:15Z",
        "link": "http://arxiv.org/abs/cs/0202003v3",
        "categories": [
            "cs.DC",
            "F.1.2; C.2.4; B.3.2; B.4.3; D.1.3; D.4.1; D.4.4"
        ]
    },
    {
        "title": "Secure History Preservation Through Timeline Entanglement",
        "authors": [
            "Petros Maniatis",
            "Mary Baker"
        ],
        "summary": "A secure timeline is a tamper-evident historic record of the states through which a system goes throughout its operational history. Secure timelines can help us reason about the temporal ordering of system states in a provable manner. We extend secure timelines to encompass multiple, mutually distrustful services, using timeline entanglement. Timeline entanglement associates disparate timelines maintained at independent systems, by linking undeniably the past of one timeline to the future of another. Timeline entanglement is a sound method to map a time step in the history of one service onto the timeline of another, and helps clients of entangled services to get persistent temporal proofs for services rendered that survive the demise or non-cooperation of the originating service. In this paper we present the design and implementation of Timeweave, our service development framework for timeline entanglement based on two novel disk-based authenticated data structures. We evaluate Timeweave's performance characteristics and show that it can be efficiently deployed in a loosely-coupled distributed system of a few hundred services with overhead of roughly 2-8% of the processing resources of a PC-grade system.",
        "published": "2002-02-06T21:52:09Z",
        "link": "http://arxiv.org/abs/cs/0202005v1",
        "categories": [
            "cs.DC",
            "cs.CR",
            "cs.DB",
            "cs.DS",
            "D.4.6; K.6.5; E.2; C.2.4"
        ]
    },
    {
        "title": "Hypernets -- Good (G)news for Gnutella",
        "authors": [
            "N. J. Gunther"
        ],
        "summary": "Criticism of Gnutella network scalability has rested on the bandwidth attributes of the original interconnection topology: a Cayley tree. Trees, in general, are known to have lower aggregate bandwidth than higher dimensional topologies e.g., hypercubes, meshes and tori. Gnutella was intended to support thousands to millions of peers. Studies of interconnection topologies in the literature, however, have focused on hardware implementations which are limited by cost to a few thousand nodes. Since the Gnutella network is virtual, hyper-topologies are relatively unfettered by such constraints. We present performance models for several plausible hyper-topologies and compare their query throughput up to millions of peers. The virtual hypercube and the virtual hypertorus are shown to offer near linear scalability subject to the number of peer TCP/IP connections that can be simultaneously kept open.",
        "published": "2002-02-16T21:46:14Z",
        "link": "http://arxiv.org/abs/cs/0202019v2",
        "categories": [
            "cs.PF",
            "cs.DC",
            "cs.IR",
            "cs.NI",
            "C.2.4; C.4; D.4.8"
        ]
    },
    {
        "title": "SPINning Parallel Systems Software",
        "authors": [
            "O. S. Matlin",
            "E. Lusk",
            "W. McCune"
        ],
        "summary": "We describe our experiences in using SPIN to verify parts of the Multi Purpose Daemon (MPD) parallel process management system. MPD is a distributed collection of processes connected by Unix network sockets. MPD is dynamic: processes and connections among them are created and destroyed as MPD is initialized, runs user processes, recovers from faults, and terminates. This dynamic nature is easily expressible in the SPIN/PROMELA framework but poses performance and scalability challenges. We present here the results of expressing some of the parallel algorithms of MPD and executing both simulation and verification runs with SPIN.",
        "published": "2002-03-06T18:15:03Z",
        "link": "http://arxiv.org/abs/cs/0203009v1",
        "categories": [
            "cs.LO",
            "cs.DC",
            "F.3.1; D.1.3"
        ]
    },
    {
        "title": "GridSim: A Toolkit for the Modeling and Simulation of Distributed   Resource Management and Scheduling for Grid Computing",
        "authors": [
            "Rajkumar Buyya",
            "Manzur Murshed"
        ],
        "summary": "Clusters, grids, and peer-to-peer (P2P) networks have emerged as popular paradigms for next generation parallel and distributed computing. The management of resources and scheduling of applications in such large-scale distributed systems is a complex undertaking. In order to prove the effectiveness of resource brokers and associated scheduling algorithms, their performance needs to be evaluated under different scenarios such as varying number of resources and users with different requirements. In a grid environment, it is hard and even impossible to perform scheduler performance evaluation in a repeatable and controllable manner as resources and users are distributed across multiple organizations with their own policies. To overcome this limitation, we have developed a Java-based discrete-event grid simulation toolkit called GridSim. The toolkit supports modeling and simulation of heterogeneous grid resources (both time- and space-shared), users and application models. It provides primitives for creation of application tasks, mapping of tasks to resources, and their management. To demonstrate suitability of the GridSim toolkit, we have simulated a Nimrod-G like grid resource broker and evaluated the performance of deadline and budget constrained cost- and time-minimization scheduling algorithms.",
        "published": "2002-03-14T03:44:18Z",
        "link": "http://arxiv.org/abs/cs/0203019v1",
        "categories": [
            "cs.DC",
            "C5"
        ]
    },
    {
        "title": "A Deadline and Budget Constrained Cost-Time Optimisation Algorithm for   Scheduling Task Farming Applications on Global Grids",
        "authors": [
            "Rajkumar Buyya",
            "Manzur Murshed"
        ],
        "summary": "Computational Grids and peer-to-peer (P2P) networks enable the sharing, selection, and aggregation of geographically distributed resources for solving large-scale problems in science, engineering, and commerce. The management and composition of resources and services for scheduling applications, however, becomes a complex undertaking. We have proposed a computational economy framework for regulating the supply and demand for resources and allocating them for applications based on the users quality of services requirements. The framework requires economy driven deadline and budget constrained (DBC) scheduling algorithms for allocating resources to application jobs in such a way that the users requirements are met. In this paper, we propose a new scheduling algorithm, called DBC cost-time optimisation, which extends the DBC cost-optimisation algorithm to optimise for time, keeping the cost of computation at the minimum. The superiority of this new scheduling algorithm, in achieving lower job completion time, is demonstrated by simulating the World-Wide Grid and scheduling task-farming applications for different deadline and budget scenarios using both this new and the cost optimisation scheduling algorithms.",
        "published": "2002-03-14T03:50:19Z",
        "link": "http://arxiv.org/abs/cs/0203020v1",
        "categories": [
            "cs.DC",
            "C5"
        ]
    },
    {
        "title": "Source Routing and Scheduling in Packet Networks",
        "authors": [
            "Matthew Andrews",
            "Antonio Fernandez",
            "Ashish Goel",
            "Lisa Zhang"
        ],
        "summary": "We study {\\em routing} and {\\em scheduling} in packet-switched networks. We assume an adversary that controls the injection time, source, and destination for each packet injected. A set of paths for these packets is {\\em admissible} if no link in the network is overloaded. We present the first on-line routing algorithm that finds a set of admissible paths whenever this is feasible. Our algorithm calculates a path for each packet as soon as it is injected at its source using a simple shortest path computation. The length of a link reflects its current congestion. We also show how our algorithm can be implemented under today's Internet routing paradigms.   When the paths are known (either given by the adversary or computed as above) our goal is to schedule the packets along the given paths so that the packets experience small end-to-end delays. The best previous delay bounds for deterministic and distributed scheduling protocols were exponential in the path length. In this paper we present the first deterministic and distributed scheduling protocol that guarantees a polynomial end-to-end delay for every packet.   Finally, we discuss the effects of combining routing with scheduling. We first show that some unstable scheduling protocols remain unstable no matter how the paths are chosen. However, the freedom to choose paths can make a difference. For example, we show that a ring with parallel links is stable for all greedy scheduling protocols if paths are chosen intelligently, whereas this is not the case if the adversary specifies the paths.",
        "published": "2002-03-28T12:15:14Z",
        "link": "http://arxiv.org/abs/cs/0203030v2",
        "categories": [
            "cs.NI",
            "cs.DC",
            "C.2.1; C.2.6; F.2.2"
        ]
    },
    {
        "title": "Economic-based Distributed Resource Management and Scheduling for Grid   Computing",
        "authors": [
            "Rajkumar Buyya"
        ],
        "summary": "Computational Grids, emerging as an infrastructure for next generation computing, enable the sharing, selection, and aggregation of geographically distributed resources for solving large-scale problems in science, engineering, and commerce. As the resources in the Grid are heterogeneous and geographically distributed with varying availability and a variety of usage and cost policies for diverse users at different times and, priorities as well as goals that vary with time. The management of resources and application scheduling in such a large and distributed environment is a complex task. This thesis proposes a distributed computational economy as an effective metaphor for the management of resources and application scheduling. It proposes an architectural framework that supports resource trading and quality of services based scheduling. It enables the regulation of supply and demand for resources and provides an incentive for resource owners for participating in the Grid and motives the users to trade-off between the deadline, budget, and the required level of quality of service. The thesis demonstrates the capability of economic-based systems for peer-to-peer distributed computing by developing users' quality-of-service requirements driven scheduling strategies and algorithms. It demonstrates their effectiveness by performing scheduling experiments on the World-Wide Grid for solving parameter sweep applications.",
        "published": "2002-04-22T20:06:42Z",
        "link": "http://arxiv.org/abs/cs/0204048v1",
        "categories": [
            "cs.DC",
            "C.1.3, C.1.4, C.2.4"
        ]
    },
    {
        "title": "Weaves: A Novel Direct Code Execution Interface for Parallel High   Performance Scientific Codes",
        "authors": [
            "Srinidhi Varadarajan",
            "Joy Mukherjee",
            "Naren Ramakrishnan"
        ],
        "summary": "Scientific codes are increasingly being used in compositional settings, especially problem solving environments (PSEs). Typical compositional modeling frameworks require significant buy-in, in the form of commitment to a particular style of programming (e.g., distributed object components). While this solution is feasible for newer generations of component-based scientific codes, large legacy code bases present a veritable software engineering nightmare. We introduce Weaves a novel framework that enables modeling, composition, direct code execution, performance characterization, adaptation, and control of unmodified high performance scientific codes. Weaves is an efficient generalized framework for parallel compositional modeling that is a proper superset of the threads and processes models of programming. In this paper, our focus is on the transparent code execution interface enabled by Weaves. We identify design constraints, their impact on implementation alternatives, configuration scenarios, and present results from a prototype implementation on Intel x86 architectures.",
        "published": "2002-05-04T02:17:09Z",
        "link": "http://arxiv.org/abs/cs/0205004v1",
        "categories": [
            "cs.DC",
            "cs.PF",
            "D.4.1; I.6"
        ]
    },
    {
        "title": "An Overview of a Grid Architecture for Scientific Computing",
        "authors": [
            "A. Waananen",
            "M. Ellert",
            "A. Konstantinov",
            "B. Konya",
            "O. Smirnova"
        ],
        "summary": "This document gives an overview of a Grid testbed architecture proposal for the NorduGrid project. The aim of the project is to establish an inter-Nordic testbed facility for implementation of wide area computing and data handling. The architecture is supposed to define a Grid system suitable for solving data intensive problems at the Large Hadron Collider at CERN. We present the various architecture components needed for such a system. After that we go on to give a description of the dynamics by showing the task flow.",
        "published": "2002-05-14T19:22:00Z",
        "link": "http://arxiv.org/abs/cs/0205021v1",
        "categories": [
            "cs.DC",
            "C.2.4;C.5;J.2"
        ]
    },
    {
        "title": "Performance evaluation of the GridFTP within the NorduGrid project",
        "authors": [
            "M. Ellert",
            "A. Konstantinov",
            "B. Konya",
            "O. Smirnova",
            "A. Waananen"
        ],
        "summary": "This report presents results of the tests measuring the performance of multi-threaded file transfers, using the GridFTP implementation of the Globus project over the NorduGrid network resources. Point to point WAN tests, carried out between the sites of Copenhagen, Lund, Oslo and Uppsala, are described. It was found that multiple threaded download via the high performance GridFTP protocol can significantly improve file transfer performance, and can serve as a reliable data",
        "published": "2002-05-14T19:55:37Z",
        "link": "http://arxiv.org/abs/cs/0205023v2",
        "categories": [
            "cs.DC",
            "C.2.2;C.2.4;C.5"
        ]
    },
    {
        "title": "A Primal-Dual Parallel Approximation Technique Applied to Weighted Set   and Vertex Cover",
        "authors": [
            "Samir Khuller",
            "Uzi Vishkin",
            "Neal Young"
        ],
        "summary": "The paper describes a simple deterministic parallel/distributed (2+epsilon)-approximation algorithm for the minimum-weight vertex-cover problem and its dual (edge/element packing).",
        "published": "2002-05-18T04:22:29Z",
        "link": "http://arxiv.org/abs/cs/0205037v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.0; G.1.6; C.2.4"
        ]
    },
    {
        "title": "Fast Deterministic Consensus in a Noisy Environment",
        "authors": [
            "James Aspnes"
        ],
        "summary": "It is well known that the consensus problem cannot be solved deterministically in an asynchronous environment, but that randomized solutions are possible. We propose a new model, called noisy scheduling, in which an adversarial schedule is perturbed randomly, and show that in this model randomness in the environment can substitute for randomness in the algorithm. In particular, we show that a simplified, deterministic version of Chandra's wait-free shared-memory consensus algorithm (PODC, 1996, pp. 166-175) solves consensus in time at most logarithmic in the number of active processes. The proof of termination is based on showing that a race between independent delayed renewal processes produces a winner quickly. In addition, we show that the protocol finishes in constant time using quantum and priority-based scheduling on a uniprocessor, suggesting that it is robust against the choice of model over a wide range.",
        "published": "2002-06-07T18:50:04Z",
        "link": "http://arxiv.org/abs/cs/0206012v2",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2"
        ]
    },
    {
        "title": "A Multilevel Approach to Topology-Aware Collective Operations in   Computational Grids",
        "authors": [
            "N. T. Karonis",
            "B. de Supinski",
            "I. Foster",
            "W. Gropp",
            "E. Lusk"
        ],
        "summary": "The efficient implementation of collective communiction operations has received much attention. Initial efforts produced \"optimal\" trees based on network communication models that assumed equal point-to-point latencies between any two processes. This assumption is violated in most practical settings, however, particularly in heterogeneous systems such as clusters of SMPs and wide-area \"computational Grids,\" with the result that collective operations perform suboptimally. In response, more recent work has focused on creating topology-aware trees for collective operations that minimize communication across slower channels (e.g., a wide-area network). While these efforts have significant communication benefits, they all limit their view of the network to only two layers. We present a strategy based upon a multilayer view of the network. By creating multilevel topology-aware trees we take advantage of communication cost differences at every level in the network. We used this strategy to implement topology-aware versions of several MPI collective operations in MPICH-G2, the Globus Toolkit[tm]-enabled version of the popular MPICH implementation of the MPI standard. Using information about topology provided by MPICH-G2, we construct these multilevel topology-aware trees automatically during execution. We present results demonstrating the advantages of our multilevel approach by comparing it to the default (topology-unaware) implementation provided by MPICH and a topology-aware two-layer implementation.",
        "published": "2002-06-24T18:28:21Z",
        "link": "http://arxiv.org/abs/cs/0206038v1",
        "categories": [
            "cs.DC",
            "D.1.3"
        ]
    },
    {
        "title": "MPICH-G2: A Grid-Enabled Implementation of the Message Passing Interface",
        "authors": [
            "N. T. Karonis",
            "B. Toonen",
            "I. Foster"
        ],
        "summary": "Application development for distributed computing \"Grids\" can benefit from tools that variously hide or enable application-level management of critical aspects of the heterogeneous environment. As part of an investigation of these issues, we have developed MPICH-G2, a Grid-enabled implementation of the Message Passing Interface (MPI) that allows a user to run MPI programs across multiple computers, at the same or different sites, using the same commands that would be used on a parallel computer. This library extends the Argonne MPICH implementation of MPI to use services provided by the Globus Toolkit for authentication, authorization, resource allocation, executable staging, and I/O, as well as for process creation, monitoring, and control. Various performance-critical operations, including startup and collective operations, are configured to exploit network topology information. The library also exploits MPI constructs for performance management; for example, the MPI communicator construct is used for application-level discovery of, and adaptation to, both network topology and network quality-of-service mechanisms. We describe the MPICH-G2 design and implementation, present performance results, and review application experiences, including record-setting distributed simulations.",
        "published": "2002-06-25T19:55:45Z",
        "link": "http://arxiv.org/abs/cs/0206040v2",
        "categories": [
            "cs.DC",
            "D.1.3"
        ]
    },
    {
        "title": "National Infrastructure Contingencies: Survey of Wireless Technology   Support",
        "authors": [
            "Ronald M. Fussell"
        ],
        "summary": "In modern society, the flow of information has become the lifeblood of commerce and social interaction. This movement of data supports most aspects of the United States economy in particular, as well as, serving as the vehicle upon which governmental agencies react to social conditions. In addition, it is understood that the continuance of efficient and reliable data communications during times of national or regional disaster remains a priority in the United States. The coordination of emergency response and area revitalization / rehabilitation efforts between local, state, and federal emergency response is increasingly necessary as agencies strive to work more seamlessly between the affected organizations. Additionally, international support is often made available to react to such adverse conditions as wildfire suppression scenarios and therefore require the efficient management of workforce and associated logistics support.   It is through the examination of the issues related to un-tethered data transmission during infrastructure contingencies that responders may best tailor a unified approach to the rapid recovery after disasters occur.",
        "published": "2002-07-01T18:09:44Z",
        "link": "http://arxiv.org/abs/cs/0207001v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "C.2.5"
        ]
    },
    {
        "title": "System Description for a Scalable, Fault-Tolerant, Distributed Garbage   Collector",
        "authors": [
            "N. Allen",
            "T. Terriberry"
        ],
        "summary": "We describe an efficient and fault-tolerant algorithm for distributed cyclic garbage collection. The algorithm imposes few requirements on the local machines and allows for flexibility in the choice of local collector and distributed acyclic garbage collector to use with it. We have emphasized reducing the number and size of network messages without sacrificing the promptness of collection throughout the algorithm. Our proposed collector is a variant of back tracing to avoid extensive synchronization between machines. We have added an explicit forward tracing stage to the standard back tracing stage and designed a tuned heuristic to reduce the total amount of work done by the collector. Of particular note is the development of fault-tolerant cooperation between traces and a heuristic that aggressively reduces the set of suspect objects.",
        "published": "2002-07-10T00:53:23Z",
        "link": "http://arxiv.org/abs/cs/0207036v1",
        "categories": [
            "cs.DC",
            "C.2.4; D.4.5"
        ]
    },
    {
        "title": "Libra: An Economy driven Job Scheduling System for Clusters",
        "authors": [
            "Jahanzeb Sherwani",
            "Nosheen Ali",
            "Nausheen Lotia",
            "Zahra Hayat",
            "Rajkumar Buyya"
        ],
        "summary": "Clusters of computers have emerged as mainstream parallel and distributed platforms for high-performance, high-throughput and high-availability computing. To enable effective resource management on clusters, numerous cluster managements systems and schedulers have been designed. However, their focus has essentially been on maximizing CPU performance, but not on improving the value of utility delivered to the user and quality of services. This paper presents a new computational economy driven scheduling system called Libra, which has been designed to support allocation of resources based on the users? quality of service (QoS) requirements. It is intended to work as an add-on to the existing queuing and resource management system. The first version has been implemented as a plugin scheduler to the PBS (Portable Batch System) system. The scheduler offers market-based economy driven service for managing batch jobs on clusters by scheduling CPU time according to user utility as determined by their budget and deadline rather than system performance considerations. The Libra scheduler ensures that both these constraints are met within an O(n) run-time. The Libra scheduler has been simulated using the GridSim toolkit to carry out a detailed performance analysis. Results show that the deadline and budget based proportional resource allocation strategy improves the utility of the system and user satisfaction as compared to system-centric scheduling strategies.",
        "published": "2002-07-22T11:35:33Z",
        "link": "http://arxiv.org/abs/cs/0207077v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "* C.1.4"
        ]
    },
    {
        "title": "Eternity variables to prove simulation of specifications",
        "authors": [
            "Wim H. Hesselink"
        ],
        "summary": "Simulations of specifications are introduced as a unification and generalization of refinement mappings, history variables, forward simulations, prophecy variables, and backward simulations. A specification implements another specification if and only if there is a simulation from the first one to the second one that satisfies a certain condition. By adding stutterings, the formalism allows that the concrete behaviours take more (or possibly less) steps than the abstract ones.   Eternity variables are introduced as a more powerful alternative for prophecy variables and backward simulations. This formalism is semantically complete: every simulation that preserves quiescence is a composition of a forward simulation, an extension with eternity variables, and a refinement mapping. This result does not need finite invisible nondeterminism and machine closure as in the Abadi-Lamport Theorem. Internal continuity is weakened to preservation of quiescence.",
        "published": "2002-07-29T09:12:22Z",
        "link": "http://arxiv.org/abs/cs/0207095v4",
        "categories": [
            "cs.DC",
            "cs.LO",
            "F.1.1;F.3.1"
        ]
    },
    {
        "title": "Noncontiguous I/O through PVFS",
        "authors": [
            "Avery Ching",
            "Alok Choudhary",
            "Wei-keng Liao",
            "Rob Ross",
            "William Gropp"
        ],
        "summary": "With the tremendous advances in processor and memory technology, I/O has risen to become the bottleneck in high-performance computing for many applications. The development of parallel file systems has helped to ease the performance gap, but I/O still remains an area needing significant performance improvement. Research has found that noncontiguous I/O access patterns in scientific applications combined with current file system methods to perform these accesses lead to unacceptable performance for large data sets. To enhance performance of noncontiguous I/O we have created list I/O, a native version of noncontiguous I/O. We have used the Parallel Virtual File System (PVFS) to implement our ideas. Our research and experimentation shows that list I/O outperforms current noncontiguous I/O access methods in most I/O situations and can substantially enhance the performance of real-world scientific applications.",
        "published": "2002-07-29T16:20:25Z",
        "link": "http://arxiv.org/abs/cs/0207096v1",
        "categories": [
            "cs.DC",
            "D.4.3"
        ]
    },
    {
        "title": "Detecting Race Conditions in Parallel Programs that Use Semaphores",
        "authors": [
            "Philip N. Klein",
            "Hsueh-I Lu",
            "Rob H. B. Netzer"
        ],
        "summary": "We address the problem of detecting race conditions in programs that use semaphores for synchronization. Netzer and Miller showed that it is NP-complete to detect race conditions in programs that use many semaphores. We show in this paper that it remains NP-complete even if only two semaphores are used in the parallel programs.   For the tractable case, i.e., using only one semaphore, we give two algorithms for detecting race conditions from the trace of executing a parallel program on p processors, where n semaphore operations are executed. The first algorithm determines in O(n) time whether a race condition exists between any two given operations. The second algorithm runs in O(np log n) time and outputs a compact representation from which one can determine in O(1) time whether a race condition exists between any two given operations. The second algorithm is near-optimal in that the running time is only O(log n) times the time required simply to write down the output.",
        "published": "2002-08-03T17:20:58Z",
        "link": "http://arxiv.org/abs/cs/0208004v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2; G.2.2; D.1.3; D.4.1; E.1"
        ]
    },
    {
        "title": "TeraScale SneakerNet: Using Inexpensive Disks for Backup, Archiving, and   Data Exchange",
        "authors": [
            "Jim Gray",
            "Wyman Chong",
            "Tom Barclay",
            "Alex Szalay",
            "Jan vandenBerg"
        ],
        "summary": "Large datasets are most economically trnsmitted via parcel post given the current economics of wide-area networking. This article describes how the Sloan Digital Sky Survey ships terabyte scale datasets both within the US and to Europe and Asia. We 3GT storage bricks (Ghz processor, GB ram, GbpsEthernet, TB disk) for about 2k$ each. These bricks act as database servers on the LAN. They are loaded at one site and read at the second site. The paper describes the bricks, their economics, and some software issues that they raise.",
        "published": "2002-08-07T22:32:46Z",
        "link": "http://arxiv.org/abs/cs/0208011v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "C.2.0;C.2.4; C.4.4;H.3;K.6"
        ]
    },
    {
        "title": "Web Services for the Virtual Observatory",
        "authors": [
            "Alexander S. Szalay",
            "Tamas Budavari",
            "Tanu Malika",
            "Jim Gray",
            "Ani Thakara"
        ],
        "summary": "Web Services form a new, emerging paradigm to handle distributed access to resources over the Internet. There are platform independent standards (SOAP, WSDL), which make the developers? task considerably easier. This article discusses how web services could be used in the context of the Virtual Observatory. We envisage a multi-layer architecture, with interoperating services. A well-designed lower layer consisting of simple, standard services implemented by most data providers will go a long way towards establishing a modular architecture. More complex applications can be built upon this core layer. We present two prototype applications, the SdssCutout and the SkyQuery as examples of this layered architecture.",
        "published": "2002-08-07T22:58:37Z",
        "link": "http://arxiv.org/abs/cs/0208014v1",
        "categories": [
            "cs.DC",
            "cs.DL",
            "C.23.1;C.2.4;D.2.12;D.4.7;H.2;H.3;H.4;J.2;J.3"
        ]
    },
    {
        "title": "Implicit Simulations using Messaging Protocols",
        "authors": [
            "G. A. Kohring"
        ],
        "summary": "A novel algorithm for performing parallel, distributed computer simulations on the Internet using IP control messages is introduced. The algorithm employs carefully constructed ICMP packets which enable the required computations to be completed as part of the standard IP communication protocol. After providing a detailed description of the algorithm, experimental applications in the areas of stochastic neural networks and deterministic cellular automata are discussed. As an example of the algorithms potential power, a simulation of a deterministic cellular automaton involving 10^5 Internet connected devices was performed.",
        "published": "2002-08-14T09:11:20Z",
        "link": "http://arxiv.org/abs/cs/0208021v1",
        "categories": [
            "cs.DC",
            "D.1.3"
        ]
    },
    {
        "title": "A Unified Theory of Shared Memory Consistency",
        "authors": [
            "Robert C. Steinke",
            "Gary J. Nutt"
        ],
        "summary": "Memory consistency models have been developed to specify what values may be returned by a read given that, in a distributed system, memory operations may only be partially ordered. Before this work, consistency models were defined independently. Each model followed a set of rules which was separate from the rules of every other model. In our work we have defined a set of four consistency properties. Any subset of the four properties yields a set of rules which constitute a consistency model. Every consistency model previously described in the literature can be defined based on our four properties. Therefore, we present these properties as a unfied theory of shared memory consistency.",
        "published": "2002-08-19T19:09:04Z",
        "link": "http://arxiv.org/abs/cs/0208027v1",
        "categories": [
            "cs.DC",
            "D.1.3;F.1.2"
        ]
    },
    {
        "title": "Randomized protocols for asynchronous consensus",
        "authors": [
            "James Aspnes"
        ],
        "summary": "The famous Fischer, Lynch, and Paterson impossibility proof shows that it is impossible to solve the consensus problem in a natural model of an asynchronous distributed system if even a single process can fail. Since its publication, two decades of work on fault-tolerant asynchronous consensus algorithms have evaded this impossibility result by using extended models that provide (a) randomization, (b) additional timing assumptions, (c) failure detectors, or (d) stronger synchronization mechanisms than are available in the basic model. Concentrating on the first of these approaches, we illustrate the history and structure of randomized asynchronous consensus protocols by giving detailed descriptions of several such protocols.",
        "published": "2002-09-06T15:36:58Z",
        "link": "http://arxiv.org/abs/cs/0209014v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2; F.1.2"
        ]
    },
    {
        "title": "Practical Load Balancing for Content Requests in Peer-to-Peer Networks",
        "authors": [
            "Mema Roussopoulos",
            "Mary Baker"
        ],
        "summary": "This paper studies the problem of load-balancing the demand for content in a peer-to-peer network across heterogeneous peer nodes that hold replicas of the content. Previous decentralized load balancing techniques in distributed systems base their decisions on periodic updates containing information about load or available capacity observed at the serving entities. We show that these techniques do not work well in the peer-to-peer context; either they do not address peer node heterogeneity, or they suffer from significant load oscillations. We propose a new decentralized algorithm, Max-Cap, based on the maximum inherent capacities of the replica nodes and show that unlike previous algorithms, it is not tied to the timeliness or frequency of updates. Yet, Max-Cap can handle the heterogeneity of a peer-to-peer environment without suffering from load oscillations.",
        "published": "2002-09-21T00:09:02Z",
        "link": "http://arxiv.org/abs/cs/0209023v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Errors in Low and Lapsley's article \"Optimization Flow Control, I: Basic   Algorithm and Convergence\"",
        "authors": [
            "Andrzej Karbowski"
        ],
        "summary": "In the note two errors in Low and Lapsley's article \"Optimization Flow Control, I: Basic Algorithm and Convergence\", \"IEEE/ACM Transactions on Networking\", 7(6), pp. 861-874, 1999, are shown. Because of these errors the proofs of both theorems presented in the article are incomplete and some assessments are wrong.",
        "published": "2002-09-21T22:53:21Z",
        "link": "http://arxiv.org/abs/cs/0209024v2",
        "categories": [
            "cs.NI",
            "cs.DC",
            "C.2.3; G.1.6"
        ]
    },
    {
        "title": "Correction to Low and Lapsley's article \"Optimization Flow Control, I:   Basic Algorithm and Convergence\"",
        "authors": [
            "Andrzej Karbowski"
        ],
        "summary": "In the note an error in Low and Lapsley's article (\"Optimization Flow Control, I: Basic Algorithm and Convergence\", IEEE/ACM Transactions on Networking, 7(6), pp. 861-874, 1999) is pointed out. Because of this error the proof of the Theorem 2 presented in the article is incomplete and some assessments are wrong. In the second part of the note the author proposes a correction to this proof.",
        "published": "2002-09-21T23:19:13Z",
        "link": "http://arxiv.org/abs/cs/0209025v2",
        "categories": [
            "cs.NI",
            "cs.DC",
            "C.2.3; G.1.6"
        ]
    },
    {
        "title": "Mapping the Gnutella Network: Properties of Large-Scale Peer-to-Peer   Systems and Implications for System Design",
        "authors": [
            "Matei Ripeanu",
            "Ian Foster",
            "Adriana Iamnitchi"
        ],
        "summary": "Despite recent excitement generated by the peer-to-peer (P2P) paradigm and the surprisingly rapid deployment of some P2P applications, there are few quantitative evaluations of P2P systems behavior. The open architecture, achieved scale, and self-organizing structure of the Gnutella network make it an interesting P2P architecture to study. Like most other P2P applications, Gnutella builds, at the application level, a virtual network with its own routing mechanisms. The topology of this virtual network and the routing mechanisms used have a significant influence on application properties such as performance, reliability, and scalability. We have built a \"crawler\" to extract the topology of Gnutella's application level network. In this paper we analyze the topology graph and evaluate generated network traffic. Our two major findings are that: (1) although Gnutella is not a pure power-law network, its current configuration has the benefits and drawbacks of a power-law structure, and (2) the Gnutella virtual network topology does not match well the underlying Internet topology, hence leading to ineffective use of the physical networking infrastructure. These findings guide us to propose changes to the Gnutella protocol and implementations that may bring significant performance and scalability improvements. We believe that our findings as well as our measurement and analysis techniques have broad applicability to P2P systems and provide unique insights into P2P system design tradeoffs.",
        "published": "2002-09-25T08:27:35Z",
        "link": "http://arxiv.org/abs/cs/0209028v1",
        "categories": [
            "cs.DC",
            "cond-mat.stat-mech",
            "cs.NI",
            "C.2.4"
        ]
    },
    {
        "title": "A generalization of Amdahl's law and relative conditions of parallelism",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this work I present a generalization of Amdahl's law on the limits of a parallel implementation with many processors. In particular I establish some mathematical relations involving the number of processors and the dimension of the treated problem, and with these conditions I define, on the ground of the reachable speedup, some classes of parallelism for the implementations. I also derive a condition for obtaining superlinear speedup. The used mathematical technics are those of differential calculus. I describe some examples from classical problems offered by the specialized literature on the subject.",
        "published": "2002-09-25T08:50:18Z",
        "link": "http://arxiv.org/abs/cs/0209029v1",
        "categories": [
            "cs.DC",
            "cs.PF",
            "F.1.2; D.2.8"
        ]
    },
    {
        "title": "Locating Data in (Small-World?) Peer-to-Peer Scientific Collaborations",
        "authors": [
            "Adriana Iamnitchi",
            "Matei Ripeanu",
            "Ian Foster"
        ],
        "summary": "Data-sharing scientific collaborations have particular characteristics, potentially different from the current peer-to-peer environments. In this paper we advocate the benefits of exploiting emergent patterns in self-configuring networks specialized for scientific data-sharing collaborations. We speculate that a peer-to-peer scientific collaboration network will exhibit small-world topology, as do a large number of social networks for which the same pattern has been documented. We propose a solution for locating data in decentralized, scientific, data-sharing environments that exploits the small-worlds topology. The research challenge we raise is: what protocols should be used to allow a self-configuring peer-to-peer network to form small worlds similar to the way in which the humans that use the network do in their social interactions?",
        "published": "2002-09-26T19:14:00Z",
        "link": "http://arxiv.org/abs/cs/0209031v1",
        "categories": [
            "cs.DC",
            "cond-mat",
            "C.2.4"
        ]
    },
    {
        "title": "GridBank: A Grid Accounting Services Architecture (GASA) for Distributed   Systems Sharing and Integration",
        "authors": [
            "Alexander Barmouta Rajkumar Buyya"
        ],
        "summary": "Computational Grids are emerging as new infrastructure for Internet-based parallel and distributed computing. They enable the sharing, exchange, discovery, and aggregation of resources distributed across multiple administrative domains, organizations and enterprises. To accomplish this, Grids need infrastructure that supports various services: security, uniform access, resource management, scheduling, application composition, computational economy, and accountability. Many Grid projects have developed technologies that provide many of these services with an exception of accountability. To overcome this limitation, we propose a new infrastructure called Grid Bank that provides services for accounting. This paper presents requirements of Grid accountability and different models within which it can operate and proposes Grid Bank Services Architecture that meets them. The paper highlights implementation issues with detailed discussion on format for various records/database that the GridBank need to maintain. It also presents protocols for interaction between GridBank and various components within Grid computing environments.",
        "published": "2002-10-01T10:41:52Z",
        "link": "http://arxiv.org/abs/cs/0210002v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "A Random Structure for Optimum Cache Size Distributed hash table (DHT)   Peer-to-Peer design",
        "authors": [
            "Nima Sarshar",
            "Vwani Roychowdhury"
        ],
        "summary": "We propose a new and easily-realizable distributed hash table (DHT) peer-to-peer structure, incorporating a random caching strategy that allows for {\\em polylogarithmic search time} while having only a {\\em constant cache} size. We also show that a very large class of deterministic caching strategies, which covers almost all previously proposed DHT systems, can not achieve polylog search time with constant cache size. In general, the new scheme is the first known DHT structure with the following highly-desired properties: (a) Random caching strategy with constant cache size; (b) Average search time of $O(log^{2}(N))$; (c) Guaranteed search time of $O(log^{3}(N))$; (d) Truly local cache dynamics with constant overhead for node deletions and additions; (e) Self-organization from any initial network state towards the desired structure; and (f) Allows a seamless means for various trade-offs, e.g., search speed or anonymity at the expense of larger cache size.",
        "published": "2002-10-14T03:37:11Z",
        "link": "http://arxiv.org/abs/cs/0210010v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "H.3.3"
        ]
    },
    {
        "title": "A New Interpretation of Amdahl's Law and Geometric Scalability",
        "authors": [
            "Neil J. Gunther"
        ],
        "summary": "The multiprocessor effect refers to the loss of computing cycles due to processing overhead. Amdahl's law and the Multiprocessing Factor (MPF) are two scaling models used in industry and academia for estimating multiprocessor capacity in the presence of this multiprocessor effect. Both models express different laws of diminishing returns. Amdahl's law identifies diminishing processor capacity with a fixed degree of serialization in the workload, while the MPF model treats it as a constant geometric ratio. The utility of both models for performance evaluation stems from the presence of a single parameter that can be determined easily from a small set of benchmark measurements. This utility, however, is marred by a dilemma. The two models produce different results, especially for large processor configurations that are so important for today's applications. The question naturally arises: Which of these two models is the correct one to use? Ignoring this question merely reduces capacity prediction to arbitrary curve-fitting. Removing the dilemma requires a dynamical interpretation of these scaling models. We present a physical interpretation based on queueing theory and show that Amdahl's law corresponds to synchronous queueing in a bus model while the MPF model belongs to a Coxian server model. The latter exhibits unphysical effects such as sublinear response times hence, we caution against its use for large multiprocessor configurations.",
        "published": "2002-10-17T18:20:08Z",
        "link": "http://arxiv.org/abs/cs/0210017v1",
        "categories": [
            "cs.DC",
            "cs.PF",
            "B.8; C.4; C.5.5; D.4.8; F.1.2"
        ]
    },
    {
        "title": "A Historic Name-Trail Service",
        "authors": [
            "Petros Maniatis",
            "Mary Baker"
        ],
        "summary": "People change the identifiers through which they are reachable online as they change jobs or residences or Internet service providers. This kind of personal mobility makes reaching people online error-prone. As people move, they do not always know who or what has cached their now obsolete identifiers so as to inform them of the move. Use of these old identifiers can cause delivery failure of important messages, or worse, may cause delivery of messages to unintended recipients. For example, a sensitive email message sent to my now obsolete work address at a former place of employment may reach my unfriendly former boss instead of me.   In this paper we describe HINTS, a historic name-trail service. This service provides a persistent way to name willing participants online using today's transient online identifiers. HINTS accomplishes this by connecting together the names a person uses along with the times during which those names were valid for the person, thus giving people control over the historic use of their names. A correspondent who wishes to reach a mobile person can use an obsolete online name for that person, qualified with a time at which the online name was successfully used; HINTS resolves this historic name to a current valid online identifier for the intended recipient, if that recipient has chosen to leave a name trail in HINTS.",
        "published": "2002-10-19T10:27:04Z",
        "link": "http://arxiv.org/abs/cs/0210019v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "C.2.3; C.2.5"
        ]
    },
    {
        "title": "Algorithmic scalability in globally constrained conservative parallel   discrete event simulations of asynchronous systems",
        "authors": [
            "A. Kolakowska",
            "M. A. Novotny",
            "G. Korniss"
        ],
        "summary": "We consider parallel simulations for asynchronous systems employing L processing elements that are arranged on a ring. Processors communicate only among the nearest neighbors and advance their local simulated time only if it is guaranteed that this does not violate causality. In simulations with no constraints, in the infinite L-limit the utilization scales (Korniss et al, PRL 84, 2000); but, the width of the virtual time horizon diverges (i.e., the measurement phase of the algorithm does not scale). In this work, we introduce a moving window global constraint, which modifies the algorithm so that the measurement phase scales as well. We present results of systematic studies in which the system size (i.e., L and the volume load per processor) as well as the constraint are varied. The constraint eliminates the extreme fluctuations in the virtual time horizon, provides a bound on its width, and controls the average progress rate. The width of the window constraint can serve as a tuning parameter that, for a given volume load per processor, could be adjusted to optimize the utilization so as to maximize the efficiency. This result may find numerous applications in modeling the evolution of general spatially extended short-range interacting systems with asynchronous dynamics, including dynamic Monte Carlo studies.",
        "published": "2002-11-12T23:35:12Z",
        "link": "http://arxiv.org/abs/cs/0211013v1",
        "categories": [
            "cs.DC",
            "cond-mat.stat-mech",
            "cs.DS",
            "physics.comp-ph",
            "D.1.3; F.1.1; F.2.0; G.3"
        ]
    },
    {
        "title": "Use of openMosix for parallel I/O balancing on storage in Linux cluster",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this paper I present some experiences made in the matter of I/O for Linux Clustering. In particular is illustrated the use of the package openMosix, a balancer of workload for processes running in a cluster of nodes. I describe some tests for balancing the load of I/O storage massive processes in a cluster with four components. This work is been written for the proceedings of the workshop Linux cluster: the openMosix approach held at CINECA, Bologna, Italy, on 28 november 2002.",
        "published": "2002-12-06T12:57:50Z",
        "link": "http://arxiv.org/abs/cs/0212006v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.1.2; C.2.4; H.3.3"
        ]
    },
    {
        "title": "Fast Hands-free Writing by Gaze Direction",
        "authors": [
            "David J. Ward",
            "David J. C. MacKay"
        ],
        "summary": "We describe a method for text entry based on inverse arithmetic coding that relies on gaze direction and which is faster and more accurate than using an on-screen keyboard.   These benefits are derived from two innovations: the writing task is matched to the capabilities of the eye, and a language model is used to make predictable words and phrases easier to write.",
        "published": "2002-04-12T15:07:46Z",
        "link": "http://arxiv.org/abs/cs/0204030v4",
        "categories": [
            "cs.HC",
            "cs.AI",
            "H.5.2;K.4.2;H.1.1;H.5.1;I.2.7"
        ]
    },
    {
        "title": "Sotto Voce: Exploring the Interplay of Conversation and Mobile Audio   Spaces",
        "authors": [
            "Paul M. Aoki",
            "Rebecca E. Grinter",
            "Amy Hurst",
            "Margaret H. Szymanski",
            "James D. Thornton",
            "Allison Woodruff"
        ],
        "summary": "In addition to providing information to individual visitors, electronic guidebooks have the potential to facilitate social interaction between visitors and their companions. However, many systems impede visitor interaction. By contrast, our electronic guidebook, Sotto Voce, has social interaction as a primary design goal. The system enables visitors to share audio information - specifically, they can hear each other's guidebook activity using a technologically mediated audio eavesdropping mechanism. We conducted a study of visitors using Sotto Voce while touring a historic house. The results indicate that visitors are able to use the system effectively, both as a conversational resource and as an information appliance. More surprisingly, our results suggest that the technologically mediated audio often cohered the visitors' conversation and activity to a far greater degree than audio delivered through the open air.",
        "published": "2002-05-21T02:55:20Z",
        "link": "http://arxiv.org/abs/cs/0205053v1",
        "categories": [
            "cs.HC",
            "cs.SD",
            "H.5.1; H.5.2; H.5.3; H.5.5; J.5; K.3.1"
        ]
    },
    {
        "title": "Eavesdropping on Electronic Guidebooks: Observing Learning Resources in   Shared Listening Environments",
        "authors": [
            "Allison Woodruff",
            "Paul M. Aoki",
            "Rebecca E. Grinter",
            "Amy Hurst",
            "Margaret H. Szymanski",
            "James D. Thornton"
        ],
        "summary": "We describe an electronic guidebook, Sotto Voce, that enables visitors to share audio information by eavesdropping on each other's guidebook activity. We have conducted three studies of visitors using electronic guidebooks in a historic house: one study with open air audio played through speakers and two studies with eavesdropped audio. An analysis of visitor interaction in these studies suggests that eavesdropped audio provides more social and interactive learning resources than open air audio played through speakers.",
        "published": "2002-05-21T03:07:01Z",
        "link": "http://arxiv.org/abs/cs/0205054v1",
        "categories": [
            "cs.HC",
            "H.5.1; H.5.2; H.5.3; H.5.5; J.5; K.3.1"
        ]
    },
    {
        "title": "Practical Strategies for Integrating a Conversation Analyst in an   Iterative Design Process",
        "authors": [
            "Allison Woodruff",
            "Margaret H. Szymanski",
            "Rebecca E. Grinter",
            "Paul M. Aoki"
        ],
        "summary": "We present a case study of an iterative design process that includes a conversation analyst. We discuss potential benefits of conversation analysis for design, and we describe our strategies for integrating the conversation analyst in the design process. Since the analyst on our team had no previous exposure to design or engineering, and none of the other members of our team had any experience with conversation analysis, we needed to build a foundation for our interaction. One of our key strategies was to pair the conversation analyst with a designer in a highly interactive collaboration. Our tactics have been effective on our project, leading to valuable results that we believe we could not have obtained using another method. We hope that this paper can serve as a practical guide to those interested in establishing a productive and efficient working relationship between a conversation analyst and the other members of a design team.",
        "published": "2002-05-21T03:16:49Z",
        "link": "http://arxiv.org/abs/cs/0205055v1",
        "categories": [
            "cs.HC",
            "H.5.2; H.5.3"
        ]
    },
    {
        "title": "A Connection-Centric Survey of Recommender Systems Research",
        "authors": [
            "Saverio Perugini",
            "Marcos Andre Goncalves",
            "Edward A. Fox"
        ],
        "summary": "Recommender systems attempt to reduce information overload and retain customers by selecting a subset of items from a universal set based on user preferences. While research in recommender systems grew out of information retrieval and filtering, the topic has steadily advanced into a legitimate and challenging research area of its own. Recommender systems have traditionally been studied from a content-based filtering vs. collaborative design perspective. Recommendations, however, are not delivered within a vacuum, but rather cast within an informal community of users and social context. Therefore, ultimately all recommender systems make connections among people and thus should be surveyed from such a perspective. This viewpoint is under-emphasized in the recommender systems literature. We therefore take a connection-oriented viewpoint toward recommender systems research. We posit that recommendation has an inherently social element and is ultimately intended to connect people either directly as a result of explicit user modeling or indirectly through the discovery of relationships implicit in extant data. Thus, recommender systems are characterized by how they model users to bring people together: explicitly or implicitly. Finally, user modeling and the connection-centric viewpoint raise broadening and social issues--such as evaluation, targeting, and privacy and trust--which we also briefly address.",
        "published": "2002-05-22T08:36:32Z",
        "link": "http://arxiv.org/abs/cs/0205059v2",
        "categories": [
            "cs.IR",
            "cs.HC",
            "A.1;H.1.0;H.1.2;H.3.0;H.3.3;H.3.4;H.3.5;H.4.2;H.5.2;H.5.4"
        ]
    },
    {
        "title": "Interleaved semantic interpretation in environment-based parsing",
        "authors": [
            "William Schuler"
        ],
        "summary": "This paper extends a polynomial-time parsing algorithm that resolves structural ambiguity in input to a speech-based user interface by calculating and comparing the denotations of rival constituents, given some model of the interfaced application environment (Schuler 2001). The algorithm is extended to incorporate a full set of logical operators, including quantifiers and conjunctions, into this calculation without increasing the complexity of the overall algorithm beyond polynomial time, both in terms of the length of the input and the number of entities in the environment model.",
        "published": "2002-06-18T15:11:52Z",
        "link": "http://arxiv.org/abs/cs/0206026v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "I.2.7; H.2.5"
        ]
    },
    {
        "title": "A Compact Graph Model of Handwritten Images: Integration into   Authentification and Recognition",
        "authors": [
            "Denis V. Popel"
        ],
        "summary": "A novel algorithm for creating a mathematical model of curved shapes is introduced. The core of the algorithm is based on building a graph representation of the contoured image, which occupies less storage space than produced by raster compression techniques. Different advanced applications of the mathematical model are discussed: recognition of handwritten characters and verification of handwritten text and signatures for authentification purposes. Reducing the storage requirements due to the efficient mathematical model results in faster retrieval and processing times. The experimental outcomes in compression of contoured images and recognition of handwritten numerals are given.",
        "published": "2002-07-04T16:02:41Z",
        "link": "http://arxiv.org/abs/cs/0207013v1",
        "categories": [
            "cs.HC",
            "cs.DS",
            "G.2.2"
        ]
    },
    {
        "title": "Current state of the Sonix -- the IBR-2 instrument control software and   plans for future developments",
        "authors": [
            "A. S. Kirilov"
        ],
        "summary": "The Sonix is the main control software for the IBR-2 instruments. This is a modular configurable and flexible system created using the Varman (real time database) and the X11/OS9 graphical package in the OS-9 environment. In the last few years we were mostly focused on making this system more reliable and user friendly. Because the VME hardware and software upgrade is rather expensive we would like to replace existing VME + OS9 control computers with the PC+Windows XP ones in the future. This could be done with the help of VME-PCI adapters.",
        "published": "2002-10-16T07:42:30Z",
        "link": "http://arxiv.org/abs/cs/0210014v1",
        "categories": [
            "cs.HC",
            "D.2.2"
        ]
    },
    {
        "title": "Prosody Based Co-analysis for Continuous Recognition of Coverbal   Gestures",
        "authors": [
            "Sanshzar Kettebekov",
            "Mohammed Yeasin",
            "Rajeev Sharma"
        ],
        "summary": "Although speech and gesture recognition has been studied extensively, all the successful attempts of combining them in the unified framework were semantically motivated, e.g., keyword-gesture cooccurrence. Such formulations inherited the complexity of natural language processing. This paper presents a Bayesian formulation that uses a phenomenon of gesture and speech articulation for improving accuracy of automatic recognition of continuous coverbal gestures. The prosodic features from the speech signal were coanalyzed with the visual signal to learn the prior probability of co-occurrence of the prominent spoken segments with the particular kinematical phases of gestures. It was found that the above co-analysis helps in detecting and disambiguating visually small gestures, which subsequently improves the rate of continuous gesture recognition. The efficacy of the proposed approach was demonstrated on a large database collected from the weather channel broadcast. This formulation opens new avenues for bottom-up frameworks of multimodal integration.",
        "published": "2002-11-05T19:27:32Z",
        "link": "http://arxiv.org/abs/cs/0211005v1",
        "categories": [
            "cs.CV",
            "cs.HC",
            "H.5.2; I.5.4"
        ]
    },
    {
        "title": "Annotations for HTML to VoiceXML Transcoding: Producing Voice WebPages   with Usability in Mind",
        "authors": [
            "Zhiyan Shao",
            "Robert Capra",
            "Manuel A. Perez-Quinones"
        ],
        "summary": "Web pages contain a large variety of information, but are largely designed for use by graphical web browsers. Mobile access to web-based information often requires presenting HTML web pages using channels that are limited in their graphical capabilities such as small-screens or audio-only interfaces. Content transcoding and annotations have been explored as methods for intelligently presenting HTML documents. Much of this work has focused on transcoding for small-screen devices such as are found on PDAs and cell phones. Here, we focus on the use of annotations and transcoding for presenting HTML content through a voice user interface instantiated in VoiceXML. This transcoded voice interface is designed with an assumption that it will not be used for extended web browsing by voice, but rather to quickly gain directed access to information on web pages. We have found repeated structures that are common in the presentation of data on web pages that are well suited for voice presentation and navigation. In this paper, we describe these structures and their use in an annotation system we have implemented that produces a VoiceXML interface to information originally embedded in HTML documents. We describe the transcoding process used to translate HTML into VoiceXML, including transcoding features we have designed to lead to highly usable VoiceXML code.",
        "published": "2002-11-26T15:41:59Z",
        "link": "http://arxiv.org/abs/cs/0211037v1",
        "categories": [
            "cs.HC",
            "H.1.2, H.5.2"
        ]
    },
    {
        "title": "Local Community Identification through User Access Patterns",
        "authors": [
            "Rodrigo B. Almeida",
            "Virgilio A. F. Almeida"
        ],
        "summary": "Community identification algorithms have been used to enhance the quality of the services perceived by its users. Although algorithms for community have a widespread use in the Web, their application to portals or specific subsets of the Web has not been much studied. In this paper, we propose a technique for local community identification that takes into account user access behavior derived from access logs of servers in the Web. The technique takes a departure from the existing community algorithms since it changes the focus of in terest, moving from authors to users. Our approach does not use relations imposed by authors (e.g. hyperlinks in the case of Web pages). It uses information derived from user accesses to a service in order to infer relationships. The communities identified are of great interest to content providers since they can be used to improve quality of their services. We also propose an evaluation methodology for analyzing the results obtained by the algorithm. We present two case studies based on actual data from two services: an online bookstore and an online radio. The case of the online radio is particularly relevant, because it emphasizes the contribution of the proposed algorithm to find out communities in an environment (i.e., streaming media service) without links, that represent the relations imposed by authors (e.g. hyperlinks in the case of Web pages).",
        "published": "2002-12-16T17:56:33Z",
        "link": "http://arxiv.org/abs/cs/0212045v1",
        "categories": [
            "cs.IR",
            "cs.HC",
            "I.5.3;H.1.2;J.4"
        ]
    }
]