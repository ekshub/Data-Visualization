[
    {
        "title": "Exactness of Belief Propagation for Some Graphical Models with Loops",
        "authors": [
            "Michael Chertkov"
        ],
        "summary": "It is well known that an arbitrary graphical model of statistical inference defined on a tree, i.e. on a graph without loops, is solved exactly and efficiently by an iterative Belief Propagation (BP) algorithm convergent to unique minimum of the so-called Bethe free energy functional. For a general graphical model on a loopy graph the functional may show multiple minima, the iterative BP algorithm may converge to one of the minima or may not converge at all, and the global minimum of the Bethe free energy functional is not guaranteed to correspond to the optimal Maximum-Likelihood (ML) solution in the zero-temperature limit. However, there are exceptions to this general rule, discussed in \\cite{05KW} and \\cite{08BSS} in two different contexts, where zero-temperature version of the BP algorithm finds ML solution for special models on graphs with loops. These two models share a key feature: their ML solutions can be found by an efficient Linear Programming (LP) algorithm with a Totally-Uni-Modular (TUM) matrix of constraints. Generalizing the two models we consider a class of graphical models reducible in the zero temperature limit to LP with TUM constraints. Assuming that a gedanken algorithm, g-BP, funding the global minimum of the Bethe free energy is available we show that in the limit of zero temperature g-BP outputs the ML solution. Our consideration is based on equivalence established between gapless Linear Programming (LP) relaxation of the graphical model in the $T\\to 0$ limit and respective LP version of the Bethe-Free energy minimization.",
        "published": "2008-01-02T06:07:07Z",
        "link": "http://arxiv.org/abs/0801.0341v4",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.other",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Le terme et le concept : fondements d'une ontoterminologie",
        "authors": [
            "Christophe Roche"
        ],
        "summary": "Most definitions of ontology, viewed as a \"specification of a conceptualization\", agree on the fact that if an ontology can take different forms, it necessarily includes a vocabulary of terms and some specification of their meaning in relation to the domain's conceptualization. And as domain knowledge is mainly conveyed through scientific and technical texts, we can hope to extract some useful information from them for building ontology. But is it as simple as this? In this article we shall see that the lexical structure, i.e. the network of words linked by linguistic relationships, does not necessarily match the domain conceptualization. We have to bear in mind that writing documents is the concern of textual linguistics, of which one of the principles is the incompleteness of text, whereas building ontology - viewed as task-independent knowledge - is concerned with conceptualization based on formal and not natural languages. Nevertheless, the famous Sapir and Whorf hypothesis, concerning the interdependence of thought and language, is also applicable to formal languages. This means that the way an ontology is built and a concept is defined depends directly on the formal language which is used; and the results will not be the same. The introduction of the notion of ontoterminology allows to take into account epistemological principles for formal ontology building.",
        "published": "2008-01-08T20:12:02Z",
        "link": "http://arxiv.org/abs/0801.1275v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Stream Computing",
        "authors": [
            "Subhash Kak"
        ],
        "summary": "Stream computing is the use of multiple autonomic and parallel modules together with integrative processors at a higher level of abstraction to embody \"intelligent\" processing. The biological basis of this computing is sketched and the matter of learning is examined.",
        "published": "2008-01-09T14:59:31Z",
        "link": "http://arxiv.org/abs/0801.1336v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Factored Value Iteration Converges",
        "authors": [
            "Istvan Szita",
            "Andras Lorincz"
        ],
        "summary": "In this paper we propose a novel algorithm, factored value iteration (FVI), for the approximate solution of factored Markov decision processes (fMDPs). The traditional approximate value iteration algorithm is modified in two ways. For one, the least-squares projection operator is modified so that it does not increase max-norm, and thus preserves convergence. The other modification is that we uniformly sample polynomially many samples from the (exponentially large) state space. This way, the complexity of our algorithm becomes polynomial in the size of the fMDP description length. We prove that the algorithm is convergent. We also derive an upper bound on the difference between our approximate solution and the optimal one, and also on the error introduced by sampling. We analyze various projection operators with respect to their computation complexity and their convergence when combined with approximate value iteration.",
        "published": "2008-01-14T13:09:06Z",
        "link": "http://arxiv.org/abs/0801.2069v2",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Analysis of Estimation of Distribution Algorithms and Genetic Algorithms   on NK Landscapes",
        "authors": [
            "Martin Pelikan"
        ],
        "summary": "This study analyzes performance of several genetic and evolutionary algorithms on randomly generated NK fitness landscapes with various values of n and k. A large number of NK problem instances are first generated for each n and k, and the global optimum of each instance is obtained using the branch-and-bound algorithm. Next, the hierarchical Bayesian optimization algorithm (hBOA), the univariate marginal distribution algorithm (UMDA), and the simple genetic algorithm (GA) with uniform and two-point crossover operators are applied to all generated instances. Performance of all algorithms is then analyzed and compared, and the results are discussed.",
        "published": "2008-01-21T00:20:50Z",
        "link": "http://arxiv.org/abs/0801.3111v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.6; I.2.8; G.1.6"
        ]
    },
    {
        "title": "iBOA: The Incremental Bayesian Optimization Algorithm",
        "authors": [
            "Martin Pelikan",
            "Kumara Sastry",
            "David E. Goldberg"
        ],
        "summary": "This paper proposes the incremental Bayesian optimization algorithm (iBOA), which modifies standard BOA by removing the population of solutions and using incremental updates of the Bayesian network. iBOA is shown to be able to learn and exploit unrestricted Bayesian networks using incremental techniques for updating both the structure as well as the parameters of the probabilistic model. This represents an important step toward the design of competent incremental estimation of distribution algorithms that can solve difficult nearly decomposable problems scalably and reliably.",
        "published": "2008-01-21T00:34:55Z",
        "link": "http://arxiv.org/abs/0801.3113v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.6; I.2.8; G.1.6"
        ]
    },
    {
        "title": "From k-SAT to k-CSP: Two Generalized Algorithms",
        "authors": [
            "Liang Li",
            "Xin Li",
            "Tian Liu",
            "Ke Xu"
        ],
        "summary": "Constraint satisfaction problems (CSPs) models many important intractable NP-hard problems such as propositional satisfiability problem (SAT). Algorithms with non-trivial upper bounds on running time for restricted SAT with bounded clause length k (k-SAT) can be classified into three styles: DPLL-like, PPSZ-like and Local Search, with local search algorithms having already been generalized to CSP with bounded constraint arity k (k-CSP). We generalize a DPLL-like algorithm in its simplest form and a PPSZ-like algorithm from k-SAT to k-CSP. As far as we know, this is the first attempt to use PPSZ-like strategy to solve k-CSP, and before little work has been focused on the DPLL-like or PPSZ-like strategies for k-CSP.",
        "published": "2008-01-21T08:07:33Z",
        "link": "http://arxiv.org/abs/0801.3147v1",
        "categories": [
            "cs.DS",
            "cs.AI",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "On the Effects of Idiotypic Interactions for Recommendation Communities   in Artificial Immune Systems",
        "authors": [
            "Steve Cayzer",
            "Uwe Aickelin"
        ],
        "summary": "It has previously been shown that a recommender based on immune system idiotypic principles can out perform one based on correlation alone. This paper reports the results of work in progress, where we undertake some investigations into the nature of this beneficial effect. The initial findings are that the immune system recommender tends to produce different neighbourhoods, and that the superior performance of this recommender is due partly to the different neighbourhoods, and partly to the way that the idiotypic effect is used to weight each neighbours recommendations.",
        "published": "2008-01-23T09:59:06Z",
        "link": "http://arxiv.org/abs/0801.3539v3",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A Recommender System based on the Immune Network",
        "authors": [
            "Steve Cazyer",
            "Uwe Aickelin"
        ],
        "summary": "The immune system is a complex biological system with a highly distributed, adaptive and self-organising nature. This paper presents an artificial immune system (AIS) that exploits some of these characteristics and is applied to the task of film recommendation by collaborative filtering (CF). Natural evolution and in particular the immune system have not been designed for classical optimisation. However, for this problem, we are not interested in finding a single optimum. Rather we intend to identify a sub-set of good matches on which recommendations can be based. It is our hypothesis that an AIS built on two central aspects of the biological immune system will be an ideal candidate to achieve this: Antigen - antibody interaction for matching and antibody - antibody interaction for diversity. Computational results are presented in support of this conjecture and compared to those found by other CF techniques.",
        "published": "2008-01-23T10:42:49Z",
        "link": "http://arxiv.org/abs/0801.3547v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "The Danger Theory and Its Application to Artificial Immune Systems",
        "authors": [
            "Uwe Aickelin",
            "Steve Cayzer"
        ],
        "summary": "Over the last decade, a new idea challenging the classical self-non-self viewpoint has become popular amongst immunologists. It is called the Danger Theory. In this conceptual paper, we look at this theory from the perspective of Artificial Immune System practitioners. An overview of the Danger Theory is presented with particular emphasis on analogies in the Artificial Immune Systems world. A number of potential application areas are then used to provide a framing for a critical assessment of the concept, and its relevance for Artificial Immune Systems.",
        "published": "2008-01-23T11:01:31Z",
        "link": "http://arxiv.org/abs/0801.3549v3",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CR"
        ]
    },
    {
        "title": "Partnering Strategies for Fitness Evaluation in a Pyramidal Evolutionary   Algorithm",
        "authors": [
            "Uwe Aickelin",
            "Larry Bull"
        ],
        "summary": "This paper combines the idea of a hierarchical distributed genetic algorithm with different inter-agent partnering strategies. Cascading clusters of sub-populations are built from bottom up, with higher-level sub-populations optimising larger parts of the problem. Hence higher-level sub-populations search a larger search space with a lower resolution whilst lower-level sub-populations search a smaller search space with a higher resolution. The effects of different partner selection schemes for (sub-)fitness evaluation purposes are examined for two multiple-choice optimisation problems. It is shown that random partnering strategies perform best by providing better sampling and more diversity.",
        "published": "2008-01-23T11:12:39Z",
        "link": "http://arxiv.org/abs/0801.3550v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "On the Scaling Window of Model RB",
        "authors": [
            "Chunyan Zhao",
            "Ke Xu",
            "Zhiming Zheng"
        ],
        "summary": "This paper analyzes the scaling window of a random CSP model (i.e. model RB) for which we can identify the threshold points exactly, denoted by $r_{cr}$ or $p_{cr}$. For this model, we establish the scaling window $W(n,\\delta)=(r_{-}(n,\\delta), r_{+}(n,\\delta))$ such that the probability of a random instance being satisfiable is greater than $1-\\delta$ for $r<r_{-}(n,\\delta)$ and is less than $\\delta$ for $r>r_{+}(n,\\delta)$. Specifically, we obtain the following result $$W(n,\\delta)=(r_{cr}-\\Theta(\\frac{1}{n^{1-\\epsilon}\\ln n}), \\ r_{cr}+\\Theta(\\frac{1}{n\\ln n})),$$ where $0\\leq\\epsilon<1$ is a constant. A similar result with respect to the other parameter $p$ is also obtained. Since the instances generated by model RB have been shown to be hard at the threshold, this is the first attempt, as far as we know, to analyze the scaling window of such a model with hard instances.",
        "published": "2008-01-25T02:18:00Z",
        "link": "http://arxiv.org/abs/0801.3871v1",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "cs.AI"
        ]
    },
    {
        "title": "Movie Recommendation Systems Using An Artificial Immune System",
        "authors": [
            "Qi Chen",
            "Uwe Aickelin"
        ],
        "summary": "We apply the Artificial Immune System (AIS) technology to the Collaborative Filtering (CF) technology when we build the movie recommendation system. Two different affinity measure algorithms of AIS, Kendall tau and Weighted Kappa, are used to calculate the correlation coefficients for this movie recommendation system. From the testing we think that Weighted Kappa is more suitable than Kendall tau for movie problems.",
        "published": "2008-01-28T14:19:12Z",
        "link": "http://arxiv.org/abs/0801.4287v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "On Affinity Measures for Artificial Immune System Movie Recommenders",
        "authors": [
            "Uwe Aickelin",
            "Qi Chen"
        ],
        "summary": "We combine Artificial Immune Systems 'AIS', technology with Collaborative Filtering 'CF' and use it to build a movie recommendation system. We already know that Artificial Immune Systems work well as movie recommenders from previous work by Cayzer and Aickelin 3, 4, 5. Here our aim is to investigate the effect of different affinity measure algorithms for the AIS. Two different affinity measures, Kendalls Tau and Weighted Kappa, are used to calculate the correlation coefficients for the movie recommender. We compare the results with those published previously and show that Weighted Kappa is more suitable than others for movie problems. We also show that AIS are generally robust movie recommenders and that, as long as a suitable affinity measure is chosen, results are good.",
        "published": "2008-01-28T15:14:45Z",
        "link": "http://arxiv.org/abs/0801.4307v3",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CY"
        ]
    },
    {
        "title": "Artificial Immune Systems (AIS) - A New Paradigm for Heuristic Decision   Making",
        "authors": [
            "Uwe Aickelin"
        ],
        "summary": "Over the last few years, more and more heuristic decision making techniques have been inspired by nature, e.g. evolutionary algorithms, ant colony optimisation and simulated annealing. More recently, a novel computational intelligence technique inspired by immunology has emerged, called Artificial Immune Systems (AIS). This immune system inspired technique has already been useful in solving some computational problems. In this keynote, we will very briefly describe the immune system metaphors that are relevant to AIS. We will then give some illustrative real-world problems suitable for AIS use and show a step-by-step algorithm walkthrough. A comparison of AIS to other well-known algorithms and areas for future work will round this keynote off. It should be noted that as AIS is still a young and evolving field, there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from the examples given here.",
        "published": "2008-01-28T15:32:05Z",
        "link": "http://arxiv.org/abs/0801.4314v3",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "On the Complexity of Binary Samples",
        "authors": [
            "Joel Ratsaby"
        ],
        "summary": "Consider a class $\\mH$ of binary functions $h: X\\to\\{-1, +1\\}$ on a finite interval $X=[0, B]\\subset \\Real$. Define the {\\em sample width} of $h$ on a finite subset (a sample) $S\\subset X$ as $\\w_S(h) \\equiv \\min_{x\\in S} |\\w_h(x)|$, where $\\w_h(x) = h(x) \\max\\{a\\geq 0: h(z)=h(x), x-a\\leq z\\leq x+a\\}$. Let $\\mathbb{S}_\\ell$ be the space of all samples in $X$ of cardinality $\\ell$ and consider sets of wide samples, i.e., {\\em hypersets} which are defined as $A_{\\beta, h} = \\{S\\in \\mathbb{S}_\\ell: \\w_{S}(h) \\geq \\beta\\}$. Through an application of the Sauer-Shelah result on the density of sets an upper estimate is obtained on the growth function (or trace) of the class $\\{A_{\\beta, h}: h\\in\\mH\\}$, $\\beta>0$, i.e., on the number of possible dichotomies obtained by intersecting all hypersets with a fixed collection of samples $S\\in\\mathbb{S}_\\ell$ of cardinality $m$. The estimate is $2\\sum_{i=0}^{2\\lfloor B/(2\\beta)\\rfloor}{m-\\ell\\choose i}$.",
        "published": "2008-01-30T23:14:19Z",
        "link": "http://arxiv.org/abs/0801.4794v1",
        "categories": [
            "cs.DM",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Shallow Models for Non-Iterative Modal Logics",
        "authors": [
            "Lutz Schröder",
            "Dirk Patinson"
        ],
        "summary": "The methods used to establish PSPACE-bounds for modal logics can roughly be grouped into two classes: syntax driven methods establish that exhaustive proof search can be performed in polynomial space whereas semantic approaches directly construct shallow models. In this paper, we follow the latter approach and establish generic PSPACE-bounds for a large and heterogeneous class of modal logics in a coalgebraic framework. In particular, no complete axiomatisation of the logic under scrutiny is needed. This does not only complement our earlier, syntactic, approach conceptually, but also covers a wide variety of new examples which are difficult to harness by purely syntactic means. Apart from re-proving known complexity bounds for a large variety of structurally different logics, we apply our method to obtain previously unknown PSPACE-bounds for Elgesem's logic of agency and for graded modal logic over reflexive frames.",
        "published": "2008-02-01T13:11:09Z",
        "link": "http://arxiv.org/abs/0802.0116v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "cs.MA",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "Network as a computer: ranking paths to find flows",
        "authors": [
            "Dusko Pavlovic"
        ],
        "summary": "We explore a simple mathematical model of network computation, based on Markov chains. Similar models apply to a broad range of computational phenomena, arising in networks of computers, as well as in genetic, and neural nets, in social networks, and so on. The main problem of interaction with such spontaneously evolving computational systems is that the data are not uniformly structured. An interesting approach is to try to extract the semantical content of the data from their distribution among the nodes. A concept is then identified by finding the community of nodes that share it. The task of data structuring is thus reduced to the task of finding the network communities, as groups of nodes that together perform some non-local data processing. Towards this goal, we extend the ranking methods from nodes to paths. This allows us to extract some information about the likely flow biases from the available static information about the network.",
        "published": "2008-02-10T05:33:37Z",
        "link": "http://arxiv.org/abs/0802.1306v1",
        "categories": [
            "cs.IR",
            "cs.AI",
            "math.CT",
            "H.3.3; I.2.4; I.2.6"
        ]
    },
    {
        "title": "Les Agents comme des interpréteurs Scheme : Spécification dynamique   par la communication",
        "authors": [
            "Clément Jonquet",
            "Stefano A. Cerri"
        ],
        "summary": "We proposed in previous papers an extension and an implementation of the STROBE model, which regards the Agents as Scheme interpreters. These Agents are able to interpret messages in a dedicated environment including an interpreter that learns from the current conversation therefore representing evolving meta-level Agent's knowledge. When the Agent's interpreter is a nondeterministic one, the dialogues may consist of subsequent refinements of specifications in the form of constraint sets. The paper presents a worked out example of dynamic service generation - such as necessary on Grids - by exploiting STROBE Agents equipped with a nondeterministic interpreter. It shows how enabling dynamic specification of a problem. Then it illustrates how these principles could be effective for other applications. Details of the implementation are not provided here, but are available.",
        "published": "2008-02-11T08:55:46Z",
        "link": "http://arxiv.org/abs/0802.1393v1",
        "categories": [
            "cs.MA",
            "cs.AI"
        ]
    },
    {
        "title": "New Implementation Framework for Saturation-Based Reasoning",
        "authors": [
            "Alexandre Riazanov"
        ],
        "summary": "The saturation-based reasoning methods are among the most theoretically developed ones and are used by most of the state-of-the-art first-order logic reasoners. In the last decade there was a sharp increase in performance of such systems, which I attribute to the use of advanced calculi and the intensified research in implementation techniques. However, nowadays we are witnessing a slowdown in performance progress, which may be considered as a sign that the saturation-based technology is reaching its inherent limits. The position I am trying to put forward in this paper is that such scepticism is premature and a sharp improvement in performance may potentially be reached by adopting new architectural principles for saturation. The top-level algorithms and corresponding designs used in the state-of-the-art saturation-based theorem provers have (at least) two inherent drawbacks: the insufficient flexibility of the used inference selection mechanisms and the lack of means for intelligent prioritising of search directions. In this position paper I analyse these drawbacks and present two ideas on how they could be overcome. In particular, I propose a flexible low-cost high-precision mechanism for inference selection, intended to overcome problems associated with the currently used instances of clause selection-based procedures. I also outline a method for intelligent prioritising of search directions, based on probing the search space by exploring generalised search directions. I discuss some technical issues related to implementation of the proposed architectural principles and outline possible solutions.",
        "published": "2008-02-15T01:51:29Z",
        "link": "http://arxiv.org/abs/0802.2127v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Anisotropic selection in cellular genetic algorithms",
        "authors": [
            "David Simoncini",
            "Sébastien Verel",
            "Philippe Collard",
            "Manuel Clergue"
        ],
        "summary": "In this paper we introduce a new selection scheme in cellular genetic algorithms (cGAs). Anisotropic Selection (AS) promotes diversity and allows accurate control of the selective pressure. First we compare this new scheme with the classical rectangular grid shapes solution according to the selective pressure: we can obtain the same takeover time with the two techniques although the spreading of the best individual is different. We then give experimental results that show to what extent AS promotes the emergence of niches that support low coupling and high cohesion. Finally, using a cGA with anisotropic selection on a Quadratic Assignment Problem we show the existence of an anisotropic optimal value for which the best average performance is observed. Further work will focus on the selective pressure self-adjustment ability provided by this new selection scheme.",
        "published": "2008-02-18T07:30:04Z",
        "link": "http://arxiv.org/abs/0802.2429v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Design and Implementation of Aggregate Functions in the DLV System",
        "authors": [
            "Wolfgang Faber",
            "Gerald Pfeifer",
            "Nicola Leone",
            "Tina Dell'Armi",
            "Giuseppe Ielpa"
        ],
        "summary": "Disjunctive Logic Programming (DLP) is a very expressive formalism: it allows for expressing every property of finite structures that is decidable in the complexity class SigmaP2 (= NP^NP). Despite this high expressiveness, there are some simple properties, often arising in real-world applications, which cannot be encoded in a simple and natural manner. Especially properties that require the use of arithmetic operators (like sum, times, or count) on a set or multiset of elements, which satisfy some conditions, cannot be naturally expressed in classic DLP.   To overcome this deficiency, we extend DLP by aggregate functions in a conservative way. In particular, we avoid the introduction of constructs with disputed semantics, by requiring aggregates to be stratified. We formally define the semantics of the extended language (called DLP^A), and illustrate how it can be profitably used for representing knowledge. Furthermore, we analyze the computational complexity of DLP^A, showing that the addition of aggregates does not bring a higher cost in that respect. Finally, we provide an implementation of DLP^A in DLV -- a state-of-the-art DLP system -- and report on experiments which confirm the usefulness of the proposed extension also for the efficiency of computation.",
        "published": "2008-02-21T15:44:09Z",
        "link": "http://arxiv.org/abs/0802.3137v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; I.2.4; D.3.1; D.1.6"
        ]
    },
    {
        "title": "Characterization of the convergence of stationary Fokker-Planck learning",
        "authors": [
            "Arturo Berrones"
        ],
        "summary": "The convergence properties of the stationary Fokker-Planck algorithm for the estimation of the asymptotic density of stochastic search processes is studied. Theoretical and empirical arguments for the characterization of convergence of the estimation in the case of separable and nonseparable nonlinear optimization problems are given. Some implications of the convergence of stationary Fokker-Planck learning for the inference of parameters in artificial neural network models are outlined.",
        "published": "2008-02-21T23:41:09Z",
        "link": "http://arxiv.org/abs/0802.3235v3",
        "categories": [
            "cs.NE",
            "cond-mat.dis-nn",
            "cs.AI"
        ]
    },
    {
        "title": "Use of Rapid Probabilistic Argumentation for Ranking on Large Complex   Networks",
        "authors": [
            "Burak Cetin",
            "Haluk Bingol"
        ],
        "summary": "We introduce a family of novel ranking algorithms called ERank which run in linear/near linear time and build on explicitly modeling a network as uncertain evidence. The model uses Probabilistic Argumentation Systems (PAS) which are a combination of probability theory and propositional logic, and also a special case of Dempster-Shafer Theory of Evidence. ERank rapidly generates approximate results for the NP-complete problem involved enabling the use of the technique in large networks. We use a previously introduced PAS model for citation networks generalizing it for all networks. We propose a statistical test to be used for comparing the performances of different ranking algorithms based on a clustering validity test. Our experimentation using this test on a real-world network shows ERank to have the best performance in comparison to well-known algorithms including PageRank, closeness, and betweenness.",
        "published": "2008-02-22T11:49:16Z",
        "link": "http://arxiv.org/abs/0802.3293v1",
        "categories": [
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Processing Information in Quantum Decision Theory",
        "authors": [
            "V. I. Yukalov",
            "D. Sornette"
        ],
        "summary": "A survey is given summarizing the state of the art of describing information processing in Quantum Decision Theory, which has been recently advanced as a novel variant of decision making, based on the mathematical theory of separable Hilbert spaces. This mathematical structure captures the effect of superposition of composite prospects, including many incorporated intended actions. The theory characterizes entangled decision making, non-commutativity of subsequent decisions, and intention interference. The self-consistent procedure of decision making, in the frame of the quantum decision theory, takes into account both the available objective information as well as subjective contextual effects. This quantum approach avoids any paradox typical of classical decision theory. Conditional maximization of entropy, equivalent to the minimization of an information functional, makes it possible to connect the quantum and classical decision theories, showing that the latter is the limit of the former under vanishing interference terms.",
        "published": "2008-02-25T11:08:19Z",
        "link": "http://arxiv.org/abs/0802.3597v3",
        "categories": [
            "physics.soc-ph",
            "cs.AI",
            "quant-ph"
        ]
    },
    {
        "title": "Knowledge Technologies",
        "authors": [
            "Nick Milton"
        ],
        "summary": "Several technologies are emerging that provide new ways to capture, store, present and use knowledge. This book is the first to provide a comprehensive introduction to five of the most important of these technologies: Knowledge Engineering, Knowledge Based Engineering, Knowledge Webs, Ontologies and Semantic Webs. For each of these, answers are given to a number of key questions (What is it? How does it operate? How is a system developed? What can it be used for? What tools are available? What are the main issues?). The book is aimed at students, researchers and practitioners interested in Knowledge Management, Artificial Intelligence, Design Engineering and Web Technologies.   During the 1990s, Nick worked at the University of Nottingham on the application of AI techniques to knowledge management and on various knowledge acquisition projects to develop expert systems for military applications. In 1999, he joined Epistemics where he worked on numerous knowledge projects and helped establish knowledge management programmes at large organisations in the engineering, technology and legal sectors. He is author of the book \"Knowledge Acquisition in Practice\", which describes a step-by-step procedure for acquiring and implementing expertise. He maintains strong links with leading research organisations working on knowledge technologies, such as knowledge-based engineering, ontologies and semantic technologies.",
        "published": "2008-02-26T11:26:09Z",
        "link": "http://arxiv.org/abs/0802.3789v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.LG",
            "cs.SE"
        ]
    },
    {
        "title": "Belief Propagation and Loop Series on Planar Graphs",
        "authors": [
            "Michael Chertkov",
            "Vladimir Y. Chernyak",
            "Razvan Teodorescu"
        ],
        "summary": "We discuss a generic model of Bayesian inference with binary variables defined on edges of a planar graph. The Loop Calculus approach of [1, 2] is used to evaluate the resulting series expansion for the partition function. We show that, for planar graphs, truncating the series at single-connected loops reduces, via a map reminiscent of the Fisher transformation [3], to evaluating the partition function of the dimer matching model on an auxiliary planar graph. Thus, the truncated series can be easily re-summed, using the Pfaffian formula of Kasteleyn [4]. This allows to identify a big class of computationally tractable planar models reducible to a dimer model via the Belief Propagation (gauge) transformation. The Pfaffian representation can also be extended to the full Loop Series, in which case the expansion becomes a sum of Pfaffian contributions, each associated with dimer matchings on an extension to a subgraph of the original graph. Algorithmic consequences of the Pfaffian representation, as well as relations to quantum and non-planar models, are discussed.",
        "published": "2008-02-27T05:30:51Z",
        "link": "http://arxiv.org/abs/0802.3950v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Brain architecture: A design for natural computation",
        "authors": [
            "Marcus Kaiser"
        ],
        "summary": "Fifty years ago, John von Neumann compared the architecture of the brain with that of computers that he invented and which is still in use today. In those days, the organisation of computers was based on concepts of brain organisation. Here, we give an update on current results on the global organisation of neural systems. For neural systems, we outline how the spatial and topological architecture of neuronal and cortical networks facilitates robustness against failures, fast processing, and balanced network activation. Finally, we discuss mechanisms of self-organization for such architectures. After all, the organization of the brain might again inspire computer architecture.",
        "published": "2008-02-27T13:00:38Z",
        "link": "http://arxiv.org/abs/0802.4010v1",
        "categories": [
            "q-bio.NC",
            "cs.AI",
            "cs.NE",
            "physics.soc-ph"
        ]
    },
    {
        "title": "The Generation of Textual Entailment with NLML in an Intelligent   Dialogue system for Language Learning CSIEC",
        "authors": [
            "Jiyou Jia"
        ],
        "summary": "This research report introduces the generation of textual entailment within the project CSIEC (Computer Simulation in Educational Communication), an interactive web-based human-computer dialogue system with natural language for English instruction. The generation of textual entailment (GTE) is critical to the further improvement of CSIEC project. Up to now we have found few literatures related with GTE. Simulating the process that a human being learns English as a foreign language we explore our naive approach to tackle the GTE problem and its algorithm within the framework of CSIEC, i.e. rule annotation in NLML, pattern recognition (matching), and entailment transformation. The time and space complexity of our algorithm is tested with some entailment examples. Further works include the rules annotation based on the English textbooks and a GUI interface for normal users to edit the entailment rules.",
        "published": "2008-02-29T06:16:29Z",
        "link": "http://arxiv.org/abs/0802.4326v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CY"
        ]
    },
    {
        "title": "Automated Termination Proofs for Logic Programs by Term Rewriting",
        "authors": [
            "P. Schneider-Kamp",
            "J. Giesl",
            "A. Serebrenik",
            "R. Thiemann"
        ],
        "summary": "There are two kinds of approaches for termination analysis of logic programs: \"transformational\" and \"direct\" ones. Direct approaches prove termination directly on the basis of the logic program. Transformational approaches transform a logic program into a term rewrite system (TRS) and then analyze termination of the resulting TRS instead. Thus, transformational approaches make all methods previously developed for TRSs available for logic programs as well. However, the applicability of most existing transformations is quite restricted, as they can only be used for certain subclasses of logic programs. (Most of them are restricted to well-moded programs.) In this paper we improve these transformations such that they become applicable for any definite logic program. To simulate the behavior of logic programs by TRSs, we slightly modify the notion of rewriting by permitting infinite terms. We show that our transformation results in TRSs which are indeed suitable for automated termination analysis. In contrast to most other methods for termination of logic programs, our technique is also sound for logic programming without occur check, which is typically used in practice. We implemented our approach in the termination prover AProVE and successfully evaluated it on a large collection of examples.",
        "published": "2008-03-02T14:53:01Z",
        "link": "http://arxiv.org/abs/0803.0014v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL",
            "F.3.1; D.1.6; I.2.2; I.2.3"
        ]
    },
    {
        "title": "The Future of Scientific Simulations: from Artificial Life to Artificial   Cosmogenesis",
        "authors": [
            "Clement Vidal"
        ],
        "summary": "This philosophical paper explores the relation between modern scientific simulations and the future of the universe. We argue that a simulation of an entire universe will result from future scientific activity. This requires us to tackle the challenge of simulating open-ended evolution at all levels in a single simulation. The simulation should encompass not only biological evolution, but also physical evolution (a level below) and cultural evolution (a level above). The simulation would allow us to probe what would happen if we would \"replay the tape of the universe\" with the same or different laws and initial conditions. We also distinguish between real-world and artificial-world modelling. Assuming that intelligent life could indeed simulate an entire universe, this leads to two tentative hypotheses. Some authors have argued that we may already be in a simulation run by an intelligent entity. Or, if such a simulation could be made real, this would lead to the production of a new universe. This last direction is argued with a careful speculative philosophical approach, emphasizing the imperative to find a solution to the heat death problem in cosmology. The reader is invited to consult Annex 1 for an overview of the logical structure of this paper. -- Keywords: far future, future of science, ALife, simulation, realization, cosmology, heat death, fine-tuning, physical eschatology, cosmological natural selection, cosmological artificial selection, artificial cosmogenesis, selfish biocosm hypothesis, meduso-anthropic principle, developmental singularity hypothesis, role of intelligent life.",
        "published": "2008-03-07T14:42:02Z",
        "link": "http://arxiv.org/abs/0803.1087v3",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Serious Flaws in Korf et al.'s Analysis on Time Complexity of A*",
        "authors": [
            "Hang Dinh"
        ],
        "summary": "This paper has been withdrawn.",
        "published": "2008-03-08T02:47:27Z",
        "link": "http://arxiv.org/abs/0803.1207v3",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Hybrid Reasoning and the Future of Iconic Representations",
        "authors": [
            "Catherine Recanati"
        ],
        "summary": "We give a brief overview of the main characteristics of diagrammatic reasoning, analyze a case of human reasoning in a mastermind game, and explain why hybrid representation systems (HRS) are particularly attractive and promising for Artificial General Intelligence and Computer Science in general.",
        "published": "2008-03-10T16:44:55Z",
        "link": "http://arxiv.org/abs/0803.1457v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Dempster-Shafer for Anomaly Detection",
        "authors": [
            "Qi Chen",
            "Uwe Aickelin"
        ],
        "summary": "In this paper, we implement an anomaly detection system using the Dempster-Shafer method. Using two standard benchmark problems we show that by combining multiple signals it is possible to achieve better results than by using a single signal. We further show that by applying this approach to a real-world email dataset the algorithm works for email worm detection. Dempster-Shafer can be a promising method for anomaly detection problems with multiple features (data sources), and two or more classes.",
        "published": "2008-03-11T12:39:01Z",
        "link": "http://arxiv.org/abs/0803.1568v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CR"
        ]
    },
    {
        "title": "Improved evolutionary generation of XSLT stylesheets",
        "authors": [
            "Pablo Garcia-Sanchez",
            "J. L. J. Laredo",
            "J. P. Sevilla",
            "Pedro Castillo",
            "J. J. Merelo"
        ],
        "summary": "This paper introduces a procedure based on genetic programming to evolve XSLT programs (usually called stylesheets or logicsheets). XSLT is a general purpose, document-oriented functional language, generally used to transform XML documents (or, in general, solve any problem that can be coded as an XML document). The proposed solution uses a tree representation for the stylesheets as well as diverse specific operators in order to obtain, in the studied cases and a reasonable time, a XSLT stylesheet that performs the transformation. Several types of representation have been compared, resulting in different performance and degree of success.",
        "published": "2008-03-13T11:43:25Z",
        "link": "http://arxiv.org/abs/0803.1926v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Danger Theory: The Link between AIS and IDS?",
        "authors": [
            "Uwe Aickelin",
            "Peter Bentley",
            "Steve Cayzer",
            "Kim Jungwon",
            "Julie McLeod"
        ],
        "summary": "We present ideas about creating a next generation Intrusion Detection System based on the latest immunological theories. The central challenge with computer security is determining the difference between normal and potentially harmful activity. For half a century, developers have protected their systems by coding rules that identify and block specific events. However, the nature of current and future threats in conjunction with ever larger IT systems urgently requires the development of automated and adaptive defensive tools. A promising solution is emerging in the form of Artificial Immune Systems. The Human Immune System can detect and defend against harmful and previously unseen invaders, so can we not build a similar Intrusion Detection System for our computers.",
        "published": "2008-03-13T16:01:10Z",
        "link": "http://arxiv.org/abs/0803.1997v2",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CR"
        ]
    },
    {
        "title": "An Ant-Based Model for Multiple Sequence Alignment",
        "authors": [
            "Frédéric Guinand",
            "Yoann Pigné"
        ],
        "summary": "Multiple sequence alignment is a key process in today's biology, and finding a relevant alignment of several sequences is much more challenging than just optimizing some improbable evaluation functions. Our approach for addressing multiple sequence alignment focuses on the building of structures in a new graph model: the factor graph model. This model relies on block-based formulation of the original problem, formulation that seems to be one of the most suitable ways for capturing evolutionary aspects of alignment. The structures are implicitly built by a colony of ants laying down pheromones in the factor graphs, according to relations between blocks belonging to the different sequences.",
        "published": "2008-03-14T06:58:56Z",
        "link": "http://arxiv.org/abs/0803.2092v1",
        "categories": [
            "q-bio.QM",
            "cs.AI"
        ]
    },
    {
        "title": "Conditioning Probabilistic Databases",
        "authors": [
            "Christoph Koch",
            "Dan Olteanu"
        ],
        "summary": "Past research on probabilistic databases has studied the problem of answering queries on a static database. Application scenarios of probabilistic databases however often involve the conditioning of a database using additional information in the form of new evidence. The conditioning problem is thus to transform a probabilistic database of priors into a posterior probabilistic database which is materialized for subsequent query processing or further refinement. It turns out that the conditioning problem is closely related to the problem of computing exact tuple confidence values.   It is known that exact confidence computation is an NP-hard problem. This has led researchers to consider approximation techniques for confidence computation. However, neither conditioning nor exact confidence computation can be solved using such techniques.   In this paper we present efficient techniques for both problems. We study several problem decomposition methods and heuristics that are based on the most successful search techniques from constraint satisfaction, such as the Davis-Putnam algorithm. We complement this with a thorough experimental evaluation of the algorithms proposed. Our experiments show that our exact algorithms scale well to realistic database sizes and can in some scenarios compete with the most efficient previous approximation algorithms.",
        "published": "2008-03-14T17:23:34Z",
        "link": "http://arxiv.org/abs/0803.2212v2",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.1; H.2.4"
        ]
    },
    {
        "title": "Tableau-based decision procedures for logics of strategic ability in   multi-agent systems",
        "authors": [
            "Valentin Goranko",
            "Dmitry Shkatov"
        ],
        "summary": "We develop an incremental tableau-based decision procedures for the   Alternating-time temporal logic ATL and some of its variants.   While running within the theoretically established complexity upper bound, we claim that our tableau is practically more efficient in the average case than other decision procedures for ATL known so far. Besides, the ease of its adaptation to variants of ATL demonstrates the flexibility of the proposed procedure.",
        "published": "2008-03-15T16:22:53Z",
        "link": "http://arxiv.org/abs/0803.2306v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.MA",
            "F.4.1; I.2.3; I.2.4; I.2.11"
        ]
    },
    {
        "title": "An Indirect Genetic Algorithm for Set Covering Problems",
        "authors": [
            "Uwe Aickelin"
        ],
        "summary": "This paper presents a new type of genetic algorithm for the set covering problem. It differs from previous evolutionary approaches first because it is an indirect algorithm, i.e. the actual solutions are found by an external decoder function. The genetic algorithm itself provides this decoder with permutations of the solution variables and other parameters. Second, it will be shown that results can be further improved by adding another indirect optimisation layer. The decoder will not directly seek out low cost solutions but instead aims for good exploitable solutions. These are then post optimised by another hill-climbing algorithm. Although seemingly more complicated, we will show that this three-stage approach has advantages in terms of solution quality, speed and adaptability to new types of problems over more direct approaches. Extensive computational results are presented and compared to the latest evolutionary and other heuristic approaches to the same data instances.",
        "published": "2008-03-20T10:58:32Z",
        "link": "http://arxiv.org/abs/0803.2965v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "On the Application of Hierarchical Coevolutionary Genetic Algorithms:   Recombination and Evaluation Partners",
        "authors": [
            "Uwe Aickelin",
            "Larry Bull"
        ],
        "summary": "This paper examines the use of a hierarchical coevolutionary genetic algorithm under different partnering strategies. Cascading clusters of sub-populations are built from the bottom up, with higher-level sub-populations optimising larger parts of the problem. Hence higher-level sub-populations potentially search a larger search space with a lower resolution whilst lower-level sub-populations search a smaller search space with a higher resolution. The effects of different partner selection schemes amongst the sub-populations on solution quality are examined for two constrained optimisation problems. We examine a number of recombination partnering strategies in the construction of higher-level individuals and a number of related schemes for evaluating sub-solutions. It is shown that partnering strategies that exploit problem-specific knowledge are superior and can counter inappropriate (sub)fitness measurements.",
        "published": "2008-03-20T11:09:39Z",
        "link": "http://arxiv.org/abs/0803.2966v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A Recommender System based on Idiotypic Artificial Immune Networks",
        "authors": [
            "Steve Cayzer",
            "Uwe Aickelin"
        ],
        "summary": "The immune system is a complex biological system with a highly distributed, adaptive and self-organising nature. This paper presents an Artificial Immune System (AIS) that exploits some of these characteristics and is applied to the task of film recommendation by Collaborative Filtering (CF). Natural evolution and in particular the immune system have not been designed for classical optimisation. However, for this problem, we are not interested in finding a single optimum. Rather we intend to identify a sub-set of good matches on which recommendations can be based. It is our hypothesis that an AIS built on two central aspects of the biological immune system will be an ideal candidate to achieve this: Antigen-antibody interaction for matching and idiotypic antibody-antibody interaction for diversity. Computational results are presented in support of this conjecture and compared to those found by other CF techniques.",
        "published": "2008-03-20T11:27:56Z",
        "link": "http://arxiv.org/abs/0803.2970v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Idiotypic Immune Networks in Mobile Robot Control",
        "authors": [
            "Amanda Whitbrook",
            "Uwe Aickelin",
            "Jonathan Garibaldi"
        ],
        "summary": "Jerne's idiotypic network theory postulates that the immune response involves inter-antibody stimulation and suppression as well as matching to antigens. The theory has proved the most popular Artificial Immune System (ais) model for incorporation into behavior-based robotics but guidelines for implementing idiotypic selection are scarce. Furthermore, the direct effects of employing the technique have not been demonstrated in the form of a comparison with non-idiotypic systems. This paper aims to address these issues. A method for integrating an idiotypic ais network with a Reinforcement Learning based control system (rl) is described and the mechanisms underlying antibody stimulation and suppression are explained in detail. Some hypotheses that account for the network advantage are put forward and tested using three systems with increasing idiotypic complexity. The basic rl, a simplified hybrid ais-rl that implements idiotypic selection independently of derived concentration levels and a full hybrid ais-rl scheme are examined. The test bed takes the form of a simulated Pioneer robot that is required to navigate through maze worlds detecting and tracking door markers.",
        "published": "2008-03-20T12:24:43Z",
        "link": "http://arxiv.org/abs/0803.2981v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.RO"
        ]
    },
    {
        "title": "Eye-Tracking Evolutionary Algorithm to minimize user's fatigue in IEC   applied to Interactive One-Max problem",
        "authors": [
            "Denis Pallez",
            "Philippe Collard",
            "Thierry Baccino",
            "Laurent Dumercy"
        ],
        "summary": "In this paper, we describe a new algorithm that consists in combining an eye-tracker for minimizing the fatigue of a user during the evaluation process of Interactive Evolutionary Computation. The approach is then applied to the Interactive One-Max optimization problem.",
        "published": "2008-03-21T16:11:38Z",
        "link": "http://arxiv.org/abs/0803.3192v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Node discovery in a networked organization",
        "authors": [
            "Yoshiharu Maeno"
        ],
        "summary": "In this paper, I present a method to solve a node discovery problem in a networked organization. Covert nodes refer to the nodes which are not observable directly. They affect social interactions, but do not appear in the surveillance logs which record the participants of the social interactions. Discovering the covert nodes is defined as identifying the suspicious logs where the covert nodes would appear if the covert nodes became overt. A mathematical model is developed for the maximal likelihood estimation of the network behind the social interactions and for the identification of the suspicious logs. Precision, recall, and F measure characteristics are demonstrated with the dataset generated from a real organization and the computationally synthesized datasets. The performance is close to the theoretical limit for any covert nodes in the networks of any topologies and sizes if the ratio of the number of observation to the number of possible communication patterns is large.",
        "published": "2008-03-24T05:53:39Z",
        "link": "http://arxiv.org/abs/0803.3363v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Robustness and Regularization of Support Vector Machines",
        "authors": [
            "Huan Xu",
            "Constantine Caramanis",
            "Shie Mannor"
        ],
        "summary": "We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classification that explicitly build in protection to noise, and at the same time control overfitting. On the analysis front, the equivalence of robustness and regularization, provides a robust optimization interpretation for the success of regularized SVMs. We use the this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well.",
        "published": "2008-03-25T03:51:59Z",
        "link": "http://arxiv.org/abs/0803.3490v2",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Multiagent Approach for the Representation of Information in a Decision   Support System",
        "authors": [
            "Fahem Kebair",
            "Frédéric Serin"
        ],
        "summary": "In an emergency situation, the actors need an assistance allowing them to react swiftly and efficiently. In this prospect, we present in this paper a decision support system that aims to prepare actors in a crisis situation thanks to a decision-making support. The global architecture of this system is presented in the first part. Then we focus on a part of this system which is designed to represent the information of the current situation. This part is composed of a multiagent system that is made of factual agents. Each agent carries a semantic feature and aims to represent a partial part of a situation. The agents develop thanks to their interactions by comparing their semantic features using proximity measures and according to specific ontologies.",
        "published": "2008-03-25T07:43:36Z",
        "link": "http://arxiv.org/abs/0803.3501v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Reinforcement Learning by Value Gradients",
        "authors": [
            "Michael Fairbank"
        ],
        "summary": "The concept of the value-gradient is introduced and developed in the context of reinforcement learning. It is shown that by learning the value-gradients exploration or stochastic behaviour is no longer needed to find locally optimal trajectories. This is the main motivation for using value-gradients, and it is argued that learning value-gradients is the actual objective of any value-function learning algorithm for control problems. It is also argued that learning value-gradients is significantly more efficient than learning just the values, and this argument is supported in experiments by efficiency gains of several orders of magnitude, in several problem domains. Once value-gradients are introduced into learning, several analyses become possible. For example, a surprising equivalence between a value-gradient learning algorithm and a policy-gradient learning algorithm is proven, and this provides a robust convergence proof for control problems using a value function with a general function approximator.",
        "published": "2008-03-25T11:57:15Z",
        "link": "http://arxiv.org/abs/0803.3539v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Preferred extensions as stable models",
        "authors": [
            "Juan Carlos Nieves",
            "Mauricio Osorio",
            "Ulises Cortés"
        ],
        "summary": "Given an argumentation framework AF, we introduce a mapping function that constructs a disjunctive logic program P, such that the preferred extensions of AF correspond to the stable models of P, after intersecting each stable model with the relevant atoms. The given mapping function is of polynomial size w.r.t. AF. In particular, we identify that there is a direct relationship between the minimal models of a propositional formula and the preferred extensions of an argumentation framework by working on representing the defeated arguments. Then we show how to infer the preferred extensions of an argumentation framework by using UNSAT algorithms and disjunctive stable model solvers. The relevance of this result is that we define a direct relationship between one of the most satisfactory argumentation semantics and one of the most successful approach of non-monotonic reasoning i.e., logic programming with the stable model semantics.",
        "published": "2008-03-26T20:08:31Z",
        "link": "http://arxiv.org/abs/0803.3812v1",
        "categories": [
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "Artificial Immune Systems Tutorial",
        "authors": [
            "Uwe Aickelin",
            "Dipankar Dasgupta"
        ],
        "summary": "The biological immune system is a robust, complex, adaptive system that defends the body from foreign pathogens. It is able to categorize all cells (or molecules) within the body as self-cells or non-self cells. It does this with the help of a distributed task force that has the intelligence to take action from a local and also a global perspective using its network of chemical messengers for communication. There are two major branches of the immune system. The innate immune system is an unchanging mechanism that detects and destroys certain invading organisms, whilst the adaptive immune system responds to previously unknown foreign cells and builds a response to them that can remain in the body over a long period of time. This remarkable information processing biological system has caught the attention of computer science in recent years. A novel computational intelligence technique, inspired by immunology, has emerged, called Artificial Immune Systems. Several concepts from the immune have been extracted and applied for solution to real world science and engineering problems. In this tutorial, we briefly describe the immune system metaphors that are relevant to existing Artificial Immune Systems methods. We will then show illustrative real-world problems suitable for Artificial Immune Systems and give a step-by-step algorithm walkthrough for one such problem. A comparison of the Artificial Immune Systems to other well-known algorithms, areas for future work, tips & tricks and a list of resources will round this tutorial off. It should be noted that as Artificial Immune Systems is still a young and evolving field, there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from time to time and from those examples given here.",
        "published": "2008-03-27T12:55:59Z",
        "link": "http://arxiv.org/abs/0803.3912v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Reflective visualization and verbalization of unconscious preference",
        "authors": [
            "Yoshiharu Maeno",
            "Yukio Ohsawa"
        ],
        "summary": "A new method is presented, that can help a person become aware of his or her unconscious preferences, and convey them to others in the form of verbal explanation. The method combines the concepts of reflection, visualization, and verbalization. The method was tested in an experiment where the unconscious preferences of the subjects for various artworks were investigated. In the experiment, two lessons were learned. The first is that it helps the subjects become aware of their unconscious preferences to verbalize weak preferences as compared with strong preferences through discussion over preference diagrams. The second is that it is effective to introduce an adjustable factor into visualization to adapt to the differences in the subjects and to foster their mutual understanding.",
        "published": "2008-03-28T09:36:58Z",
        "link": "http://arxiv.org/abs/0803.4074v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Combinatorial Explorations in Su-Doku",
        "authors": [
            "Jean-Marie Chauvet"
        ],
        "summary": "Su-Doku, a popular combinatorial puzzle, provides an excellent testbench for heuristic explorations. Several interesting questions arise from its deceptively simple set of rules. How many distinct Su-Doku grids are there? How to find a solution to a Su-Doku puzzle? Is there a unique solution to a given Su-Doku puzzle? What is a good estimation of a puzzle's difficulty? What is the minimum puzzle size (the number of \"givens\")?   This paper explores how these questions are related to the well-known alldifferent constraint which emerges in a wide variety of Constraint Satisfaction Problems (CSP) and compares various algorithmic approaches based on different formulations of Su-Doku.",
        "published": "2008-03-29T10:07:42Z",
        "link": "http://arxiv.org/abs/0803.4253v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "I.2.8; F.2.2"
        ]
    },
    {
        "title": "Grammar-Based Random Walkers in Semantic Networks",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "Semantic networks qualify the meaning of an edge relating any two vertices. Determining which vertices are most \"central\" in a semantic network is difficult because one relationship type may be deemed subjectively more important than another. For this reason, research into semantic network metrics has focused primarily on context-based rankings (i.e. user prescribed contexts). Moreover, many of the current semantic network metrics rank semantic associations (i.e. directed paths between two vertices) and not the vertices themselves. This article presents a framework for calculating semantically meaningful primary eigenvector-based metrics such as eigenvector centrality and PageRank in semantic networks using a modified version of the random walker model of Markov chain analysis. Random walkers, in the context of this article, are constrained by a grammar, where the grammar is a user defined data structure that determines the meaning of the final vertex ranking. The ideas in this article are presented within the context of the Resource Description Framework (RDF) of the Semantic Web initiative.",
        "published": "2008-03-31T00:13:26Z",
        "link": "http://arxiv.org/abs/0803.4355v2",
        "categories": [
            "cs.AI",
            "cs.DS",
            "I.2.4; F.2.1; F.2.2"
        ]
    },
    {
        "title": "Binary Decision Diagrams for Affine Approximation",
        "authors": [
            "Kevin Henshall",
            "Peter Schachte",
            "Harald Søndergaard",
            "Leigh Whiting"
        ],
        "summary": "Selman and Kautz's work on ``knowledge compilation'' established how approximation (strengthening and/or weakening) of a propositional knowledge-base can be used to speed up query processing, at the expense of completeness. In this classical approach, querying uses Horn over- and under-approximations of a given knowledge-base, which is represented as a propositional formula in conjunctive normal form (CNF). Along with the class of Horn functions, one could imagine other Boolean function classes that might serve the same purpose, owing to attractive deduction-computational properties similar to those of the Horn functions. Indeed, Zanuttini has suggested that the class of affine Boolean functions could be useful in knowledge compilation and has presented an affine approximation algorithm. Since CNF is awkward for presenting affine functions, Zanuttini considers both a sets-of-models representation and the use of modulo 2 congruence equations. In this paper, we propose an algorithm based on reduced ordered binary decision diagrams (ROBDDs). This leads to a representation which is more compact than the sets of models and, once we have established some useful properties of affine Boolean functions, a more efficient algorithm.",
        "published": "2008-04-01T05:08:44Z",
        "link": "http://arxiv.org/abs/0804.0066v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Support Vector Machine Classification with Indefinite Kernels",
        "authors": [
            "Ronny Luss",
            "Alexandre d'Aspremont"
        ],
        "summary": "We propose a method for support vector machine classification using indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex loss function, our algorithm simultaneously computes support vectors and a proxy kernel matrix used in forming the loss. This can be interpreted as a penalized kernel learning problem where indefinite kernel matrices are treated as a noisy observations of a true Mercer kernel. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the projected gradient or analytic center cutting plane methods. We compare the performance of our technique with other methods on several classic data sets.",
        "published": "2008-04-01T14:55:33Z",
        "link": "http://arxiv.org/abs/0804.0188v2",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Permeability Analysis based on information granulation theory",
        "authors": [
            "M. Sharifzadeh",
            "H. Owladeghaffari",
            "K. Shahriar",
            "E. Bakhtavar"
        ],
        "summary": "This paper describes application of information granulation theory, on the analysis of \"lugeon data\". In this manner, using a combining of Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and fuzzy granules are obtained. Balancing of crisp granules and sub- fuzzy granules, within non fuzzy information (initial granulation), is rendered in open-close iteration. Using two criteria, \"simplicity of rules \"and \"suitable adaptive threshold error level\", stability of algorithm is guaranteed. In other part of paper, rough set theory (RST), to approximate analysis, has been employed >.Validation of the proposed methods, on the large data set of in-situ permeability in rock masses, in the Shivashan dam, Iran, has been highlighted. By the implementation of the proposed algorithm on the lugeon data set, was proved the suggested method, relating the approximate analysis on the permeability, could be applied.",
        "published": "2008-04-02T13:45:51Z",
        "link": "http://arxiv.org/abs/0804.0352v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "F.4.1; K.3.2"
        ]
    },
    {
        "title": "Graphical Estimation of Permeability Using RST&NFIS",
        "authors": [
            "H. Owladeghaffari",
            "K. Shahriar W. Pedrycz"
        ],
        "summary": "This paper pursues some applications of Rough Set Theory (RST) and neural-fuzzy model to analysis of \"lugeon data\". In the manner, using Self Organizing Map (SOM) as a pre-processing the data are scaled and then the dominant rules by RST, are elicited. Based on these rules variations of permeability in the different levels of Shivashan dam, Iran has been highlighted. Then, via using a combining of SOM and an adaptive Neuro-Fuzzy Inference System (NFIS) another analysis on the data was carried out. Finally, a brief comparison between the obtained results of RST and SOM-NFIS (briefly SONFIS) has been rendered.",
        "published": "2008-04-02T13:56:10Z",
        "link": "http://arxiv.org/abs/0804.0353v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Application of Rough Set Theory to Analysis of Hydrocyclone Operation",
        "authors": [
            "H. Owladeghaffari",
            "M. Ejtemaei",
            "M. Irannajad"
        ],
        "summary": "This paper describes application of rough set theory, on the analysis of hydrocyclone operation. In this manner, using Self Organizing Map (SOM) as preprocessing step, best crisp granules of data are obtained. Then, using a combining of SOM and rough set theory (RST)-called SORST-, the dominant rules on the information table, obtained from laboratory tests, are extracted. Based on these rules, an approximate estimation on decision attribute is fulfilled. Finally, a brief comparison of this method with the SOM-NFIS system (briefly SONFIS) is highlighted.",
        "published": "2008-04-03T11:47:55Z",
        "link": "http://arxiv.org/abs/0804.0528v1",
        "categories": [
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Agent-Based Perception of an Environment in an Emergency Situation",
        "authors": [
            "Fahem Kebair",
            "Frédéric Serin",
            "Cyrille Bertelle"
        ],
        "summary": "We are interested in the problem of multiagent systems development for risk detecting and emergency response in an uncertain and partially perceived environment. The evaluation of the current situation passes by three stages inside the multiagent system. In a first time, the situation is represented in a dynamic way. The second step, consists to characterise the situation and finally, it is compared with other similar known situations. In this paper, we present an information modelling of an observed environment, that we have applied on the RoboCupRescue Simulation System. Information coming from the environment are formatted according to a taxonomy and using semantic features. The latter are defined thanks to a fine ontology of the domain and are managed by factual agents that aim to represent dynamically the current situation.",
        "published": "2008-04-03T13:45:43Z",
        "link": "http://arxiv.org/abs/0804.0558v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "An Artificial Immune System as a Recommender System for Web Sites",
        "authors": [
            "Tom Morrison",
            "Uwe Aickelin"
        ],
        "summary": "Artificial Immune Systems have been used successfully to build recommender systems for film databases. In this research, an attempt is made to extend this idea to web site recommendation. A collection of more than 1000 individuals web profiles (alternatively called preferences / favourites / bookmarks file) will be used. URLs will be classified using the DMOZ (Directory Mozilla) database of the Open Directory Project as our ontology. This will then be used as the data for the Artificial Immune Systems rather than the actual addresses. The first attempt will involve using a simple classification code number coupled with the number of pages within that classification code. However, this implementation does not make use of the hierarchical tree-like structure of DMOZ. Consideration will then be given to the construction of a similarity measure for web profiles that makes use of this hierarchical information to build a better-informed Artificial Immune System.",
        "published": "2008-04-03T14:43:44Z",
        "link": "http://arxiv.org/abs/0804.0573v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Explicit Learning: an Effort towards Human Scheduling Algorithms",
        "authors": [
            "Jingpeng Li",
            "Uwe Aickelin"
        ],
        "summary": "Scheduling problems are generally NP-hard combinatorial problems, and a lot of research has been done to solve these problems heuristically. However, most of the previous approaches are problem-specific and research into the development of a general scheduling algorithm is still in its infancy.   Mimicking the natural evolutionary process of the survival of the fittest, Genetic Algorithms (GAs) have attracted much attention in solving difficult scheduling problems in recent years. Some obstacles exist when using GAs: there is no canonical mechanism to deal with constraints, which are commonly met in most real-world scheduling problems, and small changes to a solution are difficult. To overcome both difficulties, indirect approaches have been presented (in [1] and [2]) for nurse scheduling and driver scheduling, where GAs are used by mapping the solution space, and separate decoding routines then build solutions to the original problem.",
        "published": "2008-04-03T15:31:52Z",
        "link": "http://arxiv.org/abs/0804.0580v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Symmetry Breaking for Maximum Satisfiability",
        "authors": [
            "Joao Marques-Silva",
            "Ines Lynce",
            "Vasco Manquinho"
        ],
        "summary": "Symmetries are intrinsic to many combinatorial problems including Boolean Satisfiability (SAT) and Constraint Programming (CP). In SAT, the identification of symmetry breaking predicates (SBPs) is a well-known, often effective, technique for solving hard problems. The identification of SBPs in SAT has been the subject of significant improvements in recent years, resulting in more compact SBPs and more effective algorithms. The identification of SBPs has also been applied to pseudo-Boolean (PB) constraints, showing that symmetry breaking can also be an effective technique for PB constraints. This paper extends further the application of SBPs, and shows that SBPs can be identified and used in Maximum Satisfiability (MaxSAT), as well as in its most well-known variants, including partial MaxSAT, weighted MaxSAT and weighted partial MaxSAT. As with SAT and PB, symmetry breaking predicates for MaxSAT and variants are shown to be effective for a representative number of problem domains, allowing solving problem instances that current state of the art MaxSAT solvers could not otherwise solve.",
        "published": "2008-04-03T18:19:43Z",
        "link": "http://arxiv.org/abs/0804.0599v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "On the Influence of Selection Operators on Performances in Cellular   Genetic Algorithms",
        "authors": [
            "David Simoncini",
            "Philippe Collard",
            "Sébastien Verel",
            "Manuel Clergue"
        ],
        "summary": "In this paper, we study the influence of the selective pressure on the performance of cellular genetic algorithms. Cellular genetic algorithms are genetic algorithms where the population is embedded on a toroidal grid. This structure makes the propagation of the best so far individual slow down, and allows to keep in the population potentially good solutions. We present two selective pressure reducing strategies in order to slow down even more the best solution propagation. We experiment these strategies on a hard optimization problem, the quadratic assignment problem, and we show that there is a value for of the control parameter for both which gives the best performance. This optimal value does not find explanation on only the selective pressure, measured either by take over time and diversity evolution. This study makes us conclude that we need other tools than the sole selective pressure measures to explain the performances of cellular genetic algorithms.",
        "published": "2008-04-05T13:27:51Z",
        "link": "http://arxiv.org/abs/0804.0852v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A Unified Semi-Supervised Dimensionality Reduction Framework for   Manifold Learning",
        "authors": [
            "Ratthachat Chatpatanasiri",
            "Boonserm Kijsirikul"
        ],
        "summary": "We present a general framework of semi-supervised dimensionality reduction for manifold learning which naturally generalizes existing supervised and unsupervised learning frameworks which apply the spectral decomposition. Algorithms derived under our framework are able to employ both labeled and unlabeled examples and are able to handle complex problems where data form separate clusters of manifolds. Our framework offers simple views, explains relationships among existing frameworks and provides further extensions which can improve existing algorithms. Furthermore, a new semi-supervised kernelization framework called ``KPCA trick'' is proposed to handle non-linear problems.",
        "published": "2008-04-06T18:14:34Z",
        "link": "http://arxiv.org/abs/0804.0924v2",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Geometric Data Analysis, From Correspondence Analysis to Structured Data   Analysis (book review)",
        "authors": [
            "Fionn Murtagh"
        ],
        "summary": "Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004, xi+475 pp.",
        "published": "2008-04-08T11:31:55Z",
        "link": "http://arxiv.org/abs/0804.1244v1",
        "categories": [
            "cs.AI",
            "I.5; G.3; H.3; I.7; J.4"
        ]
    },
    {
        "title": "A $O(\\log m)$, deterministic, polynomial-time computable approximation   of Lewis Carroll's scoring rule",
        "authors": [
            "Jason Covey",
            "Christopher Homan"
        ],
        "summary": "We provide deterministic, polynomial-time computable voting rules that approximate Dodgson's and (the ``minimization version'' of) Young's scoring rules to within a logarithmic factor. Our approximation of Dodgson's rule is tight up to a constant factor, as Dodgson's rule is $\\NP$-hard to approximate to within some logarithmic factor. The ``maximization version'' of Young's rule is known to be $\\NP$-hard to approximate by any constant factor. Both approximations are simple, and natural as rules in their own right: Given a candidate we wish to score, we can regard either its Dodgson or Young score as the edit distance between a given set of voter preferences and one in which the candidate to be scored is the Condorcet winner. (The difference between the two scoring rules is the type of edits allowed.) We regard the marginal cost of a sequence of edits to be the number of edits divided by the number of reductions (in the candidate's deficit against any of its opponents in the pairwise race against that opponent) that the edits yield. Over a series of rounds, our scoring rules greedily choose a sequence of edits that modify exactly one voter's preferences and whose marginal cost is no greater than any other such single-vote-modifying sequence.",
        "published": "2008-04-09T07:12:29Z",
        "link": "http://arxiv.org/abs/0804.1421v1",
        "categories": [
            "cs.GT",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "On Kernelization of Supervised Mahalanobis Distance Learners",
        "authors": [
            "Ratthachat Chatpatanasiri",
            "Teesid Korsrilabutr",
            "Pasakorn Tangchanachaianan",
            "Boonserm Kijsirikul"
        ],
        "summary": "This paper focuses on the problem of kernelizing an existing supervised Mahalanobis distance learner. The following features are included in the paper. Firstly, three popular learners, namely, \"neighborhood component analysis\", \"large margin nearest neighbors\" and \"discriminant neighborhood embedding\", which do not have kernel versions are kernelized in order to improve their classification performances. Secondly, an alternative kernelization framework called \"KPCA trick\" is presented. Implementing a learner in the new framework gains several advantages over the standard framework, e.g. no mathematical formulas and no reprogramming are required for a kernel implementation, the framework avoids troublesome problems such as singularity, etc. Thirdly, while the truths of representer theorems are just assumptions in previous papers related to ours, here, representer theorems are formally proven. The proofs validate both the kernel trick and the KPCA trick in the context of Mahalanobis distance learning. Fourthly, unlike previous works which always apply brute force methods to select a kernel, we investigate two approaches which can be efficiently adopted to construct an appropriate kernel for a given dataset. Finally, numerical results on various real-world datasets are presented.",
        "published": "2008-04-09T09:40:51Z",
        "link": "http://arxiv.org/abs/0804.1441v3",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "The Choquet integral for the aggregation of interval scales in   multicriteria decision making",
        "authors": [
            "Christophe Labreuche",
            "Michel Grabisch"
        ],
        "summary": "This paper addresses the question of which models fit with information concerning the preferences of the decision maker over each attribute, and his preferences about aggregation of criteria (interacting criteria). We show that the conditions induced by these information plus some intuitive conditions lead to a unique possible aggregation operator: the Choquet integral.",
        "published": "2008-04-10T17:01:11Z",
        "link": "http://arxiv.org/abs/0804.1762v1",
        "categories": [
            "cs.DM",
            "cs.AI"
        ]
    },
    {
        "title": "Towards Physarum robots: computing and manipulating on water surface",
        "authors": [
            "Andrew Adamatzky"
        ],
        "summary": "Plasmodium of Physarym polycephalum is an ideal biological substrate for implementing concurrent and parallel computation, including combinatorial geometry and optimization on graphs. We report results of scoping experiments on Physarum computing in conditions of minimal friction, on the water surface. We show that plasmodium of Physarum is capable for computing a basic spanning trees and manipulating of light-weight objects. We speculate that our results pave the pathways towards design and implementation of amorphous biological robots.",
        "published": "2008-04-13T00:42:28Z",
        "link": "http://arxiv.org/abs/0804.2036v1",
        "categories": [
            "cs.RO",
            "cs.AI"
        ]
    },
    {
        "title": "From Qualitative to Quantitative Proofs of Security Properties Using   First-Order Conditional Logic",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "A first-order conditional logic is considered, with semantics given by a variant of epsilon-semantics, where p -> q means that Pr(q | p) approaches 1 super-polynomially --faster than any inverse polynomial. This type of convergence is needed for reasoning about security protocols. A complete axiomatization is provided for this semantics, and it is shown how a qualitative proof of the correctness of a security protocol can be automatically converted to a quantitative proof appropriate for reasoning about concrete security.",
        "published": "2008-04-14T12:06:04Z",
        "link": "http://arxiv.org/abs/0804.2155v1",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.LO",
            "D.4.6; I.2.3; I.2.4; F.4.1; K.6.5"
        ]
    },
    {
        "title": "Causal models have no complete axiomatic characterization",
        "authors": [
            "Sanjiang Li"
        ],
        "summary": "Markov networks and Bayesian networks are effective graphic representations of the dependencies embedded in probabilistic models. It is well known that independencies captured by Markov networks (called graph-isomorphs) have a finite axiomatic characterization. This paper, however, shows that independencies captured by Bayesian networks (called causal models) have no axiomatization by using even countably many Horn or disjunctive clauses. This is because a sub-independency model of a causal model may be not causal, while graph-isomorphs are closed under sub-models.",
        "published": "2008-04-15T14:28:34Z",
        "link": "http://arxiv.org/abs/0804.2401v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "An Analysis of Key Factors for the Success of the Communal Management of   Knowledge",
        "authors": [
            "Isabelle Bourdon",
            "Chris Kimble"
        ],
        "summary": "This paper explores the links between Knowledge Management and new community-based models of the organization from both a theoretical and an empirical perspective. From a theoretical standpoint, we look at Communities of Practice (CoPs) and Knowledge Management (KM) and explore the links between the two as they relate to the use of information systems to manage knowledge. We begin by reviewing technologically supported approaches to KM and introduce the idea of \"Systemes d'Aide a la Gestion des Connaissances\" SAGC (Systems to aid the Management of Knowledge). Following this we examine the contribution that communal structures such as CoPs can make to intraorganizational KM and highlight some of 'success factors' for this approach to KM that are found in the literature. From an empirical standpoint, we present the results of a survey involving the Chief Knowledge Officers (CKOs) of twelve large French businesses; the objective of this study was to identify the factors that might influence the success of such approaches. The survey was analysed using thematic content analysis and the results are presented here with some short illustrative quotes from the CKOs. Finally, the paper concludes with some brief reflections on what can be learnt from looking at this problem from these two perspectives.",
        "published": "2008-04-17T16:00:47Z",
        "link": "http://arxiv.org/abs/0804.2844v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "K.6; H.2"
        ]
    },
    {
        "title": "On the performance of approximate equilibria in congestion games",
        "authors": [
            "George Christodoulou",
            "Elias Koutsoupias",
            "Paul Spirakis"
        ],
        "summary": "We study the performance of approximate Nash equilibria for linear congestion games. We consider how much the price of anarchy worsens and how much the price of stability improves as a function of the approximation factor $\\epsilon$. We give (almost) tight upper and lower bounds for both the price of anarchy and the price of stability for atomic and non-atomic congestion games. Our results not only encompass and generalize the existing results of exact equilibria to $\\epsilon$-Nash equilibria, but they also provide a unified approach which reveals the common threads of the atomic and non-atomic price of anarchy results. By expanding the spectrum, we also cast the existing results in a new light. For example, the Pigou network, which gives tight results for exact Nash equilibria of selfish routing, remains tight for the price of stability of $\\epsilon$-Nash equilibria but not for the price of anarchy.",
        "published": "2008-04-21T12:45:08Z",
        "link": "http://arxiv.org/abs/0804.3160v2",
        "categories": [
            "cs.GT",
            "cs.AI",
            "cs.NI"
        ]
    },
    {
        "title": "A New Approach to Automated Epileptic Diagnosis Using EEG and   Probabilistic Neural Network",
        "authors": [
            "Forrest Sheng Bao",
            "Donald Yu-Chun Lie",
            "Yuanlin Zhang"
        ],
        "summary": "Epilepsy is one of the most common neurological disorders that greatly impair patient' daily lives. Traditional epileptic diagnosis relies on tedious visual screening by neurologists from lengthy EEG recording that requires the presence of seizure (ictal) activities. Nowadays, there are many systems helping the neurologists to quickly find interesting segments of the lengthy signal by automatic seizure detection. However, we notice that it is very difficult, if not impossible, to obtain long-term EEG data with seizure activities for epilepsy patients in areas lack of medical resources and trained neurologists. Therefore, we propose to study automated epileptic diagnosis using interictal EEG data that is much easier to collect than ictal data. The authors are not aware of any report on automated EEG diagnostic system that can accurately distinguish patients' interictal EEG from the EEG of normal people. The research presented in this paper, therefore, aims to develop an automated diagnostic system that can use interictal EEG data to diagnose whether the person is epileptic. Such a system should also detect seizure activities for further investigation by doctors and potential patient monitoring. To develop such a system, we extract four classes of features from the EEG data and build a Probabilistic Neural Network (PNN) fed with these features. Leave-one-out cross-validation (LOO-CV) on a widely used epileptic-normal data set reflects an impressive 99.5% accuracy of our system on distinguishing normal people's EEG from patient's interictal EEG. We also find our system can be used in patient monitoring (seizure detection) and seizure focus localization, with 96.7% and 77.5% accuracy respectively on the data set.",
        "published": "2008-04-21T17:07:59Z",
        "link": "http://arxiv.org/abs/0804.3361v2",
        "categories": [
            "cs.AI",
            "cs.CV",
            "I.5.4; I.2.1"
        ]
    },
    {
        "title": "Grainy Numbers",
        "authors": [
            "Gilles Champenois"
        ],
        "summary": "Grainy numbers are defined as tuples of bits. They form a lattice where the meet and the join operations are an addition and a multiplication. They may be substituted for the real numbers in the definition of fuzzy sets. The aim is to propose an alternative negation for the complement that we'll call supplement.",
        "published": "2008-04-25T09:36:17Z",
        "link": "http://arxiv.org/abs/0804.4073v6",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "SimDialog: A visual game dialog editor",
        "authors": [
            "C. Owen",
            "F. Biocca",
            "C. Bohil",
            "J. Conley"
        ],
        "summary": "SimDialog is a visual editor for dialog in computer games. This paper presents the design of SimDialog, illustrating how script writers and non-programmers can easily create dialog for video games with complex branching structures and dynamic response characteristics. The system creates dialog as a directed graph. This allows for play using the dialog with a state-based cause and effect system that controls selection of non-player character responses and can provide a basic scoring mechanism for games.",
        "published": "2008-04-30T18:44:52Z",
        "link": "http://arxiv.org/abs/0804.4885v1",
        "categories": [
            "cs.HC",
            "cs.AI"
        ]
    },
    {
        "title": "A Pseudo-Boolean Solution to the Maximum Quartet Consistency Problem",
        "authors": [
            "Antonio Morgado",
            "Joao Marques-Silva"
        ],
        "summary": "Determining the evolutionary history of a given biological data is an important task in biological sciences. Given a set of quartet topologies over a set of taxa, the Maximum Quartet Consistency (MQC) problem consists of computing a global phylogeny that satisfies the maximum number of quartets. A number of solutions have been proposed for the MQC problem, including Dynamic Programming, Constraint Programming, and more recently Answer Set Programming (ASP). ASP is currently the most efficient approach for optimally solving the MQC problem. This paper proposes encoding the MQC problem with pseudo-Boolean (PB) constraints. The use of PB allows solving the MQC problem with efficient PB solvers, and also allows considering different modeling approaches for the MQC problem. Initial results are promising, and suggest that PB can be an effective alternative for solving the MQC problem.",
        "published": "2008-05-02T10:06:27Z",
        "link": "http://arxiv.org/abs/0805.0202v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Phase transition in SONFIS&SORST",
        "authors": [
            "Hamed Owladeghaffari"
        ],
        "summary": "In this study, we introduce general frame of MAny Connected Intelligent Particles Systems (MACIPS). Connections and interconnections between particles get a complex behavior of such merely simple system (system in system).Contribution of natural computing, under information granulation theory, are the main topics of this spacious skeleton. Upon this clue, we organize two algorithms involved a few prominent intelligent computing and approximate reasoning methods: self organizing feature map (SOM), Neuro- Fuzzy Inference System and Rough Set Theory (RST). Over this, we show how our algorithms can be taken as a linkage of government-society interaction, where government catches various fashions of behavior: solid (absolute) or flexible. So, transition of such society, by changing of connectivity parameters (noise) from order to disorder is inferred. Add to this, one may find an indirect mapping among financial systems and eventual market fluctuations with MACIPS. Keywords: phase transition, SONFIS, SORST, many connected intelligent particles system, society-government interaction",
        "published": "2008-05-05T04:19:50Z",
        "link": "http://arxiv.org/abs/0805.0459v1",
        "categories": [
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Order to Disorder Transitions in Hybrid Intelligent Systems: a Hatch to   the Interactions of Nations -Governments",
        "authors": [
            "Hamed Owladeghaffari"
        ],
        "summary": "In this study, under general frame of MAny Connected Intelligent Particles Systems (MACIPS), we reproduce two new simple subsets of such intelligent complex network, namely hybrid intelligent systems, involved a few prominent intelligent computing and approximate reasoning methods: self organizing feature map (SOM), Neuro-Fuzzy Inference System and Rough Set Theory (RST). Over this, we show how our algorithms can be construed as a linkage of government-society interaction, where government catches various fashions of behavior: solid (absolute) or flexible. So, transition of such society, by changing of connectivity parameters (noise) from order to disorder is inferred. Add to this, one may find an indirect mapping among financial systems and eventual market fluctuations with MACIPS.",
        "published": "2008-05-06T04:22:36Z",
        "link": "http://arxiv.org/abs/0805.0642v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "math.IT",
            "F.1.1"
        ]
    },
    {
        "title": "AGNOSCO - Identification of Infected Nodes with artificial Ant Colonies",
        "authors": [
            "Michael Hilker",
            "Christoph Schommer"
        ],
        "summary": "If a computer node is infected by a virus, worm or a backdoor, then this is a security risk for the complete network structure where the node is associated. Existing Network Intrusion Detection Systems (NIDS) provide a certain amount of support for the identification of such infected nodes but suffer from the need of plenty of communication and computational power. In this article, we present a novel approach called AGNOSCO to support the identification of infected nodes through the usage of artificial ant colonies. It is shown that AGNOSCO overcomes the communication and computational power problem while identifying infected nodes properly.",
        "published": "2008-05-06T19:47:11Z",
        "link": "http://arxiv.org/abs/0805.0785v2",
        "categories": [
            "cs.AI",
            "cs.MA",
            "C.2.0"
        ]
    },
    {
        "title": "Adaptive Affinity Propagation Clustering",
        "authors": [
            "Kaijun Wang",
            "Junying Zhang",
            "Dan Li",
            "Xinna Zhang",
            "Tao Guo"
        ],
        "summary": "Affinity propagation clustering (AP) has two limitations: it is hard to know what value of parameter 'preference' can yield an optimal clustering solution, and oscillations cannot be eliminated automatically if occur. The adaptive AP method is proposed to overcome these limitations, including adaptive scanning of preferences to search space of the number of clusters for finding the optimal clustering solution, adaptive adjustment of damping factors to eliminate oscillations, and adaptive escaping from oscillations when the damping adjustment technique fails. Experimental results on simulated and real data sets show that the adaptive AP is effective and can outperform AP in quality of clustering results.",
        "published": "2008-05-08T04:20:20Z",
        "link": "http://arxiv.org/abs/0805.1096v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Contact state analysis using NFIS and SOM",
        "authors": [
            "H. Owladeghaffari"
        ],
        "summary": "This paper reports application of neuro- fuzzy inference system (NFIS) and self organizing feature map neural networks (SOM) on detection of contact state in a block system. In this manner, on a simple system, the evolution of contact states, by parallelization of DDA, has been investigated. So, a comparison between NFIS and SOM results has been presented. The results show applicability of the proposed methods, by different accuracy, on detection of contact's distribution.",
        "published": "2008-05-08T12:10:21Z",
        "link": "http://arxiv.org/abs/0805.1153v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Assessment of effective parameters on dilution using approximate   reasoning methods in longwall mining method, Iran coal mines",
        "authors": [
            "H. Owladeghaffari",
            "K. Shahriar",
            "G. H. R. Saeedi"
        ],
        "summary": "Approximately more than 90% of all coal production in Iranian underground mines is derived directly longwall mining method. Out of seam dilution is one of the essential problems in these mines. Therefore the dilution can impose the additional cost of mining and milling. As a result, recognition of the effective parameters on the dilution has a remarkable role in industry. In this way, this paper has analyzed the influence of 13 parameters (attributed variables) versus the decision attribute (dilution value), so that using two approximate reasoning methods, namely Rough Set Theory (RST) and Self Organizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our collected data sets has been extracted. The other benefit of later methods is to predict new unknown cases. So, the reduced sets (reducts) by RST have been obtained. Therefore the emerged results by utilizing mentioned methods shows that the high sensitive variables are thickness of layer, length of stope, rate of advance, number of miners, type of advancing.",
        "published": "2008-05-09T07:08:57Z",
        "link": "http://arxiv.org/abs/0805.1288v1",
        "categories": [
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "A Fast Algorithm and Datalog Inexpressibility for Temporal Reasoning",
        "authors": [
            "Manuel Bodirsky",
            "Jan Kara"
        ],
        "summary": "We introduce a new tractable temporal constraint language, which strictly contains the Ord-Horn language of Buerkert and Nebel and the class of AND/OR precedence constraints. The algorithm we present for this language decides whether a given set of constraints is consistent in time that is quadratic in the input size. We also prove that (unlike Ord-Horn) this language cannot be solved by Datalog or by establishing local consistency.",
        "published": "2008-05-10T13:49:45Z",
        "link": "http://arxiv.org/abs/0805.1473v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4"
        ]
    },
    {
        "title": "Swarm-Based Spatial Sorting",
        "authors": [
            "Martyn Amos",
            "Oliver Don"
        ],
        "summary": "Purpose: To present an algorithm for spatially sorting objects into an annular structure. Design/Methodology/Approach: A swarm-based model that requires only stochastic agent behaviour coupled with a pheromone-inspired \"attraction-repulsion\" mechanism. Findings: The algorithm consistently generates high-quality annular structures, and is particularly powerful in situations where the initial configuration of objects is similar to those observed in nature. Research limitations/implications: Experimental evidence supports previous theoretical arguments about the nature and mechanism of spatial sorting by insects. Practical implications: The algorithm may find applications in distributed robotics. Originality/value: The model offers a powerful minimal algorithmic framework, and also sheds further light on the nature of attraction-repulsion algorithms and underlying natural processes.",
        "published": "2008-05-12T19:46:29Z",
        "link": "http://arxiv.org/abs/0805.1727v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Distributed Self Management for Distributed Security Systems",
        "authors": [
            "Michael Hilker"
        ],
        "summary": "Distributed system as e.g. artificial immune systems, complex adaptive systems, or multi-agent systems are widely used in Computer Science, e.g. for network security, optimisations, or simulations. In these systems, small entities move through the network and perform certain tasks. At some time, the entities move to another place and require therefore information where to move is most profitable. Common used systems do not provide any information or use a centralised approach where a center delegates the entities. This article discusses whether small information about the neighbours enhances the performance of the overall system or not. Therefore, two information-protocols are introduced and analysed. In addition, the protocols are implemented and tested using the artificial immune system SANA that protects a network against intrusions.",
        "published": "2008-05-13T06:32:12Z",
        "link": "http://arxiv.org/abs/0805.1785v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "Next Challenges in Bringing Artificial Immune Systems to Production in   Network Security",
        "authors": [
            "Michael Hilker"
        ],
        "summary": "The human immune system protects the human body against various pathogens like e.g. biological viruses and bacteria. Artificial immune systems reuse the architecture, organization, and workflows of the human immune system for various problems in computer science. In the network security, the artificial immune system is used to secure a network and its nodes against intrusions like viruses, worms, and trojans. However, these approaches are far away from production where they are academic proof-of-concept implementations or use only a small part to protect against a certain intrusion. This article discusses the required steps to bring artificial immune systems into production in the network security domain. It furthermore figures out the challenges and provides the description and results of the prototype of an artificial immune system, which is SANA called.",
        "published": "2008-05-13T06:40:39Z",
        "link": "http://arxiv.org/abs/0805.1786v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "Rollout Sampling Approximate Policy Iteration",
        "authors": [
            "Christos Dimitrakakis",
            "Michail G. Lagoudakis"
        ],
        "summary": "Several researchers have recently investigated the connection between reinforcement learning and classification. We are motivated by proposals of approximate policy iteration schemes without value functions which focus on policy representation using classifiers and address policy learning as a supervised learning problem. This paper proposes variants of an improved policy iteration scheme which addresses the core sampling problem in evaluating a policy through simulation as a multi-armed bandit machine. The resulting algorithm offers comparable performance to the previous algorithm achieved, however, with significantly less computational effort. An order of magnitude improvement is demonstrated experimentally in two standard reinforcement learning domains: inverted pendulum and mountain-car.",
        "published": "2008-05-14T11:19:19Z",
        "link": "http://arxiv.org/abs/0805.2027v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Toward Fuzzy block theory",
        "authors": [
            "H. Owladeghaffari"
        ],
        "summary": "This study, fundamentals of fuzzy block theory, and its application in assessment of stability in underground openings, has surveyed. Using fuzzy topics and inserting them in to key block theory, in two ways, fundamentals of fuzzy block theory has been presented. In indirect combining, by coupling of adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could extract possible damage parts around a tunnel. In direct solution, some principles of block theory, by means of different fuzzy facets theory, were rewritten.",
        "published": "2008-05-15T13:43:26Z",
        "link": "http://arxiv.org/abs/0805.2308v1",
        "categories": [
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "A Kernel Method for the Two-Sample Problem",
        "authors": [
            "Arthur Gretton",
            "Karsten Borgwardt",
            "Malte J. Rasch",
            "Bernhard Scholkopf",
            "Alexander J. Smola"
        ],
        "summary": "We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.",
        "published": "2008-05-15T17:46:53Z",
        "link": "http://arxiv.org/abs/0805.2368v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "G.3; I.2.6"
        ]
    },
    {
        "title": "Analysis of hydrocyclone performance based on information granulation   theory",
        "authors": [
            "Hamed Owladeghaffari",
            "Majid Ejtemaei",
            "Mehdi Irannajad"
        ],
        "summary": "This paper describes application of information granulation theory, on the analysis of hydrocyclone perforamance. In this manner, using a combining of Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and fuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules and sub fuzzy granules, within non fuzzy information (initial granulation), is rendered in an open-close iteration. Using two criteria, \"simplicity of rules \"and \"adaptive threoshold error level\", stability of algorithm is guaranteed. Validation of the proposed method, on the data set of the hydrocyclone is rendered.",
        "published": "2008-05-16T05:11:24Z",
        "link": "http://arxiv.org/abs/0805.2440v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Learning Low-Density Separators",
        "authors": [
            "Shai Ben-David",
            "Tyler Lu",
            "David Pal",
            "Miroslava Sotakova"
        ],
        "summary": "We define a novel, basic, unsupervised learning problem - learning the lowest density homogeneous hyperplane separator of an unknown probability distribution. This task is relevant to several problems in machine learning, such as semi-supervised learning and clustering stability. We investigate the question of existence of a universally consistent algorithm for this problem. We propose two natural learning paradigms and prove that, on input unlabeled random samples generated by any member of a rich family of distributions, they are guaranteed to converge to the optimal separator for that distribution. We complement this result by showing that no learning algorithm for our task can achieve uniform learning rates (that are independent of the data generating distribution).",
        "published": "2008-05-19T17:55:08Z",
        "link": "http://arxiv.org/abs/0805.2891v2",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Cognitive Architecture for Direction of Attention Founded on Subliminal   Memory Searches, Pseudorandom and Nonstop",
        "authors": [
            "J. R. Burger"
        ],
        "summary": "By way of explaining how a brain works logically, human associative memory is modeled with logical and memory neurons, corresponding to standard digital circuits. The resulting cognitive architecture incorporates basic psychological elements such as short term and long term memory. Novel to the architecture are memory searches using cues chosen pseudorandomly from short term memory. Recalls alternated with sensory images, many tens per second, are analyzed subliminally as an ongoing process, to determine a direction of attention in short term memory.",
        "published": "2008-05-20T17:37:31Z",
        "link": "http://arxiv.org/abs/0805.3126v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.0; C.1.3"
        ]
    },
    {
        "title": "Compressing Binary Decision Diagrams",
        "authors": [
            "Esben Rune Hansen",
            "S. Srinivasa Rao",
            "Peter Tiedemann"
        ],
        "summary": "The paper introduces a new technique for compressing Binary Decision Diagrams in those cases where random access is not required. Using this technique, compression and decompression can be done in linear time in the size of the BDD and compression will in many cases reduce the size of the BDD to 1-2 bits per node. Empirical results for our compression technique are presented, including comparisons with previously introduced techniques, showing that the new technique dominate on all tested instances.",
        "published": "2008-05-21T12:44:23Z",
        "link": "http://arxiv.org/abs/0805.3267v1",
        "categories": [
            "cs.AI",
            "cs.DC"
        ]
    },
    {
        "title": "Logic programming with social features",
        "authors": [
            "Francesco Buccafurri",
            "Gianluca Caminiti"
        ],
        "summary": "In everyday life it happens that a person has to reason about what other people think and how they behave, in order to achieve his goals. In other words, an individual may be required to adapt his behaviour by reasoning about the others' mental state. In this paper we focus on a knowledge representation language derived from logic programming which both supports the representation of mental states of individual communities and provides each with the capability of reasoning about others' mental states and acting accordingly. The proposed semantics is shown to be translatable into stable model semantics of logic programs with aggregates.",
        "published": "2008-05-22T18:17:58Z",
        "link": "http://arxiv.org/abs/0805.3518v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Towards applied theories based on computability logic",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Computability logic (CL) (see http://www.cis.upenn.edu/~giorgi/cl.html) is a recently launched program for redeveloping logic as a formal theory of computability, as opposed to the formal theory of truth that logic has more traditionally been. Formulas in it represent computational problems, \"truth\" means existence of an algorithmic solution, and proofs encode such solutions. Within the line of research devoted to finding axiomatizations for ever more expressive fragments of CL, the present paper introduces a new deductive system CL12 and proves its soundness and completeness with respect to the semantics of CL. Conservatively extending classical predicate calculus and offering considerable additional expressive and deductive power, CL12 presents a reasonable, computationally meaningful, constructive alternative to classical logic as a basis for applied theories. To obtain a model example of such theories, this paper rebuilds the traditional, classical-logic-based Peano arithmetic into a computability-logic-based counterpart. Among the purposes of the present contribution is to provide a starting point for what, as the author wishes to hope, might become a new line of research with a potential of interesting findings -- an exploration of the presumably quite unusual metatheory of CL-based arithmetic and other CL-based applied systems.",
        "published": "2008-05-22T18:18:02Z",
        "link": "http://arxiv.org/abs/0805.3521v4",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "math.NT",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Constructing Folksonomies from User-specified Relations on Flickr",
        "authors": [
            "Anon Plangprasopchok",
            "Kristina Lerman"
        ],
        "summary": "Many social Web sites allow users to publish content and annotate with descriptive metadata. In addition to flat tags, some social Web sites have recently began to allow users to organize their content and metadata hierarchically. The social photosharing site Flickr, for example, allows users to group related photos in sets, and related sets in collections. The social bookmarking site Del.icio.us similarly lets users group related tags into bundles. Although the sites themselves don't impose any constraints on how these hierarchies are used, individuals generally use them to capture relationships between concepts, most commonly the broader/narrower relations. Collective annotation of content with hierarchical relations may lead to an emergent classification system, called a folksonomy. While some researchers have explored using tags as evidence for learning folksonomies, we believe that hierarchical relations described above offer a high-quality source of evidence for this task.   We propose a simple approach to aggregate shallow hierarchies created by many distinct Flickr users into a common folksonomy. Our approach uses statistics to determine if a particular relation should be retained or discarded. The relations are then woven together into larger hierarchies. Although we have not carried out a detailed quantitative evaluation of the approach, it looks very promising since it generates very reasonable, non-trivial hierarchies.",
        "published": "2008-05-24T07:02:24Z",
        "link": "http://arxiv.org/abs/0805.3747v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "The Structure of Narrative: the Case of Film Scripts",
        "authors": [
            "Fionn Murtagh",
            "Adam Ganz",
            "Stewart McKie"
        ],
        "summary": "We analyze the style and structure of story narrative using the case of film scripts. The practical importance of this is noted, especially the need to have support tools for television movie writing. We use the Casablanca film script, and scripts from six episodes of CSI (Crime Scene Investigation). For analysis of style and structure, we quantify various central perspectives discussed in McKee's book, \"Story: Substance, Structure, Style, and the Principles of Screenwriting\". Film scripts offer a useful point of departure for exploration of the analysis of more general narratives. Our methodology, using Correspondence Analysis, and hierarchical clustering, is innovative in a range of areas that we discuss. In particular this work is groundbreaking in taking the qualitative analysis of McKee and grounding this analysis in a quantitative and algorithmic framework.",
        "published": "2008-05-24T23:35:15Z",
        "link": "http://arxiv.org/abs/0805.3799v1",
        "categories": [
            "cs.AI",
            "I.5.4; I.2.7; H.3.1"
        ]
    },
    {
        "title": "An Evolutionary-Based Approach to Learning Multiple Decision Models from   Underrepresented Data",
        "authors": [
            "Vitaly Schetinin",
            "Dayou Li",
            "Carsten Maple"
        ],
        "summary": "The use of multiple Decision Models (DMs) enables to enhance the accuracy in decisions and at the same time allows users to evaluate the confidence in decision making. In this paper we explore the ability of multiple DMs to learn from a small amount of verified data. This becomes important when data samples are difficult to collect and verify. We propose an evolutionary-based approach to solving this problem. The proposed technique is examined on a few clinical problems presented by a small amount of data.",
        "published": "2008-05-24T23:37:51Z",
        "link": "http://arxiv.org/abs/0805.3800v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Feature Selection for Bayesian Evaluation of Trauma Death Risk",
        "authors": [
            "L. Jakaite",
            "V. Schetinin"
        ],
        "summary": "In the last year more than 70,000 people have been brought to the UK hospitals with serious injuries. Each time a clinician has to urgently take a patient through a screening procedure to make a reliable decision on the trauma treatment. Typically, such procedure comprises around 20 tests; however the condition of a trauma patient remains very difficult to be tested properly. What happens if these tests are ambiguously interpreted, and information about the severity of the injury will come misleading? The mistake in a decision can be fatal: using a mild treatment can put a patient at risk of dying from posttraumatic shock, while using an overtreatment can also cause death. How can we reduce the risk of the death caused by unreliable decisions? It has been shown that probabilistic reasoning, based on the Bayesian methodology of averaging over decision models, allows clinicians to evaluate the uncertainty in decision making. Based on this methodology, in this paper we aim at selecting the most important screening tests, keeping a high performance. We assume that the probabilistic reasoning within the Bayesian methodology allows us to discover new relationships between the screening tests and uncertainty in decisions. In practice, selection of the most informative tests can also reduce the cost of a screening procedure in trauma care centers. In our experiments we use the UK Trauma data to compare the efficiency of the proposed technique in terms of the performance. We also compare the uncertainty in decisions in terms of entropy.",
        "published": "2008-05-25T00:06:29Z",
        "link": "http://arxiv.org/abs/0805.3802v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Fusion for Evaluation of Image Classification in Uncertain Environments",
        "authors": [
            "Arnaud Martin"
        ],
        "summary": "We present in this article a new evaluation method for classification and segmentation of textured images in uncertain environments. In uncertain environments, real classes and boundaries are known with only a partial certainty given by the experts. Most of the time, in many presented papers, only classification or only segmentation are considered and evaluated. Here, we propose to take into account both the classification and segmentation results according to the certainty given by the experts. We present the results of this method on a fusion of classifiers of sonar images for a seabed characterization.",
        "published": "2008-05-26T11:50:25Z",
        "link": "http://arxiv.org/abs/0805.3935v1",
        "categories": [
            "cs.AI",
            "I.4; I.5"
        ]
    },
    {
        "title": "Decision Support with Belief Functions Theory for Seabed   Characterization",
        "authors": [
            "Arnaud Martin",
            "Isabelle Quidu"
        ],
        "summary": "The seabed characterization from sonar images is a very hard task because of the produced data and the unknown environment, even for an human expert. In this work we propose an original approach in order to combine binary classifiers arising from different kinds of strategies such as one-versus-one or one-versus-rest, usually used in the SVM-classification. The decision functions coming from these binary classifiers are interpreted in terms of belief functions in order to combine these functions with one of the numerous operators of the belief functions theory. Moreover, this interpretation of the decision function allows us to propose a process of decisions by taking into account the rejected observations too far removed from the learning data, and the imprecise decisions given in unions of classes. This new approach is illustrated and evaluated with a SVM in order to classify the different kinds of sediment on image sonar.",
        "published": "2008-05-26T11:55:55Z",
        "link": "http://arxiv.org/abs/0805.3939v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "math.IT",
            "I.4; I.5"
        ]
    },
    {
        "title": "Intuitive visualization of the intelligence for the run-down of   terrorist wire-pullers",
        "authors": [
            "Yoshiharu Maeno",
            "Yukio Ohsawa"
        ],
        "summary": "The investigation of the terrorist attack is a time-critical task. The investigators have a limited time window to diagnose the organizational background of the terrorists, to run down and arrest the wire-pullers, and to take an action to prevent or eradicate the terrorist attack. The intuitive interface to visualize the intelligence data set stimulates the investigators' experience and knowledge, and aids them in decision-making for an immediately effective action. This paper presents a computational method to analyze the intelligence data set on the collective actions of the perpetrators of the attack, and to visualize it into the form of a social network diagram which predicts the positions where the wire-pullers conceals themselves.",
        "published": "2008-05-26T14:28:28Z",
        "link": "http://arxiv.org/abs/0805.3972v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Goal-oriented Dialog as a Collaborative Subordinated Activity involving   Collective Acceptance",
        "authors": [
            "Sylvie Saget",
            "Marc Guyomard"
        ],
        "summary": "Modeling dialog as a collaborative activity consists notably in specifying the content of the Conversational Common Ground and the kind of social mental state involved. In previous work (Saget, 2006), we claim that Collective Acceptance is the proper social attitude for modeling Conversational Common Ground in the particular case of goal-oriented dialog. In this paper, a formalization of Collective Acceptance is shown, besides elements in order to integrate this attitude in a rational model of dialog are provided; and finally, a model of referential acts as being part of a collaborative activity is presented. The particular case of reference has been chosen in order to exemplify our claims.",
        "published": "2008-05-27T12:16:12Z",
        "link": "http://arxiv.org/abs/0805.4101v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Modeling Loosely Annotated Images with Imagined Annotations",
        "authors": [
            "Hong Tang",
            "Nozha Boujemma",
            "Yunhao Chen"
        ],
        "summary": "In this paper, we present an approach to learning latent semantic analysis models from loosely annotated images for automatic image annotation and indexing. The given annotation in training images is loose due to: (1) ambiguous correspondences between visual features and annotated keywords; (2) incomplete lists of annotated keywords. The second reason motivates us to enrich the incomplete annotation in a simple way before learning topic models. In particular, some imagined keywords are poured into the incomplete annotation through measuring similarity between keywords. Then, both given and imagined annotations are used to learning probabilistic topic models for automatically annotating new images. We conduct experiments on a typical Corel dataset of images and loose annotations, and compare the proposed method with state-of-the-art discrete annotation methods (using a set of discrete blobs to represent an image). The proposed method improves word-driven probability Latent Semantic Analysis (PLSA-words) up to a comparable performance with the best discrete annotation method, while a merit of PLSA-words is still kept, i.e., a wider semantic range.",
        "published": "2008-05-29T10:35:29Z",
        "link": "http://arxiv.org/abs/0805.4508v1",
        "categories": [
            "cs.IR",
            "cs.AI",
            "H.3.3"
        ]
    },
    {
        "title": "Rock mechanics modeling based on soft granulation theory",
        "authors": [
            "H. Owladeghaffari"
        ],
        "summary": "This paper describes application of information granulation theory, on the design of rock engineering flowcharts. Firstly, an overall flowchart, based on information granulation theory has been highlighted. Information granulation theory, in crisp (non-fuzzy) or fuzzy format, can take into account engineering experiences (especially in fuzzy shape-incomplete information or superfluous), or engineering judgments, in each step of designing procedure, while the suitable instruments modeling are employed. In this manner and to extension of soft modeling instruments, using three combinations of Self Organizing Map (SOM), Neuro-Fuzzy Inference System (NFIS), and Rough Set Theory (RST) crisp and fuzzy granules, from monitored data sets are obtained. The main underlined core of our algorithms are balancing of crisp(rough or non-fuzzy) granules and sub fuzzy granules, within non fuzzy information (initial granulation) upon the open-close iterations. Using different criteria on balancing best granules (information pockets), are obtained. Validations of our proposed methods, on the data set of in-situ permeability in rock masses in Shivashan dam, Iran have been highlighted.",
        "published": "2008-05-29T14:39:57Z",
        "link": "http://arxiv.org/abs/0805.4560v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Checking the Quality of Clinical Guidelines using Automated Reasoning   Tools",
        "authors": [
            "Arjen Hommersom",
            "Peter J. F. Lucas",
            "Patrick van Bommel"
        ],
        "summary": "Requirements about the quality of clinical guidelines can be represented by schemata borrowed from the theory of abductive diagnosis, using temporal logic to model the time-oriented aspects expressed in a guideline. Previously, we have shown that these requirements can be verified using interactive theorem proving techniques. In this paper, we investigate how this approach can be mapped to the facilities of a resolution-based theorem prover, Otter, and a complementary program that searches for finite models of first-order statements, Mace. It is shown that the reasoning required for checking the quality of a guideline can be mapped to such fully automated theorem-proving facilities. The medical quality of an actual guideline concerning diabetes mellitus 2 is investigated in this way.",
        "published": "2008-06-02T11:02:40Z",
        "link": "http://arxiv.org/abs/0806.0250v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "An Ontology-based Knowledge Management System for Industry Clusters",
        "authors": [
            "Pradorn Sureephong",
            "Nopasit Chakpitak",
            "Yacine Ouzrout",
            "Abdelaziz Bouras"
        ],
        "summary": "Knowledge-based economy forces companies in the nation to group together as a cluster in order to maintain their competitiveness in the world market. The cluster development relies on two key success factors which are knowledge sharing and collaboration between the actors in the cluster. Thus, our study tries to propose knowledge management system to support knowledge management activities within the cluster. To achieve the objectives of this study, ontology takes a very important role in knowledge management process in various ways; such as building reusable and faster knowledge-bases, better way for representing the knowledge explicitly. However, creating and representing ontology create difficulties to organization due to the ambiguity and unstructured of source of knowledge. Therefore, the objectives of this paper are to propose the methodology to create and represent ontology for the organization development by using knowledge engineering approach. The handicraft cluster in Thailand is used as a case study to illustrate our proposed methodology.",
        "published": "2008-06-03T12:38:50Z",
        "link": "http://arxiv.org/abs/0806.0526v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Collaborative model of interaction and Unmanned Vehicle Systems'   interface",
        "authors": [
            "Sylvie Saget",
            "Francois Legras",
            "Gilles Coppin"
        ],
        "summary": "The interface for the next generation of Unmanned Vehicle Systems should be an interface with multi-modal displays and input controls. Then, the role of the interface will not be restricted to be a support of the interactions between the ground operator and vehicles. Interface must take part in the interaction management too. In this paper, we show that recent works in pragmatics and philosophy provide a suitable theoretical framework for the next generation of UV System's interface. We concentrate on two main aspects of the collaborative model of interaction based on acceptance: multi-strategy approach for communicative act generation and interpretation and communicative alignment.",
        "published": "2008-06-04T14:22:38Z",
        "link": "http://arxiv.org/abs/0806.0784v1",
        "categories": [
            "cs.AI",
            "cs.HC",
            "cs.MA"
        ]
    },
    {
        "title": "Belief Propagation and Beyond for Particle Tracking",
        "authors": [
            "Michael Chertkov",
            "Lukas Kroc",
            "Massimo Vergassola"
        ],
        "summary": "We describe a novel approach to statistical learning from particles tracked while moving in a random environment. The problem consists in inferring properties of the environment from recorded snapshots. We consider here the case of a fluid seeded with identical passive particles that diffuse and are advected by a flow. Our approach rests on efficient algorithms to estimate the weighted number of possible matchings among particles in two consecutive snapshots, the partition function of the underlying graphical model. The partition function is then maximized over the model parameters, namely diffusivity and velocity gradient. A Belief Propagation (BP) scheme is the backbone of our algorithm, providing accurate results for the flow parameters we want to learn. The BP estimate is additionally improved by incorporating Loop Series (LS) contributions. For the weighted matching problem, LS is compactly expressed as a Cauchy integral, accurately estimated by a saddle point approximation. Numerical experiments show that the quality of our improved BP algorithm is comparable to the one of a fully polynomial randomized approximation scheme, based on the Markov Chain Monte Carlo (MCMC) method, while the BP-based scheme is substantially faster than the MCMC scheme.",
        "published": "2008-06-06T16:18:13Z",
        "link": "http://arxiv.org/abs/0806.1199v1",
        "categories": [
            "cs.IT",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.LG",
            "math.IT",
            "physics.flu-dyn"
        ]
    },
    {
        "title": "The Role of Artificial Intelligence Technologies in Crisis Response",
        "authors": [
            "Khaled M. Khalil",
            "M. Abdel-Aziz",
            "Taymour T. Nazmy",
            "Abdel-Badeeh M. Salem"
        ],
        "summary": "Crisis response poses many of the most difficult information technology in crisis management. It requires information and communication-intensive efforts, utilized for reducing uncertainty, calculating and comparing costs and benefits, and managing resources in a fashion beyond those regularly available to handle routine problems. In this paper, we explore the benefits of artificial intelligence technologies in crisis response. This paper discusses the role of artificial intelligence technologies; namely, robotics, ontology and semantic web, and multi-agent systems in crisis response.",
        "published": "2008-06-07T12:46:56Z",
        "link": "http://arxiv.org/abs/0806.1280v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "The end of Sleeping Beauty's nightmare",
        "authors": [
            "Berry Groisman"
        ],
        "summary": "The way a rational agent changes her belief in certain propositions/hypotheses in the light of new evidence lies at the heart of Bayesian inference. The basic natural assumption, as summarized in van Fraassen's Reflection Principle ([1984]), would be that in the absence of new evidence the belief should not change. Yet, there are examples that are claimed to violate this assumption. The apparent paradox presented by such examples, if not settled, would demonstrate the inconsistency and/or incompleteness of the Bayesian approach and without eliminating this inconsistency, the approach cannot be regarded as scientific.   The Sleeping Beauty Problem is just such an example. The existing attempts to solve the problem fall into three categories. The first two share the view that new evidence is absent, but differ about the conclusion of whether Sleeping Beauty should change her belief or not, and why. The third category is characterized by the view that, after all, new evidence (although hidden from the initial view) is involved.   My solution is radically different and does not fall in either of these categories. I deflate the paradox by arguing that the two different degrees of belief presented in the Sleeping Beauty Problem are in fact beliefs in two different propositions, i.e. there is no need to explain the (un)change of belief.",
        "published": "2008-06-08T08:42:00Z",
        "link": "http://arxiv.org/abs/0806.1316v1",
        "categories": [
            "cs.AI",
            "math.PR"
        ]
    },
    {
        "title": "Temporized Equilibria",
        "authors": [
            "Riccardo Alberti"
        ],
        "summary": "This paper has been withdrawn by the author due to a crucial error in the submission action.",
        "published": "2008-06-08T17:39:56Z",
        "link": "http://arxiv.org/abs/0806.1343v4",
        "categories": [
            "cs.GT",
            "cs.AI"
        ]
    },
    {
        "title": "Data-Complexity of the Two-Variable Fragment with Counting Quantifiers",
        "authors": [
            "Ian Pratt-Hartmann"
        ],
        "summary": "The data-complexity of both satisfiability and finite satisfiability for the two-variable fragment with counting is NP-complete; the data-complexity of both query-answering and finite query-answering for the two-variable guarded fragment with counting is co-NP-complete.",
        "published": "2008-06-10T11:08:07Z",
        "link": "http://arxiv.org/abs/0806.1636v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Toward a combination rule to deal with partial conflict and specificity   in belief functions theory",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "summary": "We present and discuss a mixed conjunctive and disjunctive rule, a generalization of conflict repartition rules, and a combination of these two rules. In the belief functions theory one of the major problem is the conflict repartition enlightened by the famous Zadeh's example. To date, many combination rules have been proposed in order to solve a solution to this problem. Moreover, it can be important to consider the specificity of the responses of the experts. Since few year some unification rules are proposed. We have shown in our previous works the interest of the proportional conflict redistribution rule. We propose here a mixed combination rule following the proportional conflict redistribution rule modified by a discounting procedure. This rule generalizes many combination rules.",
        "published": "2008-06-10T12:03:43Z",
        "link": "http://arxiv.org/abs/0806.1640v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Evaluation for Uncertain Image Classification and Segmentation",
        "authors": [
            "Arnaud Martin",
            "Hicham Laanaya",
            "Andreas Arnold-Bos"
        ],
        "summary": "Each year, numerous segmentation and classification algorithms are invented or reused to solve problems where machine vision is needed. Generally, the efficiency of these algorithms is compared against the results given by one or many human experts. However, in many situations, the location of the real boundaries of the objects as well as their classes are not known with certainty by the human experts. Furthermore, only one aspect of the segmentation and classification problem is generally evaluated. In this paper we present a new evaluation method for classification and segmentation of image, where we take into account both the classification and segmentation results as well as the level of certainty given by the experts. As a concrete example of our method, we evaluate an automatic seabed characterization algorithm based on sonar images.",
        "published": "2008-06-11T07:02:45Z",
        "link": "http://arxiv.org/abs/0806.1796v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.4; I.5"
        ]
    },
    {
        "title": "A new generalization of the proportional conflict redistribution rule   stable in terms of decision",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "summary": "In this chapter, we present and discuss a new generalized proportional conflict redistribution rule. The Dezert-Smarandache extension of the Demster-Shafer theory has relaunched the studies on the combination rules especially for the management of the conflict. Many combination rules have been proposed in the last few years. We study here different combination rules and compare them in terms of decision on didactic example and on generated data. Indeed, in real applications, we need a reliable decision and it is the final results that matter. This chapter shows that a fine proportional conflict redistribution rule must be preferred for the combination in the belief function theory.",
        "published": "2008-06-11T07:05:48Z",
        "link": "http://arxiv.org/abs/0806.1797v1",
        "categories": [
            "cs.AI",
            "I.4; I.5"
        ]
    },
    {
        "title": "Human expert fusion for image classification",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "summary": "In image classification, merging the opinion of several human experts is very important for different tasks such as the evaluation or the training. Indeed, the ground truth is rarely known before the scene imaging. We propose here different models in order to fuse the informations given by two or more experts. The considered unit for the classification, a small tile of the image, can contain one or more kind of the considered classes given by the experts. A second problem that we have to take into account, is the amount of certainty of the expert has for each pixel of the tile. In order to solve these problems we define five models in the context of the Dempster-Shafer Theory and in the context of the Dezert-Smarandache Theory and we study the possible decisions with these models.",
        "published": "2008-06-11T07:09:15Z",
        "link": "http://arxiv.org/abs/0806.1798v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.4; I.5"
        ]
    },
    {
        "title": "Une nouvelle règle de combinaison répartissant le conflit -   Applications en imagerie Sonar et classification de cibles Radar",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "summary": "These last years, there were many studies on the problem of the conflict coming from information combination, especially in evidence theory. We can summarise the solutions for manage the conflict into three different approaches: first, we can try to suppress or reduce the conflict before the combination step, secondly, we can manage the conflict in order to give no influence of the conflict in the combination step, and then take into account the conflict in the decision step, thirdly, we can take into account the conflict in the combination step. The first approach is certainly the better, but not always feasible. It is difficult to say which approach is the best between the second and the third. However, the most important is the produced results in applications. We propose here a new combination rule that distributes the conflict proportionally on the element given this conflict. We compare these different combination rules on real data in Sonar imagery and Radar target classification.",
        "published": "2008-06-11T07:31:36Z",
        "link": "http://arxiv.org/abs/0806.1802v1",
        "categories": [
            "cs.AI",
            "I.4; I.5"
        ]
    },
    {
        "title": "Perfect Derived Propagators",
        "authors": [
            "Christian Schulte",
            "Guido Tack"
        ],
        "summary": "When implementing a propagator for a constraint, one must decide about variants: When implementing min, should one also implement max? Should one implement linear equations both with and without coefficients? Constraint variants are ubiquitous: implementing them requires considerable (if not prohibitive) effort and decreases maintainability, but will deliver better performance.   This paper shows how to use variable views, previously introduced for an implementation architecture, to derive perfect propagator variants. A model for views and derived propagators is introduced. Derived propagators are proved to be indeed perfect in that they inherit essential properties such as correctness and domain and bounds consistency. Techniques for systematically deriving propagators such as transformation, generalization, specialization, and channeling are developed for several variable domains. We evaluate the massive impact of derived propagators. Without derived propagators, Gecode would require 140000 rather than 40000 lines of code for propagators.",
        "published": "2008-06-11T08:03:35Z",
        "link": "http://arxiv.org/abs/0806.1806v1",
        "categories": [
            "cs.AI",
            "D.3.2; D.3.3"
        ]
    },
    {
        "title": "Fusion de classifieurs pour la classification d'images sonar",
        "authors": [
            "Arnaud Martin"
        ],
        "summary": "In this paper, we present some high level information fusion approaches for numeric and symbolic data. We study the interest of such method particularly for classifier fusion. A comparative study is made in a context of sea bed characterization from sonar images. The classi- fication of kind of sediment is a difficult problem because of the data complexity. We compare high level information fusion and give the obtained performance.",
        "published": "2008-06-12T06:42:07Z",
        "link": "http://arxiv.org/abs/0806.2006v2",
        "categories": [
            "cs.CV",
            "cs.AI"
        ]
    },
    {
        "title": "Experts Fusion and Multilayer Perceptron Based on Belief Learning for   Sonar Image Classification",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "summary": "The sonar images provide a rapid view of the seabed in order to characterize it. However, in such as uncertain environment, real seabed is unknown and the only information we can obtain, is the interpretation of different human experts, sometimes in conflict. In this paper, we propose to manage this conflict in order to provide a robust reality for the learning step of classification algorithms. The classification is conducted by a multilayer perceptron, taking into account the uncertainty of the reality in the learning stage. The results of this seabed characterization are presented on real sonar images.",
        "published": "2008-06-12T06:44:55Z",
        "link": "http://arxiv.org/abs/0806.2007v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.4; I.5"
        ]
    },
    {
        "title": "Generalized proportional conflict redistribution rule applied to Sonar   imagery and Radar targets classification",
        "authors": [
            "Arnaud Martin",
            "Christophe Osswald"
        ],
        "summary": "In this chapter, we present two applications in information fusion in order to evaluate the generalized proportional conflict redistribution rule presented in the chapter \\cite{Martin06a}. Most of the time the combination rules are evaluated only on simple examples. We study here different combination rules and compare them in terms of decision on real data. Indeed, in real applications, we need a reliable decision and it is the final results that matter. Two applications are presented here: a fusion of human experts opinions on the kind of underwater sediments depict on sonar image and a classifier fusion for radar targets recognition.",
        "published": "2008-06-12T06:47:26Z",
        "link": "http://arxiv.org/abs/0806.2008v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.4; I.5"
        ]
    },
    {
        "title": "Beyond Nash Equilibrium: Solution Concepts for the 21st Century",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "Nash equilibrium is the most commonly-used notion of equilibrium in game theory. However, it suffers from numerous problems. Some are well known in the game theory community; for example, the Nash equilibrium of repeated prisoner's dilemma is neither normatively nor descriptively reasonable. However, new problems arise when considering Nash equilibrium from a computer science perspective: for example, Nash equilibrium is not robust (it does not tolerate ``faulty'' or ``unexpected'' behavior), it does not deal with coalitions, it does not take computation cost into account, and it does not deal with cases where players are not aware of all aspects of the game. Solution concepts that try to address these shortcomings of Nash equilibrium are discussed.",
        "published": "2008-06-12T19:23:07Z",
        "link": "http://arxiv.org/abs/0806.2139v1",
        "categories": [
            "cs.GT",
            "cs.AI",
            "cs.CR",
            "cs.DC",
            "C.2.4; F.0; I.2.11; J.4"
        ]
    },
    {
        "title": "Defaults and Normality in Causal Structures",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "A serious defect with the Halpern-Pearl (HP) definition of causality is repaired by combining a theory of causality with a theory of defaults. In addition, it is shown that (despite a claim to the contrary) a cause according to the HP condition need not be a single conjunct. A definition of causality motivated by Wright's NESS test is shown to always hold for a single conjunct. Moreover, conditions that hold for all the examples considered by HP are given that guarantee that causality according to (this version) of the NESS test is equivalent to the HP definition.",
        "published": "2008-06-12T19:27:57Z",
        "link": "http://arxiv.org/abs/0806.2140v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "An Intelligent Multi-Agent Recommender System for Human Capacity   Building",
        "authors": [
            "Vukosi N. Marivate",
            "George Ssali",
            "Tshilidzi Marwala"
        ],
        "summary": "This paper presents a Multi-Agent approach to the problem of recommending training courses to engineering professionals. The recommendation system is built as a proof of concept and limited to the electrical and mechanical engineering disciplines. Through user modelling and data collection from a survey, collaborative filtering recommendation is implemented using intelligent agents. The agents work together in recommending meaningful training courses and updating the course information. The system uses a users profile and keywords from courses to rank courses. A ranking accuracy for courses of 90% is achieved while flexibility is achieved using an agent that retrieves information autonomously using data mining techniques from websites. This manner of recommendation is scalable and adaptable. Further improvements can be made using clustering and recording user feedback.",
        "published": "2008-06-13T09:56:36Z",
        "link": "http://arxiv.org/abs/0806.2216v1",
        "categories": [
            "cs.AI",
            "cs.HC"
        ]
    },
    {
        "title": "Development of Hybrid Intelligent Systems and their Applications from   Engineering Systems to Complex Systems",
        "authors": [
            "Hamed Owladeghaffari"
        ],
        "summary": "In this study, we introduce general frame of MAny Connected Intelligent Particles Systems (MACIPS). Connections and interconnections between particles get a complex behavior of such merely simple system (system in system).Contribution of natural computing, under information granulation theory, are the main topic of this spacious skeleton. Upon this clue, we organize different algorithms involved a few prominent intelligent computing and approximate reasoning methods such as self organizing feature map (SOM)[9], Neuro- Fuzzy Inference System[10], Rough Set Theory (RST)[11], collaborative clustering, Genetic Algorithm and Ant Colony System. Upon this, we have employed our algorithms on the several engineering systems, especially emerged systems in Civil and Mineral processing. In other process, we investigated how our algorithms can be taken as a linkage of government-society interaction, where government catches various fashions of behavior: solid (absolute) or flexible. So, transition of such society, by changing of connectivity parameters (noise) from order to disorder is inferred. Add to this, one may find an indirect mapping among finical systems and eventual market fluctuations with MACIPS. In the following sections, we will mention the main topics of the suggested proposal, briefly Details of the proposed algorithms can be found in the references.",
        "published": "2008-06-14T03:44:35Z",
        "link": "http://arxiv.org/abs/0806.2356v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Neural networks in 3D medical scan visualization",
        "authors": [
            "Dženan Zukić",
            "Andreas Elsner",
            "Zikrija Avdagić",
            "Gitta Domik"
        ],
        "summary": "For medical volume visualization, one of the most important tasks is to reveal clinically relevant details from the 3D scan (CT, MRI ...), e.g. the coronary arteries, without obscuring them with less significant parts. These volume datasets contain different materials which are difficult to extract and visualize with 1D transfer functions based solely on the attenuation coefficient. Multi-dimensional transfer functions allow a much more precise classification of data which makes it easier to separate different surfaces from each other. Unfortunately, setting up multi-dimensional transfer functions can become a fairly complex task, generally accomplished by trial and error. This paper explains neural networks, and then presents an efficient way to speed up visualization process by semi-automatic transfer function generation. We describe how to use neural networks to detect distinctive features shown in the 2D histogram of the volume data and how to use this information for data classification.",
        "published": "2008-06-18T08:36:15Z",
        "link": "http://arxiv.org/abs/0806.2925v2",
        "categories": [
            "cs.AI",
            "cs.GR",
            "I.3; I.2.6"
        ]
    },
    {
        "title": "Use of a Quantum Computer and the Quick Medical Reference To Give an   Approximate Diagnosis",
        "authors": [
            "Robert R. Tucci"
        ],
        "summary": "The Quick Medical Reference (QMR) is a compendium of statistical knowledge connecting diseases to findings (symptoms). The information in QMR can be represented as a Bayesian network. The inference problem (or, in more medical language, giving a diagnosis) for the QMR is to, given some findings, find the probability of each disease. Rejection sampling and likelihood weighted sampling (a.k.a. likelihood weighting) are two simple algorithms for making approximate inferences from an arbitrary Bayesian net (and from the QMR Bayesian net in particular). Heretofore, the samples for these two algorithms have been obtained with a conventional \"classical computer\". In this paper, we will show that two analogous algorithms exist for the QMR Bayesian net, where the samples are obtained with a quantum computer. We expect that these two algorithms, implemented on a quantum computer, can also be used to make inferences (and predictions) with other Bayesian nets.",
        "published": "2008-06-24T18:29:13Z",
        "link": "http://arxiv.org/abs/0806.3949v2",
        "categories": [
            "quant-ph",
            "cs.AI"
        ]
    },
    {
        "title": "On Sequences with Non-Learnable Subsequences",
        "authors": [
            "Vladimir V. V'yugin"
        ],
        "summary": "The remarkable results of Foster and Vohra was a starting point for a series of papers which show that any sequence of outcomes can be learned (with no prior knowledge) using some universal randomized forecasting algorithm and forecast-dependent checking rules. We show that for the class of all computationally efficient outcome-forecast-based checking rules, this property is violated. Moreover, we present a probabilistic algorithm generating with probability close to one a sequence with a subsequence which simultaneously miscalibrates all partially weakly computable randomized forecasting algorithms. %subsequences non-learnable by each randomized algorithm.   According to the Dawid's prequential framework we consider partial recursive randomized algorithms.",
        "published": "2008-06-26T15:21:00Z",
        "link": "http://arxiv.org/abs/0806.4341v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "F.4.1; I.2.6"
        ]
    },
    {
        "title": "Prediction with Expert Advice in Games with Unbounded One-Step Gains",
        "authors": [
            "Vladimir V. V'yugin"
        ],
        "summary": "The games of prediction with expert advice are considered in this paper. We present some modification of Kalai and Vempala algorithm of following the perturbed leader for the case of unrestrictedly large one-step gains. We show that in general case the cumulative gain of any probabilistic prediction algorithm can be much worse than the gain of some expert of the pool. Nevertheless, we give the lower bound for this cumulative gain in general case and construct a universal algorithm which has the optimal performance; we also prove that in case when one-step gains of experts of the pool have ``limited deviations'' the performance of our algorithm is close to the performance of the best expert.",
        "published": "2008-06-26T20:21:06Z",
        "link": "http://arxiv.org/abs/0806.4391v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "On empirical meaning of randomness with respect to a real parameter",
        "authors": [
            "Vladimir V'yugin"
        ],
        "summary": "We study the empirical meaning of randomness with respect to a family of probability distributions $P_\\theta$, where $\\theta$ is a real parameter, using algorithmic randomness theory. In the case when for a computable probability distribution $P_\\theta$ an effectively strongly consistent estimate exists, we show that the Levin's a priory semicomputable semimeasure of the set of all $P_\\theta$-random sequences is positive if and only if the parameter $\\theta$ is a computable real number. The different methods for generating ``meaningful'' $P_\\theta$-random sequences with noncomputable $\\theta$ are discussed.",
        "published": "2008-06-27T10:49:33Z",
        "link": "http://arxiv.org/abs/0806.4484v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "The model of quantum evolution",
        "authors": [
            "Konstantin P. Wishnevsky"
        ],
        "summary": "This paper has been withdrawn by the author due to extremely unscientific errors.",
        "published": "2008-06-27T12:59:45Z",
        "link": "http://arxiv.org/abs/0806.4511v5",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A Fixed-Parameter Algorithm for Random Instances of Weighted d-CNF   Satisfiability",
        "authors": [
            "Yong Gao"
        ],
        "summary": "We study random instances of the weighted $d$-CNF satisfiability problem (WEIGHTED $d$-SAT), a generic W[1]-complete problem. A random instance of the problem consists of a fixed parameter $k$ and a random $d$-CNF formula $\\weicnf{n}{p}{k, d}$ generated as follows: for each subset of $d$ variables and with probability $p$, a clause over the $d$ variables is selected uniformly at random from among the $2^d - 1$ clauses that contain at least one negated literals.   We show that random instances of WEIGHTED $d$-SAT can be solved in $O(k^2n + n^{O(1)})$-time with high probability, indicating that typical instances of WEIGHTED $d$-SAT under this instance distribution are fixed-parameter tractable. The result also hold for random instances from the model $\\weicnf{n}{p}{k,d}(d')$ where clauses containing less than $d' (1 < d' < d)$ negated literals are forbidden, and for random instances of the renormalized (miniaturized) version of WEIGHTED $d$-SAT in certain range of the random model's parameter $p(n)$. This, together with our previous results on the threshold behavior and the resolution complexity of unsatisfiable instances of $\\weicnf{n}{p}{k, d}$, provides an almost complete characterization of the typical-case behavior of random instances of WEIGHTED $d$-SAT.",
        "published": "2008-06-28T05:03:47Z",
        "link": "http://arxiv.org/abs/0806.4652v1",
        "categories": [
            "cs.DS",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Sparse Online Learning via Truncated Gradient",
        "authors": [
            "John Langford",
            "Lihong Li",
            "Tong Zhang"
        ],
        "summary": "We propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions. This method has several essential properties: The degree of sparsity is continuous -- a parameter controls the rate of sparsification from no sparsification to total sparsification. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular $L_1$-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online learning guarantees. The approach works well empirically. We apply the approach to several datasets and find that for datasets with large numbers of features, substantial sparsity is discoverable.",
        "published": "2008-06-28T14:19:50Z",
        "link": "http://arxiv.org/abs/0806.4686v2",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "A new Hedging algorithm and its application to inferring latent random   variables",
        "authors": [
            "Yoav Freund",
            "Daniel Hsu"
        ],
        "summary": "We present a new online learning algorithm for cumulative discounted gain. This learning algorithm does not use exponential weights on the experts. Instead, it uses a weighting scheme that depends on the regret of the master algorithm relative to the experts. In particular, experts whose discounted cumulative gain is smaller (worse) than that of the master algorithm receive zero weight. We also sketch how a regret-based algorithm can be used as an alternative to Bayesian averaging in the context of inferring latent random variables.",
        "published": "2008-06-30T05:30:30Z",
        "link": "http://arxiv.org/abs/0806.4802v1",
        "categories": [
            "cs.GT",
            "cs.AI"
        ]
    },
    {
        "title": "Unveiling the mystery of visual information processing in human brain",
        "authors": [
            "Emanuel Diamant"
        ],
        "summary": "It is generally accepted that human vision is an extremely powerful information processing system that facilitates our interaction with the surrounding world. However, despite extended and extensive research efforts, which encompass many exploration fields, the underlying fundamentals and operational principles of visual information processing in human brain remain unknown. We still are unable to figure out where and how along the path from eyes to the cortex the sensory input perceived by the retina is converted into a meaningful object representation, which can be consciously manipulated by the brain. Studying the vast literature considering the various aspects of brain information processing, I was surprised to learn that the respected scholarly discussion is totally indifferent to the basic keynote question: \"What is information?\" in general or \"What is visual information?\" in particular. In the old days, it was assumed that any scientific research approach has first to define its basic departure points. Why was it overlooked in brain information processing research remains a conundrum. In this paper, I am trying to find a remedy for this bizarre situation. I propose an uncommon definition of \"information\", which can be derived from Kolmogorov's Complexity Theory and Chaitin's notion of Algorithmic Information. Embracing this new definition leads to an inevitable revision of traditional dogmas that shape the state of the art of brain information processing research. I hope this revision would better serve the challenging goal of human visual information processing modeling.",
        "published": "2008-07-02T12:33:48Z",
        "link": "http://arxiv.org/abs/0807.0337v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "cs.IT",
            "math.IT",
            "q-bio.NC"
        ]
    },
    {
        "title": "Modeling belief systems with scale-free networks",
        "authors": [
            "Miklos Antal",
            "Laszlo Balogh"
        ],
        "summary": "Evolution of belief systems has always been in focus of cognitive research. In this paper we delineate a new model describing belief systems as a network of statements considered true. Testing the model a small number of parameters enabled us to reproduce a variety of well-known mechanisms ranging from opinion changes to development of psychological problems. The self-organizing opinion structure showed a scale-free degree distribution. The novelty of our work lies in applying a convenient set of definitions allowing us to depict opinion network dynamics in a highly favorable way, which resulted in a scale-free belief network. As an additional benefit, we listed several conjectural consequences in a number of areas related to thinking and reasoning.",
        "published": "2008-07-03T09:38:19Z",
        "link": "http://arxiv.org/abs/0807.0517v1",
        "categories": [
            "cs.AI",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Belief decision support and reject for textured images characterization",
        "authors": [
            "Arnaud Martin"
        ],
        "summary": "The textured images' classification assumes to consider the images in terms of area with the same texture. In uncertain environment, it could be better to take an imprecise decision or to reject the area corresponding to an unlearning class. Moreover, on the areas that are the classification units, we can have more than one texture. These considerations allows us to develop a belief decision model permitting to reject an area as unlearning and to decide on unions and intersections of learning classes. The proposed approach finds all its justification in an application of seabed characterization from sonar images, which contributes to an illustration.",
        "published": "2008-07-03T19:46:21Z",
        "link": "http://arxiv.org/abs/0807.0627v1",
        "categories": [
            "cs.AI",
            "I.4; I.5"
        ]
    },
    {
        "title": "The Correspondence Analysis Platform for Uncovering Deep Structure in   Data and Information",
        "authors": [
            "Fionn Murtagh"
        ],
        "summary": "We study two aspects of information semantics: (i) the collection of all relationships, (ii) tracking and spotting anomaly and change. The first is implemented by endowing all relevant information spaces with a Euclidean metric in a common projected space. The second is modelled by an induced ultrametric. A very general way to achieve a Euclidean embedding of different information spaces based on cross-tabulation counts (and from other input data formats) is provided by Correspondence Analysis. From there, the induced ultrametric that we are particularly interested in takes a sequential - e.g. temporal - ordering of the data into account. We employ such a perspective to look at narrative, \"the flow of thought and the flow of language\" (Chafe). In application to policy decision making, we show how we can focus analysis in a small number of dimensions.",
        "published": "2008-07-06T15:22:54Z",
        "link": "http://arxiv.org/abs/0807.0908v2",
        "categories": [
            "cs.AI",
            "I.5.4; H.3.1; I.2.7"
        ]
    },
    {
        "title": "Algorithm Selection as a Bandit Problem with Unbounded Losses",
        "authors": [
            "Matteo Gagliolo",
            "Juergen Schmidhuber"
        ],
        "summary": "Algorithm selection is typically based on models of algorithm performance, learned during a separate offline training sequence, which can be prohibitively expensive. In recent work, we adopted an online approach, in which a performance model is iteratively updated and used to guide selection on a sequence of problem instances. The resulting exploration-exploitation trade-off was represented as a bandit problem with expert advice, using an existing solver for this game, but this required the setting of an arbitrary bound on algorithm runtimes, thus invalidating the optimal regret of the solver. In this paper, we propose a simpler framework for representing algorithm selection as a bandit problem, with partial information, and an unknown bound on losses. We adapt an existing solver to this game, proving a bound on its expected regret, which holds also for the resulting algorithm selection technique. We present preliminary experiments with a set of SAT solvers on a mixed SAT-UNSAT benchmark.",
        "published": "2008-07-09T16:47:36Z",
        "link": "http://arxiv.org/abs/0807.1494v1",
        "categories": [
            "cs.AI",
            "cs.GT",
            "cs.LG",
            "F.2.2; G.3; I.1.2; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Extension of Inagaki General Weighted Operators and A New Fusion Rule   Class of Proportional Redistribution of Intersection Masses",
        "authors": [
            "Florentin Smarandache"
        ],
        "summary": "In this paper we extend Inagaki Weighted Operators fusion rule (WO) in information fusion by doing redistribution of not only the conflicting mass, but also of masses of non-empty intersections, that we call Double Weighted Operators (DWO). Then we propose a new fusion rule Class of Proportional Redistribution of Intersection Masses (CPRIM), which generates many interesting particular fusion rules in information fusion. Both formulas are presented for any number of sources of information. An application and comparison with other fusion rules are given in the last section.",
        "published": "2008-07-11T18:30:10Z",
        "link": "http://arxiv.org/abs/0807.1906v3",
        "categories": [
            "cs.AI",
            "I.4.8"
        ]
    },
    {
        "title": "Multi-Instance Learning by Treating Instances As Non-I.I.D. Samples",
        "authors": [
            "Zhi-Hua Zhou",
            "Yu-Yin Sun",
            "Yu-Feng Li"
        ],
        "summary": "Multi-instance learning attempts to learn from a training set consisting of labeled bags each containing many unlabeled instances. Previous studies typically treat the instances in the bags as independently and identically distributed. However, the instances in a bag are rarely independent, and therefore a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits the relations among instances. In this paper, we propose a simple yet effective multi-instance learning method, which regards each bag as a graph and uses a specific kernel to distinguish the graphs by considering the features of the nodes as well as the features of the edges that convey some relations among instances. The effectiveness of the proposed method is validated by experiments.",
        "published": "2008-07-12T20:19:18Z",
        "link": "http://arxiv.org/abs/0807.1997v4",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Hardware/Software Co-Design for Spike Based Recognition",
        "authors": [
            "Arfan Ghani",
            "Martin McGinnity",
            "Liam Maguire",
            "Jim Harkin"
        ],
        "summary": "The practical applications based on recurrent spiking neurons are limited due to their non-trivial learning algorithms. The temporal nature of spiking neurons is more favorable for hardware implementation where signals can be represented in binary form and communication can be done through the use of spikes. This work investigates the potential of recurrent spiking neurons implementations on reconfigurable platforms and their applicability in temporal based applications. A theoretical framework of reservoir computing is investigated for hardware/software implementation. In this framework, only readout neurons are trained which overcomes the burden of training at the network level. These recurrent neural networks are termed as microcircuits which are viewed as basic computational units in cortical computation. This paper investigates the potential of recurrent neural reservoirs and presents a novel hardware/software strategy for their implementation on FPGAs. The design is implemented and the functionality is tested in the context of speech recognition application.",
        "published": "2008-07-14T23:44:47Z",
        "link": "http://arxiv.org/abs/0807.2282v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CE",
            "C.1.3"
        ]
    },
    {
        "title": "CPBVP: A Constraint-Programming Framework for Bounded Program   Verification",
        "authors": [
            "Hélène Collavizza",
            "Michel Rueher",
            "Pascal Van Hentenryck"
        ],
        "summary": "This paper studies how to verify the conformity of a program with its specification and proposes a novel constraint-programming framework for bounded program verification (CPBPV). The CPBPV framework uses constraint stores to represent the specification and the program and explores execution paths nondeterministically. The input program is partially correct if each constraint store so produced implies the post-condition. CPBPV does not explore spurious execution paths as it incrementally prunes execution paths early by detecting that the constraint store is not consistent. CPBPV uses the rich language of constraint programming to express the constraint store. Finally, CPBPV is parametrized with a list of solvers which are tried in sequence, starting with the least expensive and less general. Experimental results often produce orders of magnitude improvements over earlier approaches, running times being often independent of the variable domains. Moreover, CPBPV was able to detect subtle errors in some programs while other frameworks based on model checking have failed.",
        "published": "2008-07-15T14:18:43Z",
        "link": "http://arxiv.org/abs/0807.2383v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Constructing a Knowledge Base for Gene Regulatory Dynamics by Formal   Concept Analysis Methods",
        "authors": [
            "Johannes Wollbold",
            "Reinhard Guthke",
            "Bernhard Ganter"
        ],
        "summary": "Our aim is to build a set of rules, such that reasoning over temporal dependencies within gene regulatory networks is possible. The underlying transitions may be obtained by discretizing observed time series, or they are generated based on existing knowledge, e.g. by Boolean networks or their nondeterministic generalization. We use the mathematical discipline of formal concept analysis (FCA), which has been applied successfully in domains as knowledge representation, data mining or software engineering. By the attribute exploration algorithm, an expert or a supporting computer program is enabled to decide about the validity of a minimal set of implications and thus to construct a sound and complete knowledge base. From this all valid implications are derivable that relate to the selected properties of a set of genes. We present results of our method for the initiation of sporulation in Bacillus subtilis. However the formal structures are exhibited in a most general manner. Therefore the approach may be adapted to signal transduction or metabolic networks, as well as to discrete temporal transitions in many biological and nonbiological areas.",
        "published": "2008-07-21T15:46:22Z",
        "link": "http://arxiv.org/abs/0807.3287v1",
        "categories": [
            "q-bio.MN",
            "cs.AI",
            "math.LO"
        ]
    },
    {
        "title": "Implementing general belief function framework with a practical   codification for low complexity",
        "authors": [
            "Arnaud Martin"
        ],
        "summary": "In this chapter, we propose a new practical codification of the elements of the Venn diagram in order to easily manipulate the focal elements. In order to reduce the complexity, the eventual constraints must be integrated in the codification at the beginning. Hence, we only consider a reduced hyper power set $D_r^\\Theta$ that can be $2^\\Theta$ or $D^\\Theta$. We describe all the steps of a general belief function framework. The step of decision is particularly studied, indeed, when we can decide on intersections of the singletons of the discernment space no actual decision functions are easily to use. Hence, two approaches are proposed, an extension of previous one and an approach based on the specificity of the elements on which to decide. The principal goal of this chapter is to provide practical codes of a general belief function framework for the researchers and users needing the belief function theory.",
        "published": "2008-07-22T13:50:22Z",
        "link": "http://arxiv.org/abs/0807.3483v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A new probabilistic transformation of belief mass assignment",
        "authors": [
            "Jean Dezert",
            "Florentin Smarandache"
        ],
        "summary": "In this paper, we propose in Dezert-Smarandache Theory (DSmT) framework, a new probabilistic transformation, called DSmP, in order to build a subjective probability measure from any basic belief assignment defined on any model of the frame of discernment. Several examples are given to show how the DSmP transformation works and we compare it to main existing transformations proposed in the literature so far. We show the advantages of DSmP over classical transformations in term of Probabilistic Information Content (PIC). The direct extension of this transformation for dealing with qualitative belief assignments is also presented.",
        "published": "2008-07-23T13:49:30Z",
        "link": "http://arxiv.org/abs/0807.3669v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A Distributed Process Infrastructure for a Distributed Data Structure",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "The Resource Description Framework (RDF) is continuing to grow outside the bounds of its initial function as a metadata framework and into the domain of general-purpose data modeling. This expansion has been facilitated by the continued increase in the capacity and speed of RDF database repositories known as triple-stores. High-end RDF triple-stores can hold and process on the order of 10 billion triples. In an effort to provide a seamless integration of the data contained in RDF repositories, the Linked Data community is providing specifications for linking RDF data sets into a universal distributed graph that can be traversed by both man and machine. While the seamless integration of RDF data sets is important, at the scale of the data sets that currently exist and will ultimately grow to become, the \"download and index\" philosophy of the World Wide Web will not so easily map over to the Semantic Web. This essay discusses the importance of adding a distributed RDF process infrastructure to the current distributed RDF data structure.",
        "published": "2008-07-24T15:16:16Z",
        "link": "http://arxiv.org/abs/0807.3908v1",
        "categories": [
            "cs.AI",
            "cs.DL",
            "I.2.4; C.1.4"
        ]
    },
    {
        "title": "On Introspection, Metacognitive Control and Augmented Data Mining Live   Cycles",
        "authors": [
            "Daniel Sonntag"
        ],
        "summary": "We discuss metacognitive modelling as an enhancement to cognitive modelling and computing. Metacognitive control mechanisms should enable AI systems to self-reflect, reason about their actions, and to adapt to new situations. In this respect, we propose implementation details of a knowledge taxonomy and an augmented data mining life cycle which supports a live integration of obtained models.",
        "published": "2008-07-28T12:05:16Z",
        "link": "http://arxiv.org/abs/0807.4417v2",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "An Image-Based Sensor System for Autonomous Rendez-Vous with   Uncooperative Satellites",
        "authors": [
            "Carlos Miravet",
            "Luis Pascual",
            "Eloise Krouch",
            "Juan Manuel del Cura"
        ],
        "summary": "In this paper are described the image processing algorithms developed by SENER, Ingenieria y Sistemas to cope with the problem of image-based, autonomous rendez-vous (RV) with an orbiting satellite. The methods developed have a direct application in the OLEV (Orbital Life Extension Extension Vehicle) mission. OLEV is a commercial mission under development by a consortium formed by Swedish Space Corporation, Kayser-Threde and SENER, aimed to extend the operational life of geostationary telecommunication satellites by supplying them control, navigation and guidance services. OLEV is planned to use a set of cameras to determine the angular position and distance to the client satellite during the complete phases of rendez-vous and docking, thus enabling the operation with satellites not equipped with any specific navigational aid to provide support during the approach. The ability to operate with un-equipped client satellites significantly expands the range of applicability of the system under development, compared to other competing video technologies already tested in previous spatial missions, such as the ones described here below.",
        "published": "2008-07-28T15:46:02Z",
        "link": "http://arxiv.org/abs/0807.4478v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.4.3; I.4.8"
        ]
    },
    {
        "title": "AceWiki: A Natural and Expressive Semantic Wiki",
        "authors": [
            "Tobias Kuhn"
        ],
        "summary": "We present AceWiki, a prototype of a new kind of semantic wiki using the controlled natural language Attempto Controlled English (ACE) for representing its content. ACE is a subset of English with a restricted grammar and a formal semantics. The use of ACE has two important advantages over existing semantic wikis. First, we can improve the usability and achieve a shallow learning curve. Second, ACE is more expressive than the formal languages of existing semantic wikis. Our evaluation shows that people who are not familiar with the formal foundations of the Semantic Web are able to deal with AceWiki after a very short learning phase and without the help of an expert.",
        "published": "2008-07-29T09:54:44Z",
        "link": "http://arxiv.org/abs/0807.4618v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "H.5.2; I.2.4"
        ]
    },
    {
        "title": "AceWiki: Collaborative Ontology Management in Controlled Natural   Language",
        "authors": [
            "Tobias Kuhn"
        ],
        "summary": "AceWiki is a prototype that shows how a semantic wiki using controlled natural language - Attempto Controlled English (ACE) in our case - can make ontology management easy for everybody. Sentences in ACE can automatically be translated into first-order logic, OWL, or SWRL. AceWiki integrates the OWL reasoner Pellet and ensures that the ontology is always consistent. Previous results have shown that people with no background in logic are able to add formal knowledge to AceWiki without being instructed or trained in advance.",
        "published": "2008-07-29T10:15:38Z",
        "link": "http://arxiv.org/abs/0807.4623v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "H.5.2; I.2.4"
        ]
    },
    {
        "title": "Hacia una teoria de unificacion para los comportamientos cognitivos",
        "authors": [
            "Sergio Miguel"
        ],
        "summary": "Each cognitive science tries to understand a set of cognitive behaviors. The structuring of knowledge of this nature's aspect is far from what it can be expected about a science. Until now universal standard consistently describing the set of cognitive behaviors has not been found, and there are many questions about the cognitive behaviors for which only there are opinions of members of the scientific community. This article has three proposals. The first proposal is to raise to the scientific community the necessity of unified the cognitive behaviors. The second proposal is claim the application of the Newton's reasoning rules about nature of his book, Philosophiae Naturalis Principia Mathematica, to the cognitive behaviors. The third is to propose a scientific theory, currently developing, that follows the rules established by Newton to make sense of nature, and could be the theory to explain all the cognitive behaviors.",
        "published": "2008-07-29T15:11:12Z",
        "link": "http://arxiv.org/abs/0807.4680v3",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Message-passing for Maximum Weight Independent Set",
        "authors": [
            "Sujay Sanghavi",
            "Devavrat Shah",
            "Alan Willsky"
        ],
        "summary": "We investigate the use of message-passing algorithms for the problem of finding the max-weight independent set (MWIS) in a graph. First, we study the performance of the classical loopy max-product belief propagation. We show that each fixed point estimate of max-product can be mapped in a natural way to an extreme point of the LP polytope associated with the MWIS problem. However, this extreme point may not be the one that maximizes the value of node weights; the particular extreme point at final convergence depends on the initialization of max-product. We then show that if max-product is started from the natural initialization of uninformative messages, it always solves the correct LP -- if it converges. This result is obtained via a direct analysis of the iterative algorithm, and cannot be obtained by looking only at fixed points.   The tightness of the LP relaxation is thus necessary for max-product optimality, but it is not sufficient. Motivated by this observation, we show that a simple modification of max-product becomes gradient descent on (a convexified version of) the dual of the LP, and converges to the dual optimum. We also develop a message-passing algorithm that recovers the primal MWIS solution from the output of the descent algorithm. We show that the MWIS estimate obtained using these two algorithms in conjunction is correct when the graph is bipartite and the MWIS is unique.   Finally, we show that any problem of MAP estimation for probability distributions over finite domains can be reduced to an MWIS problem. We believe this reduction will yield new insights and algorithms for MAP estimation.",
        "published": "2008-07-31T15:26:27Z",
        "link": "http://arxiv.org/abs/0807.5091v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "I'm sorry to say, but your understanding of image processing   fundamentals is absolutely wrong",
        "authors": [
            "Emanuel Diamant"
        ],
        "summary": "The ongoing discussion whether modern vision systems have to be viewed as visually-enabled cognitive systems or cognitively-enabled vision systems is groundless, because perceptual and cognitive faculties of vision are separate components of human (and consequently, artificial) information processing system modeling.",
        "published": "2008-08-01T04:45:17Z",
        "link": "http://arxiv.org/abs/0808.0056v1",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.IR",
            "cs.RO",
            "q-bio.NC"
        ]
    },
    {
        "title": "Mathematical Structure of Quantum Decision Theory",
        "authors": [
            "V. I. Yukalov",
            "D. Sornette"
        ],
        "summary": "One of the most complex systems is the human brain whose formalized functioning is characterized by decision theory. We present a \"Quantum Decision Theory\" of decision making, based on the mathematical theory of separable Hilbert spaces. This mathematical structure captures the effect of superposition of composite prospects, including many incorporated intentions, which allows us to explain a variety of interesting fallacies and anomalies that have been reported to particularize the decision making of real human beings. The theory describes entangled decision making, non-commutativity of subsequent decisions, and intention interference of composite prospects. We demonstrate how the violation of the Savage's sure-thing principle (disjunction effect) can be explained as a result of the interference of intentions, when making decisions under uncertainty. The conjunction fallacy is also explained by the presence of the interference terms. We demonstrate that all known anomalies and paradoxes, documented in the context of classical decision theory, are reducible to just a few mathematical archetypes, all of which finding straightforward explanations in the frame of the developed quantum approach.",
        "published": "2008-08-01T13:14:20Z",
        "link": "http://arxiv.org/abs/0808.0112v3",
        "categories": [
            "cs.AI",
            "math-ph",
            "math.MP",
            "quant-ph"
        ]
    },
    {
        "title": "Text Modeling using Unsupervised Topic Models and Concept Hierarchies",
        "authors": [
            "Chaitanya Chemudugunta",
            "Padhraic Smyth",
            "Mark Steyvers"
        ],
        "summary": "Statistical topic models provide a general data-driven framework for automated discovery of high-level knowledge from large collections of text documents. While topic models can potentially discover a broad range of themes in a data set, the interpretability of the learned topics is not always ideal. Human-defined concepts, on the other hand, tend to be semantically richer due to careful selection of words to define concepts but they tend not to cover the themes in a data set exhaustively. In this paper, we propose a probabilistic framework to combine a hierarchy of human-defined semantic concepts with statistical topic models to seek the best of both worlds. Experimental results using two different sources of concept hierarchies and two collections of text documents indicate that this combination leads to systematic improvements in the quality of the associated language models as well as enabling new techniques for inferring and visualizing the semantics of a document.",
        "published": "2008-08-07T07:59:29Z",
        "link": "http://arxiv.org/abs/0808.0973v1",
        "categories": [
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Verified Null-Move Pruning",
        "authors": [
            "Omid David-Tabibi",
            "Nathan S. Netanyahu"
        ],
        "summary": "In this article we review standard null-move pruning and introduce our extended version of it, which we call verified null-move pruning. In verified null-move pruning, whenever the shallow null-move search indicates a fail-high, instead of cutting off the search from the current node, the search is continued with reduced depth.   Our experiments with verified null-move pruning show that on average, it constructs a smaller search tree with greater tactical strength in comparison to standard null-move pruning. Moreover, unlike standard null-move pruning, which fails badly in zugzwang positions, verified null-move pruning manages to detect most zugzwangs and in such cases conducts a re-search to obtain the correct result. In addition, verified null-move pruning is very easy to implement, and any standard null-move pruning program can use verified null-move pruning by modifying only a few lines of code.",
        "published": "2008-08-08T12:44:10Z",
        "link": "http://arxiv.org/abs/0808.1125v1",
        "categories": [
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "Commonsense Knowledge, Ontology and Ordinary Language",
        "authors": [
            "Walid S. Saba"
        ],
        "summary": "Over two decades ago a \"quite revolution\" overwhelmingly replaced knowledgebased approaches in natural language processing (NLP) by quantitative (e.g., statistical, corpus-based, machine learning) methods. Although it is our firm belief that purely quantitative approaches cannot be the only paradigm for NLP, dissatisfaction with purely engineering approaches to the construction of large knowledge bases for NLP are somewhat justified. In this paper we hope to demonstrate that both trends are partly misguided and that the time has come to enrich logical semantics with an ontological structure that reflects our commonsense view of the world and the way we talk about in ordinary language. In this paper it will be demonstrated that assuming such an ontological structure a number of challenges in the semantics of natural language (e.g., metonymy, intensionality, copredication, nominal compounds, etc.) can be properly and uniformly addressed.",
        "published": "2008-08-08T14:37:45Z",
        "link": "http://arxiv.org/abs/0808.1211v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Comparison between CPBPV, ESC/Java, CBMC, Blast, EUREKA and Why for   Bounded Program Verification",
        "authors": [
            "Hélène Collavizza",
            "Michel Rueher",
            "Pascal Van Hentenryck"
        ],
        "summary": "This report describes experimental results for a set of benchmarks on program verification. It compares the capabilities of CPBVP \"Constraint Programming framework for Bounded Program Verification\" [4] with the following frameworks: ESC/Java, CBMC, Blast, EUREKA and Why.",
        "published": "2008-08-11T12:55:19Z",
        "link": "http://arxiv.org/abs/0808.1508v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Initial Results on the F-logic to OWL Bi-directional Translation on a   Tabled Prolog Engine",
        "authors": [
            "Paul Fodor"
        ],
        "summary": "In this paper, we show our results on the bi-directional data exchange between the F-logic language supported by the Flora2 system and the OWL language. Most of the TBox and ABox axioms are translated preserving the semantics between the two representations, such as: proper inclusion, individual definition, functional properties, while some axioms and restrictions require a change in the semantics, such as: numbered and qualified cardinality restrictions. For the second case, we translate the OWL definite style inference rules into F-logic style constraints. We also describe a set of reasoning examples using the above translation, including the reasoning in Flora2 of a variety of ABox queries.",
        "published": "2008-08-12T19:58:59Z",
        "link": "http://arxiv.org/abs/0808.1721v1",
        "categories": [
            "cs.AI",
            "cs.SE"
        ]
    },
    {
        "title": "Building an interpretable fuzzy rule base from data using Orthogonal   Least Squares Application to a depollution problem",
        "authors": [
            "Sébastien Destercke",
            "Serge Guillaume",
            "Brigitte Charnomordic"
        ],
        "summary": "In many fields where human understanding plays a crucial role, such as bioprocesses, the capacity of extracting knowledge from data is of critical importance. Within this framework, fuzzy learning methods, if properly used, can greatly help human experts. Amongst these methods, the aim of orthogonal transformations, which have been proven to be mathematically robust, is to build rules from a set of training data and to select the most important ones by linear regression or rank revealing techniques. The OLS algorithm is a good representative of those methods. However, it was originally designed so that it only cared about numerical performance. Thus, we propose some modifications of the original method to take interpretability into account. After recalling the original algorithm, this paper presents the changes made to the original method, then discusses some results obtained from benchmark problems. Finally, the algorithm is applied to a real-world fault detection depollution problem.",
        "published": "2008-08-21T19:54:04Z",
        "link": "http://arxiv.org/abs/0808.2984v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "n-ary Fuzzy Logic and Neutrosophic Logic Operators",
        "authors": [
            "Florentin Smarandache",
            "V. Christianto"
        ],
        "summary": "We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy logic and neutrosophic logic operators using the smarandache codification of the Venn diagram and a defined vector neutrosophic law. In such way, new operators in neutrosophic logic/set/probability are built.",
        "published": "2008-08-22T16:10:36Z",
        "link": "http://arxiv.org/abs/0808.3109v3",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Multi-Instance Multi-Label Learning",
        "authors": [
            "Zhi-Hua Zhou",
            "Min-Ling Zhang",
            "Sheng-Jun Huang",
            "Yu-Feng Li"
        ],
        "summary": "In this paper, we propose the MIML (Multi-Instance Multi-Label learning) framework where an example is described by multiple instances and associated with multiple class labels. Compared to traditional learning frameworks, the MIML framework is more convenient and natural for representing complicated objects which have multiple semantic meanings. To learn from MIML examples, we propose the MimlBoost and MimlSvm algorithms based on a simple degeneration strategy, and experiments show that solving problems involving complicated objects with multiple semantic meanings in the MIML framework can lead to good performance. Considering that the degeneration process may lose information, we propose the D-MimlSvm algorithm which tackles MIML problems directly in a regularization framework. Moreover, we show that even when we do not have access to the real objects and thus cannot capture more information from real objects by using the MIML representation, MIML is still useful. We propose the InsDif and SubCod algorithms. InsDif works by transforming single-instances into the MIML representation for learning, while SubCod works by transforming single-label examples into the MIML representation for learning. Experiments show that in some tasks they are able to achieve better performance than learning the single-instances or single-label examples directly.",
        "published": "2008-08-24T06:31:43Z",
        "link": "http://arxiv.org/abs/0808.3231v4",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Randomised Variable Neighbourhood Search for Multi Objective   Optimisation",
        "authors": [
            "Martin Josef Geiger"
        ],
        "summary": "Various local search approaches have recently been applied to machine scheduling problems under multiple objectives. Their foremost consideration is the identification of the set of Pareto optimal alternatives. An important aspect of successfully solving these problems lies in the definition of an appropriate neighbourhood structure. Unclear in this context remains, how interdependencies within the fitness landscape affect the resolution of the problem.   The paper presents a study of neighbourhood search operators for multiple objective flow shop scheduling. Experiments have been carried out with twelve different combinations of criteria. To derive exact conclusions, small problem instances, for which the optimal solutions are known, have been chosen. Statistical tests show that no single neighbourhood operator is able to equally identify all Pareto optimal alternatives. Significant improvements however have been obtained by hybridising the solution algorithm using a randomised variable neighbourhood search technique.",
        "published": "2008-09-01T15:32:49Z",
        "link": "http://arxiv.org/abs/0809.0271v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Foundations of the Pareto Iterated Local Search Metaheuristic",
        "authors": [
            "Martin Josef Geiger"
        ],
        "summary": "The paper describes the proposition and application of a local search metaheuristic for multi-objective optimization problems. It is based on two main principles of heuristic search, intensification through variable neighborhoods, and diversification through perturbations and successive iterations in favorable regions of the search space. The concept is successfully tested on permutation flow shop scheduling problems under multiple objectives. While the obtained results are encouraging in terms of their quality, another positive attribute of the approach is its' simplicity as it does require the setting of only very few parameters. The implementation of the Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer system of local search heuristics for multi-objective scheduling which has been awarded the European Academic Software Award 2002 in Ronneby, Sweden (http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)",
        "published": "2008-09-02T11:29:45Z",
        "link": "http://arxiv.org/abs/0809.0406v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A Computational Study of Genetic Crossover Operators for Multi-Objective   Vehicle Routing Problem with Soft Time Windows",
        "authors": [
            "Martin Josef Geiger"
        ],
        "summary": "The article describes an investigation of the effectiveness of genetic algorithms for multi-objective combinatorial optimization (MOCO) by presenting an application for the vehicle routing problem with soft time windows. The work is motivated by the question, if and how the problem structure influences the effectiveness of different configurations of the genetic algorithm. Computational results are presented for different classes of vehicle routing problems, varying in their coverage with time windows, time window size, distribution and number of customers. The results are compared with a simple, but effective local search approach for multi-objective combinatorial optimization problems.",
        "published": "2008-09-02T11:39:52Z",
        "link": "http://arxiv.org/abs/0809.0410v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Genetic Algorithms for multiple objective vehicle routing",
        "authors": [
            "Martin Josef Geiger"
        ],
        "summary": "The talk describes a general approach of a genetic algorithm for multiple objective optimization problems. A particular dominance relation between the individuals of the population is used to define a fitness operator, enabling the genetic algorithm to adress even problems with efficient, but convex-dominated alternatives. The algorithm is implemented in a multilingual computer program, solving vehicle routing problems with time windows under multiple objectives. The graphical user interface of the program shows the progress of the genetic algorithm and the main parameters of the approach can be easily modified. In addition to that, the program provides powerful decision support to the decision maker. The software has proved it's excellence at the finals of the European Academic Software Award EASA, held at the Keble college/ University of Oxford/ Great Britain.",
        "published": "2008-09-02T12:08:56Z",
        "link": "http://arxiv.org/abs/0809.0416v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "The Stock Market as a Game: An Agent Based Approach to Trading in Stocks",
        "authors": [
            "Eric Engle"
        ],
        "summary": "Just as war is sometimes fallaciously represented as a zero sum game -- when in fact war is a negative sum game - stock market trading, a positive sum game over time, is often erroneously represented as a zero sum game. This is called the \"zero sum fallacy\" -- the erroneous belief that one trader in a stock market exchange can only improve their position provided some other trader's position deteriorates. However, a positive sum game in absolute terms can be recast as a zero sum game in relative terms. Similarly it appears that negative sum games in absolute terms have been recast as zero sum games in relative terms: otherwise, why would zero sum games be used to represent situations of war? Such recasting may have heuristic or pedagogic interest but recasting must be clearly explicited or risks generating confusion.   Keywords: Game theory, stock trading and agent based AI.",
        "published": "2008-09-02T14:58:00Z",
        "link": "http://arxiv.org/abs/0809.0448v1",
        "categories": [
            "q-fin.TR",
            "cs.AI",
            "cs.GT",
            "H.4"
        ]
    },
    {
        "title": "Agent Models of Political Interactions",
        "authors": [
            "Eric Engle"
        ],
        "summary": "Looks at state interactions from an agent based AI perspective to see state interactions as an example of emergent intelligent behavior. Exposes basic principles of game theory.",
        "published": "2008-09-02T16:06:40Z",
        "link": "http://arxiv.org/abs/0809.0458v1",
        "categories": [
            "cs.AI",
            "cs.GT",
            "H.4"
        ]
    },
    {
        "title": "A framework for the interactive resolution of multi-objective vehicle   routing problems",
        "authors": [
            "Martin Josef Geiger",
            "Wolf Wenger"
        ],
        "summary": "The article presents a framework for the resolution of rich vehicle routing problems which are difficult to address with standard optimization techniques. We use local search on the basis on variable neighborhood search for the construction of the solutions, but embed the techniques in a flexible framework that allows the consideration of complex side constraints of the problem such as time windows, multiple depots, heterogeneous fleets, and, in particular, multiple optimization criteria. In order to identify a compromise alternative that meets the requirements of the decision maker, an interactive procedure is integrated in the resolution of the problem, allowing the modification of the preference information articulated by the decision maker. The framework is prototypically implemented in a computer system. First results of test runs on multiple depot vehicle routing problems with time windows are reported.",
        "published": "2008-09-03T12:22:08Z",
        "link": "http://arxiv.org/abs/0809.0610v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Improving Local Search for Fuzzy Scheduling Problems",
        "authors": [
            "Martin Josef Geiger",
            "Sanja Petrovic"
        ],
        "summary": "The integration of fuzzy set theory and fuzzy logic into scheduling is a rather new aspect with growing importance for manufacturing applications, resulting in various unsolved aspects. In the current paper, we investigate an improved local search technique for fuzzy scheduling problems with fitness plateaus, using a multi criteria formulation of the problem. We especially address the problem of changing job priorities over time as studied at the Sherwood Press Ltd, a Nottingham based printing company, who is a collaborator on the project.",
        "published": "2008-09-03T16:16:43Z",
        "link": "http://arxiv.org/abs/0809.0662v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Proposition of the Interactive Pareto Iterated Local Search Procedure -   Elements and Initial Experiments",
        "authors": [
            "Martin Josef Geiger"
        ],
        "summary": "The article presents an approach to interactively solve multi-objective optimization problems. While the identification of efficient solutions is supported by computational intelligence techniques on the basis of local search, the search is directed by partial preference information obtained from the decision maker.   An application of the approach to biobjective portfolio optimization, modeled as the well-known knapsack problem, is reported, and experimental results are reported for benchmark instances taken from the literature. In brief, we obtain encouraging results that show the applicability of the approach to the described problem.",
        "published": "2008-09-04T06:52:16Z",
        "link": "http://arxiv.org/abs/0809.0753v1",
        "categories": [
            "cs.AI",
            "cs.HC"
        ]
    },
    {
        "title": "Bin Packing Under Multiple Objectives - a Heuristic Approximation   Approach",
        "authors": [
            "Martin Josef Geiger"
        ],
        "summary": "The article proposes a heuristic approximation approach to the bin packing problem under multiple objectives. In addition to the traditional objective of minimizing the number of bins, the heterogeneousness of the elements in each bin is minimized, leading to a biobjective formulation of the problem with a tradeoff between the number of bins and their heterogeneousness. An extension of the Best-Fit approximation algorithm is presented to solve the problem. Experimental investigations have been carried out on benchmark instances of different size, ranging from 100 to 1000 items. Encouraging results have been obtained, showing the applicability of the heuristic approach to the described problem.",
        "published": "2008-09-04T07:02:29Z",
        "link": "http://arxiv.org/abs/0809.0755v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "An application of the Threshold Accepting metaheuristic for curriculum   based course timetabling",
        "authors": [
            "Martin Josef Geiger"
        ],
        "summary": "The article presents a local search approach for the solution of timetabling problems in general, with a particular implementation for competition track 3 of the International Timetabling Competition 2007 (ITC 2007). The heuristic search procedure is based on Threshold Accepting to overcome local optima. A stochastic neighborhood is proposed and implemented, randomly removing and reassigning events from the current solution.   The overall concept has been incrementally obtained from a series of experiments, which we describe in each (sub)section of the paper. In result, we successfully derived a potential candidate solution approach for the finals of track 3 of the ITC 2007.",
        "published": "2008-09-04T07:12:02Z",
        "link": "http://arxiv.org/abs/0809.0757v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Peek Arc Consistency",
        "authors": [
            "Manuel Bodirsky",
            "Hubie Chen"
        ],
        "summary": "This paper studies peek arc consistency, a reasoning technique that extends the well-known arc consistency technique for constraint satisfaction. In contrast to other more costly extensions of arc consistency that have been studied in the literature, peek arc consistency requires only linear space and quadratic time and can be parallelized in a straightforward way such that it runs in linear time with a linear number of processors. We demonstrate that for various constraint languages, peek arc consistency gives a polynomial-time decision procedure for the constraint satisfaction problem. We also present an algebraic characterization of those constraint languages that can be solved by peek arc consistency, and study the robustness of the algorithm.",
        "published": "2008-09-04T11:15:50Z",
        "link": "http://arxiv.org/abs/0809.0788v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Superposition for Fixed Domains",
        "authors": [
            "Matthias Horbach",
            "Christoph Weidenbach"
        ],
        "summary": "Superposition is an established decision procedure for a variety of first-order logic theories represented by sets of clauses. A satisfiable theory, saturated by superposition, implicitly defines a minimal term-generated model for the theory. Proving universal properties with respect to a saturated theory directly leads to a modification of the minimal model's term-generated domain, as new Skolem functions are introduced. For many applications, this is not desired.   Therefore, we propose the first superposition calculus that can explicitly represent existentially quantified variables and can thus compute with respect to a given domain. This calculus is sound and refutationally complete in the limit for a first-order fixed domain semantics. For saturated Horn theories and classes of positive formulas, we can even employ the calculus to prove properties of the minimal model itself, going beyond the scope of known superposition-based approaches.",
        "published": "2008-09-04T21:58:58Z",
        "link": "http://arxiv.org/abs/0809.0922v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; F.4.1"
        ]
    },
    {
        "title": "MOOPPS: An Optimization System for Multi Objective Scheduling",
        "authors": [
            "Martin Josef Geiger"
        ],
        "summary": "In the current paper, we present an optimization system solving multi objective production scheduling problems (MOOPPS). The identification of Pareto optimal alternatives or at least a close approximation of them is possible by a set of implemented metaheuristics. Necessary control parameters can easily be adjusted by the decision maker as the whole software is fully menu driven. This allows the comparison of different metaheuristic algorithms for the considered problem instances. Results are visualized by a graphical user interface showing the distribution of solutions in outcome space as well as their corresponding Gantt chart representation.   The identification of a most preferred solution from the set of efficient solutions is supported by a module based on the aspiration interactive method (AIM). The decision maker successively defines aspiration levels until a single solution is chosen.   After successfully competing in the finals in Ronneby, Sweden, the MOOPPS software has been awarded the European Academic Software Award 2002 (http://www.bth.se/llab/easa_2002.nsf)",
        "published": "2008-09-05T06:22:36Z",
        "link": "http://arxiv.org/abs/0809.0961v1",
        "categories": [
            "cs.AI",
            "cs.HC"
        ]
    },
    {
        "title": "Variable Neighborhood Search for the University Lecturer-Student   Assignment Problem",
        "authors": [
            "Martin Josef Geiger",
            "Wolf Wenger"
        ],
        "summary": "The paper presents a study of local search heuristics in general and variable neighborhood search in particular for the resolution of an assignment problem studied in the practical work of universities. Here, students have to be assigned to scientific topics which are proposed and supported by members of staff. The problem involves the optimization under given preferences of students which may be expressed when applying for certain topics.   It is possible to observe that variable neighborhood search leads to superior results for the tested problem instances. One instance is taken from an actual case, while others have been generated based on the real world data to support the analysis with a deeper analysis.   An extension of the problem has been formulated by integrating a second objective function that simultaneously balances the workload of the members of staff while maximizing utility of the students. The algorithmic approach has been prototypically implemented in a computer system. One important aspect in this context is the application of the research work to problems of other scientific institutions, and therefore the provision of decision support functionalities.",
        "published": "2008-09-05T17:02:55Z",
        "link": "http://arxiv.org/abs/0809.1077v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Applications of Universal Source Coding to Statistical Analysis of Time   Series",
        "authors": [
            "Boris Ryabko"
        ],
        "summary": "We show how universal codes can be used for solving some of the most important statistical problems for time series. By definition, a universal code (or a universal lossless data compressor) can compress any sequence generated by a stationary and ergodic source asymptotically to the Shannon entropy, which, in turn, is the best achievable ratio for lossless data compressors.   We consider finite-alphabet and real-valued time series and the following problems: estimation of the limiting probabilities for finite-alphabet time series and estimation of the density for real-valued time series, the on-line prediction, regression, classification (or problems with side information) for both types of the time series and the following problems of hypothesis testing: goodness-of-fit testing, or identity testing, and testing of serial independence. It is important to note that all problems are considered in the framework of classical mathematical statistics and, on the other hand, everyday methods of data compression (or archivers) can be used as a tool for the estimation and testing. It turns out, that quite often the suggested methods and tests are more powerful than known ones when they are applied in practice.",
        "published": "2008-09-07T15:37:48Z",
        "link": "http://arxiv.org/abs/0809.1226v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "ECOLANG - Communications Language for Ecological Simulations Network",
        "authors": [
            "Antonio Pereira"
        ],
        "summary": "This document describes the communication language used in one multiagent system environment for ecological simulations, based on EcoDynamo simulator application linked with several intelligent agents and visualisation applications, and extends the initial definition of the language. The agents actions and perceptions are translated into messages exchanged with the simulator application and other agents. The concepts and definitions used follow the BNF notation (Backus et al. 1960) and is inspired in the Coach Unilang language (Reis and Lau 2002).",
        "published": "2008-09-09T17:46:17Z",
        "link": "http://arxiv.org/abs/0809.1618v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Agent-based Ecological Model Calibration - on the Edge of a New Approach",
        "authors": [
            "Antonio Pereira",
            "Pedro Duarte",
            "Luis Paulo Reis"
        ],
        "summary": "The purpose of this paper is to present a new approach to ecological model calibration -- an agent-based software. This agent works on three stages: 1- It builds a matrix that synthesizes the inter-variable relationships; 2- It analyses the steady-state sensitivity of different variables to different parameters; 3- It runs the model iteratively and measures model lack of fit, adequacy and reliability. Stage 3 continues until some convergence criteria are attained. At each iteration, the agent knows from stages 1 and 2, which parameters are most likely to produce the desired shift on predicted results.",
        "published": "2008-09-09T22:12:37Z",
        "link": "http://arxiv.org/abs/0809.1686v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Randomized Distributed Configuration Management of Wireless Networks:   Multi-layer Markov Random Fields and Near-Optimality",
        "authors": [
            "Sung-eok Jeon",
            "Chunayi Ji"
        ],
        "summary": "Distributed configuration management is imperative for wireless infrastructureless networks where each node adjusts locally its physical and logical configuration through information exchange with neighbors. Two issues remain open. The first is the optimality. The second is the complexity. We study these issues through modeling, analysis, and randomized distributed algorithms. Modeling defines the optimality. We first derive a global probabilistic model for a network configuration which characterizes jointly the statistical spatial dependence of a physical- and a logical-configuration. We then show that a local model which approximates the global model is a two-layer Markov Random Field or a random bond model. The complexity of the local model is the communication range among nodes. The local model is near-optimal when the approximation error to the global model is within a given error bound. We analyze the trade-off between an approximation error and complexity, and derive sufficient conditions on the near-optimality of the local model. We validate the model, the analysis and the randomized distributed algorithms also through simulation.",
        "published": "2008-09-11T05:00:51Z",
        "link": "http://arxiv.org/abs/0809.1916v1",
        "categories": [
            "cs.DC",
            "cs.AI"
        ]
    },
    {
        "title": "Electricity Demand and Energy Consumption Management System",
        "authors": [
            "Juan Ojeda Sarmiento"
        ],
        "summary": "This project describes the electricity demand and energy consumption management system and its application to Southern Peru smelter. It is composed of an hourly demand-forecasting module and of a simulation component for a plant electrical system. The first module was done using dynamic neural networks with backpropagation training algorithm; it is used to predict the electric power demanded every hour, with an error percentage below of 1%. This information allows efficient management of energy peak demands before this happen, distributing the raise of electric load to other hours or improving those equipments that increase the demand. The simulation module is based in advanced estimation techniques, such as: parametric estimation, neural network modeling, statistic regression and previously developed models, which simulates the electric behavior of the smelter plant. These modules facilitate electricity demand and consumption proper planning, because they allow knowing the behavior of the hourly demand and the consumption patterns of the plant, including the bill components, but also energy deficiencies and opportunities for improvement, based on analysis of information about equipments, processes and production plans, as well as maintenance programs. Finally the results of its application in Southern Peru smelter are presented.",
        "published": "2008-09-14T22:26:49Z",
        "link": "http://arxiv.org/abs/0809.2421v5",
        "categories": [
            "cs.AI",
            "cs.CE"
        ]
    },
    {
        "title": "Normalized Information Distance",
        "authors": [
            "Paul M. B. Vitanyi",
            "Frank J. Balbach",
            "Rudi L. Cilibrasi",
            "Ming Li"
        ],
        "summary": "The normalized information distance is a universal distance measure for objects of all kinds. It is based on Kolmogorov complexity and thus uncomputable, but there are ways to utilize it. First, compression algorithms can be used to approximate the Kolmogorov complexity if the objects have a string representation. Second, for names and abstract concepts, page count statistics from the World Wide Web can be used. These practical realizations of the normalized information distance can then be applied to machine learning tasks, expecially clustering, to perform feature-free and parameter-free data mining. This chapter discusses the theoretical foundations of the normalized information distance and both practical realizations. It presents numerous examples of successful real-world applications based on these distance measures, ranging from bioinformatics to music clustering to machine translation.",
        "published": "2008-09-15T15:33:11Z",
        "link": "http://arxiv.org/abs/0809.2553v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "Predicting Abnormal Returns From News Using Text Classification",
        "authors": [
            "Ronny Luss",
            "Alexandre d'Aspremont"
        ],
        "summary": "We show how text from news articles can be used to predict intraday price movements of financial assets using support vector machines. Multiple kernel learning is used to combine equity returns with text as predictive features to increase classification performance and we develop an analytic center cutting plane method to solve the kernel learning problem efficiently. We observe that while the direction of returns is not predictable using either text or returns, their size is, with text features producing significantly better performance than historical returns alone.",
        "published": "2008-09-16T20:05:00Z",
        "link": "http://arxiv.org/abs/0809.2792v3",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Finding links and initiators: a graph reconstruction problem",
        "authors": [
            "Heikki Mannila",
            "Evimaria Terzi"
        ],
        "summary": "Consider a 0-1 observation matrix M, where rows correspond to entities and columns correspond to signals; a value of 1 (or 0) in cell (i,j) of M indicates that signal j has been observed (or not observed) in entity i. Given such a matrix we study the problem of inferring the underlying directed links between entities (rows) and finding which entries in the matrix are initiators.   We formally define this problem and propose an MCMC framework for estimating the links and the initiators given the matrix of observations M. We also show how this framework can be extended to incorporate a temporal aspect; instead of considering a single observation matrix M we consider a sequence of observation matrices M1,..., Mt over time.   We show the connection between our problem and several problems studied in the field of social-network analysis. We apply our method to paleontological and ecological data and show that our algorithms work well in practice and give reasonable results.",
        "published": "2008-09-17T22:28:29Z",
        "link": "http://arxiv.org/abs/0809.3027v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "physics.soc-ph",
            "H.2.8"
        ]
    },
    {
        "title": "Extended ASP tableaux and rule redundancy in normal logic programs",
        "authors": [
            "Matti Järvisalo",
            "Emilia Oikarinen"
        ],
        "summary": "We introduce an extended tableau calculus for answer set programming (ASP). The proof system is based on the ASP tableaux defined in [Gebser&Schaub, ICLP 2006], with an added extension rule. We investigate the power of Extended ASP Tableaux both theoretically and empirically. We study the relationship of Extended ASP Tableaux with the Extended Resolution proof system defined by Tseitin for sets of clauses, and separate Extended ASP Tableaux from ASP Tableaux by giving a polynomial-length proof for a family of normal logic programs P_n for which ASP Tableaux has exponential-length minimal proofs with respect to n. Additionally, Extended ASP Tableaux imply interesting insight into the effect of program simplification on the lengths of proofs in ASP. Closely related to Extended ASP Tableaux, we empirically investigate the effect of redundant rules on the efficiency of ASP solving.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2008-09-18T16:35:20Z",
        "link": "http://arxiv.org/abs/0809.3204v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Generalized Prediction Intervals for Arbitrary Distributed   High-Dimensional Data",
        "authors": [
            "Steffen Kuehn"
        ],
        "summary": "This paper generalizes the traditional statistical concept of prediction intervals for arbitrary probability density functions in high-dimensional feature spaces by introducing significance level distributions, which provides interval-independent probabilities for continuous random variables. The advantage of the transformation of a probability density function into a significance level distribution is that it enables one-class classification or outlier detection in a direct manner.",
        "published": "2008-09-19T11:02:39Z",
        "link": "http://arxiv.org/abs/0809.3352v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Learning Hidden Markov Models using Non-Negative Matrix Factorization",
        "authors": [
            "George Cybenko",
            "Valentino Crespi"
        ],
        "summary": "The Baum-Welsh algorithm together with its derivatives and variations has been the main technique for learning Hidden Markov Models (HMM) from observational data. We present an HMM learning algorithm based on the non-negative matrix factorization (NMF) of higher order Markovian statistics that is structurally different from the Baum-Welsh and its associated approaches. The described algorithm supports estimation of the number of recurrent states of an HMM and iterates the non-negative matrix factorization (NMF) algorithm to improve the learned HMM parameters. Numerical examples are provided as well.",
        "published": "2008-09-24T05:34:56Z",
        "link": "http://arxiv.org/abs/0809.4086v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Mining Meaning from Wikipedia",
        "authors": [
            "Olena Medelyan",
            "David Milne",
            "Catherine Legg",
            "Ian H. Witten"
        ],
        "summary": "Wikipedia is a goldmine of information; not just for its many readers, but also for the growing community of researchers who recognize it as a resource of exceptional scale and utility. It represents a vast investment of manual effort and judgment: a huge, constantly evolving tapestry of concepts and relations that is being applied to a host of tasks.   This article provides a comprehensive description of this work. It focuses on research that extracts and makes use of the concepts, relations, facts and descriptions found in Wikipedia, and organizes the work into four broad categories: applying Wikipedia to natural language processing; using it to facilitate information retrieval and information extraction; and as a resource for ontology building. The article addresses how Wikipedia is being used as is, how it is being improved and adapted, and how it is being combined with other structures to create entirely new resources. We identify the research groups and individuals involved, and how their work has developed in the last few years. We provide a comprehensive list of the open-source software they have produced.",
        "published": "2008-09-26T04:47:19Z",
        "link": "http://arxiv.org/abs/0809.4530v2",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.IR",
            "I.2.6; I.2.7; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Achieving compositionality of the stable model semantics for Smodels   programs",
        "authors": [
            "Emilia Oikarinen",
            "Tomi Janhunen"
        ],
        "summary": "In this paper, a Gaifman-Shapiro-style module architecture is tailored to the case of Smodels programs under the stable model semantics. The composition of Smodels program modules is suitably limited by module conditions which ensure the compatibility of the module system with stable models. Hence the semantics of an entire Smodels program depends directly on stable models assigned to its modules. This result is formalized as a module theorem which truly strengthens Lifschitz and Turner's splitting-set theorem for the class of Smodels programs. To streamline generalizations in the future, the module theorem is first proved for normal programs and then extended to cover Smodels programs using a translation from the latter class of programs to the former class. Moreover, the respective notion of module-level equivalence, namely modular equivalence, is shown to be a proper congruence relation: it is preserved under substitutions of modules that are modularly equivalent. Principles for program decomposition are also addressed. The strongly connected components of the respective dependency graph can be exploited in order to extract a module structure when there is no explicit a priori knowledge about the modules of a program. The paper includes a practical demonstration of tools that have been developed for automated (de)composition of Smodels programs.   To appear in Theory and Practice of Logic Programming.",
        "published": "2008-09-26T10:32:32Z",
        "link": "http://arxiv.org/abs/0809.4582v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A Computational Study on Emotions and Temperament in Multi-Agent Systems",
        "authors": [
            "Luis Paulo Reis",
            "Daria Barteneva",
            "Nuno Lau"
        ],
        "summary": "Recent advances in neurosciences and psychology have provided evidence that affective phenomena pervade intelligence at many levels, being inseparable from the cognitionaction loop. Perception, attention, memory, learning, decisionmaking, adaptation, communication and social interaction are some of the aspects influenced by them. This work draws its inspirations from neurobiology, psychophysics and sociology to approach the problem of building autonomous robots capable of interacting with each other and building strategies based on temperamental decision mechanism. Modelling emotions is a relatively recent focus in artificial intelligence and cognitive modelling. Such models can ideally inform our understanding of human behavior. We may see the development of computational models of emotion as a core research focus that will facilitate advances in the large array of computational systems that model, interpret or influence human behavior. We propose a model based on a scalable, flexible and modular approach to emotion which allows runtime evaluation between emotional quality and performance. The results achieved showed that the strategies based on temperamental decision mechanism strongly influence the system performance and there are evident dependency between emotional state of the agents and their temperamental type, as well as the dependency between the team performance and the temperamental configuration of the team members, and this enable us to conclude that the modular approach to emotional programming based on temperamental theory is the good choice to develop computational mind models for emotional behavioral Multi-Agent systems.",
        "published": "2008-09-27T16:33:34Z",
        "link": "http://arxiv.org/abs/0809.4784v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "cs.RO"
        ]
    },
    {
        "title": "Simulated annealing for weighted polygon packing",
        "authors": [
            "Yi-Chun Xu",
            "Ren-Bin Xiao",
            "Martyn Amos"
        ],
        "summary": "In this paper we present a new algorithm for a layout optimization problem: this concerns the placement of weighted polygons inside a circular container, the two objectives being to minimize imbalance of mass and to minimize the radius of the container. This problem carries real practical significance in industrial applications (such as the design of satellites), as well as being of significant theoretical interest. Previous work has dealt with circular or rectangular objects, but here we deal with the more realistic case where objects may be represented as polygons and the polygons are allowed to rotate. We present a solution based on simulated annealing and first test it on instances with known optima. Our results show that the algorithm obtains container radii that are close to optimal. We also compare our method with existing algorithms for the (special) rectangular case. Experimental results show that our approach out-performs these methods in terms of solution quality.",
        "published": "2008-09-29T16:22:28Z",
        "link": "http://arxiv.org/abs/0809.5005v1",
        "categories": [
            "cs.CG",
            "cs.AI"
        ]
    },
    {
        "title": "Determining the Unithood of Word Sequences using a Probabilistic   Approach",
        "authors": [
            "Wilson Wong",
            "Wei Liu",
            "Mohammed Bennamoun"
        ],
        "summary": "Most research related to unithood were conducted as part of a larger effort for the determination of termhood. Consequently, novelties are rare in this small sub-field of term extraction. In addition, existing work were mostly empirically motivated and derived. We propose a new probabilistically-derived measure, independent of any influences of termhood, that provides dedicated measures to gather linguistic evidence from parsed text and statistical evidence from Google search engine for the measurement of unithood. Our comparative study using 1,825 test cases against an existing empirically-derived function revealed an improvement in terms of precision, recall and accuracy.",
        "published": "2008-10-01T12:49:24Z",
        "link": "http://arxiv.org/abs/0810.0139v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Determining the Unithood of Word Sequences using Mutual Information and   Independence Measure",
        "authors": [
            "Wilson Wong",
            "Wei Liu",
            "Mohammed Bennamoun"
        ],
        "summary": "Most works related to unithood were conducted as part of a larger effort for the determination of termhood. Consequently, the number of independent research that study the notion of unithood and produce dedicated techniques for measuring unithood is extremely small. We propose a new approach, independent of any influences of termhood, that provides dedicated measures to gather linguistic evidence from parsed text and statistical evidence from Google search engine for the measurement of unithood. Our evaluations revealed a precision and recall of 98.68% and 91.82% respectively with an accuracy at 95.42% in measuring the unithood of 1005 test cases.",
        "published": "2008-10-01T13:00:19Z",
        "link": "http://arxiv.org/abs/0810.0156v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Enhanced Integrated Scoring for Cleaning Dirty Texts",
        "authors": [
            "Wilson Wong",
            "Wei Liu",
            "Mohammed Bennamoun"
        ],
        "summary": "An increasing number of approaches for ontology engineering from text are gearing towards the use of online sources such as company intranet and the World Wide Web. Despite such rise, not much work can be found in aspects of preprocessing and cleaning dirty texts from online sources. This paper presents an enhancement of an Integrated Scoring for Spelling error correction, Abbreviation expansion and Case restoration (ISSAC). ISSAC is implemented as part of a text preprocessing phase in an ontology engineering system. New evaluations performed on the enhanced ISSAC using 700 chat records reveal an improved accuracy of 98% as compared to 96.5% and 71% based on the use of only basic ISSAC and of Aspell, respectively.",
        "published": "2008-10-02T03:42:12Z",
        "link": "http://arxiv.org/abs/0810.0332v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Three New Complexity Results for Resource Allocation Problems",
        "authors": [
            "Bart de Keijzer"
        ],
        "summary": "We prove the following results for task allocation of indivisible resources:   - The problem of finding a leximin-maximal resource allocation is in P if the agents have max-utility functions and atomic demands.   - Deciding whether a resource allocation is Pareto-optimal is coNP-complete for agents with (1-)additive utility functions.   - Deciding whether there exists a Pareto-optimal and envy-free resource allocation is Sigma_2^p-complete for agents with (1-)additive utility functions.",
        "published": "2008-10-02T20:32:52Z",
        "link": "http://arxiv.org/abs/0810.0532v2",
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.CC",
            "cs.GT"
        ]
    },
    {
        "title": "A New Upper Bound on the Capacity of a Class of Primitive Relay Channels",
        "authors": [
            "Ravi Tandon",
            "Sennur Ulukus"
        ],
        "summary": "We obtain a new upper bound on the capacity of a class of discrete memoryless relay channels. For this class of relay channels, the relay observes an i.i.d. sequence $T$, which is independent of the channel input $X$. The channel is described by a set of probability transition functions $p(y|x,t)$ for all $(x,t,y)\\in \\mathcal{X}\\times \\mathcal{T}\\times \\mathcal{Y}$. Furthermore, a noiseless link of finite capacity $R_{0}$ exists from the relay to the receiver. Although the capacity for these channels is not known in general, the capacity of a subclass of these channels, namely when $T=g(X,Y)$, for some deterministic function $g$, was obtained in [1] and it was shown to be equal to the cut-set bound. Another instance where the capacity was obtained was in [2], where the channel output $Y$ can be written as $Y=X\\oplus Z$, where $\\oplus$ denotes modulo-$m$ addition, $Z$ is independent of $X$, $|\\mathcal{X}|=|\\mathcal{Y}|=m$, and $T$ is some stochastic function of $Z$. The compress-and-forward (CAF) achievability scheme [3] was shown to be capacity achieving in both cases.   Using our upper bound we recover the capacity results of [1] and [2]. We also obtain the capacity of a class of channels which does not fall into either of the classes studied in [1] and [2]. For this class of channels, CAF scheme is shown to be optimal but capacity is strictly less than the cut-set bound for certain values of $R_{0}$. We also evaluate our outer bound for a particular relay channel with binary multiplicative states and binary additive noise for which the channel is given as $Y=TX+N$. We show that our upper bound is strictly better than the cut-set upper bound for certain values of $R_{0}$ but it lies strictly above the rates yielded by the CAF achievability scheme.",
        "published": "2008-10-04T05:37:45Z",
        "link": "http://arxiv.org/abs/0810.0747v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT",
            "H.1.1"
        ]
    },
    {
        "title": "On-the-fly Macros",
        "authors": [
            "Hubie Chen",
            "Omer Gimenez"
        ],
        "summary": "We present a domain-independent algorithm that computes macros in a novel way. Our algorithm computes macros \"on-the-fly\" for a given set of states and does not require previously learned or inferred information, nor prior domain knowledge. The algorithm is used to define new domain-independent tractable classes of classical planning that are proved to include \\emph{Blocksworld-arm} and \\emph{Towers of Hanoi}.",
        "published": "2008-10-07T13:10:26Z",
        "link": "http://arxiv.org/abs/0810.1186v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A global physician-oriented medical information system",
        "authors": [
            "Axel Boldt",
            "Michael Janich"
        ],
        "summary": "We propose to improve medical decision making and reduce global health care costs by employing a free Internet-based medical information system with two main target groups: practicing physicians and medical researchers. After acquiring patients' consent, physicians enter medical histories, physiological data and symptoms or disorders into the system; an integrated expert system can then assist in diagnosis and statistical software provides a list of the most promising treatment options and medications, tailored to the patient. Physicians later enter information about the outcomes of the chosen treatments, data the system uses to optimize future treatment recommendations. Medical researchers can analyze the aggregate data to compare various drugs or treatments in defined patient populations on a large scale.",
        "published": "2008-10-11T02:01:45Z",
        "link": "http://arxiv.org/abs/0810.1991v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "Modeling of Social Transitions Using Intelligent Systems",
        "authors": [
            "Hamed Owladeghaffari",
            "Witold Pedrycz",
            "Mostafa Sharifzadeh"
        ],
        "summary": "In this study, we reproduce two new hybrid intelligent systems, involve three prominent intelligent computing and approximate reasoning methods: Self Organizing feature Map (SOM), Neruo-Fuzzy Inference System and Rough Set Theory (RST),called SONFIS and SORST. We show how our algorithms can be construed as a linkage of government-society interactions, where government catches various states of behaviors: solid (absolute) or flexible. So, transition of society, by changing of connectivity parameters (noise) from order to disorder is inferred.",
        "published": "2008-10-11T19:09:22Z",
        "link": "http://arxiv.org/abs/0810.2046v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Visualization Optimization : Application to the RoboCup Rescue Domain",
        "authors": [
            "Pedro Miguel Moreira",
            "Luís Paulo Reis",
            "António Augusto de Sousa"
        ],
        "summary": "In this paper we demonstrate the use of intelligent optimization methodologies on the visualization optimization of virtual / simulated environments. The problem of automatic selection of an optimized set of views, which better describes an on-going simulation over a virtual environment is addressed in the context of the RoboCup Rescue Simulation domain. A generic architecture for optimization is proposed and described. We outline the possible extensions of this architecture and argue on how several problems within the fields of Interactive Rendering and Visualization can benefit from it.",
        "published": "2008-10-13T12:53:57Z",
        "link": "http://arxiv.org/abs/0810.2021v1",
        "categories": [
            "cs.GR",
            "cs.AI",
            "I.3.7; I.2.8"
        ]
    },
    {
        "title": "Non-Negative Matrix Factorization, Convexity and Isometry",
        "authors": [
            "Nikolaos Vasiloglou",
            "Alexander G. Gray",
            "David V. Anderson"
        ],
        "summary": "In this paper we explore avenues for improving the reliability of dimensionality reduction methods such as Non-Negative Matrix Factorization (NMF) as interpretive exploratory data analysis tools. We first explore the difficulties of the optimization problem underlying NMF, showing for the first time that non-trivial NMF solutions always exist and that the optimization problem is actually convex, by using the theory of Completely Positive Factorization. We subsequently explore four novel approaches to finding globally-optimal NMF solutions using various ideas from convex optimization. We then develop a new method, isometric NMF (isoNMF), which preserves non-negativity while also providing an isometric embedding, simultaneously achieving two properties which are helpful for interpretation. Though it results in a more difficult optimization problem, we show experimentally that the resulting method is scalable and even achieves more compact spectra than standard NMF.",
        "published": "2008-10-13T20:43:24Z",
        "link": "http://arxiv.org/abs/0810.2311v2",
        "categories": [
            "cs.AI",
            "cs.CV"
        ]
    },
    {
        "title": "On combinations of local theory extensions",
        "authors": [
            "Viorica Sofronie-Stokkermans"
        ],
        "summary": "In this paper we study possibilities of efficient reasoning in combinations of theories over possibly non-disjoint signatures. We first present a class of theory extensions (called local extensions) in which hierarchical reasoning is possible, and give several examples from computer science and mathematics in which such extensions occur in a natural way. We then identify situations in which combinations of local extensions of a theory are again local extensions of that theory. We thus obtain criteria both for recognizing wider classes of local theory extensions, and for modular reasoning in combinations of theories over non-disjoint signatures.",
        "published": "2008-10-15T10:26:02Z",
        "link": "http://arxiv.org/abs/0810.2653v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "A comparison of the notions of optimality in soft constraints and   graphical games",
        "authors": [
            "Krzysztof R. Apt",
            "Francesca Rossi",
            "K. Brent Venable"
        ],
        "summary": "The notion of optimality naturally arises in many areas of applied mathematics and computer science concerned with decision making. Here we consider this notion in the context of two formalisms used for different purposes and in different research areas: graphical games and soft constraints. We relate the notion of optimality used in the area of soft constraint satisfaction problems (SCSPs) to that used in graphical games, showing that for a large class of SCSPs that includes weighted constraints every optimal solution corresponds to a Nash equilibrium that is also a Pareto efficient joint strategy.",
        "published": "2008-10-16T07:53:50Z",
        "link": "http://arxiv.org/abs/0810.2861v1",
        "categories": [
            "cs.AI",
            "cs.GT",
            "I.2.11; D.3.3"
        ]
    },
    {
        "title": "Combining Semantic Wikis and Controlled Natural Language",
        "authors": [
            "Tobias Kuhn"
        ],
        "summary": "We demonstrate AceWiki that is a semantic wiki using the controlled natural language Attempto Controlled English (ACE). The goal is to enable easy creation and modification of ontologies through the web. Texts in ACE can automatically be translated into first-order logic and other languages, for example OWL. Previous evaluation showed that ordinary people are able to use AceWiki without being instructed.",
        "published": "2008-10-17T07:19:39Z",
        "link": "http://arxiv.org/abs/0810.3076v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "H.5.2; I.2.4"
        ]
    },
    {
        "title": "On the Complexity of Core, Kernel, and Bargaining Set",
        "authors": [
            "Gianluigi Greco",
            "Enrico Malizia",
            "Luigi Palopoli",
            "Francesco Scarcello"
        ],
        "summary": "Coalitional games are mathematical models suited to analyze scenarios where players can collaborate by forming coalitions in order to obtain higher worths than by acting in isolation. A fundamental problem for coalitional games is to single out the most desirable outcomes in terms of appropriate notions of worth distributions, which are usually called solution concepts. Motivated by the fact that decisions taken by realistic players cannot involve unbounded resources, recent computer science literature reconsidered the definition of such concepts by advocating the relevance of assessing the amount of resources needed for their computation in terms of their computational complexity. By following this avenue of research, the paper provides a complete picture of the complexity issues arising with three prominent solution concepts for coalitional games with transferable utility, namely, the core, the kernel, and the bargaining set, whenever the game worth-function is represented in some reasonable compact form (otherwise, if the worths of all coalitions are explicitly listed, the input sizes are so large that complexity problems are---artificially---trivial). The starting investigation point is the setting of graph games, about which various open questions were stated in the literature. The paper gives an answer to these questions, and in addition provides new insights on the setting, by characterizing the computational complexity of the three concepts in some relevant generalizations and specializations.",
        "published": "2008-10-17T11:53:30Z",
        "link": "http://arxiv.org/abs/0810.3136v3",
        "categories": [
            "cs.GT",
            "cs.AI",
            "cs.CC",
            "F.2; J.4"
        ]
    },
    {
        "title": "Quantum robot: structure, algorithms and applications",
        "authors": [
            "Daoyi Dong",
            "Chunlin Chen",
            "Chenbin Zhang",
            "Zonghai Chen"
        ],
        "summary": "This paper has been withdrawn.",
        "published": "2008-10-18T01:18:03Z",
        "link": "http://arxiv.org/abs/0810.3283v2",
        "categories": [
            "cs.RO",
            "cs.AI",
            "quant-ph"
        ]
    },
    {
        "title": "The many faces of optimism - Extended version",
        "authors": [
            "István Szita",
            "András Lőrincz"
        ],
        "summary": "The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learning. \"Optimism in the face of uncertainty\" and model building play central roles in advanced exploration methods. Here, we integrate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in polynomial time, and give experimental evidence that it is robust and efficient compared to its ascendants.",
        "published": "2008-10-20T02:09:16Z",
        "link": "http://arxiv.org/abs/0810.3451v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LG"
        ]
    },
    {
        "title": "Social Learning Methods in Board Games",
        "authors": [
            "Vukosi N. Marivate",
            "Tshilidzi Marwala"
        ],
        "summary": "This paper discusses the effects of social learning in training of game playing agents. The training of agents in a social context instead of a self-play environment is investigated. Agents that use the reinforcement learning algorithms are trained in social settings. This mimics the way in which players of board games such as scrabble and chess mentor each other in their clubs. A Round Robin tournament and a modified Swiss tournament setting are used for the training. The agents trained using social settings are compared to self play agents and results indicate that more robust agents emerge from the social training setting. Higher state space games can benefit from such settings as diverse set of agents will have multiple strategies that increase the chances of obtaining more experienced players at the end of training. The Social Learning trained agents exhibit better playing experience than self play agents. The modified Swiss playing style spawns a larger number of better playing agents as the population size increases.",
        "published": "2008-10-20T07:04:30Z",
        "link": "http://arxiv.org/abs/0810.3474v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "The use of entropy to measure structural diversity",
        "authors": [
            "L. Masisi",
            "V. Nelwamondo",
            "T. Marwala"
        ],
        "summary": "In this paper entropy based methods are compared and used to measure structural diversity of an ensemble of 21 classifiers. This measure is mostly applied in ecology, whereby species counts are used as a measure of diversity. The measures used were Shannon entropy, Simpsons and the Berger Parker diversity indexes. As the diversity indexes increased so did the accuracy of the ensemble. An ensemble dominated by classifiers with the same structure produced poor accuracy. Uncertainty rule from information theory was also used to further define diversity. Genetic algorithms were used to find the optimal ensemble by using the diversity indices as the cost function. The method of voting was used to aggregate the decisions.",
        "published": "2008-10-20T11:09:15Z",
        "link": "http://arxiv.org/abs/0810.3525v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "q-bio.QM"
        ]
    },
    {
        "title": "A Minimum Relative Entropy Principle for Learning and Acting",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun"
        ],
        "summary": "This paper proposes a method to construct an adaptive agent that is universal with respect to a given class of experts, where each expert is an agent that has been designed specifically for a particular environment. This adaptive control problem is formalized as the problem of minimizing the relative entropy of the adaptive agent from the expert that is most suitable for the unknown environment. If the agent is a passive observer, then the optimal solution is the well-known Bayesian predictor. However, if the agent is active, then its past actions need to be treated as causal interventions on the I/O stream rather than normal probability conditions. Here it is shown that the solution to this new variational problem is given by a stochastic controller called the Bayesian control rule, which implements adaptive behavior as a mixture of experts. Furthermore, it is shown that under mild assumptions, the Bayesian control rule converges to the control law of the most suitable expert.",
        "published": "2008-10-20T16:47:47Z",
        "link": "http://arxiv.org/abs/0810.3605v3",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Quantum reinforcement learning",
        "authors": [
            "Daoyi Dong",
            "Chunlin Chen",
            "Hanxiong Li",
            "Tzyh-Jong Tarn"
        ],
        "summary": "The key approaches for machine learning, especially learning in unknown probabilistic environments are new representations and computation mechanisms. In this paper, a novel quantum reinforcement learning (QRL) method is proposed by combining quantum theory and reinforcement learning (RL). Inspired by the state superposition principle and quantum parallelism, a framework of value updating algorithm is introduced. The state (action) in traditional RL is identified as the eigen state (eigen action) in QRL. The state (action) set can be represented with a quantum superposition state and the eigen state (eigen action) can be obtained by randomly observing the simulated quantum state according to the collapse postulate of quantum measurement. The probability of the eigen action is determined by the probability amplitude, which is parallelly updated according to rewards. Some related characteristics of QRL such as convergence, optimality and balancing between exploration and exploitation are also analyzed, which shows that this approach makes a good tradeoff between exploration and exploitation using the probability amplitude and can speed up learning through the quantum parallelism. To evaluate the performance and practicability of QRL, several simulated experiments are given and the results demonstrate the effectiveness and superiority of QRL algorithm for some complex problems. The present work is also an effective exploration on the application of quantum computation to artificial intelligence.",
        "published": "2008-10-21T13:38:33Z",
        "link": "http://arxiv.org/abs/0810.3828v1",
        "categories": [
            "quant-ph",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Relationship between Diversity and Perfomance of Multiple Classifiers   for Decision Support",
        "authors": [
            "R. Musehane",
            "F. Netshiongolwe",
            "F. V. Nelwamondo",
            "L. Masisi",
            "T. Marwala"
        ],
        "summary": "The paper presents the investigation and implementation of the relationship between diversity and the performance of multiple classifiers on classification accuracy. The study is critical as to build classifiers that are strong and can generalize better. The parameters of the neural network within the committee were varied to induce diversity; hence structural diversity is the focus for this study. The hidden nodes and the activation function are the parameters that were varied. The diversity measures that were adopted from ecology such as Shannon and Simpson were used to quantify diversity. Genetic algorithm is used to find the optimal ensemble by using the accuracy as the cost function. The results observed shows that there is a relationship between structural diversity and accuracy. It is observed that the classification accuracy of an ensemble increases as the diversity increases. There was an increase of 3%-6% in the classification accuracy.",
        "published": "2008-10-21T15:42:16Z",
        "link": "http://arxiv.org/abs/0810.3865v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "On Granular Knowledge Structures",
        "authors": [
            "Yi Zeng",
            "Ning Zhong"
        ],
        "summary": "Knowledge plays a central role in human and artificial intelligence. One of the key characteristics of knowledge is its structured organization. Knowledge can be and should be presented in multiple levels and multiple views to meet people's needs in different levels of granularities and from different perspectives. In this paper, we stand on the view point of granular computing and provide our understanding on multi-level and multi-view of knowledge through granular knowledge structures (GKS). Representation of granular knowledge structures, operations for building granular knowledge structures and how to use them are investigated. As an illustration, we provide some examples through results from an analysis of proceeding papers. Results show that granular knowledge structures could help users get better understanding of the knowledge source from set theoretical, logical and visual point of views. One may consider using them to meet specific needs or solve certain kinds of problems.",
        "published": "2008-10-26T07:17:42Z",
        "link": "http://arxiv.org/abs/0810.4668v1",
        "categories": [
            "cs.AI",
            "cs.DL"
        ]
    },
    {
        "title": "A Novel Clustering Algorithm Based on a Modified Model of Random Walk",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "summary": "We introduce a modified model of random walk, and then develop two novel clustering algorithms based on it. In the algorithms, each data point in a dataset is considered as a particle which can move at random in space according to the preset rules in the modified model. Further, this data point may be also viewed as a local control subsystem, in which the controller adjusts its transition probability vector in terms of the feedbacks of all data points, and then its transition direction is identified by an event-generating function. Finally, the positions of all data points are updated. As they move in space, data points collect gradually and some separating parts emerge among them automatically. As a consequence, data points that belong to the same class are located at a same position, whereas those that belong to different classes are away from one another. Moreover, the experimental results have demonstrated that data points in the test datasets are clustered reasonably and efficiently, and the comparison with other algorithms also provides an indication of the effectiveness of the proposed algorithms.",
        "published": "2008-10-30T13:26:31Z",
        "link": "http://arxiv.org/abs/0810.5484v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Temporal Difference Updating without a Learning Rate",
        "authors": [
            "Marcus Hutter",
            "Shane Legg"
        ],
        "summary": "We derive an equation for temporal difference learning from statistical principles. Specifically, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(lambda), however it lacks the parameter alpha that specifies the learning rate. In the place of this free parameter there is now an equation for the learning rate that is specific to each state transition. We experimentally test this new learning rule against TD(lambda) and find that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins' Q(lambda) and Sarsa(lambda) and find that it again offers superior performance without a learning rate parameter.",
        "published": "2008-10-31T07:15:01Z",
        "link": "http://arxiv.org/abs/0810.5631v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "On the Possibility of Learning in Reactive Environments with Arbitrary   Dependence",
        "authors": [
            "Daniil Ryabko",
            "Marcus Hutter"
        ],
        "summary": "We address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions, i.e. environments more general than (PO)MDPs. The task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments. We find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class. We analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields, such as Markov Decision Processes and mixing conditions.",
        "published": "2008-10-31T07:58:31Z",
        "link": "http://arxiv.org/abs/0810.5636v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Conditional Independence Implication Problem: A Lattice-Theoretic   Approach",
        "authors": [
            "Mathias Niepert",
            "Dirk Van Gucht",
            "Marc Gyssens"
        ],
        "summary": "A lattice-theoretic framework is introduced that permits the study of the conditional independence (CI) implication problem relative to the class of discrete probability measures. Semi-lattices are associated with CI statements and a finite, sound and complete inference system relative to semi-lattice inclusions is presented. This system is shown to be (1) sound and complete for saturated CI statements, (2) complete for general CI statements, and (3) sound and complete for stable CI statements. These results yield a criterion that can be used to falsify instances of the implication problem and several heuristics are derived that approximate this \"lattice-exclusion\" criterion in polynomial time. Finally, we provide experimental results that relate our work to results obtained from other existing inference algorithms.",
        "published": "2008-10-31T15:52:27Z",
        "link": "http://arxiv.org/abs/0810.5717v1",
        "categories": [
            "cs.AI",
            "cs.DM"
        ]
    },
    {
        "title": "A computational model of affects",
        "authors": [
            "Mika Turkia"
        ],
        "summary": "This article provides a simple logical structure, in which affective concepts (i.e. concepts related to emotions and feelings) can be defined. The set of affects defined is similar to the set of emotions covered in the OCC model (Ortony A., Collins A., and Clore G. L.: The Cognitive Structure of Emotions. Cambridge University Press, 1988), but the model presented in this article is fully computationally defined.",
        "published": "2008-11-02T03:38:59Z",
        "link": "http://arxiv.org/abs/0811.0123v2",
        "categories": [
            "cs.AI",
            "cs.MA",
            "I.2.11; J.4"
        ]
    },
    {
        "title": "Balancing Exploration and Exploitation by an Elitist Ant System with   Exponential Pheromone Deposition Rule",
        "authors": [
            "Ayan Acharya",
            "Deepyaman Maiti",
            "Aritra Banerjee",
            "Amit Konar"
        ],
        "summary": "The paper presents an exponential pheromone deposition rule to modify the basic ant system algorithm which employs constant deposition rule. A stability analysis using differential equation is carried out to find out the values of parameters that make the ant system dynamics stable for both kinds of deposition rule. A roadmap of connected cities is chosen as the problem environment where the shortest route between two given cities is required to be discovered. Simulations performed with both forms of deposition approach using Elitist Ant System model reveal that the exponential deposition approach outperforms the classical one by a large extent. Exhaustive experiments are also carried out to find out the optimum setting of different controlling parameters for exponential deposition approach and an empirical relationship between the major controlling parameters of the algorithm and some features of problem environment.",
        "published": "2008-11-02T06:07:34Z",
        "link": "http://arxiv.org/abs/0811.0131v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A Novel Parser Design Algorithm Based on Artificial Ants",
        "authors": [
            "Deepyaman Maiti",
            "Ayan Acharya",
            "Amit Konar",
            "Janarthanan Ramadoss"
        ],
        "summary": "This article presents a unique design for a parser using the Ant Colony Optimization algorithm. The paper implements the intuitive thought process of human mind through the activities of artificial ants. The scheme presented here uses a bottom-up approach and the parsing program can directly use ambiguous or redundant grammars. We allocate a node corresponding to each production rule present in the given grammar. Each node is connected to all other nodes (representing other production rules), thereby establishing a completely connected graph susceptible to the movement of artificial ants. Each ant tries to modify this sentential form by the production rule present in the node and upgrades its position until the sentential form reduces to the start symbol S. Successful ants deposit pheromone on the links that they have traversed through. Eventually, the optimum path is discovered by the links carrying maximum amount of pheromone concentration. The design is simple, versatile, robust and effective and obviates the calculation of the above mentioned sets and precedence relation tables. Further advantages of our scheme lie in i) ascertaining whether a given string belongs to the language represented by the grammar, and ii) finding out the shortest possible path from the given string to the start symbol S in case multiple routes exist.",
        "published": "2008-11-02T06:22:55Z",
        "link": "http://arxiv.org/abs/0811.0134v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Extension of Max-Min Ant System with Exponential Pheromone Deposition   Rule",
        "authors": [
            "Ayan Acharya",
            "Deepyaman Maiti",
            "Aritra Banerjee",
            "R. Janarthanan",
            "Amit Konar"
        ],
        "summary": "The paper presents an exponential pheromone deposition approach to improve the performance of classical Ant System algorithm which employs uniform deposition rule. A simplified analysis using differential equations is carried out to study the stability of basic ant system dynamics with both exponential and constant deposition rules. A roadmap of connected cities, where the shortest path between two specified cities are to be found out, is taken as a platform to compare Max-Min Ant System model (an improved and popular model of Ant System algorithm) with exponential and constant deposition rules. Extensive simulations are performed to find the best parameter settings for non-uniform deposition approach and experiments with these parameter settings revealed that the above approach outstripped the traditional one by a large extent in terms of both solution quality and convergence time.",
        "published": "2008-11-02T06:28:50Z",
        "link": "http://arxiv.org/abs/0811.0136v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Effect of Tuned Parameters on a LSA MCQ Answering Model",
        "authors": [
            "Alain Lifchitz",
            "Sandra Jhean-Larose",
            "Guy Denhière"
        ],
        "summary": "This paper presents the current state of a work in progress, whose objective is to better understand the effects of factors that significantly influence the performance of Latent Semantic Analysis (LSA). A difficult task, which consists in answering (French) biology Multiple Choice Questions, is used to test the semantic properties of the truncated singular space and to study the relative influence of main parameters. A dedicated software has been designed to fine tune the LSA semantic space for the Multiple Choice Questions task. With optimal parameters, the performances of our simple model are quite surprisingly equal or superior to those of 7th and 8th grades students. This indicates that semantic spaces were quite good despite their low dimensions and the small sizes of training data sets. Besides, we present an original entropy global weighting of answers' terms of each question of the Multiple Choice Questions which was necessary to achieve the model's success.",
        "published": "2008-11-02T09:21:40Z",
        "link": "http://arxiv.org/abs/0811.0146v3",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ]
    },
    {
        "title": "Edhibou: a Customizable Interface for Decision Support in a Semantic   Portal",
        "authors": [
            "Fadi Badra",
            "Mathieu D'Aquin",
            "Jean Lieber",
            "Thomas Meilender"
        ],
        "summary": "The Semantic Web is becoming more and more a reality, as the required technologies have reached an appropriate level of maturity. However, at this stage, it is important to provide tools facilitating the use and deployment of these technologies by end-users. In this paper, we describe EdHibou, an automatically generated, ontology-based graphical user interface that integrates in a semantic portal. The particularity of EdHibou is that it makes use of OWL reasoning capabilities to provide intelligent features, such as decision support, upon the underlying ontology. We present an application of EdHibou to medical decision support based on a formalization of clinical guidelines in OWL and show how it can be customized thanks to an ontology of graphical components.",
        "published": "2008-11-03T14:49:44Z",
        "link": "http://arxiv.org/abs/0811.0310v1",
        "categories": [
            "cs.AI",
            "cs.HC"
        ]
    },
    {
        "title": "Cooperative interface of a swarm of UAVs",
        "authors": [
            "Sylvie Saget",
            "Francois Legras",
            "Gilles Coppin"
        ],
        "summary": "After presenting the broad context of authority sharing, we outline how introducing more natural interaction in the design of the ground operator interface of UV systems should help in allowing a single operator to manage the complexity of his/her task. Introducing new modalities is one one of the means in the realization of our vision of next- generation GOI. A more fundamental aspect resides in the interaction manager which should help balance the workload of the operator between mission and interaction, notably by applying a multi-strategy approach to generation and interpretation. We intend to apply these principles to the context of the Smaart prototype, and in this perspective, we illustrate how to characterize the workload associated with a particular operational situation.",
        "published": "2008-11-03T16:54:10Z",
        "link": "http://arxiv.org/abs/0811.0335v1",
        "categories": [
            "cs.AI",
            "cs.HC",
            "cs.MA"
        ]
    },
    {
        "title": "Document stream clustering: experimenting an incremental algorithm and   AR-based tools for highlighting dynamic trends",
        "authors": [
            "Alain Lelu",
            "Martine Cadot",
            "Pascal Cuxac"
        ],
        "summary": "We address here two major challenges presented by dynamic data mining: 1) the stability challenge: we have implemented a rigorous incremental density-based clustering algorithm, independent from any initial conditions and ordering of the data-vectors stream, 2) the cognitive challenge: we have implemented a stringent selection process of association rules between clusters at time t-1 and time t for directly generating the main conclusions about the dynamics of a data-stream. We illustrate these points with an application to a two years and 2600 documents scientific information database.",
        "published": "2008-11-03T16:56:51Z",
        "link": "http://arxiv.org/abs/0811.0340v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Embedding Non-Ground Logic Programs into Autoepistemic Logic for   Knowledge Base Combination",
        "authors": [
            "Jos de Bruijn",
            "Thomas Eiter",
            "Axel Polleres",
            "Hans Tompits"
        ],
        "summary": "In the context of the Semantic Web, several approaches to the combination of ontologies, given in terms of theories of classical first-order logic and rule bases, have been proposed. They either cast rules into classical logic or limit the interaction between rules and ontologies. Autoepistemic logic (AEL) is an attractive formalism which allows to overcome these limitations, by serving as a uniform host language to embed ontologies and nonmonotonic logic programs into it. For the latter, so far only the propositional setting has been considered. In this paper, we present three embeddings of normal and three embeddings of disjunctive non-ground logic programs under the stable model semantics into first-order AEL. While the embeddings all correspond with respect to objective ground atoms, differences arise when considering non-atomic formulas and combinations with first-order theories. We compare the embeddings with respect to stable expansions and autoepistemic consequences, considering the embeddings by themselves, as well as combinations with classical theories. Our results reveal differences and correspondences of the embeddings and provide useful guidance in the choice of a particular embedding for knowledge combination.",
        "published": "2008-11-03T18:42:01Z",
        "link": "http://arxiv.org/abs/0811.0359v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Classification dynamique d'un flux documentaire : une évaluation   statique préalable de l'algorithme GERMEN",
        "authors": [
            "Alain Lelu",
            "Pascal Cuxac",
            "Joel Johansson"
        ],
        "summary": "Data-stream clustering is an ever-expanding subdomain of knowledge extraction. Most of the past and present research effort aims at efficient scaling up for the huge data repositories. Our approach focuses on qualitative improvement, mainly for \"weak signals\" detection and precise tracking of topical evolutions in the framework of information watch - though scalability is intrinsically guaranteed in a possibly distributed implementation. Our GERMEN algorithm exhaustively picks up the whole set of density peaks of the data at time t, by identifying the local perturbations induced by the current document vector, such as changing cluster borders, or new/vanishing clusters. Optimality yields from the uniqueness 1) of the density landscape for any value of our zoom parameter, 2) of the cluster allocation operated by our border propagation rule. This results in a rigorous independence from the data presentation ranking or any initialization parameter. We present here as a first step the only assessment of a static view resulting from one year of the CNRS/INIST Pascal database in the field of geotechnics.",
        "published": "2008-11-04T20:42:52Z",
        "link": "http://arxiv.org/abs/0811.0602v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Cognitive OFDM network sensing: a free probability approach",
        "authors": [
            "Romain Couillet",
            "Merouane Debbah"
        ],
        "summary": "In this paper, a practical power detection scheme for OFDM terminals, based on recent free probability tools, is proposed. The objective is for the receiving terminal to determine the transmission power and the number of the surrounding base stations in the network. However, thesystem dimensions of the network model turn energy detection into an under-determined problem. The focus of this paper is then twofold: (i) discuss the maximum amount of information that an OFDM terminal can gather from the surrounding base stations in the network, (ii) propose a practical solution for blind cell detection using the free deconvolution tool. The efficiency of this solution is measured through simulations, which show better performance than the classical power detection methods.",
        "published": "2008-11-05T14:34:23Z",
        "link": "http://arxiv.org/abs/0811.0731v2",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT",
            "math.PR"
        ]
    },
    {
        "title": "A Bayesian Framework for Collaborative Multi-Source Signal Detection",
        "authors": [
            "Romain Couillet",
            "Merouane Debbah"
        ],
        "summary": "This paper introduces a Bayesian framework to detect multiple signals embedded in noisy observations from a sensor array. For various states of knowledge on the communication channel and the noise at the receiving sensors, a marginalization procedure based on recent tools of finite random matrix theory, in conjunction with the maximum entropy principle, is used to compute the hypothesis selection criterion. Quite remarkably, explicit expressions for the Bayesian detector are derived which enable to decide on the presence of signal sources in a noisy wireless environment. The proposed Bayesian detector is shown to outperform the classical power detector when the noise power is known and provides very good performance for limited knowledge on the noise power. Simulations corroborate the theoretical results and quantify the gain achieved using the proposed Bayesian framework.",
        "published": "2008-11-05T16:11:59Z",
        "link": "http://arxiv.org/abs/0811.0764v2",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT",
            "math.PR"
        ]
    },
    {
        "title": "Distributed Constrained Optimization with Semicoordinate Transformations",
        "authors": [
            "William Macready",
            "David Wolpert"
        ],
        "summary": "Recent work has shown how information theory extends conventional full-rationality game theory to allow bounded rational agents. The associated mathematical framework can be used to solve constrained optimization problems. This is done by translating the problem into an iterated game, where each agent controls a different variable of the problem, so that the joint probability distribution across the agents' moves gives an expected value of the objective function. The dynamics of the agents is designed to minimize a Lagrangian function of that joint distribution. Here we illustrate how the updating of the Lagrange parameters in the Lagrangian is a form of automated annealing, which focuses the joint distribution more and more tightly about the joint moves that optimize the objective function. We then investigate the use of ``semicoordinate'' variable transformations. These separate the joint state of the agents from the variables of the optimization problem, with the two connected by an onto mapping. We present experiments illustrating the ability of such transformations to facilitate optimization. We focus on the special kind of transformation in which the statistically independent states of the agents induces a mixture distribution over the optimization variables. Computer experiment illustrate this for $k$-sat constraint satisfaction problems and for unconstrained minimization of $NK$ functions.",
        "published": "2008-11-05T21:35:57Z",
        "link": "http://arxiv.org/abs/0811.0823v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Étude longitudinale d'une procédure de modélisation de   connaissances en matière de gestion du territoire agricole",
        "authors": [
            "Florence Le Ber",
            "Christian Brassac"
        ],
        "summary": "This paper gives an introduction to this issue, and presents the framework and the main steps of the Rosa project. Four teams of researchers, agronomists, computer scientists, psychologists and linguists were involved during five years within this project that aimed at the development of a knowledge based system. The purpose of the Rosa system is the modelling and the comparison of farm spatial organizations. It relies on a formalization of agronomical knowledge and thus induces a joint knowledge building process involving both the agronomists and the computer scientists. The paper describes the steps of the modelling process as well as the filming procedures set up by the psychologists and linguists in order to make explicit and to analyze the underlying knowledge building process.",
        "published": "2008-11-06T13:35:12Z",
        "link": "http://arxiv.org/abs/0811.0942v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Mining Complex Hydrobiological Data with Galois Lattices",
        "authors": [
            "Aurélie Bertaux",
            "AGNès Braud",
            "Florence Le Ber"
        ],
        "summary": "We have used Galois lattices for mining hydrobiological data. These data are about macrophytes, that are macroscopic plants living in water bodies. These plants are characterized by several biological traits, that own several modalities. Our aim is to cluster the plants according to their common traits and modalities and to find out the relations between traits. Galois lattices are efficient methods for such an aim, but apply on binary data. In this article, we detail a few approaches we used to transform complex hydrobiological data into binary data and compare the first results obtained thanks to Galois lattices.",
        "published": "2008-11-06T15:34:18Z",
        "link": "http://arxiv.org/abs/0811.0971v1",
        "categories": [
            "cs.AI",
            "q-bio.QM"
        ]
    },
    {
        "title": "Modeling Social Annotation: a Bayesian Approach",
        "authors": [
            "Anon Plangprasopchok",
            "Kristina Lerman"
        ],
        "summary": "Collaborative tagging systems, such as Delicious, CiteULike, and others, allow users to annotate resources, e.g., Web pages or scientific papers, with descriptive labels called tags. The social annotations contributed by thousands of users, can potentially be used to infer categorical knowledge, classify documents or recommend new relevant information. Traditional text inference methods do not make best use of social annotation, since they do not take into account variations in individual users' perspectives and vocabulary. In a previous work, we introduced a simple probabilistic model that takes interests of individual annotators into account in order to find hidden topics of annotated resources. Unfortunately, that approach had one major shortcoming: the number of topics and interests must be specified a priori. To address this drawback, we extend the model to a fully Bayesian framework, which offers a way to automatically estimate these numbers. In particular, the model allows the number of interests and topics to change as suggested by the structure of the data. We evaluate the proposed model in detail on the synthetic and real-world data by comparing its performance to Latent Dirichlet Allocation on the topic extraction task. For the latter evaluation, we apply the model to infer topics of Web resources from social annotations obtained from Delicious in order to discover new resources similar to a specified one. Our empirical results demonstrate that the proposed model is a promising method for exploiting social knowledge contained in user-generated annotations.",
        "published": "2008-11-09T05:49:53Z",
        "link": "http://arxiv.org/abs/0811.1319v2",
        "categories": [
            "cs.AI",
            "H.2.8; I.5.1"
        ]
    },
    {
        "title": "Airport Gate Assignment: New Model and Implementation",
        "authors": [
            "Chendong Li"
        ],
        "summary": "Airport gate assignment is of great importance in airport operations. In this paper, we study the Airport Gate Assignment Problem (AGAP), propose a new model and implement the model with Optimization Programming language (OPL). With the objective to minimize the number of conflicts of any two adjacent aircrafts assigned to the same gate, we build a mathematical model with logical constraints and the binary constraints, which can provide an efficient evaluation criterion for the Airlines to estimate the current gate assignment. To illustrate the feasibility of the model we construct experiments with the data obtained from Continental Airlines, Houston Gorge Bush Intercontinental Airport IAH, which indicate that our model is both energetic and effective. Moreover, we interpret experimental results, which further demonstrate that our proposed model can provide a powerful tool for airline companies to estimate the efficiency of their current work of gate assignment.",
        "published": "2008-11-11T02:33:30Z",
        "link": "http://arxiv.org/abs/0811.1618v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Artificial Intelligence Techniques for Steam Generator Modelling",
        "authors": [
            "Sarah Wright",
            "Tshilidzi Marwala"
        ],
        "summary": "This paper investigates the use of different Artificial Intelligence methods to predict the values of several continuous variables from a Steam Generator. The objective was to determine how the different artificial intelligence methods performed in making predictions on the given dataset. The artificial intelligence methods evaluated were Neural Networks, Support Vector Machines, and Adaptive Neuro-Fuzzy Inference Systems. The types of neural networks investigated were Multi-Layer Perceptions, and Radial Basis Function. Bayesian and committee techniques were applied to these neural networks. Each of the AI methods considered was simulated in Matlab. The results of the simulations showed that all the AI methods were capable of predicting the Steam Generator data reasonably accurately. However, the Adaptive Neuro-Fuzzy Inference system out performed the other methods in terms of accuracy and ease of implementation, while still achieving a fast execution time as well as a reasonable training time.",
        "published": "2008-11-11T14:09:36Z",
        "link": "http://arxiv.org/abs/0811.1711v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Action Theory Evolution",
        "authors": [
            "Ivan Varzinczak"
        ],
        "summary": "Like any other logical theory, domain descriptions in reasoning about actions may evolve, and thus need revision methods to adequately accommodate new information about the behavior of actions. The present work is about changing action domain descriptions in propositional dynamic logic. Its contribution is threefold: first we revisit the semantics of action theory contraction that has been done in previous work, giving more robust operators that express minimal change based on a notion of distance between Kripke-models. Second we give algorithms for syntactical action theory contraction and establish their correctness w.r.t. our semantics. Finally we state postulates for action theory contraction and assess the behavior of our operators w.r.t. them. Moreover, we also address the revision counterpart of action theory change, showing that it benefits from our semantics for contraction.",
        "published": "2008-11-12T12:05:55Z",
        "link": "http://arxiv.org/abs/0811.1878v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "The Expressive Power of Binary Submodular Functions",
        "authors": [
            "Stanislav Zivny",
            "David A. Cohen",
            "Peter G. Jeavons"
        ],
        "summary": "It has previously been an open problem whether all Boolean submodular functions can be decomposed into a sum of binary submodular functions over a possibly larger set of variables. This problem has been considered within several different contexts in computer science, including computer vision, artificial intelligence, and pseudo-Boolean optimisation. Using a connection between the expressive power of valued constraints and certain algebraic properties of functions, we answer this question negatively.   Our results have several corollaries. First, we characterise precisely which submodular functions of arity 4 can be expressed by binary submodular functions. Next, we identify a novel class of submodular functions of arbitrary arities which can be expressed by binary submodular functions, and therefore minimised efficiently using a so-called expressibility reduction to the Min-Cut problem. More importantly, our results imply limitations on this kind of reduction and establish for the first time that it cannot be used in general to minimise arbitrary submodular functions. Finally, we refute a conjecture of Promislow and Young on the structure of the extreme rays of the cone of Boolean submodular functions.",
        "published": "2008-11-12T17:21:08Z",
        "link": "http://arxiv.org/abs/0811.1885v1",
        "categories": [
            "cs.DM",
            "cs.AI",
            "cs.CV",
            "F.2.2; G.2.1; I.2.4; I.4.0; I.5.1"
        ]
    },
    {
        "title": "Modeling Cultural Dynamics",
        "authors": [
            "Liane Gabora"
        ],
        "summary": "EVOC (for EVOlution of Culture) is a computer model of culture that enables us to investigate how various factors such as barriers to cultural diffusion, the presence and choice of leaders, or changes in the ratio of innovation to imitation affect the diversity and effectiveness of ideas. It consists of neural network based agents that invent ideas for actions, and imitate neighbors' actions. The model is based on a theory of culture according to which what evolves through culture is not memes or artifacts, but the internal models of the world that give rise to them, and they evolve not through a Darwinian process of competitive exclusion but a Lamarckian process involving exchange of innovation protocols. EVOC shows an increase in mean fitness of actions over time, and an increase and then decrease in the diversity of actions. Diversity of actions is positively correlated with population size and density, and with barriers between populations. Slowly eroding borders increase fitness without sacrificing diversity by fostering specialization followed by sharing of fit actions. Introducing a leader that broadcasts its actions throughout the population increases the fitness of actions but reduces diversity of actions. Increasing the number of leaders reduces this effect. Efforts are underway to simulate the conditions under which an agent immigrating from one culture to another contributes new ideas while still fitting in.",
        "published": "2008-11-16T03:35:56Z",
        "link": "http://arxiv.org/abs/0811.2551v3",
        "categories": [
            "cs.MA",
            "cs.AI",
            "q-bio.NC"
        ]
    },
    {
        "title": "Exact phase transition of backtrack-free search with implications on the   power of greedy algorithms",
        "authors": [
            "Liang Li",
            "Tian Liu",
            "Ke Xu"
        ],
        "summary": "Backtracking is a basic strategy to solve constraint satisfaction problems (CSPs). A satisfiable CSP instance is backtrack-free if a solution can be found without encountering any dead-end during a backtracking search, implying that the instance is easy to solve. We prove an exact phase transition of backtrack-free search in some random CSPs, namely in Model RB and in Model RD. This is the first time an exact phase transition of backtrack-free search can be identified on some random CSPs. Our technical results also have interesting implications on the power of greedy algorithms, on the width of random hypergraphs and on the exact satisfiability threshold of random CSPs.",
        "published": "2008-11-19T06:33:39Z",
        "link": "http://arxiv.org/abs/0811.3055v1",
        "categories": [
            "cs.AI",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "A Spectral Algorithm for Learning Hidden Markov Models",
        "authors": [
            "Daniel Hsu",
            "Sham M. Kakade",
            "Tong Zhang"
        ],
        "summary": "Hidden Markov Models (HMMs) are one of the most fundamental and widely used statistical tools for modeling discrete time series. In general, learning HMMs from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues. We prove that under a natural separation condition (bounds on the smallest singular value of the HMM parameters), there is an efficient and provably correct algorithm for learning HMMs. The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations---it implicitly depends on this quantity through spectral properties of the underlying HMM. This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language. The algorithm is also simple, employing only a singular value decomposition and matrix multiplications.",
        "published": "2008-11-26T20:22:51Z",
        "link": "http://arxiv.org/abs/0811.4413v6",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Learning Class-Level Bayes Nets for Relational Data",
        "authors": [
            "Oliver Schulte",
            "Hassan Khosravi",
            "Flavia Moser",
            "Martin Ester"
        ],
        "summary": "Many databases store data in relational format, with different types of entities and information about links between the entities. The field of statistical-relational learning (SRL) has developed a number of new statistical models for such data. In this paper we focus on learning class-level or first-order dependencies, which model the general database statistics over attributes of linked objects and links (e.g., the percentage of A grades given in computer science classes). Class-level statistical relationships are important in themselves, and they support applications like policy making, strategic planning, and query optimization. Most current SRL methods find class-level dependencies, but their main task is to support instance-level predictions about the attributes or links of specific entities. We focus only on class-level prediction, and describe algorithms for learning class-level models that are orders of magnitude faster for this task. Our algorithms learn Bayes nets with relational structure, leveraging the efficiency of single-table nonrelational Bayes net learners. An evaluation of our methods on three data sets shows that they are computationally feasible for realistic table sizes, and that the learned structures represent the statistical information in the databases well. After learning compiles the database statistics into a Bayes net, querying these statistics via Bayes net inference is faster than with SQL queries, and does not depend on the size of the database.",
        "published": "2008-11-27T01:02:33Z",
        "link": "http://arxiv.org/abs/0811.4458v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "Probabilistic reasoning with answer sets",
        "authors": [
            "Chitta Baral",
            "Michael Gelfond",
            "Nelson Rushton"
        ],
        "summary": "This paper develops a declarative language, P-log, that combines logical and probabilistic arguments in its reasoning. Answer Set Prolog is used as the logical foundation, while causal Bayes nets serve as a probabilistic foundation. We give several non-trivial examples and illustrate the use of P-log for knowledge representation and updating of knowledge. We argue that our approach to updates is more appealing than existing approaches. We give sufficiency conditions for the coherency of P-log programs and show that Bayes nets can be easily mapped to coherent P-log programs.",
        "published": "2008-12-03T06:36:16Z",
        "link": "http://arxiv.org/abs/0812.0659v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "A Novel Clustering Algorithm Based on Quantum Games",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "summary": "Enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum game with the problem of data clustering, and then develop a quantum-game-based clustering algorithm, in which data points in a dataset are considered as players who can make decisions and implement quantum strategies in quantum games. After each round of a quantum game, each player's expected payoff is calculated. Later, he uses a link-removing-and-rewiring (LRR) function to change his neighbors and adjust the strength of links connecting to them in order to maximize his payoff. Further, algorithms are discussed and analyzed in two cases of strategies, two payoff matrixes and two LRR functions. Consequently, the simulation results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms have fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.",
        "published": "2008-12-03T15:46:03Z",
        "link": "http://arxiv.org/abs/0812.0743v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.GT",
            "cs.MA",
            "cs.NE",
            "quant-ph"
        ]
    },
    {
        "title": "Justifications for Logic Programs under Answer Set Semantics",
        "authors": [
            "Enrico Pontelli",
            "Tran Cao Son",
            "Omar Elkhatib"
        ],
        "summary": "The paper introduces the notion of off-line justification for Answer Set Programming (ASP). Justifications provide a graph-based explanation of the truth value of an atom w.r.t. a given answer set. The paper extends also this notion to provide justification of atoms during the computation of an answer set (on-line justification), and presents an integration of on-line justifications within the computation model of Smodels. Off-line and on-line justifications provide useful tools to enhance understanding of ASP, and they offer a basic data structure to support methodologies and tools for debugging answer set programs. A preliminary implementation has been developed in ASP-PROLOG.   (To appear in Theory and Practice of Logic Programming (TPLP))",
        "published": "2008-12-03T20:10:00Z",
        "link": "http://arxiv.org/abs/0812.0790v1",
        "categories": [
            "cs.AI",
            "cs.PL"
        ]
    },
    {
        "title": "Elementary epistemological features of machine intelligence",
        "authors": [
            "Marko Horvat"
        ],
        "summary": "Theoretical analysis of machine intelligence (MI) is useful for defining a common platform in both theoretical and applied artificial intelligence (AI). The goal of this paper is to set canonical definitions that can assist pragmatic research in both strong and weak AI. Described epistemological features of machine intelligence include relationship between intelligent behavior, intelligent and unintelligent machine characteristics, observable and unobservable entities and classification of intelligence. The paper also establishes algebraic definitions of efficiency and accuracy of MI tests as their quality measure. The last part of the paper addresses the learning process with respect to the traditional epistemology and the epistemology of MI described here. The proposed views on MI positively correlate to the Hegelian monistic epistemology and contribute towards amalgamating idealistic deliberations with the AI theory, particularly in a local frame of reference.",
        "published": "2008-12-04T09:25:37Z",
        "link": "http://arxiv.org/abs/0812.0885v4",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Adaptive Spam Detection Inspired by a Cross-Regulation Model of Immune   Dynamics: A Study of Concept Drift",
        "authors": [
            "Alaa Abi-Haidar",
            "Luis M. Rocha"
        ],
        "summary": "This paper proposes a novel solution to spam detection inspired by a model of the adaptive immune system known as the crossregulation model. We report on the testing of a preliminary algorithm on six e-mail corpora. We also compare our results statically and dynamically with those obtained by the Naive Bayes classifier and another binary classification method we developed previously for biomedical text-mining applications. We show that the cross-regulation model is competitive against those and thus promising as a bio-inspired algorithm for spam detection in particular, and binary classification in general.",
        "published": "2008-12-04T20:40:32Z",
        "link": "http://arxiv.org/abs/0812.1014v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "nlin.AO"
        ]
    },
    {
        "title": "An analysis of a random algorithm for estimating all the matchings",
        "authors": [
            "Jinshan Zhang"
        ],
        "summary": "Counting the number of all the matchings on a bipartite graph has been transformed into calculating the permanent of a matrix obtained from the extended bipartite graph by Yan Huo, and Rasmussen presents a simple approach (RM) to approximate the permanent, which just yields a critical ratio O($n\\omega(n)$) for almost all the 0-1 matrices, provided it's a simple promising practical way to compute this #P-complete problem. In this paper, the performance of this method will be shown when it's applied to compute all the matchings based on that transformation. The critical ratio will be proved to be very large with a certain probability, owning an increasing factor larger than any polynomial of $n$ even in the sense for almost all the 0-1 matrices. Hence, RM fails to work well when counting all the matchings via computing the permanent of the matrix. In other words, we must carefully utilize the known methods of estimating the permanent to count all the matchings through that transformation.",
        "published": "2008-12-05T12:16:53Z",
        "link": "http://arxiv.org/abs/0812.1119v1",
        "categories": [
            "cs.GR",
            "cs.AI",
            "F.2.0"
        ]
    },
    {
        "title": "Emerge-Sort: Converging to Ordered Sequences by Simple Local Operators",
        "authors": [
            "Dimitris Kalles",
            "Alexis Kaporis"
        ],
        "summary": "In this paper we examine sorting on the assumption that we do not know in advance which way to sort a sequence of numbers and we set at work simple local comparison and swap operators whose repeating application ends up in sorted sequences. These are the basic elements of Emerge-Sort, our approach to self-organizing sorting, which we then validate experimentally across a range of samples. Observing an O(n2) run-time behaviour, we note that the n/logn delay coefficient that differentiates Emerge-Sort from the classical comparison based algorithms is an instantiation of the price of anarchy we pay for not imposing a sorting order and for letting that order emerge through the local interactions.",
        "published": "2008-12-05T12:57:00Z",
        "link": "http://arxiv.org/abs/0812.1126v2",
        "categories": [
            "cs.AI",
            "cs.DS"
        ]
    },
    {
        "title": "Logic programs with propositional connectives and aggregates",
        "authors": [
            "Paolo Ferraris"
        ],
        "summary": "Answer set programming (ASP) is a logic programming paradigm that can be used to solve complex combinatorial search problems. Aggregates are an ASP construct that plays an important role in many applications. Defining a satisfactory semantics of aggregates turned out to be a difficult problem, and in this paper we propose a new approach, based on an analogy between aggregates and propositional connectives. First, we extend the definition of an answer set/stable model to cover arbitrary propositional theories; then we define aggregates on top of them both as primitive constructs and as abbreviations for formulas. Our definition of an aggregate combines expressiveness and simplicity, and it inherits many theorems about programs with nested expressions, such as theorems about strong equivalence and splitting.",
        "published": "2008-12-08T11:09:14Z",
        "link": "http://arxiv.org/abs/0812.1462v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Multi-Agent Reinforcement Learning and Genetic Policy Sharing",
        "authors": [
            "Jake Ellowitz"
        ],
        "summary": "The effects of policy sharing between agents in a multi-agent dynamical system has not been studied extensively. I simulate a system of agents optimizing the same task using reinforcement learning, to study the effects of different population densities and policy sharing. I demonstrate that sharing policies decreases the time to reach asymptotic behavior, and results in improved asymptotic behavior.",
        "published": "2008-12-09T16:13:33Z",
        "link": "http://arxiv.org/abs/0812.1599v1",
        "categories": [
            "cs.MA",
            "cs.AI"
        ]
    },
    {
        "title": "Identification of parameters underlying emotions and a classification of   emotions",
        "authors": [
            "N. Arvind Kumar"
        ],
        "summary": "The standard classification of emotions involves categorizing the expression of emotions. In this paper, parameters underlying some emotions are identified and a new classification based on these parameters is suggested.",
        "published": "2008-12-10T19:02:31Z",
        "link": "http://arxiv.org/abs/0812.1843v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Physics of risk and uncertainty in quantum decision making",
        "authors": [
            "V. I. Yukalov",
            "D. Sornette"
        ],
        "summary": "The Quantum Decision Theory, developed recently by the authors, is applied to clarify the role of risk and uncertainty in decision making and in particular in relation to the phenomenon of dynamic inconsistency. By formulating this notion in precise mathematical terms, we distinguish three types of inconsistency: time inconsistency, planning paradox, and inconsistency occurring in some discounting effects. While time inconsistency is well accounted for in classical decision theory, the planning paradox is in contradiction with classical utility theory. It finds a natural explanation in the frame of the Quantum Decision Theory. Different types of discounting effects are analyzed and shown to enjoy a straightforward explanation within the suggested theory. We also introduce a general methodology based on self-similar approximation theory for deriving the evolution equations for the probabilities of future prospects. This provides a novel classification of possible discount factors, which include the previously known cases (exponential or hyperbolic discounting), but also predicts a novel class of discount factors that decay to a strictly positive constant for very large future time horizons. This class may be useful to deal with very long-term discounting situations associated with intergenerational public policy choices, encompassing issues such as global warming and nuclear waste disposal.",
        "published": "2008-12-12T14:46:41Z",
        "link": "http://arxiv.org/abs/0812.2388v2",
        "categories": [
            "physics.soc-ph",
            "cs.AI",
            "quant-ph"
        ]
    },
    {
        "title": "A New Trend in Optimization on Multi Overcomplete Dictionary toward   Inpainting",
        "authors": [
            "SeyyedMajid Valiollahzadeh",
            "Mohammad Nazari",
            "Massoud Babaie-Zadeh",
            "Christian Jutten"
        ],
        "summary": "Recently, great attention was intended toward overcomplete dictionaries and the sparse representations they can provide. In a wide variety of signal processing problems, sparsity serves a crucial property leading to high performance. Inpainting, the process of reconstructing lost or deteriorated parts of images or videos, is an interesting application which can be handled by suitably decomposition of an image through combination of overcomplete dictionaries. This paper addresses a novel technique of such a decomposition and investigate that through inpainting of images. Simulations are presented to demonstrate the validation of our approach.",
        "published": "2008-12-12T15:56:42Z",
        "link": "http://arxiv.org/abs/0812.2405v1",
        "categories": [
            "cs.MM",
            "cs.AI"
        ]
    },
    {
        "title": "Probabilistic SVM/GMM Classifier for Speaker-Independent Vowel   Recognition in Continues Speech",
        "authors": [
            "Mohammad Nazari",
            "Abolghasem Sayadiyan",
            "SeyedMajid Valiollahzadeh"
        ],
        "summary": "In this paper, we discuss the issues in automatic recognition of vowels in Persian language. The present work focuses on new statistical method of recognition of vowels as a basic unit of syllables. First we describe a vowel detection system then briefly discuss how the detected vowels can feed to recognition unit. According to pattern recognition, Support Vector Machines (SVM) as a discriminative classifier and Gaussian mixture model (GMM) as a generative model classifier are two most popular techniques. Current state-ofthe- art systems try to combine them together for achieving more power of classification and improving the performance of the recognition systems. The main idea of the study is to combine probabilistic SVM and traditional GMM pattern classification with some characteristic of speech like band-pass energy to achieve better classification rate. This idea has been analytically formulated and tested on a FarsDat based vowel recognition system. The results show inconceivable increases in recognition accuracy. The tests have been carried out by various proposed vowel recognition algorithms and the results have been compared.",
        "published": "2008-12-12T16:08:04Z",
        "link": "http://arxiv.org/abs/0812.2411v1",
        "categories": [
            "cs.MM",
            "cs.AI"
        ]
    },
    {
        "title": "Pattern Recognition and Memory Mapping using Mirroring Neural Networks",
        "authors": [
            "Dasika Ratna Deepthi",
            "K. Eswaran"
        ],
        "summary": "In this paper, we present a new kind of learning implementation to recognize the patterns using the concept of Mirroring Neural Network (MNN) which can extract information from distinct sensory input patterns and perform pattern recognition tasks. It is also capable of being used as an advanced associative memory wherein image data is associated with voice inputs in an unsupervised manner. Since the architecture is hierarchical and modular it has the potential of being used to devise learning engines of ever increasing complexity.",
        "published": "2008-12-13T09:21:31Z",
        "link": "http://arxiv.org/abs/0812.2535v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Standard Logics Are Valuation-Nonmonotonic",
        "authors": [
            "Mladen Pavicic",
            "Norman D. Megill"
        ],
        "summary": "It has recently been discovered that both quantum and classical propositional logics can be modelled by classes of non-orthomodular and thus non-distributive lattices that properly contain standard orthomodular and Boolean classes, respectively. In this paper we prove that these logics are complete even for those classes of the former lattices from which the standard orthomodular lattices and Boolean algebras are excluded. We also show that neither quantum nor classical computers can be founded on the latter models. It follows that logics are \"valuation-nonmonotonic\" in the sense that their possible models (corresponding to their possible hardware implementations) and the valuations for them drastically change when we add new conditions to their defining conditions. These valuations can even be completely separated by putting them into disjoint lattice classes by a technique presented in the paper.",
        "published": "2008-12-15T00:49:12Z",
        "link": "http://arxiv.org/abs/0812.2702v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "quant-ph"
        ]
    },
    {
        "title": "Prediction of Platinum Prices Using Dynamically Weighted Mixture of   Experts",
        "authors": [
            "Baruch Lubinsky",
            "Bekir Genc",
            "Tshilidzi Marwala"
        ],
        "summary": "Neural networks are powerful tools for classification and regression in static environments. This paper describes a technique for creating an ensemble of neural networks that adapts dynamically to changing conditions. The model separates the input space into four regions and each network is given a weight in each region based on its performance on samples from that region. The ensemble adapts dynamically by constantly adjusting these weights based on the current performance of the networks. The data set used is a collection of financial indicators with the goal of predicting the future platinum price. An ensemble with no weightings does not improve on the naive estimate of no weekly change; our weighting algorithm gives an average percentage error of 63% for twenty weeks of prediction.",
        "published": "2008-12-15T12:35:42Z",
        "link": "http://arxiv.org/abs/0812.2785v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "New parallel programming language design: a bridge between brain models   and multi-core/many-core computers?",
        "authors": [
            "Gheorghe Stefanescu",
            "Camelia Chira"
        ],
        "summary": "The recurrent theme of this paper is that sequences of long temporal patterns as opposed to sequences of simple statements are to be fed into computation devices, being them (new proposed) models for brain activity or multi-core/many-core computers. In such models, parts of these long temporal patterns are already committed while other are predicted. This combination of matching patterns and making predictions appears as a key element in producing intelligent processing in brain models and getting efficient speculative execution on multi-core/many-core computers. A bridge between these far-apart models of computation could be provided by appropriate design of massively parallel, interactive programming languages. Agapia is a recently proposed language of this kind, where user controlled long high-level temporal structures occur at the interaction interfaces of processes. In this paper Agapia is used to link HTMs brain models with TRIPS multi-core/many-core architectures.",
        "published": "2008-12-15T22:55:19Z",
        "link": "http://arxiv.org/abs/0812.2926v1",
        "categories": [
            "cs.PL",
            "cs.AI"
        ]
    },
    {
        "title": "Analyse et structuration automatique des guides de bonnes pratiques   cliniques : essai d'évaluation",
        "authors": [
            "Amanda Bouffier",
            "Thierry Poibeau",
            "Catherine Duclos"
        ],
        "summary": "Health Practice Guideliens are supposed to unify practices and propose recommendations to physicians. This paper describes GemFrame, a system capable of semi-automatically filling an XML template from free texts in the clinical domain. The XML template includes semantic information not explicitly encoded in the text (pairs of conditions and ac-tions/recommendations). Therefore, there is a need to compute the exact scope of condi-tions over text sequences expressing the re-quired actions. We present a system developped for this task. We show that it yields good performance when applied to the analysis of French practice guidelines. We conclude with a precise evaluation of the tool.",
        "published": "2008-12-16T07:49:20Z",
        "link": "http://arxiv.org/abs/0812.2991v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A Computational Model to Disentangle Semantic Information Embedded in   Word Association Norms",
        "authors": [
            "J. Borge",
            "A. Arenas"
        ],
        "summary": "Two well-known databases of semantic relationships between pairs of words used in psycholinguistics, feature-based and association-based, are studied as complex networks. We propose an algorithm to disentangle feature based relationships from free association semantic networks. The algorithm uses the rich topology of the free association semantic network to produce a new set of relationships between words similar to those observed in feature production norms.",
        "published": "2008-12-16T14:24:23Z",
        "link": "http://arxiv.org/abs/0812.3070v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "physics.data-an",
            "physics.soc-ph"
        ]
    },
    {
        "title": "A Growing Self-Organizing Network for Reconstructing Curves and Surfaces",
        "authors": [
            "Marco Piastra"
        ],
        "summary": "Self-organizing networks such as Neural Gas, Growing Neural Gas and many others have been adopted in actual applications for both dimensionality reduction and manifold learning. Typically, in these applications, the structure of the adapted network yields a good estimate of the topology of the unknown subspace from where the input data points are sampled. The approach presented here takes a different perspective, namely by assuming that the input space is a manifold of known dimension. In return, the new type of growing self-organizing network presented gains the ability to adapt itself in way that may guarantee the effective and stable recovery of the exact topological structure of the input manifold.",
        "published": "2008-12-16T15:59:36Z",
        "link": "http://arxiv.org/abs/0812.2969v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Automatic Construction of Lightweight Domain Ontologies for Chemical   Engineering Risk Management",
        "authors": [
            "Wilson Wong",
            "Wei Liu",
            "Saujoe Liaw",
            "Nicoletta Balliu",
            "Hongwei Wu",
            "Moses Tade"
        ],
        "summary": "The need for domain ontologies in mission critical applications such as risk management and hazard identification is becoming more and more pressing. Most research on ontology learning conducted in the academia remains unrealistic for real-world applications. One of the main problems is the dependence on non-incremental, rare knowledge and textual resources, and manually-crafted patterns and rules. This paper reports work in progress aiming to address such undesirable dependencies during ontology construction. Initial experiments using a working prototype of the system revealed promising potentials in automatically constructing high-quality domain ontologies using real-world texts.",
        "published": "2008-12-18T08:58:52Z",
        "link": "http://arxiv.org/abs/0812.3478v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A New Method for Knowledge Representation in Expert System's (XMLKR)",
        "authors": [
            "Mehdi Bahrami"
        ],
        "summary": "Knowledge representation it is an essential section of a Expert Systems, Because in this section we have a framework to establish an expert system then we can modeling and use by this to design an expert system. Many method it is exist for knowledge representation but each method have problems, in this paper we introduce a new method of object oriented by XML language as XMLKR to knowledge representation, and we want to discuss advantage and disadvantage of this method.",
        "published": "2008-12-18T20:39:48Z",
        "link": "http://arxiv.org/abs/0812.3648v1",
        "categories": [
            "cs.DC",
            "cs.AI"
        ]
    },
    {
        "title": "The Offset Tree for Learning with Partial Labels",
        "authors": [
            "Alina Beygelzimer",
            "John Langford"
        ],
        "summary": "We present an algorithm, called the Offset Tree, for learning to make decisions in situations where the payoff of only one choice is observed, rather than all choices. The algorithm reduces this setting to binary classification, allowing one to reuse of any existing, fully supervised binary classification algorithm in this partial information setting. We show that the Offset Tree is an optimal reduction to binary classification. In particular, it has regret at most $(k-1)$ times the regret of the binary classifier it uses (where $k$ is the number of choices), and no reduction to binary classification can do better. This reduction is also computationally optimal, both at training and test time, requiring just $O(\\log_2 k)$ work to train on an example or make a prediction.   Experiments with the Offset Tree show that it generally performs better than several alternative approaches.",
        "published": "2008-12-21T17:45:27Z",
        "link": "http://arxiv.org/abs/0812.4044v3",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Finding Still Lifes with Memetic/Exact Hybrid Algorithms",
        "authors": [
            "Jose E. Gallardo",
            "Carlos Cotta",
            "Antonio J. Fernandez"
        ],
        "summary": "The maximum density still life problem (MDSLP) is a hard constraint optimization problem based on Conway's game of life. It is a prime example of weighted constrained optimization problem that has been recently tackled in the constraint-programming community. Bucket elimination (BE) is a complete technique commonly used to solve this kind of constraint satisfaction problem. When the memory required to apply BE is too high, a heuristic method based on it (denominated mini-buckets) can be used to calculate bounds for the optimal solution. Nevertheless, the curse of dimensionality makes these techniques unpractical for large size problems. In response to this situation, we present a memetic algorithm for the MDSLP in which BE is used as a mechanism for recombining solutions, providing the best possible child from the parental set. Subsequently, a multi-level model in which this exact/metaheuristic hybrid is further hybridized with branch-and-bound techniques and mini-buckets is studied. Extensive experimental results analyze the performance of these models and multi-parent recombination. The resulting algorithm consistently finds optimal patterns for up to date solved instances in less time than current approaches. Moreover, it is shown that this proposal provides new best known solutions for very large instances.",
        "published": "2008-12-22T13:09:11Z",
        "link": "http://arxiv.org/abs/0812.4170v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Client-server multi-task learning from distributed datasets",
        "authors": [
            "Francesco Dinuzzo",
            "Gianluigi Pillonetto",
            "Giuseppe De Nicolao"
        ],
        "summary": "A client-server architecture to simultaneously solve multiple learning tasks from distributed datasets is described. In such architecture, each client is associated with an individual learning task and the associated dataset of examples. The goal of the architecture is to perform information fusion from multiple datasets while preserving privacy of individual data. The role of the server is to collect data in real-time from the clients and codify the information in a common database. The information coded in this database can be used by all the clients to solve their individual learning task, so that each client can exploit the informative content of all the datasets without actually having access to private data of others. The proposed algorithmic framework, based on regularization theory and kernel methods, uses a suitable class of mixed effect kernels. The new method is illustrated through a simulated music recommendation system.",
        "published": "2008-12-22T16:34:39Z",
        "link": "http://arxiv.org/abs/0812.4235v2",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Driven by Compression Progress: A Simple Principle Explains Essential   Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention,   Curiosity, Creativity, Art, Science, Music, Jokes",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "I argue that data becomes temporarily interesting by itself to some self-improving, but computationally limited, subjective observer once he learns to predict or compress the data in a better way, thus making it subjectively simpler and more beautiful. Curiosity is the desire to create or discover more non-random, non-arbitrary, regular data that is novel and surprising not in the traditional sense of Boltzmann and Shannon but in the sense that it allows for compression progress because its regularity was not yet known. This drive maximizes interestingness, the first derivative of subjective beauty or compressibility, that is, the steepness of the learning curve. It motivates exploring infants, pure mathematicians, composers, artists, dancers, comedians, yourself, and (since 1990) artificial systems.",
        "published": "2008-12-23T10:14:18Z",
        "link": "http://arxiv.org/abs/0812.4360v2",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "The Latent Relation Mapping Engine: Algorithm and Experiments",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Many AI researchers and cognitive scientists have argued that analogy is the core of cognition. The most influential work on computational modeling of analogy-making is Structure Mapping Theory (SMT) and its implementation in the Structure Mapping Engine (SME). A limitation of SME is the requirement for complex hand-coded representations. We introduce the Latent Relation Mapping Engine (LRME), which combines ideas from SME and Latent Relational Analysis (LRA) in order to remove the requirement for hand-coded representations. LRME builds analogical mappings between lists of words, using a large corpus of raw text to automatically discover the semantic relations among the words. We evaluate LRME on a set of twenty analogical mapping problems, ten based on scientific analogies and ten based on common metaphors. LRME achieves human-level performance on the twenty problems. We compare LRME with a variety of alternative approaches and find that they are not able to reach the same level of performance.",
        "published": "2008-12-23T20:08:53Z",
        "link": "http://arxiv.org/abs/0812.4446v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "H.3.1, I.2.6, I.2.7"
        ]
    },
    {
        "title": "Emergence of Spontaneous Order Through Neighborhood Formation in   Peer-to-Peer Recommender Systems",
        "authors": [
            "Ernesto Diaz-Aviles",
            "Lars Schmidt-Thieme",
            "Cai-Nicolas Ziegler"
        ],
        "summary": "The advent of the Semantic Web necessitates paradigm shifts away from centralized client/server architectures towards decentralization and peer-to-peer computation, making the existence of central authorities superfluous and even impossible. At the same time, recommender systems are gaining considerable impact in e-commerce, providing people with recommendations that are personalized and tailored to their very needs. These recommender systems have traditionally been deployed with stark centralized scenarios in mind, operating in closed communities detached from their host network's outer perimeter. We aim at marrying these two worlds, i.e., decentralized peer-to-peer computing and recommender systems, in one agent-based framework. Our architecture features an epidemic-style protocol maintaining neighborhoods of like-minded peers in a robust, selforganizing fashion. In order to demonstrate our architecture's ability to retain scalability, robustness and to allow for convergence towards high-quality recommendations, we conduct offline experiments on top of the popular MovieLens dataset.",
        "published": "2008-12-23T23:26:27Z",
        "link": "http://arxiv.org/abs/0812.4460v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "cs.MA",
            "C.2.4; H.3.3"
        ]
    },
    {
        "title": "Feature Markov Decision Processes",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "General purpose intelligent learning agents cycle through (complex,non-MDP) sequences of observations, actions, and rewards. On the other hand, reinforcement learning is well-developed for small finite state Markov Decision Processes (MDPs). So far it is an art performed by human designers to extract the right state representation out of the bare observations, i.e. to reduce the agent setup to the MDP framework. Before we can think of mechanizing this search for suitable MDPs, we need a formal objective criterion. The main contribution of this article is to develop such a criterion. I also integrate the various parts into one learning algorithm. Extensions to more realistic dynamic Bayesian networks are developed in a companion article.",
        "published": "2008-12-25T00:27:22Z",
        "link": "http://arxiv.org/abs/0812.4580v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Feature Dynamic Bayesian Networks",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Feature Markov Decision Processes (PhiMDPs) are well-suited for learning agents in general environments. Nevertheless, unstructured (Phi)MDPs are limited to relatively simple environments. Structured MDPs like Dynamic Bayesian Networks (DBNs) are used for large-scale real-world problems. In this article I extend PhiMDP to PhiDBN. The primary contribution is to derive a cost criterion that allows to automatically extract the most relevant features from the environment, leading to the \"best\" DBN representation. I discuss all building blocks required for a complete general learning algorithm.",
        "published": "2008-12-25T00:32:45Z",
        "link": "http://arxiv.org/abs/0812.4581v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "I, Quantum Robot: Quantum Mind control on a Quantum Computer",
        "authors": [
            "Paola Zizzi"
        ],
        "summary": "The logic which describes quantum robots is not orthodox quantum logic, but a deductive calculus which reproduces the quantum tasks (computational processes, and actions) taking into account quantum superposition and quantum entanglement. A way toward the realization of intelligent quantum robots is to adopt a quantum metalanguage to control quantum robots. A physical implementation of a quantum metalanguage might be the use of coherent states in brain signals.",
        "published": "2008-12-25T16:31:05Z",
        "link": "http://arxiv.org/abs/0812.4614v2",
        "categories": [
            "quant-ph",
            "cs.AI",
            "cs.LO",
            "cs.RO"
        ]
    },
    {
        "title": "A New Clustering Algorithm Based Upon Flocking On Complex Network",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "summary": "We have proposed a model based upon flocking on a complex network, and then developed two clustering algorithms on the basis of it. In the algorithms, firstly a \\textit{k}-nearest neighbor (knn) graph as a weighted and directed graph is produced among all data points in a dataset each of which is regarded as an agent who can move in space, and then a time-varying complex network is created by adding long-range links for each data point. Furthermore, each data point is not only acted by its \\textit{k} nearest neighbors but also \\textit{r} long-range neighbors through fields established in space by them together, so it will take a step along the direction of the vector sum of all fields. It is more important that these long-range links provides some hidden information for each data point when it moves and at the same time accelerate its speed converging to a center. As they move in space according to the proposed model, data points that belong to the same class are located at a same position gradually, whereas those that belong to different classes are away from one another. Consequently, the experimental results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the rates of convergence of clustering algorithms are fast enough. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.",
        "published": "2008-12-30T08:30:27Z",
        "link": "http://arxiv.org/abs/0812.5032v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Staring at Economic Aggregators through Information Lenses",
        "authors": [
            "Richard Nock",
            "Nicolas Sanz",
            "Fred Celimene",
            "Frank Nielsen"
        ],
        "summary": "It is hard to exaggerate the role of economic aggregators -- functions that summarize numerous and / or heterogeneous data -- in economic models since the early XX$^{th}$ century. In many cases, as witnessed by the pioneering works of Cobb and Douglas, these functions were information quantities tailored to economic theories, i.e. they were built to fit economic phenomena. In this paper, we look at these functions from the complementary side: information. We use a recent toolbox built on top of a vast class of distortions coined by Bregman, whose application field rivals metrics' in various subfields of mathematics. This toolbox makes it possible to find the quality of an aggregator (for consumptions, prices, labor, capital, wages, etc.), from the standpoint of the information it carries. We prove a rather striking result.   From the informational standpoint, well-known economic aggregators do belong to the \\textit{optimal} set. As common economic assumptions enter the analysis, this large set shrinks, and it essentially ends up \\textit{exactly fitting} either CES, or Cobb-Douglas, or both. To summarize, in the relevant economic contexts, one could not have crafted better some aggregator from the information standpoint. We also discuss global economic behaviors of optimal information aggregators in general, and present a brief panorama of the links between economic and information aggregators.   Keywords: Economic Aggregators, CES, Cobb-Douglas, Bregman divergences",
        "published": "2008-01-02T13:23:04Z",
        "link": "http://arxiv.org/abs/0801.0390v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.OC"
        ]
    },
    {
        "title": "Online variants of the cross-entropy method",
        "authors": [
            "Istvan Szita",
            "Andras Lorincz"
        ],
        "summary": "The cross-entropy method is a simple but efficient method for global optimization. In this paper we provide two online variants of the basic CEM, together with a proof of convergence.",
        "published": "2008-01-14T06:56:42Z",
        "link": "http://arxiv.org/abs/0801.1988v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Factored Value Iteration Converges",
        "authors": [
            "Istvan Szita",
            "Andras Lorincz"
        ],
        "summary": "In this paper we propose a novel algorithm, factored value iteration (FVI), for the approximate solution of factored Markov decision processes (fMDPs). The traditional approximate value iteration algorithm is modified in two ways. For one, the least-squares projection operator is modified so that it does not increase max-norm, and thus preserves convergence. The other modification is that we uniformly sample polynomially many samples from the (exponentially large) state space. This way, the complexity of our algorithm becomes polynomial in the size of the fMDP description length. We prove that the algorithm is convergent. We also derive an upper bound on the difference between our approximate solution and the optimal one, and also on the error introduced by sampling. We analyze various projection operators with respect to their computation complexity and their convergence when combined with approximate value iteration.",
        "published": "2008-01-14T13:09:06Z",
        "link": "http://arxiv.org/abs/0801.2069v2",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "The optimal assignment kernel is not positive definite",
        "authors": [
            "Jean-Philippe Vert"
        ],
        "summary": "We prove that the optimal assignment kernel, proposed recently as an attempt to embed labeled graphs and more generally tuples of basic data to a Hilbert space, is in fact not always positive definite.",
        "published": "2008-01-26T07:32:48Z",
        "link": "http://arxiv.org/abs/0801.4061v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Information Width",
        "authors": [
            "Joel Ratsaby"
        ],
        "summary": "Kolmogorov argued that the concept of information exists also in problems with no underlying stochastic model (as Shannon's information representation) for instance, the information contained in an algorithm or in the genome. He introduced a combinatorial notion of entropy and information $I(x:\\sy)$ conveyed by a binary string $x$ about the unknown value of a variable $\\sy$. The current paper poses the following questions: what is the relationship between the information conveyed by $x$ about $\\sy$ to the description complexity of $x$ ? is there a notion of cost of information ? are there limits on how efficient $x$ conveys information ?   To answer these questions Kolmogorov's definition is extended and a new concept termed {\\em information width} which is similar to $n$-widths in approximation theory is introduced. Information of any input source, e.g., sample-based, general side-information or a hybrid of both can be evaluated by a single common formula. An application to the space of binary functions is considered.",
        "published": "2008-01-30T22:49:57Z",
        "link": "http://arxiv.org/abs/0801.4790v2",
        "categories": [
            "cs.DM",
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "On the Complexity of Binary Samples",
        "authors": [
            "Joel Ratsaby"
        ],
        "summary": "Consider a class $\\mH$ of binary functions $h: X\\to\\{-1, +1\\}$ on a finite interval $X=[0, B]\\subset \\Real$. Define the {\\em sample width} of $h$ on a finite subset (a sample) $S\\subset X$ as $\\w_S(h) \\equiv \\min_{x\\in S} |\\w_h(x)|$, where $\\w_h(x) = h(x) \\max\\{a\\geq 0: h(z)=h(x), x-a\\leq z\\leq x+a\\}$. Let $\\mathbb{S}_\\ell$ be the space of all samples in $X$ of cardinality $\\ell$ and consider sets of wide samples, i.e., {\\em hypersets} which are defined as $A_{\\beta, h} = \\{S\\in \\mathbb{S}_\\ell: \\w_{S}(h) \\geq \\beta\\}$. Through an application of the Sauer-Shelah result on the density of sets an upper estimate is obtained on the growth function (or trace) of the class $\\{A_{\\beta, h}: h\\in\\mH\\}$, $\\beta>0$, i.e., on the number of possible dichotomies obtained by intersecting all hypersets with a fixed collection of samples $S\\in\\mathbb{S}_\\ell$ of cardinality $m$. The estimate is $2\\sum_{i=0}^{2\\lfloor B/(2\\beta)\\rfloor}{m-\\ell\\choose i}$.",
        "published": "2008-01-30T23:14:19Z",
        "link": "http://arxiv.org/abs/0801.4794v1",
        "categories": [
            "cs.DM",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "New Estimation Procedures for PLS Path Modelling",
        "authors": [
            "Xavier Bry"
        ],
        "summary": "Given R groups of numerical variables X1, ... XR, we assume that each group is the result of one underlying latent variable, and that all latent variables are bound together through a linear equation system. Moreover, we assume that some explanatory latent variables may interact pairwise in one or more equations. We basically consider PLS Path Modelling's algorithm to estimate both latent variables and the model's coefficients. New \"external\" estimation schemes are proposed that draw latent variables towards strong group structures in a more flexible way. New \"internal\" estimation schemes are proposed to enable PLSPM to make good use of variable group complementarity and to deal with interactions. Application examples are given.",
        "published": "2008-02-07T15:18:27Z",
        "link": "http://arxiv.org/abs/0802.1002v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Bayesian Nonlinear Principal Component Analysis Using Random Fields",
        "authors": [
            "Heng Lian"
        ],
        "summary": "We propose a novel model for nonlinear dimension reduction motivated by the probabilistic formulation of principal component analysis. Nonlinearity is achieved by specifying different transformation matrices at different locations of the latent space and smoothing the transformation using a Markov random field type prior. The computation is made feasible by the recent advances in sampling from von Mises-Fisher distributions.",
        "published": "2008-02-09T12:22:47Z",
        "link": "http://arxiv.org/abs/0802.1258v1",
        "categories": [
            "cs.CV",
            "cs.LG"
        ]
    },
    {
        "title": "Learning Balanced Mixtures of Discrete Distributions with Small Sample",
        "authors": [
            "Shuheng Zhou"
        ],
        "summary": "We study the problem of partitioning a small sample of $n$ individuals from a mixture of $k$ product distributions over a Boolean cube $\\{0, 1\\}^K$ according to their distributions. Each distribution is described by a vector of allele frequencies in $\\R^K$. Given two distributions, we use $\\gamma$ to denote the average $\\ell_2^2$ distance in frequencies across $K$ dimensions, which measures the statistical divergence between them. We study the case assuming that bits are independently distributed across $K$ dimensions. This work demonstrates that, for a balanced input instance for $k = 2$, a certain graph-based optimization function returns the correct partition with high probability, where a weighted graph $G$ is formed over $n$ individuals, whose pairwise hamming distances between their corresponding bit vectors define the edge weights, so long as $K = \\Omega(\\ln n/\\gamma)$ and $Kn = \\tilde\\Omega(\\ln n/\\gamma^2)$. The function computes a maximum-weight balanced cut of $G$, where the weight of a cut is the sum of the weights across all edges in the cut. This result demonstrates a nice property in the high-dimensional feature space: one can trade off the number of features that are required with the size of the sample to accomplish certain tasks like clustering.",
        "published": "2008-02-10T07:38:49Z",
        "link": "http://arxiv.org/abs/0802.1244v1",
        "categories": [
            "cs.LG",
            "stat.ML"
        ]
    },
    {
        "title": "A New Approach to Collaborative Filtering: Operator Estimation with   Spectral Regularization",
        "authors": [
            "Jacob Abernethy",
            "Francis Bach",
            "Theodoros Evgeniou",
            "Jean-Philippe Vert"
        ],
        "summary": "We present a general approach for collaborative filtering (CF) using spectral regularization to learn linear operators from \"users\" to the \"objects\" they rate. Recent low-rank type matrix completion approaches to CF are shown to be special cases. However, unlike existing regularization based CF methods, our approach can be used to also incorporate information such as attributes of the users or the objects -- a limitation of existing regularization based CF methods. We then provide novel representer theorems that we use to develop new estimation methods. We provide learning algorithms based on low-rank decompositions, and test them on a standard CF dataset. The experiments indicate the advantages of generalizing the existing regularization based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach.",
        "published": "2008-02-11T12:55:34Z",
        "link": "http://arxiv.org/abs/0802.1430v2",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Combining Expert Advice Efficiently",
        "authors": [
            "Wouter Koolen",
            "Steven de Rooij"
        ],
        "summary": "We show how models for prediction with expert advice can be defined concisely and clearly using hidden Markov models (HMMs); standard HMM algorithms can then be used to efficiently calculate, among other things, how the expert predictions should be weighted according to the model. We cast many existing models as HMMs and recover the best known running times in each case. We also describe two new models: the switch distribution, which was recently developed to improve Bayesian/Minimum Description Length model selection, and a new generalisation of the fixed share algorithm based on run-length coding. We give loss bounds for all models and shed new light on their relationships.",
        "published": "2008-02-14T14:54:57Z",
        "link": "http://arxiv.org/abs/0802.2015v2",
        "categories": [
            "cs.LG",
            "cs.DS",
            "cs.IT",
            "math.IT",
            "G.3"
        ]
    },
    {
        "title": "A Radar-Shaped Statistic for Testing and Visualizing Uniformity   Properties in Computer Experiments",
        "authors": [
            "Jessica Franco",
            "Laurent Carraro",
            "Olivier Roustant",
            "Astrid Jourdan"
        ],
        "summary": "In the study of computer codes, filling space as uniformly as possible is important to describe the complexity of the investigated phenomenon. However, this property is not conserved by reducing the dimension. Some numeric experiment designs are conceived in this sense as Latin hypercubes or orthogonal arrays, but they consider only the projections onto the axes or the coordinate planes. In this article we introduce a statistic which allows studying the good distribution of points according to all 1-dimensional projections. By angularly scanning the domain, we obtain a radar type representation, allowing the uniformity defects of a design to be identified with respect to its projections onto straight lines. The advantages of this new tool are demonstrated on usual examples of space-filling designs (SFD) and a global statistic independent of the angle of rotation is studied.",
        "published": "2008-02-15T09:06:25Z",
        "link": "http://arxiv.org/abs/0802.2158v1",
        "categories": [
            "cs.LG",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Compressed Counting",
        "authors": [
            "Ping Li"
        ],
        "summary": "Counting is among the most fundamental operations in computing. For example, counting the pth frequency moment has been a very active area of research, in theoretical computer science, databases, and data mining. When p=1, the task (i.e., counting the sum) can be accomplished using a simple counter.   Compressed Counting (CC) is proposed for efficiently computing the pth frequency moment of a data stream signal A_t, where 0<p<=2. CC is applicable if the streaming data follow the Turnstile model, with the restriction that at the time t for the evaluation, A_t[i]>= 0, which includes the strict Turnstile model as a special case. For natural data streams encountered in practice, this restriction is minor.   The underly technique for CC is what we call skewed stable random projections, which captures the intuition that, when p=1 a simple counter suffices, and when p = 1+/\\Delta with small \\Delta, the sample complexity of a counter system should be low (continuously as a function of \\Delta). We show at small \\Delta the sample complexity (number of projections) k = O(1/\\epsilon) instead of O(1/\\epsilon^2).   Compressed Counting can serve a basic building block for other tasks in statistics and computing, for example, estimation entropies of data streams, parameter estimations using the method of moments and maximum likelihood.   Finally, another contribution is an algorithm for approximating the logarithmic norm, \\sum_{i=1}^D\\log A_t[i], and logarithmic distance. The logarithmic distance is useful in machine learning practice with heavy-tailed data.",
        "published": "2008-02-17T16:42:52Z",
        "link": "http://arxiv.org/abs/0802.2305v2",
        "categories": [
            "cs.IT",
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Sign Language Tutoring Tool",
        "authors": [
            "Oya Aran",
            "Ismail Ari",
            "Alexandre Benoit",
            "Ana Huerta Carrillo",
            "François-Xavier Fanard",
            "Pavel Campr",
            "Lale Akarun",
            "Alice Caplier",
            "Michele Rombaut",
            "Bulent Sankur"
        ],
        "summary": "In this project, we have developed a sign language tutor that lets users learn isolated signs by watching recorded videos and by trying the same signs. The system records the user's video and analyses it. If the sign is recognized, both verbal and animated feedback is given to the user. The system is able to recognize complex signs that involve both hand gestures and head movements and expressions. Our performance tests yield a 99% recognition rate on signs involving only manual gestures and 85% recognition rate on signs that involve both manual and non manual components, such as head movement and facial expressions.",
        "published": "2008-02-18T07:28:44Z",
        "link": "http://arxiv.org/abs/0802.2428v1",
        "categories": [
            "cs.LG",
            "cs.HC"
        ]
    },
    {
        "title": "Pure Exploration for Multi-Armed Bandit Problems",
        "authors": [
            "Sébastien Bubeck",
            "Rémi Munos",
            "Gilles Stoltz"
        ],
        "summary": "We consider the framework of stochastic multi-armed bandit problems and study the possibilities and limitations of forecasters that perform an on-line exploration of the arms. These forecasters are assessed in terms of their simple regret, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between the simple and the cumulative regret. One of the main results in the case of a finite number of arms is a general lower bound on the simple regret of a forecaster in terms of its cumulative regret: the smaller the latter, the larger the former. Keeping this result in mind, we then exhibit upper bounds on the simple regret of some forecasters. The paper ends with a study devoted to continuous-armed bandit problems; we show that the simple regret can be minimized with respect to a family of probability distributions if and only if the cumulative regret can be minimized for it. Based on this equivalence, we are able to prove that the separable metric spaces are exactly the metric spaces on which these regrets can be minimized with respect to the family of all probability distributions with continuous mean-payoff functions.",
        "published": "2008-02-19T14:05:22Z",
        "link": "http://arxiv.org/abs/0802.2655v6",
        "categories": [
            "math.ST",
            "cs.LG",
            "stat.TH"
        ]
    },
    {
        "title": "Knowledge Technologies",
        "authors": [
            "Nick Milton"
        ],
        "summary": "Several technologies are emerging that provide new ways to capture, store, present and use knowledge. This book is the first to provide a comprehensive introduction to five of the most important of these technologies: Knowledge Engineering, Knowledge Based Engineering, Knowledge Webs, Ontologies and Semantic Webs. For each of these, answers are given to a number of key questions (What is it? How does it operate? How is a system developed? What can it be used for? What tools are available? What are the main issues?). The book is aimed at students, researchers and practitioners interested in Knowledge Management, Artificial Intelligence, Design Engineering and Web Technologies.   During the 1990s, Nick worked at the University of Nottingham on the application of AI techniques to knowledge management and on various knowledge acquisition projects to develop expert systems for military applications. In 1999, he joined Epistemics where he worked on numerous knowledge projects and helped establish knowledge management programmes at large organisations in the engineering, technology and legal sectors. He is author of the book \"Knowledge Acquisition in Practice\", which describes a step-by-step procedure for acquiring and implementing expertise. He maintains strong links with leading research organisations working on knowledge technologies, such as knowledge-based engineering, ontologies and semantic technologies.",
        "published": "2008-02-26T11:26:09Z",
        "link": "http://arxiv.org/abs/0802.3789v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.LG",
            "cs.SE"
        ]
    },
    {
        "title": "What Can We Learn Privately?",
        "authors": [
            "Shiva Prasad Kasiviswanathan",
            "Homin K. Lee",
            "Kobbi Nissim",
            "Sofya Raskhodnikova",
            "Adam Smith"
        ],
        "summary": "Learning problems form an important category of computational tasks that generalizes many of the computations researchers apply to large real-life data sets. We ask: what concept classes can be learned privately, namely, by an algorithm whose output does not depend too heavily on any one input or specific training example? More precisely, we investigate learning algorithms that satisfy differential privacy, a notion that provides strong confidentiality guarantees in contexts where aggregate information is released about a database containing sensitive information about individuals. We demonstrate that, ignoring computational constraints, it is possible to privately agnostically learn any concept class using a sample size approximately logarithmic in the cardinality of the concept class. Therefore, almost anything learnable is learnable privately: specifically, if a concept class is learnable by a (non-private) algorithm with polynomial sample complexity and output size, then it can be learned privately using a polynomial number of samples. We also present a computationally efficient private PAC learner for the class of parity functions. Local (or randomized response) algorithms are a practical class of private algorithms that have received extensive investigation. We provide a precise characterization of local private learning algorithms. We show that a concept class is learnable by a local algorithm if and only if it is learnable in the statistical query (SQ) model. Finally, we present a separation between the power of interactive and noninteractive local learning algorithms.",
        "published": "2008-03-06T17:50:07Z",
        "link": "http://arxiv.org/abs/0803.0924v3",
        "categories": [
            "cs.LG",
            "cs.CC",
            "cs.CR",
            "cs.DB"
        ]
    },
    {
        "title": "Privacy Preserving ID3 over Horizontally, Vertically and Grid   Partitioned Data",
        "authors": [
            "Bart Kuijpers",
            "Vanessa Lemmens",
            "Bart Moelans",
            "Karl Tuyls"
        ],
        "summary": "We consider privacy preserving decision tree induction via ID3 in the case where the training data is horizontally or vertically distributed. Furthermore, we consider the same problem in the case where the data is both horizontally and vertically distributed, a situation we refer to as grid partitioned data. We give an algorithm for privacy preserving ID3 over horizontally partitioned data involving more than two parties. For grid partitioned data, we discuss two different evaluation methods for preserving privacy ID3, namely, first merging horizontally and developing vertically or first merging vertically and next developing horizontally. Next to introducing privacy preserving data mining over grid-partitioned data, the main contribution of this paper is that we show, by means of a complexity analysis that the former evaluation method is the more efficient.",
        "published": "2008-03-11T11:18:52Z",
        "link": "http://arxiv.org/abs/0803.1555v1",
        "categories": [
            "cs.DB",
            "cs.LG",
            "E.1; E.3; H.2.8; H.3.3"
        ]
    },
    {
        "title": "Figuring out Actors in Text Streams: Using Collocations to establish   Incremental Mind-maps",
        "authors": [
            "T. Rothenberger",
            "S. Oez",
            "E. Tahirovic",
            "C. Schommer"
        ],
        "summary": "The recognition, involvement, and description of main actors influences the story line of the whole text. This is of higher importance as the text per se represents a flow of words and expressions that once it is read it is lost. In this respect, the understanding of a text and moreover on how the actor exactly behaves is not only a major concern: as human beings try to store a given input on short-term memory while associating diverse aspects and actors with incidents, the following approach represents a virtual architecture, where collocations are concerned and taken as the associative completion of the actors' acting. Once that collocations are discovered, they become managed in separated memory blocks broken down by the actors. As for human beings, the memory blocks refer to associative mind-maps. We then present several priority functions to represent the actual temporal situation inside a mind-map to enable the user to reconstruct the recent events from the discovered temporal results.",
        "published": "2008-03-19T18:00:19Z",
        "link": "http://arxiv.org/abs/0803.2856v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.7.5; I.5.4; H.2.4"
        ]
    },
    {
        "title": "Robustness and Regularization of Support Vector Machines",
        "authors": [
            "Huan Xu",
            "Constantine Caramanis",
            "Shie Mannor"
        ],
        "summary": "We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classification that explicitly build in protection to noise, and at the same time control overfitting. On the analysis front, the equivalence of robustness and regularization, provides a robust optimization interpretation for the success of regularized SVMs. We use the this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well.",
        "published": "2008-03-25T03:51:59Z",
        "link": "http://arxiv.org/abs/0803.3490v2",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Recorded Step Directional Mutation for Faster Convergence",
        "authors": [
            "Ted Dunning"
        ],
        "summary": "Two meta-evolutionary optimization strategies described in this paper accelerate the convergence of evolutionary programming algorithms while still retaining much of their ability to deal with multi-modal problems. The strategies, called directional mutation and recorded step in this paper, can operate independently but together they greatly enhance the ability of evolutionary programming algorithms to deal with fitness landscapes characterized by long narrow valleys. The directional mutation aspect of this combined method uses correlated meta-mutation but does not introduce a full covariance matrix. These new methods are thus much more economical in terms of storage for problems with high dimensionality. Additionally, directional mutation is rotationally invariant which is a substantial advantage over self-adaptive methods which use a single variance per coordinate for problems where the natural orientation of the problem is not oriented along the axes.",
        "published": "2008-03-26T22:49:40Z",
        "link": "http://arxiv.org/abs/0803.3838v2",
        "categories": [
            "cs.NE",
            "cs.LG",
            "D.1.m; G.4"
        ]
    },
    {
        "title": "Support Vector Machine Classification with Indefinite Kernels",
        "authors": [
            "Ronny Luss",
            "Alexandre d'Aspremont"
        ],
        "summary": "We propose a method for support vector machine classification using indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex loss function, our algorithm simultaneously computes support vectors and a proxy kernel matrix used in forming the loss. This can be interpreted as a penalized kernel learning problem where indefinite kernel matrices are treated as a noisy observations of a true Mercer kernel. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the projected gradient or analytic center cutting plane methods. We compare the performance of our technique with other methods on several classic data sets.",
        "published": "2008-04-01T14:55:33Z",
        "link": "http://arxiv.org/abs/0804.0188v2",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "A Unified Semi-Supervised Dimensionality Reduction Framework for   Manifold Learning",
        "authors": [
            "Ratthachat Chatpatanasiri",
            "Boonserm Kijsirikul"
        ],
        "summary": "We present a general framework of semi-supervised dimensionality reduction for manifold learning which naturally generalizes existing supervised and unsupervised learning frameworks which apply the spectral decomposition. Algorithms derived under our framework are able to employ both labeled and unlabeled examples and are able to handle complex problems where data form separate clusters of manifolds. Our framework offers simple views, explains relationships among existing frameworks and provides further extensions which can improve existing algorithms. Furthermore, a new semi-supervised kernelization framework called ``KPCA trick'' is proposed to handle non-linear problems.",
        "published": "2008-04-06T18:14:34Z",
        "link": "http://arxiv.org/abs/0804.0924v2",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Bolasso: model consistent Lasso estimation through the bootstrap",
        "authors": [
            "Francis Bach"
        ],
        "summary": "We consider the least-square linear regression problem with regularization by the l1-norm, a problem usually referred to as the Lasso. In this paper, we present a detailed asymptotic analysis of model consistency of the Lasso. For various decays of the regularization parameter, we compute asymptotic equivalents of the probability of correct model selection (i.e., variable selection). For a specific rate decay, we show that the Lasso selects all the variables that should enter the model with probability tending to one exponentially fast, while it selects all other variables with strictly positive probability. We show that this property implies that if we run the Lasso for several bootstrapped replications of a given sample, then intersecting the supports of the Lasso bootstrap estimates leads to consistent model selection. This novel variable selection algorithm, referred to as the Bolasso, is compared favorably to other linear regression methods on synthetic data and datasets from the UCI machine learning repository.",
        "published": "2008-04-08T15:40:03Z",
        "link": "http://arxiv.org/abs/0804.1302v1",
        "categories": [
            "cs.LG",
            "math.ST",
            "stat.ML",
            "stat.TH"
        ]
    },
    {
        "title": "On Kernelization of Supervised Mahalanobis Distance Learners",
        "authors": [
            "Ratthachat Chatpatanasiri",
            "Teesid Korsrilabutr",
            "Pasakorn Tangchanachaianan",
            "Boonserm Kijsirikul"
        ],
        "summary": "This paper focuses on the problem of kernelizing an existing supervised Mahalanobis distance learner. The following features are included in the paper. Firstly, three popular learners, namely, \"neighborhood component analysis\", \"large margin nearest neighbors\" and \"discriminant neighborhood embedding\", which do not have kernel versions are kernelized in order to improve their classification performances. Secondly, an alternative kernelization framework called \"KPCA trick\" is presented. Implementing a learner in the new framework gains several advantages over the standard framework, e.g. no mathematical formulas and no reprogramming are required for a kernel implementation, the framework avoids troublesome problems such as singularity, etc. Thirdly, while the truths of representer theorems are just assumptions in previous papers related to ours, here, representer theorems are formally proven. The proofs validate both the kernel trick and the KPCA trick in the context of Mahalanobis distance learning. Fourthly, unlike previous works which always apply brute force methods to select a kernel, we investigate two approaches which can be efficiently adopted to construct an appropriate kernel for a given dataset. Finally, numerical results on various real-world datasets are presented.",
        "published": "2008-04-09T09:40:51Z",
        "link": "http://arxiv.org/abs/0804.1441v3",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Isotropic PCA and Affine-Invariant Clustering",
        "authors": [
            "S. Charles Brubaker",
            "Santosh S. Vempala"
        ],
        "summary": "We present a new algorithm for clustering points in R^n. The key property of the algorithm is that it is affine-invariant, i.e., it produces the same partition for any affine transformation of the input. It has strong guarantees when the input is drawn from a mixture model. For a mixture of two arbitrary Gaussians, the algorithm correctly classifies the sample assuming only that the two components are separable by a hyperplane, i.e., there exists a halfspace that contains most of one Gaussian and almost none of the other in probability mass. This is nearly the best possible, improving known results substantially. For k > 2 components, the algorithm requires only that there be some (k-1)-dimensional subspace in which the emoverlap in every direction is small. Here we define overlap to be the ratio of the following two quantities: 1) the average squared distance between a point and the mean of its component, and 2) the average squared distance between a point and the mean of the mixture. The main result may also be stated in the language of linear discriminant analysis: if the standard Fisher discriminant is small enough, labels are not needed to estimate the optimal subspace for projection. Our main tools are isotropic transformation, spectral projection and a simple reweighting technique. We call this combination isotropic PCA.",
        "published": "2008-04-22T17:59:03Z",
        "link": "http://arxiv.org/abs/0804.3575v2",
        "categories": [
            "cs.LG",
            "cs.CG"
        ]
    },
    {
        "title": "Multiple Random Oracles Are Better Than One",
        "authors": [
            "Jan Arpe",
            "Elchanan Mossel"
        ],
        "summary": "We study the problem of learning k-juntas given access to examples drawn from a number of different product distributions. Thus we wish to learn a function f : {-1,1}^n -> {-1,1} that depends on k (unknown) coordinates. While the best known algorithms for the general problem of learning a k-junta require running time of n^k * poly(n,2^k), we show that given access to k different product distributions with biases separated by \\gamma>0, the functions may be learned in time poly(n,2^k,\\gamma^{-k}). More generally, given access to t <= k different product distributions, the functions may be learned in time n^{k/t} * poly(n,2^k,\\gamma^{-k}). Our techniques involve novel results in Fourier analysis relating Fourier expansions with respect to different biases and a generalization of Russo's formula.",
        "published": "2008-04-23T23:18:00Z",
        "link": "http://arxiv.org/abs/0804.3817v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Dependence Structure Estimation via Copula",
        "authors": [
            "Jian Ma",
            "Zengqi Sun"
        ],
        "summary": "Dependence strucuture estimation is one of the important problems in machine learning domain and has many applications in different scientific areas. In this paper, a theoretical framework for such estimation based on copula and copula entropy -- the probabilistic theory of representation and measurement of statistical dependence, is proposed. Graphical models are considered as a special case of the copula framework. A method of the framework for estimating maximum spanning copula is proposed. Due to copula, the method is irrelevant to the properties of individual variables, insensitive to outlier and able to deal with non-Gaussianity. Experiments on both simulated data and real dataset demonstrated the effectiveness of the proposed method.",
        "published": "2008-04-28T17:14:53Z",
        "link": "http://arxiv.org/abs/0804.4451v2",
        "categories": [
            "cs.LG",
            "cs.IR",
            "stat.ME"
        ]
    },
    {
        "title": "Introduction to Relational Networks for Classification",
        "authors": [
            "Vukosi Marivate",
            "Tshilidzi Marwala"
        ],
        "summary": "The use of computational intelligence techniques for classification has been used in numerous applications. This paper compares the use of a Multi Layer Perceptron Neural Network and a new Relational Network on classifying the HIV status of women at ante-natal clinics. The paper discusses the architecture of the relational network and its merits compared to a neural network and most other computational intelligence classifiers. Results gathered from the study indicate comparable classification accuracies as well as revealed relationships between data features in the classification data. Much higher classification accuracies are recommended for future research in the area of HIV classification as well as missing data estimation.",
        "published": "2008-04-29T19:25:07Z",
        "link": "http://arxiv.org/abs/0804.4682v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "The Effect of Structural Diversity of an Ensemble of Classifiers on   Classification Accuracy",
        "authors": [
            "Lesedi Masisi",
            "Fulufhelo V. Nelwamondo",
            "Tshilidzi Marwala"
        ],
        "summary": "This paper aims to showcase the measure of structural diversity of an ensemble of 9 classifiers and then map a relationship between this structural diversity and accuracy. The structural diversity was induced by having different architectures or structures of the classifiers The Genetical Algorithms (GA) were used to derive the relationship between diversity and the classification accuracy by evolving the classifiers and then picking 9 classifiers out on an ensemble of 60 classifiers. It was found that as the ensemble became diverse the accuracy improved. However at a certain diversity measure the accuracy began to drop. The Kohavi-Wolpert variance method is used to measure the diversity of the ensemble. A method of voting is used to aggregate the results from each classifier. The lowest error was observed at a diversity measure of 0.16 with a mean square error of 0.274, when taking 0.2024 as maximum diversity measured. The parameters that were varied were: the number of hidden nodes, learning rate and the activation function.",
        "published": "2008-04-30T06:07:45Z",
        "link": "http://arxiv.org/abs/0804.4741v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "A Quadratic Loss Multi-Class SVM",
        "authors": [
            "Emmanuel Monfrini",
            "Yann Guermeur"
        ],
        "summary": "Using a support vector machine requires to set two types of hyperparameters: the soft margin parameter C and the parameters of the kernel. To perform this model selection task, the method of choice is cross-validation. Its leave-one-out variant is known to produce an estimator of the generalization error which is almost unbiased. Its major drawback rests in its time requirement. To overcome this difficulty, several upper bounds on the leave-one-out error of the pattern recognition SVM have been derived. Among those bounds, the most popular one is probably the radius-margin bound. It applies to the hard margin pattern recognition SVM, and by extension to the 2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2, as a direct extension of the 2-norm SVM to the multi-class case. For this machine, a generalized radius-margin bound is then established.",
        "published": "2008-04-30T19:59:56Z",
        "link": "http://arxiv.org/abs/0804.4898v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "On Recovery of Sparse Signals via $\\ell_1$ Minimization",
        "authors": [
            "T. Tony Cai",
            "Guangwu Xu",
            "Jun Zhang"
        ],
        "summary": "This article considers constrained $\\ell_1$ minimization methods for the recovery of high dimensional sparse signals in three settings: noiseless, bounded error and Gaussian noise. A unified and elementary treatment is given in these noise settings for two $\\ell_1$ minimization methods: the Dantzig selector and $\\ell_1$ minimization with an $\\ell_2$ constraint. The results of this paper improve the existing results in the literature by weakening the conditions and tightening the error bounds. The improvement on the conditions shows that signals with larger support can be recovered accurately. This paper also establishes connections between restricted isometry property and the mutual incoherence property. Some results of Candes, Romberg and Tao (2006) and Donoho, Elad, and Temlyakov (2006) are extended.",
        "published": "2008-05-01T20:25:27Z",
        "link": "http://arxiv.org/abs/0805.0149v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "On-line Learning of an Unlearnable True Teacher through Mobile Ensemble   Teachers",
        "authors": [
            "Takeshi Hirama",
            "Koji Hukushima"
        ],
        "summary": "On-line learning of a hierarchical learning model is studied by a method from statistical mechanics. In our model a student of a simple perceptron learns from not a true teacher directly, but ensemble teachers who learn from the true teacher with a perceptron learning rule. Since the true teacher and the ensemble teachers are expressed as non-monotonic perceptron and simple ones, respectively, the ensemble teachers go around the unlearnable true teacher with the distance between them fixed in an asymptotic steady state. The generalization performance of the student is shown to exceed that of the ensemble teachers in a transient state, as was shown in similar ensemble-teachers models. Further, it is found that moving the ensemble teachers even in the steady state, in contrast to the fixed ensemble teachers, is efficient for the performance of the student.",
        "published": "2008-05-10T15:40:24Z",
        "link": "http://arxiv.org/abs/0805.1480v1",
        "categories": [
            "cond-mat.dis-nn",
            "cs.LG"
        ]
    },
    {
        "title": "Rollout Sampling Approximate Policy Iteration",
        "authors": [
            "Christos Dimitrakakis",
            "Michail G. Lagoudakis"
        ],
        "summary": "Several researchers have recently investigated the connection between reinforcement learning and classification. We are motivated by proposals of approximate policy iteration schemes without value functions which focus on policy representation using classifiers and address policy learning as a supervised learning problem. This paper proposes variants of an improved policy iteration scheme which addresses the core sampling problem in evaluating a policy through simulation as a multi-armed bandit machine. The resulting algorithm offers comparable performance to the previous algorithm achieved, however, with significantly less computational effort. An order of magnitude improvement is demonstrated experimentally in two standard reinforcement learning domains: inverted pendulum and mountain-car.",
        "published": "2008-05-14T11:19:19Z",
        "link": "http://arxiv.org/abs/0805.2027v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "An optimization problem on the sphere",
        "authors": [
            "Andreas Maurer"
        ],
        "summary": "We prove existence and uniqueness of the minimizer for the average geodesic distance to the points of a geodesically convex set on the sphere. This implies a corresponding existence and uniqueness result for an optimal algorithm for halfspace learning, when data and target functions are drawn from the uniform distribution.",
        "published": "2008-05-15T17:25:03Z",
        "link": "http://arxiv.org/abs/0805.2362v1",
        "categories": [
            "cs.LG",
            "cs.CG"
        ]
    },
    {
        "title": "A Kernel Method for the Two-Sample Problem",
        "authors": [
            "Arthur Gretton",
            "Karsten Borgwardt",
            "Malte J. Rasch",
            "Bernhard Scholkopf",
            "Alexander J. Smola"
        ],
        "summary": "We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.",
        "published": "2008-05-15T17:46:53Z",
        "link": "http://arxiv.org/abs/0805.2368v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "G.3; I.2.6"
        ]
    },
    {
        "title": "The Margitron: A Generalised Perceptron with Margin",
        "authors": [
            "Constantinos Panagiotakopoulos",
            "Petroula Tsampouka"
        ],
        "summary": "We identify the classical Perceptron algorithm with margin as a member of a broader family of large margin classifiers which we collectively call the Margitron. The Margitron, (despite its) sharing the same update rule with the Perceptron, is shown in an incremental setting to converge in a finite number of updates to solutions possessing any desirable fraction of the maximum margin. Experiments comparing the Margitron with decomposition SVMs on tasks involving linear kernels and 2-norm soft margin are also reported.",
        "published": "2008-05-18T20:07:22Z",
        "link": "http://arxiv.org/abs/0805.2752v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Sample Selection Bias Correction Theory",
        "authors": [
            "Corinna Cortes",
            "Mehryar Mohri",
            "Michael Riley",
            "Afshin Rostamizadeh"
        ],
        "summary": "This paper presents a theoretical analysis of sample selection bias correction. The sample bias correction technique commonly used in machine learning consists of reweighting the cost of an error on each training point of a biased sample to more closely reflect the unbiased distribution. This relies on weights derived by various estimation techniques based on finite samples. We analyze the effect of an error in that estimation on the accuracy of the hypothesis returned by the learning algorithm for two estimation techniques: a cluster-based estimation technique and kernel mean matching. We also report the results of sample bias correction experiments with several data sets using these techniques. Our analysis is based on the novel concept of distributional stability which generalizes the existing concept of point-based stability. Much of our work and proof techniques can be used to analyze other importance weighting techniques and their effect on accuracy when using a distributionally stable algorithm.",
        "published": "2008-05-19T02:55:08Z",
        "link": "http://arxiv.org/abs/0805.2775v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Learning Low-Density Separators",
        "authors": [
            "Shai Ben-David",
            "Tyler Lu",
            "David Pal",
            "Miroslava Sotakova"
        ],
        "summary": "We define a novel, basic, unsupervised learning problem - learning the lowest density homogeneous hyperplane separator of an unknown probability distribution. This task is relevant to several problems in machine learning, such as semi-supervised learning and clustering stability. We investigate the question of existence of a universally consistent algorithm for this problem. We propose two natural learning paradigms and prove that, on input unlabeled random samples generated by any member of a rich family of distributions, they are guaranteed to converge to the optimal separator for that distribution. We complement this result by showing that no learning algorithm for our task can achieve uniform learning rates (that are independent of the data generating distribution).",
        "published": "2008-05-19T17:55:08Z",
        "link": "http://arxiv.org/abs/0805.2891v2",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "From Data Topology to a Modular Classifier",
        "authors": [
            "Abdel Ennaji",
            "Arnaud Ribert",
            "Yves Lecourtier"
        ],
        "summary": "This article describes an approach to designing a distributed and modular neural classifier. This approach introduces a new hierarchical clustering that enables one to determine reliable regions in the representation space by exploiting supervised information. A multilayer perceptron is then associated with each of these detected clusters and charged with recognizing elements of the associated cluster while rejecting all others. The obtained global classifier is comprised of a set of cooperating neural networks and completed by a K-nearest neighbor classifier charged with treating elements rejected by all the neural networks. Experimental results for the handwritten digit recognition problem and comparison with neural and statistical nonmodular classifiers are given.",
        "published": "2008-05-28T09:16:44Z",
        "link": "http://arxiv.org/abs/0805.4290v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Utilisation des grammaires probabilistes dans les tâches de   segmentation et d'annotation prosodique",
        "authors": [
            "Irina Nesterenko",
            "Stéphane Rauzy"
        ],
        "summary": "Nous pr\\'esentons dans cette contribution une approche \\`a la fois symbolique et probabiliste permettant d'extraire l'information sur la segmentation du signal de parole \\`a partir d'information prosodique. Nous utilisons pour ce faire des grammaires probabilistes poss\\'edant une structure hi\\'erarchique minimale. La phase de construction des grammaires ainsi que leur pouvoir de pr\\'ediction sont \\'evalu\\'es qualitativement ainsi que quantitativement.   -----   Methodologically oriented, the present work sketches an approach for prosodic information retrieval and speech segmentation, based on both symbolic and probabilistic information. We have recourse to probabilistic grammars, within which we implement a minimal hierarchical structure. Both the stages of probabilistic grammar building and its testing in prediction are explored and quantitatively and qualitatively evaluated.",
        "published": "2008-06-06T13:33:31Z",
        "link": "http://arxiv.org/abs/0806.1156v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Belief Propagation and Beyond for Particle Tracking",
        "authors": [
            "Michael Chertkov",
            "Lukas Kroc",
            "Massimo Vergassola"
        ],
        "summary": "We describe a novel approach to statistical learning from particles tracked while moving in a random environment. The problem consists in inferring properties of the environment from recorded snapshots. We consider here the case of a fluid seeded with identical passive particles that diffuse and are advected by a flow. Our approach rests on efficient algorithms to estimate the weighted number of possible matchings among particles in two consecutive snapshots, the partition function of the underlying graphical model. The partition function is then maximized over the model parameters, namely diffusivity and velocity gradient. A Belief Propagation (BP) scheme is the backbone of our algorithm, providing accurate results for the flow parameters we want to learn. The BP estimate is additionally improved by incorporating Loop Series (LS) contributions. For the weighted matching problem, LS is compactly expressed as a Cauchy integral, accurately estimated by a saddle point approximation. Numerical experiments show that the quality of our improved BP algorithm is comparable to the one of a fully polynomial randomized approximation scheme, based on the Markov Chain Monte Carlo (MCMC) method, while the BP-based scheme is substantially faster than the MCMC scheme.",
        "published": "2008-06-06T16:18:13Z",
        "link": "http://arxiv.org/abs/0806.1199v1",
        "categories": [
            "cs.IT",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.LG",
            "math.IT",
            "physics.flu-dyn"
        ]
    },
    {
        "title": "Decoding Beta-Decay Systematics: A Global Statistical Model for Beta^-   Halflives",
        "authors": [
            "N. J. Costiris",
            "E. Mavrommatis",
            "K. A. Gernoth",
            "J. W. Clark"
        ],
        "summary": "Statistical modeling of nuclear data provides a novel approach to nuclear systematics complementary to established theoretical and phenomenological approaches based on quantum theory. Continuing previous studies in which global statistical modeling is pursued within the general framework of machine learning theory, we implement advances in training algorithms designed to improved generalization, in application to the problem of reproducing and predicting the halflives of nuclear ground states that decay 100% by the beta^- mode. More specifically, fully-connected, multilayer feedforward artificial neural network models are developed using the Levenberg-Marquardt optimization algorithm together with Bayesian regularization and cross-validation. The predictive performance of models emerging from extensive computer experiments is compared with that of traditional microscopic and phenomenological models as well as with the performance of other learning systems, including earlier neural network models as well as the support vector machines recently applied to the same problem. In discussing the results, emphasis is placed on predictions for nuclei that are far from the stability line, and especially those involved in the r-process nucleosynthesis. It is found that the new statistical models can match or even surpass the predictive performance of conventional models for beta-decay systematics and accordingly should provide a valuable additional tool for exploring the expanding nuclear landscape.",
        "published": "2008-06-17T18:23:15Z",
        "link": "http://arxiv.org/abs/0806.2850v1",
        "categories": [
            "nucl-th",
            "astro-ph",
            "cond-mat.dis-nn",
            "cs.LG",
            "stat.ML"
        ]
    },
    {
        "title": "Learning Graph Matching",
        "authors": [
            "Tiberio S. Caetano",
            "Julian J. McAuley",
            "Li Cheng",
            "Quoc V. Le",
            "Alex J. Smola"
        ],
        "summary": "As a fundamental problem in pattern recognition, graph matching has applications in a variety of fields, from computer vision to computational biology. In graph matching, patterns are modeled as graphs and pattern recognition amounts to finding a correspondence between the nodes of different graphs. Many formulations of this problem can be cast in general as a quadratic assignment problem, where a linear term in the objective function encodes node compatibility and a quadratic term encodes edge compatibility. The main research focus in this theme is about designing efficient algorithms for approximately solving the quadratic assignment problem, since it is NP-hard. In this paper we turn our attention to a different question: how to estimate compatibility functions such that the solution of the resulting graph matching problem best matches the expected solution that a human would manually provide. We present a method for learning graph matching: the training examples are pairs of graphs and the `labels' are matches between them. Our experimental results reveal that learning can substantially improve the performance of standard graph matching algorithms. In particular, we find that simple linear assignment with such a learning scheme outperforms Graduated Assignment with bistochastic normalisation, a state-of-the-art quadratic assignment relaxation algorithm.",
        "published": "2008-06-17T23:28:08Z",
        "link": "http://arxiv.org/abs/0806.2890v1",
        "categories": [
            "cs.CV",
            "cs.LG"
        ]
    },
    {
        "title": "Statistical Learning of Arbitrary Computable Classifiers",
        "authors": [
            "David Soloveichik"
        ],
        "summary": "Statistical learning theory chiefly studies restricted hypothesis classes, particularly those with finite Vapnik-Chervonenkis (VC) dimension. The fundamental quantity of interest is the sample complexity: the number of samples required to learn to a specified level of accuracy. Here we consider learning over the set of all computable labeling functions. Since the VC-dimension is infinite and a priori (uniform) bounds on the number of samples are impossible, we let the learning algorithm decide when it has seen sufficient samples to have learned. We first show that learning in this setting is indeed possible, and develop a learning algorithm. We then show, however, that bounding sample complexity independently of the distribution is impossible. Notably, this impossibility is entirely due to the requirement that the learning algorithm be computable, and not due to the statistical nature of the problem.",
        "published": "2008-06-22T01:28:14Z",
        "link": "http://arxiv.org/abs/0806.3537v2",
        "categories": [
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "Agnostically Learning Juntas from Random Walks",
        "authors": [
            "Jan Arpe",
            "Elchanan Mossel"
        ],
        "summary": "We prove that the class of functions g:{-1,+1}^n -> {-1,+1} that only depend on an unknown subset of k<<n variables (so-called k-juntas) is agnostically learnable from a random walk in time polynomial in n, 2^{k^2}, epsilon^{-k}, and log(1/delta). In other words, there is an algorithm with the claimed running time that, given epsilon, delta > 0 and access to a random walk on {-1,+1}^n labeled by an arbitrary function f:{-1,+1}^n -> {-1,+1}, finds with probability at least 1-delta a k-junta that is (opt(f)+epsilon)-close to f, where opt(f) denotes the distance of a closest k-junta to f.",
        "published": "2008-06-25T23:18:44Z",
        "link": "http://arxiv.org/abs/0806.4210v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "On Sequences with Non-Learnable Subsequences",
        "authors": [
            "Vladimir V. V'yugin"
        ],
        "summary": "The remarkable results of Foster and Vohra was a starting point for a series of papers which show that any sequence of outcomes can be learned (with no prior knowledge) using some universal randomized forecasting algorithm and forecast-dependent checking rules. We show that for the class of all computationally efficient outcome-forecast-based checking rules, this property is violated. Moreover, we present a probabilistic algorithm generating with probability close to one a sequence with a subsequence which simultaneously miscalibrates all partially weakly computable randomized forecasting algorithms. %subsequences non-learnable by each randomized algorithm.   According to the Dawid's prequential framework we consider partial recursive randomized algorithms.",
        "published": "2008-06-26T15:21:00Z",
        "link": "http://arxiv.org/abs/0806.4341v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "F.4.1; I.2.6"
        ]
    },
    {
        "title": "Prediction with Expert Advice in Games with Unbounded One-Step Gains",
        "authors": [
            "Vladimir V. V'yugin"
        ],
        "summary": "The games of prediction with expert advice are considered in this paper. We present some modification of Kalai and Vempala algorithm of following the perturbed leader for the case of unrestrictedly large one-step gains. We show that in general case the cumulative gain of any probabilistic prediction algorithm can be much worse than the gain of some expert of the pool. Nevertheless, we give the lower bound for this cumulative gain in general case and construct a universal algorithm which has the optimal performance; we also prove that in case when one-step gains of experts of the pool have ``limited deviations'' the performance of our algorithm is close to the performance of the best expert.",
        "published": "2008-06-26T20:21:06Z",
        "link": "http://arxiv.org/abs/0806.4391v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "Computationally Efficient Estimators for Dimension Reductions Using   Stable Random Projections",
        "authors": [
            "Ping Li"
        ],
        "summary": "The method of stable random projections is a tool for efficiently computing the $l_\\alpha$ distances using low memory, where $0<\\alpha \\leq 2$ is a tuning parameter. The method boils down to a statistical estimation task and various estimators have been proposed, based on the geometric mean, the harmonic mean, and the fractional power etc.   This study proposes the optimal quantile estimator, whose main operation is selecting, which is considerably less expensive than taking fractional power, the main operation in previous estimators. Our experiments report that the optimal quantile estimator is nearly one order of magnitude more computationally efficient than previous estimators. For large-scale learning tasks in which storing and computing pairwise distances is a serious bottleneck, this estimator should be desirable.   In addition to its computational advantages, the optimal quantile estimator exhibits nice theoretical properties. It is more accurate than previous estimators when $\\alpha>1$. We derive its theoretical error bounds and establish the explicit (i.e., no hidden constants) sample complexity bound.",
        "published": "2008-06-27T05:19:19Z",
        "link": "http://arxiv.org/abs/0806.4422v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "On Approximating the Lp Distances for p>2",
        "authors": [
            "Ping Li"
        ],
        "summary": "Applications in machine learning and data mining require computing pairwise Lp distances in a data matrix A. For massive high-dimensional data, computing all pairwise distances of A can be infeasible. In fact, even storing A or all pairwise distances of A in the memory may be also infeasible. This paper proposes a simple method for p = 2, 4, 6, ... We first decompose the l_p (where p is even) distances into a sum of 2 marginal norms and p-1 ``inner products'' at different orders. Then we apply normal or sub-Gaussian random projections to approximate the resultant ``inner products,'' assuming that the marginal norms can be computed exactly by a linear scan. We propose two strategies for applying random projections. The basic projection strategy requires only one projection matrix but it is more difficult to analyze, while the alternative projection strategy requires p-1 projection matrices but its theoretical analysis is much easier. In terms of the accuracy, at least for p=4, the basic strategy is always more accurate than the alternative strategy if the data are non-negative, which is common in reality.",
        "published": "2008-06-27T05:36:09Z",
        "link": "http://arxiv.org/abs/0806.4423v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "On empirical meaning of randomness with respect to a real parameter",
        "authors": [
            "Vladimir V'yugin"
        ],
        "summary": "We study the empirical meaning of randomness with respect to a family of probability distributions $P_\\theta$, where $\\theta$ is a real parameter, using algorithmic randomness theory. In the case when for a computable probability distribution $P_\\theta$ an effectively strongly consistent estimate exists, we show that the Levin's a priory semicomputable semimeasure of the set of all $P_\\theta$-random sequences is positive if and only if the parameter $\\theta$ is a computable real number. The different methods for generating ``meaningful'' $P_\\theta$-random sequences with noncomputable $\\theta$ are discussed.",
        "published": "2008-06-27T10:49:33Z",
        "link": "http://arxiv.org/abs/0806.4484v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "Sparse Online Learning via Truncated Gradient",
        "authors": [
            "John Langford",
            "Lihong Li",
            "Tong Zhang"
        ],
        "summary": "We propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions. This method has several essential properties: The degree of sparsity is continuous -- a parameter controls the rate of sparsification from no sparsification to total sparsification. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular $L_1$-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online learning guarantees. The approach works well empirically. We apply the approach to several datasets and find that for datasets with large numbers of features, substantial sparsity is discoverable.",
        "published": "2008-06-28T14:19:50Z",
        "link": "http://arxiv.org/abs/0806.4686v2",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Graph Kernels",
        "authors": [
            "S. V. N. Vishwanathan",
            "Karsten M. Borgwardt",
            "Imre Risi Kondor",
            "Nicol N. Schraudolph"
        ],
        "summary": "We present a unified framework to study graph kernels, special cases of which include the random walk graph kernel \\citep{GaeFlaWro03,BorOngSchVisetal05}, marginalized graph kernel \\citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04}, and geometric kernel on graphs \\citep{Gaertner02}. Through extensions of linear algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a Sylvester equation, we construct an algorithm that improves the time complexity of kernel computation from $O(n^6)$ to $O(n^3)$. When the graphs are sparse, conjugate gradient solvers or fixed-point iterations bring our algorithm into the sub-cubic domain. Experiments on graphs from bioinformatics and other application domains show that it is often more than a thousand times faster than previous approaches. We then explore connections between diffusion kernels \\citep{KonLaf02}, regularization on graphs \\citep{SmoKon03}, and graph kernels, and use these connections to propose new graph kernels. Finally, we show that rational kernels \\citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized to graphs reduce to the random walk graph kernel.",
        "published": "2008-07-01T09:46:14Z",
        "link": "http://arxiv.org/abs/0807.0093v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Catching Up Faster by Switching Sooner: A Prequential Solution to the   AIC-BIC Dilemma",
        "authors": [
            "Tim van Erven",
            "Peter Grunwald",
            "Steven de Rooij"
        ],
        "summary": "Bayesian model averaging, model selection and its approximations such as BIC are generally statistically consistent, but sometimes achieve slower rates og convergence than other methods such as AIC and leave-one-out cross-validation. On the other hand, these other methods can br inconsistent. We identify the \"catch-up phenomenon\" as a novel explanation for the slow convergence of Bayesian methods. Based on this analysis we define the switch distribution, a modification of the Bayesian marginal distribution. We show that, under broad conditions,model selection and prediction based on the switch distribution is both consistent and achieves optimal convergence rates, thereby resolving the AIC-BIC dilemma. The method is practical; we give an efficient implementation. The switch distribution has a data compression interpretation, and can thus be viewed as a \"prequential\" or MDL method; yet it is different from the MDL methods that are usually considered in the literature. We compare the switch distribution to Bayes factor model selection and leave-one-out cross-validation.",
        "published": "2008-07-07T12:57:23Z",
        "link": "http://arxiv.org/abs/0807.1005v1",
        "categories": [
            "math.ST",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "stat.ME",
            "stat.ML",
            "stat.TH",
            "62G99; 94A99"
        ]
    },
    {
        "title": "Algorithm Selection as a Bandit Problem with Unbounded Losses",
        "authors": [
            "Matteo Gagliolo",
            "Juergen Schmidhuber"
        ],
        "summary": "Algorithm selection is typically based on models of algorithm performance, learned during a separate offline training sequence, which can be prohibitively expensive. In recent work, we adopted an online approach, in which a performance model is iteratively updated and used to guide selection on a sequence of problem instances. The resulting exploration-exploitation trade-off was represented as a bandit problem with expert advice, using an existing solver for this game, but this required the setting of an arbitrary bound on algorithm runtimes, thus invalidating the optimal regret of the solver. In this paper, we propose a simpler framework for representing algorithm selection as a bandit problem, with partial information, and an unknown bound on losses. We adapt an existing solver to this game, proving a bound on its expected regret, which holds also for the resulting algorithm selection technique. We present preliminary experiments with a set of SAT solvers on a mixed SAT-UNSAT benchmark.",
        "published": "2008-07-09T16:47:36Z",
        "link": "http://arxiv.org/abs/0807.1494v1",
        "categories": [
            "cs.AI",
            "cs.GT",
            "cs.LG",
            "F.2.2; G.3; I.1.2; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Multi-Instance Learning by Treating Instances As Non-I.I.D. Samples",
        "authors": [
            "Zhi-Hua Zhou",
            "Yu-Yin Sun",
            "Yu-Feng Li"
        ],
        "summary": "Multi-instance learning attempts to learn from a training set consisting of labeled bags each containing many unlabeled instances. Previous studies typically treat the instances in the bags as independently and identically distributed. However, the instances in a bag are rarely independent, and therefore a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits the relations among instances. In this paper, we propose a simple yet effective multi-instance learning method, which regards each bag as a graph and uses a specific kernel to distinguish the graphs by considering the features of the nodes as well as the features of the edges that convey some relations among instances. The effectiveness of the proposed method is validated by experiments.",
        "published": "2008-07-12T20:19:18Z",
        "link": "http://arxiv.org/abs/0807.1997v4",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Algorithms for Dynamic Spectrum Access with Learning for Cognitive Radio",
        "authors": [
            "Jayakrishnan Unnikrishnan",
            "Venugopal Veeravalli"
        ],
        "summary": "We study the problem of dynamic spectrum sensing and access in cognitive radio systems as a partially observed Markov decision process (POMDP). A group of cognitive users cooperatively tries to exploit vacancies in primary (licensed) channels whose occupancies follow a Markovian evolution. We first consider the scenario where the cognitive users have perfect knowledge of the distribution of the signals they receive from the primary users. For this problem, we obtain a greedy channel selection and access policy that maximizes the instantaneous reward, while satisfying a constraint on the probability of interfering with licensed transmissions. We also derive an analytical universal upper bound on the performance of the optimal policy. Through simulation, we show that our scheme achieves good performance relative to the upper bound and improved performance relative to an existing scheme.   We then consider the more practical scenario where the exact distribution of the signal from the primary is unknown. We assume a parametric model for the distribution and develop an algorithm that can learn the true distribution, still guaranteeing the constraint on the interference probability. We show that this algorithm outperforms the naive design that assumes a worst case value for the parameter. We also provide a proof for the convergence of the learning algorithm.",
        "published": "2008-07-16T23:59:28Z",
        "link": "http://arxiv.org/abs/0807.2677v4",
        "categories": [
            "cs.NI",
            "cs.LG"
        ]
    },
    {
        "title": "On Probability Distributions for Trees: Representations, Inference and   Learning",
        "authors": [
            "François Denis",
            "Amaury Habrard",
            "Rémi Gilleron",
            "Marc Tommasi",
            "Édouard Gilbert"
        ],
        "summary": "We study probability distributions over free algebras of trees. Probability distributions can be seen as particular (formal power) tree series [Berstel et al 82, Esik et al 03], i.e. mappings from trees to a semiring K . A widely studied class of tree series is the class of rational (or recognizable) tree series which can be defined either in an algebraic way or by means of multiplicity tree automata. We argue that the algebraic representation is very convenient to model probability distributions over a free algebra of trees. First, as in the string case, the algebraic representation allows to design learning algorithms for the whole class of probability distributions defined by rational tree series. Note that learning algorithms for rational tree series correspond to learning algorithms for weighted tree automata where both the structure and the weights are learned. Second, the algebraic representation can be easily extended to deal with unranked trees (like XML trees where a symbol may have an unbounded number of children). Both properties are particularly relevant for applications: nondeterministic automata are required for the inference problem to be relevant (recall that Hidden Markov Models are equivalent to nondeterministic string automata); nowadays applications for Web Information Extraction, Web Services and document processing consider unranked trees.",
        "published": "2008-07-18T14:41:44Z",
        "link": "http://arxiv.org/abs/0807.2983v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Universal Denoising of Discrete-time Continuous-Amplitude Signals",
        "authors": [
            "Kamakshi Sivaramakrishnan",
            "Tsachy Weissman"
        ],
        "summary": "We consider the problem of reconstructing a discrete-time signal (sequence) with continuous-valued components corrupted by a known memoryless channel. When performance is measured using a per-symbol loss function satisfying mild regularity conditions, we develop a sequence of denoisers that, although independent of the distribution of the underlying `clean' sequence, is universally optimal in the limit of large sequence length. This sequence of denoisers is universal in the sense of performing as well as any sliding window denoising scheme which may be optimized for the underlying clean signal. Our results are initially developed in a ``semi-stochastic'' setting, where the noiseless signal is an unknown individual sequence, and the only source of randomness is due to the channel noise. It is subsequently shown that in the fully stochastic setting, where the noiseless sequence is a stationary stochastic process, our schemes universally attain optimum performance. The proposed schemes draw from nonparametric density estimation techniques and are practically implementable. We demonstrate efficacy of the proposed schemes in denoising gray-scale images in the conventional additive white Gaussian noise setting, with additional promising results for less conventional noise distributions.",
        "published": "2008-07-22T07:42:11Z",
        "link": "http://arxiv.org/abs/0807.3396v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.ST",
            "stat.TH",
            "H.1.1"
        ]
    },
    {
        "title": "Positive factor networks: A graphical framework for modeling   non-negative sequential data",
        "authors": [
            "Brian K. Vogel"
        ],
        "summary": "We present a novel graphical framework for modeling non-negative sequential data with hierarchical structure. Our model corresponds to a network of coupled non-negative matrix factorization (NMF) modules, which we refer to as a positive factor network (PFN). The data model is linear, subject to non-negativity constraints, so that observation data consisting of an additive combination of individually representable observations is also representable by the network. This is a desirable property for modeling problems in computational auditory scene analysis, since distinct sound sources in the environment are often well-modeled as combining additively in the corresponding magnitude spectrogram. We propose inference and learning algorithms that leverage existing NMF algorithms and that are straightforward to implement. We present a target tracking example and provide results for synthetic observation data which serve to illustrate the interesting properties of PFNs and motivate their potential usefulness in applications such as music transcription, source separation, and speech recognition. We show how a target process characterized by a hierarchical state transition model can be represented as a PFN. Our results illustrate that a PFN which is defined in terms of a single target observation can then be used to effectively track the states of multiple simultaneous targets. Our results show that the quality of the inferred target states degrades gradually as the observation noise is increased. We also present results for an example in which meaningful hierarchical features are extracted from a spectrogram. Such a hierarchical representation could be useful for music transcription and source separation applications. We also propose a network for language modeling.",
        "published": "2008-07-25T22:50:46Z",
        "link": "http://arxiv.org/abs/0807.4198v2",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Mutual information is copula entropy",
        "authors": [
            "Jian Ma",
            "Zengqi Sun"
        ],
        "summary": "We prove that mutual information is actually negative copula entropy, based on which a method for mutual information estimation is proposed.",
        "published": "2008-08-06T14:20:56Z",
        "link": "http://arxiv.org/abs/0808.0845v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Building an interpretable fuzzy rule base from data using Orthogonal   Least Squares Application to a depollution problem",
        "authors": [
            "Sébastien Destercke",
            "Serge Guillaume",
            "Brigitte Charnomordic"
        ],
        "summary": "In many fields where human understanding plays a crucial role, such as bioprocesses, the capacity of extracting knowledge from data is of critical importance. Within this framework, fuzzy learning methods, if properly used, can greatly help human experts. Amongst these methods, the aim of orthogonal transformations, which have been proven to be mathematically robust, is to build rules from a set of training data and to select the most important ones by linear regression or rank revealing techniques. The OLS algorithm is a good representative of those methods. However, it was originally designed so that it only cared about numerical performance. Thus, we propose some modifications of the original method to take interpretability into account. After recalling the original algorithm, this paper presents the changes made to the original method, then discusses some results obtained from benchmark problems. Finally, the algorithm is applied to a real-world fault detection depollution problem.",
        "published": "2008-08-21T19:54:04Z",
        "link": "http://arxiv.org/abs/0808.2984v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Multi-Instance Multi-Label Learning",
        "authors": [
            "Zhi-Hua Zhou",
            "Min-Ling Zhang",
            "Sheng-Jun Huang",
            "Yu-Feng Li"
        ],
        "summary": "In this paper, we propose the MIML (Multi-Instance Multi-Label learning) framework where an example is described by multiple instances and associated with multiple class labels. Compared to traditional learning frameworks, the MIML framework is more convenient and natural for representing complicated objects which have multiple semantic meanings. To learn from MIML examples, we propose the MimlBoost and MimlSvm algorithms based on a simple degeneration strategy, and experiments show that solving problems involving complicated objects with multiple semantic meanings in the MIML framework can lead to good performance. Considering that the degeneration process may lose information, we propose the D-MimlSvm algorithm which tackles MIML problems directly in a regularization framework. Moreover, we show that even when we do not have access to the real objects and thus cannot capture more information from real objects by using the MIML representation, MIML is still useful. We propose the InsDif and SubCod algorithms. InsDif works by transforming single-instances into the MIML representation for learning, while SubCod works by transforming single-label examples into the MIML representation for learning. Experiments show that in some tasks they are able to achieve better performance than learning the single-instances or single-label examples directly.",
        "published": "2008-08-24T06:31:43Z",
        "link": "http://arxiv.org/abs/0808.3231v4",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "A game-theoretic version of Oakes' example for randomized forecasting",
        "authors": [
            "Vladimir V. V'yugin"
        ],
        "summary": "Using the game-theoretic framework for probability, Vovk and Shafer. have shown that it is always possible, using randomization, to make sequential probability forecasts that pass any countable set of well-behaved statistical tests. This result generalizes work by other authors, who consider only tests of calbration.   We complement this result with a lower bound. We show that Vovk and Shafer's result is valid only when the forecasts are computed with unrestrictedly increasing degree of accuracy.   When some level of discreteness is fixed, we present a game-theoretic generalization of Oakes' example for randomized forecasting that is a test failing any given method of deferministic forecasting; originally, this example was presented for deterministic calibration.",
        "published": "2008-08-27T17:30:22Z",
        "link": "http://arxiv.org/abs/0808.3746v2",
        "categories": [
            "cs.LG",
            "cs.GT",
            "I.2"
        ]
    },
    {
        "title": "A Variational Inference Framework for Soft-In-Soft-Out Detection in   Multiple Access Channels",
        "authors": [
            "D. D. Lin",
            "T. J. Lim"
        ],
        "summary": "We propose a unified framework for deriving and studying soft-in-soft-out (SISO) detection in interference channels using the concept of variational inference. The proposed framework may be used in multiple-access interference (MAI), inter-symbol interference (ISI), and multiple-input multiple-outpu (MIMO) channels. Without loss of generality, we will focus our attention on turbo multiuser detection, to facilitate a more concrete discussion. It is shown that, with some loss of optimality, variational inference avoids the exponential complexity of a posteriori probability (APP) detection by optimizing a closely-related, but much more manageable, objective function called variational free energy. In addition to its systematic appeal, there are several other advantages to this viewpoint. First of all, it provides unified and rigorous justifications for numerous detectors that were proposed on radically different grounds, and facilitates convenient joint detection and decoding (utilizing the turbo principle) when error-control codes are incorporated. Secondly, efficient joint parameter estimation and data detection is possible via the variational expectation maximization (EM) algorithm, such that the detrimental effect of inaccurate channel knowledge at the receiver may be dealt with systematically. We are also able to extend BPSK-based SISO detection schemes to arbitrary square QAM constellations in a rigorous manner using a variational argument.",
        "published": "2008-08-30T01:05:29Z",
        "link": "http://arxiv.org/abs/0809.0032v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks have been treated independently, using a wide variety of algorithms. These four semantic classes, however, are a tiny sample of the full range of semantic phenomena, and we cannot afford to create ad hoc algorithms for each semantic phenomenon; we need to seek a unified approach. We propose to subsume a broad range of phenomena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonym questions, ESL synonym-antonym questions, and similar-associated-both questions from cognitive psychology.",
        "published": "2008-08-31T14:00:26Z",
        "link": "http://arxiv.org/abs/0809.0124v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Principal Graphs and Manifolds",
        "authors": [
            "A. N. Gorban",
            "A. Y. Zinovyev"
        ],
        "summary": "In many physical, statistical, biological and other investigations it is desirable to approximate a system of points by objects of lower dimension and/or complexity. For this purpose, Karl Pearson invented principal component analysis in 1901 and found 'lines and planes of closest fit to system of points'. The famous k-means algorithm solves the approximation problem too, but by finite sets instead of lines and planes. This chapter gives a brief practical introduction into the methods of construction of general principal objects, i.e. objects embedded in the 'middle' of the multidimensional data set. As a basis, the unifying framework of mean squared distance approximation of finite datasets is selected. Principal graphs and manifolds are constructed as generalisations of principal components and k-means principal points. For this purpose, the family of expectation/maximisation algorithms with nearest generalisations is presented. Construction of principal graphs with controlled complexity is based on the graph grammar approach.",
        "published": "2008-09-02T18:04:53Z",
        "link": "http://arxiv.org/abs/0809.0490v2",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ]
    },
    {
        "title": "Quantum classification",
        "authors": [
            "Sébastien Gambs"
        ],
        "summary": "Quantum classification is defined as the task of predicting the associated class of an unknown quantum state drawn from an ensemble of pure states given a finite number of copies of this state. By recasting the state discrimination problem within the framework of Machine Learning (ML), we can use the notion of learning reduction coming from classical ML to solve different variants of the classification task, such as the weighted binary and the multiclass versions.",
        "published": "2008-09-02T19:56:54Z",
        "link": "http://arxiv.org/abs/0809.0444v2",
        "categories": [
            "quant-ph",
            "cs.LG"
        ]
    },
    {
        "title": "Entropy Concentration and the Empirical Coding Game",
        "authors": [
            "Peter Grunwald"
        ],
        "summary": "We give a characterization of Maximum Entropy/Minimum Relative Entropy inference by providing two `strong entropy concentration' theorems. These theorems unify and generalize Jaynes' `concentration phenomenon' and Van Campenhout and Cover's `conditional limit theorem'. The theorems characterize exactly in what sense a prior distribution Q conditioned on a given constraint, and the distribution P, minimizing the relative entropy D(P ||Q) over all distributions satisfying the constraint, are `close' to each other. We then apply our theorems to establish the relationship between entropy concentration and a game-theoretic characterization of Maximum Entropy Inference due to Topsoe and others.",
        "published": "2008-09-05T12:18:15Z",
        "link": "http://arxiv.org/abs/0809.1017v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.ST",
            "stat.ME",
            "stat.TH"
        ]
    },
    {
        "title": "Predictive Hypothesis Identification",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "While statistics focusses on hypothesis testing and on estimating (properties of) the true sampling distribution, in machine learning the performance of learning algorithms on future data is the primary issue. In this paper we bridge the gap with a general principle (PHI) that identifies hypotheses with best predictive performance. This includes predictive point and interval estimation, simple and composite hypothesis testing, (mixture) model selection, and others as special cases. For concrete instantiations we will recover well-known methods, variations thereof, and new ones. PHI nicely justifies, reconciles, and blends (a reparametrization invariant variation of) MAP, ML, MDL, and moment estimation. One particular feature of PHI is that it can genuinely deal with nested hypotheses.",
        "published": "2008-09-08T04:18:17Z",
        "link": "http://arxiv.org/abs/0809.1270v1",
        "categories": [
            "cs.LG",
            "math.ST",
            "stat.ML",
            "stat.TH"
        ]
    },
    {
        "title": "A New Framework of Multistage Estimation",
        "authors": [
            "Xinjia Chen"
        ],
        "summary": "In this paper, we have established a unified framework of multistage parameter estimation. We demonstrate that a wide variety of statistical problems such as fixed-sample-size interval estimation, point estimation with error control, bounded-width confidence intervals, interval estimation following hypothesis testing, construction of confidence sequences, can be cast into the general framework of constructing sequential random intervals with prescribed coverage probabilities. We have developed exact methods for the construction of such sequential random intervals in the context of multistage sampling. In particular, we have established inclusion principle and coverage tuning techniques to control and adjust the coverage probabilities of sequential random intervals. We have obtained concrete sampling schemes which are unprecedentedly efficient in terms of sampling effort as compared to existing procedures.",
        "published": "2008-09-08T14:03:24Z",
        "link": "http://arxiv.org/abs/0809.1241v35",
        "categories": [
            "math.ST",
            "cs.LG",
            "math.PR",
            "stat.ME",
            "stat.TH"
        ]
    },
    {
        "title": "Exploring Large Feature Spaces with Hierarchical Multiple Kernel   Learning",
        "authors": [
            "Francis Bach"
        ],
        "summary": "For supervised and unsupervised learning, positive definite kernels allow to use large and potentially infinite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the l1-norm or the block l1-norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efficiently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.",
        "published": "2008-09-09T06:48:10Z",
        "link": "http://arxiv.org/abs/0809.1493v1",
        "categories": [
            "cs.LG",
            "stat.ML"
        ]
    },
    {
        "title": "When is there a representer theorem? Vector versus matrix regularizers",
        "authors": [
            "Andreas Argyriou",
            "Charles Micchelli",
            "Massimiliano Pontil"
        ],
        "summary": "We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the inner product then the learned vector is a linear combination of the input data. This result, known as the {\\em representer theorem}, is at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, thereby completing the characterization of kernel methods based on regularization. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufficient condition for these class of matrix regularizers and highlight them with some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing function.",
        "published": "2008-09-09T16:11:12Z",
        "link": "http://arxiv.org/abs/0809.1590v1",
        "categories": [
            "cs.LG",
            "G.1.1; G.1.2; G.1.6; G.1.10; G.3; I.2.6"
        ]
    },
    {
        "title": "Clustered Multi-Task Learning: A Convex Formulation",
        "authors": [
            "Laurent Jacob",
            "Francis Bach",
            "Jean-Philippe Vert"
        ],
        "summary": "In multi-task learning several related tasks are considered simultaneously, with the hope that by an appropriate sharing of information across tasks, each task may benefit from the others. In the context of learning linear functions for supervised classification or regression, this can be achieved by including a priori information about the weight vectors associated with the tasks, and how they are expected to be related to each other. In this paper, we assume that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors. We design a new spectral norm that encodes this a priori assumption, without the prior knowledge of the partition of tasks into groups, resulting in a new convex optimization formulation for multi-task learning. We show in simulations on synthetic examples and on the IEDB MHC-I binding dataset, that our approach outperforms well-known convex methods for multi-task learning, as well as related non convex methods dedicated to the same problem.",
        "published": "2008-09-11T19:01:39Z",
        "link": "http://arxiv.org/abs/0809.2085v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Low congestion online routing and an improved mistake bound for online   prediction of graph labeling",
        "authors": [
            "Jittat Fakcharoenphol",
            "Boonserm Kijsirikul"
        ],
        "summary": "In this paper, we show a connection between a certain online low-congestion routing problem and an online prediction of graph labeling. More specifically, we prove that if there exists a routing scheme that guarantees a congestion of $\\alpha$ on any edge, there exists an online prediction algorithm with mistake bound $\\alpha$ times the cut size, which is the size of the cut induced by the label partitioning of graph vertices. With previous known bound of $O(\\log n)$ for $\\alpha$ for the routing problem on trees with $n$ vertices, we obtain an improved prediction algorithm for graphs with high effective resistance.   In contrast to previous approaches that move the graph problem into problems in vector space using graph Laplacian and rely on the analysis of the perceptron algorithm, our proof are purely combinatorial. Further more, our approach directly generalizes to the case where labels are not binary.",
        "published": "2008-09-11T19:32:49Z",
        "link": "http://arxiv.org/abs/0809.2075v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "cs.LG",
            "F.2.2"
        ]
    },
    {
        "title": "Algorithmic information theory",
        "authors": [
            "Peter D. Grunwald",
            "Paul M. B. Vitanyi"
        ],
        "summary": "We introduce algorithmic information theory, also known as the theory of Kolmogorov complexity. We explain the main concepts of this quantitative approach to defining `information'. We discuss the extent to which Kolmogorov's and Shannon's information theory have a common purpose, and where they are fundamentally different. We indicate how recent developments within the theory allow one to formally distinguish between `structural' (meaningful) and `random' information as measured by the Kolmogorov structure function, which leads to a mathematical formalization of Occam's razor in inductive inference. We end by discussing some of the philosophical implications of the theory.",
        "published": "2008-09-16T16:38:18Z",
        "link": "http://arxiv.org/abs/0809.2754v2",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Predicting Abnormal Returns From News Using Text Classification",
        "authors": [
            "Ronny Luss",
            "Alexandre d'Aspremont"
        ],
        "summary": "We show how text from news articles can be used to predict intraday price movements of financial assets using support vector machines. Multiple kernel learning is used to combine equity returns with text as predictive features to increase classification performance and we develop an analytic center cutting plane method to solve the kernel learning problem efficiently. We observe that while the direction of returns is not predictable using either text or returns, their size is, with text features producing significantly better performance than historical returns alone.",
        "published": "2008-09-16T20:05:00Z",
        "link": "http://arxiv.org/abs/0809.2792v3",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "A New Framework of Multistage Hypothesis Tests",
        "authors": [
            "Xinjia Chen"
        ],
        "summary": "In this paper, we have established a general framework of multistage hypothesis tests which applies to arbitrarily many mutually exclusive and exhaustive composite hypotheses. Within the new framework, we have constructed specific multistage tests which rigorously control the risk of committing decision errors and are more efficient than previous tests in terms of average sample number and the number of sampling operations. Without truncation, the sample numbers of our testing plans are absolutely bounded.",
        "published": "2008-09-18T14:25:06Z",
        "link": "http://arxiv.org/abs/0809.3170v25",
        "categories": [
            "math.ST",
            "cs.LG",
            "math.PR",
            "stat.ME",
            "stat.TH"
        ]
    },
    {
        "title": "Generalized Prediction Intervals for Arbitrary Distributed   High-Dimensional Data",
        "authors": [
            "Steffen Kuehn"
        ],
        "summary": "This paper generalizes the traditional statistical concept of prediction intervals for arbitrary probability density functions in high-dimensional feature spaces by introducing significance level distributions, which provides interval-independent probabilities for continuous random variables. The advantage of the transformation of a probability density function into a significance level distribution is that it enables one-class classification or outlier detection in a direct manner.",
        "published": "2008-09-19T11:02:39Z",
        "link": "http://arxiv.org/abs/0809.3352v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Robust Near-Isometric Matching via Structured Learning of Graphical   Models",
        "authors": [
            "Julian J. McAuley",
            "Tiberio S. Caetano",
            "Alexander J. Smola"
        ],
        "summary": "Models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the isometric assumption. However, real shapes from image datasets, even when expected to be related by \"almost isometric\" transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale. In this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction. The outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations. Our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times.",
        "published": "2008-09-21T23:23:26Z",
        "link": "http://arxiv.org/abs/0809.3618v1",
        "categories": [
            "cs.CV",
            "cs.LG"
        ]
    },
    {
        "title": "Learning Hidden Markov Models using Non-Negative Matrix Factorization",
        "authors": [
            "George Cybenko",
            "Valentino Crespi"
        ],
        "summary": "The Baum-Welsh algorithm together with its derivatives and variations has been the main technique for learning Hidden Markov Models (HMM) from observational data. We present an HMM learning algorithm based on the non-negative matrix factorization (NMF) of higher order Markovian statistics that is structurally different from the Baum-Welsh and its associated approaches. The described algorithm supports estimation of the number of recurrent states of an HMM and iterates the non-negative matrix factorization (NMF) algorithm to improve the learned HMM parameters. Numerical examples are provided as well.",
        "published": "2008-09-24T05:34:56Z",
        "link": "http://arxiv.org/abs/0809.4086v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Surrogate Learning - An Approach for Semi-Supervised Classification",
        "authors": [
            "Sriharsha Veeramachaneni",
            "Ravikumar Kondadadi"
        ],
        "summary": "We consider the task of learning a classifier from the feature space $\\mathcal{X}$ to the set of classes $\\mathcal{Y} = \\{0, 1\\}$, when the features can be partitioned into class-conditionally independent feature sets $\\mathcal{X}_1$ and $\\mathcal{X}_2$. We show the surprising fact that the class-conditional independence can be used to represent the original learning task in terms of 1) learning a classifier from $\\mathcal{X}_2$ to $\\mathcal{X}_1$ and 2) learning the class-conditional distribution of the feature set $\\mathcal{X}_1$. This fact can be exploited for semi-supervised learning because the former task can be accomplished purely from unlabeled samples. We present experimental evaluation of the idea in two real world applications.",
        "published": "2008-09-26T13:47:36Z",
        "link": "http://arxiv.org/abs/0809.4632v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Multi-Armed Bandits in Metric Spaces",
        "authors": [
            "Robert Kleinberg",
            "Aleksandrs Slivkins",
            "Eli Upfal"
        ],
        "summary": "In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of trials so as to maximize the total payoff of the chosen strategies. While the performance of bandit algorithms with a small finite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. The goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efficient solutions. In this work we study a very general setting for the multi-armed bandit problem in which the strategies form a metric space, and the payoff function satisfies a Lipschitz condition with respect to the metric. We refer to this problem as the \"Lipschitz MAB problem\". We present a complete solution for the multi-armed problem in this setting. That is, for every metric space (L,X) we define an isometry invariant which bounds from below the performance of Lipschitz MAB algorithms for X, and we present an algorithm which comes arbitrarily close to meeting this bound. Furthermore, our technique gives even better results for benign payoff functions.",
        "published": "2008-09-29T01:58:13Z",
        "link": "http://arxiv.org/abs/0809.4882v1",
        "categories": [
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Thresholded Basis Pursuit: An LP Algorithm for Achieving Optimal Support   Recovery for Sparse and Approximately Sparse Signals from Noisy Random   Measurements",
        "authors": [
            "V. Saligrama",
            "M. Zhao"
        ],
        "summary": "In this paper we present a linear programming solution for sign pattern recovery of a sparse signal from noisy random projections of the signal. We consider two types of noise models, input noise, where noise enters before the random projection; and output noise, where noise enters after the random projection. Sign pattern recovery involves the estimation of sign pattern of a sparse signal. Our idea is to pretend that no noise exists and solve the noiseless $\\ell_1$ problem, namely, $\\min \\|\\beta\\|_1 ~ s.t. ~ y=G \\beta$ and quantizing the resulting solution. We show that the quantized solution perfectly reconstructs the sign pattern of a sufficiently sparse signal. Specifically, we show that the sign pattern of an arbitrary k-sparse, n-dimensional signal $x$ can be recovered with $SNR=\\Omega(\\log n)$ and measurements scaling as $m= \\Omega(k \\log{n/k})$ for all sparsity levels $k$ satisfying $0< k \\leq \\alpha n$, where $\\alpha$ is a sufficiently small positive constant. Surprisingly, this bound matches the optimal \\emph{Max-Likelihood} performance bounds in terms of $SNR$, required number of measurements, and admissible sparsity level in an order-wise sense. In contrast to our results, previous results based on LASSO and Max-Correlation techniques either assume significantly larger $SNR$, sublinear sparsity levels or restrictive assumptions on signal sets. Our proof technique is based on noisy perturbation of the noiseless $\\ell_1$ problem, in that, we estimate the maximum admissible noise level before sign pattern recovery fails.",
        "published": "2008-09-29T14:01:13Z",
        "link": "http://arxiv.org/abs/0809.4883v3",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Bias-Variance Techniques for Monte Carlo Optimization: Cross-validation   for the CE Method",
        "authors": [
            "Dev Rajnarayan",
            "David Wolpert"
        ],
        "summary": "In this paper, we examine the CE method in the broad context of Monte Carlo Optimization (MCO) and Parametric Learning (PL), a type of machine learning. A well-known overarching principle used to improve the performance of many PL algorithms is the bias-variance tradeoff. This tradeoff has been used to improve PL algorithms ranging from Monte Carlo estimation of integrals, to linear estimation, to general statistical estimation. Moreover, as described by, MCO is very closely related to PL. Owing to this similarity, the bias-variance tradeoff affects MCO performance, just as it does PL performance.   In this article, we exploit the bias-variance tradeoff to enhance the performance of MCO algorithms. We use the technique of cross-validation, a technique based on the bias-variance tradeoff, to significantly improve the performance of the Cross Entropy (CE) method, which is an MCO algorithm. In previous work we have confirmed that other PL techniques improve the perfomance of other MCO algorithms. We conclude that the many techniques pioneered in PL could be investigated as ways to improve MCO algorithms in general, and the CE method in particular.",
        "published": "2008-10-06T04:58:44Z",
        "link": "http://arxiv.org/abs/0810.0877v1",
        "categories": [
            "cs.NA",
            "cs.LG"
        ]
    },
    {
        "title": "Blind Cognitive MAC Protocols",
        "authors": [
            "Omar Mehanna",
            "Ahmed Sultan",
            "Hesham El Gamal"
        ],
        "summary": "We consider the design of cognitive Medium Access Control (MAC) protocols enabling an unlicensed (secondary) transmitter-receiver pair to communicate over the idle periods of a set of licensed channels, i.e., the primary network. The objective is to maximize data throughput while maintaining the synchronization between secondary users and avoiding interference with licensed (primary) users. No statistical information about the primary traffic is assumed to be available a-priori to the secondary user. We investigate two distinct sensing scenarios. In the first, the secondary transmitter is capable of sensing all the primary channels, whereas it senses one channel only in the second scenario. In both cases, we propose MAC protocols that efficiently learn the statistics of the primary traffic online. Our simulation results demonstrate that the proposed blind protocols asymptotically achieve the throughput obtained when prior knowledge of primary traffic statistics is available.",
        "published": "2008-10-08T13:22:46Z",
        "link": "http://arxiv.org/abs/0810.1430v1",
        "categories": [
            "cs.NI",
            "cs.LG",
            "C.2.2; I.2.6; I.2.0"
        ]
    },
    {
        "title": "A Gaussian Belief Propagation Solver for Large Scale Support Vector   Machines",
        "authors": [
            "Danny Bickson",
            "Elad Yom-Tov",
            "Danny Dolev"
        ],
        "summary": "Support vector machines (SVMs) are an extremely successful type of classification and regression algorithms. Building an SVM entails solving a constrained convex quadratic programming problem, which is quadratic in the number of training samples. We introduce an efficient parallel implementation of an support vector regression solver, based on the Gaussian Belief Propagation algorithm (GaBP).   In this paper, we demonstrate that methods from the complex system domain could be utilized for performing efficient distributed computation. We compare the proposed algorithm to previously proposed distributed and single-node SVM solvers. Our comparison shows that the proposed algorithm is just as accurate as these solvers, while being significantly faster, especially for large datasets. We demonstrate scalability of the proposed algorithm to up to 1,024 computing nodes and hundreds of thousands of data points using an IBM Blue Gene supercomputer. As far as we know, our work is the largest parallel implementation of belief propagation ever done, demonstrating the applicability of this algorithm for large scale distributed computing systems.",
        "published": "2008-10-09T12:56:43Z",
        "link": "http://arxiv.org/abs/0810.1648v1",
        "categories": [
            "cs.LG",
            "cs.IT",
            "math.IT",
            "I.2.6"
        ]
    },
    {
        "title": "Faster and better: a machine learning approach to corner detection",
        "authors": [
            "Edward Rosten",
            "Reid Porter",
            "Tom Drummond"
        ],
        "summary": "The repeatability and efficiency of a corner detector determines how likely it is to be useful in a real-world application. The repeatability is importand because the same scene viewed from different positions should yield features which correspond to the same real-world 3D locations [Schmid et al 2000]. The efficiency is important because this determines whether the detector combined with further processing can operate at frame rate.   Three advances are described in this paper. First, we present a new heuristic for feature detection, and using machine learning we derive a feature detector from this which can fully process live PAL video using less than 5% of the available processing time. By comparison, most other detectors cannot even operate at frame rate (Harris detector 115%, SIFT 195%). Second, we generalize the detector, allowing it to be optimized for repeatability, with little loss of efficiency. Third, we carry out a rigorous comparison of corner detectors based on the above repeatability criterion applied to 3D scenes. We show that despite being principally constructed for speed, on these stringent tests, our heuristic detector significantly outperforms existing feature detectors. Finally, the comparison demonstrates that using machine learning produces significant improvements in repeatability, yielding a detector that is both very fast and very high quality.",
        "published": "2008-10-14T14:22:05Z",
        "link": "http://arxiv.org/abs/0810.2434v1",
        "categories": [
            "cs.CV",
            "cs.LG"
        ]
    },
    {
        "title": "A Simple Linear Ranking Algorithm Using Query Dependent Intercept   Variables",
        "authors": [
            "Nir Ailon"
        ],
        "summary": "The LETOR website contains three information retrieval datasets used as a benchmark for testing machine learning ideas for ranking. Algorithms participating in the challenge are required to assign score values to search results for a collection of queries, and are measured using standard IR ranking measures (NDCG, precision, MAP) that depend only the relative score-induced order of the results. Similarly to many of the ideas proposed in the participating algorithms, we train a linear classifier. In contrast with other participating algorithms, we define an additional free variable (intercept, or benchmark) for each query. This allows expressing the fact that results for different queries are incomparable for the purpose of determining relevance. The cost of this idea is the addition of relatively few nuisance parameters. Our approach is simple, and we used a standard logistic regression library to test it. The results beat the reported participating algorithms. Hence, it seems promising to combine our approach with other more complex ideas.",
        "published": "2008-10-15T19:03:10Z",
        "link": "http://arxiv.org/abs/0810.2764v1",
        "categories": [
            "cs.IR",
            "cs.LG"
        ]
    },
    {
        "title": "The many faces of optimism - Extended version",
        "authors": [
            "István Szita",
            "András Lőrincz"
        ],
        "summary": "The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learning. \"Optimism in the face of uncertainty\" and model building play central roles in advanced exploration methods. Here, we integrate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in polynomial time, and give experimental evidence that it is robust and efficient compared to its ascendants.",
        "published": "2008-10-20T02:09:16Z",
        "link": "http://arxiv.org/abs/0810.3451v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LG"
        ]
    },
    {
        "title": "The use of entropy to measure structural diversity",
        "authors": [
            "L. Masisi",
            "V. Nelwamondo",
            "T. Marwala"
        ],
        "summary": "In this paper entropy based methods are compared and used to measure structural diversity of an ensemble of 21 classifiers. This measure is mostly applied in ecology, whereby species counts are used as a measure of diversity. The measures used were Shannon entropy, Simpsons and the Berger Parker diversity indexes. As the diversity indexes increased so did the accuracy of the ensemble. An ensemble dominated by classifiers with the same structure produced poor accuracy. Uncertainty rule from information theory was also used to further define diversity. Genetic algorithms were used to find the optimal ensemble by using the diversity indices as the cost function. The method of voting was used to aggregate the decisions.",
        "published": "2008-10-20T11:09:15Z",
        "link": "http://arxiv.org/abs/0810.3525v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "q-bio.QM"
        ]
    },
    {
        "title": "A Minimum Relative Entropy Principle for Learning and Acting",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun"
        ],
        "summary": "This paper proposes a method to construct an adaptive agent that is universal with respect to a given class of experts, where each expert is an agent that has been designed specifically for a particular environment. This adaptive control problem is formalized as the problem of minimizing the relative entropy of the adaptive agent from the expert that is most suitable for the unknown environment. If the agent is a passive observer, then the optimal solution is the well-known Bayesian predictor. However, if the agent is active, then its past actions need to be treated as causal interventions on the I/O stream rather than normal probability conditions. Here it is shown that the solution to this new variational problem is given by a stochastic controller called the Bayesian control rule, which implements adaptive behavior as a mixture of experts. Furthermore, it is shown that under mild assumptions, the Bayesian control rule converges to the control law of the most suitable expert.",
        "published": "2008-10-20T16:47:47Z",
        "link": "http://arxiv.org/abs/0810.3605v3",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Quantum reinforcement learning",
        "authors": [
            "Daoyi Dong",
            "Chunlin Chen",
            "Hanxiong Li",
            "Tzyh-Jong Tarn"
        ],
        "summary": "The key approaches for machine learning, especially learning in unknown probabilistic environments are new representations and computation mechanisms. In this paper, a novel quantum reinforcement learning (QRL) method is proposed by combining quantum theory and reinforcement learning (RL). Inspired by the state superposition principle and quantum parallelism, a framework of value updating algorithm is introduced. The state (action) in traditional RL is identified as the eigen state (eigen action) in QRL. The state (action) set can be represented with a quantum superposition state and the eigen state (eigen action) can be obtained by randomly observing the simulated quantum state according to the collapse postulate of quantum measurement. The probability of the eigen action is determined by the probability amplitude, which is parallelly updated according to rewards. Some related characteristics of QRL such as convergence, optimality and balancing between exploration and exploitation are also analyzed, which shows that this approach makes a good tradeoff between exploration and exploitation using the probability amplitude and can speed up learning through the quantum parallelism. To evaluate the performance and practicability of QRL, several simulated experiments are given and the results demonstrate the effectiveness and superiority of QRL algorithm for some complex problems. The present work is also an effective exploration on the application of quantum computation to artificial intelligence.",
        "published": "2008-10-21T13:38:33Z",
        "link": "http://arxiv.org/abs/0810.3828v1",
        "categories": [
            "quant-ph",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Efficient Exact Inference in Planar Ising Models",
        "authors": [
            "Nicol N. Schraudolph",
            "Dmitry Kamenetsky"
        ],
        "summary": "We give polynomial-time algorithms for the exact computation of lowest-energy (ground) states, worst margin violators, log partition functions, and marginal edge probabilities in certain binary undirected graphical models. Our approach provides an interesting alternative to the well-known graph cut paradigm in that it does not impose any submodularity constraints; instead we require planarity to establish a correspondence with perfect matchings (dimer coverings) in an expanded dual graph. We implement a unified framework while delegating complex but well-understood subproblems (planar embedding, maximum-weight perfect matching) to established algorithms for which efficient implementations are freely available. Unlike graph cut methods, we can perform penalized maximum-likelihood as well as maximum-margin parameter estimation in the associated conditional random fields (CRFs), and employ marginal posterior probabilities as well as maximum a posteriori (MAP) states for prediction. Maximum-margin CRF parameter estimation on image denoising and segmentation problems shows our approach to be efficient and effective. A C++ implementation is available from http://nic.schraudolph.org/isinf/",
        "published": "2008-10-24T08:49:09Z",
        "link": "http://arxiv.org/abs/0810.4401v2",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ]
    },
    {
        "title": "Learning Isometric Separation Maps",
        "authors": [
            "Nikolaos Vasiloglou",
            "Alexander G. Gray",
            "David V. Anderson"
        ],
        "summary": "Maximum Variance Unfolding (MVU) and its variants have been very successful in embedding data-manifolds in lower dimensional spaces, often revealing the true intrinsic dimension. In this paper we show how to also incorporate supervised class information into an MVU-like method without breaking its convexity. We call this method the Isometric Separation Map and we show that the resulting kernel matrix can be used as a binary/multiclass Support Vector Machine-like method in a semi-supervised (transductive) framework. We also show that the method always finds a kernel matrix that linearly separates the training data exactly without projecting them in infinite dimensional spaces. In traditional SVMs we choose a kernel and hope that the data become linearly separable in the kernel space. In this paper we show how the hyperplane can be chosen ad-hoc and the kernel is trained so that data are always linearly separable. Comparisons with Large Margin SVMs show comparable performance.",
        "published": "2008-10-25T15:09:28Z",
        "link": "http://arxiv.org/abs/0810.4611v2",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "A Novel Clustering Algorithm Based on a Modified Model of Random Walk",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "summary": "We introduce a modified model of random walk, and then develop two novel clustering algorithms based on it. In the algorithms, each data point in a dataset is considered as a particle which can move at random in space according to the preset rules in the modified model. Further, this data point may be also viewed as a local control subsystem, in which the controller adjusts its transition probability vector in terms of the feedbacks of all data points, and then its transition direction is identified by an event-generating function. Finally, the positions of all data points are updated. As they move in space, data points collect gradually and some separating parts emerge among them automatically. As a consequence, data points that belong to the same class are located at a same position, whereas those that belong to different classes are away from one another. Moreover, the experimental results have demonstrated that data points in the test datasets are clustered reasonably and efficiently, and the comparison with other algorithms also provides an indication of the effectiveness of the proposed algorithms.",
        "published": "2008-10-30T13:26:31Z",
        "link": "http://arxiv.org/abs/0810.5484v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "A Theory of Truncated Inverse Sampling",
        "authors": [
            "Xinjia Chen"
        ],
        "summary": "In this paper, we have established a new framework of truncated inverse sampling for estimating mean values of non-negative random variables such as binomial, Poisson, hyper-geometrical, and bounded variables. We have derived explicit formulas and computational methods for designing sampling schemes to ensure prescribed levels of precision and confidence for point estimators. Moreover, we have developed interval estimation methods.",
        "published": "2008-10-30T19:52:55Z",
        "link": "http://arxiv.org/abs/0810.5551v2",
        "categories": [
            "math.ST",
            "cs.LG",
            "math.PR",
            "stat.ME",
            "stat.TH"
        ]
    },
    {
        "title": "A branch-and-bound feature selection algorithm for U-shaped cost   functions",
        "authors": [
            "Marcelo Ris",
            "Junior Barrera",
            "David C. Martins Jr"
        ],
        "summary": "This paper presents the formulation of a combinatorial optimization problem with the following characteristics: i.the search space is the power set of a finite set structured as a Boolean lattice; ii.the cost function forms a U-shaped curve when applied to any lattice chain. This formulation applies for feature selection in the context of pattern recognition. The known approaches for this problem are branch-and-bound algorithms and heuristics, that explore partially the search space. Branch-and-bound algorithms are equivalent to the full search, while heuristics are not. This paper presents a branch-and-bound algorithm that differs from the others known by exploring the lattice structure and the U-shaped chain curves of the search space. The main contribution of this paper is the architecture of this algorithm that is based on the representation and exploration of the search space by new lattice properties proven here. Several experiments, with well known public data, indicate the superiority of the proposed method to SFFS, which is a popular heuristic that gives good results in very short computational time. In all experiments, the proposed method got better or equal results in similar or even smaller computational time.",
        "published": "2008-10-30T20:24:28Z",
        "link": "http://arxiv.org/abs/0810.5573v1",
        "categories": [
            "cs.CV",
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Temporal Difference Updating without a Learning Rate",
        "authors": [
            "Marcus Hutter",
            "Shane Legg"
        ],
        "summary": "We derive an equation for temporal difference learning from statistical principles. Specifically, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(lambda), however it lacks the parameter alpha that specifies the learning rate. In the place of this free parameter there is now an equation for the learning rate that is specific to each state transition. We experimentally test this new learning rule against TD(lambda) and find that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins' Q(lambda) and Sarsa(lambda) and find that it again offers superior performance without a learning rate parameter.",
        "published": "2008-10-31T07:15:01Z",
        "link": "http://arxiv.org/abs/0810.5631v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "On the Possibility of Learning in Reactive Environments with Arbitrary   Dependence",
        "authors": [
            "Daniil Ryabko",
            "Marcus Hutter"
        ],
        "summary": "We address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions, i.e. environments more general than (PO)MDPs. The task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments. We find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class. We analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields, such as Markov Decision Processes and mixing conditions.",
        "published": "2008-10-31T07:58:31Z",
        "link": "http://arxiv.org/abs/0810.5636v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Entropy, Perception, and Relativity",
        "authors": [
            "Stefan Jaeger"
        ],
        "summary": "In this paper, I expand Shannon's definition of entropy into a new form of entropy that allows integration of information from different random events. Shannon's notion of entropy is a special case of my more general definition of entropy. I define probability using a so-called performance function, which is de facto an exponential distribution. Assuming that my general notion of entropy reflects the true uncertainty about a probabilistic event, I understand that our perceived uncertainty differs. I claim that our perception is the result of two opposing forces similar to the two famous antagonists in Chinese philosophy: Yin and Yang. Based on this idea, I show that our perceived uncertainty matches the true uncertainty in points determined by the golden ratio. I demonstrate that the well-known sigmoid function, which we typically employ in artificial neural networks as a non-linear threshold function, describes the actual performance. Furthermore, I provide a motivation for the time dilation in Einstein's Special Relativity, basically claiming that although time dilation conforms with our perception, it does not correspond to reality. At the end of the paper, I show how to apply this theoretical framework to practical applications. I present recognition rates for a pattern recognition problem, and also propose a network architecture that can take advantage of general entropy to solve complex decision problems.",
        "published": "2008-11-02T08:02:43Z",
        "link": "http://arxiv.org/abs/0811.0139v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Effect of Tuned Parameters on a LSA MCQ Answering Model",
        "authors": [
            "Alain Lifchitz",
            "Sandra Jhean-Larose",
            "Guy Denhière"
        ],
        "summary": "This paper presents the current state of a work in progress, whose objective is to better understand the effects of factors that significantly influence the performance of Latent Semantic Analysis (LSA). A difficult task, which consists in answering (French) biology Multiple Choice Questions, is used to test the semantic properties of the truncated singular space and to study the relative influence of main parameters. A dedicated software has been designed to fine tune the LSA semantic space for the Multiple Choice Questions task. With optimal parameters, the performances of our simple model are quite surprisingly equal or superior to those of 7th and 8th grades students. This indicates that semantic spaces were quite good despite their low dimensions and the small sizes of training data sets. Besides, we present an original entropy global weighting of answers' terms of each question of the Multiple Choice Questions which was necessary to achieve the model's success.",
        "published": "2008-11-02T09:21:40Z",
        "link": "http://arxiv.org/abs/0811.0146v3",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ]
    },
    {
        "title": "Adaptive Base Class Boost for Multi-class Classification",
        "authors": [
            "Ping Li"
        ],
        "summary": "We develop the concept of ABC-Boost (Adaptive Base Class Boost) for multi-class classification and present ABC-MART, a concrete implementation of ABC-Boost. The original MART (Multiple Additive Regression Trees) algorithm has been very successful in large-scale applications. For binary classification, ABC-MART recovers MART. For multi-class classification, ABC-MART considerably improves MART, as evaluated on several public data sets.",
        "published": "2008-11-08T23:23:08Z",
        "link": "http://arxiv.org/abs/0811.1250v1",
        "categories": [
            "cs.LG",
            "cs.IR"
        ]
    },
    {
        "title": "Stability Bound for Stationary Phi-mixing and Beta-mixing Processes",
        "authors": [
            "Mehryar Mohri",
            "Afshin Rostamizadeh"
        ],
        "summary": "Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm. In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties. However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed. In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence.   This paper studies the scenario where the observations are drawn from a stationary phi-mixing or beta-mixing sequence, a widely adopted assumption in the study of non-i.i.d. processes that implies a dependence between observations weakening over time. We prove novel and distinct stability-based generalization bounds for stationary phi-mixing and beta-mixing sequences. These bounds strictly generalize the bounds given in the i.i.d. case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-i.i.d. scenarios.   We also illustrate the application of our phi-mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms. These novel bounds can thus be viewed as the first theoretical basis for the use of these algorithms in non-i.i.d. scenarios.",
        "published": "2008-11-11T05:09:08Z",
        "link": "http://arxiv.org/abs/0811.1629v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Robust Regression and Lasso",
        "authors": [
            "Huan Xu",
            "Constantine Caramanis",
            "Shie Mannor"
        ],
        "summary": "Lasso, or $\\ell^1$ regularized least squares, has been explored extensively for its remarkable sparsity properties. It is shown in this paper that the solution to Lasso, in addition to its sparsity, has robustness properties: it is the solution to a robust optimization problem. This has two important consequences. First, robustness provides a connection of the regularizer to a physical property, namely, protection from noise. This allows a principled selection of the regularizer, and in particular, generalizations of Lasso that also yield convex optimization problems are obtained by considering different uncertainty sets.   Secondly, robustness can itself be used as an avenue to exploring different properties of the solution. In particular, it is shown that robustness of the solution explains why the solution is sparse. The analysis as well as the specific results obtained differ from standard sparsity results, providing different geometric intuition. Furthermore, it is shown that the robust optimization formulation is related to kernel density estimation, and based on this approach, a proof that Lasso is consistent is given using robustness directly. Finally, a theorem saying that sparsity and algorithmic stability contradict each other, and hence Lasso is not stable, is presented.",
        "published": "2008-11-11T22:46:10Z",
        "link": "http://arxiv.org/abs/0811.1790v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Land Cover Mapping Using Ensemble Feature Selection Methods",
        "authors": [
            "A. Gidudu",
            "B. Abe",
            "T. Marwala"
        ],
        "summary": "Ensemble classification is an emerging approach to land cover mapping whereby the final classification output is a result of a consensus of classifiers. Intuitively, an ensemble system should consist of base classifiers which are diverse i.e. classifiers whose decision boundaries err differently. In this paper ensemble feature selection is used to impose diversity in ensembles. The features of the constituent base classifiers for each ensemble were created through an exhaustive search algorithm using different separability indices. For each ensemble, the classification accuracy was derived as well as a diversity measure purported to give a measure of the inensemble diversity. The correlation between ensemble classification accuracy and diversity measure was determined to establish the interplay between the two variables. From the findings of this paper, diversity measures as currently formulated do not provide an adequate means upon which to constitute ensembles for land cover mapping.",
        "published": "2008-11-13T01:23:47Z",
        "link": "http://arxiv.org/abs/0811.2016v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "A Spectral Algorithm for Learning Hidden Markov Models",
        "authors": [
            "Daniel Hsu",
            "Sham M. Kakade",
            "Tong Zhang"
        ],
        "summary": "Hidden Markov Models (HMMs) are one of the most fundamental and widely used statistical tools for modeling discrete time series. In general, learning HMMs from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues. We prove that under a natural separation condition (bounds on the smallest singular value of the HMM parameters), there is an efficient and provably correct algorithm for learning HMMs. The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations---it implicitly depends on this quantity through spectral properties of the underlying HMM. This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language. The algorithm is also simple, employing only a singular value decomposition and matrix multiplications.",
        "published": "2008-11-26T20:22:51Z",
        "link": "http://arxiv.org/abs/0811.4413v6",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Learning Class-Level Bayes Nets for Relational Data",
        "authors": [
            "Oliver Schulte",
            "Hassan Khosravi",
            "Flavia Moser",
            "Martin Ester"
        ],
        "summary": "Many databases store data in relational format, with different types of entities and information about links between the entities. The field of statistical-relational learning (SRL) has developed a number of new statistical models for such data. In this paper we focus on learning class-level or first-order dependencies, which model the general database statistics over attributes of linked objects and links (e.g., the percentage of A grades given in computer science classes). Class-level statistical relationships are important in themselves, and they support applications like policy making, strategic planning, and query optimization. Most current SRL methods find class-level dependencies, but their main task is to support instance-level predictions about the attributes or links of specific entities. We focus only on class-level prediction, and describe algorithms for learning class-level models that are orders of magnitude faster for this task. Our algorithms learn Bayes nets with relational structure, leveraging the efficiency of single-table nonrelational Bayes net learners. An evaluation of our methods on three data sets shows that they are computationally feasible for realistic table sizes, and that the learned structures represent the statistical information in the databases well. After learning compiles the database statistics into a Bayes net, querying these statistics via Bayes net inference is faster than with SQL queries, and does not depend on the size of the database.",
        "published": "2008-11-27T01:02:33Z",
        "link": "http://arxiv.org/abs/0811.4458v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "k-means requires exponentially many iterations even in the plane",
        "authors": [
            "Andrea Vattani"
        ],
        "summary": "The k-means algorithm is a well-known method for partitioning n points that lie in the d-dimensional space into k clusters. Its main features are simplicity and speed in practice. Theoretically, however, the best known upper bound on its running time (i.e. O(n^{kd})) can be exponential in the number of points. Recently, Arthur and Vassilvitskii [3] showed a super-polynomial worst-case analysis, improving the best known lower bound from \\Omega(n) to 2^{\\Omega(\\sqrt{n})} with a construction in d=\\Omega(\\sqrt{n}) dimensions. In [3] they also conjectured the existence of superpolynomial lower bounds for any d >= 2.   Our contribution is twofold: we prove this conjecture and we improve the lower bound, by presenting a simple construction in the plane that leads to the exponential lower bound 2^{\\Omega(n)}.",
        "published": "2008-12-01T22:55:39Z",
        "link": "http://arxiv.org/abs/0812.0382v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Approximation Algorithms for Bregman Co-clustering and Tensor Clustering",
        "authors": [
            "Stefanie Jegelka",
            "Suvrit Sra",
            "Arindam Banerjee"
        ],
        "summary": "In the past few years powerful generalizations to the Euclidean k-means problem have been made, such as Bregman clustering [7], co-clustering (i.e., simultaneous clustering of rows and columns of an input matrix) [9,18], and tensor clustering [8,34]. Like k-means, these more general problems also suffer from the NP-hardness of the associated optimization. Researchers have developed approximation algorithms of varying degrees of sophistication for k-means, k-medians, and more recently also for Bregman clustering [2]. However, there seem to be no approximation algorithms for Bregman co- and tensor clustering. In this paper we derive the first (to our knowledge) guaranteed methods for these increasingly important clustering settings. Going beyond Bregman divergences, we also prove an approximation factor for tensor clustering with arbitrary separable metrics. Through extensive experiments we evaluate the characteristics of our method, and show that it also has practical impact.",
        "published": "2008-12-01T23:17:35Z",
        "link": "http://arxiv.org/abs/0812.0389v4",
        "categories": [
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "A Novel Clustering Algorithm Based on Quantum Games",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "summary": "Enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum game with the problem of data clustering, and then develop a quantum-game-based clustering algorithm, in which data points in a dataset are considered as players who can make decisions and implement quantum strategies in quantum games. After each round of a quantum game, each player's expected payoff is calculated. Later, he uses a link-removing-and-rewiring (LRR) function to change his neighbors and adjust the strength of links connecting to them in order to maximize his payoff. Further, algorithms are discussed and analyzed in two cases of strategies, two payoff matrixes and two LRR functions. Consequently, the simulation results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms have fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.",
        "published": "2008-12-03T15:46:03Z",
        "link": "http://arxiv.org/abs/0812.0743v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.GT",
            "cs.MA",
            "cs.NE",
            "quant-ph"
        ]
    },
    {
        "title": "Decision trees are PAC-learnable from most product distributions: a   smoothed analysis",
        "authors": [
            "Adam Tauman Kalai",
            "Shang-Hua Teng"
        ],
        "summary": "We consider the problem of PAC-learning decision trees, i.e., learning a decision tree over the n-dimensional hypercube from independent random labeled examples. Despite significant effort, no polynomial-time algorithm is known for learning polynomial-sized decision trees (even trees of any super-constant size), even when examples are assumed to be drawn from the uniform distribution on {0,1}^n. We give an algorithm that learns arbitrary polynomial-sized decision trees for {\\em most product distributions}. In particular, consider a random product distribution where the bias of each bit is chosen independently and uniformly from, say, [.49,.51]. Then with high probability over the parameters of the product distribution and the random examples drawn from it, the algorithm will learn any tree. More generally, in the spirit of smoothed analysis, we consider an arbitrary product distribution whose parameters are specified only up to a [-c,c] accuracy (perturbation), for an arbitrarily small positive constant c.",
        "published": "2008-12-04T13:34:26Z",
        "link": "http://arxiv.org/abs/0812.0933v1",
        "categories": [
            "cs.LG",
            "cs.CC"
        ]
    },
    {
        "title": "Uncovering protein interaction in abstracts and text using a novel   linear model and word proximity networks",
        "authors": [
            "Alaa Abi-Haidar",
            "Jasleen Kaur",
            "Ana G. Maguitman",
            "Predrag Radivojac",
            "Andreas Retchsteiner",
            "Karin Verspoor",
            "Zhiping Wang",
            "Luis M. Rocha"
        ],
        "summary": "We participated in three of the protein-protein interaction subtasks of the Second BioCreative Challenge: classification of abstracts relevant for protein-protein interaction (IAS), discovery of protein pairs (IPS) and text passages characterizing protein interaction (ISS) in full text documents. We approached the abstract classification task with a novel, lightweight linear model inspired by spam-detection techniques, as well as an uncertainty-based integration scheme. We also used a Support Vector Machine and the Singular Value Decomposition on the same features for comparison purposes. Our approach to the full text subtasks (protein pair and passage identification) includes a feature expansion method based on word-proximity networks. Our approach to the abstract classification task (IAS) was among the top submissions for this task in terms of the measures of performance used in the challenge evaluation (accuracy, F-score and AUC). We also report on a web-tool we produced using our approach: the Protein Interaction Abstract Relevance Evaluator (PIARE). Our approach to the full text tasks resulted in one of the highest recall rates as well as mean reciprocal rank of correct passages. Our approach to abstract classification shows that a simple linear model, using relatively few features, is capable of generalizing and uncovering the conceptual nature of protein-protein interaction from the bibliome. Since the novel approach is based on a very lightweight linear model, it can be easily ported and applied to similar problems. In full text problems, the expansion of word features with word-proximity networks is shown to be useful, though the need for some improvements is discussed.",
        "published": "2008-12-04T21:37:35Z",
        "link": "http://arxiv.org/abs/0812.1029v1",
        "categories": [
            "cs.IR",
            "cs.LG"
        ]
    },
    {
        "title": "Decomposition Principles and Online Learning in Cross-Layer Optimization   for Delay-Sensitive Applications",
        "authors": [
            "Fangwen Fu",
            "Mihaela van der Schaar"
        ],
        "summary": "In this paper, we propose a general cross-layer optimization framework in which we explicitly consider both the heterogeneous and dynamically changing characteristics of delay-sensitive applications and the underlying time-varying network conditions. We consider both the independently decodable data units (DUs, e.g. packets) and the interdependent DUs whose dependencies are captured by a directed acyclic graph (DAG). We first formulate the cross-layer design as a non-linear constrained optimization problem by assuming complete knowledge of the application characteristics and the underlying network conditions. The constrained cross-layer optimization is decomposed into several cross-layer optimization subproblems for each DU and two master problems. The proposed decomposition method determines the necessary message exchanges between layers for achieving the optimal cross-layer solution. However, the attributes (e.g. distortion impact, delay deadline etc) of future DUs as well as the network conditions are often unknown in the considered real-time applications. The impact of current cross-layer actions on the future DUs can be characterized by a state-value function in the Markov decision process (MDP) framework. Based on the dynamic programming solution to the MDP, we develop a low-complexity cross-layer optimization algorithm using online learning for each DU transmission. This online algorithm can be implemented in real-time in order to cope with unknown source characteristics, network dynamics and resource constraints. Our numerical results demonstrate the efficiency of the proposed online algorithm.",
        "published": "2008-12-05T23:14:41Z",
        "link": "http://arxiv.org/abs/0812.1244v1",
        "categories": [
            "cs.MM",
            "cs.LG"
        ]
    },
    {
        "title": "A Novel Clustering Algorithm Based on Quantum Random Walk",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "summary": "The enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum random walk (QRW) with the problem of data clustering, and develop two clustering algorithms based on the one dimensional QRW. Then, the probability distributions on the positions induced by QRW in these algorithms are investigated, which also indicates the possibility of obtaining better results. Consequently, the experimental results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms are of fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.",
        "published": "2008-12-07T15:22:27Z",
        "link": "http://arxiv.org/abs/0812.1357v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Convex Sparse Matrix Factorizations",
        "authors": [
            "Francis Bach",
            "Julien Mairal",
            "Jean Ponce"
        ],
        "summary": "We present a convex formulation of dictionary learning for sparse signal decomposition. Convexity is obtained by replacing the usual explicit upper bound on the dictionary size by a convex rank-reducing term similar to the trace norm. In particular, our formulation introduces an explicit trade-off between size and sparsity of the decomposition of rectangular matrices. Using a large set of synthetic examples, we compare the estimation abilities of the convex and non-convex approaches, showing that while the convex formulation has a single local minimum, this may lead in some cases to performance which is inferior to the local minima of the non-convex formulation.",
        "published": "2008-12-10T09:00:40Z",
        "link": "http://arxiv.org/abs/0812.1869v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Characterizing Truthful Multi-Armed Bandit Mechanisms",
        "authors": [
            "Moshe Babaioff",
            "Yogeshwer Sharma",
            "Aleksandrs Slivkins"
        ],
        "summary": "We consider a multi-round auction setting motivated by pay-per-click auctions for Internet advertising. In each round the auctioneer selects an advertiser and shows her ad, which is then either clicked or not. An advertiser derives value from clicks; the value of a click is her private information. Initially, neither the auctioneer nor the advertisers have any information about the likelihood of clicks on the advertisements. The auctioneer's goal is to design a (dominant strategies) truthful mechanism that (approximately) maximizes the social welfare.   If the advertisers bid their true private values, our problem is equivalent to the \"multi-armed bandit problem\", and thus can be viewed as a strategic version of the latter. In particular, for both problems the quality of an algorithm can be characterized by \"regret\", the difference in social welfare between the algorithm and the benchmark which always selects the same \"best\" advertisement. We investigate how the design of multi-armed bandit algorithms is affected by the restriction that the resulting mechanism must be truthful. We find that truthful mechanisms have certain strong structural properties -- essentially, they must separate exploration from exploitation -- and they incur much higher regret than the optimal multi-armed bandit algorithms. Moreover, we provide a truthful mechanism which (essentially) matches our lower bound on regret.",
        "published": "2008-12-12T04:13:01Z",
        "link": "http://arxiv.org/abs/0812.2291v7",
        "categories": [
            "cs.DS",
            "cs.GT",
            "cs.LG",
            "F.2.2; K.4.4; F.1.2; J.4"
        ]
    },
    {
        "title": "Feature Selection By KDDA For SVM-Based MultiView Face Recognition",
        "authors": [
            "Seyyed Majid Valiollahzadeh",
            "Abolghasem Sayadiyan",
            "Mohammad Nazari"
        ],
        "summary": "Applications such as face recognition that deal with high-dimensional data need a mapping technique that introduces representation of low-dimensional features with enhanced discriminatory power and a proper classifier, able to classify those complex features. Most of traditional Linear Discriminant Analysis suffer from the disadvantage that their optimality criteria are not directly related to the classification ability of the obtained feature representation. Moreover, their classification accuracy is affected by the \"small sample size\" problem which is often encountered in FR tasks. In this short paper, we combine nonlinear kernel based mapping of data called KDDA with Support Vector machine classifier to deal with both of the shortcomings in an efficient and cost effective manner. The proposed here method is compared, in terms of classification accuracy, to other commonly used FR methods on UMIST face database. Results indicate that the performance of the proposed method is overall superior to those of traditional FR approaches, such as the Eigenfaces, Fisherfaces, and D-LDA methods and traditional linear classifiers.",
        "published": "2008-12-13T19:09:03Z",
        "link": "http://arxiv.org/abs/0812.2574v1",
        "categories": [
            "cs.CV",
            "cs.LG"
        ]
    },
    {
        "title": "Face Detection Using Adaboosted SVM-Based Component Classifier",
        "authors": [
            "Seyyed Majid Valiollahzadeh",
            "Abolghasem Sayadiyan",
            "Mohammad Nazari"
        ],
        "summary": "Recently, Adaboost has been widely used to improve the accuracy of any given learning algorithm. In this paper we focus on designing an algorithm to employ combination of Adaboost with Support Vector Machine as weak component classifiers to be used in Face Detection Task. To obtain a set of effective SVM-weaklearner Classifier, this algorithm adaptively adjusts the kernel parameter in SVM instead of using a fixed one. Proposed combination outperforms in generalization in comparison with SVM on imbalanced classification problem. The proposed here method is compared, in terms of classification accuracy, to other commonly used Adaboost methods, such as Decision Trees and Neural Networks, on CMU+MIT face database. Results indicate that the performance of the proposed method is overall superior to previous Adaboost approaches.",
        "published": "2008-12-13T19:14:53Z",
        "link": "http://arxiv.org/abs/0812.2575v1",
        "categories": [
            "cs.CV",
            "cs.LG"
        ]
    },
    {
        "title": "Binary Classification Based on Potentials",
        "authors": [
            "Erik Boczko",
            "Andrew DiLullo",
            "Todd Young"
        ],
        "summary": "We introduce a simple and computationally trivial method for binary classification based on the evaluation of potential functions. We demonstrate that despite the conceptual and computational simplicity of the method its performance can match or exceed that of standard Support Vector Machine methods.",
        "published": "2008-12-16T20:41:06Z",
        "link": "http://arxiv.org/abs/0812.3145v2",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Comparison of Binary Classification Based on Signed Distance Functions   with Support Vector Machines",
        "authors": [
            "Erik M. Boczko",
            "Todd Young",
            "Minhui Zie",
            "Di Wu"
        ],
        "summary": "We investigate the performance of a simple signed distance function (SDF) based method by direct comparison with standard SVM packages, as well as K-nearest neighbor and RBFN methods. We present experimental results comparing the SDF approach with other classifiers on both synthetic geometric problems and five benchmark clinical microarray data sets. On both geometric problems and microarray data sets, the non-optimized SDF based classifiers perform just as well or slightly better than well-developed, standard SVM methods. These results demonstrate the potential accuracy of SDF-based methods on some types of problems.",
        "published": "2008-12-16T20:58:24Z",
        "link": "http://arxiv.org/abs/0812.3147v1",
        "categories": [
            "cs.LG",
            "cs.CG"
        ]
    },
    {
        "title": "Quantum Predictive Learning and Communication Complexity with Single   Input",
        "authors": [
            "Dmytro Gavinsky"
        ],
        "summary": "We define a new model of quantum learning that we call Predictive Quantum (PQ). This is a quantum analogue of PAC, where during the testing phase the student is only required to answer a polynomial number of testing queries.   We demonstrate a relational concept class that is efficiently learnable in PQ, while in any \"reasonable\" classical model exponential amount of training data would be required. This is the first unconditional separation between quantum and classical learning.   We show that our separation is the best possible in several ways; in particular, there is no analogous result for a functional class, as well as for several weaker versions of quantum learning. In order to demonstrate tightness of our separation we consider a special case of one-way communication that we call single-input mode, where Bob receives no input. Somewhat surprisingly, this setting becomes nontrivial when relational communication tasks are considered. In particular, any problem with two-sided input can be transformed into a single-input relational problem of equal classical one-way cost. We show that the situation is different in the quantum case, where the same transformation can make the communication complexity exponentially larger. This happens if and only if the original problem has exponential gap between quantum and classical one-way communication costs. We believe that these auxiliary results might be of independent interest.",
        "published": "2008-12-17T22:46:18Z",
        "link": "http://arxiv.org/abs/0812.3429v3",
        "categories": [
            "quant-ph",
            "cs.LG"
        ]
    },
    {
        "title": "Linearly Parameterized Bandits",
        "authors": [
            "Paat Rusmevichientong",
            "John N. Tsitsiklis"
        ],
        "summary": "We consider bandit problems involving a large (possibly infinite) collection of arms, in which the expected reward of each arm is a linear function of an $r$-dimensional random vector $\\mathbf{Z} \\in \\mathbb{R}^r$, where $r \\geq 2$. The objective is to minimize the cumulative regret and Bayes risk. When the set of arms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order $\\Theta(r \\sqrt{T})$, by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases. The phase-based policy is also shown to be effective if the set of arms satisfies a strong convexity condition. For the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes risk admit upper bounds of the form $O(r \\sqrt{T} \\log^{3/2} T)$.",
        "published": "2008-12-18T07:59:33Z",
        "link": "http://arxiv.org/abs/0812.3465v2",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "The Offset Tree for Learning with Partial Labels",
        "authors": [
            "Alina Beygelzimer",
            "John Langford"
        ],
        "summary": "We present an algorithm, called the Offset Tree, for learning to make decisions in situations where the payoff of only one choice is observed, rather than all choices. The algorithm reduces this setting to binary classification, allowing one to reuse of any existing, fully supervised binary classification algorithm in this partial information setting. We show that the Offset Tree is an optimal reduction to binary classification. In particular, it has regret at most $(k-1)$ times the regret of the binary classifier it uses (where $k$ is the number of choices), and no reduction to binary classification can do better. This reduction is also computationally optimal, both at training and test time, requiring just $O(\\log_2 k)$ work to train on an example or make a prediction.   Experiments with the Offset Tree show that it generally performs better than several alternative approaches.",
        "published": "2008-12-21T17:45:27Z",
        "link": "http://arxiv.org/abs/0812.4044v3",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Client-server multi-task learning from distributed datasets",
        "authors": [
            "Francesco Dinuzzo",
            "Gianluigi Pillonetto",
            "Giuseppe De Nicolao"
        ],
        "summary": "A client-server architecture to simultaneously solve multiple learning tasks from distributed datasets is described. In such architecture, each client is associated with an individual learning task and the associated dataset of examples. The goal of the architecture is to perform information fusion from multiple datasets while preserving privacy of individual data. The role of the server is to collect data in real-time from the clients and codify the information in a common database. The information coded in this database can be used by all the clients to solve their individual learning task, so that each client can exploit the informative content of all the datasets without actually having access to private data of others. The proposed algorithmic framework, based on regularization theory and kernel methods, uses a suitable class of mixed effect kernels. The new method is illustrated through a simulated music recommendation system.",
        "published": "2008-12-22T16:34:39Z",
        "link": "http://arxiv.org/abs/0812.4235v2",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "The Latent Relation Mapping Engine: Algorithm and Experiments",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Many AI researchers and cognitive scientists have argued that analogy is the core of cognition. The most influential work on computational modeling of analogy-making is Structure Mapping Theory (SMT) and its implementation in the Structure Mapping Engine (SME). A limitation of SME is the requirement for complex hand-coded representations. We introduce the Latent Relation Mapping Engine (LRME), which combines ideas from SME and Latent Relational Analysis (LRA) in order to remove the requirement for hand-coded representations. LRME builds analogical mappings between lists of words, using a large corpus of raw text to automatically discover the semantic relations among the words. We evaluate LRME on a set of twenty analogical mapping problems, ten based on scientific analogies and ten based on common metaphors. LRME achieves human-level performance on the twenty problems. We compare LRME with a variety of alternative approaches and find that they are not able to reach the same level of performance.",
        "published": "2008-12-23T20:08:53Z",
        "link": "http://arxiv.org/abs/0812.4446v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "H.3.1, I.2.6, I.2.7"
        ]
    },
    {
        "title": "Feature Markov Decision Processes",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "General purpose intelligent learning agents cycle through (complex,non-MDP) sequences of observations, actions, and rewards. On the other hand, reinforcement learning is well-developed for small finite state Markov Decision Processes (MDPs). So far it is an art performed by human designers to extract the right state representation out of the bare observations, i.e. to reduce the agent setup to the MDP framework. Before we can think of mechanizing this search for suitable MDPs, we need a formal objective criterion. The main contribution of this article is to develop such a criterion. I also integrate the various parts into one learning algorithm. Extensions to more realistic dynamic Bayesian networks are developed in a companion article.",
        "published": "2008-12-25T00:27:22Z",
        "link": "http://arxiv.org/abs/0812.4580v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Feature Dynamic Bayesian Networks",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Feature Markov Decision Processes (PhiMDPs) are well-suited for learning agents in general environments. Nevertheless, unstructured (Phi)MDPs are limited to relatively simple environments. Structured MDPs like Dynamic Bayesian Networks (DBNs) are used for large-scale real-world problems. In this article I extend PhiMDP to PhiDBN. The primary contribution is to derive a cost criterion that allows to automatically extract the most relevant features from the environment, leading to the \"best\" DBN representation. I discuss all building blocks required for a complete general learning algorithm.",
        "published": "2008-12-25T00:32:45Z",
        "link": "http://arxiv.org/abs/0812.4581v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Importance Weighted Active Learning",
        "authors": [
            "Alina Beygelzimer",
            "Sanjoy Dasgupta",
            "John Langford"
        ],
        "summary": "We present a practical and statistically consistent scheme for actively learning binary classifiers under general loss functions. Our algorithm uses importance weighting to correct sampling bias, and by controlling the variance, we are able to give rigorous label complexity bounds for the learning process. Experiments on passively labeled data show that this approach reduces the label complexity required to achieve good predictive performance on many learning problems.",
        "published": "2008-12-29T18:29:08Z",
        "link": "http://arxiv.org/abs/0812.4952v4",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "A New Clustering Algorithm Based Upon Flocking On Complex Network",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "summary": "We have proposed a model based upon flocking on a complex network, and then developed two clustering algorithms on the basis of it. In the algorithms, firstly a \\textit{k}-nearest neighbor (knn) graph as a weighted and directed graph is produced among all data points in a dataset each of which is regarded as an agent who can move in space, and then a time-varying complex network is created by adding long-range links for each data point. Furthermore, each data point is not only acted by its \\textit{k} nearest neighbors but also \\textit{r} long-range neighbors through fields established in space by them together, so it will take a step along the direction of the vector sum of all fields. It is more important that these long-range links provides some hidden information for each data point when it moves and at the same time accelerate its speed converging to a center. As they move in space according to the proposed model, data points that belong to the same class are located at a same position gradually, whereas those that belong to different classes are away from one another. Consequently, the experimental results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the rates of convergence of clustering algorithms are fast enough. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.",
        "published": "2008-12-30T08:30:27Z",
        "link": "http://arxiv.org/abs/0812.5032v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "physics.soc-ph"
        ]
    },
    {
        "title": "A Novel Clustering Algorithm Based Upon Games on Evolving Network",
        "authors": [
            "Qiang Li",
            "Zhuo Chen",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "summary": "This paper introduces a model based upon games on an evolving network, and develops three clustering algorithms according to it. In the clustering algorithms, data points for clustering are regarded as players who can make decisions in games. On the network describing relationships among data points, an edge-removing-and-rewiring (ERR) function is employed to explore in a neighborhood of a data point, which removes edges connecting to neighbors with small payoffs, and creates new edges to neighbors with larger payoffs. As such, the connections among data points vary over time. During the evolution of network, some strategies are spread in the network. As a consequence, clusters are formed automatically, in which data points with the same evolutionarily stable strategy are collected as a cluster, so the number of evolutionarily stable strategies indicates the number of clusters. Moreover, the experimental results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the comparison with other algorithms also provides an indication of the effectiveness of the proposed algorithms.",
        "published": "2008-12-30T13:22:31Z",
        "link": "http://arxiv.org/abs/0812.5064v2",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.GT",
            "nlin.AO"
        ]
    },
    {
        "title": "Is Randomness \"Native\" to Computer Science?",
        "authors": [
            "Marie Ferbus-Zanda",
            "Serge Grigorieff"
        ],
        "summary": "We survey the Kolmogorov's approach to the notion of randomness through the Kolmogorov complexity theory. The original motivation of Kolmogorov was to give up a quantitative definition of information. In this theory, an object is randomness in the sense that it has a large information content. Afterwards, we present parts of the work of Martin-Lof, Schnorr, Chaitin and Levin which supply a mathematical notion of randomness throughout diverse theories from the the 60' up to recently.",
        "published": "2008-01-01T10:26:55Z",
        "link": "http://arxiv.org/abs/0801.0289v1",
        "categories": [
            "math.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Church, Cardinal and Ordinal Representations of Integers and Kolmogorov   complexity",
        "authors": [
            "Marie Ferbus-Zanda",
            "Serge Grigorieff"
        ],
        "summary": "We consider classical representations of integers: Church's function iterators, cardinal equivalence classes of sets, ordinal equivalence classes of totally ordered sets. Since programs do not work on abstract entities and require formal representations of objects, we effectivize these abstract notions in order to allow them to be computed by programs. To any such effectivized representation is then associated a notion of Kolmogorov complexity. We prove that these Kolmogorov complexities form a strict hierarchy which coincides with that obtained by relativization to jump oracles and/or allowance of infinite computations.",
        "published": "2008-01-02T08:35:27Z",
        "link": "http://arxiv.org/abs/0801.0349v1",
        "categories": [
            "math.LO",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Refinment of the \"up to a constant\" ordering using contructive   co-immunity and alike. Application to the Min/Max hierarchy of Kolmogorov   complexities",
        "authors": [
            "Marie Ferbus-Zanda",
            "Serge Grigorieff"
        ],
        "summary": "We introduce orderings between total functions f,g: N -> N which refine the pointwise \"up to a constant\" ordering <=cte and also insure that f(x) is often much less thang(x). With such orderings, we prove a strong hierarchy theorem for Kolmogorov complexities obtained with jump oracles and/or Max or Min of partial recursive functions. We introduce a notion of second order conditional Kolmogorov complexity which yields a uniform bound for the \"up to a constant\" comparisons involved in the hierarchy theorem.",
        "published": "2008-01-02T08:35:59Z",
        "link": "http://arxiv.org/abs/0801.0350v1",
        "categories": [
            "math.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Set theoretical Representations of Integers, I",
        "authors": [
            "Marie Ferbus-Zanda",
            "Serge Grigorieff"
        ],
        "summary": "We reconsider some classical natural semantics of integers (namely iterators of functions, cardinals of sets, index of equivalence relations), in the perspective of Kolmogorov complexity. To each such semantics one can attach a simple representation of integers that we suitably effectivize in order to develop an associated Kolmogorov theory. Such effectivizations are particular instances of a general notion of \"self-enumerated system\" that we introduce in this paper. Our main result asserts that, with such effectivizations, Kolmogorov theory allows to quantitatively distinguish the underlying semantics. We characterize the families obtained by such effectivizations and prove that the associated Kolmogorov complexities constitute a hierarchy which coincides with that of Kolmogorov complexities defined via jump oracles and/or infinite computations. This contrasts with the well-known fact that usual Kolmogorov complexity does not depend (up to a constant) on the chosen arithmetic representation of integers, let it be in any base unary, binary et so on. Also, in a conceptual point of view, our result can be seen as a mean to measure the degree of abstraction of these diverse semantics.",
        "published": "2008-01-02T08:37:01Z",
        "link": "http://arxiv.org/abs/0801.0353v1",
        "categories": [
            "math.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Kolmogorov complexity in perspective",
        "authors": [
            "Marie Ferbus-Zanda",
            "Serge Grigorieff"
        ],
        "summary": "We survey the diverse approaches to the notion of information content: from Shannon entropy to Kolmogorov complexity. The main applications of Kolmogorov complexity are presented namely, the mathematical notion of randomness (which goes back to the 60's with the work of Martin-Lof, Schnorr, Chaitin, Levin), and classification, which is a recent idea with provocative implementation by Vitanyi and Cilibrasi.",
        "published": "2008-01-02T08:38:18Z",
        "link": "http://arxiv.org/abs/0801.0354v1",
        "categories": [
            "math.LO",
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the graph isomorphism problem",
        "authors": [
            "Shmuel Friedland"
        ],
        "summary": "We relate the graph isomorphism problem to the solvability of certain systems of linear equations with nonnegative variables. This version replaces the two previous versions of this paper.",
        "published": "2008-01-02T14:40:02Z",
        "link": "http://arxiv.org/abs/0801.0398v3",
        "categories": [
            "cs.CC",
            "cs.DM",
            "G.2.2; F.2.2"
        ]
    },
    {
        "title": "Analysis and Counterexamples Regarding Yatsenko's Polynomial-Time   Algorithm for Solving the Traveling Salesman Problem",
        "authors": [
            "Christopher Clingerman",
            "Jeremiah Hemphill",
            "Corey Proscia"
        ],
        "summary": "Yatsenko gives a polynomial-time algorithm for solving the traveling salesman problem. We examine the correctness of the algorithm and its construction. We also comment on Yatsenko's evaluation of the algorithm.",
        "published": "2008-01-03T04:46:16Z",
        "link": "http://arxiv.org/abs/0801.0474v1",
        "categories": [
            "cs.CC",
            "F.2.1; G.1.6; I.2.8"
        ]
    },
    {
        "title": "New results on Noncommutative and Commutative Polynomial Identity   Testing",
        "authors": [
            "V. Arvind",
            "Partha Mukhopadhyay",
            "Srikanth Srinivasan"
        ],
        "summary": "Using ideas from automata theory we design a new efficient (deterministic) identity test for the \\emph{noncommutative} polynomial identity testing problem (first introduced and studied in \\cite{RS05,BW05}). We also apply this idea to the reconstruction of black-box noncommuting algebraic branching programs. Assuming the black-box model allows us to query the ABP for the output at any given gate, we can reconstruct an (equivalent) ABP in deterministic polynomial time. Finally, we explore commutative identity testing when the coefficients of the input polynomial come from an arbitrary finite commutative ring with unity.",
        "published": "2008-01-03T12:32:41Z",
        "link": "http://arxiv.org/abs/0801.0514v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On the Length of the Wadge Hierarchy of Omega Context Free Languages",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We prove in this paper that the length of the Wadge hierarchy of omega context free languages is greater than the Cantor ordinal epsilon_omega, which is the omega-th fixed point of the ordinal exponentiation of base omega. The same result holds for the conciliating Wadge hierarchy, defined by J. Duparc, of infinitary context free languages, studied by D. Beauquier. We show also that there exist some omega context free languages which are Sigma^0_omega-complete Borel sets, improving previous results on omega context free languages and the Borel hierarchy.",
        "published": "2008-01-03T14:48:48Z",
        "link": "http://arxiv.org/abs/0801.0534v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.GT",
            "math.LO"
        ]
    },
    {
        "title": "Linear Logic by Levels and Bounded Time Complexity",
        "authors": [
            "Patrick Baillot",
            "Damiano Mazza"
        ],
        "summary": "We give a new characterization of elementary and deterministic polynomial time computation in linear logic through the proofs-as-programs correspondence. Girard's seminal results, concerning elementary and light linear logic, achieve this characterization by enforcing a stratification principle on proofs, using the notion of depth in proof nets. Here, we propose a more general form of stratification, based on inducing levels in proof nets by means of indexes, which allows us to extend Girard's systems while keeping the same complexity properties. In particular, it turns out that Girard's systems can be recovered by forcing depth and level to coincide. A consequence of the higher flexibility of levels with respect to depth is the absence of boxes for handling the paragraph modality. We use this fact to propose a variant of our polytime system in which the paragraph modality is only allowed on atoms, and which may thus serve as a basis for developing lambda-calculus type assignment systems with more efficient typing algorithms than existing ones.",
        "published": "2008-01-08T15:08:20Z",
        "link": "http://arxiv.org/abs/0801.1253v3",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Alternating Hierarchies for Time-Space Tradeoffs",
        "authors": [
            "Chris Pollett",
            "Eric Miles"
        ],
        "summary": "Nepomnjascii's Theorem states that for all 0 <= \\epsilon < 1 and k > 0 the class of languages recognized in nondeterministic time n^k and space n^\\epsilon, NTISP[n^k, n^\\epsilon ], is contained in the linear time hierarchy. By considering restrictions on the size of the universal quantifiers in the linear time hierarchy, this paper refines Nepomnjascii's result to give a sub- hierarchy, Eu-LinH, of the linear time hierarchy that is contained in NP and which contains NTISP[n^k, n^\\epsilon ]. Hence, Eu-LinH contains NL and SC. This paper investigates basic structural properties of Eu-LinH. Then the relationships between Eu-LinH and the classes NL, SC, and NP are considered to see if they can shed light on the NL = NP or SC = NP questions. Finally, a new hierarchy, zeta -LinH, is defined to reduce the space requirements needed for the upper bound on Eu-LinH.",
        "published": "2008-01-08T19:59:05Z",
        "link": "http://arxiv.org/abs/0801.1307v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3"
        ]
    },
    {
        "title": "Two graph isomorphism polytopes",
        "authors": [
            "Shmuel Onn"
        ],
        "summary": "The convex hull $\\psi_{n,n}$ of certain $(n!)^2$ tensors was considered recently in connection with graph isomorphism. We consider the convex hull $\\psi_n$ of the $n!$ diagonals among these tensors. We show: 1. The polytope $\\psi_n$ is a face of $\\psi_{n,n}$. 2. Deciding if a graph $G$ has a subgraph isomorphic to $H$ reduces to optimization over $\\psi_n$. 3. Optimization over $\\psi_n$ reduces to optimization over $\\psi_{n,n}$. In particular, this implies that the subgraph isomorphism problem reduces to optimization over $\\psi_{n,n}$.",
        "published": "2008-01-09T13:34:26Z",
        "link": "http://arxiv.org/abs/0801.1410v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "math.CO",
            "math.OC"
        ]
    },
    {
        "title": "A Most General Edge Elimination Polynomial - Thickening of Edges",
        "authors": [
            "Christian Hoffmann"
        ],
        "summary": "We consider a graph polynomial \\xi(G;x,y,z) introduced by Averbouch, Godlin, and Makowsky (2007). This graph polynomial simultaneously generalizes the Tutte polynomial as well as a bivariate chromatic polynomial defined by Dohmen, Poenitz and Tittmann (2003). We derive an identity which relates the graph polynomial of a thicked graph (i.e. a graph with each edge replaced by k copies of it) to the graph polynomial of the original graph. As a consequence, we observe that at every point (x,y,z), except for points lying within some set of dimension 2, evaluating \\xi is #P-hard.",
        "published": "2008-01-10T13:58:45Z",
        "link": "http://arxiv.org/abs/0801.1600v1",
        "categories": [
            "math.CO",
            "cs.CC"
        ]
    },
    {
        "title": "A Family of Counter Examples to an Approach to Graph Isomorphism",
        "authors": [
            "Jin-Yi Cai",
            "Pinyan Lu",
            "Mingji Xia"
        ],
        "summary": "We give a family of counter examples showing that the two sequences of polytopes $\\Phi_{n,n}$ and $\\Psi_{n,n}$ are different. These polytopes were defined recently by S. Friedland in an attempt at a polynomial time algorithm for graph isomorphism.",
        "published": "2008-01-11T12:28:05Z",
        "link": "http://arxiv.org/abs/0801.1766v2",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "An omega-power of a context-free language which is Borel above   Delta^0_omega",
        "authors": [
            "Jacques Duparc",
            "Olivier Finkel"
        ],
        "summary": "We use erasers-like basic operations on words to construct a set that is both Borel and above Delta^0_omega, built as a set V^\\omega where V is a language of finite words accepted by a pushdown automaton. In particular, this gives a first example of an omega-power of a context free language which is a Borel set of infinite rank.",
        "published": "2008-01-11T14:20:30Z",
        "link": "http://arxiv.org/abs/0801.1783v1",
        "categories": [
            "cs.CC",
            "cs.GT",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "Theoretical analysis of optimization problems - Some properties of   random k-SAT and k-XORSAT",
        "authors": [
            "Fabrizio Altarelli"
        ],
        "summary": "This thesis is divided in two parts. The first presents an overview of known results in statistical mechanics of disordered systems and its approach to random combinatorial optimization problems. The second part is a discussion of two original results.   The first result concerns DPLL heuristics for random k-XORSAT, which is equivalent to the diluted Ising p-spin model. It is well known that DPLL is unable to find the ground states in the clustered phase of the problem, i.e. that it leads to contradictions with probability 1. However, no solid argument supports this is general. A class of heuristics, which includes the well known UC and GUC, is introduced and studied. It is shown that any heuristic in this class must fail if the clause to variable ratio is larger than some constant, which depends on the heuristic but is always smaller than the clustering threshold.   The second result concerns the properties of random k-SAT at large clause to variable ratios. In this regime, it is well known that the uniform distribution of random instances is dominated by unsatisfiable instances. A general technique (based on the Replica method) to restrict the distribution to satisfiable instances with uniform weight is introduced, and is used to characterize their solutions. It is found that in the limit of large clause to variable ratios, the uniform distribution of satisfiable random k-SAT formulas is asymptotically equal to the much studied Planted distribution.   Both results are already published and available as arXiv:0709.0367 and arXiv:cs/0609101 . A more detailed and self-contained derivation is presented here.",
        "published": "2008-01-18T11:50:32Z",
        "link": "http://arxiv.org/abs/0801.2858v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "Entropy landscape and non-Gibbs solutions in constraint satisfaction   problems",
        "authors": [
            "L. Dall'Asta",
            "A. Ramezanpour",
            "R. Zecchina"
        ],
        "summary": "We study the entropy landscape of solutions for the bicoloring problem in random graphs, a representative difficult constraint satisfaction problem. Our goal is to classify which type of clusters of solutions are addressed by different algorithms. In the first part of the study we use the cavity method to obtain the number of clusters with a given internal entropy and determine the phase diagram of the problem, e.g. dynamical, rigidity and SAT-UNSAT transitions. In the second part of the paper we analyze different algorithms and locate their behavior in the entropy landscape of the problem. For instance we show that a smoothed version of a decimation strategy based on Belief Propagation is able to find solutions belonging to sub-dominant clusters even beyond the so called rigidity transition where the thermodynamically relevant clusters become frozen. These non-equilibrium solutions belong to the most probable unfrozen clusters.",
        "published": "2008-01-18T14:11:10Z",
        "link": "http://arxiv.org/abs/0801.2890v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "From k-SAT to k-CSP: Two Generalized Algorithms",
        "authors": [
            "Liang Li",
            "Xin Li",
            "Tian Liu",
            "Ke Xu"
        ],
        "summary": "Constraint satisfaction problems (CSPs) models many important intractable NP-hard problems such as propositional satisfiability problem (SAT). Algorithms with non-trivial upper bounds on running time for restricted SAT with bounded clause length k (k-SAT) can be classified into three styles: DPLL-like, PPSZ-like and Local Search, with local search algorithms having already been generalized to CSP with bounded constraint arity k (k-CSP). We generalize a DPLL-like algorithm in its simplest form and a PPSZ-like algorithm from k-SAT to k-CSP. As far as we know, this is the first attempt to use PPSZ-like strategy to solve k-CSP, and before little work has been focused on the DPLL-like or PPSZ-like strategies for k-CSP.",
        "published": "2008-01-21T08:07:33Z",
        "link": "http://arxiv.org/abs/0801.3147v1",
        "categories": [
            "cs.DS",
            "cs.AI",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Worst-Case Hermite-Korkine-Zolotarev Reduced Lattice Bases",
        "authors": [
            "Guillaume Hanrot",
            "Damien Stehlé"
        ],
        "summary": "The Hermite-Korkine-Zolotarev reduction plays a central role in strong lattice reduction algorithms. By building upon a technique introduced by Ajtai, we show the existence of Hermite-Korkine-Zolotarev reduced bases that are arguably least reduced. We prove that for such bases, Kannan's algorithm solving the shortest lattice vector problem requires $d^{\\frac{d}{2\\e}(1+o(1))}$ bit operations in dimension $d$. This matches the best complexity upper bound known for this algorithm. These bases also provide lower bounds on Schnorr's constants $\\alpha_d$ and $\\beta_d$ that are essentially equal to the best upper bounds. Finally, we also show the existence of particularly bad bases for Schnorr's hierarchy of reductions.",
        "published": "2008-01-22T09:52:35Z",
        "link": "http://arxiv.org/abs/0801.3331v2",
        "categories": [
            "math.NT",
            "cs.CC",
            "cs.CR"
        ]
    },
    {
        "title": "Multiparty Communication Complexity of Disjointness",
        "authors": [
            "Arkadev Chattopadhyay",
            "Anil Ada"
        ],
        "summary": "We obtain a lower bound of n^Omega(1) on the k-party randomized communication complexity of the Disjointness function in the `Number on the Forehead' model of multiparty communication when k is a constant. For k=o(loglog n), the bounds remain super-polylogarithmic i.e. (log n)^omega(1). The previous best lower bound for three players until recently was Omega(log n).   Our bound separates the communication complexity classes NP^{CC}_k and BPP^{CC}_k for k=o(loglog n). Furthermore, by the results of Beame, Pitassi and Segerlind \\cite{BPS07}, our bound implies proof size lower bounds for tree-like, degree k-1 threshold systems and superpolynomial size lower bounds for Lovasz-Schrijver proofs.   Sherstov \\cite{She07b} recently developed a novel technique to obtain lower bounds on two-party communication using the approximate polynomial degree of boolean functions. We obtain our results by extending his technique to the multi-party setting using ideas from Chattopadhyay \\cite{Cha07}.   A similar bound for Disjointness has been recently and independently obtained by Lee and Shraibman.",
        "published": "2008-01-23T16:39:31Z",
        "link": "http://arxiv.org/abs/0801.3624v3",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Merkle's Key Agreement Protocol is Optimal: An $O(n^2)$ Attack on any   Key Agreement from Random Oracles",
        "authors": [
            "Boaz Barak",
            "Mohammad Mahmoody"
        ],
        "summary": "We prove that every key agreement protocol in the random oracle model in which the honest users make at most $n$ queries to the oracle can be broken by an adversary who makes $O(n^2)$ queries to the oracle. This improves on the previous $\\widetilde{\\Omega}(n^6)$ query attack given by Impagliazzo and Rudich (STOC '89) and resolves an open question posed by them.   Our bound is optimal up to a constant factor since Merkle proposed a key agreement protocol in 1974 that can be easily implemented with $n$ queries to a random oracle and cannot be broken by any adversary who asks $o(n^2)$ queries.",
        "published": "2008-01-23T21:01:37Z",
        "link": "http://arxiv.org/abs/0801.3669v4",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Lower Bounds on Signatures from Symmetric Primitives",
        "authors": [
            "Boaz Barak",
            "Mohammad Mahmoody"
        ],
        "summary": "We show that every construction of one-time signature schemes from a random oracle achieves black-box security at most $2^{(1+o(1))q}$, where $q$ is the total number of oracle queries asked by the key generation, signing, and verification algorithms. That is, any such scheme can be broken with probability close to $1$ by a (computationally unbounded) adversary making $2^{(1+o(1))q}$ queries to the oracle. This is tight up to a constant factor in the number of queries, since a simple modification of Lamport's one-time signatures (Lamport '79) achieves $2^{(0.812-o(1))q}$ black-box security using $q$ queries to the oracle.   Our result extends (with a loss of a constant factor in the number of queries) also to the random permutation and ideal-cipher oracles. Since the symmetric primitives (e.g. block ciphers, hash functions, and message authentication codes) can be constructed by a constant number of queries to the mentioned oracles, as corollary we get lower bounds on the efficiency of signature schemes from symmetric primitives when the construction is black-box. This can be taken as evidence of an inherent efficiency gap between signature schemes and symmetric primitives.",
        "published": "2008-01-23T22:16:00Z",
        "link": "http://arxiv.org/abs/0801.3680v3",
        "categories": [
            "cs.CC",
            "cs.CR"
        ]
    },
    {
        "title": "Characterization of the Vertices and Extreme Directions of the Negative   Cycles Polyhedron and Hardness of Generating Vertices of 0/1-Polyhedra",
        "authors": [
            "Endre Boros",
            "Khaled Elbassioni",
            "Vladimir Gurvich",
            "Hans Raj Tiwary"
        ],
        "summary": "Given a graph $G=(V,E)$ and a weight function on the edges $w:E\\mapsto\\RR$, we consider the polyhedron $P(G,w)$ of negative-weight flows on $G$, and get a complete characterization of the vertices and extreme directions of $P(G,w)$. As a corollary, we show that, unless $P=NP$, there is no output polynomial-time algorithm to generate all the vertices of a 0/1-polyhedron. This strengthens the NP-hardness result of Khachiyan et al. (2006) for non 0/1-polyhedra, and comes in contrast with the polynomiality of vertex enumeration for 0/1-polytopes \\cite{BL98} [Bussieck and L\\\"ubbecke (1998)].",
        "published": "2008-01-24T16:16:45Z",
        "link": "http://arxiv.org/abs/0801.3790v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Dichotomy Results for Fixed-Point Existence Problems for Boolean   Dynamical Systems",
        "authors": [
            "Sven Kosub"
        ],
        "summary": "A complete classification of the computational complexity of the fixed-point existence problem for boolean dynamical systems, i.e., finite discrete dynamical systems over the domain {0, 1}, is presented. For function classes F and graph classes G, an (F, G)-system is a boolean dynamical system such that all local transition functions lie in F and the underlying graph lies in G. Let F be a class of boolean functions which is closed under composition and let G be a class of graphs which is closed under taking minors. The following dichotomy theorems are shown: (1) If F contains the self-dual functions and G contains the planar graphs then the fixed-point existence problem for (F, G)-systems with local transition function given by truth-tables is NP-complete; otherwise, it is decidable in polynomial time. (2) If F contains the self-dual functions and G contains the graphs having vertex covers of size one then the fixed-point existence problem for (F, G)-systems with local transition function given by formulas or circuits is NP-complete; otherwise, it is decidable in polynomial time.",
        "published": "2008-01-24T17:10:12Z",
        "link": "http://arxiv.org/abs/0801.3802v2",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "cs.DM",
            "nlin.AO",
            "nlin.CG",
            "F.2.2; F.1.1; F.1.3"
        ]
    },
    {
        "title": "On the Scaling Window of Model RB",
        "authors": [
            "Chunyan Zhao",
            "Ke Xu",
            "Zhiming Zheng"
        ],
        "summary": "This paper analyzes the scaling window of a random CSP model (i.e. model RB) for which we can identify the threshold points exactly, denoted by $r_{cr}$ or $p_{cr}$. For this model, we establish the scaling window $W(n,\\delta)=(r_{-}(n,\\delta), r_{+}(n,\\delta))$ such that the probability of a random instance being satisfiable is greater than $1-\\delta$ for $r<r_{-}(n,\\delta)$ and is less than $\\delta$ for $r>r_{+}(n,\\delta)$. Specifically, we obtain the following result $$W(n,\\delta)=(r_{cr}-\\Theta(\\frac{1}{n^{1-\\epsilon}\\ln n}), \\ r_{cr}+\\Theta(\\frac{1}{n\\ln n})),$$ where $0\\leq\\epsilon<1$ is a constant. A similar result with respect to the other parameter $p$ is also obtained. Since the instances generated by model RB have been shown to be hard at the threshold, this is the first attempt, as far as we know, to analyze the scaling window of such a model with hard instances.",
        "published": "2008-01-25T02:18:00Z",
        "link": "http://arxiv.org/abs/0801.3871v1",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "cs.AI"
        ]
    },
    {
        "title": "On the Continuity Set of an omega Rational Function",
        "authors": [
            "Olivier Carton",
            "Olivier Finkel",
            "Pierre Simonnet"
        ],
        "summary": "In this paper, we study the continuity of rational functions realized by B\\\"uchi finite state transducers. It has been shown by Prieur that it can be decided whether such a function is continuous. We prove here that surprisingly, it cannot be decided whether such a function F has at least one point of continuity and that its continuity set C(F) cannot be computed. In the case of a synchronous rational function, we show that its continuity set is rational and that it can be computed. Furthermore we prove that any rational Pi^0_2-subset of X^omega for some alphabet X is the continuity set C(F) of an omega-rational synchronous function F defined on X^omega.",
        "published": "2008-01-25T10:54:05Z",
        "link": "http://arxiv.org/abs/0801.3912v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Set-based complexity and biological information",
        "authors": [
            "David J. Galas",
            "Matti Nykter",
            "Gregory W. Carter",
            "Nathan D. Price",
            "Ilya Shmulevich"
        ],
        "summary": "It is not obvious what fraction of all the potential information residing in the molecules and structures of living systems is significant or meaningful to the system. Sets of random sequences or identically repeated sequences, for example, would be expected to contribute little or no useful information to a cell. This issue of quantitation of information is important since the ebb and flow of biologically significant information is essential to our quantitative understanding of biological function and evolution. Motivated specifically by these problems of biological information, we propose here a class of measures to quantify the contextual nature of the information in sets of objects, based on Kolmogorov's intrinsic complexity. Such measures discount both random and redundant information and are inherent in that they do not require a defined state space to quantify the information. The maximization of this new measure, which can be formulated in terms of the universal information distance, appears to have several useful and interesting properties, some of which we illustrate with examples.",
        "published": "2008-01-25T20:58:14Z",
        "link": "http://arxiv.org/abs/0801.4024v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT",
            "q-bio.QM"
        ]
    },
    {
        "title": "Quantified Propositional Logspace Reasoning",
        "authors": [
            "Steven Perron"
        ],
        "summary": "In this paper, we develop a quantified propositional proof systems that corresponds to logarithmic-space reasoning. We begin by defining a class SigmaCNF(2) of quantified formulas that can be evaluated in log space. Then our new proof system GL^* is defined as G_1^* with cuts restricted to SigmaCNF(2) formulas and no cut formula that is not quantifier free contains a free variable that does not appear in the final formula.   To show that GL^* is strong enough to capture log space reasoning, we translate theorems of VL into a family of tautologies that have polynomial-size GL^* proofs. VL is a theory of bounded arithmetic that is known to correspond to logarithmic-space reasoning. To do the translation, we find an appropriate axiomatization of VL, and put VL proofs into a new normal form.   To show that GL^* is not too strong, we prove the soundness of GL^* in such a way that it can be formalized in VL. This is done by giving a logarithmic-space algorithm that witnesses GL^* proofs.",
        "published": "2008-01-27T19:53:39Z",
        "link": "http://arxiv.org/abs/0801.4105v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "A statistical mechanical interpretation of algorithmic information   theory",
        "authors": [
            "Kohtaro Tadaki"
        ],
        "summary": "We develop a statistical mechanical interpretation of algorithmic information theory by introducing the notion of thermodynamic quantities, such as free energy, energy, statistical mechanical entropy, and specific heat, into algorithmic information theory. We investigate the properties of these quantities by means of program-size complexity from the point of view of algorithmic randomness. It is then discovered that, in the interpretation, the temperature plays a role as the compression rate of the values of all these thermodynamic quantities, which include the temperature itself. Reflecting this self-referential nature of the compression rate of the temperature, we obtain fixed point theorems on compression rate.",
        "published": "2008-01-28T17:42:27Z",
        "link": "http://arxiv.org/abs/0801.4194v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT",
            "math.PR",
            "quant-ph"
        ]
    },
    {
        "title": "The Complexity of Power-Index Comparison",
        "authors": [
            "Piotr Faliszewski",
            "Lane A. Hemaspaandra"
        ],
        "summary": "We study the complexity of the following problem: Given two weighted voting games G' and G'' that each contain a player p, in which of these games is p's power index value higher? We study this problem with respect to both the Shapley-Shubik power index [SS54] and the Banzhaf power index [Ban65,DS79]. Our main result is that for both of these power indices the problem is complete for probabilistic polynomial time (i.e., is PP-complete). We apply our results to partially resolve some recently proposed problems regarding the complexity of weighted voting games. We also study the complexity of the raw Shapley-Shubik power index. Deng and Papadimitriou [DP94] showed that the raw Shapley-Shubik power index is #P-metric-complete. We strengthen this by showing that the raw Shapley-Shubik power index is many-one complete for #P. And our strengthening cannot possibly be further improved to parsimonious completeness, since we observe that, in contrast with the raw Banzhaf power index, the raw Shapley-Shubik power index is not #P-parsimonious-complete.",
        "published": "2008-01-30T00:10:07Z",
        "link": "http://arxiv.org/abs/0801.4585v1",
        "categories": [
            "cs.CC",
            "cs.GT",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Breaking One-Round Key-Agreement Protocols in the Random Oracle Model",
        "authors": [
            "Miroslava Sotakova"
        ],
        "summary": "In this paper we study one-round key-agreement protocols analogous to Merkle's puzzles in the random oracle model. The players Alice and Bob are allowed to query a random permutation oracle $n$ times and upon their queries and communication, they both output the same key with high probability. We prove that Eve can always break such a protocol by querying the oracle $O(n^2)$ times. The long-time unproven optimality of the quadratic bound in the fully general, multi-round scenario has been shown recently by Barak and Mahmoody-Ghidary. The results in this paper have been found independently of their work.",
        "published": "2008-01-30T19:34:34Z",
        "link": "http://arxiv.org/abs/0801.4714v3",
        "categories": [
            "cs.CC",
            "cs.CR"
        ]
    },
    {
        "title": "Non-Deterministic Communication Complexity of Regular Languages",
        "authors": [
            "Anil Ada"
        ],
        "summary": "In this thesis, we study the place of regular languages within the communication complexity setting. In particular, we are interested in the non-deterministic communication complexity of regular languages.   We show that a regular language has either O(1) or Omega(log n) non-deterministic complexity. We obtain several linear lower bound results which cover a wide range of regular languages having linear non-deterministic complexity. These lower bound results also imply a result in semigroup theory: we obtain sufficient conditions for not being in the positive variety Pol(Com).   To obtain our results, we use algebraic techniques. In the study of regular languages, the algebraic point of view pioneered by Eilenberg (\\cite{Eil74}) has led to many interesting results. Viewing a semigroup as a computational device that recognizes languages has proven to be prolific from both semigroup theory and formal languages perspectives. In this thesis, we provide further instances of such mutualism.",
        "published": "2008-01-30T21:55:13Z",
        "link": "http://arxiv.org/abs/0801.4777v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "The REESSE2+ Public-key Encryption Scheme",
        "authors": [
            "Shenghui Su",
            "Shuwang Lv"
        ],
        "summary": "This paper gives the definitions of an anomalous super-increasing sequence and an anomalous subset sum separately, proves the two properties of an anomalous super-increasing sequence, and proposes the REESSE2+ public-key encryption scheme which includes the three algorithms for key generation, encryption and decryption. The paper discusses the necessity and sufficiency of the lever function for preventing the Shamir extremum attack, analyzes the security of REESSE2+ against extracting a private key from a public key through the exhaustive search, recovering a plaintext from a ciphertext plus a knapsack of high density through the L3 lattice basis reduction method, and heuristically obtaining a plaintext through the meet-in-the-middle attack or the adaptive-chosen-ciphertext attack. The authors evaluate the time complexity of REESSE2+ encryption and decryption algorithms, compare REESSE2+ with ECC and NTRU, and find that the encryption speed of REESSE2+ is ten thousand times faster than ECC and NTRU bearing the equivalent security, and the decryption speed of REESSE2+ is roughly equivalent to ECC and NTRU respectively.",
        "published": "2008-01-31T03:50:39Z",
        "link": "http://arxiv.org/abs/0801.4817v3",
        "categories": [
            "cs.CR",
            "cs.CC"
        ]
    },
    {
        "title": "On the Double Coset Membership Problem for Permutation Groups",
        "authors": [
            "Oleg Verbitsky"
        ],
        "summary": "We show that the Double Coset Membership problem for permutation groups possesses perfect zero-knowledge proofs.",
        "published": "2008-01-31T16:19:20Z",
        "link": "http://arxiv.org/abs/0801.4911v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Zero-Knowledge Proofs of the Conjugacy for Permutation Groups",
        "authors": [
            "Oleg Verbitsky"
        ],
        "summary": "We design a perfect zero-knowledge proof system for recognition if two permutation groups are conjugate.",
        "published": "2008-01-31T16:33:15Z",
        "link": "http://arxiv.org/abs/0801.4917v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Shallow Models for Non-Iterative Modal Logics",
        "authors": [
            "Lutz Schröder",
            "Dirk Patinson"
        ],
        "summary": "The methods used to establish PSPACE-bounds for modal logics can roughly be grouped into two classes: syntax driven methods establish that exhaustive proof search can be performed in polynomial space whereas semantic approaches directly construct shallow models. In this paper, we follow the latter approach and establish generic PSPACE-bounds for a large and heterogeneous class of modal logics in a coalgebraic framework. In particular, no complete axiomatisation of the logic under scrutiny is needed. This does not only complement our earlier, syntactic, approach conceptually, but also covers a wide variety of new examples which are difficult to harness by purely syntactic means. Apart from re-proving known complexity bounds for a large variety of structurally different logics, we apply our method to obtain previously unknown PSPACE-bounds for Elgesem's logic of agency and for graded modal logic over reflexive frames.",
        "published": "2008-02-01T13:11:09Z",
        "link": "http://arxiv.org/abs/0802.0116v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "cs.MA",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "Solving the Maximum Agreement SubTree and the Maximum Compatible Tree   problems on many bounded degree trees",
        "authors": [
            "Sylvain Guillemot",
            "Francois Nicolas"
        ],
        "summary": "Given a set of leaf-labeled trees with identical leaf sets, the well-known \"Maximum Agreement SubTree\" problem (MAST) consists of finding a subtree homeomorphically included in all input trees and with the largest number of leaves. Its variant called \"Maximum Compatible Tree\" (MCT) is less stringent, as it allows the input trees to be refined. Both problems are of particular interest in computational biology, where trees encountered have often small degrees.   In this paper, we study the parameterized complexity of MAST and MCT with respect to the maximum degree, denoted by D, of the input trees. It is known that MAST is polynomial for bounded D. As a counterpart, we show that the problem is W[1]-hard with respect to parameter D. Moreover, relying on recent advances in parameterized complexity we obtain a tight lower bound: while MAST can be solved in O(N^{O(D)}) time where N denotes the input length, we show that an O(N^{o(D)}) bound is not achievable, unless SNP is contained in SE. We also show that MCT is W[1]-hard with respect to D, and that MCT cannot be solved in O(N^{o(2^{D/2})}) time, SNP is contained in SE.",
        "published": "2008-02-01T16:18:04Z",
        "link": "http://arxiv.org/abs/0802.0024v3",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "On the complexity of finding gapped motifs",
        "authors": [
            "Morris Michael",
            "Francois Nicolas",
            "Esko Ukkonen"
        ],
        "summary": "This paper has been withdrawn by the corresponding author because the newest version is now published in Journal of Discrete Algorithms.",
        "published": "2008-02-04T00:08:40Z",
        "link": "http://arxiv.org/abs/0802.0314v6",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Approximability Distance in the Space of H-Colourability Problems",
        "authors": [
            "Tommy Färnqvist",
            "Peter Jonsson",
            "Johan Thapper"
        ],
        "summary": "A graph homomorphism is a vertex map which carries edges from a source graph to edges in a target graph. We study the approximability properties of the Weighted Maximum H-Colourable Subgraph problem (MAX H-COL). The instances of this problem are edge-weighted graphs G and the objective is to find a subgraph of G that has maximal total edge weight, under the condition that the subgraph has a homomorphism to H; note that for H=K_k this problem is equivalent to MAX k-CUT. To this end, we introduce a metric structure on the space of graphs which allows us to extend previously known approximability results to larger classes of graphs. Specifically, the approximation algorithms for MAX CUT by Goemans and Williamson and MAX k-CUT by Frieze and Jerrum can be used to yield non-trivial approximation results for MAX H-COL. For a variety of graphs, we show near-optimality results under the Unique Games Conjecture. We also use our method for comparing the performance of Frieze & Jerrum's algorithm with Hastad's approximation algorithm for general MAX 2-CSP. This comparison is, in most cases, favourable to Frieze & Jerrum.",
        "published": "2008-02-04T14:32:45Z",
        "link": "http://arxiv.org/abs/0802.0423v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Some results on (a:b)-choosability",
        "authors": [
            "Shai Gutner",
            "Michael Tarsi"
        ],
        "summary": "A solution to a problem of Erd\\H{o}s, Rubin and Taylor is obtained by showing that if a graph $G$ is $(a:b)$-choosable, and $c/d > a/b$, then $G$ is not necessarily $(c:d)$-choosable. Applying probabilistic methods, an upper bound for the $k^{th}$ choice number of a graph is given. We also prove that a directed graph with maximum outdegree $d$ and no odd directed cycle is $(k(d+1):k)$-choosable for every $k \\geq 1$. Other results presented in this article are related to the strong choice number of graphs (a generalization of the strong chromatic number). We conclude with complexity analysis of some decision problems related to graph choosability.",
        "published": "2008-02-10T17:46:54Z",
        "link": "http://arxiv.org/abs/0802.1338v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "3-Way Composition of Weighted Finite-State Transducers",
        "authors": [
            "Cyril Allauzen",
            "Mehryar Mohri"
        ],
        "summary": "Composition of weighted transducers is a fundamental algorithm used in many applications, including for computing complex edit-distances between automata, or string kernels in machine learning, or to combine different components of a speech recognition, speech synthesis, or information extraction system. We present a generalization of the composition of weighted transducers, 3-way composition, which is dramatically faster in practice than the standard composition algorithm when combining more than two transducers. The worst-case complexity of our algorithm for composing three transducers $T_1$, $T_2$, and $T_3$ resulting in $T$, \\ignore{depending on the strategy used, is $O(|T|_Q d(T_1) d(T_3) + |T|_E)$ or $(|T|_Q d(T_2) + |T|_E)$,} is $O(|T|_Q \\min(d(T_1) d(T_3), d(T_2)) + |T|_E)$, where $|\\cdot|_Q$ denotes the number of states, $|\\cdot|_E$ the number of transitions, and $d(\\cdot)$ the maximum out-degree. As in regular composition, the use of perfect hashing requires a pre-processing step with linear-time expected complexity in the size of the input transducers. In many cases, this approach significantly improves on the complexity of standard composition. Our algorithm also leads to a dramatically faster composition in practice. Furthermore, standard composition can be obtained as a special case of our algorithm. We report the results of several experiments demonstrating this improvement. These theoretical and empirical improvements significantly enhance performance in the applications already mentioned.",
        "published": "2008-02-11T16:18:40Z",
        "link": "http://arxiv.org/abs/0802.1465v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Longest paths in Planar DAGs in Unambiguous Logspace",
        "authors": [
            "Nutan Limaye",
            "Meena Mahajan",
            "Prajakta Nimbhorkar"
        ],
        "summary": "We show via two different algorithms that finding the length of the longest path in planar directed acyclic graph (DAG) is in unambiguous logspace UL, and also in the complement class co-UL. The result extends to toroidal DAGs as well.",
        "published": "2008-02-12T20:08:39Z",
        "link": "http://arxiv.org/abs/0802.1699v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A review of the Statistical Mechanics approach to Random Optimization   Problems",
        "authors": [
            "Fabrizio Altarelli",
            "Remi Monasson",
            "Guilhem Semerjian",
            "Francesco Zamponi"
        ],
        "summary": "We review the connection between statistical mechanics and the analysis of random optimization problems, with particular emphasis on the random k-SAT problem. We discuss and characterize the different phase transitions that are met in these problems, starting from basic concepts. We also discuss how statistical mechanics methods can be used to investigate the behavior of local search and decimation based algorithms.",
        "published": "2008-02-13T13:45:16Z",
        "link": "http://arxiv.org/abs/0802.1829v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "SAT Has No Wizards",
        "authors": [
            "Silvano Di Zenzo"
        ],
        "summary": "An (encoded) decision problem is a pair (E, F) where E=words that encode instances of the problem, F=words to be accepted. We use \"strings\" in a technical sense. With an NP problem (E, F) we associate the \"logogram\" of F relative to E, which conveys structural information on E, F, and how F is embedded in E. The kernel Ker(P) of a program P that solves (E, F) consists of those strings in the logogram that are used by P. There are relations between Ker(P) and the complexity of P. We develop an application to SAT that relies upon a property of internal independence of SAT. We show that SAT cannot have in its logogram strings serving as collective certificates. As consequence, all programs that solve SAT have same kernel.",
        "published": "2008-02-13T14:43:40Z",
        "link": "http://arxiv.org/abs/0802.1790v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "On the Complexity of Elementary Modal Logics",
        "authors": [
            "Edith Hemaspaandra",
            "Henning Schnoor"
        ],
        "summary": "Modal logics are widely used in computer science. The complexity of modal satisfiability problems has been investigated since the 1970s, usually proving results on a case-by-case basis. We prove a very general classification for a wide class of relevant logics: Many important subclasses of modal logics can be obtained by restricting the allowed models with first-order Horn formulas. We show that the satisfiability problem for each of these logics is either NP-complete or PSPACE-hard, and exhibit a simple classification criterion. Further, we prove matching PSPACE upper bounds for many of the PSPACE-hard logics.",
        "published": "2008-02-13T18:57:26Z",
        "link": "http://arxiv.org/abs/0802.1884v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "To Broad-Match or Not to Broad-Match : An Auctioneer's Dilemma ?",
        "authors": [
            "Sudhir Kumar Singh",
            "Vwani P. Roychowdhury"
        ],
        "summary": "We initiate the study of an interesting aspect of sponsored search advertising, namely the consequences of broad match-a feature where an ad of an advertiser can be mapped to a broader range of relevant queries, and not necessarily to the particular keyword(s) that ad is associated with. Starting with a very natural setting for strategies available to the advertisers, and via a careful look through the algorithmic lens, we first propose solution concepts for the game originating from the strategic behavior of advertisers as they try to optimize their budget allocation across various keywords. Next, we consider two broad match scenarios based on factors such as information asymmetry between advertisers and the auctioneer, and the extent of auctioneer's control on the budget splitting. In the first scenario, the advertisers have the full information about broad match and relevant parameters, and can reapportion their own budgets to utilize the extra information; in particular, the auctioneer has no direct control over budget splitting. We show that, the same broad match may lead to different equilibria, one leading to a revenue improvement, whereas another to a revenue loss. This leaves the auctioneer in a dilemma - whether to broad-match or not. This motivates us to consider another broad match scenario, where the advertisers have information only about the current scenario, and the allocation of the budgets unspent in the current scenario is in the control of the auctioneer. We observe that the auctioneer can always improve his revenue by judiciously using broad match. Thus, information seems to be a double-edged sword for the auctioneer.",
        "published": "2008-02-14T03:45:07Z",
        "link": "http://arxiv.org/abs/0802.1957v2",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Kolmogorov Complexity Theory over the Reals",
        "authors": [
            "Martin Ziegler",
            "Wouter M. Koolen"
        ],
        "summary": "Kolmogorov Complexity constitutes an integral part of computability theory, information theory, and computational complexity theory -- in the discrete setting of bits and Turing machines. Over real numbers, on the other hand, the BSS-machine (aka real-RAM) has been established as a major model of computation. This real realm has turned out to exhibit natural counterparts to many notions and results in classical complexity and recursion theory; although usually with considerably different proofs. The present work investigates similarities and differences between discrete and real Kolmogorov Complexity as introduced by Montana and Pardo (1998).",
        "published": "2008-02-14T18:30:55Z",
        "link": "http://arxiv.org/abs/0802.2027v2",
        "categories": [
            "cs.CC",
            "cs.SC",
            "F.4.1; F.1.1; E.4; I.1.2; I.1.3"
        ]
    },
    {
        "title": "Domination in graphs with bounded propagation: algorithms, formulations   and hardness results",
        "authors": [
            "Ashkan Aazami"
        ],
        "summary": "We introduce a hierarchy of problems between the \\textsc{Dominating Set} problem and the \\textsc{Power Dominating Set} (PDS) problem called the $\\ell$-round power dominating set ($\\ell$-round PDS, for short) problem. For $\\ell=1$, this is the \\textsc{Dominating Set} problem, and for $\\ell\\geq n-1$, this is the PDS problem; here $n$ denotes the number of nodes in the input graph. In PDS the goal is to find a minimum size set of nodes $S$ that power dominates all the nodes, where a node $v$ is power dominated if (1) $v$ is in $S$ or it has a neighbor in $S$, or (2) $v$ has a neighbor $u$ such that $u$ and all of its neighbors except $v$ are power dominated. Note that rule (1) is the same as for the \\textsc{Dominating Set} problem, and that rule (2) is a type of propagation rule that applies iteratively. The $\\ell$-round PDS problem has the same set of rules as PDS, except we apply rule (2) in ``parallel'' in at most $\\ell-1$ rounds. We prove that $\\ell$-round PDS cannot be approximated better than $2^{\\log^{1-\\epsilon}{n}}$ even for $\\ell=4$ in general graphs. We provide a dynamic programming algorithm to solve $\\ell$-round PDS optimally in polynomial time on graphs of bounded tree-width. We present a PTAS (polynomial time approximation scheme) for $\\ell$-round PDS on planar graphs for $\\ell=O(\\tfrac{\\log{n}}{\\log{\\log{n}}})$. Finally, we give integer programming formulations for $\\ell$-round PDS.",
        "published": "2008-02-15T02:55:52Z",
        "link": "http://arxiv.org/abs/0802.2130v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Choice numbers of graphs",
        "authors": [
            "Shai Gutner"
        ],
        "summary": "A solution to a problem of Erd\\H{o}s, Rubin and Taylor is obtained by showing that if a graph $G$ is $(a:b)$-choosable, and $c/d > a/b$, then $G$ is not necessarily $(c:d)$-choosable. The simplest case of another problem, stated by the same authors, is settled, proving that every 2-choosable graph is also $(4:2)$-choosable. Applying probabilistic methods, an upper bound for the $k^{th}$ choice number of a graph is given. We also prove that a directed graph with maximum outdegree $d$ and no odd directed cycle is $(k(d+1):k)$-choosable for every $k \\geq 1$. Other results presented in this article are related to the strong choice number of graphs (a generalization of the strong chromatic number). We conclude with complexity analysis of some decision problems related to graph choosability.",
        "published": "2008-02-15T09:05:54Z",
        "link": "http://arxiv.org/abs/0802.2157v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Approximation Resistant Predicates From Pairwise Independence",
        "authors": [
            "Per Austrin",
            "Elchanan Mossel"
        ],
        "summary": "We study the approximability of predicates on $k$ variables from a domain $[q]$, and give a new sufficient condition for such predicates to be approximation resistant under the Unique Games Conjecture. Specifically, we show that a predicate $P$ is approximation resistant if there exists a balanced pairwise independent distribution over $[q]^k$ whose support is contained in the set of satisfying assignments to $P$.",
        "published": "2008-02-15T23:21:05Z",
        "link": "http://arxiv.org/abs/0802.2300v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Compressed Counting",
        "authors": [
            "Ping Li"
        ],
        "summary": "Counting is among the most fundamental operations in computing. For example, counting the pth frequency moment has been a very active area of research, in theoretical computer science, databases, and data mining. When p=1, the task (i.e., counting the sum) can be accomplished using a simple counter.   Compressed Counting (CC) is proposed for efficiently computing the pth frequency moment of a data stream signal A_t, where 0<p<=2. CC is applicable if the streaming data follow the Turnstile model, with the restriction that at the time t for the evaluation, A_t[i]>= 0, which includes the strict Turnstile model as a special case. For natural data streams encountered in practice, this restriction is minor.   The underly technique for CC is what we call skewed stable random projections, which captures the intuition that, when p=1 a simple counter suffices, and when p = 1+/\\Delta with small \\Delta, the sample complexity of a counter system should be low (continuously as a function of \\Delta). We show at small \\Delta the sample complexity (number of projections) k = O(1/\\epsilon) instead of O(1/\\epsilon^2).   Compressed Counting can serve a basic building block for other tasks in statistics and computing, for example, estimation entropies of data streams, parameter estimations using the method of moments and maximum likelihood.   Finally, another contribution is an algorithm for approximating the logarithmic norm, \\sum_{i=1}^D\\log A_t[i], and logarithmic distance. The logarithmic distance is useful in machine learning practice with heavy-tailed data.",
        "published": "2008-02-17T16:42:52Z",
        "link": "http://arxiv.org/abs/0802.2305v2",
        "categories": [
            "cs.IT",
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Fixed Point and Aperiodic Tilings",
        "authors": [
            "Bruno Durand",
            "Andrei Romashchenko",
            "Alexander Shen"
        ],
        "summary": "An aperiodic tile set was first constructed by R.Berger while proving the undecidability of the domino problem. It turned out that aperiodic tile sets appear in many topics ranging from logic (the Entscheidungsproblem) to physics (quasicrystals) We present a new construction of an aperiodic tile set that is based on Kleene's fixed-point construction instead of geometric arguments. This construction is similar to J. von Neumann self-reproducing automata; similar ideas were also used by P. Gacs in the context of error-correcting computations. The flexibility of this construction allows us to construct a \"robust\" aperiodic tile set that does not have periodic (or close to periodic) tilings even if we allow some (sparse enough) tiling errors. This property was not known for any of the existing aperiodic tile sets.",
        "published": "2008-02-18T07:50:13Z",
        "link": "http://arxiv.org/abs/0802.2432v5",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "On Subgraph Isomorphism",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Article explicitly expresses Subgraph Isomorphism by a polynomial size asymmetric linear system.",
        "published": "2008-02-19T09:06:40Z",
        "link": "http://arxiv.org/abs/0802.2612v2",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS",
            "math.CO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "The complexity of planar graph choosability",
        "authors": [
            "Shai Gutner"
        ],
        "summary": "A graph $G$ is {\\em $k$-choosable} if for every assignment of a set $S(v)$ of $k$ colors to every vertex $v$ of $G$, there is a proper coloring of $G$ that assigns to each vertex $v$ a color from $S(v)$. We consider the complexity of deciding whether a given graph is $k$-choosable for some constant $k$. In particular, it is shown that deciding whether a given planar graph is 4-choosable is NP-hard, and so is the problem of deciding whether a given planar triangle-free graph is 3-choosable. We also obtain simple constructions of a planar graph which is not 4-choosable and a planar triangle-free graph which is not 3-choosable.",
        "published": "2008-02-19T15:26:19Z",
        "link": "http://arxiv.org/abs/0802.2668v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "The Isomorphism Problem for Planar 3-Connected Graphs is in Unambiguous   Logspace",
        "authors": [
            "Thomas Thierauf",
            "Fabian Wagner"
        ],
        "summary": "The isomorphism problem for planar graphs is known to be efficiently solvable. For planar 3-connected graphs, the isomorphism problem can be solved by efficient parallel algorithms, it is in the class $AC^1$. In this paper we improve the upper bound for planar 3-connected graphs to unambiguous logspace, in fact to $UL \\cap coUL$. As a consequence of our method we get that the isomorphism problem for oriented graphs is in $NL$. We also show that the problems are hard for $L$.",
        "published": "2008-02-20T14:03:55Z",
        "link": "http://arxiv.org/abs/0802.2825v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Equilibria, Fixed Points, and Complexity Classes",
        "authors": [
            "Mihalis Yannakakis"
        ],
        "summary": "Many models from a variety of areas involve the computation of an equilibrium or fixed point of some kind. Examples include Nash equilibria in games; market equilibria; computing optimal strategies and the values of competitive games (stochastic and other games); stable configurations of neural networks; analysing basic stochastic models for evolution like branching processes and for language like stochastic context-free grammars; and models that incorporate the basic primitives of probability and recursion like recursive Markov chains. It is not known whether these problems can be solved in polynomial time. There are certain common computational principles underlying different types of equilibria, which are captured by the complexity classes PLS, PPAD, and FIXP. Representative complete problems for these classes are respectively, pure Nash equilibria in games where they are guaranteed to exist, (mixed) Nash equilibria in 2-player normal form games, and (mixed) Nash equilibria in normal form games with 3 (or more) players. This paper reviews the underlying computational principles and the corresponding classes.",
        "published": "2008-02-20T14:11:10Z",
        "link": "http://arxiv.org/abs/0802.2831v1",
        "categories": [
            "cs.CC",
            "cs.GT"
        ]
    },
    {
        "title": "Limit complexities revisited",
        "authors": [
            "Laurent Bienvenu",
            "Andrej Muchnik",
            "Alexander Shen",
            "Nikolay Vereshchagin"
        ],
        "summary": "The main goal of this paper is to put some known results in a common perspective and to simplify their proofs. We start with a simple proof of a result from (Vereshchagin, 2002) saying that $\\limsup_n\\KS(x|n)$ (here $\\KS(x|n)$ is conditional (plain) Kolmogorov complexity of $x$ when $n$ is known) equals $\\KS^{\\mathbf{0'}(x)$, the plain Kolmogorov complexity with $\\mathbf{0'$-oracle. Then we use the same argument to prove similar results for prefix complexity (and also improve results of (Muchnik, 1987) about limit frequencies), a priori probability on binary tree and measure of effectively open sets. As a by-product, we get a criterion of $\\mathbf{0'}$ Martin-L\\\"of randomness (called also 2-randomness) proved in (Miller, 2004): a sequence $\\omega$ is 2-random if and only if there exists $c$ such that any prefix $x$ of $\\omega$ is a prefix of some string $y$ such that $\\KS(y)\\ge |y|-c$. (In the 1960ies this property was suggested in (Kolmogorov, 1968) as one of possible randomness definitions; its equivalence to 2-randomness was shown in (Miller, 2004) while proving another 2-randomness criterion (see also (Nies et al. 2005)): $\\omega$ is 2-random if and only if $\\KS(x)\\ge |x|-c$ for some $c$ and infinitely many prefixes $x$ of $\\omega$. Finally, we show that the low-basis theorem can be used to get alternative proofs for these results and to improve the result about effectively open sets; this stronger version implies the 2-randomness criterion mentioned in the previous sentence.",
        "published": "2008-02-20T14:13:31Z",
        "link": "http://arxiv.org/abs/0802.2833v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On Termination for Faulty Channel Machines",
        "authors": [
            "Patricia Bouyer",
            "Nicolas Markey",
            "Joël Ouaknine",
            "Philippe Schnoebelen",
            "James Worrell"
        ],
        "summary": "A channel machine consists of a finite controller together with several fifo channels; the controller can read messages from the head of a channel and write messages to the tail of a channel. In this paper, we focus on channel machines with insertion errors, i.e., machines in whose channels messages can spontaneously appear. Such devices have been previously introduced in the study of Metric Temporal Logic. We consider the termination problem: are all the computations of a given insertion channel machine finite? We show that this problem has non-elementary, yet primitive recursive complexity.",
        "published": "2008-02-20T14:18:52Z",
        "link": "http://arxiv.org/abs/0802.2839v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "Sublinear Communication Protocols for Multi-Party Pointer Jumping and a   Related Lower Bound",
        "authors": [
            "Joshua Brody",
            "Amit Chakrabarti"
        ],
        "summary": "We study the one-way number-on-the-forehead (NOF) communication complexity of the $k$-layer pointer jumping problem with $n$ vertices per layer. This classic problem, which has connections to many aspects of complexity theory, has seen a recent burst of research activity, seemingly preparing the ground for an $\\Omega(n)$ lower bound, for constant $k$. Our first result is a surprising sublinear -- i.e., $o(n)$ -- upper bound for the problem that holds for $k \\ge 3$, dashing hopes for such a lower bound. A closer look at the protocol achieving the upper bound shows that all but one of the players involved are collapsing, i.e., their messages depend only on the composition of the layers ahead of them. We consider protocols for the pointer jumping problem where all players are collapsing. Our second result shows that a strong $n - O(\\log n)$ lower bound does hold in this case. Our third result is another upper bound showing that nontrivial protocols for (a non-Boolean version of) pointer jumping are possible even when all players are collapsing. Our lower bound result uses a novel proof technique, different from those of earlier lower bounds that had an information-theoretic flavor. We hope this is useful in further study of the problem.",
        "published": "2008-02-20T14:20:14Z",
        "link": "http://arxiv.org/abs/0802.2843v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Lower bounds for adaptive linearity tests",
        "authors": [
            "Shachar Lovett"
        ],
        "summary": "Linearity tests are randomized algorithms which have oracle access to the truth table of some function f, and are supposed to distinguish between linear functions and functions which are far from linear. Linearity tests were first introduced by (Blum, Luby and Rubenfeld, 1993), and were later used in the PCP theorem, among other applications. The quality of a linearity test is described by its correctness c - the probability it accepts linear functions, its soundness s - the probability it accepts functions far from linear, and its query complexity q - the number of queries it makes. Linearity tests were studied in order to decrease the soundness of linearity tests, while keeping the query complexity small (for one reason, to improve PCP constructions). Samorodnitsky and Trevisan (Samorodnitsky and Trevisan 2000) constructed the Complete Graph Test, and prove that no Hyper Graph Test can perform better than the Complete Graph Test. Later in (Samorodnitsky and Trevisan 2006) they prove, among other results, that no non-adaptive linearity test can perform better than the Complete Graph Test. Their proof uses the algebraic machinery of the Gowers Norm. A result by (Ben-Sasson, Harsha and Raskhodnikova 2005) allows to generalize this lower bound also to adaptive linearity tests. We also prove the same optimal lower bound for adaptive linearity test, but our proof technique is arguably simpler and more direct than the one used in (Samorodnitsky and Trevisan 2006). We also study, like (Samorodnitsky and Trevisan 2006), the behavior of linearity tests on quadratic functions. However, instead of analyzing the Gowers Norm of certain functions, we provide a more direct combinatorial proof, studying the behavior of linearity tests on random quadratic functions...",
        "published": "2008-02-20T14:26:40Z",
        "link": "http://arxiv.org/abs/0802.2857v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "A Theory for Valiant's Matchcircuits (Extended Abstract)",
        "authors": [
            "Angsheng Li",
            "Mingji Xia"
        ],
        "summary": "The computational function of a matchgate is represented by its character matrix. In this article, we show that all nonsingular character matrices are closed under matrix inverse operation, so that for every $k$, the nonsingular character matrices of $k$-bit matchgates form a group, extending the recent work of Cai and Choudhary (2006) of the same result for the case of $k=2$, and that the single and the two-bit matchgates are universal for matchcircuits, answering a question of Valiant (2002).",
        "published": "2008-02-20T14:31:38Z",
        "link": "http://arxiv.org/abs/0802.2860v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "New Combinatorial Complete One-Way Functions",
        "authors": [
            "Arist Kojevnikov",
            "Sergey I. Nikolenko"
        ],
        "summary": "In 2003, Leonid A. Levin presented the idea of a combinatorial complete one-way function and a sketch of the proof that Tiling represents such a function. In this paper, we present two new one-way functions based on semi-Thue string rewriting systems and a version of the Post Correspondence Problem and prove their completeness. Besides, we present an alternative proof of Levin's result. We also discuss the properties a combinatorial problem should have in order to hold a complete one-way function.",
        "published": "2008-02-20T14:33:39Z",
        "link": "http://arxiv.org/abs/0802.2863v1",
        "categories": [
            "cs.CC",
            "cs.CR"
        ]
    },
    {
        "title": "Efficient Algorithms for Membership in Boolean Hierarchies of Regular   Languages",
        "authors": [
            "Christian Glasser",
            "Heinz Schmitz",
            "Victor Selivanov"
        ],
        "summary": "The purpose of this paper is to provide efficient algorithms that decide membership for classes of several Boolean hierarchies for which efficiency (or even decidability) were previously not known. We develop new forbidden-chain characterizations for the single levels of these hierarchies and obtain the following results: - The classes of the Boolean hierarchy over level $\\Sigma_1$ of the dot-depth hierarchy are decidable in $NL$ (previously only the decidability was known). The same remains true if predicates mod $d$ for fixed $d$ are allowed. - If modular predicates for arbitrary $d$ are allowed, then the classes of the Boolean hierarchy over level $\\Sigma_1$ are decidable. - For the restricted case of a two-letter alphabet, the classes of the Boolean hierarchy over level $\\Sigma_2$ of the Straubing-Th\\'erien hierarchy are decidable in $NL$. This is the first decidability result for this hierarchy. - The membership problems for all mentioned Boolean-hierarchy classes are logspace many-one hard for $NL$. - The membership problems for quasi-aperiodic languages and for $d$-quasi-aperiodic languages are logspace many-one complete for $PSPACE$.",
        "published": "2008-02-20T14:40:02Z",
        "link": "http://arxiv.org/abs/0802.2868v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Succinctness of the Complement and Intersection of Regular Expressions",
        "authors": [
            "Wouter Gelade",
            "Frank Neven"
        ],
        "summary": "We study the succinctness of the complement and intersection of regular expressions. In particular, we show that when constructing a regular expression defining the complement of a given regular expression, a double exponential size increase cannot be avoided. Similarly, when constructing a regular expression defining the intersection of a fixed and an arbitrary number of regular expressions, an exponential and double exponential size increase, respectively, can in worst-case not be avoided. All mentioned lower bounds improve the existing ones by one exponential and are tight in the sense that the target expression can be constructed in the corresponding time class, i.e., exponential or double exponential time. As a by-product, we generalize a theorem by Ehrenfeucht and Zeiger stating that there is a class of DFAs which are exponentially more succinct than regular expressions, to a fixed four-letter alphabet. When the given regular expressions are one-unambiguous, as for instance required by the XML Schema specification, the complement can be computed in polynomial time whereas the bounds concerning intersection continue to hold. For the subclass of single-occurrence regular expressions, we prove a tight exponential lower bound for intersection.",
        "published": "2008-02-20T14:40:53Z",
        "link": "http://arxiv.org/abs/0802.2869v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On the approximability of the Maximum Agreement SubTree and Maximum   Compatible Tree problems",
        "authors": [
            "Sylvain Guillemot",
            "Francois Nicolas",
            "Vincent Berry",
            "Christophe Paul"
        ],
        "summary": "This paper has been withdrawn by the corresponding author because the newest version is now published in Discrete Applied Mathematics.",
        "published": "2008-02-20T15:49:45Z",
        "link": "http://arxiv.org/abs/0802.2736v7",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "General Algorithms for Testing the Ambiguity of Finite Automata",
        "authors": [
            "Cyril Allauzen",
            "Mehryar Mohri",
            "Ashish Rastogi"
        ],
        "summary": "This paper presents efficient algorithms for testing the finite, polynomial, and exponential ambiguity of finite automata with $\\epsilon$-transitions. It gives an algorithm for testing the exponential ambiguity of an automaton $A$ in time $O(|A|_E^2)$, and finite or polynomial ambiguity in time $O(|A|_E^3)$. These complexities significantly improve over the previous best complexities given for the same problem. Furthermore, the algorithms presented are simple and are based on a general algorithm for the composition or intersection of automata. We also give an algorithm to determine the degree of polynomial ambiguity of a finite automaton $A$ that is polynomially ambiguous in time $O(|A|_E^3)$. Finally, we present an application of our algorithms to an approximate computation of the entropy of a probabilistic automaton.",
        "published": "2008-02-22T05:20:08Z",
        "link": "http://arxiv.org/abs/0802.3254v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Clusters of solutions and replica symmetry breaking in random   k-satisfiability",
        "authors": [
            "Andrea Montanari",
            "Federico Ricci-Tersenghi",
            "Guilhem Semerjian"
        ],
        "summary": "We study the set of solutions of random k-satisfiability formulae through the cavity method. It is known that, for an interval of the clause-to-variables ratio, this decomposes into an exponential number of pure states (clusters). We refine substantially this picture by: (i) determining the precise location of the clustering transition; (ii) uncovering a second `condensation' phase transition in the structure of the solution set for k larger or equal than 4. These results both follow from computing the large deviation rate of the internal entropy of pure states. From a technical point of view our main contributions are a simplified version of the cavity formalism for special values of the Parisi replica symmetry breaking parameter m (in particular for m=1 via a correspondence with the tree reconstruction problem) and new large-k expansions.",
        "published": "2008-02-25T14:17:25Z",
        "link": "http://arxiv.org/abs/0802.3627v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Generic case complexity and One-Way functions",
        "authors": [
            "Alex D. Myasnikov"
        ],
        "summary": "The goal of this paper is to introduce ideas and methodology of the generic case complexity to cryptography community. This relatively new approach allows one to analyze the behavior of an algorithm on ''most'' inputs in a simple and intuitive fashion which has some practical advantages over classical methods based on averaging. We present an alternative definition of one-way function using the concepts of generic case complexity and show its equivalence to the standard definition. In addition we demonstrate the convenience of the new approach by giving a short proof that extending adversaries to a larger class of partial algorithms with errors does not change the strength of the security assumption.",
        "published": "2008-02-26T02:23:53Z",
        "link": "http://arxiv.org/abs/0802.3734v1",
        "categories": [
            "cs.CC",
            "cs.CR"
        ]
    },
    {
        "title": "Separating NOF communication complexity classes RP and NP",
        "authors": [
            "Matei David",
            "Toniann Pitassi"
        ],
        "summary": "We provide a non-explicit separation of the number-on-forehead communication complexity classes RP and NP when the number of players is up to \\delta log(n) for any \\delta<1. Recent lower bounds on Set-Disjointness [LS08,CA08] provide an explicit separation between these classes when the number of players is only up to o(loglog(n)).",
        "published": "2008-02-26T19:58:26Z",
        "link": "http://arxiv.org/abs/0802.3860v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "An algorithmic complexity interpretation of Lin's third law of   information theory",
        "authors": [
            "Joel Ratsaby"
        ],
        "summary": "Instead of static entropy we assert that the Kolmogorov complexity of a static structure such as a solid is the proper measure of disorder (or chaoticity). A static structure in a surrounding perfectly-random universe acts as an interfering entity which introduces local disruption in randomness. This is modeled by a selection rule $R$ which selects a subsequence of the random input sequence that hits the structure. Through the inequality that relates stochasticity and chaoticity of random binary sequences we maintain that Lin's notion of stability corresponds to the stability of the frequency of 1s in the selected subsequence. This explains why more complex static structures are less stable. Lin's third law is represented as the inevitable change that static structure undergo towards conforming to the universe's perfect randomness.",
        "published": "2008-02-28T12:41:08Z",
        "link": "http://arxiv.org/abs/0802.4089v2",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A O(n^8) X O(n^7) Linear Programming Model of the Quadratic Assignment   Problem",
        "authors": [
            "Moustapha Diaby"
        ],
        "summary": "This paper has been withdrawn because Theorem 21 and Corollary 22 are in error; The modeling idea is OK, but it needs 9-dimensional variables instead of the 8-dimensional variables defined in notations 6.9.   Examples of the correct model (with 9-index variables) are: (1) Diaby, M., \"Linear Programming Formulation of the Set Partitioning Problem,\" International Journal of Operational Research 8:4 (August 2010) pp. 399-427; (2) Diaby, M., \"Linear Programming Formulation of the Vertex Coloring Problem,\" International Journal of Mathematics in Operational Research 2:3 (May 2010) pp. 259-289; (3) Diaby, M., \"The Traveling Salesman Problem: A Linear Programming Formulation,\" WSEAS Transactions on Mathematics, 6:6 (June 2007) pp. 745-754.",
        "published": "2008-02-29T00:03:19Z",
        "link": "http://arxiv.org/abs/0802.4307v6",
        "categories": [
            "cs.DM",
            "cs.CC",
            "G.1.6; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Curves That Must Be Retraced",
        "authors": [
            "Xiaoyang Gu",
            "Jack H. Lutz",
            "Elvira Mayordomo"
        ],
        "summary": "We exhibit a polynomial time computable plane curve GAMMA that has finite length, does not intersect itself, and is smooth except at one endpoint, but has the following property. For every computable parametrization f of GAMMA and every positive integer n, there is some positive-length subcurve of GAMMA that f retraces at least n times. In contrast, every computable curve of finite length that does not intersect itself has a constant-speed (hence non-retracing) parametrization that is computable relative to the halting problem.",
        "published": "2008-02-29T00:58:26Z",
        "link": "http://arxiv.org/abs/0802.4312v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A compact topology for sand automata",
        "authors": [
            "Alberto Dennunzio",
            "Pierre Guillon",
            "Benoît Masson"
        ],
        "summary": "In this paper, we exhibit a strong relation between the sand automata configuration space and the cellular automata configuration space. This relation induces a compact topology for sand automata, and a new context in which sand automata are homeomorphic to cellular automata acting on a specific subshift. We show that the existing topological results for sand automata, including the Hedlund-like representation theorem, still hold. In this context, we give a characterization of the cellular automata which are sand automata, and study some dynamical behaviors such as equicontinuity. Furthermore, we deal with the nilpotency. We show that the classical definition is not meaningful for sand automata. Then, we introduce a suitable new notion of nilpotency for sand automata. Finally, we prove that this simple dynamical behavior is undecidable.",
        "published": "2008-03-01T08:36:39Z",
        "link": "http://arxiv.org/abs/0803.0055v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "The Complexity of Testing Properties of Simple Games",
        "authors": [
            "Josep Freixas",
            "Xavier Molinero",
            "Martin Olsen",
            "Maria Serna"
        ],
        "summary": "Simple games cover voting systems in which a single alternative, such as a bill or an amendment, is pitted against the status quo. A simple game or a yes-no voting system is a set of rules that specifies exactly which collections of ``yea'' votes yield passage of the issue at hand. A collection of ``yea'' voters forms a winning coalition.   We are interested on performing a complexity analysis of problems on such games depending on the game representation. We consider four natural explicit representations, winning, loosing, minimal winning, and maximal loosing. We first analyze the computational complexity of obtaining a particular representation of a simple game from a different one. We show that some cases this transformation can be done in polynomial time while the others require exponential time. The second question is classifying the complexity for testing whether a game is simple or weighted. We show that for the four types of representation both problem can be solved in polynomial time. Finally, we provide results on the complexity of testing whether a simple game or a weighted game is of a special type. In this way, we analyze strongness, properness, decisiveness and homogeneity, which are desirable properties to be fulfilled for a simple game.",
        "published": "2008-03-04T10:20:04Z",
        "link": "http://arxiv.org/abs/0803.0404v1",
        "categories": [
            "cs.GT",
            "cs.CC"
        ]
    },
    {
        "title": "Towards an Optimal Separation of Space and Length in Resolution",
        "authors": [
            "Jakob Nordström",
            "Johan Håstad"
        ],
        "summary": "Most state-of-the-art satisfiability algorithms today are variants of the DPLL procedure augmented with clause learning. The main bottleneck for such algorithms, other than the obvious one of time, is the amount of memory used. In the field of proof complexity, the resources of time and memory correspond to the length and space of resolution proofs. There has been a long line of research trying to understand these proof complexity measures, but while strong results have been proven on length our understanding of space is still quite poor. For instance, it remains open whether the fact that a formula is provable in short length implies that it is also provable in small space or whether on the contrary these measures are unrelated in the sense that short proofs can be arbitrarily complex with respect to space.   In this paper, we present some evidence that the true answer should be that the latter case holds. We do this by proving a tight bound of Theta(sqrt(n)) on the space needed for so-called pebbling contradictions over pyramid graphs of size n. This yields the first polynomial lower bound on space that is not a consequence of a corresponding lower bound on width, another well-studied measure in resolution, as well as an improvement of the weak separation in (Nordstrom 2006) of space and width from logarithmic to polynomial.   Also, continuing the line of research initiated by (Ben-Sasson 2002) into trade-offs between different proof complexity measures, we present a simplified proof of the recent length-space trade-off result in (Hertel and Pitassi 2007), and show how our ideas can be used to prove a couple of other exponential trade-offs in resolution.",
        "published": "2008-03-05T12:52:13Z",
        "link": "http://arxiv.org/abs/0803.0661v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.4.1; F.1.3; I.2.3"
        ]
    },
    {
        "title": "Complexity Analysis of Reed-Solomon Decoding over GF(2^m) Without Using   Syndromes",
        "authors": [
            "Ning Chen",
            "Zhiyuan Yan"
        ],
        "summary": "For the majority of the applications of Reed-Solomon (RS) codes, hard decision decoding is based on syndromes. Recently, there has been renewed interest in decoding RS codes without using syndromes. In this paper, we investigate the complexity of syndromeless decoding for RS codes, and compare it to that of syndrome-based decoding. Aiming to provide guidelines to practical applications, our complexity analysis differs in several aspects from existing asymptotic complexity analysis, which is typically based on multiplicative fast Fourier transform (FFT) techniques and is usually in big O notation. First, we focus on RS codes over characteristic-2 fields, over which some multiplicative FFT techniques are not applicable. Secondly, due to moderate block lengths of RS codes in practice, our analysis is complete since all terms in the complexities are accounted for. Finally, in addition to fast implementation using additive FFT techniques, we also consider direct implementation, which is still relevant for RS codes with moderate lengths. Comparing the complexities of both syndromeless and syndrome-based decoding algorithms based on direct and fast implementations, we show that syndromeless decoding algorithms have higher complexities than syndrome-based ones for high rate RS codes regardless of the implementation. Both errors-only and errors-and-erasures decoding are considered in this paper. We also derive tighter bounds on the complexities of fast polynomial multiplications based on Cantor's approach and the fast extended Euclidean algorithm.",
        "published": "2008-03-05T18:54:35Z",
        "link": "http://arxiv.org/abs/0803.0731v2",
        "categories": [
            "cs.IT",
            "cs.CC",
            "cs.DS",
            "math.IT"
        ]
    },
    {
        "title": "Knapsack cryptosystems built on NP-hard instance",
        "authors": [
            "Laurent Evain"
        ],
        "summary": "We construct three public key knapsack cryptosystems. Standard knapsack cryptosystems hide easy instances of the knapsack problem and have been broken. The systems considered in the article face this problem: They hide a random (possibly hard) instance of the knapsack problem. We provide both complexity results (size of the key, time needed to encypher/decypher...) and experimental results. Security results are given for the second cryptosystem (the fastest one and the one with the shortest key). Probabilistic polynomial reductions show that finding the private key is as difficult as factorizing a product of two primes. We also consider heuristic attacks. First, the density of the cryptosystem can be chosen arbitrarily close to one, discarding low density attacks. Finally, we consider explicit heuristic attacks based on the LLL algorithm and we prove that with respect to these attacks, the public key is as secure as a random key.",
        "published": "2008-03-06T12:20:35Z",
        "link": "http://arxiv.org/abs/0803.0845v1",
        "categories": [
            "cs.CR",
            "cs.CC",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "What Can We Learn Privately?",
        "authors": [
            "Shiva Prasad Kasiviswanathan",
            "Homin K. Lee",
            "Kobbi Nissim",
            "Sofya Raskhodnikova",
            "Adam Smith"
        ],
        "summary": "Learning problems form an important category of computational tasks that generalizes many of the computations researchers apply to large real-life data sets. We ask: what concept classes can be learned privately, namely, by an algorithm whose output does not depend too heavily on any one input or specific training example? More precisely, we investigate learning algorithms that satisfy differential privacy, a notion that provides strong confidentiality guarantees in contexts where aggregate information is released about a database containing sensitive information about individuals. We demonstrate that, ignoring computational constraints, it is possible to privately agnostically learn any concept class using a sample size approximately logarithmic in the cardinality of the concept class. Therefore, almost anything learnable is learnable privately: specifically, if a concept class is learnable by a (non-private) algorithm with polynomial sample complexity and output size, then it can be learned privately using a polynomial number of samples. We also present a computationally efficient private PAC learner for the class of parity functions. Local (or randomized response) algorithms are a practical class of private algorithms that have received extensive investigation. We provide a precise characterization of local private learning algorithms. We show that a concept class is learnable by a local algorithm if and only if it is learnable in the statistical query (SQ) model. Finally, we present a separation between the power of interactive and noninteractive local learning algorithms.",
        "published": "2008-03-06T17:50:07Z",
        "link": "http://arxiv.org/abs/0803.0924v3",
        "categories": [
            "cs.LG",
            "cs.CC",
            "cs.CR",
            "cs.DB"
        ]
    },
    {
        "title": "Robust Stochastic Chemical Reaction Networks and Bounded Tau-Leaping",
        "authors": [
            "David Soloveichik"
        ],
        "summary": "The behavior of some stochastic chemical reaction networks is largely unaffected by slight inaccuracies in reaction rates. We formalize the robustness of state probabilities to reaction rate deviations, and describe a formal connection between robustness and efficiency of simulation. Without robustness guarantees, stochastic simulation seems to require computational time proportional to the total number of reaction events. Even if the concentration (molecular count per volume) stays bounded, the number of reaction events can be linear in the duration of simulated time and total molecular count. We show that the behavior of robust systems can be predicted such that the computational work scales linearly with the duration of simulated time and concentration, and only polylogarithmically in the total molecular count. Thus our asymptotic analysis captures the dramatic speedup when molecular counts are large, and shows that for bounded concentrations the computation time is essentially invariant with molecular count. Finally, by noticing that even robust stochastic chemical reaction networks are capable of embedding complex computational problems, we argue that the linear dependence on simulated time and concentration is likely optimal.",
        "published": "2008-03-07T17:36:54Z",
        "link": "http://arxiv.org/abs/0803.1030v3",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Self-Assembly of Discrete Self-Similar Fractals",
        "authors": [
            "Matthew J. Patitz",
            "Scott M. Summers"
        ],
        "summary": "In this paper, we search for {\\it absolute} limitations of the Tile Assembly Model (TAM), along with techniques to work around such limitations. Specifically, we investigate the self-assembly of fractal shapes in the TAM. We prove that no self-similar fractal fully weakly self-assembles at temperature 1, and that certain kinds of self-similar fractals do not strictly self-assemble at any temperature. Additionally, we extend the fiber construction from Lathrop et. al. (2007) to show that any self-similar fractal belonging to a particular class of \"nice\" self-similar fractals has a fibered version that strictly self-assembles in the TAM.",
        "published": "2008-03-12T16:43:24Z",
        "link": "http://arxiv.org/abs/0803.1672v2",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "On the Topological Complexity of Infinitary Rational Relations",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We prove in this paper that there exists some infinitary rational relations which are analytic but non Borel sets, giving an answer to a question of Simonnet [Automates et Th\\'eorie Descriptive, Ph. D. Thesis, Universit\\'e Paris 7, March 1992].",
        "published": "2008-03-12T20:14:47Z",
        "link": "http://arxiv.org/abs/0803.1841v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "Local Approximation Schemes for Topology Control",
        "authors": [
            "Mirela Damian",
            "Saurav Pandit",
            "Sriram Pemmaraju"
        ],
        "summary": "This paper presents a distributed algorithm on wireless ad-hoc networks that runs in polylogarithmic number of rounds in the size of the network and constructs a linear size, lightweight, (1+\\epsilon)-spanner for any given \\epsilon > 0. A wireless network is modeled by a d-dimensional \\alpha-quasi unit ball graph (\\alpha-UBG), which is a higher dimensional generalization of the standard unit disk graph (UDG) model. The d-dimensional \\alpha-UBG model goes beyond the unrealistic ``flat world'' assumption of UDGs and also takes into account transmission errors, fading signal strength, and physical obstructions. The main result in the paper is this: for any fixed \\epsilon > 0, 0 < \\alpha \\le 1, and d \\ge 2, there is a distributed algorithm running in O(\\log n \\log^* n) communication rounds on an n-node, d-dimensional \\alpha-UBG G that computes a (1+\\epsilon)-spanner G' of G with maximum degree \\Delta(G') = O(1) and total weight w(G') = O(w(MST(G)). This result is motivated by the topology control problem in wireless ad-hoc networks and improves on existing topology control algorithms along several dimensions. The technical contributions of the paper include a new, sequential, greedy algorithm with relaxed edge ordering and lazy updating, and clustering techniques for filtering out unnecessary edges.",
        "published": "2008-03-14T14:37:12Z",
        "link": "http://arxiv.org/abs/0803.2174v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "C.2.4; F.2.2"
        ]
    },
    {
        "title": "Digital Ecosystems: Self-Organisation of Evolving Agent Populations",
        "authors": [
            "Gerard Briscoe",
            "Philippe De Wilde"
        ],
        "summary": "A primary motivation for our research in Digital Ecosystems is the desire to exploit the self-organising properties of biological ecosystems. Ecosystems are thought to be robust, scalable architectures that can automatically solve complex, dynamic problems. Self-organisation is perhaps one of the most desirable features in the systems that we engineer, and it is important for us to be able to measure self-organising behaviour. We investigate the self-organising aspects of Digital Ecosystems, created through the application of evolutionary computing to Multi-Agent Systems (MASs), aiming to determine a macroscopic variable to characterise the self-organisation of the evolving agent populations within. We study a measure for the self-organisation called Physical Complexity; based on statistical physics, automata theory, and information theory, providing a measure of information relative to the randomness in an organism's genome, by calculating the entropy in a population. We investigate an extension to include populations of variable length, and then built upon this to construct an efficiency measure to investigate clustering within evolving agent populations. Overall an insight has been achieved into where and how self-organisation occurs in our Digital Ecosystem, and how it can be quantified.",
        "published": "2008-03-18T16:59:12Z",
        "link": "http://arxiv.org/abs/0803.2675v4",
        "categories": [
            "cs.NE",
            "cs.CC",
            "C.2.4; D.2.11; H.1.0"
        ]
    },
    {
        "title": "Locked constraint satisfaction problems",
        "authors": [
            "Lenka Zdeborová",
            "Marc Mézard"
        ],
        "summary": "We introduce and study the random \"locked\" constraint satisfaction problems. When increasing the density of constraints, they display a broad \"clustered\" phase in which the space of solutions is divided into many isolated points. While the phase diagram can be found easily, these problems, in their clustered phase, are extremely hard from the algorithmic point of view: the best known algorithms all fail to find solutions. We thus propose new benchmarks of really hard optimization problems and provide insight into the origin of their typical hardness.",
        "published": "2008-03-20T10:11:00Z",
        "link": "http://arxiv.org/abs/0803.2955v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "A New Upper Bound for Max-2-Sat: A Graph-Theoretic Approach",
        "authors": [
            "Daniel Raible",
            "Henning Fernau"
        ],
        "summary": "In {\\sc MaxSat}, we ask for an assignment which satisfies the maximum number of clauses for a boolean formula in CNF. We present an algorithm yielding a run time upper bound of $O^*(2^{\\frac{1}{6.2158}})$ for {\\sc Max-2-Sat} (each clause contains at most 2 literals), where $K$ is the number of clauses. The run time has been achieved by using heuristic priorities on the choice of the variable on which we branch. The implementation of these heuristic priorities is rather simple, though they have a significant effect on the run time. The analysis is done using a tailored non-standard measure.",
        "published": "2008-03-25T11:32:22Z",
        "link": "http://arxiv.org/abs/0803.3531v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Product theorems via semidefinite programming",
        "authors": [
            "Troy Lee",
            "Rajat Mittal"
        ],
        "summary": "The tendency of semidefinite programs to compose perfectly under product has been exploited many times in complexity theory: for example, by Lovasz to determine the Shannon capacity of the pentagon; to show a direct sum theorem for non-deterministic communication complexity and direct product theorems for discrepancy; and in interactive proof systems to show parallel repetition theorems for restricted classes of games.   Despite all these examples of product theorems--some going back nearly thirty years--it was only recently that Mittal and Szegedy began to develop a general theory to explain when and why semidefinite programs behave perfectly under product. This theory captured many examples in the literature, but there were also some notable exceptions which it could not explain--namely, an early parallel repetition result of Feige and Lovasz, and a direct product theorem for the discrepancy method of communication complexity by Lee, Shraibman, and Spalek.   We extend the theory of Mittal and Szegedy to explain these cases as well. Indeed, to the best of our knowledge, our theory captures all examples of semidefinite product theorems in the literature.",
        "published": "2008-03-28T20:27:47Z",
        "link": "http://arxiv.org/abs/0803.4206v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Combinatorial Explorations in Su-Doku",
        "authors": [
            "Jean-Marie Chauvet"
        ],
        "summary": "Su-Doku, a popular combinatorial puzzle, provides an excellent testbench for heuristic explorations. Several interesting questions arise from its deceptively simple set of rules. How many distinct Su-Doku grids are there? How to find a solution to a Su-Doku puzzle? Is there a unique solution to a given Su-Doku puzzle? What is a good estimation of a puzzle's difficulty? What is the minimum puzzle size (the number of \"givens\")?   This paper explores how these questions are related to the well-known alldifferent constraint which emerges in a wide variety of Constraint Satisfaction Problems (CSP) and compares various algorithmic approaches based on different formulations of Su-Doku.",
        "published": "2008-03-29T10:07:42Z",
        "link": "http://arxiv.org/abs/0803.4253v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "I.2.8; F.2.2"
        ]
    },
    {
        "title": "On Two Dimensional Orthogonal Knapsack Problem",
        "authors": [
            "Xin Han",
            "Kazuo Iwama",
            "Guochuan Zhang"
        ],
        "summary": "In this paper, we study the following knapsack problem: Given a list of squares with profits, we are requested to pack a sublist of them into a rectangular bin (not a unit square bin) to make profits in the bin as large as possible. We first observe there is a Polynomial Time Approximation Scheme (PTAS) for the problem of packing weighted squares into rectangular bins with large resources, then apply the PTAS to the problem of packing squares with profits into a rectangular bin and get a $\\frac65+\\epsilon$ approximation algorithm.",
        "published": "2008-03-29T11:15:11Z",
        "link": "http://arxiv.org/abs/0803.4260v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Common Permutation Problem",
        "authors": [
            "Marián Dvorský"
        ],
        "summary": "In this paper we show that the following problem is NP-complete: Given an alphabet $\\Sigma$ and two strings over $\\Sigma$, the question is whether there exists a permutation of $\\Sigma$ which is a subsequence of both of the given strings.",
        "published": "2008-03-29T11:32:29Z",
        "link": "http://arxiv.org/abs/0803.4261v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A Dual Polynomial for OR",
        "authors": [
            "Robert Spalek"
        ],
        "summary": "We reprove that the approximate degree of the OR function on n bits is Omega(sqrt(n)). We consider a linear program which is feasible if and only if there is an approximate polynomial for a given function, and apply the duality theory. The duality theory says that the primal program has no solution if and only if its dual has a solution. Therefore one can prove the nonexistence of an approximate polynomial by exhibiting a dual solution, coined the dual polynomial. We construct such a polynomial.",
        "published": "2008-03-31T18:20:53Z",
        "link": "http://arxiv.org/abs/0803.4516v1",
        "categories": [
            "cs.CC",
            "F.1.2"
        ]
    },
    {
        "title": "The quantum moment problem and bounds on entangled multi-prover games",
        "authors": [
            "Andrew C. Doherty",
            "Yeong-Cherng Liang",
            "Ben Toner",
            "Stephanie Wehner"
        ],
        "summary": "We study the quantum moment problem: Given a conditional probability distribution together with some polynomial constraints, does there exist a quantum state rho and a collection of measurement operators such that (i) the probability of obtaining a particular outcome when a particular measurement is performed on rho is specified by the conditional probability distribution, and (ii) the measurement operators satisfy the constraints. For example, the constraints might specify that some measurement operators must commute.   We show that if an instance of the quantum moment problem is unsatisfiable, then there exists a certificate of a particular form proving this. Our proof is based on a recent result in algebraic geometry, the noncommutative Positivstellensatz of Helton and McCullough [Trans. Amer. Math. Soc., 356(9):3721, 2004].   A special case of the quantum moment problem is to compute the value of one-round multi-prover games with entangled provers. Under the conjecture that the provers need only share states in finite-dimensional Hilbert spaces, we prove that a hierarchy of semidefinite programs similar to the one given by Navascues, Pironio and Acin [Phys. Rev. Lett., 98:010401, 2007] converges to the entangled value of the game. It follows that the class of languages recognized by a multi-prover interactive proof system where the provers share entanglement is recursive.",
        "published": "2008-03-31T18:39:29Z",
        "link": "http://arxiv.org/abs/0803.4373v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Optimization of Enzymatic Biochemical Logic for Noise Reduction and   Scalability: How Many Biocomputing Gates Can Be Interconnected in a Circuit?",
        "authors": [
            "V. Privman",
            "G. Strack",
            "D. Solenov",
            "M. Pita",
            "E. Katz"
        ],
        "summary": "We report an experimental evaluation of the \"input-output surface\" for a biochemical AND gate. The obtained data are modeled within the rate-equation approach, with the aim to map out the gate function and cast it in the language of logic variables appropriate for analysis of Boolean logic for scalability. In order to minimize \"analog\" noise, we consider a theoretical approach for determining an optimal set for the process parameters to minimize \"analog\" noise amplification for gate concatenation. We establish that under optimized conditions, presently studied biochemical gates can be concatenated for up to order 10 processing steps. Beyond that, new paradigms for avoiding noise build-up will have to be developed. We offer a general discussion of the ideas and possible future challenges for both experimental and theoretical research for advancing scalable biochemical computing.",
        "published": "2008-03-31T19:52:30Z",
        "link": "http://arxiv.org/abs/0803.4197v2",
        "categories": [
            "q-bio.MN",
            "cond-mat.other",
            "cond-mat.soft",
            "cs.CC",
            "q-bio.BM",
            "q-bio.OT",
            "q-bio.QM",
            "quant-ph"
        ]
    },
    {
        "title": "Exhaustive enumeration unveils clustering and freezing in random 3-SAT",
        "authors": [
            "John Ardelius",
            "Lenka Zdeborová"
        ],
        "summary": "We study geometrical properties of the complete set of solutions of the random 3-satisfiability problem. We show that even for moderate system sizes the number of clusters corresponds surprisingly well with the theoretic asymptotic prediction. We locate the freezing transition in the space of solutions which has been conjectured to be relevant in explaining the onset of computational hardness in random constraint satisfaction problems.",
        "published": "2008-04-02T14:32:44Z",
        "link": "http://arxiv.org/abs/0804.0362v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "On the approximability of minmax (regret) network optimization problems",
        "authors": [
            "Adam Kasperski",
            "Pawel Zielinski"
        ],
        "summary": "In this paper the minmax (regret) versions of some basic polynomially solvable deterministic network problems are discussed. It is shown that if the number of scenarios is unbounded, then the problems under consideration are not approximable within $\\log^{1-\\epsilon} K$ for any $\\epsilon>0$ unless NP $\\subseteq$ DTIME$(n^{\\mathrm{poly} \\log n})$, where $K$ is the number of scenarios.",
        "published": "2008-04-02T16:56:03Z",
        "link": "http://arxiv.org/abs/0804.0396v3",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "A Parameterized Perspective on $P_2$-Packings",
        "authors": [
            "Jianer Chen",
            "Henning Fernau",
            "Dan Ning",
            "Daniel Raible",
            "Jianxin Wang"
        ],
        "summary": "}We study (vertex-disjoint) $P_2$-packings in graphs under a parameterized perspective. Starting from a maximal $P_2$-packing $\\p$ of size $j$ we use extremal arguments for determining how many vertices of $\\p$ appear in some $P_2$-packing of size $(j+1)$. We basically can 'reuse' $2.5j$ vertices. We also present a kernelization algorithm that gives a kernel of size bounded by $7k$. With these two results we build an algorithm which constructs a $P_2$-packing of size $k$ in time $\\Oh^*(2.482^{3k})$.",
        "published": "2008-04-03T14:36:19Z",
        "link": "http://arxiv.org/abs/0804.0570v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "On ad hoc routing with guaranteed delivery",
        "authors": [
            "Mark Braverman"
        ],
        "summary": "We point out a simple poly-time log-space routing algorithm in ad hoc networks with guaranteed delivery using universal exploration sequences.",
        "published": "2008-04-05T16:16:38Z",
        "link": "http://arxiv.org/abs/0804.0862v1",
        "categories": [
            "cs.DC",
            "cs.CC",
            "C.2.2; F.2.2"
        ]
    },
    {
        "title": "Derandomizing the Isolation Lemma and Lower Bounds for Circuit Size",
        "authors": [
            "V. Arvind",
            "Partha Mukhopadhyay"
        ],
        "summary": "The isolation lemma of Mulmuley et al \\cite{MVV87} is an important tool in the design of randomized algorithms and has played an important role in several nontrivial complexity upper bounds. On the other hand, polynomial identity testing is a well-studied algorithmic problem with efficient randomized algorithms and the problem of obtaining efficient \\emph{deterministic} identity tests has received a lot of attention recently. The goal of this note is to compare the isolation lemma with polynomial identity testing: 1. We show that derandomizing reasonably restricted versions of the isolation lemma implies circuit size lower bounds. We derive the circuit lower bounds by examining the connection between the isolation lemma and polynomial identity testing. We give a randomized polynomial-time identity test for noncommutative circuits of polynomial degree based on the isolation lemma. Using this result, we show that derandomizing the isolation lemma implies noncommutative circuit size lower bounds. The restricted versions of the isolation lemma we consider are natural and would suffice for the standard applications of the isolation lemma. 2. From the result of Klivans-Spielman \\cite{KS01} we observe that there is a randomized polynomial-time identity test for commutative circuits of polynomial degree, also based on a more general isolation lemma for linear forms. Consequently, derandomization of (a suitable version of) this isolation lemma also implies circuit size lower bounds in the commutative setting.",
        "published": "2008-04-07T04:04:21Z",
        "link": "http://arxiv.org/abs/0804.0957v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "P is a proper subset of NP",
        "authors": [
            "Jerrald Meek"
        ],
        "summary": "The purpose of this article is to examine and limit the conditions in which the P complexity class could be equivalent to the NP complexity class. Proof is provided by demonstrating that as the number of clauses in a NP-complete problem approaches infinity, the number of input sets processed per computation performed also approaches infinity when solved by a polynomial time solution. It is then possible to determine that the only deterministic optimization of a NP-complete problem that could prove P = NP would be one that examines no more than a polynomial number of input sets for a given problem.   It is then shown that subdividing the set of all possible input sets into a representative polynomial search partition is a problem in the FEXP complexity class. The findings of this article are combined with the findings of other articles in this series of 4 articles. The final conclusion will be demonstrated that P =/= NP.",
        "published": "2008-04-07T16:58:59Z",
        "link": "http://arxiv.org/abs/0804.1079v12",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Adversary lower bounds for nonadaptive quantum algorithms",
        "authors": [
            "Pacal Koiran",
            "Jürgen Landes",
            "Natacha Portier",
            "Penghui Yao"
        ],
        "summary": "We present general methods for proving lower bounds on the query complexity of nonadaptive quantum algorithms. Our results are based on the adversary method of Ambainis.",
        "published": "2008-04-09T09:36:50Z",
        "link": "http://arxiv.org/abs/0804.1440v1",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "A complexity dichotomy for partition functions with mixed signs",
        "authors": [
            "Leslie Ann Goldberg",
            "Martin Grohe",
            "Mark Jerrum",
            "Marc Thurley"
        ],
        "summary": "Partition functions, also known as homomorphism functions, form a rich family of graph invariants that contain combinatorial invariants such as the number of k-colourings or the number of independent sets of a graph and also the partition functions of certain \"spin glass\" models of statistical physics such as the Ising model.   Building on earlier work by Dyer, Greenhill and Bulatov, Grohe, we completely classify the computational complexity of partition functions. Our main result is a dichotomy theorem stating that every partition function is either computable in polynomial time or #P-complete. Partition functions are described by symmetric matrices with real entries, and we prove that it is decidable in polynomial time in terms of the matrix whether a given partition function is in polynomial time or #P-complete.   While in general it is very complicated to give an explicit algebraic or combinatorial description of the tractable cases, for partition functions described by a Hadamard matrices -- these turn out to be central in our proofs -- we obtain a simple algebraic tractability criterion, which says that the tractable cases are those \"representable\" by a quadratic polynomial over the field GF(2).",
        "published": "2008-04-11T18:36:01Z",
        "link": "http://arxiv.org/abs/0804.1932v3",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2.1; F.2.2; G.2.1"
        ]
    },
    {
        "title": "Schemes for Deterministic Polynomial Factoring",
        "authors": [
            "Gábor Ivanyos",
            "Marek Karpinski",
            "Nitin Saxena"
        ],
        "summary": "In this work we relate the deterministic complexity of factoring polynomials (over finite fields) to certain combinatorial objects we call m-schemes. We extend the known conditional deterministic subexponential time polynomial factoring algorithm for finite fields to get an underlying m-scheme. We demonstrate how the properties of m-schemes relate to improvements in the deterministic complexity of factoring polynomials over finite fields assuming the generalized Riemann Hypothesis (GRH). In particular, we give the first deterministic polynomial time algorithm (assuming GRH) to find a nontrivial factor of a polynomial of prime degree n where (n-1) is a smooth number.",
        "published": "2008-04-11T23:04:17Z",
        "link": "http://arxiv.org/abs/0804.1974v1",
        "categories": [
            "cs.CC",
            "cs.SC"
        ]
    },
    {
        "title": "Parimutuel Betting on Permutations",
        "authors": [
            "Shipra Agrawal",
            "Zizhuo Wang",
            "Yinyu Ye"
        ],
        "summary": "We focus on a permutation betting market under parimutuel call auction model where traders bet on the final ranking of n candidates. We present a Proportional Betting mechanism for this market. Our mechanism allows the traders to bet on any subset of the n x n 'candidate-rank' pairs, and rewards them proportionally to the number of pairs that appear in the final outcome. We show that market organizer's decision problem for this mechanism can be formulated as a convex program of polynomial size. More importantly, the formulation yields a set of n x n unique marginal prices that are sufficient to price the bets in this mechanism, and are computable in polynomial-time. The marginal prices reflect the traders' beliefs about the marginal distributions over outcomes. We also propose techniques to compute the joint distribution over n! permutations from these marginal distributions. We show that using a maximum entropy criterion, we can obtain a concise parametric form (with only n x n parameters) for the joint distribution which is defined over an exponentially large state space. We then present an approximation algorithm for computing the parameters of this distribution. In fact, the algorithm addresses the generic problem of finding the maximum entropy distribution over permutations that has a given mean, and may be of independent interest.",
        "published": "2008-04-15T00:20:17Z",
        "link": "http://arxiv.org/abs/0804.2288v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DS",
            "cs.MA",
            "I.2.1; F.2"
        ]
    },
    {
        "title": "Theory and Applications of Two-dimensional, Null-boundary,   Nine-Neighborhood, Cellular Automata Linear rules",
        "authors": [
            "Pabitra Pal Choudhury",
            "Birendra Kumar Nayak",
            "Sudhakar Sahoo",
            "Sunil Pankaj Rath"
        ],
        "summary": "This paper deals with the theory and application of 2-Dimensional, nine-neighborhood, null- boundary, uniform as well as hybrid Cellular Automata (2D CA) linear rules in image processing. These rules are classified into nine groups depending upon the number of neighboring cells influences the cell under consideration. All the Uniform rules have been found to be rendering multiple copies of a given image depending on the groups to which they belong where as Hybrid rules are also shown to be characterizing the phenomena of zooming in, zooming out, thickening and thinning of a given image. Further, using hybrid CA rules a new searching algorithm is developed called Sweepers algorithm which is found to be applicable to simulate many inter disciplinary research areas like migration of organisms towards a single point destination, Single Attractor and Multiple Attractor Cellular Automata Theory, Pattern Classification and Clustering Problem, Image compression, Encryption and Decryption problems, Density Classification problem etc.",
        "published": "2008-04-15T10:17:35Z",
        "link": "http://arxiv.org/abs/0804.2346v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.CV"
        ]
    },
    {
        "title": "Universal Quantum Circuits",
        "authors": [
            "Debajyoti Bera",
            "Stephen Fenner",
            "Frederic Green",
            "Steve Homer"
        ],
        "summary": "We define and construct efficient depth-universal and almost-size-universal quantum circuits. Such circuits can be viewed as general-purpose simulators for central classes of quantum circuits and can be used to capture the computational power of the circuit class being simulated. For depth we construct universal circuits whose depth is the same order as the circuits being simulated. For size, there is a log factor blow-up in the universal circuits constructed here. We prove that this construction is nearly optimal.",
        "published": "2008-04-15T16:44:23Z",
        "link": "http://arxiv.org/abs/0804.2429v1",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "A Critique of a Polynomial-time SAT Solver Devised by Sergey Gubin",
        "authors": [
            "Ian Christopher",
            "Dennis Huo",
            "Bryan Jacobs"
        ],
        "summary": "This paper refutes the validity of the polynomial-time algorithm for solving satisfiability proposed by Sergey Gubin. Gubin introduces the algorithm using 3-SAT and eventually expands it to accept a broad range of forms of the Boolean satisfiability problem. Because 3-SAT is NP-complete, the algorithm would have implied P = NP, had it been correct. Additionally, this paper refutes the correctness of his polynomial-time reduction of SAT to 2-SAT.",
        "published": "2008-04-16T23:00:51Z",
        "link": "http://arxiv.org/abs/0804.2699v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Generalized Modal Satisfiability",
        "authors": [
            "Edith Hemaspaandra",
            "Henning Schnoor",
            "Ilka Schnoor"
        ],
        "summary": "It is well known that modal satisfiability is PSPACE-complete (Ladner 1977). However, the complexity may decrease if we restrict the set of propositional operators used. Note that there exist an infinite number of propositional operators, since a propositional operator is simply a Boolean function. We completely classify the complexity of modal satisfiability for every finite set of propositional operators, i.e., in contrast to previous work, we classify an infinite number of problems. We show that, depending on the set of propositional operators, modal satisfiability is PSPACE-complete, coNP-complete, or in P. We obtain this trichotomy not only for modal formulas, but also for their more succinct representation using modal circuits. We consider both the uni-modal and the multi-modal case, and study the dual problem of validity as well.",
        "published": "2008-04-17T06:57:50Z",
        "link": "http://arxiv.org/abs/0804.2729v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Parameterized Low-distortion Embeddings - Graph metrics into lines and   trees",
        "authors": [
            "Michael Fellows",
            "Fedor Fomin",
            "Daniel Lokshtanov",
            "Elena Losievskaja",
            "Frances A. Rosamond",
            "Saket Saurabh"
        ],
        "summary": "We revisit the issue of low-distortion embedding of metric spaces into the line, and more generally, into the shortest path metric of trees, from the parameterized complexity perspective.Let $M=M(G)$ be the shortest path metric of an edge weighted graph $G=(V,E)$ on $n$ vertices. We describe algorithms for the problem of finding a low distortion non-contracting embedding of $M$ into line and tree metrics.   We give an $O(nd^4(2d+1)^{2d})$ time algorithm that for an unweighted graph metric $M$ and integer $d$ either constructs an embedding of $M$ into the line with distortion at most $d$, or concludes that no such embedding exists. We find the result surprising, because the considered problem bears a strong resemblance to the notoriously hard Bandwidth Minimization problem which does not admit any FPT algorithm unless an unlikely collapse of parameterized complexity classes occurs.   We show that our algorithm can also be applied to construct small distortion embeddings of weighted graph metrics. The running time of our algorithm is $O(n(dW)^4(2d+1)^{2dW})$ where $W$ is the largest edge weight of the input graph. We also show that deciding whether a weighted graph metric $M(G)$ with maximum weight $W < |V(G)|$ can be embedded into the line with distortion at most $d$ is NP-Complete for every fixed rational $d \\geq 2$. This rules out any possibility of an algorithm with running time $O((nW)^{h(d)})$ where $h$ is a function of $d$ alone.   We generalize the result on embedding into the line by proving that for any tree $T$ with maximum degree $\\Delta$, embedding of $M$ into a shortest path metric of $T$ is FPT, parameterized by $(\\Delta,d)$.",
        "published": "2008-04-18T14:39:41Z",
        "link": "http://arxiv.org/abs/0804.3028v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Wadge Degrees of Infinitary Rational Relations",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We show that, from the topological point of view, 2-tape B\\\"uchi automata have the same accepting power as Turing machines equipped with a B\\\"uchi acceptance condition. The Borel and the Wadge hierarchies of the class RAT_omega of infinitary rational relations accepted by 2-tape B\\\"uchi automata are equal to the Borel and the Wadge hierarchies of omega-languages accepted by real-time B\\\"uchi 1-counter automata or by B\\\"uchi Turing machines. In particular, for every non-null recursive ordinal $\\alpha$, there exist some $\\Sigma^0_\\alpha$-complete and some $\\Pi^0_\\alpha$-complete infinitary rational relations. And the supremum of the set of Borel ranks of infinitary rational relations is an ordinal $\\gamma^1_2$ which is strictly greater than the first non-recursive ordinal $\\omega_1^{CK}$. This very surprising result gives answers to questions of Simonnet (1992) and of Lescow and Thomas (1988,1994).",
        "published": "2008-04-21T19:12:44Z",
        "link": "http://arxiv.org/abs/0804.3266v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "Towards a stable definition of Kolmogorov-Chaitin complexity",
        "authors": [
            "Jean-Paul Delahaye",
            "Hector Zenil"
        ],
        "summary": "Although information content is invariant up to an additive constant, the range of possible additive constants applicable to programming languages is so large that in practice it plays a major role in the actual evaluation of K(s), the Kolmogorov-Chaitin complexity of a string s. Some attempts have been made to arrive at a framework stable enough for a concrete definition of K, independent of any constant under a programming language, by appealing to the \"naturalness\" of the language in question. The aim of this paper is to present an approach to overcome the problem by looking at a set of models of computation converging in output probability distribution such that that \"naturalness\" can be inferred, thereby providing a framework for a stable definition of K under the set of convergent models of computation.",
        "published": "2008-04-22T07:18:37Z",
        "link": "http://arxiv.org/abs/0804.3459v3",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "Combinatorial invariants for graph isomorphism problem",
        "authors": [
            "Jarek Duda"
        ],
        "summary": "Presented approach in polynomial time calculates large number of invariants for each vertex, which won't change with graph isomorphism and should fully determine the graph. For example numbers of closed paths of length k for given starting vertex, what can be though as the diagonal terms of k-th power of the adjacency matrix. For k=2 we would get degree of verities invariant, higher describes local topology deeper. Now if two graphs are isomorphic, they have the same set of such vectors of invariants - we can sort theses vectors lexicographically and compare them. If they agree, permutations from sorting allow to reconstruct the isomorphism. I'm presenting arguments that these invariants should fully determine the graph, but unfortunately I can't prove it in this moment. This approach can give hope, that maybe P=NP - instead of checking all instances, we should make arithmetics on these large numbers.",
        "published": "2008-04-22T22:16:46Z",
        "link": "http://arxiv.org/abs/0804.3615v4",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "On Computing the Shadows and Slices of Polytopes",
        "authors": [
            "Hans Raj Tiwary"
        ],
        "summary": "We study the complexity of computing the projection of an arbitrary $d$-polytope along $k$ orthogonal vectors for various input and output forms. We show that if $d$ and $k$ are part of the input (i.e. not a constant) and we are interested in output-sensitive algorithms, then in most forms the problem is equivalent to enumerating vertices of polytopes, except in two where it is NP-hard. In two other forms the problem is trivial. We also review the complexity of computing projections when the projection directions are in some sense non-degenerate. For full-dimensional polytopes containing origin in the interior, projection is an operation dual to intersecting the polytope with a suitable linear subspace and so the results in this paper can be dualized by interchanging vertices with facets and projection with intersection. To compare the complexity of projection and vertex enumeration, we define new complexity classes based on the complexity of Vertex Enumeration.",
        "published": "2008-04-25T17:05:16Z",
        "link": "http://arxiv.org/abs/0804.4150v2",
        "categories": [
            "cs.CC",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Lattice Problems, Gauge Functions and Parameterized Algorithms",
        "authors": [
            "V. Arvind",
            "Pushkar S. Joglekar"
        ],
        "summary": "Given a k-dimensional subspace M\\subseteq \\R^n and a full rank integer lattice L\\subseteq \\R^n, the \\emph{subspace avoiding problem} SAP is to find a shortest vector in L\\setminus M. Treating k as a parameter, we obtain new parameterized approximation and exact algorithms for SAP based on the AKS sieving technique. More precisely, we give a randomized $(1+\\epsilon)$-approximation algorithm for parameterized SAP that runs in time 2^{O(n)}.(1/\\epsilon)^k, where the parameter k is the dimension of the subspace M. Thus, we obtain a 2^{O(n)} time algorithm for \\epsilon=2^{-O(n/k)}. We also give a 2^{O(n+k\\log k)} exact algorithm for the parameterized SAP for any \\ell_p norm.   Several of our algorithms work for all gauge functions as metric with some natural restrictions, in particular for all \\ell_p norms. We also prove an \\Omega(2^n) lower bound on the query complexity of AKS sieving based exact algorithms for SVP that accesses the gauge function as oracle.",
        "published": "2008-04-30T06:39:21Z",
        "link": "http://arxiv.org/abs/0804.4744v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "The communication complexity of non-signaling distributions",
        "authors": [
            "Julien Degorre",
            "Marc Kaplan",
            "Sophie Laplante",
            "Jérémie Roland"
        ],
        "summary": "We study a model of communication complexity that encompasses many well-studied problems, including classical and quantum communication complexity, the complexity of simulating distributions arising from bipartite measurements of shared quantum states, and XOR games. In this model, Alice gets an input x, Bob gets an input y, and their goal is to each produce an output a,b distributed according to some pre-specified joint distribution p(a,b|x,y).   We introduce a new technique based on affine combinations of lower-complexity distributions. Specifically, we introduce two complexity measures, one which gives lower bounds on classical communication, and one for quantum communication. These measures can be expressed as convex optimization problems. We show that the dual formulations have a striking interpretation, since they coincide with maximum violations of Bell and Tsirelson inequalities. The dual expressions are closely related to the winning probability of XOR games. These lower bounds subsume many known communication complexity lower bound methods, most notably the recent lower bounds of Linial and Shraibman for the special case of Boolean functions.   We show that the gap between the quantum and classical lower bounds is at most linear in the size of the support of the distribution, and does not depend on the size of the inputs. This translates into a bound on the gap between maximal Bell and Tsirelson inequality violations, which was previously known only for the case of distributions with Boolean outcomes and uniform marginals.   Finally, we give an exponential upper bound on quantum and classical communication complexity in the simultaneous messages model, for any non-signaling distribution. One consequence is a simple proof that any quantum distribution can be approximated with a constant number of bits of communication.",
        "published": "2008-04-30T16:08:58Z",
        "link": "http://arxiv.org/abs/0804.4859v5",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "The Tsallis entropy and the Shannon entropy of a universal probability",
        "authors": [
            "Kohtaro Tadaki"
        ],
        "summary": "We study the properties of Tsallis entropy and Shannon entropy from the point of view of algorithmic randomness. In algorithmic information theory, there are two equivalent ways to define the program-size complexity K(s) of a given finite binary string s. In the standard way, K(s) is defined as the length of the shortest input string for the universal self-delimiting Turing machine to output s. In the other way, the so-called universal probability m is introduced first, and then K(s) is defined as -log_2 m(s) without reference to the concept of program-size. In this paper, we investigate the properties of the Shannon entropy, the power sum, and the Tsallis entropy of a universal probability by means of the notion of program-size complexity. We determine the convergence or divergence of each of these three quantities, and evaluate its degree of randomness if it converges.",
        "published": "2008-05-01T21:32:09Z",
        "link": "http://arxiv.org/abs/0805.0154v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "Algorithms for Probabilistically-Constrained Models of Risk-Averse   Stochastic Optimization with Black-Box Distributions",
        "authors": [
            "Chaitanya Swamy"
        ],
        "summary": "We consider various stochastic models that incorporate the notion of risk-averseness into the standard 2-stage recourse model, and develop novel techniques for solving the algorithmic problems arising in these models. A key notable feature of our work that distinguishes it from work in some other related models, such as the (standard) budget model and the (demand-) robust model, is that we obtain results in the black-box setting, that is, where one is given only sampling access to the underlying distribution. Our first model, which we call the risk-averse budget model, incorporates the notion of risk-averseness via a probabilistic constraint that restricts the probability (according to the underlying distribution) with which the second-stage cost may exceed a given budget B to at most a given input threshold \\rho. We also a consider a closely-related model that we call the risk-averse robust model, where we seek to minimize the first-stage cost and the (1-\\rho)-quantile of the second-stage cost.   We obtain approximation algorithms for a variety of combinatorial optimization problems including the set cover, vertex cover, multicut on trees, min cut, and facility location problems, in the risk-averse budget and robust models with black-box distributions. We obtain near-optimal solutions that preserve the budget approximately and incur a small blow-up of the probability threshold (both of which are unavoidable). To the best of our knowledge, these are the first approximation results for problems involving probabilistic constraints and black-box distributions. A major component of our results is a fully polynomial approximation scheme for solving the LP-relaxation of the risk-averse problem.",
        "published": "2008-05-04T03:57:52Z",
        "link": "http://arxiv.org/abs/0805.0389v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM",
            "F.2.2; G.1.6; G.3"
        ]
    },
    {
        "title": "The Tractability of Model-Checking for LTL: The Good, the Bad, and the   Ugly Fragments",
        "authors": [
            "Michael Bauland",
            "Martin Mundhenk",
            "Thomas Schneider",
            "Henning Schnoor",
            "Ilka Schnoor",
            "Heribert Vollmer"
        ],
        "summary": "In a seminal paper from 1985, Sistla and Clarke showed that the model-checking problem for Linear Temporal Logic (LTL) is either NP-complete or PSPACE-complete, depending on the set of temporal operators used. If, in contrast, the set of propositional operators is restricted, the complexity may decrease. This paper systematically studies the model-checking problem for LTL formulae over restricted sets of propositional and temporal operators. For almost all combinations of temporal and propositional operators, we determine whether the model-checking problem is tractable (in P) or intractable (NP-hard). We then focus on the tractable cases, showing that they all are NL-complete or even logspace solvable. This leads to a surprising gap in complexity between tractable and intractable cases. It is worth noting that our analysis covers an infinite set of problems, since there are infinitely many sets of propositional operators.",
        "published": "2008-05-05T09:48:23Z",
        "link": "http://arxiv.org/abs/0805.0498v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; I.2.4"
        ]
    },
    {
        "title": "Analysis of the Deterministic Polynomial Time Solvability of the   0-1-Knapsack Problem",
        "authors": [
            "Jerrald Meek"
        ],
        "summary": "Previously the author has demonstrated that a representative polynomial search partition is required to solve a NP-complete problem in deterministic polynomial time. It has also been demonstrated that finding such a partition can only be done in deterministic polynomial time if the form of the problem provides a simple method for producing the partition. It is the purpose of this article to demonstrate that no deterministic polynomial time method exists to produce a representative polynomial search partition for the Knapsack problem.",
        "published": "2008-05-05T12:34:27Z",
        "link": "http://arxiv.org/abs/0805.0517v5",
        "categories": [
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "On Expanded Cyclic Codes",
        "authors": [
            "Yingquan Wu"
        ],
        "summary": "The paper has a threefold purpose. The first purpose is to present an explicit description of expanded cyclic codes defined in $\\GF(q^m)$. The proposed explicit construction of expanded generator matrix and expanded parity check matrix maintains the symbol-wise algebraic structure and thus keeps many important original characteristics. The second purpose of this paper is to identify a class of constant-weight cyclic codes. Specifically, we show that a well-known class of $q$-ary BCH codes excluding the all-zero codeword are constant-weight cyclic codes. Moreover, we show this class of codes achieve the Plotkin bound. The last purpose of the paper is to characterize expanded cyclic codes utilizing the proposed expanded generator matrix and parity check matrix. We characterize the properties of component codewords of a codeword and particularly identify the precise conditions under which a codeword can be represented by a subbasis. Our developments reveal an alternative while more general view on the subspace subcodes of Reed-Solomon codes. With the new insights, we present an improved lower bound on the minimum distance of an expanded cyclic code by exploiting the generalized concatenated structure. We also show that the fixed-rate binary expanded Reed-Solomon codes are asymptotically \"bad\", in the sense that the ratio of minimum distance over code length diminishes with code length going to infinity. It overturns the prevalent conjecture that they are \"good\" codes and deviates from the ensemble of generalized Reed-Solomon codes which asymptotically achieves the Gilbert-Varshamov bound.",
        "published": "2008-05-05T22:51:12Z",
        "link": "http://arxiv.org/abs/0805.0615v2",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT",
            "math.RA"
        ]
    },
    {
        "title": "Bounds for self-stabilization in unidirectional networks",
        "authors": [
            "Samuel Bernard",
            "Stéphane Devismes",
            "Maria Gradinariu Potop-Butucaru",
            "Sébastien Tixeuil"
        ],
        "summary": "A distributed algorithm is self-stabilizing if after faults and attacks hit the system and place it in some arbitrary global state, the systems recovers from this catastrophic situation without external intervention in finite time. Unidirectional networks preclude many common techniques in self-stabilization from being used, such as preserving local predicates. In this paper, we investigate the intrinsic complexity of achieving self-stabilization in unidirectional networks, and focus on the classical vertex coloring problem. When deterministic solutions are considered, we prove a lower bound of $n$ states per process (where $n$ is the network size) and a recovery time of at least $n(n-1)/2$ actions in total. We present a deterministic algorithm with matching upper bounds that performs in arbitrary graphs. When probabilistic solutions are considered, we observe that at least $\\Delta + 1$ states per process and a recovery time of $\\Omega(n)$ actions in total are required (where $\\Delta$ denotes the maximal degree of the underlying simple undirected graph). We present a probabilistically self-stabilizing algorithm that uses $\\mathtt{k}$ states per process, where $\\mathtt{k}$ is a parameter of the algorithm. When $\\mathtt{k}=\\Delta+1$, the algorithm recovers in expected $O(\\Delta n)$ actions. When $\\mathtt{k}$ may grow arbitrarily, the algorithm recovers in expected O(n) actions in total. Thus, our algorithm can be made optimal with respect to space or time complexity.",
        "published": "2008-05-07T07:39:14Z",
        "link": "http://arxiv.org/abs/0805.0851v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Nonlinear Optimization over a Weighted Independence System",
        "authors": [
            "Jon Lee",
            "Shmuel Onn",
            "Robert Weismantel"
        ],
        "summary": "We consider the problem of optimizing a nonlinear objective function over a weighted independence system presented by a linear-optimization oracle. We provide a polynomial-time algorithm that determines an r-best solution for nonlinear functions of the total weight of an independent set, where r is a constant that depends on certain Frobenius numbers of the individual weights and is independent of the size of the ground set. In contrast, we show that finding an optimal (0-best) solution requires exponential time even in a very special case of the problem.",
        "published": "2008-05-07T11:05:01Z",
        "link": "http://arxiv.org/abs/0805.0954v1",
        "categories": [
            "math.CO",
            "cs.CC",
            "cs.DM",
            "math.OC",
            "05A, 15A, 51M, 52A, 52B, 52C, 62H, 68Q, 68R, 68U, 68W, 90B, 90C"
        ]
    },
    {
        "title": "Decomposition Techniques for Subgraph Matching",
        "authors": [
            "Stephane Zampelli",
            "Martin Mann",
            "Yves Deville",
            "Rolf Backofen"
        ],
        "summary": "In the constraint programming framework, state-of-the-art static and dynamic decomposition techniques are hard to apply to problems with complete initial constraint graphs. For such problems, we propose a hybrid approach of these techniques in the presence of global constraints. In particular, we solve the subgraph isomorphism problem. Further we design specific heuristics for this hard problem, exploiting its special structure to achieve decomposition. The underlying idea is to precompute a static heuristic on a subset of its constraint network, to follow this static ordering until a first problem decomposition is available, and to switch afterwards to a fully propagated, dynamically decomposing search. Experimental results show that, for sparse graphs, our decomposition method solves more instances than dedicated, state-of-the-art matching algorithms or standard constraint programming approaches.",
        "published": "2008-05-07T17:41:47Z",
        "link": "http://arxiv.org/abs/0805.1030v1",
        "categories": [
            "cs.CC",
            "cs.CL"
        ]
    },
    {
        "title": "Physically-Relativized Church-Turing Hypotheses",
        "authors": [
            "Martin Ziegler"
        ],
        "summary": "We turn `the' Church-Turing Hypothesis from an ambiguous source of sensational speculations into a (collection of) sound and well-defined scientific problem(s):   Examining recent controversies, and causes for misunderstanding, concerning the state of the Church-Turing Hypothesis (CTH), suggests to study the CTH relative to an arbitrary but specific physical theory--rather than vaguely referring to ``nature'' in general. To this end we combine (and compare) physical structuralism with (models of computation in) complexity theory. The benefit of this formal framework is illustrated by reporting on some previous, and giving one new, example result(s) of computability and complexity in computational physics.",
        "published": "2008-05-09T07:27:37Z",
        "link": "http://arxiv.org/abs/0805.1292v1",
        "categories": [
            "physics.comp-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Almost-natural proofs",
        "authors": [
            "Timothy Y. Chow"
        ],
        "summary": "Razborov and Rudich have shown that so-called \"natural proofs\" are not useful for separating P from NP unless hard pseudorandom number generators do not exist. This famous result is widely regarded as a serious barrier to proving strong lower bounds in circuit complexity theory.   By definition, a natural combinatorial property satisfies two conditions, constructivity and largeness. Our main result is that if the largeness condition is weakened slightly, then not only does the Razborov-Rudich proof break down, but such \"almost-natural\" (and useful) properties provably exist. Specifically, under the same pseudorandomness assumption that Razborov and Rudich make, a simple, explicit property that we call \"discrimination\" suffices to separate P/poly from NP; discrimination is nearly linear-time computable and almost large, having density 2^{-q(n)} where q is a quasi-polynomial function. For those who hope to separate P from NP using random function properties in some sense, discrimination is interesting, because it is constructive, yet may be thought of as a minor alteration of a property of a random function.   The proof relies heavily on the self-defeating character of natural proofs. Our proof technique also yields an unconditional result, namely that there exist almost-large and useful properties that are constructive, if we are allowed to call non-uniform low-complexity classes \"constructive.\" We note, though, that this unconditional result can also be proved by a more conventional counting argument.   Finally, we give an alternative proof, communicated to us by Salil Vadhan at FOCS 2008, of one of our theorems, and we make some speculative remarks on the future prospects for proving strong circuit lower bounds.",
        "published": "2008-05-09T18:14:43Z",
        "link": "http://arxiv.org/abs/0805.1385v3",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Model Checking One-clock Priced Timed Automata",
        "authors": [
            "Patricia Bouyer",
            "Kim G. Larsen",
            "Nicolas Markey"
        ],
        "summary": "We consider the model of priced (a.k.a. weighted) timed automata, an extension of timed automata with cost information on both locations and transitions, and we study various model-checking problems for that model based on extensions of classical temporal logics with cost constraints on modalities. We prove that, under the assumption that the model has only one clock, model-checking this class of models against the logic WCTL, CTL with cost-constrained modalities, is PSPACE-complete (while it has been shown undecidable as soon as the model has three clocks). We also prove that model-checking WMTL, LTL with cost-constrained modalities, is decidable only if there is a single clock in the model and a single stopwatch cost variable (i.e., whose slopes lie in {0,1}).",
        "published": "2008-05-10T08:56:07Z",
        "link": "http://arxiv.org/abs/0805.1457v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.GT",
            "F.1.1; F.3.1"
        ]
    },
    {
        "title": "Efficiently Simulating Higher-Order Arithmetic by a First-Order Theory   Modulo",
        "authors": [
            "Guillaume Burel"
        ],
        "summary": "In deduction modulo, a theory is not represented by a set of axioms but by a congruence on propositions modulo which the inference rules of standard deductive systems---such as for instance natural deduction---are applied. Therefore, the reasoning that is intrinsic of the theory does not appear in the length of proofs. In general, the congruence is defined through a rewrite system over terms and propositions. We define a rigorous framework to study proof lengths in deduction modulo, where the congruence must be computed in polynomial time. We show that even very simple rewrite systems lead to arbitrary proof-length speed-ups in deduction modulo, compared to using axioms. As higher-order logic can be encoded as a first-order theory in deduction modulo, we also study how to reinterpret, thanks to deduction modulo, the speed-ups between higher-order and first-order arithmetics that were stated by G\\\"odel. We define a first-order rewrite system with a congruence decidable in polynomial time such that proofs of higher-order arithmetic can be linearly translated into first-order arithmetic modulo that system. We also present the whole higher-order arithmetic as a first-order system without resorting to any axiom, where proofs have the same length as in the axiomatic presentation.",
        "published": "2008-05-10T10:42:54Z",
        "link": "http://arxiv.org/abs/0805.1464v5",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.CC"
        ]
    },
    {
        "title": "Efficiently Testing Sparse GF(2) Polynomials",
        "authors": [
            "Ilias Diakonikolas",
            "Homin K. Lee",
            "Kevin Matulef",
            "Rocco A. Servedio",
            "Andrew Wan"
        ],
        "summary": "We give the first algorithm that is both query-efficient and time-efficient for testing whether an unknown function $f: \\{0,1\\}^n \\to \\{0,1\\}$ is an $s$-sparse GF(2) polynomial versus $\\eps$-far from every such polynomial. Our algorithm makes $\\poly(s,1/\\eps)$ black-box queries to $f$ and runs in time $n \\cdot \\poly(s,1/\\eps)$. The only previous algorithm for this testing problem \\cite{DLM+:07} used poly$(s,1/\\eps)$ queries, but had running time exponential in $s$ and super-polynomial in $1/\\eps$.   Our approach significantly extends the ``testing by implicit learning'' methodology of \\cite{DLM+:07}. The learning component of that earlier work was a brute-force exhaustive search over a concept class to find a hypothesis consistent with a sample of random examples. In this work, the learning component is a sophisticated exact learning algorithm for sparse GF(2) polynomials due to Schapire and Sellie \\cite{SchapireSellie:96}. A crucial element of this work, which enables us to simulate the membership queries required by \\cite{SchapireSellie:96}, is an analysis establishing new properties of how sparse GF(2) polynomials simplify under certain restrictions of ``low-influence'' sets of variables.",
        "published": "2008-05-13T00:51:30Z",
        "link": "http://arxiv.org/abs/0805.1765v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Rollout Sampling Approximate Policy Iteration",
        "authors": [
            "Christos Dimitrakakis",
            "Michail G. Lagoudakis"
        ],
        "summary": "Several researchers have recently investigated the connection between reinforcement learning and classification. We are motivated by proposals of approximate policy iteration schemes without value functions which focus on policy representation using classifiers and address policy learning as a supervised learning problem. This paper proposes variants of an improved policy iteration scheme which addresses the core sampling problem in evaluating a policy through simulation as a multi-armed bandit machine. The resulting algorithm offers comparable performance to the previous algorithm achieved, however, with significantly less computational effort. An order of magnitude improvement is demonstrated experimentally in two standard reinforcement learning domains: inverted pendulum and mountain-car.",
        "published": "2008-05-14T11:19:19Z",
        "link": "http://arxiv.org/abs/0805.2027v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Least change in the Determinant or Permanent of a matrix under   perturbation of a single element: continuous and discrete cases",
        "authors": [
            "Genta Ito"
        ],
        "summary": "We formulate the problem of finding the probability that the determinant of a matrix undergoes the least change upon perturbation of one of its elements, provided that most or all of the elements of the matrix are chosen at random and that the randomly chosen elements have a fixed probability of being non-zero. Also, we show that the procedure for finding the probability that the determinant undergoes the least change depends on whether the random variables for the matrix elements are continuous or discrete.",
        "published": "2008-05-14T15:01:46Z",
        "link": "http://arxiv.org/abs/0805.2081v1",
        "categories": [
            "cs.DM",
            "cs.CC"
        ]
    },
    {
        "title": "Approximate formulation of the probability that the Determinant or   Permanent of a matrix undergoes the least change under perturbation of a   single element",
        "authors": [
            "Genta Ito"
        ],
        "summary": "In an earlier paper, we discussed the probability that the determinant of a matrix undergoes the least change upon perturbation of one of its elements, provided that most or all of the elements of the matrix are chosen at random and that the randomly chosen elements have a fixed probability of being non-zero. In this paper, we derive approximate formulas for that probability by assuming that the terms in the permanent of a matrix are independent of one another, and we apply that assumption to several classes of matrices. In the course of deriving those formulas, we identified several integer sequences that are not listed on Sloane's Web site.",
        "published": "2008-05-14T15:05:44Z",
        "link": "http://arxiv.org/abs/0805.2083v1",
        "categories": [
            "cs.DM",
            "cs.CC"
        ]
    },
    {
        "title": "Communication Lower Bounds Using Dual Polynomials",
        "authors": [
            "Alexander A. Sherstov"
        ],
        "summary": "Representations of Boolean functions by real polynomials play an important role in complexity theory. Typically, one is interested in the least degree of a polynomial p(x_1,...,x_n) that approximates or sign-represents a given Boolean function f(x_1,...,x_n). This article surveys a new and growing body of work in communication complexity that centers around the dual objects, i.e., polynomials that certify the difficulty of approximating or sign-representing a given function. We provide a unified guide to the following results, complete with all the key proofs:   (1) Sherstov's Degree/Discrepancy Theorem, which translates lower bounds on the threshold degree of a Boolean function into upper bounds on the discrepancy of a related function;   (2) Two different methods for proving lower bounds on bounded-error communication based on the approximate degree: Sherstov's pattern matrix method and Shi and Zhu's block composition method;   (3) Extension of the pattern matrix method to the multiparty model, obtained by Lee and Shraibman and by Chattopadhyay and Ada, and the resulting improved lower bounds for DISJOINTNESS;   (4) David and Pitassi's separation of NP and BPP in multiparty communication complexity for k=(1-eps)log n players.",
        "published": "2008-05-14T18:52:06Z",
        "link": "http://arxiv.org/abs/0805.2135v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Independence of P vs. NP in regards to oracle relativizations",
        "authors": [
            "Jerrald Meek"
        ],
        "summary": "This is the third article in a series of four articles dealing with the P vs. NP question. The purpose of this work is to demonstrate that the methods used in the first two articles of this series are not affected by oracle relativizations. Furthermore, the solution to the P vs. NP problem is actually independent of oracle relativizations.",
        "published": "2008-05-14T21:10:55Z",
        "link": "http://arxiv.org/abs/0805.2170v6",
        "categories": [
            "cs.CC",
            "F.2.0"
        ]
    },
    {
        "title": "Malicious Bayesian Congestion Games",
        "authors": [
            "Martin Gairing"
        ],
        "summary": "In this paper, we introduce malicious Bayesian congestion games as an extension to congestion games where players might act in a malicious way. In such a game each player has two types. Either the player is a rational player seeking to minimize her own delay, or - with a certain probability - the player is malicious in which case her only goal is to disturb the other players as much as possible.   We show that such games do in general not possess a Bayesian Nash equilibrium in pure strategies (i.e. a pure Bayesian Nash equilibrium). Moreover, given a game, we show that it is NP-complete to decide whether it admits a pure Bayesian Nash equilibrium. This result even holds when resource latency functions are linear, each player is malicious with the same probability, and all strategy sets consist of singleton sets. For a slightly more restricted class of malicious Bayesian congestion games, we provide easy checkable properties that are necessary and sufficient for the existence of a pure Bayesian Nash equilibrium.   In the second part of the paper we study the impact of the malicious types on the overall performance of the system (i.e. the social cost). To measure this impact, we use the Price of Malice. We provide (tight) bounds on the Price of Malice for an interesting class of malicious Bayesian congestion games. Moreover, we show that for certain congestion games the advent of malicious types can also be beneficial to the system in the sense that the social cost of the worst case equilibrium decreases. We provide a tight bound on the maximum factor by which this happens.",
        "published": "2008-05-15T23:00:44Z",
        "link": "http://arxiv.org/abs/0805.2421v2",
        "categories": [
            "cs.GT",
            "cs.CC"
        ]
    },
    {
        "title": "Equivalent characterizations of partial randomness for a recursively   enumerable real",
        "authors": [
            "Kohtaro Tadaki"
        ],
        "summary": "A real number \\alpha is called recursively enumerable if there exists a computable, increasing sequence of rational numbers which converges to \\alpha. The randomness of a recursively enumerable real \\alpha can be characterized in various ways using each of the notions; program-size complexity, Martin-L\\\"{o}f test, Chaitin's \\Omega number, the domination and \\Omega-likeness of \\alpha, the universality of a computable, increasing sequence of rational numbers which converges to \\alpha, and universal probability. In this paper, we generalize these characterizations of randomness over the notion of partial randomness by parameterizing each of the notions above by a real number T\\in(0,1]. We thus present several equivalent characterizations of partial randomness for a recursively enumerable real number.",
        "published": "2008-05-17T17:44:34Z",
        "link": "http://arxiv.org/abs/0805.2691v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT",
            "math.LO"
        ]
    },
    {
        "title": "A New Structural Property of SAT",
        "authors": [
            "Silvano Di Zenzo"
        ],
        "summary": "We review a minimum set of notions from our previous paper on structural properties of SAT at arXiv:0802.1790 that will allow us to define and discuss the \"complete internal independence\" of a decision problem. This property is strictly stronger than the independence property that was called \"strong internal independence\" in cited paper. We show that SAT exhibits this property. We argue that this form of independence of a decision problem is the strongest possible for a problem. By relying upon this maximally strong form of internal independence, we reformulate in more strict terms the informal remarks on possible exponentiality of SAT that concluded our previous paper. The net result of that reformulation is a hint for a proof for SAT being exponential. We conjecture that a complete proof of that proposition can be obtained by strictly following the line of given hint of proof.",
        "published": "2008-05-20T12:07:51Z",
        "link": "http://arxiv.org/abs/0805.3058v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Extensional Uniformity for Boolean Circuits",
        "authors": [
            "Pierre McKenzie",
            "Michael Thomas",
            "Heribert Vollmer"
        ],
        "summary": "Imposing an extensional uniformity condition on a non-uniform circuit complexity class C means simply intersecting C with a uniform class L. By contrast, the usual intensional uniformity conditions require that a resource-bounded machine be able to exhibit the circuits in the circuit family defining C. We say that (C,L) has the \"Uniformity Duality Property\" if the extensionally uniform class C \\cap L can be captured intensionally by means of adding so-called \"L-numerical predicates\" to the first-order descriptive complexity apparatus describing the connection language of the circuit family defining C.   This paper exhibits positive instances and negative instances of the Uniformity Duality Property.",
        "published": "2008-05-27T08:49:36Z",
        "link": "http://arxiv.org/abs/0805.4072v2",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "An NP-hardness Result on the Monoid Frobenius Problem",
        "authors": [
            "Zhi Xu",
            "J. Shallit"
        ],
        "summary": "The following problem is NP-hard: given a regular expression $E$, decide if $E^*$ is not co-finite.",
        "published": "2008-05-27T16:18:19Z",
        "link": "http://arxiv.org/abs/0805.4049v3",
        "categories": [
            "cs.DM",
            "cs.CC",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Report on article The Travelling Salesman Problem: A Linear Programming   Formulation",
        "authors": [
            "Radoslaw Hofman"
        ],
        "summary": "This article describes counter example prepared in order to prove that linear formulation of TSP problem proposed in [arXiv:0803.4354] is incorrect (it applies also to QAP problem formulation in [arXiv:0802.4307]). Article refers not only to model itself, but also to ability of extension of proposed model to be correct.",
        "published": "2008-05-30T10:22:45Z",
        "link": "http://arxiv.org/abs/0805.4718v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "G.1.6; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Sincere-Strategy Preference-Based Approval Voting Fully Resists   Constructive Control and Broadly Resists Destructive Control",
        "authors": [
            "Gabor Erdelyi",
            "Markus Nowak",
            "Joerg Rothe"
        ],
        "summary": "We study sincere-strategy preference-based approval voting (SP-AV), a system proposed by Brams and Sanver [Electoral Studies, 25(2):287-305, 2006], and here adjusted so as to coerce admissibility of the votes (rather than excluding inadmissible votes a priori), with respect to procedural control. In such control scenarios, an external agent seeks to change the outcome of an election via actions such as adding/deleting/partitioning either candidates or voters. SP-AV combines the voters' preference rankings with their approvals of candidates, where in elections with at least two candidates the voters' approval strategies are adjusted--if needed--to approve of their most-preferred candidate and to disapprove of their least-preferred candidate. This rule coerces admissibility of the votes even in the presence of control actions, and hybridizes, in effect, approval with pluralitiy voting.   We prove that this system is computationally resistant (i.e., the corresponding control problems are NP-hard) to 19 out of 22 types of constructive and destructive control. Thus, SP-AV has more resistances to control than is currently known for any other natural voting system with a polynomial-time winner problem. In particular, SP-AV is (after Copeland voting, see Faliszewski et al. [AAIM-2008, Springer LNCS 5034, pp. 165-176, 2008]) the second natural voting system with an easy winner-determination procedure that is known to have full resistance to constructive control, and unlike Copeland voting it in addition displays broad resistance to destructive control.",
        "published": "2008-06-03T13:27:16Z",
        "link": "http://arxiv.org/abs/0806.0535v5",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Upper and Lower Bounds on Black-Box Steganography",
        "authors": [
            "Nenad Dedić",
            "Gene Itkis",
            "Leonid Reyzin",
            "Scott Russell"
        ],
        "summary": "We study the limitations of steganography when the sender is not using any properties of the underlying channel beyond its entropy and the ability to sample from it. On the negative side, we show that the number of samples the sender must obtain from the channel is exponential in the rate of the stegosystem. On the positive side, we present the first secret-key stegosystem that essentially matches this lower bound regardless of the entropy of the underlying channel. Furthermore, for high-entropy channels, we present the first secret-key stegosystem that matches this lower bound statelessly (i.e., without requiring synchronized state between sender and receiver).",
        "published": "2008-06-04T19:07:48Z",
        "link": "http://arxiv.org/abs/0806.0837v1",
        "categories": [
            "cs.CR",
            "cs.CC",
            "cs.IT",
            "math.IT",
            "D.4.6; E.3; H.1.1"
        ]
    },
    {
        "title": "Drawing (Complete) Binary Tanglegrams: Hardness, Approximation,   Fixed-Parameter Tractability",
        "authors": [
            "Kevin Buchin",
            "Maike Buchin",
            "Jaroslaw Byrka",
            "Martin Nöllenburg",
            "Yoshio Okamoto",
            "Rodrigo I. Silveira",
            "Alexander Wolff"
        ],
        "summary": "A \\emph{binary tanglegram} is a drawing of a pair of rooted binary trees whose leaf sets are in one-to-one correspondence; matching leaves are connected by inter-tree edges. For applications, for example, in phylogenetics, it is essential that both trees are drawn without edge crossings and that the inter-tree edges have as few crossings as possible. It is known that finding a tanglegram with the minimum number of crossings is NP-hard and that the problem is fixed-parameter tractable with respect to that number.   We prove that under the Unique Games Conjecture there is no constant-factor approximation for binary trees. We show that the problem is NP-hard even if both trees are complete binary trees. For this case we give an $O(n^3)$-time 2-approximation and a new, simple fixed-parameter algorithm. We show that the maximization version of the dual problem for binary trees can be reduced to a version of MaxCut for which the algorithm of Goemans and Williamson yields a 0.878-approximation.",
        "published": "2008-06-05T09:31:57Z",
        "link": "http://arxiv.org/abs/0806.0920v3",
        "categories": [
            "cs.CG",
            "cs.CC"
        ]
    },
    {
        "title": "3-connected Planar Graph Isomorphism is in Log-space",
        "authors": [
            "Samir Datta",
            "Nutan Limaye",
            "Prajakta Nimbhorkar"
        ],
        "summary": "We show that the isomorphism of 3-connected planar graphs can be decided in deterministic log-space. This improves the previously known bound UL$\\cap$coUL of Thierauf and Wagner.",
        "published": "2008-06-05T19:33:29Z",
        "link": "http://arxiv.org/abs/0806.1041v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "The Mixing Time of Glauber Dynamics for Colouring Regular Trees",
        "authors": [
            "Leslie Ann Goldberg",
            "Mark Jerrum",
            "Marek Karpinski"
        ],
        "summary": "We consider Metropolis Glauber dynamics for sampling proper $q$-colourings of the $n$-vertex complete $b$-ary tree when $3\\leq q\\leq b/2\\ln(b)$. We give both upper and lower bounds on the mixing time. For fixed $q$ and $b$, our upper bound is $n^{O(b/\\log b)}$ and our lower bound is $n^{\\Omega(b/q \\log(b))}$, where the constants implicit in the $O()$ and $\\Omega()$ notation do not depend upon $n$, $q$ or $b$.",
        "published": "2008-06-05T19:58:32Z",
        "link": "http://arxiv.org/abs/0806.0921v1",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "On the Approximability of Comparing Genomes with Duplicates",
        "authors": [
            "Sébastien Angibaud",
            "Guillaume Fertin",
            "Irena Rusu",
            "Annelyse Thevenin",
            "Stéphane Vialette"
        ],
        "summary": "A central problem in comparative genomics consists in computing a (dis-)similarity measure between two genomes, e.g. in order to construct a phylogeny. All the existing measures are defined on genomes without duplicates. However, we know that genes can be duplicated within the same genome. One possible approach to overcome this difficulty is to establish a one-to-one correspondence (i.e. a matching) between genes of both genomes, where the correspondence is chosen in order to optimize the studied measure. In this paper, we are interested in three measures (number of breakpoints, number of common intervals and number of conserved intervals) and three models of matching (exemplar, intermediate and maximum matching models). We prove that, for each model and each measure M, computing a matching between two genomes that optimizes M is APX-hard. We also study the complexity of the following problem: is there an exemplarization (resp. an intermediate/maximum matching) that induces no breakpoint? We prove the problem to be NP-Complete in the exemplar model for a new class of instances, and we show that the problem is in P in the maximum matching model. We also focus on a fourth measure: the number of adjacencies, for which we give several approximation algorithms in the maximum matching model, in the case where genomes contain the same number of duplications of each gene.",
        "published": "2008-06-06T08:15:19Z",
        "link": "http://arxiv.org/abs/0806.1103v1",
        "categories": [
            "q-bio.QM",
            "cs.CC"
        ]
    },
    {
        "title": "Topological Complexity of Context-Free omega-Languages: A Survey",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We survey recent results on the topological complexity of context-free omega-languages which form the second level of the Chomsky hierarchy of languages of infinite words. In particular, we consider the Borel hierarchy and the Wadge hierarchy of non-deterministic or deterministic context-free omega-languages. We study also decision problems, the links with the notions of ambiguity and of degrees of ambiguity, and the special case of omega-powers.",
        "published": "2008-06-09T10:03:44Z",
        "link": "http://arxiv.org/abs/0806.1413v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "Data-Complexity of the Two-Variable Fragment with Counting Quantifiers",
        "authors": [
            "Ian Pratt-Hartmann"
        ],
        "summary": "The data-complexity of both satisfiability and finite satisfiability for the two-variable fragment with counting is NP-complete; the data-complexity of both query-answering and finite query-answering for the two-variable guarded fragment with counting is co-NP-complete.",
        "published": "2008-06-10T11:08:07Z",
        "link": "http://arxiv.org/abs/0806.1636v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Frequency of Correctness versus Average-Case Polynomial Time and   Generalized Juntas",
        "authors": [
            "Gabor Erdelyi",
            "Lane A. Hemaspaandra",
            "Joerg Rothe",
            "Holger Spakowski"
        ],
        "summary": "We prove that every distributional problem solvable in polynomial time on the average with respect to the uniform distribution has a frequently self-knowingly correct polynomial-time algorithm. We also study some features of probability weight of correctness with respect to generalizations of Procaccia and Rosenschein's junta distributions [PR07b].",
        "published": "2008-06-16T12:03:37Z",
        "link": "http://arxiv.org/abs/0806.2555v1",
        "categories": [
            "cs.CC",
            "cs.GT",
            "cs.MA",
            "F.1.3; F.2.2; I.2.11"
        ]
    },
    {
        "title": "Statistical Physics of Hard Optimization Problems",
        "authors": [
            "Lenka Zdeborová"
        ],
        "summary": "Optimization is fundamental in many areas of science, from computer science and information theory to engineering and statistical physics, as well as to biology or social sciences. It typically involves a large number of variables and a cost function depending on these variables. Optimization problems in the NP-complete class are particularly difficult, it is believed that the number of operations required to minimize the cost function is in the most difficult cases exponential in the system size. However, even in an NP-complete problem the practically arising instances might, in fact, be easy to solve. The principal question we address in this thesis is: How to recognize if an NP-complete constraint satisfaction problem is typically hard and what are the main reasons for this? We adopt approaches from the statistical physics of disordered systems, in particular the cavity method developed originally to describe glassy systems. We describe new properties of the space of solutions in two of the most studied constraint satisfaction problems - random satisfiability and random graph coloring. We suggest a relation between the existence of the so-called frozen variables and the algorithmic hardness of a problem. Based on these insights, we introduce a new class of problems which we named \"locked\" constraint satisfaction, where the statistical description is easily solvable, but from the algorithmic point of view they are even more challenging than the canonical satisfiability.",
        "published": "2008-06-25T14:40:11Z",
        "link": "http://arxiv.org/abs/0806.4112v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "A Fixed-Parameter Algorithm for Random Instances of Weighted d-CNF   Satisfiability",
        "authors": [
            "Yong Gao"
        ],
        "summary": "We study random instances of the weighted $d$-CNF satisfiability problem (WEIGHTED $d$-SAT), a generic W[1]-complete problem. A random instance of the problem consists of a fixed parameter $k$ and a random $d$-CNF formula $\\weicnf{n}{p}{k, d}$ generated as follows: for each subset of $d$ variables and with probability $p$, a clause over the $d$ variables is selected uniformly at random from among the $2^d - 1$ clauses that contain at least one negated literals.   We show that random instances of WEIGHTED $d$-SAT can be solved in $O(k^2n + n^{O(1)})$-time with high probability, indicating that typical instances of WEIGHTED $d$-SAT under this instance distribution are fixed-parameter tractable. The result also hold for random instances from the model $\\weicnf{n}{p}{k,d}(d')$ where clauses containing less than $d' (1 < d' < d)$ negated literals are forbidden, and for random instances of the renormalized (miniaturized) version of WEIGHTED $d$-SAT in certain range of the random model's parameter $p(n)$. This, together with our previous results on the threshold behavior and the resolution complexity of unsatisfiable instances of $\\weicnf{n}{p}{k, d}$, provides an almost complete characterization of the typical-case behavior of random instances of WEIGHTED $d$-SAT.",
        "published": "2008-06-28T05:03:47Z",
        "link": "http://arxiv.org/abs/0806.4652v1",
        "categories": [
            "cs.DS",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Research report: State complexity of operations on two-way quantum   finite automata",
        "authors": [
            "Daowen Qiu"
        ],
        "summary": "This paper deals with the size complexity of minimal {\\it two-way quantum finite automata} (2qfa's) necessary for operations to perform on all inputs of each fixed length. Such a complexity measure, known as state complexity of operations, is useful in measuring how much information is necessary to convert languages. We focus on intersection, union, reversals, and catenation operations and show some upper bounds of state complexity of operations on 2qfa's. Also, we present a number of non-regular languages and prove that these languages can be accepted by 2qfa's with one-sided error probabilities within linear time. Notably, these examples show that our bounds obtained for these operations are not tight, and therefore worth improving. We give an instance to show that the upper bound of the state number for the simulation of one-way deterministic finite automata by two-way reversible finite automata is not tight in general.",
        "published": "2008-07-03T02:26:17Z",
        "link": "http://arxiv.org/abs/0807.0476v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "F.1.1"
        ]
    },
    {
        "title": "Algorithmic Problem Complexity",
        "authors": [
            "Mark Burgin"
        ],
        "summary": "People solve different problems and know that some of them are simple, some are complex and some insoluble. The main goal of this work is to develop a mathematical theory of algorithmic complexity for problems. This theory is aimed at determination of computer abilities in solving different problems and estimation of resources that computers need to do this. Here we build the part of this theory related to static measures of algorithms. At first, we consider problems for finite words and study algorithmic complexity of such problems, building optimal complexity measures. Then we consider problems for such infinite objects as functions and study algorithmic complexity of these problems, also building optimal complexity measures. In the second part of the work, complexity of algorithmic problems, such as the halting problem for Turing machines, is measured by the classes of automata that are necessary to solve this problem. To classify different problems with respect to their complexity, inductive Turing machines, which extend possibilities of Turing machines, are used. A hierarchy of inductive Turing machines generates an inductive hierarchy of algorithmic problems. Here we specifically consider algorithmic problems related to Turing machines and inductive Turing machines, and find a place for these problems in the inductive hierarchy of algorithmic problems.",
        "published": "2008-07-04T02:45:46Z",
        "link": "http://arxiv.org/abs/0807.0672v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Quantum Query Complexity of Multilinear Identity Testing",
        "authors": [
            "V. Arvind",
            "Partha Mukhopadhyay"
        ],
        "summary": "Motivated by the quantum algorithm in \\cite{MN05} for testing commutativity of black-box groups, we study the following problem: Given a black-box finite ring $R=\\angle{r_1,...,r_k}$ where $\\{r_1,r_2,...,r_k\\}$ is an additive generating set for $R$ and a multilinear polynomial $f(x_1,...,x_m)$ over $R$ also accessed as a black-box function $f:R^m\\to R$ (where we allow the indeterminates $x_1,...,x_m$ to be commuting or noncommuting), we study the problem of testing if $f$ is an \\emph{identity} for the ring $R$. More precisely, the problem is to test if $f(a_1,a_2,...,a_m)=0$ for all $a_i\\in R$.   We give a quantum algorithm with query complexity $O(m(1+\\alpha)^{m/2} k^{\\frac{m}{m+1}})$ assuming $k\\geq (1+1/\\alpha)^{m+1}$. Towards a lower bound, we also discuss a reduction from a version of $m$-collision to this problem.   We also observe a randomized test with query complexity $4^mmk$ and constant success probability and a deterministic test with $k^m$ query complexity.",
        "published": "2008-07-09T10:39:19Z",
        "link": "http://arxiv.org/abs/0807.1412v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "The cost of probabilistic gathering in oblivious robot networks",
        "authors": [
            "Julien Clement",
            "Xavier Defago",
            "Maria Gradinariu Potop-Butucaru",
            "Stephane Messika"
        ],
        "summary": "In this paper we address the complexity issues of two agreement problems in oblivious robot networks namely gathering and scattering. These abstractions are fundamental coordination problems in cooperative mobile robotics. Moreover, their oblivious characteristics makes them appealing for self-stabilization since they are self-stabilizing with no extra-cost. Given a set of robots with arbitrary initial location and no initial agreement on a global coordinate system, gathering requires that all robots reach the exact same but not predetermined location while scattering aims at scatter robots such that no two robots share the same location. Both deterministic gathering and scattering have been proved impossible under arbitrary schedulers therefore probabilistic solutions have been recently proposed. The contribution of this paper is twofold. First, we propose a detailed complexity analysis of the existent probabilistic gathering algorithms in both fault-free and fault-prone environments. We consider both crash and byzantine-prone environments. Moreover, using Markov chains tools and additional assumptions on the environment we prove that the gathering convergence time can be reduced from O(n^2) (the best known tight bound) to O(nln(n)). Additionally, we prove that in crash-prone environments gathering is achieved in O(nln(n)+2f). Second, using the same technique we prove that the best known scattering strategy converges in fault-free systems is O(n) (which is one to optimal) while in crash-prone environments it needs O(n-f). Finally, we conclude the paper with a discussion related to different strategies to gather oblivious robots.",
        "published": "2008-07-10T22:43:42Z",
        "link": "http://arxiv.org/abs/0807.1753v1",
        "categories": [
            "cs.DC",
            "cs.CC"
        ]
    },
    {
        "title": "Derandomizing the Lovasz Local Lemma more effectively",
        "authors": [
            "Robin A. Moser"
        ],
        "summary": "The famous Lovasz Local Lemma [EL75] is a powerful tool to non-constructively prove the existence of combinatorial objects meeting a prescribed collection of criteria. Kratochvil et al. applied this technique to prove that a k-CNF in which each variable appears at most 2^k/(ek) times is always satisfiable [KST93]. In a breakthrough paper, Beck found that if we lower the occurrences to O(2^(k/48)/k), then a deterministic polynomial-time algorithm can find a satisfying assignment to such an instance [Bec91]. Alon randomized the algorithm and required O(2^(k/8)/k) occurrences [Alo91]. In [Mos06], we exhibited a refinement of his method which copes with O(2^(k/6)/k) of them. The hitherto best known randomized algorithm is due to Srinivasan and is capable of solving O(2^(k/4)/k) occurrence instances [Sri08]. Answering two questions asked by Srinivasan, we shall now present an approach that tolerates O(2^(k/2)/k) occurrences per variable and which can most easily be derandomized. The new algorithm bases on an alternative type of witness tree structure and drops a number of limiting aspects common to all previous methods.",
        "published": "2008-07-14T09:55:09Z",
        "link": "http://arxiv.org/abs/0807.2120v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2; G.2"
        ]
    },
    {
        "title": "Inapproximability for metric embeddings into R^d",
        "authors": [
            "Jiri Matousek",
            "Anastasios Sidiropoulos"
        ],
        "summary": "We consider the problem of computing the smallest possible distortion for embedding of a given n-point metric space into R^d, where d is fixed (and small). For d=1, it was known that approximating the minimum distortion with a factor better than roughly n^(1/12) is NP-hard. From this result we derive inapproximability with factor roughly n^(1/(22d-10)) for every fixed d\\ge 2, by a conceptually very simple reduction. However, the proof of correctness involves a nontrivial result in geometric topology (whose current proof is based on ideas due to Jussi Vaisala).   For d\\ge 3, we obtain a stronger inapproximability result by a different reduction: assuming P \\ne NP, no polynomial-time algorithm can distinguish between spaces embeddable in R^d with constant distortion from spaces requiring distortion at least n^(c/d), for a constant c>0. The exponent c/d has the correct order of magnitude, since every n-point metric space can be embedded in R^d with distortion O(n^{2/d}\\log^{3/2}n) and such an embedding can be constructed in polynomial time by random projection.   For d=2, we give an example of a metric space that requires a large distortion for embedding in R^2, while all not too large subspaces of it embed almost isometrically.",
        "published": "2008-07-15T19:21:09Z",
        "link": "http://arxiv.org/abs/0807.2472v1",
        "categories": [
            "cs.CG",
            "cs.CC"
        ]
    },
    {
        "title": "An $O(\\log n)$-approximation for the Set Cover Problem with Set   Ownership",
        "authors": [
            "Mira Gonen",
            "Yuval Shavitt"
        ],
        "summary": "In highly distributed Internet measurement systems distributed agents periodically measure the Internet using a tool called {\\tt traceroute}, which discovers a path in the network graph. Each agent performs many traceroute measurement to a set of destinations in the network, and thus reveals a portion of the Internet graph as it is seen from the agent locations. In every period we need to check whether previously discovered edges still exist in this period, a process termed {\\em validation}. For this end we maintain a database of all the different measurements performed by each agent. Our aim is to be able to {\\em validate} the existence of all previously discovered edges in the minimum possible time. In this work we formulate the validation problem as a generalization of the well know set cover problem. We reduce the set cover problem to the validation problem, thus proving that the validation problem is ${\\cal NP}$-hard. We present a $O(\\log n)$-approximation algorithm to the validation problem, where $n$ in the number of edges that need to be validated. We also show that unless ${\\cal P = NP}$ the approximation ratio of the validation problem is $\\Omega(\\log n)$.",
        "published": "2008-07-21T19:30:12Z",
        "link": "http://arxiv.org/abs/0807.3326v1",
        "categories": [
            "cs.NI",
            "cs.CC"
        ]
    },
    {
        "title": "Formal semantics of language and the Richard-Berry paradox",
        "authors": [
            "Stefano Crespi Reghizzi"
        ],
        "summary": "The classical logical antinomy known as Richard-Berry paradox is combined with plausible assumptions about the size i.e. the descriptional complexity of Turing machines formalizing certain sentences, to show that formalization of language leads to contradiction.",
        "published": "2008-07-24T10:31:55Z",
        "link": "http://arxiv.org/abs/0807.3845v1",
        "categories": [
            "cs.CL",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "On the random satisfiable process",
        "authors": [
            "Michael Krivelevich",
            "Benny Sudakov",
            "Dan Vilenchik"
        ],
        "summary": "In this work we suggest a new model for generating random satisfiable k-CNF formulas. To generate such formulas -- randomly permute all 2^k\\binom{n}{k} possible clauses over the variables x_1, ..., x_n, and starting from the empty formula, go over the clauses one by one, including each new clause as you go along if after its addition the formula remains satisfiable. We study the evolution of this process, namely the distribution over formulas obtained after scanning through the first m clauses (in the random permutation's order).   Random processes with conditioning on a certain property being respected are widely studied in the context of graph properties. This study was pioneered by Ruci\\'nski and Wormald in 1992 for graphs with a fixed degree sequence, and also by Erd\\H{o}s, Suen, and Winkler in 1995 for triangle-free and bipartite graphs. Since then many other graph properties were studied such as planarity and H-freeness. Thus our model is a natural extension of this approach to the satisfiability setting.   Our main contribution is as follows. For m \\geq cn, c=c(k) a sufficiently large constant, we are able to characterize the structure of the solution space of a typical formula in this distribution. Specifically, we show that typically all satisfying assignments are essentially clustered in one cluster, and all but e^{-\\Omega(m/n)} n of the variables take the same value in all satisfying assignments. We also describe a polynomial time algorithm that finds with high probability a satisfying assignment for such formulas.",
        "published": "2008-07-27T20:10:38Z",
        "link": "http://arxiv.org/abs/0807.4326v1",
        "categories": [
            "math.CO",
            "cs.CC",
            "math.PR"
        ]
    },
    {
        "title": "Approximate kernel clustering",
        "authors": [
            "Subhash Khot",
            "Assaf Naor"
        ],
        "summary": "In the kernel clustering problem we are given a large $n\\times n$ positive semi-definite matrix $A=(a_{ij})$ with $\\sum_{i,j=1}^na_{ij}=0$ and a small $k\\times k$ positive semi-definite matrix $B=(b_{ij})$. The goal is to find a partition $S_1,...,S_k$ of $\\{1,... n\\}$ which maximizes the quantity $$ \\sum_{i,j=1}^k (\\sum_{(i,j)\\in S_i\\times S_j}a_{ij})b_{ij}. $$ We study the computational complexity of this generic clustering problem which originates in the theory of machine learning. We design a constant factor polynomial time approximation algorithm for this problem, answering a question posed by Song, Smola, Gretton and Borgwardt. In some cases we manage to compute the sharp approximation threshold for this problem assuming the Unique Games Conjecture (UGC). In particular, when $B$ is the $3\\times 3$ identity matrix the UGC hardness threshold of this problem is exactly $\\frac{16\\pi}{27}$. We present and study a geometric conjecture of independent interest which we show would imply that the UGC threshold when $B$ is the $k\\times k$ identity matrix is $\\frac{8\\pi}{9}(1-\\frac{1}{k})$ for every $k\\ge 3$.",
        "published": "2008-07-29T10:40:55Z",
        "link": "http://arxiv.org/abs/0807.4626v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "math.FA"
        ]
    },
    {
        "title": "Branching proofs of infeasibility in low density subset sum problems",
        "authors": [
            "Gabor Pataki",
            "Mustafa Tural"
        ],
        "summary": "We prove that the subset sum problem has a polynomial time computable certificate of infeasibility for all $a$ weight vectors with density at most $1/(2n)$ and for almost all integer right hand sides. The certificate is branching on a hyperplane, i.e. by a methodology dual to the one explored by Lagarias and Odlyzko; Frieze; Furst and Kannan; and Coster et. al.   The proof has two ingredients. We first prove that a vector that is near parallel to $a$ is a suitable branching direction, regardless of the density. Then we show that for a low density $a$ such a near parallel vector can be computed using diophantine approximation, via a methodology introduced by Frank and Tardos.   We also show that there is a small number of long intervals whose disjoint union covers the integer right hand sides, for which the infeasibility is proven by branching on the above hyperplane.",
        "published": "2008-07-31T20:59:00Z",
        "link": "http://arxiv.org/abs/0808.0023v1",
        "categories": [
            "cs.CC",
            "cs.CR",
            "math.CO",
            "math.OC",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Quantum walk based search algorithms",
        "authors": [
            "Miklos Santha"
        ],
        "summary": "In this survey paper we give an intuitive treatment of the discrete time quantization of classical Markov chains. Grover search and the quantum walk based search algorithms of Ambainis, Szegedy and Magniez et al. will be stated as quantum analogues of classical search procedures. We present a rather detailed description of a somewhat simplified version of the MNRS algorithm. Finally, in the query complexity model, we show how quantum walks can be applied to the following search problems: Element Distinctness, Matrix Product Verification, Restricted Range Associativity, Triangle, and Group Commutativity.",
        "published": "2008-08-01T06:01:37Z",
        "link": "http://arxiv.org/abs/0808.0059v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Logics for the Relational Syllogistic",
        "authors": [
            "Ian Pratt-Hartmann",
            "Lawrence S. Moss"
        ],
        "summary": "The Aristotelian syllogistic cannot account for the validity of many inferences involving relational facts. In this paper, we investigate the prospects for providing a relational syllogistic. We identify several fragments based on (a) whether negation is permitted on all nouns, including those in the subject of a sentence; and (b) whether the subject noun phrase may contain a relative clause. The logics we present are extensions of the classical syllogistic, and we pay special attention to the question of whether reductio ad absurdum is needed. Thus our main goal is to derive results on the existence (or non-existence) of syllogistic proof systems for relational fragments. We also determine the computational complexity of all our fragments.",
        "published": "2008-08-04T22:26:38Z",
        "link": "http://arxiv.org/abs/0808.0521v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.CL",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Model Checking Positive Equality-free FO: Boolean Structures and   Digraphs of Size Three",
        "authors": [
            "Barnaby Martin"
        ],
        "summary": "We study the model checking problem, for fixed structures A, over positive equality-free first-order logic -- a natural generalisation of the non-uniform quantified constraint satisfaction problem QCSP(A). We prove a complete complexity classification for this problem when A ranges over 1.) boolean structures and 2.) digraphs of size (less than or equal to) three. The former class displays dichotomy between Logspace and Pspace-complete, while the latter class displays tetrachotomy between Logspace, NP-complete, co-NP-complete and Pspace-complete.",
        "published": "2008-08-05T13:33:43Z",
        "link": "http://arxiv.org/abs/0808.0647v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "On Complexity of Minimum Leaf Out-branching Problem",
        "authors": [
            "Peter Dankelmann",
            "Gregory Gutin",
            "Eun Jung Kim"
        ],
        "summary": "Given a digraph $D$, the Minimum Leaf Out-Branching problem (MinLOB) is the problem of finding in $D$ an out-branching with the minimum possible number of leaves, i.e., vertices of out-degree 0. Gutin, Razgon and Kim (2008) proved that MinLOB is polynomial time solvable for acyclic digraphs which are exactly the digraphs of directed path-width (DAG-width, directed tree-width, respectively) 0. We investigate how much one can extend this polynomiality result. We prove that already for digraphs of directed path-width (directed tree-width, DAG-width, respectively) 1, MinLOB is NP-hard. On the other hand, we show that for digraphs of restricted directed tree-width (directed path-width, DAG-width, respectively) and a fixed integer $k$, the problem of checking whether there is an out-branching with at most $k$ leaves is polynomial time solvable.",
        "published": "2008-08-07T08:41:48Z",
        "link": "http://arxiv.org/abs/0808.0980v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Graham's Schedules and the Number Partition Problem",
        "authors": [
            "Seenu S. Reddi"
        ],
        "summary": "We show the equivalence of the Number Partition Problem and the two processor scheduling problem. We establish a priori bounds on the completion times for the scheduling problem which are tighter than Graham's but almost on par with a posteriori bounds of Coffman and Sethi. We conclude the paper with a characterization of the asymptotic behavior of the scheduling problem which relates to the spread of the processing times and the number of jobs.",
        "published": "2008-08-07T22:17:56Z",
        "link": "http://arxiv.org/abs/0808.1119v1",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "On Bounded Integer Programming",
        "authors": [
            "Thân Quang Khoát"
        ],
        "summary": "We present an efficient reduction from the Bounded integer programming (BIP) to the Subspace avoiding problem (SAP) in lattice theory. The reduction has some special properties with some interesting consequences. The first is the new upper time bound for BIP, $poly(\\varphi)\\cdot n^{n+o(n)}$ (where $n$ and $\\varphi$ are the dimension and the input size of the problem, respectively). This is the best bound up to now for BIP. The second consequence is the proof that #SAP, for some norms, is #P-hard under semi-reductions. It follows that the counting version of the Generalized closest vector problem is also #P-hard under semi-reductions. Furthermore, we also show that under some reasonable assumptions, BIP is solvable in probabilistic time $2^{O(n)}$.",
        "published": "2008-08-09T17:04:44Z",
        "link": "http://arxiv.org/abs/0808.1364v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "G.1.6; F.1.3; F.2; G.2.1"
        ]
    },
    {
        "title": "The Peculiar Phase Structure of Random Graph Bisection",
        "authors": [
            "Allon G. Percus",
            "Gabriel Istrate",
            "Bruno Goncalves",
            "Robert Z. Sumi",
            "Stefan Boettcher"
        ],
        "summary": "The mincut graph bisection problem involves partitioning the n vertices of a graph into disjoint subsets, each containing exactly n/2 vertices, while minimizing the number of \"cut\" edges with an endpoint in each subset. When considered over sparse random graphs, the phase structure of the graph bisection problem displays certain familiar properties, but also some surprises. It is known that when the mean degree is below the critical value of 2 log 2, the cutsize is zero with high probability. We study how the minimum cutsize increases with mean degree above this critical threshold, finding a new analytical upper bound that improves considerably upon previous bounds. Combined with recent results on expander graphs, our bound suggests the unusual scenario that random graph bisection is replica symmetric up to and beyond the critical threshold, with a replica symmetry breaking transition possibly taking place above the threshold. An intriguing algorithmic consequence is that although the problem is NP-hard, we can find near-optimal cutsizes (whose ratio to the optimal value approaches 1 asymptotically) in polynomial time for typical instances near the phase transition.",
        "published": "2008-08-11T17:59:02Z",
        "link": "http://arxiv.org/abs/0808.1549v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Finding cores of random 2-SAT formulae via Poisson cloning",
        "authors": [
            "Jeong Han Kim"
        ],
        "summary": "For the random 2-SAT formula $F(n,p)$, let $F_C (n,p)$ be the formula left after the pure literal algorithm applied to $F(n,p)$ stops. Using the recently developed Poisson cloning model together with the cut-off line algorithm (COLA), we completely analyze the structure of $F_{C} (n,p)$. In particular, it is shown that, for $\\gl:= p(2n-1) = 1+\\gs $ with $\\gs\\gg n^{-1/3}$, the core of $F(n,p)$ has $\\thl^2 n +O((\\thl n)^{1/2})$ variables and $\\thl^2 \\gl n+O((\\thl n))^{1/2}$ clauses, with high probability, where $\\thl$ is the larger solution of the equation $\\th- (1-e^{-\\thl \\gl})=0$. We also estimate the probability of $F(n,p)$ being satisfiable to obtain $$ \\pr[ F_2(n, \\sfrac{\\gl}{2n-1}) is satisfiable ] = \\caseth{1-\\frac{1+o(1)}{16\\gs^3 n}}{if $\\gl= 1-\\gs$ with $\\gs\\gg n^{-1/3}$}{}{}{e^{-\\Theta(\\gs^3n)}}{if $\\gl=1+\\gs$ with $\\gs\\gg n^{-1/3}$,} $$ where $o(1)$ goes to 0 as $\\gs$ goes to 0. This improves the bounds of Bollob\\'as et al. \\cite{BBCKW}.",
        "published": "2008-08-12T00:38:59Z",
        "link": "http://arxiv.org/abs/0808.1599v1",
        "categories": [
            "math.CO",
            "cs.CC",
            "math.PR",
            "05C80"
        ]
    },
    {
        "title": "Characterization Of any Non-linear Boolean function Using A Set of   Linear Operators",
        "authors": [
            "Sudhakar Sahoo",
            "Pabitra Pal Choudhury",
            "Mithun Chakraborty"
        ],
        "summary": "Global dynamics of a non-linear Cellular Automata is, in general irregular, asymmetric and unpredictable as opposed to that of a linear CA, which is highly systematic and tractable. In the past efforts have been made to systematize non-linear CA evolutions in the light of Boolean derivatives and Jacobian Matrices. In this paper two different efforts have been made: first we try to systematize non-linear CA evolution in the light of deviant states and non-deviant states. For all the non-deviant states the nearest linear rule matrix is applicable where as for the deviant states we have a set of other matrices. Second using algebraic manipulation, an efficient algorithm is proposed by which every Non-linear Boolean function can be characterized by a sequence of binary matrices.",
        "published": "2008-08-12T11:04:47Z",
        "link": "http://arxiv.org/abs/0808.1641v1",
        "categories": [
            "cs.CC",
            "nlin.CG"
        ]
    },
    {
        "title": "Communication Complexities of XOR functions",
        "authors": [
            "Yaoyun Shi",
            "Zhiqiang Zhang"
        ],
        "summary": "We call $F:\\{0, 1\\}^n\\times \\{0, 1\\}^n\\to\\{0, 1\\}$ a symmetric XOR function if for a function $S:\\{0, 1, ..., n\\}\\to\\{0, 1\\}$, $F(x, y)=S(|x\\oplus y|)$, for any $x, y\\in\\{0, 1\\}^n$, where $|x\\oplus y|$ is the Hamming weight of the bit-wise XOR of $x$ and $y$.   We show that for any such function, (a) the deterministic communication complexity is always $\\Theta(n)$ except for four simple functions that have a constant complexity, and (b) up to a polylog factor, the error-bounded randomized and quantum communication complexities are $\\Theta(r_0+r_1)$, where $r_0$ and $r_1$ are the minimum integers such that $r_0, r_1\\leq n/2$ and $S(k)=S(k+2)$ for all $k\\in[r_0, n-r_1)$.",
        "published": "2008-08-13T00:48:36Z",
        "link": "http://arxiv.org/abs/0808.1762v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Transitive-Closure Spanners",
        "authors": [
            "Arnab Bhattacharyya",
            "Elena Grigorescu",
            "Kyomin Jung",
            "Sofya Raskhodnikova",
            "David P. Woodruff"
        ],
        "summary": "Given a directed graph G = (V,E) and an integer k>=1, a k-transitive-closure-spanner (k-TC-spanner) of G is a directed graph H = (V, E_H) that has (1) the same transitive-closure as G and (2) diameter at most k. These spanners were implicitly studied in access control, data structures, and property testing, and properties of these spanners have been rediscovered over the span of 20 years. The main goal in each of these applications is to obtain the sparsest k-TC-spanners. We bring these diverse areas under the unifying framework of TC-spanners.   We initiate the study of approximability of the size of the sparsest k-TC-spanner for a given directed graph. We completely resolve the approximability of 2-TC-spanners, showing that it is Theta(log n) unless P = NP. For k>2, we present a polynomial-time algorithm that finds a k-TC-spanner with size within O((n log n)^{1-1/k}) of the optimum. Our algorithmic techniques also yield algorithms with the best-known approximation ratio for well-studied problems on directed spanners when k>3: DIRECTED k-SPANNER, CLIENT/SERVER DIRECTED k-SPANNER, and k-DIAMETER SPANNING SUBGRAPH. For constant k>=3, we show that the size of the sparsest k-TC-spanner is hard to approximate with 2^{log^{1-eps} n} ratio unless NP \\subseteq DTIME(n^{polylog n}}). Finally, we study the size of the sparsest k-TC-spanners for H-minor-free graph families. Combining our constructions with our insight that 2-TC-spanners can be used for designing property testers, we obtain a monotonicity tester with O(log^2 n /eps) queries for any poset whose transitive reduction is an H-minor free digraph, improving the Theta(sqrt(n) log n/eps)-queries required of the tester due to Fischer et al (2002).",
        "published": "2008-08-13T06:44:10Z",
        "link": "http://arxiv.org/abs/0808.1787v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2; G.2.2; G.2.3"
        ]
    },
    {
        "title": "Decision Problems For Convex Languages",
        "authors": [
            "Janusz Brzozowski",
            "Jeffrey Shallit",
            "Zhi Xu"
        ],
        "summary": "In this paper we examine decision problems associated with various classes of convex languages, studied by Ang and Brzozowski (under the name \"continuous languages\"). We show that we can decide whether a given language L is prefix-, suffix-, factor-, or subword-convex in polynomial time if L is represented by a DFA, but that the problem is PSPACE-hard if L is represented by an NFA. In the case that a regular language is not convex, we prove tight upper bounds on the length of the shortest words demonstrating this fact, in terms of the number of states of an accepting DFA. Similar results are proved for some subclasses of convex languages: the prefix-, suffix-, factor-, and subword-closed languages, and the prefix-, suffix-, factor-, and subword-free languages.",
        "published": "2008-08-14T01:50:07Z",
        "link": "http://arxiv.org/abs/0808.1928v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.FL"
        ]
    },
    {
        "title": "Every Computably Enumerable Random Real Is Provably Computably   Enumerable Random",
        "authors": [
            "Cristian S. Calude",
            "Nicholas J. Hay"
        ],
        "summary": "We prove that every computably enumerable (c.e.) random real is provable in Peano Arithmetic (PA) to be c.e. random. A major step in the proof is to show that the theorem stating that \"a real is c.e. and random iff it is the halting probability of a universal prefix-free Turing machine\" can be proven in PA. Our proof, which is simpler than the standard one, can also be used for the original theorem.   Our positive result can be contrasted with the case of computable functions, where not every computable function is provably computable in PA, or even more interestingly, with the fact that almost all random finite strings are not provably random in PA.   We also prove two negative results: a) there exists a universal machine whose universality cannot be proved in PA, b) there exists a universal machine $U$ such that, based on $U$, PA cannot prove the randomness of its halting probability.   The paper also includes a sharper form of the Kraft-Chaitin Theorem, as well as a formal proof of this theorem written with the proof assistant Isabelle.",
        "published": "2008-08-15T23:46:48Z",
        "link": "http://arxiv.org/abs/0808.2220v5",
        "categories": [
            "cs.CC",
            "cs.LO",
            "math.LO",
            "H.1.1"
        ]
    },
    {
        "title": "On NFAs Where All States are Final, Initial, or Both",
        "authors": [
            "Jui-Yi Kao",
            "Narad Rampersad",
            "Jeffrey Shallit"
        ],
        "summary": "We examine questions involving nondeterministic finite automata where all states are final, initial, or both initial and final. First, we prove hardness results for the nonuniversality and inequivalence problems for these NFAs. Next, we characterize the languages accepted. Finally, we discuss some state complexity problems involving such automata.",
        "published": "2008-08-18T16:11:16Z",
        "link": "http://arxiv.org/abs/0808.2417v2",
        "categories": [
            "cs.CC",
            "cs.FL"
        ]
    },
    {
        "title": "Closed Timelike Curves Make Quantum and Classical Computing Equivalent",
        "authors": [
            "Scott Aaronson",
            "John Watrous"
        ],
        "summary": "While closed timelike curves (CTCs) are not known to exist, studying their consequences has led to nontrivial insights in general relativity, quantum information, and other areas. In this paper we show that if CTCs existed, then quantum computers would be no more powerful than classical computers: both would have the (extremely large) power of the complexity class PSPACE, consisting of all problems solvable by a conventional computer using a polynomial amount of memory. This solves an open problem proposed by one of us in 2005, and gives an essentially complete understanding of computational complexity in the presence of CTCs. Following the work of Deutsch, we treat a CTC as simply a region of spacetime where a \"causal consistency\" condition is imposed, meaning that Nature has to produce a (probabilistic or quantum) fixed-point of some evolution operator. Our conclusion is then a consequence of the following theorem: given any quantum circuit (not necessarily unitary), a fixed-point of the circuit can be (implicitly) computed in polynomial space. This theorem might have independent applications in quantum information.",
        "published": "2008-08-19T23:04:28Z",
        "link": "http://arxiv.org/abs/0808.2669v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Multitask Efficiencies in the Decision Tree Model",
        "authors": [
            "Andrew Drucker"
        ],
        "summary": "In Direct Sum problems [KRW], one tries to show that for a given computational model, the complexity of computing a collection of finite functions on independent inputs is approximately the sum of their individual complexities. In this paper, by contrast, we study the diversity of ways in which the joint computational complexity can behave when all the functions are evaluated on a common input. We focus on the deterministic decision tree model, with depth as the complexity measure; in this model we prove a result to the effect that the 'obvious' constraints on joint computational complexity are essentially the only ones.   The proof uses an intriguing new type of cryptographic data structure called a `mystery bin' which we construct using a small polynomial separation between deterministic and unambiguous query complexity shown by Savicky. We also pose a variant of the Direct Sum Conjecture of [KRW] which, if proved for a single family of functions, could yield an analogous result for models such as the communication model.",
        "published": "2008-08-20T11:15:33Z",
        "link": "http://arxiv.org/abs/0808.2662v3",
        "categories": [
            "cs.CC",
            "F.1.1; F.1.3"
        ]
    },
    {
        "title": "Analysis of the postulates produced by Karp's Theorem",
        "authors": [
            "Jerrald Meek"
        ],
        "summary": "This is the final article in a series of four articles. Richard Karp has proven that a deterministic polynomial time solution to K-SAT will result in a deterministic polynomial time solution to all NP-Complete problems. However, it is demonstrated that a deterministic polynomial time solution to any NP-Complete problem does not necessarily produce a deterministic polynomial time solution to all NP-Complete problems.",
        "published": "2008-08-24T02:59:29Z",
        "link": "http://arxiv.org/abs/0808.3222v5",
        "categories": [
            "cs.CC",
            "F.2.0"
        ]
    },
    {
        "title": "Efficient algorithms for the basis of finite Abelian groups",
        "authors": [
            "Gregory Karagiorgos",
            "Dimitrios Poulakis"
        ],
        "summary": "Let $G$ be a finite abelian group $G$ with $N$ elements. In this paper we give a O(N) time algorithm for computing a basis of $G$. Furthermore, we obtain an algorithm for computing a basis from a generating system of $G$ with $M$ elements having time complexity $O(M\\sum_{p|N} e(p)\\lceil p^{1/2}\\rceil^{\\mu(p)})$, where $p$ runs over all the prime divisors of $N$, and $p^{e(p)}$, $\\mu(p)$ are the exponent and the number of cyclic groups which are direct factors of the $p$-primary component of $G$, respectively. In case where $G$ is a cyclic group having a generating system with $M$ elements, a $O(MN^{\\epsilon})$ time algorithm for the computation of a basis of $G$ is obtained.",
        "published": "2008-08-25T11:01:15Z",
        "link": "http://arxiv.org/abs/0808.3331v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Linear Programming Formulation of the Boolean Satisfiability Problem",
        "authors": [
            "Moustapha Diaby"
        ],
        "summary": "In this paper, we present a new, graph-based modeling approach and a polynomial-sized linear programming (LP) formulation of the Boolean satisfiability problem (SAT). The approach is illustrated with a numerical example.",
        "published": "2008-08-25T17:20:16Z",
        "link": "http://arxiv.org/abs/0808.3386v8",
        "categories": [
            "cs.DM",
            "cs.CC",
            "F.2.2; G.1.6; G.2"
        ]
    },
    {
        "title": "Approaching Blokh-Zyablov Error Exponent with Linear-Time   Encodable/Decodable Codes",
        "authors": [
            "Zheng Wang",
            "Jie Luo"
        ],
        "summary": "Guruswami and Indyk showed in [1] that Forney's error exponent can be achieved with linear coding complexity over binary symmetric channels. This paper extends this conclusion to general discrete-time memoryless channels and shows that Forney's and Blokh-Zyablov error exponents can be arbitrarily approached by one-level and multi-level concatenated codes with linear encoding/decoding complexity. The key result is a revision to Forney's general minimum distance decoding algorithm, which enables a low complexity integration of Guruswami-Indyk's outer codes into the concatenated coding schemes.",
        "published": "2008-08-27T19:32:45Z",
        "link": "http://arxiv.org/abs/0808.3756v2",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "The Complexity of Reasoning for Fragments of Default Logic",
        "authors": [
            "Olaf Beyersdorff",
            "Arne Meier",
            "Michael Thomas",
            "Heribert Vollmer"
        ],
        "summary": "Default logic was introduced by Reiter in 1980. In 1992, Gottlob classified the complexity of the extension existence problem for propositional default logic as $\\SigmaPtwo$-complete, and the complexity of the credulous and skeptical reasoning problem as SigmaP2-complete, resp. PiP2-complete. Additionally, he investigated restrictions on the default rules, i.e., semi-normal default rules. Selman made in 1992 a similar approach with disjunction-free and unary default rules. In this paper we systematically restrict the set of allowed propositional connectives. We give a complete complexity classification for all sets of Boolean functions in the meaning of Post's lattice for all three common decision problems for propositional default logic. We show that the complexity is a hexachotomy (SigmaP2-, DeltaP2-, NP-, P-, NL-complete, trivial) for the extension existence problem, while for the credulous and skeptical reasoning problem we obtain similar classifications without trivial cases.",
        "published": "2008-08-28T11:14:41Z",
        "link": "http://arxiv.org/abs/0808.3884v4",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.2.2; F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Swapping Lemmas for Regular and Context-Free Languages",
        "authors": [
            "Tomoyuki Yamakami"
        ],
        "summary": "In formal language theory, one of the most fundamental tools, known as pumping lemmas, is extremely useful for regular and context-free languages. However, there are natural properties for which the pumping lemmas are of little use. One of such examples concerns a notion of advice, which depends only on the size of an underlying input. A standard pumping lemma encounters difficulty in proving that a given language is not regular in the presence of advice. We develop its substitution, called a swapping lemma for regular languages, to demonstrate the non-regularity of a target language with advice. For context-free languages, we also present a similar form of swapping lemma, which serves as a technical tool to show that certain languages are not context-free with advice.",
        "published": "2008-08-29T16:09:08Z",
        "link": "http://arxiv.org/abs/0808.4122v2",
        "categories": [
            "cs.CC",
            "cs.CL",
            "cs.FL"
        ]
    },
    {
        "title": "Languages recognized with unbounded error by quantum finite automata",
        "authors": [
            "Abuzer Yakaryilmaz",
            "A. C. Cem Say"
        ],
        "summary": "This paper has been superseded by arXiv:1007.3624",
        "published": "2008-08-30T20:16:57Z",
        "link": "http://arxiv.org/abs/0809.0073v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Approximating Transitivity in Directed Networks",
        "authors": [
            "Piotr Berman",
            "Bhaskar DasGupta",
            "Marek Karpinski"
        ],
        "summary": "We study the problem of computing a minimum equivalent digraph (also known as the problem of computing a strong transitive reduction) and its maximum objective function variant, with two types of extensions. First, we allow to declare a set $D\\subset E$ and require that a valid solution $A$ satisfies $D\\subset A$ (it is sometimes called transitive reduction problem). In the second extension (called $p$-ary transitive reduction), we have integer edge labeling and we view two paths as equivalent if they have the same beginning, ending and the sum of labels modulo $p$. A solution $A\\subseteq E$ is valid if it gives an equivalent path for every original path. For all problems we establish the following: polynomial time minimization of $|A|$ within ratio 1.5, maximization of $|E-A|$ within ratio 2, MAX-SNP hardness even of the length of simple cycles is limited to 5. Furthermore, we believe that the combinatorial technique behind the approximation algorithm for the minimization version might be of interest to other graph connectivity problems as well.",
        "published": "2008-09-01T08:58:32Z",
        "link": "http://arxiv.org/abs/0809.0188v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Linear Kernelizations for Restricted 3-Hitting Set Problems",
        "authors": [
            "Xuan Cai"
        ],
        "summary": "The 3-\\textsc{Hitting Set} problem is also called the \\textsc{Vertex Cover} problem on 3-uniform hypergraphs. In this paper, we address kernelizations of the \\textsc{Vertex Cover} problem on 3-uniform hypergraphs. We show that this problem admits a linear kernel in three classes of 3-uniform hypergraphs. We also obtain lower and upper bounds on the kernel size for them by the parametric duality.",
        "published": "2008-09-01T14:52:30Z",
        "link": "http://arxiv.org/abs/0809.0257v4",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Instruction sequences and non-uniform complexity theory",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "We develop theory concerning non-uniform complexity in a setting in which the notion of single-pass instruction sequence considered in program algebra is the central notion. We define counterparts of the complexity classes P/poly and NP/poly and formulate a counterpart of the complexity theoretic conjecture that NP is not included in P/poly. In addition, we define a notion of completeness for the counterpart of NP/poly using a non-uniform reducibility relation and formulate complexity hypotheses which concern restrictions on the instruction sequences used for computation. We think that the theory developed opens up an additional way of investigating issues concerning non-uniform complexity.",
        "published": "2008-09-02T05:45:48Z",
        "link": "http://arxiv.org/abs/0809.0352v3",
        "categories": [
            "cs.CC",
            "F.1.1; F.1.3"
        ]
    },
    {
        "title": "No-signaling, intractability and entanglement",
        "authors": [
            "R. Srikanth"
        ],
        "summary": "We consider the problem of deriving the no-signaling condition from the assumption that, as seen from a complexity theoretic perspective, the universe is not an exponential place. A fact that disallows such a derivation is the existence of {\\em polynomial superluminal} gates, hypothetical primitive operations that enable superluminal signaling but not the efficient solution of intractable problems. It therefore follows, if this assumption is a basic principle of physics, either that it must be supplemented with additional assumptions to prohibit such gates, or, improbably, that no-signaling is not a universal condition. Yet, a gate of this kind is possibly implicit, though not recognized as such, in a decade-old quantum optical experiment involving position-momentum entangled photons. Here we describe a feasible modified version experiment that appears to explicitly demonstrate the action of this gate. Some obvious counter-claims are shown to be invalid. We believe that the unexpected possibility of polynomial superluminal operations arises because some practically measured quantum optical quantities are not describable as standard quantum mechanical observables.",
        "published": "2008-09-03T11:10:36Z",
        "link": "http://arxiv.org/abs/0809.0600v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Peek Arc Consistency",
        "authors": [
            "Manuel Bodirsky",
            "Hubie Chen"
        ],
        "summary": "This paper studies peek arc consistency, a reasoning technique that extends the well-known arc consistency technique for constraint satisfaction. In contrast to other more costly extensions of arc consistency that have been studied in the literature, peek arc consistency requires only linear space and quadratic time and can be parallelized in a straightforward way such that it runs in linear time with a linear number of processors. We demonstrate that for various constraint languages, peek arc consistency gives a polynomial-time decision procedure for the constraint satisfaction problem. We also present an algebraic characterization of those constraint languages that can be solved by peek arc consistency, and study the robustness of the algorithm.",
        "published": "2008-09-04T11:15:50Z",
        "link": "http://arxiv.org/abs/0809.0788v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Topological Complexity of omega-Powers : Extended Abstract",
        "authors": [
            "Olivier Finkel",
            "Dominique Lecomte"
        ],
        "summary": "This is an extended abstract presenting new results on the topological complexity of omega-powers (which are included in a paper \"Classical and effective descriptive complexities of omega-powers\" available from arXiv:0708.4176) and reflecting also some open questions which were discussed during the Dagstuhl seminar on \"Topological and Game-Theoretic Aspects of Infinite Computations\" 29.06.08 - 04.07.08.",
        "published": "2008-09-10T15:13:34Z",
        "link": "http://arxiv.org/abs/0809.1812v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "The complexity of counting solutions to Generalised Satisfiability   Problems modulo k",
        "authors": [
            "John Faben"
        ],
        "summary": "Generalised Satisfiability Problems (or Boolean Constraint Satisfaction Problems), introduced by Schaefer in 1978, are a general class of problem which allow the systematic study of the complexity of satisfiability problems with different types of constraints. In 1979, Valiant introduced the complexity class parity P, the problem of counting the number of solutions to NP problems modulo two. Others have since considered the question of counting modulo other integers.   We give a dichotomy theorem for the complexity of counting the number of solutions to Generalised Satisfiability Problems modulo integers. This follows from an earlier result of Creignou and Hermann which gave a counting dichotomy for these types of problem, and the dichotomy itself is almost identical. Specifically, counting the number of solutions to a Generalised Satisfiability Problem can be done in polynomial time if all the relations are affine. Otherwise, except for one special case with k = 2, it is #_kP-complete.",
        "published": "2008-09-10T16:23:53Z",
        "link": "http://arxiv.org/abs/0809.1836v1",
        "categories": [
            "cs.CC",
            "F.2.2; F.4.1; G.2.1"
        ]
    },
    {
        "title": "How to Integrate a Polynomial over a Simplex",
        "authors": [
            "Velleda Baldoni",
            "Nicole Berline",
            "Jesus De Loera",
            "Matthias Köppe",
            "Michèle Vergne"
        ],
        "summary": "This paper settles the computational complexity of the problem of integrating a polynomial function f over a rational simplex. We prove that the problem is NP-hard for arbitrary polynomials via a generalization of a theorem of Motzkin and Straus. On the other hand, if the polynomial depends only on a fixed number of variables, while its degree and the dimension of the simplex are allowed to vary, we prove that integration can be done in polynomial time. As a consequence, for polynomials of fixed total degree, there is a polynomial time algorithm as well. We conclude the article with extensions to other polytopes, discussion of other available methods and experimental results.",
        "published": "2008-09-11T19:00:12Z",
        "link": "http://arxiv.org/abs/0809.2083v3",
        "categories": [
            "math.MG",
            "cs.CC",
            "cs.SC"
        ]
    },
    {
        "title": "An approximation algorithm for approximation rank",
        "authors": [
            "Troy Lee",
            "Adi Shraibman"
        ],
        "summary": "One of the strongest techniques available for showing lower bounds on quantum communication complexity is the logarithm of the approximation rank of the communication matrix--the minimum rank of a matrix which is entrywise close to the communication matrix. This technique has two main drawbacks: it is difficult to compute, and it is not known to lower bound quantum communication complexity with entanglement.   Linial and Shraibman recently introduced a norm, called gamma_2^{alpha}, to quantum communication complexity, showing that it can be used to lower bound communication with entanglement. Here the parameter alpha is a measure of approximation which is related to the allowable error probability of the protocol. This bound can be written as a semidefinite program and gives bounds at least as large as many techniques in the literature, although it is smaller than the corresponding alpha-approximation rank, rk_alpha. We show that in fact log gamma_2^{alpha}(A)$ and log rk_{alpha}(A)$ agree up to small factors. As corollaries we obtain a constant factor polynomial time approximation algorithm to the logarithm of approximate rank, and that the logarithm of approximation rank is a lower bound for quantum communication complexity with entanglement.",
        "published": "2008-09-11T20:06:56Z",
        "link": "http://arxiv.org/abs/0809.2093v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Datalog and Constraint Satisfaction with Infinite Templates",
        "authors": [
            "Manuel Bodirsky",
            "Victor Dalmau"
        ],
        "summary": "On finite structures, there is a well-known connection between the expressive power of Datalog, finite variable logics, the existential pebble game, and bounded hypertree duality. We study this connection for infinite structures. This has applications for constraint satisfaction with infinite templates. If the template Gamma is omega-categorical, we present various equivalent characterizations of those Gamma such that the constraint satisfaction problem (CSP) for Gamma can be solved by a Datalog program. We also show that CSP(Gamma) can be solved in polynomial time for arbitrary omega-categorical structures Gamma if the input is restricted to instances of bounded treewidth. Finally, we characterize those omega-categorical templates whose CSP has Datalog width 1, and those whose CSP has strict Datalog width k.",
        "published": "2008-09-14T10:01:27Z",
        "link": "http://arxiv.org/abs/0809.2386v3",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "A Log-space Algorithm for Canonization of Planar Graphs",
        "authors": [
            "Samir Datta",
            "Nutan Limaye",
            "Prajakta Nimbhorkar",
            "Thomas Thierauf",
            "Fabian Wagner"
        ],
        "summary": "Graph Isomorphism is the prime example of a computational problem with a wide difference between the best known lower and upper bounds on its complexity. We bridge this gap for a natural and important special case, planar graph isomorphism, by presenting an upper bound that matches the known logspace hardness [Lindell'92]. In fact, we show the formally stronger result that planar graph canonization is in logspace. This improves the previously known upper bound of AC1 [MillerReif'91].   Our algorithm first constructs the biconnected component tree of a connected planar graph and then refines each biconnected component into a triconnected component tree. The next step is to logspace reduce the biconnected planar graph isomorphism and canonization problems to those for 3-connected planar graphs, which are known to be in logspace by [DattaLimayeNimbhorkar'08]. This is achieved by using the above decomposition, and by making significant modifications to Lindell's algorithm for tree canonization, along with changes in the space complexity analysis.   The reduction from the connected case to the biconnected case requires further new ideas, including a non-trivial case analysis and a group theoretic lemma to bound the number of automorphisms of a colored 3-connected planar graph. This lemma is crucial for the reduction to work in logspace.",
        "published": "2008-09-15T06:22:39Z",
        "link": "http://arxiv.org/abs/0809.2319v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Depth as Randomness Deficiency",
        "authors": [
            "Luis Antunes",
            "Armando Matos",
            "Andre Souto",
            "Paul Vitanyi"
        ],
        "summary": "Depth of an object concerns a tradeoff between computation time and excess of program length over the shortest program length required to obtain the object. It gives an unconditional lower bound on the computation time from a given program in absence of auxiliary information. Variants known as logical depth and computational depth are expressed in Kolmogorov complexity theory.   We derive quantitative relation between logical depth and computational depth and unify the different depth notions by relating them to A. Kolmogorov and L. Levin's fruitful notion of randomness deficiency. Subsequently, we revisit the computational depth of infinite strings, introducing the notion of super deep sequences and relate it with other approaches.",
        "published": "2008-09-15T15:11:31Z",
        "link": "http://arxiv.org/abs/0809.2546v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On Time-Bounded Incompressibility of Compressible Strings and Sequences",
        "authors": [
            "E. G. Daylight",
            "W. M. Koolen",
            "P. M. B. Vitanyi"
        ],
        "summary": "For every total recursive time bound $t$, a constant fraction of all compressible (low Kolmogorov complexity) strings is $t$-bounded incompressible (high time-bounded Kolmogorov complexity); there are uncountably many infinite sequences of which every initial segment of length $n$ is compressible to $\\log n$ yet $t$-bounded incompressible below ${1/4}n - \\log n$; and there are countable infinitely many recursive infinite sequence of which every initial segment is similarly $t$-bounded incompressible. These results are related to, but different from, Barzdins's lemma.",
        "published": "2008-09-17T17:05:11Z",
        "link": "http://arxiv.org/abs/0809.2965v4",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Monadic Datalog over Finite Structures with Bounded Treewidth",
        "authors": [
            "Georg Gottlob",
            "Reinhard Pichler",
            "Fang Wei"
        ],
        "summary": "Bounded treewidth and Monadic Second Order (MSO) logic have proved to be key concepts in establishing fixed-parameter tractability results. Indeed, by Courcelle's Theorem we know: Any property of finite structures, which is expressible by an MSO sentence, can be decided in linear time (data complexity) if the structures have bounded treewidth.   In principle, Courcelle's Theorem can be applied directly to construct concrete algorithms by transforming the MSO evaluation problem into a tree language recognition problem. The latter can then be solved via a finite tree automaton (FTA). However, this approach has turned out to be problematical, since even relatively simple MSO formulae may lead to a ``state explosion'' of the FTA.   In this work we propose monadic datalog (i.e., datalog where all intentional predicate symbols are unary) as an alternative method to tackle this class of fixed-parameter tractable problems. We show that if some property of finite structures is expressible in MSO then this property can also be expressed by means of a monadic datalog program over the structure plus the tree decomposition.   Moreover, we show that the resulting fragment of datalog can be evaluated in linear time (both w.r.t. the program size and w.r.t. the data size). This new approach is put to work by devising new algorithms for the 3-Colorability problem of graphs and for the PRIMALITY problem of relational schemas (i.e., testing if some attribute in a relational schema is part of a key). We also report on experimental results with a prototype implementation.",
        "published": "2008-09-18T12:40:49Z",
        "link": "http://arxiv.org/abs/0809.3140v1",
        "categories": [
            "cs.DB",
            "cs.CC",
            "cs.LO",
            "F.2.2; F.4.1"
        ]
    },
    {
        "title": "Fermions and Loops on Graphs. I. Loop Calculus for Determinant",
        "authors": [
            "Vladimir Y. Chernyak",
            "Michael Chertkov"
        ],
        "summary": "This paper is the first in the series devoted to evaluation of the partition function in statistical models on graphs with loops in terms of the Berezin/fermion integrals. The paper focuses on a representation of the determinant of a square matrix in terms of a finite series, where each term corresponds to a loop on the graph. The representation is based on a fermion version of the Loop Calculus, previously introduced by the authors for graphical models with finite alphabets. Our construction contains two levels. First, we represent the determinant in terms of an integral over anti-commuting Grassman variables, with some reparametrization/gauge freedom hidden in the formulation. Second, we show that a special choice of the gauge, called BP (Bethe-Peierls or Belief Propagation) gauge, yields the desired loop representation. The set of gauge-fixing BP conditions is equivalent to the Gaussian BP equations, discussed in the past as efficient (linear scaling) heuristics for estimating the covariance of a sparse positive matrix.",
        "published": "2008-09-20T03:11:44Z",
        "link": "http://arxiv.org/abs/0809.3479v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC",
            "cs.IT",
            "hep-th",
            "math.IT"
        ]
    },
    {
        "title": "Fermions and Loops on Graphs. II. Monomer-Dimer Model as Series of   Determinants",
        "authors": [
            "Vladimir Y. Chernyak",
            "Michael Chertkov"
        ],
        "summary": "We continue the discussion of the fermion models on graphs that started in the first paper of the series. Here we introduce a Graphical Gauge Model (GGM) and show that : (a) it can be stated as an average/sum of a determinant defined on the graph over $\\mathbb{Z}_{2}$ (binary) gauge field; (b) it is equivalent to the Monomer-Dimer (MD) model on the graph; (c) the partition function of the model allows an explicit expression in terms of a series over disjoint directed cycles, where each term is a product of local contributions along the cycle and the determinant of a matrix defined on the remainder of the graph (excluding the cycle). We also establish a relation between the MD model on the graph and the determinant series, discussed in the first paper, however, considered using simple non-Belief-Propagation choice of the gauge. We conclude with a discussion of possible analytic and algorithmic consequences of these results, as well as related questions and challenges.",
        "published": "2008-09-20T03:17:59Z",
        "link": "http://arxiv.org/abs/0809.3481v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC",
            "cs.IT",
            "hep-th",
            "math.IT"
        ]
    },
    {
        "title": "Approximating acyclicity parameters of sparse hypergraphs",
        "authors": [
            "Fedor V. Fomin",
            "Petr A. Golovach",
            "Dimitrios M. Thilikos"
        ],
        "summary": "The notions of hypertree width and generalized hypertree width were introduced by Gottlob, Leone, and Scarcello in order to extend the concept of hypergraph acyclicity. These notions were further generalized by Grohe and Marx, who introduced the fractional hypertree width of a hypergraph. All these width parameters on hypergraphs are useful for extending tractability of many problems in database theory and artificial intelligence. In this paper, we study the approximability of (generalized, fractional) hyper treewidth of sparse hypergraphs where the criterion of sparsity reflects the sparsity of their incidence graphs. Our first step is to prove that the (generalized, fractional) hypertree width of a hypergraph H is constant-factor sandwiched by the treewidth of its incidence graph, when the incidence graph belongs to some apex-minor-free graph class. This determines the combinatorial borderline above which the notion of (generalized, fractional) hypertree width becomes essentially more general than treewidth, justifying that way its functionality as a hypergraph acyclicity measure. While for more general sparse families of hypergraphs treewidth of incidence graphs and all hypertree width parameters may differ arbitrarily, there are sparse families where a constant factor approximation algorithm is possible. In particular, we give a constant factor approximation polynomial time algorithm for (generalized, fractional) hypertree width on hypergraphs whose incidence graphs belong to some H-minor-free graph class.",
        "published": "2008-09-22T08:17:22Z",
        "link": "http://arxiv.org/abs/0809.3646v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Improved Monotone Circuit Depth Upper Bound for Directed Graph   Reachability",
        "authors": [
            "Sergey Volkov"
        ],
        "summary": "We prove that the directed graph reachability problem (transitive closure) can be solved by monotone fan-in 2 boolean circuits of depth (1/2+o(1))(log n)^2, where n is the number of nodes. This improves the previous known upper bound (1+o(1))(log n)^2. The proof is non-constructive, but we give a constructive proof of the upper bound (7/8+o(1))(log n)^2.",
        "published": "2008-09-22T15:14:06Z",
        "link": "http://arxiv.org/abs/0809.3614v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "From one solution of a 3-satisfiability formula to a solution cluster:   Frozen variables and entropy",
        "authors": [
            "Kang Li",
            "Hui Ma",
            "Haijun Zhou"
        ],
        "summary": "A solution to a 3-satisfiability (3-SAT) formula can be expanded into a cluster, all other solutions of which are reachable from this one through a sequence of single-spin flips. Some variables in the solution cluster are frozen to the same spin values by one of two different mechanisms: frozen-core formation and long-range frustrations. While frozen cores are identified by a local whitening algorithm, long-range frustrations are very difficult to trace, and they make an entropic belief-propagation (BP) algorithm fail to converge. For BP to reach a fixed point the spin values of a tiny fraction of variables (chosen according to the whitening algorithm) are externally fixed during the iteration. From the calculated entropy values, we infer that, for a large random 3-SAT formula with constraint density close to the satisfiability threshold, the solutions obtained by the survey-propagation or the walksat algorithm belong neither to the most dominating clusters of the formula nor to the most abundant clusters. This work indicates that a single solution cluster of a random 3-SAT formula may have further community structures.",
        "published": "2008-09-25T07:50:22Z",
        "link": "http://arxiv.org/abs/0809.4332v3",
        "categories": [
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "Llull and Copeland Voting Computationally Resist Bribery and Control",
        "authors": [
            "Piotr Faliszewski",
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "The only systems previously known to be resistant to all the standard control types were highly artificial election systems created by hybridization. We study a parameterized version of Copeland voting, denoted by Copeland^\\alpha, where the parameter \\alpha is a rational number between 0 and 1 that specifies how ties are valued in the pairwise comparisons of candidates. We prove that Copeland^{0.5}, the system commonly referred to as \"Copeland voting,\" provides full resistance to constructive control, and we prove the same for Copeland^\\alpha, for all rational \\alpha, 0 < \\alpha < 1. Copeland voting is the first natural election system proven to have full resistance to constructive control. We also prove that both Copeland^1 (Llull elections) and Copeland^0 are resistant to all standard types of constructive control other than one variant of addition of candidates. Moreover, we show that for each rational \\alpha, 0 \\leq \\alpha \\leq 1, Copeland^\\alpha voting is fully resistant to bribery attacks, and we establish fixed-parameter tractability of bounded-case control for Copeland^\\alpha. We also study Copeland^\\alpha elections under more flexible models such as microbribery and extended control and we integrate the potential irrationality of voter preferences into many of our results.",
        "published": "2008-09-25T19:49:38Z",
        "link": "http://arxiv.org/abs/0809.4484v2",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Complexity Classes as Mathematical Axioms",
        "authors": [
            "M. Freedman"
        ],
        "summary": "Treating a conjecture, P^#P != NP, on the separation of complexity classes as an axiom, an implication is found in three manifold topology with little obvious connection to complexity theory. This is reminiscent of Harvey Friedman's work on finitistic interpretations of large cardinal axioms.",
        "published": "2008-09-30T22:51:58Z",
        "link": "http://arxiv.org/abs/0810.0033v4",
        "categories": [
            "cs.CC",
            "math.GT"
        ]
    },
    {
        "title": "Three New Complexity Results for Resource Allocation Problems",
        "authors": [
            "Bart de Keijzer"
        ],
        "summary": "We prove the following results for task allocation of indivisible resources:   - The problem of finding a leximin-maximal resource allocation is in P if the agents have max-utility functions and atomic demands.   - Deciding whether a resource allocation is Pareto-optimal is coNP-complete for agents with (1-)additive utility functions.   - Deciding whether there exists a Pareto-optimal and envy-free resource allocation is Sigma_2^p-complete for agents with (1-)additive utility functions.",
        "published": "2008-10-02T20:32:52Z",
        "link": "http://arxiv.org/abs/0810.0532v2",
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.CC",
            "cs.GT"
        ]
    },
    {
        "title": "Oracularization and Two-Prover One-Round Interactive Proofs against   Nonlocal Strategies",
        "authors": [
            "Tsuyoshi Ito",
            "Hirotada Kobayashi",
            "Keiji Matsumoto"
        ],
        "summary": "A central problem in quantum computational complexity is how to prevent entanglement-assisted cheating in multi-prover interactive proof systems. It is well-known that the standard oracularization technique completely fails in some proof systems under the existence of prior entanglement. This paper studies two constructions of two-prover one-round interactive proof systems based on oracularization. First, it is proved that the two-prover one-round interactive proof system for PSPACE by Cai, Condon, and Lipton still achieves exponentially small soundness error in the existence of prior entanglement between dishonest provers (and more strongly, even if dishonest provers are allowed to use arbitrary no-signaling strategies). It follows that, unless the polynomial-time hierarchy collapses to the second level, two-prover systems are still advantageous to single-prover systems even when only malicious provers can use quantum information. Second, it is proved that the two-prover one-round interactive proof system obtained by oracularizing a three-query probabilistically checkable proof system becomes sound in a weak sense even against dishonest entangled provers with the help of a dummy question. As a consequence, every language in NEXP has a two-prover one-round interactive proof system of perfect completeness, albeit with exponentially small gap between completeness and soundness, in which each prover responds with only two bits. In other words, it is NP-hard to approximate within an inverse-polynomial the value of a classical two-prover one-round game, even when provers are entangled and each sends a two-bit answer to a verifier.",
        "published": "2008-10-03T18:24:50Z",
        "link": "http://arxiv.org/abs/0810.0693v1",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.CR"
        ]
    },
    {
        "title": "A simple constant-probability RP reduction from NP to Parity P",
        "authors": [
            "Cristopher Moore",
            "Alexander Russell"
        ],
        "summary": "The proof of Toda's celebrated theorem that the polynomial hierarchy is contained in $\\P^{# P}$ relies on the fact that, under mild technical conditions on the complexity class $C$, we have $\\exists C \\subset BP \\cdot \\oplus C$. More concretely, there is a randomized reduction which transforms nonempty sets and the empty set, respectively, into sets of odd or even size. The customary method is to invoke Valiant's and Vazirani's randomized reduction from NP to UP, followed by amplification of the resulting success probability from $1/\\poly(n)$ to a constant by combining the parities of $\\poly(n)$ trials. Here we give a direct algebraic reduction which achieves constant success probability without the need for amplification. Our reduction is very simple, and its analysis relies on well-known properties of the Legendre symbol in finite fields.",
        "published": "2008-10-06T17:23:06Z",
        "link": "http://arxiv.org/abs/0810.1018v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Constraint satisfaction problems with isolated solutions are hard",
        "authors": [
            "Lenka Zdeborová",
            "Marc Mézard"
        ],
        "summary": "We study the phase diagram and the algorithmic hardness of the random `locked' constraint satisfaction problems, and compare them to the commonly studied 'non-locked' problems like satisfiability of boolean formulas or graph coloring. The special property of the locked problems is that clusters of solutions are isolated points. This simplifies significantly the determination of the phase diagram, which makes the locked problems particularly appealing from the mathematical point of view. On the other hand we show empirically that the clustered phase of these problems is extremely hard from the algorithmic point of view: the best known algorithms all fail to find solutions. Our results suggest that the easy/hard transition (for currently known algorithms) in the locked problems coincides with the clustering transition. These should thus be regarded as new benchmarks of really hard constraint satisfaction problems.",
        "published": "2008-10-08T18:28:28Z",
        "link": "http://arxiv.org/abs/0810.1499v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "1.25 Approximation Algorithm for the Steiner Tree Problem with Distances   One and Two",
        "authors": [
            "Piotr Berman",
            "Marek Karpinski",
            "Alex Zelikovsky"
        ],
        "summary": "We give a 1.25 approximation algorithm for the Steiner Tree Problem with distances one and two, improving on the best known bound for that problem.",
        "published": "2008-10-10T11:25:09Z",
        "link": "http://arxiv.org/abs/0810.1851v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Counting cocircuits and convex two-colourings is #P-complete",
        "authors": [
            "Andrew J. Goodall",
            "Steven D. Noble"
        ],
        "summary": "We prove that the problem of counting the number of colourings of the vertices of a graph with at most two colours, such that the colour classes induce connected subgraphs is #P-complete. We also show that the closely related problem of counting the number of cocircuits of a graph is #P-complete.",
        "published": "2008-10-11T17:11:35Z",
        "link": "http://arxiv.org/abs/0810.2042v1",
        "categories": [
            "math.CO",
            "cs.CC",
            "05C15; 68R10; 68Q17"
        ]
    },
    {
        "title": "Divisibility, Smoothness and Cryptographic Applications",
        "authors": [
            "David Naccache",
            "Igor E. Shparlinski"
        ],
        "summary": "This paper deals with products of moderate-size primes, familiarly known as smooth numbers. Smooth numbers play a crucial role in information theory, signal processing and cryptography.   We present various properties of smooth numbers relating to their enumeration, distribution and occurrence in various integer sequences. We then turn our attention to cryptographic applications in which smooth numbers play a pivotal role.",
        "published": "2008-10-12T02:28:07Z",
        "link": "http://arxiv.org/abs/0810.2067v3",
        "categories": [
            "math.NT",
            "cs.CC",
            "cs.CR",
            "11N25; 11Y16; 9460"
        ]
    },
    {
        "title": "On finite functions with non-trivial arity gap",
        "authors": [
            "Slavcho Shtrakov",
            "Joerg Koppitz"
        ],
        "summary": "Given an $n$-ary   $k-$valued function $f$, $gap(f)$ denotes the minimal number of essential variables in $f$ which become fictive when identifying any two distinct essential variables in $f$.   We particularly solve a problem concerning the explicit determination of $n$-ary   $k-$valued functions $f$ with $2\\leq gap(f)\\leq n\\leq k$. Our methods yield new combinatorial results about the number of such functions.",
        "published": "2008-10-13T18:09:27Z",
        "link": "http://arxiv.org/abs/0810.2279v6",
        "categories": [
            "cs.DM",
            "cs.CC",
            "G.2.0"
        ]
    },
    {
        "title": "On the Complexity of Core, Kernel, and Bargaining Set",
        "authors": [
            "Gianluigi Greco",
            "Enrico Malizia",
            "Luigi Palopoli",
            "Francesco Scarcello"
        ],
        "summary": "Coalitional games are mathematical models suited to analyze scenarios where players can collaborate by forming coalitions in order to obtain higher worths than by acting in isolation. A fundamental problem for coalitional games is to single out the most desirable outcomes in terms of appropriate notions of worth distributions, which are usually called solution concepts. Motivated by the fact that decisions taken by realistic players cannot involve unbounded resources, recent computer science literature reconsidered the definition of such concepts by advocating the relevance of assessing the amount of resources needed for their computation in terms of their computational complexity. By following this avenue of research, the paper provides a complete picture of the complexity issues arising with three prominent solution concepts for coalitional games with transferable utility, namely, the core, the kernel, and the bargaining set, whenever the game worth-function is represented in some reasonable compact form (otherwise, if the worths of all coalitions are explicitly listed, the input sizes are so large that complexity problems are---artificially---trivial). The starting investigation point is the setting of graph games, about which various open questions were stated in the literature. The paper gives an answer to these questions, and in addition provides new insights on the setting, by characterizing the computational complexity of the three concepts in some relevant generalizations and specializations.",
        "published": "2008-10-17T11:53:30Z",
        "link": "http://arxiv.org/abs/0810.3136v3",
        "categories": [
            "cs.GT",
            "cs.AI",
            "cs.CC",
            "F.2; J.4"
        ]
    },
    {
        "title": "Reduced Kronecker coefficients and counter-examples to Mulmuley's strong   saturation conjecture SH",
        "authors": [
            "Emmanuel Briand",
            "Rosa Orellana",
            "Mercedes Rosas"
        ],
        "summary": "We provide counter-examples to Mulmuley's strong saturation conjecture (strong SH) for the Kronecker coefficients. This conjecture was proposed in the setting of Geometric Complexity Theory to show that deciding whether or not a Kronecker coefficient is zero can be done in polynomial time. We also provide a short proof of the #P-hardness of computing the Kronecker coefficients. Both results rely on the connections between the Kronecker coefficients and another family of structural constants in the representation theory of the symmetric groups: Murnaghan's reduced Kronecker coefficients.   An appendix by Mulmuley introduces a relaxed form of the saturation hypothesis SH, still strong enough for the aims of Geometric Complexity Theory.",
        "published": "2008-10-17T14:09:04Z",
        "link": "http://arxiv.org/abs/0810.3163v3",
        "categories": [
            "math.CO",
            "cs.CC",
            "math.RT"
        ]
    },
    {
        "title": "The many faces of optimism - Extended version",
        "authors": [
            "István Szita",
            "András Lőrincz"
        ],
        "summary": "The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learning. \"Optimism in the face of uncertainty\" and model building play central roles in advanced exploration methods. Here, we integrate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in polynomial time, and give experimental evidence that it is robust and efficient compared to its ascendants.",
        "published": "2008-10-20T02:09:16Z",
        "link": "http://arxiv.org/abs/0810.3451v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LG"
        ]
    },
    {
        "title": "Recursive Concurrent Stochastic Games",
        "authors": [
            "Kousha Etessami",
            "Mihalis Yannakakis"
        ],
        "summary": "We study Recursive Concurrent Stochastic Games (RCSGs), extending our recent analysis of recursive simple stochastic games to a concurrent setting where the two players choose moves simultaneously and independently at each state. For multi-exit games, our earlier work already showed undecidability for basic questions like termination, thus we focus on the important case of single-exit RCSGs (1-RCSGs).   We first characterize the value of a 1-RCSG termination game as the least fixed point solution of a system of nonlinear minimax functional equations, and use it to show PSPACE decidability for the quantitative termination problem. We then give a strategy improvement technique, which we use to show that player 1 (maximizer) has \\epsilon-optimal randomized Stackless & Memoryless (r-SM) strategies for all \\epsilon > 0, while player 2 (minimizer) has optimal r-SM strategies. Thus, such games are r-SM-determined. These results mirror and generalize in a strong sense the randomized memoryless determinacy results for finite stochastic games, and extend the classic Hoffman-Karp strategy improvement approach from the finite to an infinite state setting. The proofs in our infinite-state setting are very different however, relying on subtle analytic properties of certain power series that arise from studying 1-RCSGs.   We show that our upper bounds, even for qualitative (probability 1) termination, can not be improved, even to NP, without a major breakthrough, by giving two reductions: first a P-time reduction from the long-standing square-root sum problem to the quantitative termination decision problem for finite concurrent stochastic games, and then a P-time reduction from the latter problem to the qualitative termination problem for 1-RCSGs.",
        "published": "2008-10-20T15:18:44Z",
        "link": "http://arxiv.org/abs/0810.3581v3",
        "categories": [
            "cs.GT",
            "cs.CC",
            "G.3; F.2; F.1.1"
        ]
    },
    {
        "title": "An Efficient Quantum Algorithm for the Hidden Subgroup Problem over   Weyl-Heisenberg Groups",
        "authors": [
            "Hari Krovi",
            "Martin Roetteler"
        ],
        "summary": "Many exponential speedups that have been achieved in quantum computing are obtained via hidden subgroup problems (HSPs). We show that the HSP over Weyl-Heisenberg groups can be solved efficiently on a quantum computer. These groups are well-known in physics and play an important role in the theory of quantum error-correcting codes. Our algorithm is based on non-commutative Fourier analysis of coset states which are quantum states that arise from a given black-box function. We use Clebsch-Gordan decompositions to combine and reduce tensor products of irreducible representations. Furthermore, we use a new technique of changing labels of irreducible representations to obtain low-dimensional irreducible representations in the decomposition process. A feature of the presented algorithm is that in each iteration of the algorithm the quantum computer operates on two coset states simultaneously. This is an improvement over the previously best known quantum algorithm for these groups which required four coset states.",
        "published": "2008-10-20T21:50:45Z",
        "link": "http://arxiv.org/abs/0810.3695v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Ogden's Lemma for Regular Tree Languages",
        "authors": [
            "Marco Kuhlmann"
        ],
        "summary": "We motivate and prove a strong pumping lemma for regular tree languages. The new lemma can be seen as the natural correspondent of Ogden's lemma for context-free string languages.",
        "published": "2008-10-23T19:43:26Z",
        "link": "http://arxiv.org/abs/0810.4249v1",
        "categories": [
            "cs.CC",
            "F.4.3"
        ]
    },
    {
        "title": "New Constructions for Query-Efficient Locally Decodable Codes of   Subexponential Length",
        "authors": [
            "Toshiya Itoh",
            "Yasuhiro Suzuki"
        ],
        "summary": "A $(k,\\delta,\\epsilon)$-locally decodable code $C: F_{q}^{n} \\to F_{q}^{N}$ is an error-correcting code that encodes each message $\\vec{x}=(x_{1},x_{2},...,x_{n}) \\in F_{q}^{n}$ to $C(\\vec{x}) \\in F_{q}^{N}$ and has the following property: For any $\\vec{y} \\in {\\bf F}_{q}^{N}$ such that $d(\\vec{y},C(\\vec{x})) \\leq \\delta N$ and each $1 \\leq i \\leq n$, the symbol $x_{i}$ of $\\vec{x}$ can be recovered with probability at least $1-\\epsilon$ by a randomized decoding algorithm looking only at $k$ coordinates of $\\vec{y}$. The efficiency of a $(k,\\delta,\\epsilon)$-locally decodable code $C: F_{q}^{n} \\to F_{q}^{N}$ is measured by the code length $N$ and the number $k$ of queries. For any $k$-query locally decodable code $C: F_{q}^{n} \\to F_{q}^{N}$, the code length $N$ is conjectured to be exponential of $n$, however, this was disproved. Yekhanin [In Proc. of STOC, 2007] showed that there exists a 3-query locally decodable code $C: F_{2}^{n} \\to F_{2}^{N}$ such that $N=\\exp(n^{(1/\\log \\log n)})$ assuming that the number of Mersenne primes is infinite. For a 3-query locally decodable code $C: F_{q}^{n} \\to F_{q}^{N}$, Efremenko [ECCC Report No.69, 2008] reduced the code length further to $N=\\exp(n^{O((\\log \\log n/ \\log n)^{1/2})})$, and also showed that for any integer $r>1$, there exists a $k$-query locally decodable code $C: F_{q}^{n} \\to F_{q}^{N}$ such that $k \\leq 2^{r}$ and $N=\\exp(n^{O((\\log \\log n/ \\log n)^{1-1/r})})$. In this paper, we present a query-efficient locally decodable code and show that for any integer $r>1$, there exists a $k$-query locally decodable code $C: F_{q}^{n} \\to F_{q}^{N}$ such that $k \\leq 3 \\cdot 2^{r-2}$ and $N=\\exp(n^{O((\\log \\log n/ \\log n)^{1-1/r})})$.",
        "published": "2008-10-25T04:53:23Z",
        "link": "http://arxiv.org/abs/0810.4576v2",
        "categories": [
            "cs.CC",
            "cs.CR",
            "F.1.2; F.2.2; G.2.1"
        ]
    },
    {
        "title": "Kernel(s) for Problems With no Kernel: On Out-Trees With Many Leaves",
        "authors": [
            "Henning Fernau",
            "Fedor V. Fomin",
            "Daniel Lokshtanov",
            "Daniel Raible",
            "Saket Saurabh",
            "Yngve Villanger"
        ],
        "summary": "The {\\sc $k$-Leaf Out-Branching} problem is to find an out-branching (i.e. a rooted oriented spanning tree) with at least $k$ leaves in a given digraph. The problem has recently received much attention from the viewpoint of parameterized algorithms {alonLNCS4596,AlonFGKS07fsttcs,BoDo2,KnLaRo}. In this paper we step aside and take a kernelization based approach to the {\\sc $k$-Leaf-Out-Branching} problem. We give the first polynomial kernel for {\\sc Rooted $k$-Leaf-Out-Branching}, a variant of {\\sc $k$-Leaf-Out-Branching} where the root of the tree searched for is also a part of the input. Our kernel has cubic size and is obtained using extremal combinatorics.   For the {\\sc $k$-Leaf-Out-Branching} problem we show that no polynomial kernel is possible unless polynomial hierarchy collapses to third level %$PH=\\Sigma_p^3$ by applying a recent breakthrough result by Bodlaender et al. {BDFH08} in a non-trivial fashion. However our positive results for {\\sc Rooted $k$-Leaf-Out-Branching} immediately imply that the seemingly intractable the {\\sc $k$-Leaf-Out-Branching} problem admits a data reduction to $n$ independent $O(k^3)$ kernels. These two results, tractability and intractability side by side, are the first separating {\\it many-to-one kernelization} from {\\it Turing kernelization}. This answers affirmatively an open problem regarding \"cheat kernelization\" raised in {IWPECOPEN08}.",
        "published": "2008-10-27T11:53:47Z",
        "link": "http://arxiv.org/abs/0810.4796v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "The Pursuit of Uniqueness: Extending Valiant-Vazirani Theorem to the   Probabilistic and Quantum Settings",
        "authors": [
            "Dorit Aharonov",
            "Michael Ben-Or",
            "Fernando G. S. L. Brandao",
            "Or Sattath"
        ],
        "summary": "Valiant-Vazirani showed in 1985 [VV85] that solving NP with the promise that \"yes\" instances have only one witness is powerful enough to solve the entire NP class (under randomized reductions).   We are interested in extending this result to the quantum setting. We prove extensions to the classes Merlin-Arthur MA and Quantum-Classical-Merlin-Arthur QCMA. Our results have implications for the complexity of approximating the ground state energy of a quantum local Hamiltonian with a unique ground state and an inverse polynomial spectral gap. We show that the estimation (to within polynomial accuracy) of the ground state energy of poly-gapped 1-D local Hamiltonians is QCMA-hard [AN02], under randomized reductions. This is in stark contrast to the case of constant gapped 1-D Hamiltonians, which is in NP [Has07]. Moreover, it shows that unless QCMA can be reduced to NP by randomized reductions, there is no classical description of the ground state of every poly-gapped local Hamiltonian that allows efficient calculation of expectation values.   Finally, we discuss a few of the obstacles to the establishment of an analogous result to the class Quantum-Merlin-Arthur (QMA). In particular, we show that random projections fail to provide a polynomial gap between two witnesses.",
        "published": "2008-10-27T18:23:31Z",
        "link": "http://arxiv.org/abs/0810.4840v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "FPT Algorithms and Kernels for the Directed $k$-Leaf Problem",
        "authors": [
            "Jean Daligault",
            "Gregory Gutin",
            "Eun Jung Kim",
            "Anders Yeo"
        ],
        "summary": "A subgraph $T$ of a digraph $D$ is an {\\em out-branching} if $T$ is an oriented spanning tree with only one vertex of in-degree zero (called the {\\em root}). The vertices of $T$ of out-degree zero are {\\em leaves}. In the {\\sc Directed $k$-Leaf} Problem, we are given a digraph $D$ and an integral parameter $k$, and we are to decide whether $D$ has an out-branching with at least $k$ leaves. Recently, Kneis et al. (2008) obtained an algorithm for the problem of running time $4^{k}\\cdot n^{O(1)}$. We describe a new algorithm for the problem of running time $3.72^{k}\\cdot n^{O(1)}$. In {\\sc Rooted Directed $k$-Leaf} Problem, apart from $D$ and $k$, we are given a vertex $r$ of $D$ and we are to decide whether $D$ has an out-branching rooted at $r$ with at least $k$ leaves. Very recently, Fernau et al. (2008) found an $O(k^3)$-size kernel for {\\sc Rooted Directed $k$-Leaf}. In this paper, we obtain an $O(k)$ kernel for {\\sc Rooted Directed $k$-Leaf} restricted to acyclic digraphs.",
        "published": "2008-10-27T21:44:42Z",
        "link": "http://arxiv.org/abs/0810.4946v3",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Automatic structures of bounded degree revisited",
        "authors": [
            "Dietrich Kuske",
            "Markus Lohrey"
        ],
        "summary": "The first-order theory of a string automatic structure is known to be decidable, but there are examples of string automatic structures with nonelementary first-order theories. We prove that the first-order theory of a string automatic structure of bounded degree is decidable in doubly exponential space (for injective automatic presentations, this holds even uniformly). This result is shown to be optimal since we also present a string automatic structure of bounded degree whose first-order theory is hard for 2EXPSPACE. We prove similar results also for tree automatic structures. These findings close the gaps left open in a previous paper of the second author by improving both, the lower and the upper bounds.",
        "published": "2008-10-28T09:49:06Z",
        "link": "http://arxiv.org/abs/0810.4998v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "P is not equal to NP",
        "authors": [
            "Sten-Ake Tarnlund"
        ],
        "summary": "SAT is not in P, is true and provable in a simply consistent extension B' of a first order theory B of computing, with a single finite axiom characterizing a universal Turing machine. Therefore, P is not equal to NP, is true and provable in a simply consistent extension B\" of B.",
        "published": "2008-10-28T16:32:58Z",
        "link": "http://arxiv.org/abs/0810.5056v2",
        "categories": [
            "cs.CC",
            "cs.LO",
            "D.1.6; F.1.3; F.4.1"
        ]
    },
    {
        "title": "Kaltofen's division-free determinant algorithm differentiated for matrix   adjoint computation",
        "authors": [
            "Gilles Villard"
        ],
        "summary": "Kaltofen has proposed a new approach in 1992 for computing matrix determinants without divisions. The algorithm is based on a baby steps/giant steps construction of Krylov subspaces, and computes the determinant as the constant term of a characteristic polynomial. For matrices over an abstract ring, by the results of Baur and Strassen, the determinant algorithm, actually a straight-line program, leads to an algorithm with the same complexity for computing the adjoint of a matrix. However, the latter adjoint algorithm is obtained by the reverse mode of automatic differentiation, hence somehow is not \"explicit\". We present an alternative (still closely related) algorithm for the adjoint thatcan be implemented directly, we mean without resorting to an automatic transformation. The algorithm is deduced by applying program differentiation techniques \"by hand\" to Kaltofen's method, and is completely decribed. As subproblem, we study the differentiation of programs that compute minimum polynomials of lineraly generated sequences, and we use a lazy polynomial evaluation mechanism for reducing the cost of Strassen's avoidance of divisions in our case.",
        "published": "2008-10-31T09:43:48Z",
        "link": "http://arxiv.org/abs/0810.5647v1",
        "categories": [
            "cs.SC",
            "cs.CC"
        ]
    },
    {
        "title": "Multi-Objective Model Checking of Markov Decision Processes",
        "authors": [
            "Kousha Etessami",
            "Marta Kwiatkowska",
            "Moshe Y. Vardi",
            "Mihalis Yannakakis"
        ],
        "summary": "We study and provide efficient algorithms for multi-objective model checking problems for Markov Decision Processes (MDPs). Given an MDP, M, and given multiple linear-time (\\omega -regular or LTL) properties \\varphi\\_i, and probabilities r\\_i \\epsilon [0,1], i=1,...,k, we ask whether there exists a strategy \\sigma for the controller such that, for all i, the probability that a trajectory of M controlled by \\sigma satisfies \\varphi\\_i is at least r\\_i. We provide an algorithm that decides whether there exists such a strategy and if so produces it, and which runs in time polynomial in the size of the MDP. Such a strategy may require the use of both randomization and memory. We also consider more general multi-objective \\omega -regular queries, which we motivate with an application to assume-guarantee compositional reasoning for probabilistic systems.   Note that there can be trade-offs between different properties: satisfying property \\varphi\\_1 with high probability may necessitate satisfying \\varphi\\_2 with low probability. Viewing this as a multi-objective optimization problem, we want information about the \"trade-off curve\" or Pareto curve for maximizing the probabilities of different properties. We show that one can compute an approximate Pareto curve with respect to a set of \\omega -regular properties in time polynomial in the size of the MDP.   Our quantitative upper bounds use LP methods. We also study qualitative multi-objective model checking problems, and we show that these can be analysed by purely graph-theoretic methods, even though the strategies may still require both randomization and memory.",
        "published": "2008-10-31T16:18:14Z",
        "link": "http://arxiv.org/abs/0810.5728v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.GT",
            "G.3; F.2; F.3.1; F.4.1"
        ]
    },
    {
        "title": "A complexity dichotomy for hypergraph partition functions",
        "authors": [
            "Martin Dyer",
            "Leslie Ann Goldberg",
            "Mark Jerrum"
        ],
        "summary": "We consider the complexity of counting homomorphisms from an $r$-uniform hypergraph $G$ to a symmetric $r$-ary relation $H$. We give a dichotomy theorem for $r>2$, showing for which $H$ this problem is in FP and for which $H$ it is #P-complete. This generalises a theorem of Dyer and Greenhill (2000) for the case $r=2$, which corresponds to counting graph homomorphisms. Our dichotomy theorem extends to the case in which the relation $H$ is weighted, and the goal is to compute the \\emph{partition function}, which is the sum of weights of the homomorphisms. This problem is motivated by statistical physics, where it arises as computing the partition function for particle models in which certain combinations of $r$ sites interact symmetrically. In the weighted case, our dichotomy theorem generalises a result of Bulatov and Grohe (2005) for graphs, where $r=2$. When $r=2$, the polynomial time cases of the dichotomy correspond simply to rank-1 weights. Surprisingly, for all $r>2$ the polynomial time cases of the dichotomy have rather more structure. It turns out that the weights must be superimposed on a combinatorial structure defined by solutions of an equation over an Abelian group. Our result also gives a dichotomy for a closely related constraint satisfaction problem.",
        "published": "2008-10-31T22:42:57Z",
        "link": "http://arxiv.org/abs/0811.0037v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2.2; F.4.1; G.2.1"
        ]
    },
    {
        "title": "Solving the P/NP Problem under Intrinsic Uncertainty",
        "authors": [
            "Stefan Jaeger"
        ],
        "summary": "Heisenberg's uncertainty principle states that it is not possible to compute both the position and momentum of an electron with absolute certainty. However, this computational limitation, which is central to quantum mechanics, has no counterpart in theoretical computer science. Here, I will show that we can distinguish between the complexity classes P and NP when we consider intrinsic uncertainty in our computations, and take uncertainty about whether a bit belongs to the program code or machine input into account. Given intrinsic uncertainty, every output is uncertain, and computations become meaningful only in combination with a confidence level. In particular, it is impossible to compute solutions with absolute certainty as this requires infinite run-time. Considering intrinsic uncertainty, I will present a function that is in NP but not in P, and thus prove that P is a proper subset of NP. I will also show that all traditional hard decision problems have polynomial-time algorithms that provide solutions with confidence under uncertainty.",
        "published": "2008-11-04T10:08:47Z",
        "link": "http://arxiv.org/abs/0811.0463v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Secure Arithmetic Computation with No Honest Majority",
        "authors": [
            "Yuval Ishai",
            "Manoj Prabhakaran",
            "Amit Sahai"
        ],
        "summary": "We study the complexity of securely evaluating arithmetic circuits over finite rings. This question is motivated by natural secure computation tasks. Focusing mainly on the case of two-party protocols with security against malicious parties, our main goals are to: (1) only make black-box calls to the ring operations and standard cryptographic primitives, and (2) minimize the number of such black-box calls as well as the communication overhead.   We present several solutions which differ in their efficiency, generality, and underlying intractability assumptions. These include:   1. An unconditionally secure protocol in the OT-hybrid model which makes a black-box use of an arbitrary ring $R$, but where the number of ring operations grows linearly with (an upper bound on) $\\log|R|$.   2. Computationally secure protocols in the OT-hybrid model which make a black-box use of an underlying ring, and in which the number of ring operations does not grow with the ring size. These results extend a previous approach of Naor and Pinkas for secure polynomial evaluation (SIAM J. Comput., 35(5), 2006).   3. A protocol for the rings $\\mathbb{Z}_m=\\mathbb{Z}/m\\mathbb{Z}$ which only makes a black-box use of a homomorphic encryption scheme. When $m$ is prime, the (amortized) number of calls to the encryption scheme for each gate of the circuit is constant.   All of our protocols are in fact UC-secure in the OT-hybrid model and can be generalized to multiparty computation with an arbitrary number of malicious parties.",
        "published": "2008-11-04T11:18:06Z",
        "link": "http://arxiv.org/abs/0811.0475v3",
        "categories": [
            "cs.CR",
            "cs.CC"
        ]
    },
    {
        "title": "Algorithmic complexity and randomness in elastic solids",
        "authors": [
            "J. Ratsaby",
            "J. Chaskalovic"
        ],
        "summary": "A system comprised of an elastic solid and its response to an external random force sequence is shown to behave based on the principles of the theory of algorithmic complexity and randomness. The solid distorts the randomness of an input force sequence in a way proportional to its algorithmic complexity. We demonstrate this by numerical analysis of a one-dimensional vibrating elastic solid (the system) on which we apply a maximally random input force. The level of complexity of the system is controlled via external parameters. The output response is the field of displacements observed at several positions on the body. The algorithmic complexity and stochasticity of the resulting output displacement sequence is measured and compared against the complexity of the system. The results show that the higher the system complexity the more random-deficient the output sequence. This agrees with the theory introduced in [16] which states that physical systems such as this behave as algorithmic selection-rules which act on random actions in their surroundings.",
        "published": "2008-11-04T21:34:54Z",
        "link": "http://arxiv.org/abs/0811.0623v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Note on the Inversion Complexity of Boolean Functions in Boolean   Formulas",
        "authors": [
            "Hiroki Morizumi"
        ],
        "summary": "In this note, we consider the minimum number of NOT operators in a Boolean formula representing a Boolean function. In circuit complexity theory, the minimum number of NOT gates in a Boolean circuit computing a Boolean function $f$ is called the inversion complexity of $f$. In 1958, Markov determined the inversion complexity of every Boolean function and particularly proved that $\\lceil \\log_2(n+1) \\rceil$ NOT gates are sufficient to compute any Boolean function on $n$ variables. As far as we know, no result is known for inversion complexity in Boolean formulas, i.e., the minimum number of NOT operators in a Boolean formula representing a Boolean function. The aim of this note is showing that we can determine the inversion complexity of every Boolean function in Boolean formulas by arguments based on the study of circuit complexity.",
        "published": "2008-11-05T11:22:15Z",
        "link": "http://arxiv.org/abs/0811.0699v1",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "The Complexity of Propositional Implication",
        "authors": [
            "Olaf Beyersdorff",
            "Arne Meier",
            "Michael Thomas",
            "Heribert Vollmer"
        ],
        "summary": "The question whether a set of formulae G implies a formula f is fundamental. The present paper studies the complexity of the above implication problem for propositional formulae that are built from a systematically restricted set of Boolean connectives. We give a complete complexity classification for all sets of Boolean functions in the meaning of Post's lattice and show that the implication problem is efficentily solvable only if the connectives are definable using the constants {false,true} and only one of {and,or,xor}. The problem remains coNP-complete in all other cases. We also consider the restriction of G to singletons.",
        "published": "2008-11-06T14:44:57Z",
        "link": "http://arxiv.org/abs/0811.0959v3",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Non-classical Role of Potential Energy in Adiabatic Quantum Annealing",
        "authors": [
            "Arnab Das"
        ],
        "summary": "Adiabatic quantum annealing is a paradigm of analog quantum computation, where a given computational job is converted to the task of finding the global minimum of some classical potential energy function and the search for the global potential minimum is performed by employing external kinetic quantum fluctuations and subsequent slow reduction (annealing) of them. In this method, the entire potential energy landscape (PEL) may be accessed simultaneously through a delocalized wave-function, in contrast to a classical search, where the searcher has to visit different points in the landscape (i.e., individual classical configurations) sequentially. Thus in such searches, the role of the potential energy might be significantly different in the two cases. Here we discuss this in the context of searching of a single isolated hole (potential minimum) in a golf-course type gradient free PEL. We show, that the quantum particle would be able to locate the hole faster if the hole is deeper, while the classical particle of course would have no scope to exploit the depth of the hole. We also discuss the effect of the underlying quantum phase transition on the adiabatic dynamics.",
        "published": "2008-11-06T15:54:36Z",
        "link": "http://arxiv.org/abs/0811.0881v1",
        "categories": [
            "quant-ph",
            "cond-mat.stat-mech",
            "cs.CC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Modular difference logic is hard",
        "authors": [
            "Nikolaj Bjørner",
            "Andreas Blass",
            "Yuri Gurevich",
            "Madan Musuvathi"
        ],
        "summary": "In connection with machine arithmetic, we are interested in systems of constraints of the form x + k \\leq y + k'. Over integers, the satisfiability problem for such systems is polynomial time. The problem becomes NP complete if we restrict attention to the residues for a fixed modulus N.",
        "published": "2008-11-06T19:52:45Z",
        "link": "http://arxiv.org/abs/0811.0987v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Symbolic Backwards-Reachability Analysis for Higher-Order Pushdown   Systems",
        "authors": [
            "Matthew Hague",
            "C. -H. Luke Ong"
        ],
        "summary": "Higher-order pushdown systems (PDSs) generalise pushdown systems through the use of higher-order stacks, that is, a nested \"stack of stacks\" structure. These systems may be used to model higher-order programs and are closely related to the Caucal hierarchy of infinite graphs and safe higher-order recursion schemes.   We consider the backwards-reachability problem over higher-order Alternating PDSs (APDSs), a generalisation of higher-order PDSs. This builds on and extends previous work on pushdown systems and context-free higher-order processes in a non-trivial manner. In particular, we show that the set of configurations from which a regular set of higher-order APDS configurations is reachable is regular and computable in n-EXPTIME. In fact, the problem is n-EXPTIME-complete.   We show that this work has several applications in the verification of higher-order PDSs, such as linear-time model-checking, alternation-free mu-calculus model-checking and the computation of winning regions of reachability games.",
        "published": "2008-11-07T10:29:11Z",
        "link": "http://arxiv.org/abs/0811.1103v2",
        "categories": [
            "cs.CC",
            "cs.GT",
            "F.1.1"
        ]
    },
    {
        "title": "Resolution Trees with Lemmas: Resolution Refinements that Characterize   DLL Algorithms with Clause Learning",
        "authors": [
            "Samuel R. Buss",
            "Jan Hoffmann",
            "Jan Johannsen"
        ],
        "summary": "Resolution refinements called w-resolution trees with lemmas (WRTL) and with input lemmas (WRTI) are introduced. Dag-like resolution is equivalent to both WRTL and WRTI when there is no regularity condition. For regular proofs, an exponential separation between regular dag-like resolution and both regular WRTL and regular WRTI is given.   It is proved that DLL proof search algorithms that use clause learning based on unit propagation can be polynomially simulated by regular WRTI. More generally, non-greedy DLL algorithms with learning by unit propagation are equivalent to regular WRTI. A general form of clause learning, called DLL-Learn, is defined that is equivalent to regular WRTL.   A variable extension method is used to give simulations of resolution by regular WRTI, using a simplified form of proof trace extensions. DLL-Learn and non-greedy DLL algorithms with learning by unit propagation can use variable extensions to simulate general resolution without doing restarts.   Finally, an exponential lower bound for WRTL where the lemmas are restricted to short clauses is shown.",
        "published": "2008-11-07T15:31:58Z",
        "link": "http://arxiv.org/abs/0811.1075v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "Applying Practice to Theory",
        "authors": [
            "Ryan Williams"
        ],
        "summary": "How can complexity theory and algorithms benefit from practical advances in computing? We give a short overview of some prior work using practical computing to attack problems in computational complexity and algorithms, informally describe how linear program solvers may be used to help prove new lower bounds for satisfiability, and suggest a research program for developing new understanding in circuit complexity.",
        "published": "2008-11-09T00:49:41Z",
        "link": "http://arxiv.org/abs/0811.1305v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "A Divergence Formula for Randomness and Dimension",
        "authors": [
            "Jack H. Lutz"
        ],
        "summary": "If $S$ is an infinite sequence over a finite alphabet $\\Sigma$ and $\\beta$ is a probability measure on $\\Sigma$, then the {\\it dimension} of $ S$ with respect to $\\beta$, written $\\dim^\\beta(S)$, is a constructive version of Billingsley dimension that coincides with the (constructive Hausdorff) dimension $\\dim(S)$ when $\\beta$ is the uniform probability measure. This paper shows that $\\dim^\\beta(S)$ and its dual $\\Dim^\\beta(S)$, the {\\it strong dimension} of $S$ with respect to $\\beta$, can be used in conjunction with randomness to measure the similarity of two probability measures $\\alpha$ and $\\beta$ on $\\Sigma$. Specifically, we prove that the {\\it divergence formula} \\[   \\dim^\\beta(R) = \\Dim^\\beta(R) =\\frac{\\CH(\\alpha)}{\\CH(\\alpha) + \\D(\\alpha || \\beta)} \\] holds whenever $\\alpha$ and $\\beta$ are computable, positive probability measures on $\\Sigma$ and $R \\in \\Sigma^\\infty$ is random with respect to $\\alpha$. In this formula, $\\CH(\\alpha)$ is the Shannon entropy of $\\alpha$, and $\\D(\\alpha||\\beta)$ is the Kullback-Leibler divergence between $\\alpha$ and $\\beta$. We also show that the above formula holds for all sequences $R$ that are $\\alpha$-normal (in the sense of Borel) when $\\dim^\\beta(R)$ and $\\Dim^\\beta(R)$ are replaced by the more effective finite-state dimensions $\\dimfs^\\beta(R)$ and $\\Dimfs^\\beta(R)$. In the course of proving this, we also prove finite-state compression characterizations of $\\dimfs^\\beta(S)$ and $\\Dimfs^\\beta(S)$.",
        "published": "2008-11-12T06:30:55Z",
        "link": "http://arxiv.org/abs/0811.1825v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Computing voting power in easy weighted voting games",
        "authors": [
            "Haris Aziz",
            "Mike Paterson"
        ],
        "summary": "Weighted voting games are ubiquitous mathematical models which are used in economics, political science, neuroscience, threshold logic, reliability theory and distributed systems. They model situations where agents with variable voting weight vote in favour of or against a decision. A coalition of agents is winning if and only if the sum of weights of the coalition exceeds or equals a specified quota. The Banzhaf index is a measure of voting power of an agent in a weighted voting game. It depends on the number of coalitions in which the agent is the difference in the coalition winning or losing. It is well known that computing Banzhaf indices in a weighted voting game is NP-hard. We give a comprehensive classification of weighted voting games which can be solved in polynomial time. Among other results, we provide a polynomial ($O(k{(\\frac{n}{k})}^k)$) algorithm to compute the Banzhaf indices in weighted voting games in which the number of weight values is bounded by $k$.",
        "published": "2008-11-15T14:55:51Z",
        "link": "http://arxiv.org/abs/0811.2497v2",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "On models of a nondeterministic computation",
        "authors": [
            "M. N. Vyalyi"
        ],
        "summary": "In this paper we consider a nondeterministic computation by deterministic multi-head 2-way automata having a read-only access to an auxiliary memory. The memory contains additional data (a guess) and computation is successful iff it is successful for some memory content. Also we consider the case of restricted guesses in which a guess should satisfy some constraint. We show that the standard complexity classes such as L, NL, P, NP, PSPACE can be characterized in terms of these models of nondeterministic computation. These characterizations differ from the well-known ones by absence of alternation.",
        "published": "2008-11-16T16:22:11Z",
        "link": "http://arxiv.org/abs/0811.2586v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Topological Dynamics of Cellular Automata: Dimension Matters",
        "authors": [
            "Mathieu Sablik",
            "Guillaume Theyssier"
        ],
        "summary": "Topological dynamics of cellular automata (CA), inherited from classical dynamical systems theory, has been essentially studied in dimension 1. This paper focuses on higher dimensional CA and aims at showing that the situation is different and more complex starting from dimension 2. The main results are the existence of non sensitive CA without equicontinuous points, the non-recursivity of sensitivity constants, the existence of CA having only non-recursive equicontinuous points and the existence of CA having only countably many equicontinuous points. They all show a difference between dimension 1 and higher dimensions. Thanks to these new constructions, we also extend undecidability results concerning topological classification previously obtained in the 1D case. Finally, we show that the set of sensitive CA is only Pi_2 in dimension 1, but becomes Sigma_3-hard for dimension 3.",
        "published": "2008-11-17T15:30:44Z",
        "link": "http://arxiv.org/abs/0811.2731v2",
        "categories": [
            "cs.DM",
            "cs.CC"
        ]
    },
    {
        "title": "Geometric properties of satisfying assignments of random   $ε$-1-in-k SAT",
        "authors": [
            "Gabriel Istrate"
        ],
        "summary": "We study the geometric structure of the set of solutions of random $\\epsilon$-1-in-k SAT problem. For $l\\geq 1$, two satisfying assignments $A$ and $B$ are $l$-connected if there exists a sequence of satisfying assignments connecting them by changing at most $l$ bits at a time.   We first prove that w.h.p. two assignments of a random $\\epsilon$-1-in-$k$ SAT instance are $O(\\log n)$-connected, conditional on being satisfying assignments. Also, there exists $\\epsilon_{0}\\in (0,\\frac{1}{k-2})$ such that w.h.p. no two satisfying assignments at distance at least $\\epsilon_{0}\\cdot n$ form a \"hole\" in the set of assignments. We believe that this is true for all $\\epsilon >0$, and thus satisfying assignments of a random 1-in-$k$ SAT instance form a single cluster.",
        "published": "2008-11-19T15:35:42Z",
        "link": "http://arxiv.org/abs/0811.3116v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2.2; G.2"
        ]
    },
    {
        "title": "An Almost Optimal Rank Bound for Depth-3 Identities",
        "authors": [
            "Nitin Saxena",
            "C. Seshadhri"
        ],
        "summary": "We show that the rank of a depth-3 circuit (over any field) that is simple, minimal and zero is at most k^3\\log d. The previous best rank bound known was 2^{O(k^2)}(\\log d)^{k-2} by Dvir and Shpilka (STOC 2005). This almost resolves the rank question first posed by Dvir and Shpilka (as we also provide a simple and minimal identity of rank \\Omega(k\\log d)).   Our rank bound significantly improves (dependence on k exponentially reduced) the best known deterministic black-box identity tests for depth-3 circuits by Karnin and Shpilka (CCC 2008). Our techniques also shed light on the factorization pattern of nonzero depth-3 circuits, most strikingly: the rank of linear factors of a simple, minimal and nonzero depth-3 circuit (over any field) is at most k^3\\log d.   The novel feature of this work is a new notion of maps between sets of linear forms, called \"ideal matchings\", used to study depth-3 circuits. We prove interesting structural results about depth-3 identities using these techniques. We believe that these can lead to the goal of a deterministic polynomial time identity test for these circuits.",
        "published": "2008-11-19T17:41:06Z",
        "link": "http://arxiv.org/abs/0811.3161v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Trading GRH for algebra: algorithms for factoring polynomials and   related structures",
        "authors": [
            "Gábor Ivanyos",
            "Marek Karpinski",
            "Lajos Rónyai",
            "Nitin Saxena"
        ],
        "summary": "In this paper we develop techniques that eliminate the need of the Generalized Riemann Hypothesis (GRH) from various (almost all) known results about deterministic polynomial factoring over finite fields. Our main result shows that given a polynomial f(x) of degree n over a finite field k, we can find in deterministic poly(n^{\\log n},\\log |k|) time \"either\" a nontrivial factor of f(x) \"or\" a nontrivial automorphism of k[x]/(f(x)) of order n. This main tool leads to various new GRH-free results, most striking of which are:   (1) Given a noncommutative algebra over a finite field, we can find a zero divisor in deterministic subexponential time.   (2) Given a positive integer r such that either 8|r or r has at least two distinct odd prime factors. There is a deterministic polynomial time algorithm to find a nontrivial factor of the r-th cyclotomic polynomial over a finite field.   In this paper, following the seminal work of Lenstra (1991) on constructing isomorphisms between finite fields, we further generalize classical Galois theory constructs like cyclotomic extensions, Kummer extensions, Teichmuller subgroups, to the case of commutative semisimple algebras with automorphisms. These generalized constructs help eliminate the dependence on GRH.",
        "published": "2008-11-19T17:57:25Z",
        "link": "http://arxiv.org/abs/0811.3165v2",
        "categories": [
            "cs.CC",
            "cs.SC"
        ]
    },
    {
        "title": "Quantum algorithms for highly non-linear Boolean functions",
        "authors": [
            "Martin Roetteler"
        ],
        "summary": "Attempts to separate the power of classical and quantum models of computation have a long history. The ultimate goal is to find exponential separations for computational problems. However, such separations do not come a dime a dozen: while there were some early successes in the form of hidden subgroup problems for abelian groups--which generalize Shor's factoring algorithm perhaps most faithfully--only for a handful of non-abelian groups efficient quantum algorithms were found. Recently, problems have gotten increased attention that seek to identify hidden sub-structures of other combinatorial and algebraic objects besides groups. In this paper we provide new examples for exponential separations by considering hidden shift problems that are defined for several classes of highly non-linear Boolean functions. These so-called bent functions arise in cryptography, where their property of having perfectly flat Fourier spectra on the Boolean hypercube gives them resilience against certain types of attack. We present new quantum algorithms that solve the hidden shift problems for several well-known classes of bent functions in polynomial time and with a constant number of queries, while the classical query complexity is shown to be exponential. Our approach uses a technique that exploits the duality between bent functions and their Fourier transforms.",
        "published": "2008-11-19T21:13:00Z",
        "link": "http://arxiv.org/abs/0811.3208v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Revisiting Norm Estimation in Data Streams",
        "authors": [
            "Daniel M. Kane",
            "Jelani Nelson",
            "David P. Woodruff"
        ],
        "summary": "The problem of estimating the pth moment F_p (p nonnegative and real) in data streams is as follows. There is a vector x which starts at 0, and many updates of the form x_i <-- x_i + v come sequentially in a stream. The algorithm also receives an error parameter 0 < eps < 1. The goal is then to output an approximation with relative error at most eps to F_p = ||x||_p^p.   Previously, it was known that polylogarithmic space (in the vector length n) was achievable if and only if p <= 2. We make several new contributions in this regime, including:   (*) An optimal space algorithm for 0 < p < 2, which, unlike previous algorithms which had optimal dependence on 1/eps but sub-optimal dependence on n, does not rely on a generic pseudorandom generator.   (*) A near-optimal space algorithm for p = 0 with optimal update and query time.   (*) A near-optimal space algorithm for the \"distinct elements\" problem (p = 0 and all updates have v = 1) with optimal update and query time.   (*) Improved L_2 --> L_2 dimensionality reduction in a stream.   (*) New 1-pass lower bounds to show optimality and near-optimality of our algorithms, as well as of some previous algorithms (the \"AMS sketch\" for p = 2, and the L_1-difference algorithm of Feigenbaum et al.).   As corollaries of our work, we also obtain a few separations in the complexity of moment estimation problems: F_0 in 1 pass vs. 2 passes, p = 0 vs. p > 0, and F_0 with strictly positive updates vs. arbitrary updates.",
        "published": "2008-11-21T22:55:07Z",
        "link": "http://arxiv.org/abs/0811.3648v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Highly Undecidable Problems about Recognizability by Tiling Systems",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "Altenbernd, Thomas and W\\\"ohrle have considered acceptance of languages of infinite two-dimensional words (infinite pictures) by finite tiling systems, with usual acceptance conditions, such as the B\\\"uchi and Muller ones [1]. It was proved in [9] that it is undecidable whether a B\\\"uchi-recognizable language of infinite pictures is E-recognizable (respectively, A-recognizable). We show here that these two decision problems are actually $\\Pi_2^1$-complete, hence located at the second level of the analytical hierarchy, and \"highly undecidable\". We give the exact degree of numerous other undecidable problems for B\\\"uchi-recognizable languages of infinite pictures. In particular, the non-emptiness and the infiniteness problems are $\\Sigma^1_1$-complete, and the universality problem, the inclusion problem, the equivalence problem, the determinizability problem, the complementability problem, are all $\\Pi^1_2$-complete. It is also $\\Pi^1_2$-complete to determine whether a given B\\\"uchi recognizable language of infinite pictures can be accepted row by row using an automaton model over ordinal words of length $\\omega^2$.",
        "published": "2008-11-22T17:41:28Z",
        "link": "http://arxiv.org/abs/0811.3704v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "Communication Efficiency in Self-stabilizing Silent Protocols",
        "authors": [
            "Stéphane Devismes",
            "Toshimitsu Masuzawa",
            "Sébastien Tixeuil"
        ],
        "summary": "Self-stabilization is a general paradigm to provide forward recovery capabilities to distributed systems and networks. Intuitively, a protocol is self-stabilizing if it is able to recover without external intervention from any catastrophic transient failure. In this paper, our focus is to lower the communication complexity of self-stabilizing protocols \\emph{below} the need of checking every neighbor forever. In more details, the contribution of the paper is threefold: (i) We provide new complexity measures for communication efficiency of self-stabilizing protocols, especially in the stabilized phase or when there are no faults, (ii) On the negative side, we show that for non-trivial problems such as coloring, maximal matching, and maximal independent set, it is impossible to get (deterministic or probabilistic) self-stabilizing solutions where every participant communicates with less than every neighbor in the stabilized phase, and (iii) On the positive side, we present protocols for coloring, maximal matching, and maximal independent set such that a fraction of the participants communicates with exactly one neighbor in the stabilized phase.",
        "published": "2008-11-23T17:29:25Z",
        "link": "http://arxiv.org/abs/0811.3760v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Real Computation with Least Discrete Advice: A Complexity Theory of   Nonuniform Computability",
        "authors": [
            "Martin Ziegler"
        ],
        "summary": "It is folklore particularly in numerical and computer sciences that, instead of solving some general problem f:A->B, additional structural information about the input x in A (that is any kind of promise that x belongs to a certain subset A' of A) should be taken advantage of. Some examples from real number computation show that such discrete advice can even make the difference between computability and uncomputability. We turn this into a both topological and combinatorial complexity theory of information, investigating for several practical problems how much advice is necessary and sufficient to render them computable.   Specifically, finding a nontrivial solution to a homogeneous linear equation A*x=0 for a given singular real NxN-matrix A is possible when knowing rank(A)=0,1,...,N-1; and we show this to be best possible. Similarly, diagonalizing (i.e. finding a BASIS of eigenvectors of) a given real symmetric NxN-matrix is possible when knowing the number of distinct eigenvalues: an integer between 1 and N (the latter corresponding to the nondegenerate case). And again we show that N-fold (i.e. roughly log N bits of) additional information is indeed necessary in order to render this problem (continuous and) computable; whereas for finding SOME SINGLE eigenvector of A, providing the truncated binary logarithm of the least-dimensional eigenspace of A--i.e. Theta(log N)-fold advice--is sufficient and optimal.",
        "published": "2008-11-24T00:06:40Z",
        "link": "http://arxiv.org/abs/0811.3782v5",
        "categories": [
            "cs.CC",
            "math.LO",
            "F.1.1; F.2.1; F.4.1; H.1.1; G.1.3"
        ]
    },
    {
        "title": "On the Complexity of Matroid Isomorphism Problem",
        "authors": [
            "Raghavendra Rao B. V.",
            "Jayalal M. N. Sarma"
        ],
        "summary": "We study the complexity of testing if two given matroids are isomorphic. The problem is easily seen to be in $\\Sigma_2^p$. In the case of linear matroids, which are represented over polynomially growing fields, we note that the problem is unlikely to be $\\Sigma_2^p$-complete and is $\\co\\NP$-hard. We show that when the rank of the matroid is bounded by a constant, linear matroid isomorphism, and matroid isomorphism are both polynomial time many-one equivalent to graph isomorphism. We give a polynomial time Turing reduction from graphic matroid isomorphism problem to the graph isomorphism problem. Using this, we are able to show that graphic matroid isomorphism testing for planar graphs can be done in deterministic polynomial time. We then give a polynomial time many-one reduction from bounded rank matroid isomorphism problem to graphic matroid isomorphism, thus showing that all the above problems are polynomial time equivalent. Further, for linear and graphic matroids, we prove that the automorphism problem is polynomial time equivalent to the corresponding isomorphism problems. In addition, we give a polynomial time membership test algorithm for the automorphism group of a graphic matroid.",
        "published": "2008-11-24T14:19:02Z",
        "link": "http://arxiv.org/abs/0811.3859v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A polytime proof of correctness of the Rabin-Miller algorithm from   Fermat's little theorem",
        "authors": [
            "Grzegorz Herman",
            "Michael Soltys"
        ],
        "summary": "Although a deterministic polytime algorithm for primality testing is now known, the Rabin-Miller randomized test of primality continues being the most efficient and widely used algorithm.   We prove the correctness of the Rabin-Miller algorithm in the theory V1 for polynomial time reasoning, from Fermat's little theorem. This is interesting because the Rabin-Miller algorithm is a polytime randomized algorithm, which runs in the class RP (i.e., the class of polytime Monte-Carlo algorithms), with a sampling space exponential in the length of the binary encoding of the input number. (The class RP contains polytime P.) However, we show how to express the correctness in the language of V1, and we also show that we can prove the formula expressing correctness with polytime reasoning from Fermat's Little theorem, which is generally expected to be independent of V1.   Our proof is also conceptually very basic in the sense that we use the extended Euclid's algorithm, for computing greatest common divisors, as the main workhorse of the proof. For example, we make do without proving the Chinese Reminder theorem, which is used in the standard proofs.",
        "published": "2008-11-24T20:34:32Z",
        "link": "http://arxiv.org/abs/0811.3959v1",
        "categories": [
            "cs.CC",
            "cs.CR"
        ]
    },
    {
        "title": "Extractors and an efficient variant of Muchnik's theorem",
        "authors": [
            "Daniil Musatov"
        ],
        "summary": "Muchnik's theorem about simple conditional descriprion states that for all words $a$ and $b$ there exists a short program $p$ transforming $a$ to $b$ that has the least possible length and is simple conditional on $b$. This paper presents a new proof of this theorem, based on extractors. Employing the extractor technique, two new versions of Muchnik's theorem for space- and time-bounded Kolmogorov complexity are proven.",
        "published": "2008-11-24T20:49:50Z",
        "link": "http://arxiv.org/abs/0811.3958v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "How robust is quicksort average complexity?",
        "authors": [
            "Suman Kumar Sourabh",
            "Soubhik Chakraborty"
        ],
        "summary": "The paper questions the robustness of average case time complexity of the fast and popular quicksort algorithm. Among the six standard probability distributions examined in the paper, only continuous uniform, exponential and standard normal are supporting it whereas the others are supporting the worst case complexity measure. To the question -why are we getting the worst case complexity measure each time the average case measure is discredited? -- one logical answer is average case complexity under the universal distribution equals worst case complexity. This answer, which is hard to challenge, however gives no idea as to which of the standard probability distributions come under the umbrella of universality. The morale is that average case complexity measures, in cases where they are different from those in worst case, should be deemed as robust provided only they get the support from at least the standard probability distributions, both discrete and continuous. Regretfully, this is not the case with quicksort.",
        "published": "2008-11-26T17:23:22Z",
        "link": "http://arxiv.org/abs/0811.4376v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Dichotomy Results for Fixed Point Counting in Boolean Dynamical Systems",
        "authors": [
            "Christopher M. Homan",
            "Sven Kosub"
        ],
        "summary": "We present dichotomy theorems regarding the computational complexity of counting fixed points in boolean (discrete) dynamical systems, i.e., finite discrete dynamical systems over the domain {0,1}. For a class F of boolean functions and a class G of graphs, an (F,G)-system is a boolean dynamical system with local transitions functions lying in F and graphs in G. We show that, if local transition functions are given by lookup tables, then the following complexity classification holds: Let F be a class of boolean functions closed under superposition and let G be a graph class closed under taking minors. If F contains all min-functions, all max-functions, or all self-dual and monotone functions, and G contains all planar graphs, then it is #P-complete to compute the number of fixed points in an (F,G)-system; otherwise it is computable in polynomial time. We also prove a dichotomy theorem for the case that local transition functions are given by formulas (over logical bases). This theorem has a significantly more complicated structure than the theorem for lookup tables. A corresponding theorem for boolean circuits coincides with the theorem for formulas.",
        "published": "2008-12-01T12:56:42Z",
        "link": "http://arxiv.org/abs/0812.0283v1",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "cs.DM",
            "nlin.AO",
            "nlin.CG",
            "F.2.2; F.1.1; F.1.3"
        ]
    },
    {
        "title": "Hierarchy and equivalence of multi-letter quantum finite automata",
        "authors": [
            "Daowen Qiu",
            "Sheng Yu"
        ],
        "summary": "Multi-letter {\\it quantum finite automata} (QFAs) were a new one-way QFA model proposed recently by Belovs, Rosmanis, and Smotrovs (LNCS, Vol. 4588, Springer, Berlin, 2007, pp. 60-71), and they showed that multi-letter QFAs can accept with no error some regular languages ($(a+b)^{*}b$) that are unacceptable by the one-way QFAs. In this paper, we continue to study multi-letter QFAs. We mainly focus on two issues: (1) we show that $(k+1)$-letter QFAs are computationally more powerful than $k$-letter QFAs, that is, $(k+1)$-letter QFAs can accept some regular languages that are unacceptable by any $k$-letter QFA. A comparison with the one-way QFAs is made by some examples; (2) we prove that a $k_{1}$-letter QFA ${\\cal A}_1$ and another $k_{2}$-letter QFA ${\\cal A}_2$ are equivalent if and only if they are $(n_{1}+n_{2})^{4}+k-1$-equivalent, and the time complexity of determining the equivalence of two multi-letter QFAs using this method is $O(n^{12}+k^{2}n^{4}+kn^{8})$, where $n_{1}$ and $n_{2}$ are the numbers of states of ${\\cal A}_{1}$ and ${\\cal A}_{2}$, respectively, and $k=\\max(k_{1},k_{2})$. Some other issues are addressed for further consideration.",
        "published": "2008-12-04T03:10:29Z",
        "link": "http://arxiv.org/abs/0812.0852v3",
        "categories": [
            "cs.CC",
            "F.1.1; F.1.2; F.4.3"
        ]
    },
    {
        "title": "Decision trees are PAC-learnable from most product distributions: a   smoothed analysis",
        "authors": [
            "Adam Tauman Kalai",
            "Shang-Hua Teng"
        ],
        "summary": "We consider the problem of PAC-learning decision trees, i.e., learning a decision tree over the n-dimensional hypercube from independent random labeled examples. Despite significant effort, no polynomial-time algorithm is known for learning polynomial-sized decision trees (even trees of any super-constant size), even when examples are assumed to be drawn from the uniform distribution on {0,1}^n. We give an algorithm that learns arbitrary polynomial-sized decision trees for {\\em most product distributions}. In particular, consider a random product distribution where the bias of each bit is chosen independently and uniformly from, say, [.49,.51]. Then with high probability over the parameters of the product distribution and the random examples drawn from it, the algorithm will learn any tree. More generally, in the spirit of smoothed analysis, we consider an arbitrary product distribution whose parameters are specified only up to a [-c,c] accuracy (perturbation), for an arbitrarily small positive constant c.",
        "published": "2008-12-04T13:34:26Z",
        "link": "http://arxiv.org/abs/0812.0933v1",
        "categories": [
            "cs.LG",
            "cs.CC"
        ]
    },
    {
        "title": "Decidability of the Equivalence of Multi-Letter Quantum Finite Automata",
        "authors": [
            "Daowen Qiu",
            "Xiangfu Zou",
            "Lvzhou Li",
            "Paulo Mateus"
        ],
        "summary": "Multi-letter {\\it quantum finite automata} (QFAs) were a quantum variant of classical {\\it one-way multi-head finite automata} (J. Hromkovi\\v{c}, Acta Informatica 19 (1983) 377-384), and it has been shown that this new one-way QFAs (multi-letter QFAs) can accept with no error some regular languages $(a+b)^{*}b$ that are unacceptable by the previous one-way QFAs. In this paper, we study the decidability of the equivalence of multi-letter QFAs, and the main technical contributions are as follows: (1) We show that any two automata, a $k_{1}$-letter QFA ${\\cal A}_1$ and a $k_{2}$-letter QFA ${\\cal A}_2$, over the same input alphabet $\\Sigma$ are equivalent if and only if they are $(n^2m^{k-1}-m^{k-1}+k)$-equivalent, where $m=|\\Sigma|$ is the cardinality of $\\Sigma$, $k=\\max(k_{1},k_{2})$, and $n=n_{1}+n_{2}$, with $n_{1}$ and $n_{2}$ being the numbers of states of ${\\cal A}_{1}$ and ${\\cal A}_{2}$, respectively. When $k=1$, we obtain the decidability of equivalence of measure-once QFAs in the literature. It is worth mentioning that our technical method is essentially different from that for the decidability of the case of single input alphabet (i.e., $m=1$). (2) However, if we determine the equivalence of multi-letter QFAs by checking all strings of length not more than $ n^2m^{k-1}-m^{k-1}+k$, then the worst time complexity is exponential, i.e., $O(n^6m^{n^2m^{k-1}-m^{k-1}+2k-1})$. Therefore, we design a polynomial-time $O(m^{2k-1}n^{8}+km^kn^{6})$ algorithm for determining the equivalence of any two multi-letter QFAs. Here, the time complexity is concerning the number of states in the multi-letter QFAs, and $k$ is thought of as a constant.",
        "published": "2008-12-05T00:53:20Z",
        "link": "http://arxiv.org/abs/0812.1061v6",
        "categories": [
            "cs.FL",
            "cs.CC",
            "F.1.1; F.1.2; F.4.3"
        ]
    },
    {
        "title": "Polynomial hierarchy, Betti numbers and a real analogue of Toda's   theorem",
        "authors": [
            "Saugata Basu",
            "Thierry Zell"
        ],
        "summary": "Toda proved in 1989 that the (discrete) polynomial time hierarchy, $\\mathbf{PH}$, is contained in the class $\\mathbf{P}^{#\\mathbf{P}}$, namely the class of languages that can be decided by a Turing machine in polynomial time given access to an oracle with the power to compute a function in the counting complexity class $#\\mathbf{P}$. This result which illustrates the power of counting is considered to be a seminal result in computational complexity theory. An analogous result in the complexity theory over the reals (in the sense of Blum-Shub-Smale real machines) has been missing so far. In this paper we formulate and prove a real analogue of Toda's theorem. Unlike Toda's proof in the discrete case, which relied on sophisticated combinatorial arguments, our proof is topological in nature. As a consequence of our techniques we are also able to relate the computational hardness of two extremely well-studied problems in algorithmic semi-algebraic geometry -- namely the problem of deciding sentences in the first order theory of the reals with a constant number of quantifier alternations, and that of computing Betti numbers of semi-algebraic sets. We obtain a polynomial time reduction of the compact version of the first problem to the second. This latter result might be of independent interest to researchers in algorithmic semi-algebraic geometry.",
        "published": "2008-12-05T19:09:27Z",
        "link": "http://arxiv.org/abs/0812.1200v3",
        "categories": [
            "cs.CC",
            "math.AT",
            "math.CO",
            "math.LO"
        ]
    },
    {
        "title": "Unary finite automata vs. arithmetic progressions",
        "authors": [
            "Anthony Widjaja To"
        ],
        "summary": "We point out a subtle error in the proof of Chrobak's theorem that every unary NFA can be represented as a union of arithmetic progressions that is at most quadratically large. We propose a correction for this and show how Martinez's polynomial time algorithm, which realizes Chrobak's theorem, can be made correct accordingly. We also show that Martinez's algorithm cannot be improved to have logarithmic space, unless L = NL.",
        "published": "2008-12-06T14:18:05Z",
        "link": "http://arxiv.org/abs/0812.1291v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "An Extension of the Permutation Group Enumeration Technique (Collapse of   the Polynomial Hierarchy: $\\mathbf{NP = P}$)",
        "authors": [
            "Javaid Aslam"
        ],
        "summary": "The distinguishing result of this paper is a $\\mathbf{P}$-time enumerable partition of all the potential perfect matchings in a bipartite graph. This partition is a set of equivalence classes induced by the missing edges in the potential perfect matchings.   We capture the behavior of these missing edges in a polynomially bounded representation of the exponentially many perfect matchings by a graph theoretic structure, called MinSet Sequence, where MinSet is a P-time enumerable structure derived from a graph theoretic counterpart of a generating set of the symmetric group. This leads to a polynomially bounded generating set of all the classes, enabling the enumeration of perfect matchings in polynomial time. The sequential time complexity of this $\\mathbf{\\#P}$-complete problem is shown to be $O(n^{45}\\log n)$.   And thus we prove a result even more surprising than $\\mathbf{NP = P}$, that is, $\\mathbf{\\#P}=\\mathbf{FP}$, where $\\mathbf{FP}$ is the class of functions, $f: \\{0, 1\\}^* \\rightarrow \\mathbb{N} $, computable in polynomial time on a deterministic model of computation.",
        "published": "2008-12-07T19:47:28Z",
        "link": "http://arxiv.org/abs/0812.1385v26",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.2.0"
        ]
    },
    {
        "title": "Scarf is Ppad-Complete",
        "authors": [
            "Shiva Kintali"
        ],
        "summary": "Scarf's lemma is one of the fundamental results in combinatorics, originally introduced to study the core of an N-person game. Over the last four decades, the usefulness of Scarf's lemma has been demonstrated in several important combinatorial problems seeking \"stable\" solutions. However, the complexity of the computational version of Scarf's lemma (SCARF) remained open. In this paper, we prove that SCARF is complete for the complexity class PPAD. This proves that SCARF is as hard as the computational versions of Brouwer's fixed point theorem and Sperner's lemma. Hence, there is no polynomial-time algorithm for SCARF unless PPAD \\subseteq P. We also show that fractional stable paths problem and finding strong fractional kernels in digraphs are PPAD-hard.",
        "published": "2008-12-09T01:07:25Z",
        "link": "http://arxiv.org/abs/0812.1601v4",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Dynamic Complexity of Formal Languages",
        "authors": [
            "Wouter Gelade",
            "Marcel Marquardt",
            "Thomas Schwentick"
        ],
        "summary": "The paper investigates the power of the dynamic complexity classes DynFO, DynQF and DynPROP over string languages. The latter two classes contain problems that can be maintained using quantifier-free first-order updates, with and without auxiliary functions, respectively. It is shown that the languages maintainable in DynPROP exactly are the regular languages, even when allowing arbitrary precomputation. This enables lower bounds for DynPROP and separates DynPROP from DynQF and DynFO. Further, it is shown that any context-free language can be maintained in DynFO and a number of specific context-free languages, for example all Dyck-languages, are maintainable in DynQF. Furthermore, the dynamic complexity of regular tree languages is investigated and some results concerning arbitrary structures are obtained: there exist first-order definable properties which are not maintainable in DynPROP. On the other hand any existential first-order property can be maintained in DynQF when allowing precomputation.",
        "published": "2008-12-10T14:13:57Z",
        "link": "http://arxiv.org/abs/0812.1915v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "A Factor 3/2 Approximation for Generalized Steiner Tree Problem with   Distances One and Two",
        "authors": [
            "Piotr Berman",
            "Marek Karpinski",
            "Alex Zelikovsky"
        ],
        "summary": "We design a 3/2 approximation algorithm for the Generalized Steiner Tree problem (GST) in metrics with distances 1 and 2. This is the first polynomial time approximation algorithm for a wide class of non-geometric metric GST instances with approximation factor below 2.",
        "published": "2008-12-11T12:50:54Z",
        "link": "http://arxiv.org/abs/0812.2137v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Efficient Isomorphism Testing for a Class of Group Extensions",
        "authors": [
            "Francois Le Gall"
        ],
        "summary": "The group isomorphism problem asks whether two given groups are isomorphic or not. Whereas the case where both groups are abelian is well understood and can be solved efficiently, very little is known about the complexity of isomorphism testing for nonabelian groups. In this paper we study this problem for a class of groups corresponding to one of the simplest ways of constructing nonabelian groups from abelian groups: the groups that are extensions of an abelian group A by a cyclic group of order m. We present an efficient algorithm solving the group isomorphism problem for all the groups of this class such that the order of A is coprime with m. More precisely, our algorithm runs in time almost linear in the orders of the input groups and works in the general setting where the groups are given as black-boxes.",
        "published": "2008-12-12T09:39:02Z",
        "link": "http://arxiv.org/abs/0812.2298v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "math.GR",
            "quant-ph"
        ]
    },
    {
        "title": "Approximating the least hypervolume contributor: NP-hard in general, but   fast in practice",
        "authors": [
            "Karl Bringmann",
            "Tobias Friedrich"
        ],
        "summary": "The hypervolume indicator is an increasingly popular set measure to compare the quality of two Pareto sets. The basic ingredient of most hypervolume indicator based optimization algorithms is the calculation of the hypervolume contribution of single solutions regarding a Pareto set. We show that exact calculation of the hypervolume contribution is #P-hard while its approximation is NP-hard. The same holds for the calculation of the minimal contribution. We also prove that it is NP-hard to decide whether a solution has the least hypervolume contribution. Even deciding whether the contribution of a solution is at most $(1+\\eps)$ times the minimal contribution is NP-hard. This implies that it is neither possible to efficiently find the least contributing solution (unless $P = NP$) nor to approximate it (unless $NP = BPP$).   Nevertheless, in the second part of the paper we present a fast approximation algorithm for this problem. We prove that for arbitrarily given $\\eps,\\delta>0$ it calculates a solution with contribution at most $(1+\\eps)$ times the minimal contribution with probability at least $(1-\\delta)$. Though it cannot run in polynomial time for all instances, it performs extremely fast on various benchmark datasets. The algorithm solves very large problem instances which are intractable for exact algorithms (e.g., 10000 solutions in 100 dimensions) within a few seconds.",
        "published": "2008-12-14T13:57:10Z",
        "link": "http://arxiv.org/abs/0812.2636v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Two conjectures such that the proof of any one of them will lead to the   proof that P = NP",
        "authors": [
            "Malay Dutta"
        ],
        "summary": "In this paper we define a construct called a time-graph. A complete time-graph of order n is the cartesian product of a complete graph with n vertices and a linear graph with n vertices. A time-graph of order n is given by a subset of the set of edges E(n) of such a graph. The notion of a hamiltonian time-graph is defined in a natural way and we define the Hamiltonian time-graph problem (HAMTG) as : Given a time-graph is it hamiltonian ? We show that the Hamiltonian path problem (HAMP) can be transformed to HAMTG in polynomial time. We then define certain vector spaces of functions from E(n) and E(n)xE(n) to B = {0,1}, the field of two elements and derive certain properties of these spaces. We give two conjectures about these spaces and prove that if any one of these conjectures is true, we get a polynomial time algorithm for the Hamiltonian path problem. Since the Hamiltonian path problem is NP-complete we obtain the proof of P = NP provided any one of the two conjectures is true.",
        "published": "2008-12-17T07:03:53Z",
        "link": "http://arxiv.org/abs/0812.3214v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A randomized polynomial-time algorithm for the Spanning Hypertree   Problem on 3-uniform hypergraphs",
        "authors": [
            "Sergio Caracciolo",
            "Gregor Masbaum",
            "Alan D. Sokal",
            "Andrea Sportiello"
        ],
        "summary": "Consider the problem of determining whether there exists a spanning hypertree in a given k-uniform hypergraph. This problem is trivially in P for k=2, and is NP-complete for k>= 4, whereas for k=3, there exists a polynomial-time algorithm based on Lovasz' theory of polymatroid matching. Here we give a completely different, randomized polynomial-time algorithm in the case k=3. The main ingredients are a Pfaffian formula by Vaintrob and one of the authors (G.M.) for a polynomial that enumerates spanning hypertrees with some signs, and a lemma on the number of roots of polynomials over a finite field.",
        "published": "2008-12-18T17:17:33Z",
        "link": "http://arxiv.org/abs/0812.3593v1",
        "categories": [
            "cs.CC",
            "math.CO"
        ]
    },
    {
        "title": "Graph Field Automata",
        "authors": [
            "Joshua Herman",
            "Keith David Pedersen"
        ],
        "summary": "The Graph Automata have been the paradigm in the expression of utilizing Graphs as a language. Matrix Graph grammars \\cite{Pedro} are an algebratization of graph rewriting systems. Here we present the dual of this formalizm which some extensions which we term Graph Field Automata The advantage to this approach is a framework for expressing machines that can use Matrix Graph Grammars.",
        "published": "2008-12-22T00:26:29Z",
        "link": "http://arxiv.org/abs/0812.4009v21",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "The Complexity of Weighted Boolean #CSP with Mixed Signs",
        "authors": [
            "Andrei Bulatov",
            "Martin Dyer",
            "Leslie Ann Goldberg",
            "Markus Jalsenius",
            "David Richerby"
        ],
        "summary": "We give a complexity dichotomy for the problem of computing the partition function of a weighted Boolean constraint satisfaction problem. Such a problem is parameterized by a set of rational-valued functions, which generalize constraints. Each function assigns a weight to every assignment to a set of Boolean variables. Our dichotomy extends previous work in which the weight functions were restricted to being non-negative. We represent a weight function as a product of the form (-1)^s g, where the polynomial s determines the sign of the weight and the non-negative function g determines its magnitude. We show that the problem of computing the partition function (the sum of the weights of all possible variable assignments) is in polynomial time if either every weight function can be defined by a \"pure affine\" magnitude with a quadratic sign polynomial or every function can be defined by a magnitude of \"product type\" with a linear sign polynomial. In all other cases, computing the partition function is FP^#P-complete.",
        "published": "2008-12-22T12:56:42Z",
        "link": "http://arxiv.org/abs/0812.4171v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2.2; F.4.1; G.2.1"
        ]
    },
    {
        "title": "On Some Classes of Functions and Hypercubes",
        "authors": [
            "Dimiter Stoichkov Kovachev"
        ],
        "summary": "In this paper, some classes of discrete functions of $k$-valued logic are considered, that depend on sets of their variables in a particular way. Obtained results allow to \"construct\" these functions and to present them in their tabular, analytical or matrix form, that is, as hypercubes, and in particular Latin hypercubes. Results connected with identifying of variables of some classes of functions are obtained.",
        "published": "2008-12-23T11:00:10Z",
        "link": "http://arxiv.org/abs/0812.4367v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "G.2.0"
        ]
    },
    {
        "title": "Church, Cardinal and Ordinal Representations of Integers and Kolmogorov   complexity",
        "authors": [
            "Marie Ferbus-Zanda",
            "Serge Grigorieff"
        ],
        "summary": "We consider classical representations of integers: Church's function iterators, cardinal equivalence classes of sets, ordinal equivalence classes of totally ordered sets. Since programs do not work on abstract entities and require formal representations of objects, we effectivize these abstract notions in order to allow them to be computed by programs. To any such effectivized representation is then associated a notion of Kolmogorov complexity. We prove that these Kolmogorov complexities form a strict hierarchy which coincides with that obtained by relativization to jump oracles and/or allowance of infinite computations.",
        "published": "2008-01-02T08:35:27Z",
        "link": "http://arxiv.org/abs/0801.0349v1",
        "categories": [
            "math.LO",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Kolmogorov complexities Kmax, Kmin on computable partially ordered sets",
        "authors": [
            "Marie Ferbus-Zanda",
            "Serge Grigorieff"
        ],
        "summary": "We introduce a machine free mathematical framework to get a natural formalization of some general notions of infinite computation in the context of Kolmogorov complexity. Namely, the classes Max^{X\\to D}_{PR} and Max^{X\\to D}_{Rec} of functions X \\to D which are pointwise maximum of partial or total computable sequences of functions where D = (D,<) is some computable partially ordered set. The enumeration theorem and the invariance theorem always hold for Max^{X\\to D}_{PR}, leading to a variant KD;max of Kolmogorov complexity. We characterize the orders D such that the enumeration theorem (resp. the invariance theorem) also holds for Max^{X\\to D}_{Rec} . It turns out that Max^{X\\to D}_{Rec} may satisfy the invariance theorem but not the enumeration theorem. Also, when Max^{X\\to D}_{Rec} satisfies the invariance theorem then the Kolmogorov complexities associated to Max^{X\\to D}_{Rec} and Max^{X\\to D}_{PR} are equal (up to a constant).   Letting K^D_{min} = K^{D^{rev}}_{max}, where D^{rev} is the reverse order, we prove that either K^D_{min} =_{ct} K^D_{max} =_{ct} K^D (=_{ct} is equality up to a constant) or K^D_{min}, K^D_{max} are <=_{ct} incomparable and <_{ct} K^D and >_{ct} K^{0',D}. We characterize the orders leading to each case. We also show that K^D_{min}, K^D_{max} cannot be both much smaller than K^D at any point.   These results are proved in a more general setting with two orders on D, one extending the other.",
        "published": "2008-01-02T08:36:29Z",
        "link": "http://arxiv.org/abs/0801.0351v1",
        "categories": [
            "math.LO",
            "cs.LO"
        ]
    },
    {
        "title": "Topology and Ambiguity in Omega Context Free Languages",
        "authors": [
            "Olivier Finkel",
            "Pierre Simonnet"
        ],
        "summary": "We study the links between the topological complexity of an omega context free language and its degree of ambiguity. In particular, using known facts from classical descriptive set theory, we prove that non Borel omega context free languages which are recognized by B\\\"uchi pushdown automata have a maximum degree of ambiguity. This result implies that degrees of ambiguity are really not preserved by the operation of taking the omega power of a finitary context free language. We prove also that taking the adherence or the delta-limit of a finitary language preserves neither unambiguity nor inherent ambiguity. On the other side we show that methods used in the study of omega context free languages can also be applied to study the notion of ambiguity in infinitary rational relations accepted by B\\\"uchi 2-tape automata and we get first results in that direction.",
        "published": "2008-01-03T14:47:36Z",
        "link": "http://arxiv.org/abs/0801.0533v1",
        "categories": [
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "On the Length of the Wadge Hierarchy of Omega Context Free Languages",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We prove in this paper that the length of the Wadge hierarchy of omega context free languages is greater than the Cantor ordinal epsilon_omega, which is the omega-th fixed point of the ordinal exponentiation of base omega. The same result holds for the conciliating Wadge hierarchy, defined by J. Duparc, of infinitary context free languages, studied by D. Beauquier. We show also that there exist some omega context free languages which are Sigma^0_omega-complete Borel sets, improving previous results on omega context free languages and the Borel hierarchy.",
        "published": "2008-01-03T14:48:48Z",
        "link": "http://arxiv.org/abs/0801.0534v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.GT",
            "math.LO"
        ]
    },
    {
        "title": "An omega-Power of a Finitary Language Which is a Borel Set of Infinite   Rank",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "Omega-powers of finitary languages are omega languages in the form V^omega, where V is a finitary language over a finite alphabet X. Since the set of infinite words over X can be equipped with the usual Cantor topology, the question of the topological complexity of omega-powers naturally arises and has been raised by Niwinski, by Simonnet, and by Staiger. It has been recently proved that for each integer n > 0, there exist some omega-powers of context free languages which are Pi^0_n-complete Borel sets, and that there exists a context free language L such that L^omega is analytic but not Borel. But the question was still open whether there exists a finitary language V such that V^omega is a Borel set of infinite rank. We answer this question in this paper, giving an example of a finitary language whose omega-power is Borel of infinite rank.",
        "published": "2008-01-03T14:49:23Z",
        "link": "http://arxiv.org/abs/0801.0535v1",
        "categories": [
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "On Infinite Real Trace Rational Languages of Maximum Topological   Complexity",
        "authors": [
            "Olivier Finkel",
            "Jean-Pierre Ressayre",
            "Pierre Simonnet"
        ],
        "summary": "We consider the set of infinite real traces, over a dependence alphabet (Gamma, D) with no isolated letter, equipped with the topology induced by the prefix metric. We then prove that all rational languages of infinite real traces are analytic sets and that there exist some rational languages of infinite real traces which are analytic but non Borel sets, and even Sigma^1_1-complete, hence of maximum possible topological complexity.",
        "published": "2008-01-03T14:51:16Z",
        "link": "http://arxiv.org/abs/0801.0537v1",
        "categories": [
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "A logical analysis of entanglement and separability in quantum   higher-order functions",
        "authors": [
            "F. Prost",
            "C. Zerrari"
        ],
        "summary": "We present a logical separability analysis for a functional quantum computation language. This logic is inspired by previous works on logical analysis of aliasing for imperative functional programs. Both analyses share similarities notably because they are highly non-compositional. Quantum setting is harder to deal with since it introduces non determinism and thus considerably modifies semantics and validity of logical assertions. This logic is the first proposal of entanglement/separability analysis dealing with a functional quantum programming language with higher-order functions.",
        "published": "2008-01-04T10:12:59Z",
        "link": "http://arxiv.org/abs/0801.0649v1",
        "categories": [
            "cs.LO",
            "F.3.1; D.2.1; D.2.4; D.3.1"
        ]
    },
    {
        "title": "Finite-state concurrent programs can be expressed pairwise",
        "authors": [
            "Paul C. Attie"
        ],
        "summary": "We present a \\emph{pairwise normal form} for finite-state shared memory concurrent programs: all variables are shared between exactly two processes, and the guards on transitions are conjunctions of conditions over this pairwise shared state. This representation has been used to efficiently (in polynomial time) synthesize and model-check correctness properties of concurrent programs. Our main result is that any finite state concurrent program can be transformed into pairwise normal form. Specifically, if $Q$ is an arbitrary finite-state shared memory concurrent program, then there exists a finite-state shared memory concurrent program $P$ expressed in pairwise normal form such that $P$ is strongly bisimilar to $Q$. Our result is constructive: we give an algorithm for producing $P$, given $Q$.",
        "published": "2008-01-04T13:14:31Z",
        "link": "http://arxiv.org/abs/0801.0677v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A linear-non-linear model for a computational call-by-value lambda   calculus (extended abstract)",
        "authors": [
            "Peter Selinger",
            "Benoît Valiron"
        ],
        "summary": "We give a categorical semantics for a call-by-value linear lambda calculus. Such a lambda calculus was used by Selinger and Valiron as the backbone of a functional programming language for quantum computation. One feature of this lambda calculus is its linear type system, which includes a duplicability operator \"!\" as in linear logic. Another main feature is its call-by-value reduction strategy, together with a side-effect to model probabilistic measurements. The \"!\" operator gives rise to a comonad, as in the linear logic models of Seely, Bierman, and Benton. The side-effects give rise to a monad, as in Moggi's computational lambda calculus. It is this combination of a monad and a comonad that makes the present paper interesting. We show that our categorical semantics is sound and complete.",
        "published": "2008-01-05T15:21:17Z",
        "link": "http://arxiv.org/abs/0801.0813v1",
        "categories": [
            "cs.LO",
            "D.3.1; F.4.1"
        ]
    },
    {
        "title": "On the Refinement of Liveness Properties of Distributed Systems",
        "authors": [
            "Paul C. Attie"
        ],
        "summary": "We present a new approach for reasoning about liveness properties of distributed systems, represented as automata. Our approach is based on simulation relations, and requires reasoning only over finite execution fragments. Current simulation-relation based methods for reasoning about liveness properties of automata require reasoning over entire executions, since they involve a proof obligation of the form: if a concrete and abstract execution ``correspond'' via the simulation, and the concrete execution is live, then so is the abstract execution.   Our contribution consists of (1) a formalism for defining liveness properties, (2) a proof method for liveness properties based on that formalism, and (3) two expressive completeness results: firstly, our formalism can express any liveness property which satisfies a natural ``robustness'' condition, and secondly, our formalism can express any liveness property at all, provided that history variables can be used",
        "published": "2008-01-07T11:55:03Z",
        "link": "http://arxiv.org/abs/0801.0949v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Generative Unbinding of Names",
        "authors": [
            "Andrew M. Pitts",
            "Mark R. Shinwell"
        ],
        "summary": "This paper is concerned with the form of typed name binding used by the FreshML family of languages. Its characteristic feature is that a name binding is represented by an abstract (name,value)-pair that may only be deconstructed via the generation of fresh bound names. The paper proves a new result about what operations on names can co-exist with this construct. In FreshML the only observation one can make of names is to test whether or not they are equal. This restricted amount of observation was thought necessary to ensure that there is no observable difference between alpha-equivalent name binders. Yet from an algorithmic point of view it would be desirable to allow other operations and relations on names, such as a total ordering. This paper shows that, contrary to expectations, one may add not just ordering, but almost any relation or numerical function on names without disturbing the fundamental correctness result about this form of typed name binding (that object-level alpha-equivalence precisely corresponds to contextual equivalence at the programming meta-level), so long as one takes the state of dynamically created names into account.",
        "published": "2008-01-08T15:04:56Z",
        "link": "http://arxiv.org/abs/0801.1251v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.3.1; D.3.3; F.3.2"
        ]
    },
    {
        "title": "Linear Logic by Levels and Bounded Time Complexity",
        "authors": [
            "Patrick Baillot",
            "Damiano Mazza"
        ],
        "summary": "We give a new characterization of elementary and deterministic polynomial time computation in linear logic through the proofs-as-programs correspondence. Girard's seminal results, concerning elementary and light linear logic, achieve this characterization by enforcing a stratification principle on proofs, using the notion of depth in proof nets. Here, we propose a more general form of stratification, based on inducing levels in proof nets by means of indexes, which allows us to extend Girard's systems while keeping the same complexity properties. In particular, it turns out that Girard's systems can be recovered by forcing depth and level to coincide. A consequence of the higher flexibility of levels with respect to depth is the absence of boxes for handling the paragraph modality. We use this fact to propose a variant of our polytime system in which the paragraph modality is only allowed on atoms, and which may thus serve as a basis for developing lambda-calculus type assignment systems with more efficient typing algorithms than existing ones.",
        "published": "2008-01-08T15:08:20Z",
        "link": "http://arxiv.org/abs/0801.1253v3",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Almost 2-SAT is Fixed-Parameter Tractable",
        "authors": [
            "Igor Razgon",
            "Barry O'Sullivan"
        ],
        "summary": "We consider the following problem. Given a 2-CNF formula, is it possible to remove at most $k$ clauses so that the resulting 2-CNF formula is satisfiable? This problem is known to different research communities in Theoretical Computer Science under the names 'Almost 2-SAT', 'All-but-$k$ 2-SAT', '2-CNF deletion', '2-SAT deletion'. The status of fixed-parameter tractability of this problem is a long-standing open question in the area of Parameterized Complexity. We resolve this open question by proposing an algorithm which solves this problem in $O(15^k*k*m^3)$ and thus we show that this problem is fixed-parameter tractable.",
        "published": "2008-01-08T19:04:14Z",
        "link": "http://arxiv.org/abs/0801.1300v4",
        "categories": [
            "cs.DS",
            "cs.CG",
            "cs.LO"
        ]
    },
    {
        "title": "Alternating Hierarchies for Time-Space Tradeoffs",
        "authors": [
            "Chris Pollett",
            "Eric Miles"
        ],
        "summary": "Nepomnjascii's Theorem states that for all 0 <= \\epsilon < 1 and k > 0 the class of languages recognized in nondeterministic time n^k and space n^\\epsilon, NTISP[n^k, n^\\epsilon ], is contained in the linear time hierarchy. By considering restrictions on the size of the universal quantifiers in the linear time hierarchy, this paper refines Nepomnjascii's result to give a sub- hierarchy, Eu-LinH, of the linear time hierarchy that is contained in NP and which contains NTISP[n^k, n^\\epsilon ]. Hence, Eu-LinH contains NL and SC. This paper investigates basic structural properties of Eu-LinH. Then the relationships between Eu-LinH and the classes NL, SC, and NP are considered to see if they can shed light on the NL = NP or SC = NP questions. Finally, a new hierarchy, zeta -LinH, is defined to reduce the space requirements needed for the upper bound on Eu-LinH.",
        "published": "2008-01-08T19:59:05Z",
        "link": "http://arxiv.org/abs/0801.1307v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3"
        ]
    },
    {
        "title": "Synthesis of Large Dynamic Concurrent Programs from Dynamic   Specifications",
        "authors": [
            "Paul C. Attie"
        ],
        "summary": "We present a tractable method for synthesizing arbitrarily large concurrent programs, for a shared memory model with common hardware-available primitives such as atomic registers, compare-and-swap, load-linked/store conditional, etc. The programs we synthesize are dynamic: new processes can be created and added at run-time, and so our programs are not finite-state, in general. Nevertheless, we successfully exploit automatic synthesis and model-checking methods based on propositional temporal logic. Our method is algorithmically efficient, with complexity polynomial in the number of component processes (of the program) that are ``alive'' at any time. Our method does not explicitly construct the automata-theoretic product of all processes that are alive, thereby avoiding \\intr{state explosion}. Instead, for each pair of processes which interact, our method constructs an automata-theoretic product (\\intr{pair-machine}) which embodies all the possible interactions of these two processes. From each pair-machine, we can synthesize a correct \\intr{pair-program} which coordinates the two involved processes as needed. We allow such pair-programs to be added dynamically at run-time. They are then ``composed conjunctively'' with the currently alive pair-programs to re-synthesize the program as it results after addition of the new pair-program. We are thus able to add new behaviors, which result in new properties being satisfied, at run-time. We establish a ``large model'' theorem which shows that the synthesized large program inherits correctness properties from the pair-programs.",
        "published": "2008-01-10T21:27:42Z",
        "link": "http://arxiv.org/abs/0801.1687v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "An omega-power of a context-free language which is Borel above   Delta^0_omega",
        "authors": [
            "Jacques Duparc",
            "Olivier Finkel"
        ],
        "summary": "We use erasers-like basic operations on words to construct a set that is both Borel and above Delta^0_omega, built as a set V^\\omega where V is a language of finite words accepted by a pushdown automaton. In particular, this gives a first example of an omega-power of a context free language which is a Borel set of infinite rank.",
        "published": "2008-01-11T14:20:30Z",
        "link": "http://arxiv.org/abs/0801.1783v1",
        "categories": [
            "cs.CC",
            "cs.GT",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "An Application of the Feferman-Vaught Theorem to Automata and Logics   for<br> Words over an Infinite Alphabet",
        "authors": [
            "Alexis Bès"
        ],
        "summary": "We show that a special case of the Feferman-Vaught composition theorem gives rise to a natural notion of automata for finite words over an infinite alphabet, with good closure and decidability properties, as well as several logical characterizations. We also consider a slight extension of the Feferman-Vaught formalism which allows to express more relations between component values (such as equality), and prove related decidability results.   From this result we get new classes of decidable logics for words over an infinite alphabet.",
        "published": "2008-01-16T14:39:27Z",
        "link": "http://arxiv.org/abs/0801.2498v2",
        "categories": [
            "cs.LO",
            "F.1.1; F.4.1; F.4.3"
        ]
    },
    {
        "title": "Cut Elimination for a Logic with Generic Judgments and Induction",
        "authors": [
            "Alwen Tiu"
        ],
        "summary": "This paper presents a cut-elimination proof for the logic $LG^\\omega$, which is an extension of a proof system for encoding generic judgments, the logic $\\FOLDNb$ of Miller and Tiu, with an induction principle. The logic $LG^\\omega$, just as $\\FOLDNb$, features extensions of first-order intuitionistic logic with fixed points and a ``generic quantifier'', $\\nabla$, which is used to reason about the dynamics of bindings in object systems encoded in the logic. A previous attempt to extend $\\FOLDNb$ with an induction principle has been unsuccessful in modeling some behaviours of bindings in inductive specifications. It turns out that this problem can be solved by relaxing some restrictions on $\\nabla$, in particular by adding the axiom $B \\equiv \\nabla x. B$, where $x$ is not free in $B$. We show that by adopting the equivariance principle, the presentation of the extended logic can be much simplified. This paper contains the technical proofs for the results stated in \\cite{tiu07entcs}; readers are encouraged to consult \\cite{tiu07entcs} for motivations and examples for $LG^\\omega.$",
        "published": "2008-01-20T08:34:22Z",
        "link": "http://arxiv.org/abs/0801.3065v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "A hierarchy of behavioral equivalences in the $π$-calculus with noisy   channels",
        "authors": [
            "Yongzhi Cao"
        ],
        "summary": "The $\\pi$-calculus is a process algebra where agents interact by sending communication links to each other via noiseless communication channels. Taking into account the reality of noisy channels, an extension of the $\\pi$-calculus, called the $\\pi_N$-calculus, has been introduced recently. In this paper, we present an early transitional semantics of the $\\pi_N$-calculus, which is not a directly translated version of the late semantics of $\\pi_N$, and then extend six kinds of behavioral equivalences consisting of reduction bisimilarity, barbed bisimilarity, barbed equivalence, barbed congruence, bisimilarity, and full bisimilarity into the $\\pi_N$-calculus. Such behavioral equivalences are cast in a hierarchy, which is helpful to verify behavioral equivalence of two agents. In particular, we show that due to the noisy nature of channels, the coincidence of bisimilarity and barbed equivalence, as well as the coincidence of full bisimilarity and barbed congruence, in the $\\pi$-calculus does not hold in $\\pi_N$.",
        "published": "2008-01-21T00:42:52Z",
        "link": "http://arxiv.org/abs/0801.3117v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Modular Compilation of a Synchronous Language",
        "authors": [
            "Annie Ressouche",
            "Daniel Gaffé",
            "Valérie Roy"
        ],
        "summary": "Synchronous languages rely on formal methods to ease the development of applications in an efficient and reusable way. Formal methods have been advocated as a means of increasing the reliability of systems, especially those which are safety or business critical. It is still difficult to develop automatic specification and verification tools due to limitations like state explosion, undecidability, etc... In this work, we design a new specification model based on a reactive synchronous approach. Then, we benefit from a formal framework well suited to perform compilation and formal validation of systems. In practice, we design and implement a special purpose language (LE) and its two semantics: the ehavioral semantics helps us to define a program by the set of its behaviors and avoid ambiguousness in programs' interpretation; the execution equational semantics allows the modular compilation of programs into software and hardware targets (c code, vhdl code, fpga synthesis, observers). Our approach is pertinent considering the two main requirements of critical realistic applications: the modular compilation allows us to deal with large systems, the model-based approach provides us with formal validation.",
        "published": "2008-01-24T15:24:46Z",
        "link": "http://arxiv.org/abs/0801.3715v1",
        "categories": [
            "cs.PL",
            "cs.LO"
        ]
    },
    {
        "title": "On the Continuity Set of an omega Rational Function",
        "authors": [
            "Olivier Carton",
            "Olivier Finkel",
            "Pierre Simonnet"
        ],
        "summary": "In this paper, we study the continuity of rational functions realized by B\\\"uchi finite state transducers. It has been shown by Prieur that it can be decided whether such a function is continuous. We prove here that surprisingly, it cannot be decided whether such a function F has at least one point of continuity and that its continuity set C(F) cannot be computed. In the case of a synchronous rational function, we show that its continuity set is rational and that it can be computed. Furthermore we prove that any rational Pi^0_2-subset of X^omega for some alphabet X is the continuity set C(F) of an omega-rational synchronous function F defined on X^omega.",
        "published": "2008-01-25T10:54:05Z",
        "link": "http://arxiv.org/abs/0801.3912v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Quantified Propositional Logspace Reasoning",
        "authors": [
            "Steven Perron"
        ],
        "summary": "In this paper, we develop a quantified propositional proof systems that corresponds to logarithmic-space reasoning. We begin by defining a class SigmaCNF(2) of quantified formulas that can be evaluated in log space. Then our new proof system GL^* is defined as G_1^* with cuts restricted to SigmaCNF(2) formulas and no cut formula that is not quantifier free contains a free variable that does not appear in the final formula.   To show that GL^* is strong enough to capture log space reasoning, we translate theorems of VL into a family of tautologies that have polynomial-size GL^* proofs. VL is a theory of bounded arithmetic that is known to correspond to logarithmic-space reasoning. To do the translation, we find an appropriate axiomatization of VL, and put VL proofs into a new normal form.   To show that GL^* is not too strong, we prove the soundness of GL^* in such a way that it can be formalized in VL. This is done by giving a logarithmic-space algorithm that witnesses GL^* proofs.",
        "published": "2008-01-27T19:53:39Z",
        "link": "http://arxiv.org/abs/0801.4105v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Quantum entanglement analysis based on abstract interpretation",
        "authors": [
            "Simon Perdrix"
        ],
        "summary": "Entanglement is a non local property of quantum states which has no classical counterpart and plays a decisive role in quantum information theory. Several protocols, like the teleportation, are based on quantum entangled states. Moreover, any quantum algorithm which does not create entanglement can be efficiently simulated on a classical computer. The exact role of the entanglement is nevertheless not well understood. Since an exact analysis of entanglement evolution induces an exponential slowdown, we consider approximative analysis based on the framework of abstract interpretation. In this paper, a concrete quantum semantics based on superoperators is associated with a simple quantum programming language. The representation of entanglement, i.e. the design of the abstract domain is a key issue. A representation of entanglement as a partition of the memory is chosen. An abstract semantics is introduced, and the soundness of the approximation is proven.",
        "published": "2008-01-28T10:45:47Z",
        "link": "http://arxiv.org/abs/0801.4230v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "quant-ph"
        ]
    },
    {
        "title": "Concerning Olga, the Beautiful Little Street Dancer (Adjectives as   Higher-Order Polymorphic Functions)",
        "authors": [
            "Walid S. Saba"
        ],
        "summary": "In this paper we suggest a typed compositional seman-tics for nominal compounds of the form [Adj Noun] that models adjectives as higher-order polymorphic functions, and where types are assumed to represent concepts in an ontology that reflects our commonsense view of the world and the way we talk about it in or-dinary language. In addition to [Adj Noun] compounds our proposal seems also to suggest a plausible explana-tion for well known adjective ordering restrictions.",
        "published": "2008-01-30T19:40:45Z",
        "link": "http://arxiv.org/abs/0801.4746v5",
        "categories": [
            "cs.CL",
            "cs.LO"
        ]
    },
    {
        "title": "Shallow Models for Non-Iterative Modal Logics",
        "authors": [
            "Lutz Schröder",
            "Dirk Patinson"
        ],
        "summary": "The methods used to establish PSPACE-bounds for modal logics can roughly be grouped into two classes: syntax driven methods establish that exhaustive proof search can be performed in polynomial space whereas semantic approaches directly construct shallow models. In this paper, we follow the latter approach and establish generic PSPACE-bounds for a large and heterogeneous class of modal logics in a coalgebraic framework. In particular, no complete axiomatisation of the logic under scrutiny is needed. This does not only complement our earlier, syntactic, approach conceptually, but also covers a wide variety of new examples which are difficult to harness by purely syntactic means. Apart from re-proving known complexity bounds for a large variety of structurally different logics, we apply our method to obtain previously unknown PSPACE-bounds for Elgesem's logic of agency and for graded modal logic over reflexive frames.",
        "published": "2008-02-01T13:11:09Z",
        "link": "http://arxiv.org/abs/0802.0116v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "cs.MA",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "A topological formal treatment for scenario-based software specification   of concurrent real-time systems",
        "authors": [
            "Miriam C. B. Alves",
            "Christine C. Dantas",
            "Nanci N. Arai",
            "Rovedy B. da Silva"
        ],
        "summary": "Real-time systems are computing systems in which the meeting of their requirements is vital for their correctness. Consequently, if the real-time requirements of these systems are poorly understood and verified, the results can be disastrous and lead to irremediable project failures at the early phases of development. The present work addresses the problem of detecting deadlock situations early in the requirements specification phase of a concurrent real time system, proposing a simple proof-of-concepts prototype that joins scenario-based requirements specifications and techniques based on topology. The efforts are concentrated in the integration of the formal representation of Message Sequence Chart scenarios into the deadlock detection algorithm of Fajstrup et al., based on geometric and algebraic topology.",
        "published": "2008-02-01T22:12:47Z",
        "link": "http://arxiv.org/abs/0802.0212v1",
        "categories": [
            "cs.SE",
            "cs.LO"
        ]
    },
    {
        "title": "Independence and concurrent separation logic",
        "authors": [
            "Jonathan Hayman",
            "Glynn Winskel"
        ],
        "summary": "A compositional Petri net-based semantics is given to a simple language allowing pointer manipulation and parallelism. The model is then applied to give a notion of validity to the judgements made by concurrent separation logic that emphasizes the process-environment duality inherent in such rely-guarantee reasoning. Soundness of the rules of concurrent separation logic with respect to this definition of validity is shown. The independence information retained by the Petri net model is then exploited to characterize the independence of parallel processes enforced by the logic. This is shown to permit a refinement operation capable of changing the granularity of atomic actions.",
        "published": "2008-02-06T15:39:20Z",
        "link": "http://arxiv.org/abs/0802.0820v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.2; F.3.1; D.3.1; F.1.2"
        ]
    },
    {
        "title": "Combining generic judgments with recursive definitions",
        "authors": [
            "Andrew Gacek",
            "Dale Miller",
            "Gopalan Nadathur"
        ],
        "summary": "Many semantical aspects of programming languages, such as their operational semantics and their type assignment calculi, are specified by describing appropriate proof systems. Recent research has identified two proof-theoretic features that allow direct, logic-based reasoning about such descriptions: the treatment of atomic judgments as fixed points (recursive definitions) and an encoding of binding constructs via generic judgments. However, the logics encompassing these two features have thus far treated them orthogonally: that is, they do not provide the ability to define object-logic properties that themselves depend on an intrinsic treatment of binding. We propose a new and simple integration of these features within an intuitionistic logic enhanced with induction over natural numbers and we show that the resulting logic is consistent. The pivotal benefit of the integration is that it allows recursive definitions to not just encode simple, traditional forms of atomic judgments but also to capture generic properties pertaining to such judgments. The usefulness of this logic is illustrated by showing how it can provide elegant treatments of object-logic contexts that appear in proofs involving typing calculi and of arbitrarily cascading substitutions that play a role in reducibility arguments.",
        "published": "2008-02-06T19:18:57Z",
        "link": "http://arxiv.org/abs/0802.0865v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Lower Bounds for Complementation of omega-Automata Via the Full Automata   Technique",
        "authors": [
            "Qiqi Yan"
        ],
        "summary": "In this paper, we first introduce a lower bound technique for the state complexity of transformations of automata. Namely we suggest first considering the class of full automata in lower bound analysis, and later reducing the size of the large alphabet via alphabet substitutions. Then we apply such technique to the complementation of nondeterministic \\omega-automata, and obtain several lower bound results. Particularly, we prove an \\omega((0.76n)^n) lower bound for B\\\"uchi complementation, which also holds for almost every complementation or determinization transformation of nondeterministic omega-automata, and prove an optimal (\\omega(nk))^n lower bound for the complementation of generalized B\\\"uchi automata, which holds for Streett automata as well.",
        "published": "2008-02-08T23:13:52Z",
        "link": "http://arxiv.org/abs/0802.1226v3",
        "categories": [
            "cs.LO",
            "F.4.1; F.4.3"
        ]
    },
    {
        "title": "On the Complexity of Elementary Modal Logics",
        "authors": [
            "Edith Hemaspaandra",
            "Henning Schnoor"
        ],
        "summary": "Modal logics are widely used in computer science. The complexity of modal satisfiability problems has been investigated since the 1970s, usually proving results on a case-by-case basis. We prove a very general classification for a wide class of relevant logics: Many important subclasses of modal logics can be obtained by restricting the allowed models with first-order Horn formulas. We show that the satisfiability problem for each of these logics is either NP-complete or PSPACE-hard, and exhibit a simple classification criterion. Further, we prove matching PSPACE upper bounds for many of the PSPACE-hard logics.",
        "published": "2008-02-13T18:57:26Z",
        "link": "http://arxiv.org/abs/0802.1884v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "New Implementation Framework for Saturation-Based Reasoning",
        "authors": [
            "Alexandre Riazanov"
        ],
        "summary": "The saturation-based reasoning methods are among the most theoretically developed ones and are used by most of the state-of-the-art first-order logic reasoners. In the last decade there was a sharp increase in performance of such systems, which I attribute to the use of advanced calculi and the intensified research in implementation techniques. However, nowadays we are witnessing a slowdown in performance progress, which may be considered as a sign that the saturation-based technology is reaching its inherent limits. The position I am trying to put forward in this paper is that such scepticism is premature and a sharp improvement in performance may potentially be reached by adopting new architectural principles for saturation. The top-level algorithms and corresponding designs used in the state-of-the-art saturation-based theorem provers have (at least) two inherent drawbacks: the insufficient flexibility of the used inference selection mechanisms and the lack of means for intelligent prioritising of search directions. In this position paper I analyse these drawbacks and present two ideas on how they could be overcome. In particular, I propose a flexible low-cost high-precision mechanism for inference selection, intended to overcome problems associated with the currently used instances of clause selection-based procedures. I also outline a method for intelligent prioritising of search directions, based on probing the search space by exploring generalised search directions. I discuss some technical issues related to implementation of the proposed architectural principles and outline possible solutions.",
        "published": "2008-02-15T01:51:29Z",
        "link": "http://arxiv.org/abs/0802.2127v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Discrete Jordan Curve Theorem: A proof formalized in Coq with hypermaps",
        "authors": [
            "Jean-François Dufourd"
        ],
        "summary": "This paper presents a formalized proof of a discrete form of the Jordan Curve Theorem. It is based on a hypermap model of planar subdivisions, formal specifications and proofs assisted by the Coq system. Fundamental properties are proven by structural or noetherian induction: Genus Theorem, Euler's Formula, constructive planarity criteria. A notion of ring of faces is inductively defined and a Jordan Curve Theorem is stated and proven for any planar hypermap.",
        "published": "2008-02-20T14:23:01Z",
        "link": "http://arxiv.org/abs/0802.2853v1",
        "categories": [
            "cs.LO",
            "cs.DM"
        ]
    },
    {
        "title": "Compatibility of Shelah and Stupp's and Muchnik's iteration with   fragments of monadic second order logic",
        "authors": [
            "Dietrich Kuske"
        ],
        "summary": "We investigate the relation between the theory of the iterations in the sense of Shelah-Stupp and of Muchnik, resp., and the theory of the base structure for several logics. These logics are obtained from the restriction of set quantification in monadic second order logic to certain subsets like, e.g., finite sets, chains, and finite unions of chains. We show that these theories of the Shelah-Stupp iteration can be reduced to corresponding theories of the base structure. This fails for Muchnik's iteration.",
        "published": "2008-02-20T14:33:04Z",
        "link": "http://arxiv.org/abs/0802.2862v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Cardinality and counting quantifiers on omega-automatic structures",
        "authors": [
            "Lukasz Kaiser",
            "Sasha Rubin",
            "Vince Bárány"
        ],
        "summary": "We investigate structures that can be represented by omega-automata, so called omega-automatic structures, and prove that relations defined over such structures in first-order logic expanded by the first-order quantifiers `there exist at most $\\aleph_0$ many', 'there exist finitely many' and 'there exist $k$ modulo $m$ many' are omega-regular. The proof identifies certain algebraic properties of omega-semigroups. As a consequence an omega-regular equivalence relation of countable index has an omega-regular set of representatives. This implies Blumensath's conjecture that a countable structure with an $\\omega$-automatic presentation can be represented using automata on finite words. This also complements a very recent result of Hj\\\"orth, Khoussainov, Montalban and Nies showing that there is an omega-automatic structure which has no injective presentation.",
        "published": "2008-02-20T14:37:32Z",
        "link": "http://arxiv.org/abs/0802.2866v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Model Checking Games for the Quantitative mu-Calculus",
        "authors": [
            "Diana Fischer",
            "Erich Grädel",
            "Lukasz Kaiser"
        ],
        "summary": "We investigate quantitative extensions of modal logic and the modal mu-calculus, and study the question whether the tight connection between logic and games can be lifted from the qualitative logics to their quantitative counterparts. It turns out that, if the quantitative mu-calculus is defined in an appropriate way respecting the duality properties between the logical operators, then its model checking problem can indeed be characterised by a quantitative variant of parity games. However, these quantitative games have quite different properties than their classical counterparts, in particular they are, in general, not positionally determined. The correspondence between the logic and the games goes both ways: the value of a formula on a quantitative transition system coincides with the value of the associated quantitative game, and conversely, the values of quantitative parity games are definable in the quantitative mu-calculus.",
        "published": "2008-02-20T14:57:24Z",
        "link": "http://arxiv.org/abs/0802.2871v1",
        "categories": [
            "cs.LO",
            "cs.GT"
        ]
    },
    {
        "title": "Design and Implementation of Aggregate Functions in the DLV System",
        "authors": [
            "Wolfgang Faber",
            "Gerald Pfeifer",
            "Nicola Leone",
            "Tina Dell'Armi",
            "Giuseppe Ielpa"
        ],
        "summary": "Disjunctive Logic Programming (DLP) is a very expressive formalism: it allows for expressing every property of finite structures that is decidable in the complexity class SigmaP2 (= NP^NP). Despite this high expressiveness, there are some simple properties, often arising in real-world applications, which cannot be encoded in a simple and natural manner. Especially properties that require the use of arithmetic operators (like sum, times, or count) on a set or multiset of elements, which satisfy some conditions, cannot be naturally expressed in classic DLP.   To overcome this deficiency, we extend DLP by aggregate functions in a conservative way. In particular, we avoid the introduction of constructs with disputed semantics, by requiring aggregates to be stratified. We formally define the semantics of the extended language (called DLP^A), and illustrate how it can be profitably used for representing knowledge. Furthermore, we analyze the computational complexity of DLP^A, showing that the addition of aggregates does not bring a higher cost in that respect. Finally, we provide an implementation of DLP^A in DLV -- a state-of-the-art DLP system -- and report on experiments which confirm the usefulness of the proposed extension also for the efficiency of computation.",
        "published": "2008-02-21T15:44:09Z",
        "link": "http://arxiv.org/abs/0802.3137v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; I.2.4; D.3.1; D.1.6"
        ]
    },
    {
        "title": "Automatic Verification of Correspondences for Security Protocols",
        "authors": [
            "Bruno Blanchet"
        ],
        "summary": "We present a new technique for verifying correspondences in security protocols. In particular, correspondences can be used to formalize authentication. Our technique is fully automatic, it can handle an unbounded number of sessions of the protocol, and it is efficient in practice. It significantly extends a previous technique for the verification of secrecy. The protocol is represented in an extension of the pi calculus with fairly arbitrary cryptographic primitives. This protocol representation includes the specification of the correspondence to be verified, but no other annotation. This representation is then translated into an abstract representation by Horn clauses, which is used to prove the desired correspondence. Our technique has been proved correct and implemented. We have tested it on various protocols from the literature. The experimental results show that these protocols can be verified by our technique in less than 1 s.",
        "published": "2008-02-23T14:03:56Z",
        "link": "http://arxiv.org/abs/0802.3444v1",
        "categories": [
            "cs.CR",
            "cs.LO",
            "D.2.4; D.4.6; F.3.1"
        ]
    },
    {
        "title": "Towards a formalization of budgets",
        "authors": [
            "Jan A. Bergstra",
            "Sanne Nolst Trenité",
            "Mark B. van der Zwaag"
        ],
        "summary": "We go into the need for, and the requirements on, a formal theory of budgets. We present a simple algebraic theory of rational budgets, i.e., budgets in which amounts of money are specified by functions on the rational numbers. This theory is based on the tuplix calculus. We go into the importance of using totalized models for the rational numbers. We present a case study on the educational budget of a university department offering master programs.",
        "published": "2008-02-25T13:11:32Z",
        "link": "http://arxiv.org/abs/0802.3617v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Color Graphs: An Efficient Model For Two-Dimensional Cellular Automata   Linear Rules",
        "authors": [
            "Birendra Kumar Nayak",
            "Sudhakar Sahoo",
            "Sushant Kumar Rout"
        ],
        "summary": "Two-dimensional nine neighbor hood rectangular Cellular Automata rules can be modeled using many different techniques like Rule matrices, State Transition Diagrams, Boolean functions, Algebraic Normal Form etc. In this paper, a new model is introduced using color graphs to model all the 512 linear rules. The graph theoretic properties therefore studied in this paper simplifies the analysis of all linear rules in comparison with other ways of its study.",
        "published": "2008-02-25T13:54:50Z",
        "link": "http://arxiv.org/abs/0802.3626v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Deriving Sorting Algorithms",
        "authors": [
            "José Bacelar Almeida",
            "Jorge Sousa Pinto"
        ],
        "summary": "This paper proposes new derivations of three well-known sorting algorithms, in their functional formulation. The approach we use is based on three main ingredients: first, the algorithms are derived from a simpler algorithm, i.e. the specification is already a solution to the problem (in this sense our derivations are program transformations). Secondly, a mixture of inductive and coinductive arguments are used in a uniform, algebraic style in our reasoning. Finally, the approach uses structural invariants so as to strengthen the equational reasoning with logical arguments that cannot be captured in the algebraic framework.",
        "published": "2008-02-26T19:47:57Z",
        "link": "http://arxiv.org/abs/0802.3881v1",
        "categories": [
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "Syntax diagrams as a formalism for representation of syntactic relations   of formal languages",
        "authors": [
            "Vladimir Lapshin"
        ],
        "summary": "The new approach to representation of syntax of formal languages-- a formalism of syntax diagrams is offered. Syntax diagrams look a convenient language for the description of syntactic relations in the languages having nonlinear representation of texts, for example, for representation of syntax lows of the language of structural chemical formulas. The formalism of neighbourhood grammar is used to describe the set of correct syntax constructs. The neighbourhood the grammar consists of a set of families of \"neighbourhoods\"-- the diagrams defined for each symbol of the language's alphabet. The syntax diagram is correct if each symbol is included into this diagram together with some neighbourhood. In other words, correct diagrams are needed to be covered by elements of the neighbourhood grammar. Thus, the grammar of formal language can be represented as system of the covers defined for each correct syntax diagram.",
        "published": "2008-02-27T08:59:58Z",
        "link": "http://arxiv.org/abs/0802.3974v2",
        "categories": [
            "cs.LO",
            "F.4.2; F.4.3"
        ]
    },
    {
        "title": "A Qualitative Modal Representation of Quantum Register Transformations",
        "authors": [
            "Andrea Masini",
            "Luca Viganò",
            "Margherita Zorzi"
        ],
        "summary": "We introduce two modal natural deduction systems that are suitable to represent and reason about transformations of quantum registers in an abstract, qualitative, way. Quantum registers represent quantum systems, and can be viewed as the structure of quantum data for quantum operations. Our systems provide a modal framework for reasoning about operations on quantum registers (unitary transformations and measurements), in terms of possible worlds (as abstractions of quantum registers) and accessibility relations between these worlds. We give a Kripke--style semantics that formally describes quantum register transformations and prove the soundness and completeness of our systems with respect to this semantics.",
        "published": "2008-02-27T17:43:23Z",
        "link": "http://arxiv.org/abs/0802.4057v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Language of Boolean functions its Grammar and Machine",
        "authors": [
            "Birendra Kumar Nayak",
            "Sudhakar Sahoo"
        ],
        "summary": "In this paper an algorithm is designed which generates in-equivalent Boolean functions of any number of variables from the four Boolean functions of single variable. The grammar for such set of Boolean function is provided. The Turing Machine that accepts such set is constructed.",
        "published": "2008-02-28T06:21:49Z",
        "link": "http://arxiv.org/abs/0802.4131v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Safety alternating automata on data words",
        "authors": [
            "Ranko Lazic"
        ],
        "summary": "A data word is a sequence of pairs of a letter from a finite alphabet and an element from an infinite set, where the latter can only be compared for equality. Safety one-way alternating automata with one register on infinite data words are considered, their nonemptiness is shown EXPSPACE-complete, and their inclusion decidable but not primitive recursive. The same complexity bounds are obtained for satisfiability and refinement, respectively, for the safety fragment of linear temporal logic with freeze quantification. Dropping the safety restriction, adding past temporal operators, or adding one more register, each causes undecidability.",
        "published": "2008-02-28T16:54:31Z",
        "link": "http://arxiv.org/abs/0802.4237v3",
        "categories": [
            "cs.LO",
            "F.4.1; F.1.1"
        ]
    },
    {
        "title": "Automated Termination Proofs for Logic Programs by Term Rewriting",
        "authors": [
            "P. Schneider-Kamp",
            "J. Giesl",
            "A. Serebrenik",
            "R. Thiemann"
        ],
        "summary": "There are two kinds of approaches for termination analysis of logic programs: \"transformational\" and \"direct\" ones. Direct approaches prove termination directly on the basis of the logic program. Transformational approaches transform a logic program into a term rewrite system (TRS) and then analyze termination of the resulting TRS instead. Thus, transformational approaches make all methods previously developed for TRSs available for logic programs as well. However, the applicability of most existing transformations is quite restricted, as they can only be used for certain subclasses of logic programs. (Most of them are restricted to well-moded programs.) In this paper we improve these transformations such that they become applicable for any definite logic program. To simulate the behavior of logic programs by TRSs, we slightly modify the notion of rewriting by permitting infinite terms. We show that our transformation results in TRSs which are indeed suitable for automated termination analysis. In contrast to most other methods for termination of logic programs, our technique is also sound for logic programming without occur check, which is typically used in practice. We implemented our approach in the termination prover AProVE and successfully evaluated it on a large collection of examples.",
        "published": "2008-03-02T14:53:01Z",
        "link": "http://arxiv.org/abs/0803.0014v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL",
            "F.3.1; D.1.6; I.2.2; I.2.3"
        ]
    },
    {
        "title": "Thread algebra for poly-threading",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "Threads as considered in basic thread algebra are primarily looked upon as behaviours exhibited by sequential programs on execution. It is a fact of life that sequential programs are often fragmented. Consequently, fragmented program behaviours are frequently found. In this paper, we consider this phenomenon. We extend basic thread algebra with the barest mechanism for sequencing of threads that are taken for fragments. This mechanism, called poly-threading, supports both autonomous and non-autonomous thread selection in sequencing. We relate the resulting theory to the algebraic theory of processes known as ACP and use it to describe analytic execution architectures suited for fragmented programs. We also consider the case where the steps of fragmented program behaviours are interleaved in the ways of non-distributed and distributed multi-threading.",
        "published": "2008-03-04T07:18:46Z",
        "link": "http://arxiv.org/abs/0803.0378v2",
        "categories": [
            "cs.LO",
            "D.4.1; F.1.1; F.1.2; F.3.2"
        ]
    },
    {
        "title": "Towards an Optimal Separation of Space and Length in Resolution",
        "authors": [
            "Jakob Nordström",
            "Johan Håstad"
        ],
        "summary": "Most state-of-the-art satisfiability algorithms today are variants of the DPLL procedure augmented with clause learning. The main bottleneck for such algorithms, other than the obvious one of time, is the amount of memory used. In the field of proof complexity, the resources of time and memory correspond to the length and space of resolution proofs. There has been a long line of research trying to understand these proof complexity measures, but while strong results have been proven on length our understanding of space is still quite poor. For instance, it remains open whether the fact that a formula is provable in short length implies that it is also provable in small space or whether on the contrary these measures are unrelated in the sense that short proofs can be arbitrarily complex with respect to space.   In this paper, we present some evidence that the true answer should be that the latter case holds. We do this by proving a tight bound of Theta(sqrt(n)) on the space needed for so-called pebbling contradictions over pyramid graphs of size n. This yields the first polynomial lower bound on space that is not a consequence of a corresponding lower bound on width, another well-studied measure in resolution, as well as an improvement of the weak separation in (Nordstrom 2006) of space and width from logarithmic to polynomial.   Also, continuing the line of research initiated by (Ben-Sasson 2002) into trade-offs between different proof complexity measures, we present a simplified proof of the recent length-space trade-off result in (Hertel and Pitassi 2007), and show how our ideas can be used to prove a couple of other exponential trade-offs in resolution.",
        "published": "2008-03-05T12:52:13Z",
        "link": "http://arxiv.org/abs/0803.0661v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.4.1; F.1.3; I.2.3"
        ]
    },
    {
        "title": "Hybrid Reasoning and the Future of Iconic Representations",
        "authors": [
            "Catherine Recanati"
        ],
        "summary": "We give a brief overview of the main characteristics of diagrammatic reasoning, analyze a case of human reasoning in a mastermind game, and explain why hybrid representation systems (HRS) are particularly attractive and promising for Artificial General Intelligence and Computer Science in general.",
        "published": "2008-03-10T16:44:55Z",
        "link": "http://arxiv.org/abs/0803.1457v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "A Quantifier Elimination Algorithm for Linear Real Arithmetic",
        "authors": [
            "David Monniaux"
        ],
        "summary": "We propose a new quantifier elimination algorithm for the theory of linear real arithmetic. This algorithm uses as subroutine satisfiability modulo this theory, a problem for which there are several implementations available. The quantifier elimination algorithm presented in the paper is compared, on examples arising from program analysis problems, to several other implementations, all of which cannot solve some of the examples that our algorithm solves easily.",
        "published": "2008-03-11T12:55:25Z",
        "link": "http://arxiv.org/abs/0803.1575v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "On Winning Conditions of High Borel Complexity in Pushdown Games",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "Some decidable winning conditions of arbitrarily high finite Borel complexity for games on finite graphs or on pushdown graphs have been recently presented by O. Serre in [ Games with Winning Conditions of High Borel Complexity, in the Proceedings of the International Conference ICALP 2004, LNCS, Volume 3142, p. 1150-1162 ]. We answer in this paper several questions which were raised by Serre in the above cited paper. We first show that, for every positive integer n, the class C_n(A), which arises in the definition of decidable winning conditions, is included in the class of non-ambiguous context free omega languages, and that it is neither closed under union nor under intersection. We prove also that there exists pushdown games, equipped with such decidable winning conditions, where the winning sets are not deterministic context free languages, giving examples of winning sets which are non-deterministic non-ambiguous context free languages, inherently ambiguous context free languages, or even non context free languages.",
        "published": "2008-03-12T19:50:26Z",
        "link": "http://arxiv.org/abs/0803.1830v1",
        "categories": [
            "cs.LO",
            "cs.GT",
            "math.LO"
        ]
    },
    {
        "title": "On the Topological Complexity of Infinitary Rational Relations",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We prove in this paper that there exists some infinitary rational relations which are analytic but non Borel sets, giving an answer to a question of Simonnet [Automates et Th\\'eorie Descriptive, Ph. D. Thesis, Universit\\'e Paris 7, March 1992].",
        "published": "2008-03-12T20:14:47Z",
        "link": "http://arxiv.org/abs/0803.1841v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "Closure Properties of Locally Finite Omega Languages",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "Locally finite omega languages were introduced by Ressayre in [Journal of Symbolic Logic, Volume 53, No. 4, p.1009-1026]. They generalize omega languages accepted by finite automata or defined by monadic second order sentences. We study here closure properties of the family LOC_omega of locally finite omega languages. In particular we show that the class LOC_omega is neither closed under intersection nor under complementation, giving an answer to a question of Ressayre.",
        "published": "2008-03-12T20:16:34Z",
        "link": "http://arxiv.org/abs/0803.1842v1",
        "categories": [
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "The Abella Interactive Theorem Prover (System Description)",
        "authors": [
            "Andrew Gacek"
        ],
        "summary": "Abella is an interactive system for reasoning about aspects of object languages that have been formally presented through recursive rules based on syntactic structure. Abella utilizes a two-level logic approach to specification and reasoning. One level is defined by a specification logic which supports a transparent encoding of structural semantics rules and also enables their execution. The second level, called the reasoning logic, embeds the specification logic and allows the development of proofs of properties about specifications. An important characteristic of both logics is that they exploit the lambda tree syntax approach to treating binding in object languages. Amongst other things, Abella has been used to prove normalizability properties of the lambda calculus, cut admissibility for a sequent calculus and type uniqueness and subject reduction properties. This paper discusses the logical foundations of Abella, outlines the style of theorem proving that it supports and finally describes some of its recent applications.",
        "published": "2008-03-15T16:15:10Z",
        "link": "http://arxiv.org/abs/0803.2305v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Tableau-based decision procedures for logics of strategic ability in   multi-agent systems",
        "authors": [
            "Valentin Goranko",
            "Dmitry Shkatov"
        ],
        "summary": "We develop an incremental tableau-based decision procedures for the   Alternating-time temporal logic ATL and some of its variants.   While running within the theoretically established complexity upper bound, we claim that our tableau is practically more efficient in the average case than other decision procedures for ATL known so far. Besides, the ease of its adaptation to variants of ATL demonstrates the flexibility of the proposed procedure.",
        "published": "2008-03-15T16:22:53Z",
        "link": "http://arxiv.org/abs/0803.2306v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.MA",
            "F.4.1; I.2.3; I.2.4; I.2.11"
        ]
    },
    {
        "title": "Lissom, a Source Level Proof Carrying Code Platform",
        "authors": [
            "Joao Gomes",
            "Daniel Martins",
            "Simao Melo de Sousa",
            "Jorge Sousa Pinto"
        ],
        "summary": "This paper introduces a proposal for a Proof Carrying Code (PCC) architecture called Lissom. Started as a challenge for final year Computing students, Lissom was thought as a mean to prove to a sceptic community, and in particular to students, that formal verification tools can be put to practice in a realistic environment, and be used to solve complex and concrete problems. The attractiveness of the problems that PCC addresses has already brought students to show interest in this project.",
        "published": "2008-03-15T18:53:06Z",
        "link": "http://arxiv.org/abs/0803.2317v1",
        "categories": [
            "cs.LO",
            "cs.SE"
        ]
    },
    {
        "title": "Logical Queries over Views: Decidability and Expressiveness",
        "authors": [
            "James Bailey",
            "Guozhu Dong",
            "Anthony Widjaja To"
        ],
        "summary": "We study the problem of deciding satisfiability of first order logic queries over views, our aim being to delimit the boundary between the decidable and the undecidable fragments of this language. Views currently occupy a central place in database research, due to their role in applications such as information integration and data warehousing. Our main result is the identification of a decidable class of first order queries over unary conjunctive views that generalises the decidability of the classical class of first order sentences over unary relations, known as the Lowenheim class. We then demonstrate how various extensions of this class lead to undecidability and also provide some expressivity results. Besides its theoretical interest, our new decidable class is potentially interesting for use in applications such as deciding implication of complex dependencies, analysis of a restricted class of active database rules, and ontology reasoning.",
        "published": "2008-03-18T02:07:12Z",
        "link": "http://arxiv.org/abs/0803.2559v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "F.4.1; H.2.3"
        ]
    },
    {
        "title": "Concurrent Composition and Algebras of Events, Actions, and Processes",
        "authors": [
            "Mark Burgin a",
            "Marc L. Smith"
        ],
        "summary": "There are many different models of concurrent processes. The goal of this work is to introduce a common formalized framework for current research in this area and to eliminate shortcomings of existing models of concurrency. Following up the previous research of the authors and other researchers on concurrency, here we build a high-level metamodel EAP (event-action-process) for concurrent processes. This metamodel comprises a variety of other models of concurrent processes. We shape mathematical models for, and study events, actions, and processes in relation to important practical problems, such as communication in networks, concurrent programming, and distributed computations. In the third section of the work, a three-level algebra of events, actions and processes is constructed and studied as a new stage of algebra for concurrent processes. Relations between EAP process algebra and other models of concurrency are considered in the fourth section of this work.",
        "published": "2008-03-21T00:27:38Z",
        "link": "http://arxiv.org/abs/0803.3099v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.4.1"
        ]
    },
    {
        "title": "Labeled Natural Deduction Systems for a Family of Tense Logics",
        "authors": [
            "Luca Viganò",
            "Marco Volpe"
        ],
        "summary": "We give labeled natural deduction systems for a family of tense logics extending the basic linear tense logic Kl. We prove that our systems are sound and complete with respect to the usual Kripke semantics, and that they possess a number of useful normalization properties (in particular, derivations reduce to a normal form that enjoys a subformula property). We also discuss how to extend our systems to capture richer logics like (fragments of) LTL.",
        "published": "2008-03-21T15:53:56Z",
        "link": "http://arxiv.org/abs/0803.3187v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Some results on $\\mathbb{R}$-computable structures",
        "authors": [
            "Wesley Calvert",
            "John E. Porter"
        ],
        "summary": "This survey paper examines the effective model theory obtained with the BSS model of real number computation. It treats the following topics: computable ordinals, satisfaction of computable infinitary formulas, forcing as a construction technique, effective categoricity, effective topology, and relations with other models for the effective theory of uncountable structures.",
        "published": "2008-03-24T13:59:53Z",
        "link": "http://arxiv.org/abs/0803.3404v2",
        "categories": [
            "cs.DB",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "Approximating a Behavioural Pseudometric without Discount for<br>   Probabilistic Systems",
        "authors": [
            "Franck van Breugel",
            "Babita Sharma",
            "James Worrell"
        ],
        "summary": "Desharnais, Gupta, Jagadeesan and Panangaden introduced a family of behavioural pseudometrics for probabilistic transition systems. These pseudometrics are a quantitative analogue of probabilistic bisimilarity. Distance zero captures probabilistic bisimilarity. Each pseudometric has a discount factor, a real number in the interval (0, 1]. The smaller the discount factor, the more the future is discounted. If the discount factor is one, then the future is not discounted at all. Desharnais et al. showed that the behavioural distances can be calculated up to any desired degree of accuracy if the discount factor is smaller than one. In this paper, we show that the distances can also be approximated if the future is not discounted. A key ingredient of our algorithm is Tarski's decision procedure for the first order theory over real closed fields. By exploiting the Kantorovich-Rubinstein duality theorem we can restrict to the existential fragment for which more efficient decision procedures exist.",
        "published": "2008-03-26T19:08:40Z",
        "link": "http://arxiv.org/abs/0803.3796v3",
        "categories": [
            "cs.LO",
            "F.3.1; F.3.2"
        ]
    },
    {
        "title": "Cancellation Meadows: a Generic Basis Theorem and Some Applications",
        "authors": [
            "Jan A. Bergstra",
            "Inge Bethke",
            "Alban Ponse"
        ],
        "summary": "Let Q_0 denote the rational numbers expanded to a \"meadow\", that is, after taking its zero-totalized form (0^{-1}=0) as the preferred interpretation. In this paper we consider \"cancellation meadows\", i.e., meadows without proper zero divisors, such as $Q_0$ and prove a generic completeness result. We apply this result to cancellation meadows expanded with differentiation operators, the sign function, and with floor, ceiling and a signed variant of the square root, respectively. We give an equational axiomatization of these operators and thus obtain a finite basis for various expanded cancellation meadows.",
        "published": "2008-03-27T16:01:31Z",
        "link": "http://arxiv.org/abs/0803.3969v3",
        "categories": [
            "math.RA",
            "cs.LO",
            "AC, RA"
        ]
    },
    {
        "title": "Binary Decision Diagrams for Affine Approximation",
        "authors": [
            "Kevin Henshall",
            "Peter Schachte",
            "Harald Søndergaard",
            "Leigh Whiting"
        ],
        "summary": "Selman and Kautz's work on ``knowledge compilation'' established how approximation (strengthening and/or weakening) of a propositional knowledge-base can be used to speed up query processing, at the expense of completeness. In this classical approach, querying uses Horn over- and under-approximations of a given knowledge-base, which is represented as a propositional formula in conjunctive normal form (CNF). Along with the class of Horn functions, one could imagine other Boolean function classes that might serve the same purpose, owing to attractive deduction-computational properties similar to those of the Horn functions. Indeed, Zanuttini has suggested that the class of affine Boolean functions could be useful in knowledge compilation and has presented an affine approximation algorithm. Since CNF is awkward for presenting affine functions, Zanuttini considers both a sets-of-models representation and the use of modulo 2 congruence equations. In this paper, we propose an algorithm based on reduced ordered binary decision diagrams (ROBDDs). This leads to a representation which is more compact than the sets of models and, once we have established some useful properties of affine Boolean functions, a more efficient algorithm.",
        "published": "2008-04-01T05:08:44Z",
        "link": "http://arxiv.org/abs/0804.0066v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "A proof theoretic analysis of intruder theories",
        "authors": [
            "Alwen Tiu",
            "Rajeev Gore"
        ],
        "summary": "We consider the problem of intruder deduction in security protocol analysis: that is, deciding whether a given message $M$ can be deduced from a set of messages $\\Gamma$ under the theory of blind signatures and arbitrary convergent equational theories modulo associativity and commutativity (AC) of certain binary operators. The traditional formulations of intruder deduction are usually given in natural-deduction-like systems and proving decidability requires significant effort in showing that the rules are \"local\" in some sense. By using the well-known translation between natural deduction and sequent calculus, we recast the intruder deduction problem as proof search in sequent calculus, in which locality is immediate. Using standard proof theoretic methods, such as permutability of rules and cut elimination, we show that the intruder deduction problem can be reduced, in polynomial time, to the elementary deduction problems, which amounts to solving certain equations in the underlying individual equational theories. We further show that this result extends to combinations of disjoint AC-convergent theories whereby the decidability of intruder deduction under the combined theory reduces to the decidability of elementary deduction in each constituent theory. Although various researchers have reported similar results for individual cases, our work shows that these results can be obtained using a systematic and uniform methodology based on the sequent calculus.",
        "published": "2008-04-02T00:21:27Z",
        "link": "http://arxiv.org/abs/0804.0273v2",
        "categories": [
            "cs.LO",
            "cs.CR",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Symmetry Breaking for Maximum Satisfiability",
        "authors": [
            "Joao Marques-Silva",
            "Ines Lynce",
            "Vasco Manquinho"
        ],
        "summary": "Symmetries are intrinsic to many combinatorial problems including Boolean Satisfiability (SAT) and Constraint Programming (CP). In SAT, the identification of symmetry breaking predicates (SBPs) is a well-known, often effective, technique for solving hard problems. The identification of SBPs in SAT has been the subject of significant improvements in recent years, resulting in more compact SBPs and more effective algorithms. The identification of SBPs has also been applied to pseudo-Boolean (PB) constraints, showing that symmetry breaking can also be an effective technique for PB constraints. This paper extends further the application of SBPs, and shows that SBPs can be identified and used in Maximum Satisfiability (MaxSAT), as well as in its most well-known variants, including partial MaxSAT, weighted MaxSAT and weighted partial MaxSAT. As with SAT and PB, symmetry breaking predicates for MaxSAT and variants are shown to be effective for a representative number of problem domains, allowing solving problem instances that current state of the art MaxSAT solvers could not otherwise solve.",
        "published": "2008-04-03T18:19:43Z",
        "link": "http://arxiv.org/abs/0804.0599v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Weak Affine Light Typing is complete with respect to Safe Recursion on   Notation",
        "authors": [
            "Luca Roversi"
        ],
        "summary": "Weak affine light typing (WALT) assigns light affine linear formulae as types to a subset of lambda-terms of System F. WALT is poly-time sound: if a lambda-term M has type in WALT, M can be evaluated with a polynomial cost in the dimension of the derivation that gives it a type. The evaluation proceeds under any strategy of a rewriting relation which is a mix of both call-by-name and call-by-value beta-reductions. WALT weakens, namely generalizes, the notion of \"stratification of deductions\", common to some Light Systems -- those logical systems, derived from Linear logic, to characterize the set of Polynomial functions -- . A weaker stratification allows to define a compositional embedding of Safe recursion on notation (SRN) into WALT. It turns out that the expressivity of WALT is strictly stronger than the one of the known Light Systems. The embedding passes through the representation of a subsystem of SRN. It is obtained by restricting the composition scheme of SRN to one that can only use its safe variables linearly. On one side, this suggests that SRN, in fact, can be redefined in terms of more primitive constructs. On the other, the embedding of SRN into WALT enjoys the two following remarkable aspects. Every datatype, required by the embedding, is represented from scratch, showing the strong structural proof-theoretical roots of WALT. Moreover, the embedding highlights a stratification structure of the normal and safe arguments, normally hidden inside the world of SRN-normal/safe variables: the less an argument is \"polyomially impredicative\", the deeper, in a formal, proof-theoretical sense, it is represented inside WALT. Finally, since WALT is SRN-complete it is also polynomial-time complete since SRN is.",
        "published": "2008-04-04T08:14:22Z",
        "link": "http://arxiv.org/abs/0804.0660v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Semi-continuous Sized Types and Termination",
        "authors": [
            "Andreas Abel"
        ],
        "summary": "Some type-based approaches to termination use sized types: an ordinal bound for the size of a data structure is stored in its type. A recursive function over a sized type is accepted if it is visible in the type system that recursive calls occur just at a smaller size. This approach is only sound if the type of the recursive function is admissible, i.e., depends on the size index in a certain way. To explore the space of admissible functions in the presence of higher-kinded data types and impredicative polymorphism, a semantics is developed where sized types are interpreted as functions from ordinals into sets of strongly normalizing terms. It is shown that upper semi-continuity of such functions is a sufficient semantic criterion for admissibility. To provide a syntactical criterion, a calculus for semi-continuous functions is developed.",
        "published": "2008-04-05T22:27:29Z",
        "link": "http://arxiv.org/abs/0804.0876v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "A Semi-Automatic Framework to Discover Epistemic Modalities in   Scientific Articles",
        "authors": [
            "Sviatlana Danilava",
            "Christoph Schommer"
        ],
        "summary": "Documents in scientific newspapers are often marked by attitudes and opinions of the author and/or other persons, who contribute with objective and subjective statements and arguments as well. In this respect, the attitude is often accomplished by a linguistic modality. As in languages like english, french and german, the modality is expressed by special verbs like can, must, may, etc. and the subjunctive mood, an occurrence of modalities often induces that these verbs take over the role of modality. This is not correct as it is proven that modality is the instrument of the whole sentence where both the adverbs, modal particles, punctuation marks, and the intonation of a sentence contribute. Often, a combination of all these instruments are necessary to express a modality. In this work, we concern with the finding of modal verbs in scientific texts as a pre-step towards the discovery of the attitude of an author. Whereas the input will be an arbitrary text, the output consists of zones representing modalities.",
        "published": "2008-04-07T14:13:27Z",
        "link": "http://arxiv.org/abs/0804.1033v1",
        "categories": [
            "cs.CL",
            "cs.LO",
            "J.5; I.2.7; F.4.3"
        ]
    },
    {
        "title": "The Geometry of Interaction of Differential Interaction Nets",
        "authors": [
            "Marc de Falco"
        ],
        "summary": "The Geometry of Interaction purpose is to give a semantic of proofs or programs accounting for their dynamics. The initial presentation, translated as an algebraic weighting of paths in proofnets, led to a better characterization of the lambda-calculus optimal reduction. Recently Ehrhard and Regnier have introduced an extension of the Multiplicative Exponential fragment of Linear Logic (MELL) that is able to express non-deterministic behaviour of programs and a proofnet-like calculus: Differential Interaction Nets. This paper constructs a proper Geometry of Interaction (GoI) for this extension. We consider it both as an algebraic theory and as a concrete reversible computation. We draw links between this GoI and the one of MELL. As a by-product we give for the first time an equational theory suitable for the GoI of the Multiplicative Additive fragment of Linear Logic.",
        "published": "2008-04-09T08:57:47Z",
        "link": "http://arxiv.org/abs/0804.1435v1",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Mechanizing the Metatheory of LF",
        "authors": [
            "Christian Urban",
            "James Cheney",
            "Stefan Berghofer"
        ],
        "summary": "LF is a dependent type theory in which many other formal systems can be conveniently embedded. However, correct use of LF relies on nontrivial metatheoretic developments such as proofs of correctness of decision procedures for LF's judgments. Although detailed informal proofs of these properties have been published, they have not been formally verified in a theorem prover. We have formalized these properties within Isabelle/HOL using the Nominal Datatype Package, closely following a recent article by Harper and Pfenning. In the process, we identified and resolved a gap in one of the proofs and a small number of minor lacunae in others. We also formally derive a version of the type checking algorithm from which Isabelle/HOL can generate executable code. Besides its intrinsic interest, our formalization provides a foundation for studying the adequacy of LF encodings, the correctness of Twelf-style metatheoretic reasoning, and the metatheory of extensions to LF.",
        "published": "2008-04-10T11:10:26Z",
        "link": "http://arxiv.org/abs/0804.1667v3",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "On affine usages in signal-based communication",
        "authors": [
            "Roberto Amadio",
            "Mehdi Dogguy"
        ],
        "summary": "We describe a type system for a synchronous pi-calculus formalising the notion of affine usage in signal-based communication. In particular, we identify a limited number of usages that preserve affinity and that can be composed. As a main application of the resulting system, we show that typable programs are deterministic.",
        "published": "2008-04-10T15:16:06Z",
        "link": "http://arxiv.org/abs/0804.1729v3",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Lambda-Free Logical Frameworks",
        "authors": [
            "Robin Adams"
        ],
        "summary": "We present the definition of the logical framework TF, the Type Framework. TF is a lambda-free logical framework; it does not include lambda-abstraction or product kinds. We give formal proofs of several results in the metatheory of TF, and show how it can be conservatively embedded in the logical framework LF: its judgements can be seen as the judgements of LF that are in beta-normal, eta-long normal form. We show how several properties, such as adequacy theorems for object theories and the injectivity of constants, can be proven more easily in TF, and then `lifted' to LF.",
        "published": "2008-04-11T11:32:51Z",
        "link": "http://arxiv.org/abs/0804.1879v2",
        "categories": [
            "cs.LO",
            "F.4.1; F.4.3"
        ]
    },
    {
        "title": "A Logic Programming Framework for Combinational Circuit Synthesis",
        "authors": [
            "Paul Tarau",
            "Brenda Luderman"
        ],
        "summary": "Logic Programming languages and combinational circuit synthesis tools share a common \"combinatorial search over logic formulae\" background. This paper attempts to reconnect the two fields with a fresh look at Prolog encodings for the combinatorial objects involved in circuit synthesis. While benefiting from Prolog's fast unification algorithm and built-in backtracking mechanism, efficiency of our search algorithm is ensured by using parallel bitstring operations together with logic variable equality propagation, as a mapping mechanism from primary inputs to the leaves of candidate Leaf-DAGs implementing a combinational circuit specification. After an exhaustive expressiveness comparison of various minimal libraries, a surprising first-runner, Strict Boolean Inequality \"<\" together with constant function \"1\" also turns out to have small transistor-count implementations, competitive to NAND-only or NOR-only libraries. As a practical outcome, a more realistic circuit synthesizer is implemented that combines rewriting-based simplification of (<,1) circuits with exhaustive Leaf-DAG circuit search.   Keywords: logic programming and circuit design, combinatorial object generation, exact combinational circuit synthesis, universal boolean logic libraries, symbolic rewriting, minimal transistor-count circuit synthesis",
        "published": "2008-04-14T02:40:31Z",
        "link": "http://arxiv.org/abs/0804.2095v1",
        "categories": [
            "cs.LO",
            "cs.CE",
            "cs.DM",
            "cs.PL"
        ]
    },
    {
        "title": "From Qualitative to Quantitative Proofs of Security Properties Using   First-Order Conditional Logic",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "A first-order conditional logic is considered, with semantics given by a variant of epsilon-semantics, where p -> q means that Pr(q | p) approaches 1 super-polynomially --faster than any inverse polynomial. This type of convergence is needed for reasoning about security protocols. A complete axiomatization is provided for this semantics, and it is shown how a qualitative proof of the correctness of a security protocol can be automatically converted to a quantitative proof appropriate for reasoning about concrete security.",
        "published": "2008-04-14T12:06:04Z",
        "link": "http://arxiv.org/abs/0804.2155v1",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.LO",
            "D.4.6; I.2.3; I.2.4; F.4.1; K.6.5"
        ]
    },
    {
        "title": "Causal models have no complete axiomatic characterization",
        "authors": [
            "Sanjiang Li"
        ],
        "summary": "Markov networks and Bayesian networks are effective graphic representations of the dependencies embedded in probabilistic models. It is well known that independencies captured by Markov networks (called graph-isomorphs) have a finite axiomatic characterization. This paper, however, shows that independencies captured by Bayesian networks (called causal models) have no axiomatization by using even countably many Horn or disjunctive clauses. This is because a sub-independency model of a causal model may be not causal, while graph-isomorphs are closed under sub-models.",
        "published": "2008-04-15T14:28:34Z",
        "link": "http://arxiv.org/abs/0804.2401v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "On the Expressiveness and Complexity of ATL",
        "authors": [
            "Francois Laroussinie",
            "Nicolas Markey",
            "Ghassan Oreiby"
        ],
        "summary": "ATL is a temporal logic geared towards the specification and verification of properties in multi-agents systems. It allows to reason on the existence of strategies for coalitions of agents in order to enforce a given property. In this paper, we first precisely characterize the complexity of ATL model-checking over Alternating Transition Systems and Concurrent Game Structures when the number of agents is not fixed. We prove that it is \\Delta^P_2 - and \\Delta^P_?_3-complete, depending on the underlying multi-agent model (ATS and CGS resp.). We also consider the same problems for some extensions of ATL. We then consider expressiveness issues. We show how ATS and CGS are related and provide translations between these models w.r.t. alternating bisimulation. We also prove that the standard definition of ATL (built on modalities \"Next\", \"Always\" and \"Until\") cannot express the duals of its modalities: it is necessary to explicitely add the modality \"Release\".",
        "published": "2008-04-15T17:18:46Z",
        "link": "http://arxiv.org/abs/0804.2435v3",
        "categories": [
            "cs.LO",
            "cs.GT",
            "cs.MA",
            "F.1.1; F.3.1"
        ]
    },
    {
        "title": "Short proofs of strong normalization",
        "authors": [
            "Aleksander Wojdyga"
        ],
        "summary": "This paper presents simple, syntactic strong normalization proofs for the simply-typed lambda-calculus and the polymorphic lambda-calculus (system F) with the full set of logical connectives, and all the permutative reductions. The normalization proofs use translations of terms and types to systems, for which strong normalization property is known.",
        "published": "2008-04-16T07:09:59Z",
        "link": "http://arxiv.org/abs/0804.2535v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Generalized Modal Satisfiability",
        "authors": [
            "Edith Hemaspaandra",
            "Henning Schnoor",
            "Ilka Schnoor"
        ],
        "summary": "It is well known that modal satisfiability is PSPACE-complete (Ladner 1977). However, the complexity may decrease if we restrict the set of propositional operators used. Note that there exist an infinite number of propositional operators, since a propositional operator is simply a Boolean function. We completely classify the complexity of modal satisfiability for every finite set of propositional operators, i.e., in contrast to previous work, we classify an infinite number of problems. We show that, depending on the set of propositional operators, modal satisfiability is PSPACE-complete, coNP-complete, or in P. We obtain this trichotomy not only for modal formulas, but also for their more succinct representation using modal circuits. We consider both the uni-modal and the multi-modal case, and study the dual problem of validity as well.",
        "published": "2008-04-17T06:57:50Z",
        "link": "http://arxiv.org/abs/0804.2729v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Experiments in Model-Checking Optimistic Replication Algorithms",
        "authors": [
            "Hanifa Boucheneb",
            "Abdessamad Imine"
        ],
        "summary": "This paper describes a series of model-checking experiments to verify optimistic replication algorithms based on Operational Transformation (OT) approach used for supporting collaborative edition. We formally define, using tool UPPAAL, the behavior and the main consistency requirement (i.e. convergence property) of the collaborative editing systems, as well as the abstract behavior of the environment where these systems are supposed to operate. Due to data replication and the unpredictable nature of user interactions, such systems have infinitely many states. So, we show how to exploit some features of the UPPAAL specification language to attenuate the severe state explosion problem. Two models are proposed. The first one, called concrete model, is very close to the system implementation but runs up against a severe explosion of states. The second model, called symbolic model, aims to overcome the limitation of the concrete model by delaying the effective selection and execution of editing operations until the construction of symbolic execution traces of all sites is completed. Experimental results have shown that the symbolic model allows a significant gain in both space and time. Using the symbolic model, we have been able to show that if the number of sites exceeds 2 then the convergence property is not satisfied for all OT algorithms considered here. A counterexample is provided for every algorithm.",
        "published": "2008-04-18T14:04:38Z",
        "link": "http://arxiv.org/abs/0804.3023v2",
        "categories": [
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Visibly Tree Automata with Memory and Constraints",
        "authors": [
            "Hubert Comon-Lundh",
            "Florent Jacquemard",
            "Nicolas Perrin"
        ],
        "summary": "Tree automata with one memory have been introduced in 2001. They generalize both pushdown (word) automata and the tree automata with constraints of equality between brothers of Bogaert and Tison. Though it has a decidable emptiness problem, the main weakness of this model is its lack of good closure properties.   We propose a generalization of the visibly pushdown automata of Alur and Madhusudan to a family of tree recognizers which carry along their (bottom-up) computation an auxiliary unbounded memory with a tree structure (instead of a symbol stack). In other words, these recognizers, called Visibly Tree Automata with Memory (VTAM) define a subclass of tree automata with one memory enjoying Boolean closure properties. We show in particular that they can be determinized and the problems like emptiness, membership, inclusion and universality are decidable for VTAM. Moreover, we propose several extensions of VTAM whose transitions may be constrained by different kinds of tests between memories and also constraints a la Bogaert and Tison comparing brother subtrees in the tree in input. We show that some of these classes of constrained VTAM keep the good closure and decidability properties, and we demonstrate their expressiveness with relevant examples of tree languages.",
        "published": "2008-04-18T16:27:34Z",
        "link": "http://arxiv.org/abs/0804.3065v2",
        "categories": [
            "cs.LO",
            "F.1.1; F.1.2; I.2.2; I.2.3"
        ]
    },
    {
        "title": "A lower bound on web services composition",
        "authors": [
            "Anca Muscholl",
            "Igor Walukiewicz"
        ],
        "summary": "A web service is modeled here as a finite state machine. A composition problem for web services is to decide if a given web service can be constructed from a given set of web services; where the construction is understood as a simulation of the specification by a fully asynchronous product of the given services. We show an EXPTIME-lower bound for this problem, thus matching the known upper bound. Our result also applies to richer models of web services, such as the Roman model.",
        "published": "2008-04-18T21:15:47Z",
        "link": "http://arxiv.org/abs/0804.3105v2",
        "categories": [
            "cs.LO",
            "F.1.2; F.3.1"
        ]
    },
    {
        "title": "Differential Meadows",
        "authors": [
            "Jan A. Bergstra",
            "Alban Ponse"
        ],
        "summary": "A meadow is a zero totalised field (0^{-1}=0), and a cancellation meadow is a meadow without proper zero divisors. In this paper we consider differential meadows, i.e., meadows equipped with differentiation operators. We give an equational axiomatization of these operators and thus obtain a finite basis for differential cancellation meadows. Using the Zariski topology we prove the existence of a differential cancellation meadow.",
        "published": "2008-04-21T15:29:40Z",
        "link": "http://arxiv.org/abs/0804.3336v1",
        "categories": [
            "math.RA",
            "cs.LO",
            "math.AC",
            "AC; RA"
        ]
    },
    {
        "title": "On the Expressive Power of Multiple Heads in CHR",
        "authors": [
            "Cinzia Di Giusto",
            "Maurizio Gabbrielli",
            "Maria Chiara Meo"
        ],
        "summary": "Constraint Handling Rules (CHR) is a committed-choice declarative language which has been originally designed for writing constraint solvers and which is nowadays a general purpose language. CHR programs consist of multi-headed guarded rules which allow to rewrite constraints into simpler ones until a solved form is reached. Many empirical evidences suggest that multiple heads augment the expressive power of the language, however no formal result in this direction has been proved, so far.   In the first part of this paper we analyze the Turing completeness of CHR with respect to the underneath constraint theory. We prove that if the constraint theory is powerful enough then restricting to single head rules does not affect the Turing completeness of the language. On the other hand, differently from the case of the multi-headed language, the single head CHR language is not Turing powerful when the underlying signature (for the constraint theory) does not contain function symbols.   In the second part we prove that, no matter which constraint theory is considered, under some reasonable assumptions it is not possible to encode the CHR language (with multi-headed rules) into a single headed language while preserving the semantics of the programs. We also show that, under some stronger assumptions, considering an increasing number of atoms in the head of a rule augments the expressive power of the language.   These results provide a formal proof for the claim that multiple heads augment the expressive power of the CHR language.",
        "published": "2008-04-21T16:21:43Z",
        "link": "http://arxiv.org/abs/0804.3351v6",
        "categories": [
            "cs.LO",
            "cs.DC",
            "D.3.2; D.3.3; F.1.1; F.1.2; F.3.3"
        ]
    },
    {
        "title": "Wadge Degrees of Infinitary Rational Relations",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We show that, from the topological point of view, 2-tape B\\\"uchi automata have the same accepting power as Turing machines equipped with a B\\\"uchi acceptance condition. The Borel and the Wadge hierarchies of the class RAT_omega of infinitary rational relations accepted by 2-tape B\\\"uchi automata are equal to the Borel and the Wadge hierarchies of omega-languages accepted by real-time B\\\"uchi 1-counter automata or by B\\\"uchi Turing machines. In particular, for every non-null recursive ordinal $\\alpha$, there exist some $\\Sigma^0_\\alpha$-complete and some $\\Pi^0_\\alpha$-complete infinitary rational relations. And the supremum of the set of Borel ranks of infinitary rational relations is an ordinal $\\gamma^1_2$ which is strictly greater than the first non-recursive ordinal $\\omega_1^{CK}$. This very surprising result gives answers to questions of Simonnet (1992) and of Lescow and Thomas (1988,1994).",
        "published": "2008-04-21T19:12:44Z",
        "link": "http://arxiv.org/abs/0804.3266v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "Lecture notes on the lambda calculus",
        "authors": [
            "Peter Selinger"
        ],
        "summary": "This is a set of lecture notes that developed out of courses on the lambda calculus that I taught at the University of Ottawa in 2001 and at Dalhousie University in 2007 and 2013. Topics covered in these notes include the untyped lambda calculus, the Church-Rosser theorem, combinatory algebras, the simply-typed lambda calculus, the Curry-Howard isomorphism, weak and strong normalization, polymorphism, type inference, denotational semantics, complete partial orders, and the language PCF.",
        "published": "2008-04-22T03:16:03Z",
        "link": "http://arxiv.org/abs/0804.3434v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "From formal proofs to mathematical proofs: a safe, incremental way for   building in first-order decision procedures",
        "authors": [
            "Frédéric Blanqui",
            "Jean-Pierre Jouannaud",
            "Pierre-Yves Strub"
        ],
        "summary": "We investigate here a new version of the Calculus of Inductive Constructions (CIC) on which the proof assistant Coq is based: the Calculus of Congruent Inductive Constructions, which truly extends CIC by building in arbitrary first-order decision procedures: deduction is still in charge of the CIC kernel, while computation is outsourced to dedicated first-order decision procedures that can be taken from the shelves provided they deliver a proof certificate. The soundness of the whole system becomes an incremental property following from the soundness of the certificate checkers and that of the kernel. A detailed example shows that the resulting style of proofs becomes closer to that of the working mathematician.",
        "published": "2008-04-23T16:56:46Z",
        "link": "http://arxiv.org/abs/0804.3762v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Reasoning in Abella about Structural Operational Semantics   Specifications",
        "authors": [
            "Andrew Gacek",
            "Dale Miller",
            "Gopalan Nadathur"
        ],
        "summary": "The approach to reasoning about structural operational semantics style specifications supported by the Abella system is discussed. This approach uses lambda tree syntax to treat object language binding and encodes binding related properties in generic judgments. Further, object language specifications are embedded directly into the reasoning framework through recursive definitions. The treatment of binding via generic judgments implicitly enforces distinctness and atomicity in the names used for bound variables. These properties must, however, be made explicit in reasoning tasks. This objective can be achieved by allowing recursive definitions to also specify generic properties of atomic predicates. The utility of these various logical features in the Abella system is demonstrated through actual reasoning tasks. Brief comparisons with a few other logic based approaches are also made.",
        "published": "2008-04-24T15:22:02Z",
        "link": "http://arxiv.org/abs/0804.3914v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Logic Mining Using Neural Networks",
        "authors": [
            "Saratha Sathasivam",
            "Wan Ahmad Tajuddin Wan Abdullah"
        ],
        "summary": "Knowledge could be gained from experts, specialists in the area of interest, or it can be gained by induction from sets of data. Automatic induction of knowledge from data sets, usually stored in large databases, is called data mining. Data mining methods are important in the management of complex systems. There are many technologies available to data mining practitioners, including Artificial Neural Networks, Regression, and Decision Trees. Neural networks have been successfully applied in wide range of supervised and unsupervised learning applications. Neural network methods are not commonly used for data mining tasks, because they often produce incomprehensible models, and require long training times. One way in which the collective properties of a neural network may be used to implement a computational task is by way of the concept of energy minimization. The Hopfield network is well-known example of such an approach. The Hopfield network is useful as content addressable memory or an analog computer for solving combinatorial-type optimization problems. Wan Abdullah [1] proposed a method of doing logic programming on a Hopfield neural network. Optimization of logical inconsistency is carried out by the network after the connection strengths are defined from the logic program; the network relaxes to neural states corresponding to a valid interpretation. In this article, we describe how Hopfield network is able to induce logical rules from large database by using reverse analysis method: given the values of the connections of a network, we can hope to know what logical rules are entrenched in the database.",
        "published": "2008-04-25T09:30:28Z",
        "link": "http://arxiv.org/abs/0804.4071v1",
        "categories": [
            "cs.LO",
            "cs.NE"
        ]
    },
    {
        "title": "Grainy Numbers",
        "authors": [
            "Gilles Champenois"
        ],
        "summary": "Grainy numbers are defined as tuples of bits. They form a lattice where the meet and the join operations are an addition and a multiplication. They may be substituted for the real numbers in the definition of fuzzy sets. The aim is to propose an alternative negation for the complement that we'll call supplement.",
        "published": "2008-04-25T09:36:17Z",
        "link": "http://arxiv.org/abs/0804.4073v6",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Logic Learning in Hopfield Networks",
        "authors": [
            "Saratha Sathasivam",
            "Wan Ahmad Tajuddin Wan Abdullah"
        ],
        "summary": "Synaptic weights for neurons in logic programming can be calculated either by using Hebbian learning or by Wan Abdullah's method. In other words, Hebbian learning for governing events corresponding to some respective program clauses is equivalent with learning using Wan Abdullah's method for the same respective program clauses. In this paper we will evaluate experimentally the equivalence between these two types of learning through computer simulations.",
        "published": "2008-04-25T09:46:46Z",
        "link": "http://arxiv.org/abs/0804.4075v1",
        "categories": [
            "cs.LO",
            "cs.NE"
        ]
    },
    {
        "title": "Practical Automated Partial Verification of Multi-Paradigm Real-Time   Models",
        "authors": [
            "Carlo A. Furia",
            "Matteo Pradella",
            "Matteo Rossi"
        ],
        "summary": "This article introduces a fully automated verification technique that permits to analyze real-time systems described using a continuous notion of time and a mixture of operational (i.e., automata-based) and descriptive (i.e., logic-based) formalisms. The technique relies on the reduction, under reasonable assumptions, of the continuous-time verification problem to its discrete-time counterpart. This reconciles in a viable and effective way the dense/discrete and operational/descriptive dichotomies that are often encountered in practice when it comes to specifying and analyzing complex critical systems. The article investigates the applicability of the technique through a significant example centered on a communication protocol. More precisely, concurrent runs of the protocol are formalized by parallel instances of a Timed Automaton, while the synchronization rules between these instances are specified through Metric Temporal Logic formulas, thus creating a multi-paradigm model. Verification tests run on this model using a bounded validity checker implementing the technique show consistent results and interesting performances.",
        "published": "2008-04-28T12:04:12Z",
        "link": "http://arxiv.org/abs/0804.4383v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Strategy Improvement for Concurrent Safety Games",
        "authors": [
            "Krishnendu Chatterjee",
            "Luca de Alfaro",
            "Thomas A. Henzinger"
        ],
        "summary": "We consider concurrent games played on graphs. At every round of the game, each player simultaneously and independently selects a move; the moves jointly determine the transition to a successor state. Two basic objectives are the safety objective: ``stay forever in a set F of states'', and its dual, the reachability objective, ``reach a set R of states''. We present in this paper a strategy improvement algorithm for computing the value of a concurrent safety game, that is, the maximal probability with which player 1 can enforce the safety objective. The algorithm yields a sequence of player-1 strategies which ensure probabilities of winning that converge monotonically to the value of the safety game.   The significance of the result is twofold. First, while strategy improvement algorithms were known for Markov decision processes and turn-based games, as well as for concurrent reachability games, this is the first strategy improvement algorithm for concurrent safety games. Second, and most importantly, the improvement algorithm provides a way to approximate the value of a concurrent safety game from below (the known value-iteration algorithms approximate the value from above). Thus, when used together with value-iteration algorithms, or with strategy improvement algorithms for reachability games, our algorithm leads to the first practical algorithm for computing converging upper and lower bounds for the value of reachability and safety games.",
        "published": "2008-04-29T05:09:45Z",
        "link": "http://arxiv.org/abs/0804.4530v1",
        "categories": [
            "cs.GT",
            "cs.LO"
        ]
    },
    {
        "title": "Data linkage algebra, data linkage dynamics, and priority rewriting",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "We introduce an algebra of data linkages. Data linkages are intended for modelling the states of computations in which dynamic data structures are involved. We present a simple model of computation in which states of computations are modelled as data linkages and state changes take place by means of certain actions. We describe the state changes and replies that result from performing those actions by means of a term rewriting system with rule priorities. The model in question is an upgrade of molecular dynamics. The upgrading is mainly concerned with the features to deal with values and the features to reclaim garbage.",
        "published": "2008-04-29T09:55:33Z",
        "link": "http://arxiv.org/abs/0804.4565v4",
        "categories": [
            "cs.LO",
            "D.3.3; D.4.2; F.1.1; F.3.2; F.3.3"
        ]
    },
    {
        "title": "A Pseudo-Boolean Solution to the Maximum Quartet Consistency Problem",
        "authors": [
            "Antonio Morgado",
            "Joao Marques-Silva"
        ],
        "summary": "Determining the evolutionary history of a given biological data is an important task in biological sciences. Given a set of quartet topologies over a set of taxa, the Maximum Quartet Consistency (MQC) problem consists of computing a global phylogeny that satisfies the maximum number of quartets. A number of solutions have been proposed for the MQC problem, including Dynamic Programming, Constraint Programming, and more recently Answer Set Programming (ASP). ASP is currently the most efficient approach for optimally solving the MQC problem. This paper proposes encoding the MQC problem with pseudo-Boolean (PB) constraints. The use of PB allows solving the MQC problem with efficient PB solvers, and also allows considering different modeling approaches for the MQC problem. Initial results are promising, and suggest that PB can be an effective alternative for solving the MQC problem.",
        "published": "2008-05-02T10:06:27Z",
        "link": "http://arxiv.org/abs/0805.0202v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Alternating Automata on Data Trees and XPath Satisfiability",
        "authors": [
            "Marcin Jurdzinski",
            "Ranko Lazic"
        ],
        "summary": "A data tree is an unranked ordered tree whose every node is labelled by a letter from a finite alphabet and an element (\"datum\") from an infinite set, where the latter can only be compared for equality. The article considers alternating automata on data trees that can move downward and rightward, and have one register for storing data. The main results are that nonemptiness over finite data trees is decidable but not primitive recursive, and that nonemptiness of safety automata is decidable but not elementary. The proofs use nondeterministic tree automata with faulty counters. Allowing upward moves, leftward moves, or two registers, each causes undecidability. As corollaries, decidability is obtained for two data-sensitive fragments of the XPath query language.",
        "published": "2008-05-03T00:12:15Z",
        "link": "http://arxiv.org/abs/0805.0330v4",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.FL",
            "F.4.1; F.1.1; H.2.3"
        ]
    },
    {
        "title": "The Tractability of Model-Checking for LTL: The Good, the Bad, and the   Ugly Fragments",
        "authors": [
            "Michael Bauland",
            "Martin Mundhenk",
            "Thomas Schneider",
            "Henning Schnoor",
            "Ilka Schnoor",
            "Heribert Vollmer"
        ],
        "summary": "In a seminal paper from 1985, Sistla and Clarke showed that the model-checking problem for Linear Temporal Logic (LTL) is either NP-complete or PSPACE-complete, depending on the set of temporal operators used. If, in contrast, the set of propositional operators is restricted, the complexity may decrease. This paper systematically studies the model-checking problem for LTL formulae over restricted sets of propositional and temporal operators. For almost all combinations of temporal and propositional operators, we determine whether the model-checking problem is tractable (in P) or intractable (NP-hard). We then focus on the tractable cases, showing that they all are NL-complete or even logspace solvable. This leads to a surprising gap in complexity between tractable and intractable cases. It is worth noting that our analysis covers an infinite set of problems, since there are infinitely many sets of propositional operators.",
        "published": "2008-05-05T09:48:23Z",
        "link": "http://arxiv.org/abs/0805.0498v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; I.2.4"
        ]
    },
    {
        "title": "Discrete Mathematics for Computer Science, Some Notes",
        "authors": [
            "Jean Gallier"
        ],
        "summary": "These are notes on discrete mathematics for computer scientists. The presentation is somewhat unconventional. Indeed I begin with a discussion of the basic rules of mathematical reasoning and of the notion of proof formalized in a natural deduction system ``a la Prawitz''. The rest of the material is more or less traditional but I emphasize partial functions more than usual (after all, programs may not terminate for all input) and I provide a fairly complete account of the basic concepts of graph theory.",
        "published": "2008-05-05T18:52:00Z",
        "link": "http://arxiv.org/abs/0805.0585v1",
        "categories": [
            "cs.DM",
            "cs.LO",
            "F.2.2; F.1.1"
        ]
    },
    {
        "title": "Relational Parametricity and Separation Logic",
        "authors": [
            "Lars Birkedal",
            "Hongseok Yang"
        ],
        "summary": "Separation logic is a recent extension of Hoare logic for reasoning about programs with references to shared mutable data structures. In this paper, we provide a new interpretation of the logic for a programming language with higher types. Our interpretation is based on Reynolds's relational parametricity, and it provides a formal connection between separation logic and data abstraction.",
        "published": "2008-05-06T19:27:14Z",
        "link": "http://arxiv.org/abs/0805.0783v2",
        "categories": [
            "cs.LO",
            "F.3; D.3"
        ]
    },
    {
        "title": "Presentation of a Game Semantics for First-Order Propositional Logic",
        "authors": [
            "Samuel Mimram"
        ],
        "summary": "Game semantics aim at describing the interactive behaviour of proofs by interpreting formulas as games on which proofs induce strategies. In this article, we introduce a game semantics for a fragment of first order propositional logic. One of the main difficulties that has to be faced when constructing such semantics is to make them precise by characterizing definable strategies - that is strategies which actually behave like a proof. This characterization is usually done by restricting to the model to strategies satisfying subtle combinatory conditions such as innocence, whose preservation under composition is often difficult to show. Here, we present an original methodology to achieve this task which requires to combine tools from game semantics, rewriting theory and categorical algebra. We introduce a diagrammatic presentation of definable strategies by the means of generators and relations: those strategies can be generated from a finite set of ``atomic'' strategies and that the equality between strategies generated in such a way admits a finite axiomatization. These generators satisfy laws which are a variation of bialgebras laws, thus bridging algebra and denotational semantics in a clean and unexpected way.",
        "published": "2008-05-07T06:53:47Z",
        "link": "http://arxiv.org/abs/0805.0845v1",
        "categories": [
            "cs.LO",
            "math.CT",
            "math.LO"
        ]
    },
    {
        "title": "A language for mathematical knowledge management",
        "authors": [
            "Steven Kieffer",
            "Jeremy Avigad",
            "Harvey Friedman"
        ],
        "summary": "We argue that the language of Zermelo Fraenkel set theory with definitions and partial functions provides the most promising bedrock semantics for communicating and sharing mathematical knowledge. We then describe a syntactic sugaring of that language that provides a way of writing remarkably readable assertions without straying far from the set-theoretic semantics. We illustrate with some examples of formalized textbook definitions from elementary set theory and point-set topology. We also present statistics concerning the complexity of these definitions, under various complexity measures.",
        "published": "2008-05-09T17:16:50Z",
        "link": "http://arxiv.org/abs/0805.1386v3",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.4"
        ]
    },
    {
        "title": "Linear Time Algorithm for Weak Parity Games",
        "authors": [
            "Krishnendu Chatterjee"
        ],
        "summary": "We consider games played on graphs with the winning conditions for the players specified as weak-parity conditions. In weak-parity conditions the winner of a play is decided by looking into the set of states appearing in the play, rather than the set of states appearing infinitely often in the play. A naive analysis of the classical algorithm for weak-parity games yields a quadratic time algorithm. We present a linear time algorithm for solving weak-parity games.",
        "published": "2008-05-09T19:12:30Z",
        "link": "http://arxiv.org/abs/0805.1391v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Model Checking One-clock Priced Timed Automata",
        "authors": [
            "Patricia Bouyer",
            "Kim G. Larsen",
            "Nicolas Markey"
        ],
        "summary": "We consider the model of priced (a.k.a. weighted) timed automata, an extension of timed automata with cost information on both locations and transitions, and we study various model-checking problems for that model based on extensions of classical temporal logics with cost constraints on modalities. We prove that, under the assumption that the model has only one clock, model-checking this class of models against the logic WCTL, CTL with cost-constrained modalities, is PSPACE-complete (while it has been shown undecidable as soon as the model has three clocks). We also prove that model-checking WMTL, LTL with cost-constrained modalities, is decidable only if there is a single clock in the model and a single stopwatch cost variable (i.e., whose slopes lie in {0,1}).",
        "published": "2008-05-10T08:56:07Z",
        "link": "http://arxiv.org/abs/0805.1457v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.GT",
            "F.1.1; F.3.1"
        ]
    },
    {
        "title": "Efficiently Simulating Higher-Order Arithmetic by a First-Order Theory   Modulo",
        "authors": [
            "Guillaume Burel"
        ],
        "summary": "In deduction modulo, a theory is not represented by a set of axioms but by a congruence on propositions modulo which the inference rules of standard deductive systems---such as for instance natural deduction---are applied. Therefore, the reasoning that is intrinsic of the theory does not appear in the length of proofs. In general, the congruence is defined through a rewrite system over terms and propositions. We define a rigorous framework to study proof lengths in deduction modulo, where the congruence must be computed in polynomial time. We show that even very simple rewrite systems lead to arbitrary proof-length speed-ups in deduction modulo, compared to using axioms. As higher-order logic can be encoded as a first-order theory in deduction modulo, we also study how to reinterpret, thanks to deduction modulo, the speed-ups between higher-order and first-order arithmetics that were stated by G\\\"odel. We define a first-order rewrite system with a congruence decidable in polynomial time such that proofs of higher-order arithmetic can be linearly translated into first-order arithmetic modulo that system. We also present the whole higher-order arithmetic as a first-order system without resorting to any axiom, where proofs have the same length as in the axiomatic presentation.",
        "published": "2008-05-10T10:42:54Z",
        "link": "http://arxiv.org/abs/0805.1464v5",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.CC"
        ]
    },
    {
        "title": "A Fast Algorithm and Datalog Inexpressibility for Temporal Reasoning",
        "authors": [
            "Manuel Bodirsky",
            "Jan Kara"
        ],
        "summary": "We introduce a new tractable temporal constraint language, which strictly contains the Ord-Horn language of Buerkert and Nebel and the class of AND/OR precedence constraints. The algorithm we present for this language decides whether a given set of constraints is consistent in time that is quadratic in the input size. We also prove that (unlike Ord-Horn) this language cannot be solved by Datalog or by establishing local consistency.",
        "published": "2008-05-10T13:49:45Z",
        "link": "http://arxiv.org/abs/0805.1473v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4"
        ]
    },
    {
        "title": "(Mechanical) Reasoning on Infinite Extensive Games",
        "authors": [
            "Pierre Lescanne"
        ],
        "summary": "In order to better understand reasoning involved in analyzing infinite games in extensive form, we performed experiments in the proof assistant Coq that are reported here.",
        "published": "2008-05-13T08:09:51Z",
        "link": "http://arxiv.org/abs/0805.1798v4",
        "categories": [
            "cs.GT",
            "cs.LO"
        ]
    },
    {
        "title": "Tuplix Calculus Specifications of Financial Transfer Networks",
        "authors": [
            "J. A. Bergstra",
            "S. Nolst Trenite",
            "M. B. van der Zwaag"
        ],
        "summary": "We study the application of Tuplix Calculus in modular financial budget design. We formalize organizational structure using financial transfer networks. We consider the notion of flux of money over a network, and a way to enforce the matching of influx and outflux for parts of a network. We exploit so-called signed attribute notation to make internal streams visible through encapsulations. Finally, we propose a Tuplix Calculus construct for the definition of data functions.",
        "published": "2008-05-13T09:05:41Z",
        "link": "http://arxiv.org/abs/0805.1806v1",
        "categories": [
            "cs.CE",
            "cs.LO"
        ]
    },
    {
        "title": "Lower Bound for the Communication Complexity of the Russian Cards   Problem",
        "authors": [
            "Aiswarya Cyriac",
            "K. Murali Krishnan"
        ],
        "summary": "In this paper it is shown that no public announcement scheme that can be modeled in Dynamic Epistemic Logic (DEL) can solve the Russian Cards Problem (RCP) in one announcement. Since DEL is a general model for any public announcement scheme we conclude that there exist no single announcement solution to the RCP. The proof demonstrates the utility of DEL in proving lower bounds for communication protocols. It is also shown that a general version of RCP has no two announcement solution when the adversary has sufficiently large number of cards.",
        "published": "2008-05-14T06:31:48Z",
        "link": "http://arxiv.org/abs/0805.1974v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Algorithms and Bounds for Rollout Sampling Approximate Policy Iteration",
        "authors": [
            "Christos Dimitrakakis",
            "Michail G. Lagoudakis"
        ],
        "summary": "Several approximate policy iteration schemes without value functions, which focus on policy representation using classifiers and address policy learning as a supervised learning problem, have been proposed recently. Finding good policies with such methods requires not only an appropriate classifier, but also reliable examples of best actions, covering the state space sufficiently. Up to this time, little work has been done on appropriate covering schemes and on methods for reducing the sample complexity of such methods, especially in continuous state spaces. This paper focuses on the simplest possible covering scheme (a discretized grid over the state space) and performs a sample-complexity comparison between the simplest (and previously commonly used) rollout sampling allocation strategy, which allocates samples equally at each state under consideration, and an almost as simple method, which allocates samples only as needed and requires significantly fewer samples.",
        "published": "2008-05-14T11:20:29Z",
        "link": "http://arxiv.org/abs/0805.2015v2",
        "categories": [
            "stat.ML",
            "cs.LO",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Replication via Invalidating the Applicability of the Fixed Point   Theorem",
        "authors": [
            "Genta Ito"
        ],
        "summary": "We present a construction of a certain infinite complete partial order (CPO) that differs from the standard construction used in Scott's denotational semantics. In addition, we construct several other infinite CPO's. For some of those, we apply the usual Fixed Point Theorem (FPT) to yield a fixed point for every continuous function $\\mu:2\\to 2$ (where 2 denotes the set $\\{0,1\\}$), while for the other CPO's we cannot invoke that theorem to yield such fixed points. Every element of each of these CPO's is a binary string in the monotypic form and we show that invalidation of the applicability of the FPT to the CPO that Scott's constructed yields the concept of replication.",
        "published": "2008-05-14T13:45:16Z",
        "link": "http://arxiv.org/abs/0805.2063v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Mnesors for databases",
        "authors": [
            "Gilles Champenois"
        ],
        "summary": "We add commutativity to axioms defining mnesors and substitute a bitrop for the lattice. We show that it can be applied to relational database querying: set union, intersection and selection are redifined only from the mnesor addition and the granular multiplication. Union-compatibility is not required.",
        "published": "2008-05-14T22:01:31Z",
        "link": "http://arxiv.org/abs/0805.2179v4",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Certified Exact Transcendental Real Number Computation in Coq",
        "authors": [
            "Russell O'Connor"
        ],
        "summary": "Reasoning about real number expressions in a proof assistant is challenging. Several problems in theorem proving can be solved by using exact real number computation. I have implemented a library for reasoning and computing with complete metric spaces in the Coq proof assistant and used this library to build a constructive real number implementation including elementary real number functions and proofs of correctness. Using this library, I have created a tactic that automatically proves strict inequalities over closed elementary real number expressions by computation.",
        "published": "2008-05-16T18:02:24Z",
        "link": "http://arxiv.org/abs/0805.2438v1",
        "categories": [
            "cs.LO",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "Algorithms for Büchi Games",
        "authors": [
            "Krishnendu Chatterjee",
            "Thomas A. Henzinger",
            "Nir Piterman"
        ],
        "summary": "The classical algorithm for solving B\\\"uchi games requires time $O(n\\cdot m)$ for game graphs with $n$ states and $m$ edges. For game graphs with constant outdegree, the best known algorithm has running time $O(n^2/\\log n)$. We present two new algorithms for B\\\"uchi games. First, we give an algorithm that performs at most $O(m)$ more work than the classical algorithm, but runs in time O(n) on infinitely many graphs of constant outdegree on which the classical algorithm requires time $O(n^2)$. Second, we give an algorithm with running time $O(n\\cdot m\\cdot\\log\\delta(n)/\\log n)$, where $1\\le\\delta(n)\\le n$ is the outdegree of the game graph. Note that this algorithm performs asymptotically better than the classical algorithm if $\\delta(n)=O(\\log n)$.",
        "published": "2008-05-16T20:38:54Z",
        "link": "http://arxiv.org/abs/0805.2620v1",
        "categories": [
            "cs.GT",
            "cs.LO"
        ]
    },
    {
        "title": "Proof Search Specifications of Bisimulation and Modal Logics for the   pi-Calculus",
        "authors": [
            "Alwen Tiu",
            "Dale Miller"
        ],
        "summary": "We specify the operational semantics and bisimulation relations for the finite pi-calculus within a logic that contains the nabla quantifier for encoding generic judgments and definitions for encoding fixed points. Since we restrict to the finite case, the ability of the logic to unfold fixed points allows this logic to be complete for both the inductive nature of operational semantics and the coinductive nature of bisimulation. The nabla quantifier helps with the delicate issues surrounding the scope of variables within pi-calculus expressions and their executions (proofs). We illustrate several merits of the logical specifications permitted by this logic: they are natural and declarative; they contain no side-conditions concerning names of variables while maintaining a completely formal treatment of such variables; differences between late and open bisimulation relations arise from familar logic distinctions; the interplay between the three quantifiers (for all, exists, and nabla) and their scopes can explain the differences between early and late bisimulation and between various modal operators based on bound input and output actions; and proof search involving the application of inference rules, unification, and backtracking can provide complete proof systems for one-step transitions, bisimulation, and satisfaction in modal logic. We also illustrate how one can encode the pi-calculus with replications, in an extended logic with induction and co-induction.",
        "published": "2008-05-19T04:33:28Z",
        "link": "http://arxiv.org/abs/0805.2785v3",
        "categories": [
            "cs.LO",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Model Checking Event-B by Encoding into Alloy",
        "authors": [
            "Paulo J. Matos",
            "Joao Marques-Silva"
        ],
        "summary": "As systems become ever more complex, verification becomes more main stream. Event-B and Alloy are two formal specification languages based on fairly different methodologies. While Event-B uses theorem provers to prove that invariants hold for a given specification, Alloy uses a SAT-based model finder. In some settings, Event-B invariants may not be proved automatically, and so the often difficult step of interactive proof is required. One solution for this problem is to validate invariants with model checking. This work studies the encoding of Event-B machines and contexts to Alloy in order to perform temporal model checking with Alloy's SAT-based engine.",
        "published": "2008-05-21T11:35:25Z",
        "link": "http://arxiv.org/abs/0805.3256v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "k-Hyperarc Consistency for Soft Constraints over Divisible Residuated   Lattices",
        "authors": [
            "Simone Bova"
        ],
        "summary": "We investigate the applicability of divisible residuated lattices (DRLs) as a general evaluation framework for soft constraint satisfaction problems (soft CSPs). DRLs are in fact natural candidates for this role, since they form the algebraic semantics of a large family of substructural and fuzzy logics.   We present the following results. (i) We show that DRLs subsume important valuation structures for soft constraints, such as commutative idempotent semirings and fair valuation structures, in the sense that the last two are members of certain subvarieties of DRLs (namely, Heyting algebras and BL-algebras respectively). (ii) In the spirit of previous work of J. Larrosa and T. Schiex [2004], and S. Bistarelli and F. Gadducci [2006] we describe a polynomial-time algorithm that enforces k-hyperarc consistency on soft CSPs evaluated over DRLs. Observed that, in general, DRLs are neither idempotent nor totally ordered, this algorithm amounts to a generalization of the available algorithms that enforce k-hyperarc consistency.",
        "published": "2008-05-21T11:54:51Z",
        "link": "http://arxiv.org/abs/0805.3261v2",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Enriched MU-Calculi Module Checking",
        "authors": [
            "Alessandro Ferrante",
            "Aniello Murano",
            "Mimmo Parente"
        ],
        "summary": "The model checking problem for open systems has been intensively studied in the literature, for both finite-state (module checking) and infinite-state (pushdown module checking) systems, with respect to Ctl and Ctl*. In this paper, we further investigate this problem with respect to the \\mu-calculus enriched with nominals and graded modalities (hybrid graded Mu-calculus), in both the finite-state and infinite-state settings. Using an automata-theoretic approach, we show that hybrid graded \\mu-calculus module checking is solvable in exponential time, while hybrid graded \\mu-calculus pushdown module checking is solvable in double-exponential time. These results are also tight since they match the known lower bounds for Ctl. We also investigate the module checking problem with respect to the hybrid graded \\mu-calculus enriched with inverse programs (Fully enriched \\mu-calculus): by showing a reduction from the domino problem, we show its undecidability. We conclude with a short overview of the model checking problem for the Fully enriched Mu-calculus and the fragments obtained by dropping at least one of the additional constructs.",
        "published": "2008-05-22T13:29:44Z",
        "link": "http://arxiv.org/abs/0805.3462v2",
        "categories": [
            "cs.LO",
            "F.1.1; F.1.2; F.3.1; D.2.4"
        ]
    },
    {
        "title": "Towards applied theories based on computability logic",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Computability logic (CL) (see http://www.cis.upenn.edu/~giorgi/cl.html) is a recently launched program for redeveloping logic as a formal theory of computability, as opposed to the formal theory of truth that logic has more traditionally been. Formulas in it represent computational problems, \"truth\" means existence of an algorithmic solution, and proofs encode such solutions. Within the line of research devoted to finding axiomatizations for ever more expressive fragments of CL, the present paper introduces a new deductive system CL12 and proves its soundness and completeness with respect to the semantics of CL. Conservatively extending classical predicate calculus and offering considerable additional expressive and deductive power, CL12 presents a reasonable, computationally meaningful, constructive alternative to classical logic as a basis for applied theories. To obtain a model example of such theories, this paper rebuilds the traditional, classical-logic-based Peano arithmetic into a computability-logic-based counterpart. Among the purposes of the present contribution is to provide a starting point for what, as the author wishes to hope, might become a new line of research with a potential of interesting findings -- an exploration of the presumably quite unusual metatheory of CL-based arithmetic and other CL-based applied systems.",
        "published": "2008-05-22T18:18:02Z",
        "link": "http://arxiv.org/abs/0805.3521v4",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "math.NT",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Extensional Uniformity for Boolean Circuits",
        "authors": [
            "Pierre McKenzie",
            "Michael Thomas",
            "Heribert Vollmer"
        ],
        "summary": "Imposing an extensional uniformity condition on a non-uniform circuit complexity class C means simply intersecting C with a uniform class L. By contrast, the usual intensional uniformity conditions require that a resource-bounded machine be able to exhibit the circuits in the circuit family defining C. We say that (C,L) has the \"Uniformity Duality Property\" if the extensionally uniform class C \\cap L can be captured intensionally by means of adding so-called \"L-numerical predicates\" to the first-order descriptive complexity apparatus describing the connection language of the circuit family defining C.   This paper exhibits positive instances and negative instances of the Uniformity Duality Property.",
        "published": "2008-05-27T08:49:36Z",
        "link": "http://arxiv.org/abs/0805.4072v2",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Environment Assumptions for Synthesis",
        "authors": [
            "Krishnendu Chatterjee",
            "Thomas A. Henzinger",
            "Barbara Jobstmann"
        ],
        "summary": "The synthesis problem asks to construct a reactive finite-state system from an $\\omega$-regular specification. Initial specifications are often unrealizable, which means that there is no system that implements the specification. A common reason for unrealizability is that assumptions on the environment of the system are incomplete. We study the problem of correcting an unrealizable specification $\\phi$ by computing an environment assumption $\\psi$ such that the new specification $\\psi\\to\\phi$ is realizable. Our aim is to construct an assumption $\\psi$ that constrains only the environment and is as weak as possible. We present a two-step algorithm for computing assumptions. The algorithm operates on the game graph that is used to answer the realizability question. First, we compute a safety assumption that removes a minimal set of environment edges from the graph. Second, we compute a liveness assumption that puts fairness conditions on some of the remaining environment edges. We show that the problem of finding a minimal set of fair edges is computationally hard, and we use probabilistic games to compute a locally minimal fairness assumption.",
        "published": "2008-05-27T16:17:34Z",
        "link": "http://arxiv.org/abs/0805.4167v1",
        "categories": [
            "cs.GT",
            "cs.LO"
        ]
    },
    {
        "title": "Pattern-based Model-to-Model Transformation: Long Version",
        "authors": [
            "Juan de Lara",
            "Esther Guerra"
        ],
        "summary": "We present a new, high-level approach for the specification of model-to-model transformations based on declarative patterns. These are (atomic or composite) constraints on triple graphs declaring the allowed or forbidden relationships between source and target models. In this way, a transformation is defined by specifying a set of triple graph constraints that should be satisfied by the result of the transformation.   The description of the transformation is then compiled into lower-level operational mechanisms to perform forward or backward transformations, as well as to establish mappings between two existent models. In this paper we study one of such mechanisms based on the generation of operational triple graph grammar rules. Moreover, we exploit deduction techniques at the specification level to generate more specialized constraints (preserving the specification semantics) reflecting pattern dependencies, from which additional rules can be derived.   This is an extended version of the paper submitted to ICGT'08, with additional definitions and proofs.",
        "published": "2008-05-30T12:48:16Z",
        "link": "http://arxiv.org/abs/0805.4745v1",
        "categories": [
            "cs.SE",
            "cs.DM",
            "cs.LO"
        ]
    },
    {
        "title": "Canonical calculi with (n,k)-ary quantifiers",
        "authors": [
            "Arnon Avron",
            "Anna Zamansky"
        ],
        "summary": "Propositional canonical Gentzen-type systems, introduced in 2001 by Avron and Lev, are systems which in addition to the standard axioms and structural rules have only logical rules in which exactly one occurrence of a connective is introduced and no other connective is mentioned. A constructive coherence criterion for the non-triviality of such systems was defined and it was shown that a system of this kind admits cut-elimination iff it is coherent. The semantics of such systems is provided using two-valued non-deterministic matrices (2Nmatrices). In 2005 Zamansky and Avron extended these results to systems with unary quantifiers of a very restricted form. In this paper we substantially extend the characterization of canonical systems to (n,k)-ary quantifiers, which bind k distinct variables and connect n formulas, and show that the coherence criterion remains constructive for such systems. Then we focus on the case of k&#8712;{0,1} and for a canonical calculus G show that it is coherent precisely when it has a strongly characteristic 2Nmatrix, which in turn is equivalent to admitting strong cut-elimination.",
        "published": "2008-05-31T15:35:40Z",
        "link": "http://arxiv.org/abs/0806.0081v4",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "A note on clique-width and tree-width for structures",
        "authors": [
            "Hans Adler",
            "Isolde Adler"
        ],
        "summary": "We give a simple proof that the straightforward generalisation of clique-width to arbitrary structures can be unbounded on structures of bounded tree-width. This can be corrected by allowing fusion of elements.",
        "published": "2008-06-02T10:41:48Z",
        "link": "http://arxiv.org/abs/0806.0103v2",
        "categories": [
            "cs.LO",
            "F.4.1; G.2.2"
        ]
    },
    {
        "title": "Checking the Quality of Clinical Guidelines using Automated Reasoning   Tools",
        "authors": [
            "Arjen Hommersom",
            "Peter J. F. Lucas",
            "Patrick van Bommel"
        ],
        "summary": "Requirements about the quality of clinical guidelines can be represented by schemata borrowed from the theory of abductive diagnosis, using temporal logic to model the time-oriented aspects expressed in a guideline. Previously, we have shown that these requirements can be verified using interactive theorem proving techniques. In this paper, we investigate how this approach can be mapped to the facilities of a resolution-based theorem prover, Otter, and a complementary program that searches for finite models of first-order statements, Mace. It is shown that the reasoning required for checking the quality of a guideline can be mapped to such fully automated theorem-proving facilities. The medical quality of an actual guideline concerning diabetes mellitus 2 is investigated in this way.",
        "published": "2008-06-02T11:02:40Z",
        "link": "http://arxiv.org/abs/0806.0250v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Metric Structures and Probabilistic Computation",
        "authors": [
            "Wesley Calvert"
        ],
        "summary": "Continuous first-order logic is used to apply model-theoretic analysis to analytic structures (e.g. Hilbert spaces, Banach spaces, probability spaces, etc.). Classical computable model theory is used to examine the algorithmic structure of mathematical objects that can be described in classical first-order logic. The present paper shows that probabilistic computation (sometimes called randomized computation) can play an analogous role for structures described in continuous first-order logic. The main result of this paper is an effective completeness theorem, showing that every decidable continuous first-order theory has a probabilistically decidable model. Later sections give examples of the application of this framework to various classes of structures, and to some problems of computational complexity theory.",
        "published": "2008-06-02T21:15:00Z",
        "link": "http://arxiv.org/abs/0806.0398v1",
        "categories": [
            "math.LO",
            "cs.LO",
            "math.FA"
        ]
    },
    {
        "title": "On convergence-sensitive bisimulation and the embedding of CCS in timed   CCS",
        "authors": [
            "Roberto Amadio"
        ],
        "summary": "We propose a notion of convergence-sensitive bisimulation that is built just over the notions of (internal) reduction and of (static) context. In the framework of timed CCS, we characterise this notion of `contextual' bisimulation via the usual labelled transition system. We also remark that it provides a suitable semantic framework for a fully abstract embedding of untimed processes into timed ones. Finally, we show that the notion can be refined to include sensitivity to divergence.",
        "published": "2008-06-05T09:32:07Z",
        "link": "http://arxiv.org/abs/0806.0936v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Significant Diagnostic Counterexamples in Probabilistic Model Checking",
        "authors": [
            "Miguel E. Andres",
            "Pedro D'Argenio",
            "Peter van Rossum"
        ],
        "summary": "This paper presents a novel technique for counterexample generation in probabilistic model checking of Markov Chains and Markov Decision Processes. (Finite) paths in counterexamples are grouped together in witnesses that are likely to provide similar debugging information to the user. We list five properties that witnesses should satisfy in order to be useful as debugging aid: similarity, accuracy, originality, significance, and finiteness. Our witnesses contain paths that behave similar outside strongly connected components.   This papers shows how to compute these witnesses by reducing the problem of generating counterexamples for general properties over Markov Decision Processes, in several steps, to the easy problem of generating counterexamples for reachability properties over acyclic Markov Chains.",
        "published": "2008-06-06T13:09:49Z",
        "link": "http://arxiv.org/abs/0806.1139v1",
        "categories": [
            "cs.LO",
            "cs.PF",
            "B.8; C.4; D.2.4; G.3"
        ]
    },
    {
        "title": "Extracting Programs from Constructive HOL Proofs via IZF   Set-Theoretic<br> Semantics",
        "authors": [
            "Robert Constable",
            "Wojciech Moczydlowski"
        ],
        "summary": "Church's Higher Order Logic is a basis for influential proof assistants -- HOL and PVS. Church's logic has a simple set-theoretic semantics, making it trustworthy and extensible. We factor HOL into a constructive core plus axioms of excluded middle and choice. We similarly factor standard set theory, ZFC, into a constructive core, IZF, and axioms of excluded middle and choice. Then we provide the standard set-theoretic semantics in such a way that the constructive core of HOL is mapped into IZF. We use the disjunction, numerical existence and term existence properties of IZF to provide a program extraction capability from proofs in the constructive core.   We can implement the disjunction and numerical existence properties in two different ways: one using Rathjen's realizability for IZF and the other using a new direct weak normalization result for IZF by Moczydlowski. The latter can also be used for the term existence property.",
        "published": "2008-06-07T13:43:46Z",
        "link": "http://arxiv.org/abs/0806.1281v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "The Separation of Duty with Privilege Calculus",
        "authors": [
            "Chenggong Lv",
            "Jun Wang",
            "Lu Liu",
            "Weijia You"
        ],
        "summary": "This paper presents Privilege Calculus (PC) as a new approach of knowledge representation for Separation of Duty (SD) in the view of process and intents to improve the reconfigurability and traceability of SD. PC presumes that the structure of SD should be reduced to the structure of privilege and then the regulation of system should be analyzed with the help of forms of privilege.",
        "published": "2008-06-07T15:13:45Z",
        "link": "http://arxiv.org/abs/0806.1284v1",
        "categories": [
            "cs.CR",
            "cs.LO"
        ]
    },
    {
        "title": "Topological Complexity of Context-Free omega-Languages: A Survey",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "We survey recent results on the topological complexity of context-free omega-languages which form the second level of the Chomsky hierarchy of languages of infinite words. In particular, we consider the Borel hierarchy and the Wadge hierarchy of non-deterministic or deterministic context-free omega-languages. We study also decision problems, the links with the notions of ambiguity and of degrees of ambiguity, and the special case of omega-powers.",
        "published": "2008-06-09T10:03:44Z",
        "link": "http://arxiv.org/abs/0806.1413v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "The injectivity of the global function of a cellular automaton in the   hyperbolic plane is undecidable",
        "authors": [
            "Margenstern Maurice"
        ],
        "summary": "In this paper, we look at the following question. We consider cellular automata in the hyperbolic plane and we consider the global function defined on all possible configurations. Is the injectivity of this function undecidable? The problem was answered positively in the case of the Euclidean plane by Jarkko Kari, in 1994. In the present paper, we show that the answer is also positive for the hyperbolic plane: the problem is undecidable.",
        "published": "2008-06-10T10:58:30Z",
        "link": "http://arxiv.org/abs/0806.1602v2",
        "categories": [
            "cs.CG",
            "cs.LO",
            "F.2.2"
        ]
    },
    {
        "title": "Data-Complexity of the Two-Variable Fragment with Counting Quantifiers",
        "authors": [
            "Ian Pratt-Hartmann"
        ],
        "summary": "The data-complexity of both satisfiability and finite satisfiability for the two-variable fragment with counting is NP-complete; the data-complexity of both query-answering and finite query-answering for the two-variable guarded fragment with counting is co-NP-complete.",
        "published": "2008-06-10T11:08:07Z",
        "link": "http://arxiv.org/abs/0806.1636v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Consistency and Completeness of Rewriting in the Calculus of   Constructions",
        "authors": [
            "Daria Walukiewicz-Chrzaszcz",
            "Jacek Chrzaszcz"
        ],
        "summary": "Adding rewriting to a proof assistant based on the Curry-Howard isomorphism, such as Coq, may greatly improve usability of the tool. Unfortunately adding an arbitrary set of rewrite rules may render the underlying formal system undecidable and inconsistent. While ways to ensure termination and confluence, and hence decidability of type-checking, have already been studied to some extent, logical consistency has got little attention so far. In this paper we show that consistency is a consequence of canonicity, which in turn follows from the assumption that all functions defined by rewrite rules are complete. We provide a sound and terminating, but necessarily incomplete algorithm to verify this property. The algorithm accepts all definitions that follow dependent pattern matching schemes presented by Coquand and studied by McBride in his PhD thesis. It also accepts many definitions by rewriting, containing rules which depart from standard pattern matching.",
        "published": "2008-06-10T20:27:28Z",
        "link": "http://arxiv.org/abs/0806.1749v3",
        "categories": [
            "cs.LO",
            "cs.SC",
            "F.4.1; F.4.2"
        ]
    },
    {
        "title": "Full Abstraction for a Recursively Typed Lambda Calculus with Parallel   Conditional",
        "authors": [
            "Fritz Müller"
        ],
        "summary": "We define the syntax and reduction relation of a recursively typed lambda calculus with a parallel case-function (a parallel conditional). The reduction is shown to be confluent. We interpret the recursive types as information systems in a restricted form, which we call prime systems. A denotational semantics is defined with this interpretation. We define the syntactical normal form approximations of a term and prove the Approximation Theorem: The semantics of a term equals the limit of the semantics of its approximations. The proof uses inclusive predicates (logical relations). The semantics is adequate with respect to the observation of Boolean values. It is also fully abstract in the presence of the parallel case-function.",
        "published": "2008-06-11T15:29:47Z",
        "link": "http://arxiv.org/abs/0806.1827v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Effective lambda-models vs recursively enumerable lambda-theories",
        "authors": [
            "Chantal Berline",
            "Giulio Manzonetto",
            "Antonio Salibra"
        ],
        "summary": "A longstanding open problem is whether there exists a non syntactical model of the untyped lambda-calculus whose theory is exactly the least lambda-theory (l-beta). In this paper we investigate the more general question of whether the equational/order theory of a model of the (untyped) lambda-calculus can be recursively enumerable (r.e. for brevity). We introduce a notion of effective model of lambda-calculus calculus, which covers in particular all the models individually introduced in the literature. We prove that the order theory of an effective model is never r.e.; from this it follows that its equational theory cannot be l-beta or l-beta-eta. We then show that no effective model living in the stable or strongly stable semantics has an r.e. equational theory. Concerning Scott's semantics, we investigate the class of graph models and prove that no order theory of a graph model can be r.e., and that there exists an effective graph model whose equational/order theory is minimum among all theories of graph models. Finally, we show that the class of graph models enjoys a kind of downwards Lowenheim-Skolem theorem.",
        "published": "2008-06-13T15:02:03Z",
        "link": "http://arxiv.org/abs/0806.2264v1",
        "categories": [
            "math.LO",
            "cs.LO",
            "03B40; 03D45; 03C65"
        ]
    },
    {
        "title": "Logical Reasoning for Higher-Order Functions with Local State",
        "authors": [
            "Nobuko Yoshida",
            "Kohei Honda",
            "Martin Berger"
        ],
        "summary": "We introduce an extension of Hoare logic for call-by-value higher-order functions with ML-like local reference generation. Local references may be generated dynamically and exported outside their scope, may store higher-order functions and may be used to construct complex mutable data structures. This primitive is captured logically using a predicate asserting reachability of a reference name from a possibly higher-order datum and quantifiers over hidden references. We explore the logic's descriptive and reasoning power with non-trivial programming examples combining higher-order procedures and dynamically generated local state. Axioms for reachability and local invariant play a central role for reasoning about the examples.",
        "published": "2008-06-15T14:43:25Z",
        "link": "http://arxiv.org/abs/0806.2448v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.3; D.3.2; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "The computability path ordering: the end of a quest",
        "authors": [
            "Frédéric Blanqui",
            "Jean-Pierre Jouannaud",
            "Albert Rubio"
        ],
        "summary": "In this paper, we first briefly survey automated termination proof methods for higher-order calculi. We then concentrate on the higher-order recursive path ordering, for which we provide an improved definition, the Computability Path Ordering. This new definition appears indeed to capture the essence of computability arguments \\`a la Tait and Girard, therefore explaining the name of the improved ordering.",
        "published": "2008-06-16T11:39:25Z",
        "link": "http://arxiv.org/abs/0806.2517v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Data-Oblivious Stream Productivity",
        "authors": [
            "Joerg Endrullis",
            "Clemens Grabmayer",
            "Dimitri Hendriks"
        ],
        "summary": "We are concerned with demonstrating productivity of specifications of infinite streams of data, based on orthogonal rewrite rules. In general, this property is undecidable, but for restricted formats computable sufficient conditions can be obtained. The usual analysis disregards the identity of data, thus leading to approaches that we call data-oblivious. We present a method that is provably optimal among all such data-oblivious approaches. This means that in order to improve on the algorithm in this paper one has to proceed in a data-aware fashion.",
        "published": "2008-06-16T22:02:56Z",
        "link": "http://arxiv.org/abs/0806.2680v5",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "A logic with temporally accessible iteration",
        "authors": [
            "Alexei Lisitsa"
        ],
        "summary": "Deficiency in expressive power of the first-order logic has led to developing its numerous extensions by fixed point operators, such as Least Fixed-Point (LFP), inflationary fixed-point (IFP), partial fixed-point (PFP), etc. These logics have been extensively studied in finite model theory, database theory, descriptive complexity. In this paper we introduce unifying framework, the logic with iteration operator, in which iteration steps may be accessed by temporal logic formulae. We show that proposed logic FO+TAI subsumes all mentioned fixed point extensions as well as many other fixed point logics as natural fragments. On the other hand we show that over finite structures FO+TAI is no more expressive than FO+PFP. Further we show that adding the same machinery to the logic of monotone inductions (FO+LFP) does not increase its expressive power either.",
        "published": "2008-06-17T14:36:43Z",
        "link": "http://arxiv.org/abs/0806.2802v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Strategy Iteration using Non-Deterministic Strategies for Solving Parity   Games",
        "authors": [
            "Michael Luttenberger"
        ],
        "summary": "This article extends the idea of solving parity games by strategy iteration to non-deterministic strategies: In a non-deterministic strategy a player restricts himself to some non-empty subset of possible actions at a given node, instead of limiting himself to exactly one action. We show that a strategy-improvement algorithm by by Bjoerklund, Sandberg, and Vorobyov can easily be adapted to the more general setting of non-deterministic strategies. Further, we show that applying the heuristic of \"all profitable switches\" leads to choosing a \"locally optimal\" successor strategy in the setting of non-deterministic strategies, thereby obtaining an easy proof of an algorithm by Schewe. In contrast to the algorithm by Bjoerklund et al., we present our algorithm directly for parity games which allows us to compare it to the algorithm by Jurdzinski and Voege: We show that the valuations used in both algorithm coincide on parity game arenas in which one player can \"surrender\". Thus, our algorithm can also be seen as a generalization of the one by Jurdzinski and Voege to non-deterministic strategies. Finally, using non-deterministic strategies allows us to show that the number of improvement steps is bound from above by O(1.724^n). For strategy-improvement algorithms, this bound was previously only known to be attainable by using randomization.",
        "published": "2008-06-18T08:32:17Z",
        "link": "http://arxiv.org/abs/0806.2923v4",
        "categories": [
            "cs.GT",
            "cs.LO",
            "D.2.4; F.3.1; F.4.1"
        ]
    },
    {
        "title": "The Kleene-Rosser Paradox, The Liar's Paradox & A Fuzzy Logic   Programming Paradox Imply SAT is (NOT) NP-complete",
        "authors": [
            "Rafee Ebrahim Kamouna"
        ],
        "summary": "After examining the {\\bf P} versus {\\bf NP} problem against the Kleene-Rosser paradox of the $\\lambda$-calculus [94], it was found that it represents a counter-example to NP-completeness. We prove that it contradicts the proof of Cook's theorem. A logical formalization of the liar's paradox leads to the same result. This formalization of the liar's paradox into a computable form is a 2-valued instance of a fuzzy logic programming paradox discovered in the system of [90]. Three proofs that show that {\\bf SAT} is (NOT) NP-complete are presented. The counter-example classes to NP-completeness are also counter-examples to Fagin's theorem [36] and the Immermann-Vardi theorem [89,110], the fundamental results of descriptive complexity. All these results show that {\\bf ZF$\\not$C} is inconsistent.",
        "published": "2008-06-18T10:00:38Z",
        "link": "http://arxiv.org/abs/0806.2947v8",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A Computer Verified Theory of Compact Sets",
        "authors": [
            "Russell O'Connor"
        ],
        "summary": "Compact sets in constructive mathematics capture our intuition of what computable subsets of the plane (or any other complete metric space) ought to be. A good representation of compact sets provides an efficient means of creating and displaying images with a computer. In this paper, I build upon existing work about complete metric spaces to define compact sets as the completion of the space of finite sets under the Hausdorff metric. This definition allowed me to quickly develop a computer verified theory of compact sets. I applied this theory to compute provably correct plots of uniformly continuous functions.",
        "published": "2008-06-19T18:09:01Z",
        "link": "http://arxiv.org/abs/0806.3209v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Separability in the Ambient Logic",
        "authors": [
            "Daniel Hirschkoff",
            "Etienne Lozes",
            "Davide Sangiorgi"
        ],
        "summary": "The \\it{Ambient Logic} (AL) has been proposed for expressing properties of process mobility in the calculus of Mobile Ambients (MA), and as a basis for query languages on semistructured data. We study some basic questions concerning the discriminating power of AL, focusing on the equivalence on processes induced by the logic $(=_L>)$. As underlying calculi besides MA we consider a subcalculus in which an image-finiteness condition holds and that we prove to be Turing complete. Synchronous variants of these calculi are studied as well. In these calculi, we provide two operational characterisations of $_=L$: a coinductive one (as a form of bisimilarity) and an inductive one (based on structual properties of processes). After showing $_=L$ to be stricly finer than barbed congruence, we establish axiomatisations of $_=L$ on the subcalculus of MA (both the asynchronous and the synchronous version), enabling us to relate $_=L$ to structural congruence. We also present some (un)decidability results that are related to the above separation properties for AL: the undecidability of $_=L$ on MA and its decidability on the subcalculus.",
        "published": "2008-06-24T10:00:00Z",
        "link": "http://arxiv.org/abs/0806.3849v2",
        "categories": [
            "cs.LO",
            "cs.MA",
            "cs.PL",
            "F.3.2; F.4.1"
        ]
    },
    {
        "title": "Data linkage dynamics with shedding",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "We study shedding in the setting of data linkage dynamics, a simple model of computation that bears on the use of dynamic data structures in programming. Shedding is complementary to garbage collection. With shedding, each time a link to a data object is updated by a program, it is determined whether or not the link will possibly be used once again by the program, and if not the link is automatically removed. Thus, everything is made garbage as soon as it can be viewed as garbage. By that, the effectiveness of garbage collection becomes maximal.",
        "published": "2008-06-25T07:15:35Z",
        "link": "http://arxiv.org/abs/0806.4034v2",
        "categories": [
            "cs.LO",
            "D.3.3; D.4.2; F.1.1; F.3.3"
        ]
    },
    {
        "title": "Complexity of Hybrid Logics over Transitive Frames",
        "authors": [
            "Martin Mundhenk",
            "Thomas Schneider",
            "Thomas Schwentick",
            "Volker Weber"
        ],
        "summary": "This paper examines the complexity of hybrid logics over transitive frames, transitive trees, and linear frames. We show that satisfiability over transitive frames for the hybrid language extended with the downarrow operator is NEXPTIME-complete. This is in contrast to undecidability of satisfiability over arbitrary frames for this language (Areces, Blackburn, Marx 1999). It is also shown that adding the @ operator or the past modality leads to undecidability over transitive frames. This is again in contrast to the case of transitive trees and linear frames, where we show these languages to be nonelementarily decidable. Moreover, we establish 2EXPTIME and EXPTIME upper bounds for satisfiability over transitive frames and transitive trees, respectively, for the hybrid Until/Since language. An EXPTIME lower bound is shown to hold for the modal Until language over both frame classes.",
        "published": "2008-06-25T15:38:43Z",
        "link": "http://arxiv.org/abs/0806.4130v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Interpolation in local theory extensions",
        "authors": [
            "Viorica Sofronie-Stokkermans"
        ],
        "summary": "In this paper we study interpolation in local extensions of a base theory. We identify situations in which it is possible to obtain interpolants in a hierarchical manner, by using a prover and a procedure for generating interpolants in the base theory as black-boxes. We present several examples of theory extensions in which interpolants can be computed this way, and discuss applications in verification, knowledge representation, and modular reasoning in combinations of local theories.",
        "published": "2008-06-27T15:51:02Z",
        "link": "http://arxiv.org/abs/0806.4553v2",
        "categories": [
            "cs.LO",
            "cs.SE",
            "F.4.1; F.3.1"
        ]
    },
    {
        "title": "The Heap Lambda Machine",
        "authors": [
            "Anton Salikhmetov"
        ],
        "summary": "This paper introduces a new machine architecture for evaluating lambda expressions using the normal-order reduction, which guarantees that every lambda expression will be evaluated if the expression has its normal form and the system has enough memory. The architecture considered here operates using heap memory only. Lambda expressions are represented as graphs, and all algorithms used in the processing unit of this machine are non-recursive.",
        "published": "2008-06-27T23:22:38Z",
        "link": "http://arxiv.org/abs/0806.4631v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Termination of lambda-calculus with the extra Call-By-Value rule known   as assoc",
        "authors": [
            "Stéphane Lengrand"
        ],
        "summary": "In this paper we prove that any lambda-term that is strongly normalising for beta-reduction is also strongly normalising for beta,assoc-reduction. assoc is a call-by-value rule that has been used in works by Moggi, Joachimsky, Espirito Santo and others. The result has often been justified with incomplete or incorrect proofs. Here we give one in full details.",
        "published": "2008-06-30T11:29:21Z",
        "link": "http://arxiv.org/abs/0806.4859v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Game Refinement Relations and Metrics",
        "authors": [
            "Luca de Alfaro",
            "Rupak Majumdar",
            "Vishwanath Raman",
            "Mariëlle Stoelinga"
        ],
        "summary": "We consider two-player games played over finite state spaces for an infinite number of rounds. At each state, the players simultaneously choose moves; the moves determine a successor state. It is often advantageous for players to choose probability distributions over moves, rather than single moves. Given a goal, for example, reach a target state, the question of winning is thus a probabilistic one: what is the maximal probability of winning from a given state?   On these game structures, two fundamental notions are those of equivalences and metrics. Given a set of winning conditions, two states are equivalent if the players can win the same games with the same probability from both states. Metrics provide a bound on the difference in the probabilities of winning across states, capturing a quantitative notion of state similarity.   We introduce equivalences and metrics for two-player game structures, and we show that they characterize the difference in probability of winning games whose goals are expressed in the quantitative mu-calculus. The quantitative mu-calculus can express a large set of goals, including reachability, safety, and omega-regular properties. Thus, we claim that our relations and metrics provide the canonical extensions to games, of the classical notion of bisimulation for transition systems. We develop our results both for equivalences and metrics, which generalize bisimulation, and for asymmetrical versions, which generalize simulation.",
        "published": "2008-06-30T17:55:15Z",
        "link": "http://arxiv.org/abs/0806.4956v2",
        "categories": [
            "cs.LO",
            "F.4.1; F.1.1"
        ]
    },
    {
        "title": "Knowledge bases over algebraic models. Some notes about informational   equivalence",
        "authors": [
            "Knyazhansky Marina",
            "Plotkin Tatjana"
        ],
        "summary": "The recent advances in knowledge base research and the growing importance of effective knowledge management raised an important question of knowledge base equivalence verification. This problem has not been stated earlier, at least in a way that allows speaking about algorithms for verification of informational equivalence, because the informal definition of knowledge bases makes formal solution of this problem impossible. In this paper we provide an implementable formal algorithm for knowledge base equivalence verification based on the formal definition of knowledge base proposed by Plotkin B. and Plotkin T., and study some important properties of automorphic equivalence of models. We also describe the concept of equivalence and formulate the criterion for the equivalence of knowledge bases defined over finite models. Further we define multi-models and automorphic equivalence of models and multi-models, that is generalization of automorphic equivalence of algebras.",
        "published": "2008-07-04T09:16:13Z",
        "link": "http://arxiv.org/abs/0807.0704v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A General Framework for Sound and Complete Floyd-Hoare Logics",
        "authors": [
            "Rob Arthan",
            "Ursula Martin",
            "Erik A. Mathiesen",
            "Paulo Oliva"
        ],
        "summary": "This paper presents an abstraction of Hoare logic to traced symmetric monoidal categories, a very general framework for the theory of systems. Our abstraction is based on a traced monoidal functor from an arbitrary traced monoidal category into the category of pre-orders and monotone relations. We give several examples of how our theory generalises usual Hoare logics (partial correctness of while programs, partial correctness of pointer programs), and provide some case studies on how it can be used to develop new Hoare logics (run-time analysis of while programs and stream circuits).",
        "published": "2008-07-07T13:40:00Z",
        "link": "http://arxiv.org/abs/0807.1016v1",
        "categories": [
            "cs.LO",
            "cs.OH",
            "F.3.1"
        ]
    },
    {
        "title": "Timed Parity Games: Complexity and Robustness",
        "authors": [
            "Krishnendu Chatterjee",
            "Thomas A. Henzinger",
            "Vinayak Prabhu"
        ],
        "summary": "We consider two-player games played in real time on game structures with clocks and parity objectives. The games are concurrent in that at each turn, both players independently propose a time delay and an action, and the action with the shorter delay is chosen. To prevent a player from winning by blocking time, we restrict each player to strategies that ensure that the player cannot be responsible for causing a zeno run. First, we present an efficient reduction of these games to turn-based (i.e., nonconcurrent) finite-state (i.e., untimed) parity games. The states of the resulting game are pairs of clock regions of the original game. Our reduction improves the best known complexity for solving timed parity games. Moreover, the rich class of algorithms for classical parity games can now be applied to timed parity games.   Second, we consider two restricted classes of strategies for the player that represents the controller in a real-time synthesis problem, namely, limit-robust and bounded-robust strategies. Using a limit-robust strategy, the controller cannot choose an exact real-valued time delay but must allow for some nonzero jitter in each of its actions. If there is a given lower bound on the jitter, then the strategy is bounded-robust. We show that exact strategies are more powerful than limit-robust strategies, which are more powerful than bounded-robust strategies for any bound. For both kinds of robust strategies, we present efficient reductions to standard timed automaton games. These reductions provide algorithms for the synthesis of robust real-time controllers.",
        "published": "2008-07-08T06:37:35Z",
        "link": "http://arxiv.org/abs/0807.1165v1",
        "categories": [
            "cs.LO",
            "cs.GT"
        ]
    },
    {
        "title": "A Counterexample Guided Abstraction-Refinement Framework for Markov   Decision Processes",
        "authors": [
            "Rohit Chadha",
            "Mahesh Viswanthan"
        ],
        "summary": "The main challenge in using abstractions effectively, is to construct a suitable abstraction for the system being verified. One approach that tries to address this problem is that of {\\it counterexample guided abstraction-refinement (CEGAR)}, wherein one starts with a coarse abstraction of the system, and progressively refines it, based on invalid counterexamples seen in prior model checking runs, until either an abstraction proves the correctness of the system or a valid counterexample is generated. While CEGAR has been successfully used in verifying non-probabilistic systems automatically, CEGAR has not been applied in the context of probabilistic systems. The main issues that need to be tackled in order to extend the approach to probabilistic systems is a suitable notion of ``counterexample'', algorithms to generate counterexamples, check their validity, and then automatically refine an abstraction based on an invalid counterexample. In this paper, we address these issues, and present a CEGAR framework for Markov Decision Processes.",
        "published": "2008-07-08T19:47:11Z",
        "link": "http://arxiv.org/abs/0807.1173v1",
        "categories": [
            "cs.SE",
            "cs.LO",
            "D.2.4"
        ]
    },
    {
        "title": "Inductive and Coinductive Components of Corecursive Functions in Coq",
        "authors": [
            "Yves Bertot",
            "Ekaterina Komendantskaya"
        ],
        "summary": "In Constructive Type Theory, recursive and corecursive definitions are subject to syntactic restrictions which guarantee termination for recursive functions and productivity for corecursive functions. However, many terminating and productive functions do not pass the syntactic tests. Bove proposed in her thesis an elegant reformulation of the method of accessibility predicates that widens the range of terminative recursive functions formalisable in Constructive Type Theory. In this paper, we pursue the same goal for productive corecursive functions. Notably, our method of formalisation of coinductive definitions of productive functions in Coq requires not only the use of ad-hoc predicates, but also a systematic algorithm that separates the inductive and coinductive parts of functions.",
        "published": "2008-07-09T19:16:25Z",
        "link": "http://arxiv.org/abs/0807.1524v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Coinductive Formal Reasoning in Exact Real Arithmetic",
        "authors": [
            "Milad Niqui"
        ],
        "summary": "In this article we present a method for formally proving the correctness of the lazy algorithms for computing homographic and quadratic transformations -- of which field operations are special cases-- on a representation of real numbers by coinductive streams. The algorithms work on coinductive stream of M\\\"{o}bius maps and form the basis of the Edalat--Potts exact real arithmetic. We use the machinery of the Coq proof assistant for the coinductive types to present the formalisation. The formalised algorithms are only partially productive, i.e., they do not output provably infinite streams for all possible inputs. We show how to deal with this partiality in the presence of syntactic restrictions posed by the constructive type theory of Coq. Furthermore we show that the type theoretic techniques that we develop are compatible with the semantics of the algorithms as continuous maps on real numbers. The resulting Coq formalisation is available for public download.",
        "published": "2008-07-10T14:56:05Z",
        "link": "http://arxiv.org/abs/0807.1669v2",
        "categories": [
            "cs.LO",
            "F.3.1; D.2.4"
        ]
    },
    {
        "title": "Topological Observations on Multiplicative Additive Linear Logic",
        "authors": [
            "André Hirschowitz",
            "Michel Hirschowitz",
            "Tom Hirschowitz"
        ],
        "summary": "As an attempt to uncover the topological nature of composition of strategies in game semantics, we present a ``topological'' game for Multiplicative Additive Linear Logic without propositional variables, including cut moves. We recast the notion of (winning) strategy and the question of cut elimination in this context, and prove a cut elimination theorem. Finally, we prove soundness and completeness. The topology plays a crucial role, in particular through the fact that strategies form a sheaf.",
        "published": "2008-07-14T16:41:48Z",
        "link": "http://arxiv.org/abs/0807.2636v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "CPBVP: A Constraint-Programming Framework for Bounded Program   Verification",
        "authors": [
            "Hélène Collavizza",
            "Michel Rueher",
            "Pascal Van Hentenryck"
        ],
        "summary": "This paper studies how to verify the conformity of a program with its specification and proposes a novel constraint-programming framework for bounded program verification (CPBPV). The CPBPV framework uses constraint stores to represent the specification and the program and explores execution paths nondeterministically. The input program is partially correct if each constraint store so produced implies the post-condition. CPBPV does not explore spurious execution paths as it incrementally prunes execution paths early by detecting that the constraint store is not consistent. CPBPV uses the rich language of constraint programming to express the constraint store. Finally, CPBPV is parametrized with a list of solvers which are tried in sequence, starting with the least expensive and less general. Experimental results often produce orders of magnitude improvements over earlier approaches, running times being often independent of the variable domains. Moreover, CPBPV was able to detect subtle errors in some programs while other frameworks based on model checking have failed.",
        "published": "2008-07-15T14:18:43Z",
        "link": "http://arxiv.org/abs/0807.2383v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Two Fuzzy Logic Programming Paradoxes Imply Continuum Hypothesis=\"False\"   & Axiom of Choice=\"False\" Imply ZFC is Inconsistent",
        "authors": [
            "Rafee Ebrahim Kamouna"
        ],
        "summary": "Two different paradoxes of the fuzzy logic programming system of [29] are presented. The first paradox is due to two distinct (contradictory) truth values for every ground atom of FLP, one is syntactical, the other is semantical. The second paradox concerns the cardinality of the valid FLP formulas which is found to have contradictory values: both $\\aleph_0$ the cardinality of the natural numbers, and $c$, the cardinality of the continuum. The result is that CH=\"False\" and Axiom of Choice=\"False\". Hence, ZFC is inconsistent.",
        "published": "2008-07-16T10:58:40Z",
        "link": "http://arxiv.org/abs/0807.2543v4",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Perturbed affine arithmetic for invariant computation in numerical   program analysis",
        "authors": [
            "Eric Goubault",
            "Sylvie Putot"
        ],
        "summary": "We completely describe a new domain for abstract interpretation of numerical programs. Fixpoint iteration in this domain is proved to converge to finite precise invariants for (at least) the class of stable linear recursive filters of any order. Good evidence shows it behaves well also for some non-linear schemes. The result, and the structure of the domain, rely on an interesting interplay between order and topology.",
        "published": "2008-07-18T12:41:52Z",
        "link": "http://arxiv.org/abs/0807.2961v1",
        "categories": [
            "cs.LO",
            "cs.NA",
            "D.2.4; F.3.1; F.3.2"
        ]
    },
    {
        "title": "Proposition Algebra with Projective Limits",
        "authors": [
            "J. A. Bergstra",
            "A. Ponse"
        ],
        "summary": "Sequential propositional logic deviates from ordinary propositional logic by taking into account that during the sequential evaluation of a propositional statement,atomic propositions may yield different Boolean values at repeated occurrences. We introduce `free valuations' to capture this dynamics of a propositional statement's environment. The resulting logic is phrased as an equationally specified algebra rather than in the form of proof rules, and is named `proposition algebra'. It is strictly more general than Boolean algebra to the extent that the classical connectives fail to be expressively complete in the sequential case. The four axioms for free valuation congruence are then combined with other axioms in order define a few more valuation congruences that gradually identify more propositional statements, up to static valuation congruence (which is the setting of conventional propositional logic).   Proposition algebra is developed in a fashion similar to the process algebra ACP and the program algebra PGA, via an algebraic specification which has a meaningful initial algebra for which a range of coarser congruences are considered important as well. In addition infinite objects (that is propositional statements, processes and programs respectively) are dealt with by means of an inverse limit construction which allows the transfer of knowledge concerning finite objects to facts about infinite ones while reducing all facts about infinite objects to an infinity of facts about finite ones in return.",
        "published": "2008-07-23T12:31:11Z",
        "link": "http://arxiv.org/abs/0807.3648v5",
        "categories": [
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "Formal semantics of language and the Richard-Berry paradox",
        "authors": [
            "Stefano Crespi Reghizzi"
        ],
        "summary": "The classical logical antinomy known as Richard-Berry paradox is combined with plausible assumptions about the size i.e. the descriptional complexity of Turing machines formalizing certain sentences, to show that formalization of language leads to contradiction.",
        "published": "2008-07-24T10:31:55Z",
        "link": "http://arxiv.org/abs/0807.3845v1",
        "categories": [
            "cs.CL",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Rational streams coalgebraically",
        "authors": [
            "J. J. M. M. Rutten"
        ],
        "summary": "We study rational streams (over a field) from a coalgebraic perspective. Exploiting the finality of the set of streams, we present an elementary and uniform proof of the equivalence of four notions of representability of rational streams: by finite dimensional linear systems; by finite stream circuits; by finite weighted stream automata; and by finite dimensional subsystems of the set of streams.",
        "published": "2008-07-25T11:36:23Z",
        "link": "http://arxiv.org/abs/0807.4073v2",
        "categories": [
            "cs.LO",
            "F.1.1; G.1.0"
        ]
    },
    {
        "title": "A Compositional Query Algebra for Second-Order Logic and Uncertain   Databases",
        "authors": [
            "Christoph Koch"
        ],
        "summary": "World-set algebra is a variable-free query language for uncertain databases. It constitutes the core of the query language implemented in MayBMS, an uncertain database system. This paper shows that world-set algebra captures exactly second-order logic over finite structures, or equivalently, the polynomial hierarchy. The proofs also imply that world-set algebra is closed under composition, a previously open problem.",
        "published": "2008-07-29T11:22:01Z",
        "link": "http://arxiv.org/abs/0807.4620v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2.3"
        ]
    },
    {
        "title": "Exhaustible sets in higher-type computation",
        "authors": [
            "Martin Escardo"
        ],
        "summary": "We say that a set is exhaustible if it admits algorithmic universal quantification for continuous predicates in finite time, and searchable if there is an algorithm that, given any continuous predicate, either selects an element for which the predicate holds or else tells there is no example. The Cantor space of infinite sequences of binary digits is known to be searchable. Searchable sets are exhaustible, and we show that the converse also holds for sets of hereditarily total elements in the hierarchy of continuous functionals; moreover, a selection functional can be constructed uniformly from a quantification functional. We prove that searchable sets are closed under intersections with decidable sets, and under the formation of computable images and of finite and countably infinite products. This is related to the fact, established here, that exhaustible sets are topologically compact. We obtain a complete description of exhaustible total sets by developing a computational version of a topological Arzela--Ascoli type characterization of compact subsets of function spaces. We also show that, in the non-empty case, they are precisely the computable images of the Cantor space. The emphasis of this paper is on the theory of exhaustible and searchable sets, but we also briefly sketch applications.",
        "published": "2008-08-04T13:41:09Z",
        "link": "http://arxiv.org/abs/0808.0441v2",
        "categories": [
            "cs.LO",
            "F.4.1; F.3.2"
        ]
    },
    {
        "title": "Logics for the Relational Syllogistic",
        "authors": [
            "Ian Pratt-Hartmann",
            "Lawrence S. Moss"
        ],
        "summary": "The Aristotelian syllogistic cannot account for the validity of many inferences involving relational facts. In this paper, we investigate the prospects for providing a relational syllogistic. We identify several fragments based on (a) whether negation is permitted on all nouns, including those in the subject of a sentence; and (b) whether the subject noun phrase may contain a relative clause. The logics we present are extensions of the classical syllogistic, and we pay special attention to the question of whether reductio ad absurdum is needed. Thus our main goal is to derive results on the existence (or non-existence) of syllogistic proof systems for relational fragments. We also determine the computational complexity of all our fragments.",
        "published": "2008-08-04T22:26:38Z",
        "link": "http://arxiv.org/abs/0808.0521v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.CL",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Executable Set Theory and Arithmetic Encodings in Prolog",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "The paper is organized as a self-contained literate Prolog program that implements elements of an executable finite set theory with focus on combinatorial generation and arithmetic encodings. The complete Prolog code is available at http://logic.csci.unt.edu/tarau/research/2008/pHFS.zip . First, ranking and unranking functions for some \"mathematically elegant\" data types in the universe of Hereditarily Finite Sets with Urelements are provided, resulting in arithmetic encodings for powersets, hypergraphs, ordinals and choice functions. After implementing a digraph representation of Hereditarily Finite Sets we define {\\em decoration functions} that can recover well-founded sets from encodings of their associated acyclic digraphs. We conclude with an encoding of arbitrary digraphs and discuss a concept of duality induced by the set membership relation. In the process, we uncover the surprising possibility of internally sharing isomorphic objects, independently of their language level types and meanings.",
        "published": "2008-08-05T04:59:56Z",
        "link": "http://arxiv.org/abs/0808.0540v1",
        "categories": [
            "cs.LO",
            "cs.DM",
            "cs.DS",
            "cs.MS",
            "cs.SC"
        ]
    },
    {
        "title": "Ranking and Unranking of Hereditarily Finite Functions and Permutations",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "Prolog's ability to return multiple answers on backtracking provides an elegant mechanism to derive reversible encodings of combinatorial objects as Natural Numbers i.e. {\\em ranking} and {\\em unranking} functions. Starting from a generalization of Ackerman's encoding of Hereditarily Finite Sets with Urelements and a novel tupling/untupling operation, we derive encodings for Finite Functions and use them as building blocks for an executable theory of {\\em Hereditarily Finite Functions}. The more difficult problem of {\\em ranking} and {\\em unranking} {\\em Hereditarily Finite Permutations} is then tackled using Lehmer codes and factoradics.   The paper is organized as a self-contained literate Prolog program available at \\url{http://logic.csci.unt.edu/tarau/research/2008/pHFF.zip}",
        "published": "2008-08-05T05:20:51Z",
        "link": "http://arxiv.org/abs/0808.0554v1",
        "categories": [
            "cs.LO",
            "cs.MS"
        ]
    },
    {
        "title": "Pairing Functions, Boolean Evaluation and Binary Decision Diagrams in   Prolog",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "A \"pairing function\" J associates a unique natural number z to any two natural numbers x,y such that for two \"unpairing functions\" K and L, the equalities K(J(x,y))=x, L(J(x,y))=y and J(K(z),L(z))=z hold. Using pairing functions on natural number representations of truth tables, we derive an encoding for Binary Decision Diagrams with the unique property that its boolean evaluation faithfully mimics its structural conversion to a a natural number through recursive application of a matching pairing function. We then use this result to derive {\\em ranking} and {\\em unranking} functions for BDDs and reduced BDDs. The paper is organized as a self-contained literate Prolog program, available at http://logic.csci.unt.edu/tarau/research/2008/pBDD.zip   Keywords: logic programming and computational mathematics, pairing/unpairing functions, encodings of boolean functions, binary decision diagrams, natural number representations of truth tables",
        "published": "2008-08-05T05:33:09Z",
        "link": "http://arxiv.org/abs/0808.0555v2",
        "categories": [
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Model Checking Positive Equality-free FO: Boolean Structures and   Digraphs of Size Three",
        "authors": [
            "Barnaby Martin"
        ],
        "summary": "We study the model checking problem, for fixed structures A, over positive equality-free first-order logic -- a natural generalisation of the non-uniform quantified constraint satisfaction problem QCSP(A). We prove a complete complexity classification for this problem when A ranges over 1.) boolean structures and 2.) digraphs of size (less than or equal to) three. The former class displays dichotomy between Logspace and Pspace-complete, while the latter class displays tetrachotomy between Logspace, NP-complete, co-NP-complete and Pspace-complete.",
        "published": "2008-08-05T13:33:43Z",
        "link": "http://arxiv.org/abs/0808.0647v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Verification of Peterson's Algorithm for Leader Election in a   Unidirectional Asynchronous Ring Using NuSMV",
        "authors": [
            "Amin Ansari"
        ],
        "summary": "The finite intrinsic nature of the most distributed algorithms gives us this ability to use model checking tools for verification of this type of algorithms. In this paper, I attempt to use NuSMV as a model checking tool for verifying necessary properties of Peterson's algorithm for leader election problem in a unidirectional asynchronous ring topology. Peterson's algorithm for an asynchronous ring supposes that each node in the ring has a unique ID and also a queue for dealing with storage problem. By considering that the queue can have any combination of values, a constructed model for a ring with only four nodes will have more than a billion states. Although it seems that model checking is not a feasible approach for this problem, I attempt to use several effective limiting assumptions for hiring formal model checking approach without losing the correct functionality of the Peterson's algorithm. These enforced limiting assumptions target the degree of freedom in the model checking process and significantly decrease the CPU time, memory usage and the total number of page faults. By deploying these limitations, the number of nodes can be increased from four to eight in the model checking process with NuSMV.",
        "published": "2008-08-07T06:02:59Z",
        "link": "http://arxiv.org/abs/0808.0962v1",
        "categories": [
            "cs.LO",
            "cs.DC",
            "D.2.4; C.2.2"
        ]
    },
    {
        "title": "A Formal Foundation for XrML",
        "authors": [
            "Joseph Y. Halpern",
            "Vicky Weissman"
        ],
        "summary": "XrML is becoming a popular language in industry for writing software licenses. The semantics for XrML is implicitly given by an algorithm that determines if a permission follows from a set of licenses. We focus on a fragment of the language and use it to highlight some problematic aspects of the algorithm. We then correct the problems, introduce formal semantics, and show that our semantics captures the (corrected) algorithm. Next, we consider the complexity of determining if a permission is implied by a set of XrML licenses. We prove that the general problem is undecidable, but it is polynomial-time computable for an expressive fragment of the language. We extend XrML to capture a wider range of licenses by adding negation to the language. Finally, we discuss the key differences between XrML and MPEG-21, an international standard based on XrML.",
        "published": "2008-08-08T16:25:58Z",
        "link": "http://arxiv.org/abs/0808.1215v1",
        "categories": [
            "cs.CR",
            "cs.LO",
            "K.6.5; K.4.4; D.4.6; F.3; H.2.7"
        ]
    },
    {
        "title": "Ockham's razor and reasoning about information flow",
        "authors": [
            "Mehrnoosh Sadrzadeh"
        ],
        "summary": "What is the minimal algebraic structure to reason about information flow? Do we really need the full power of Boolean algebras with co-closure and de Morgan dual operators? How much can we weaken and still be able to reason about multi-agent scenarios in a tidy compositional way? This paper provides some answers.",
        "published": "2008-08-09T13:51:44Z",
        "link": "http://arxiv.org/abs/0808.1354v1",
        "categories": [
            "math.LO",
            "cs.LO"
        ]
    },
    {
        "title": "Comparison between CPBPV, ESC/Java, CBMC, Blast, EUREKA and Why for   Bounded Program Verification",
        "authors": [
            "Hélène Collavizza",
            "Michel Rueher",
            "Pascal Van Hentenryck"
        ],
        "summary": "This report describes experimental results for a set of benchmarks on program verification. It compares the capabilities of CPBVP \"Constraint Programming framework for Bounded Program Verification\" [4] with the following frameworks: ESC/Java, CBMC, Blast, EUREKA and Why.",
        "published": "2008-08-11T12:55:19Z",
        "link": "http://arxiv.org/abs/0808.1508v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Every Computably Enumerable Random Real Is Provably Computably   Enumerable Random",
        "authors": [
            "Cristian S. Calude",
            "Nicholas J. Hay"
        ],
        "summary": "We prove that every computably enumerable (c.e.) random real is provable in Peano Arithmetic (PA) to be c.e. random. A major step in the proof is to show that the theorem stating that \"a real is c.e. and random iff it is the halting probability of a universal prefix-free Turing machine\" can be proven in PA. Our proof, which is simpler than the standard one, can also be used for the original theorem.   Our positive result can be contrasted with the case of computable functions, where not every computable function is provably computable in PA, or even more interestingly, with the fact that almost all random finite strings are not provably random in PA.   We also prove two negative results: a) there exists a universal machine whose universality cannot be proved in PA, b) there exists a universal machine $U$ such that, based on $U$, PA cannot prove the randomness of its halting probability.   The paper also includes a sharper form of the Kraft-Chaitin Theorem, as well as a formal proof of this theorem written with the proof assistant Isabelle.",
        "published": "2008-08-15T23:46:48Z",
        "link": "http://arxiv.org/abs/0808.2220v5",
        "categories": [
            "cs.CC",
            "cs.LO",
            "math.LO",
            "H.1.1"
        ]
    },
    {
        "title": "Classical Knowledge for Quantum Security",
        "authors": [
            "Ellie D'Hondt",
            "Mehrnoosh Sadrzadeh"
        ],
        "summary": "We propose a decision procedure for analysing security of quantum cryptographic protocols, combining a classical algebraic rewrite system for knowledge with an operational semantics for quantum distributed computing. As a test case, we use our procedure to reason about security properties of a recently developed quantum secret sharing protocol that uses graph states. We analyze three different scenarios based on the safety assumptions of the classical and quantum channels and discover the path of an attack in the presence of an adversary. The epistemic analysis that leads to this and similar types of attacks is purely based on our classical notion of knowledge.",
        "published": "2008-08-26T19:34:07Z",
        "link": "http://arxiv.org/abs/0808.3574v1",
        "categories": [
            "cs.CR",
            "cs.LO",
            "quant-ph"
        ]
    },
    {
        "title": "Flow Faster: Efficient Decision Algorithms for Probabilistic Simulations",
        "authors": [
            "Lijun Zhang",
            "Holger Hermanns",
            "Friedrich Eisenbrand",
            "David N. Jansen"
        ],
        "summary": "Strong and weak simulation relations have been proposed for Markov chains, while strong simulation and strong probabilistic simulation relations have been proposed for probabilistic automata. However, decision algorithms for strong and weak simulation over Markov chains, and for strong simulation over probabilistic automata are not efficient, which makes it as yet unclear whether they can be used as effectively as their non-probabilistic counterparts. This paper presents drastically improved algorithms to decide whether some (discrete- or continuous-time) Markov chain strongly or weakly simulates another, or whether a probabilistic automaton strongly simulates another. The key innovation is the use of parametric maximum flow techniques to amortize computations. We also present a novel algorithm for deciding strong probabilistic simulation preorders on probabilistic automata, which has polynomial complexity via a reduction to an LP problem. When extending the algorithms for probabilistic automata to their continuous-time counterpart, we retain the same complexity for both strong and strong probabilistic simulations.",
        "published": "2008-08-27T08:35:44Z",
        "link": "http://arxiv.org/abs/0808.3651v3",
        "categories": [
            "cs.LO",
            "F.2.1; F.3.1; G.2.2; G.3"
        ]
    },
    {
        "title": "The Complexity of Reasoning for Fragments of Default Logic",
        "authors": [
            "Olaf Beyersdorff",
            "Arne Meier",
            "Michael Thomas",
            "Heribert Vollmer"
        ],
        "summary": "Default logic was introduced by Reiter in 1980. In 1992, Gottlob classified the complexity of the extension existence problem for propositional default logic as $\\SigmaPtwo$-complete, and the complexity of the credulous and skeptical reasoning problem as SigmaP2-complete, resp. PiP2-complete. Additionally, he investigated restrictions on the default rules, i.e., semi-normal default rules. Selman made in 1992 a similar approach with disjunction-free and unary default rules. In this paper we systematically restrict the set of allowed propositional connectives. We give a complete complexity classification for all sets of Boolean functions in the meaning of Post's lattice for all three common decision problems for propositional default logic. We show that the complexity is a hexachotomy (SigmaP2-, DeltaP2-, NP-, P-, NL-complete, trivial) for the extension existence problem, while for the credulous and skeptical reasoning problem we obtain similar classifications without trivial cases.",
        "published": "2008-08-28T11:14:41Z",
        "link": "http://arxiv.org/abs/0808.3884v4",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.2.2; F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "On the strength of proof-irrelevant type theories",
        "authors": [
            "Benjamin Werner"
        ],
        "summary": "We present a type theory with some proof-irrelevance built into the conversion rule. We argue that this feature is useful when type theory is used as the logical formalism underlying a theorem prover. We also show a close relation with the subset types of the theory of PVS. We show that in these theories, because of the additional extentionality, the axiom of choice implies the decidability of equality, that is, almost classical logic. Finally we describe a simple set-theoretic semantics.",
        "published": "2008-08-28T15:31:31Z",
        "link": "http://arxiv.org/abs/0808.3928v2",
        "categories": [
            "cs.LO",
            "F.4.1; F.3.1"
        ]
    },
    {
        "title": "Tableau-based decision procedure for the multi-agent epistemic logic   with operators of common and distributed knowledge",
        "authors": [
            "Valentin Goranko",
            "Dmitry Shkatov"
        ],
        "summary": "We develop an incremental-tableau-based decision procedure for the multi-agent epistemic logic MAEL(CD) (aka S5_n (CD)), whose language contains operators of individual knowledge for a finite set Ag of agents, as well as operators of distributed and common knowledge among all agents in Ag. Our tableau procedure works in (deterministic) exponential time, thus establishing an upper bound for MAEL(CD)-satisfiability that matches the (implicit) lower-bound known from earlier results, which implies ExpTime-completeness of MAEL(CD)-satisfiability. Therefore, our procedure provides a complexity-optimal algorithm for checking MAEL(CD)-satisfiability, which, however, in most cases is much more efficient. We prove soundness and completeness of the procedure, and illustrate it with an example.",
        "published": "2008-08-29T17:09:06Z",
        "link": "http://arxiv.org/abs/0808.4133v1",
        "categories": [
            "cs.LO",
            "cs.MA",
            "F.4.1; I.2.4; I.2.11"
        ]
    },
    {
        "title": "Model Checking Probabilistic Timed Automata with One or Two Clocks",
        "authors": [
            "Marcin Jurdzinski",
            "Francois Laroussinie",
            "Jeremy Sproston"
        ],
        "summary": "Probabilistic timed automata are an extension of timed automata with discrete probability distributions. We consider model-checking algorithms for the subclasses of probabilistic timed automata which have one or two clocks. Firstly, we show that PCTL probabilistic model-checking problems (such as determining whether a set of target states can be reached with probability at least 0.99 regardless of how nondeterminism is resolved) are PTIME-complete for one-clock probabilistic timed automata, and are EXPTIME-complete for probabilistic timed automata with two clocks. Secondly, we show that, for one-clock probabilistic timed automata, the model-checking problem for the probabilistic timed temporal logic PCTL is EXPTIME-complete. However, the model-checking problem for the subclass of PCTL which does not permit both punctual timing bounds, which require the occurrence of an event at an exact time point, and comparisons with probability bounds other than 0 or 1, is PTIME-complete for one-clock probabilistic timed automata.",
        "published": "2008-08-30T13:26:48Z",
        "link": "http://arxiv.org/abs/0809.0060v3",
        "categories": [
            "cs.LO",
            "D.2.4; F.4.1; G.3"
        ]
    },
    {
        "title": "Light Logics and the Call-by-Value Lambda Calculus",
        "authors": [
            "Paolo Coppola",
            "Ugo Dal Lago",
            "Simona Ronchi Della Rocca"
        ],
        "summary": "The so-called light logics have been introduced as logical systems enjoying quite remarkable normalization properties. Designing a type assignment system for pure lambda calculus from these logics, however, is problematic. In this paper we show that shifting from usual call-by-name to call-by-value lambda calculus allows regaining strong connections with the underlying logic. This will be done in the context of Elementary Affine Logic (EAL), designing a type system in natural deduction style assigning EAL formulae to lambda terms.",
        "published": "2008-09-01T09:46:36Z",
        "link": "http://arxiv.org/abs/0809.0195v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "The Complexity of Enriched Mu-Calculi",
        "authors": [
            "Piero A. Bonatti",
            "Carsten Lutz",
            "Aniello Murano",
            "Moshe Y. Vardi"
        ],
        "summary": "The fully enriched &mu;-calculus is the extension of the propositional &mu;-calculus with inverse programs, graded modalities, and nominals. While satisfiability in several expressive fragments of the fully enriched &mu;-calculus is known to be decidable and ExpTime-complete, it has recently been proved that the full calculus is undecidable. In this paper, we study the fragments of the fully enriched &mu;-calculus that are obtained by dropping at least one of the additional constructs. We show that, in all fragments obtained in this way, satisfiability is decidable and ExpTime-complete. Thus, we identify a family of decidable logics that are maximal (and incomparable) in expressive power. Our results are obtained by introducing two new automata models, showing that their emptiness problems are ExpTime-complete, and then reducing satisfiability in the relevant logics to these problems. The automata models we introduce are two-way graded alternating parity automata over infinite trees (2GAPTs) and fully enriched automata (FEAs) over infinite forests. The former are a common generalization of two incomparable automata models from the literature. The latter extend alternating automata in a similar way as the fully enriched &mu;-calculus extends the standard &mu;-calculus.",
        "published": "2008-09-02T07:51:04Z",
        "link": "http://arxiv.org/abs/0809.0360v2",
        "categories": [
            "cs.LO",
            "cs.CL",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Interaction Grammars",
        "authors": [
            "Bruno Guillaume",
            "Guy Perrier"
        ],
        "summary": "Interaction Grammar (IG) is a grammatical formalism based on the notion of polarity. Polarities express the resource sensitivity of natural languages by modelling the distinction between saturated and unsaturated syntactic structures. Syntactic composition is represented as a chemical reaction guided by the saturation of polarities. It is expressed in a model-theoretic framework where grammars are constraint systems using the notion of tree description and parsing appears as a process of building tree description models satisfying criteria of saturation and minimality.",
        "published": "2008-09-02T18:37:01Z",
        "link": "http://arxiv.org/abs/0809.0494v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Peek Arc Consistency",
        "authors": [
            "Manuel Bodirsky",
            "Hubie Chen"
        ],
        "summary": "This paper studies peek arc consistency, a reasoning technique that extends the well-known arc consistency technique for constraint satisfaction. In contrast to other more costly extensions of arc consistency that have been studied in the literature, peek arc consistency requires only linear space and quadratic time and can be parallelized in a straightforward way such that it runs in linear time with a linear number of processors. We demonstrate that for various constraint languages, peek arc consistency gives a polynomial-time decision procedure for the constraint satisfaction problem. We also present an algebraic characterization of those constraint languages that can be solved by peek arc consistency, and study the robustness of the algorithm.",
        "published": "2008-09-04T11:15:50Z",
        "link": "http://arxiv.org/abs/0809.0788v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Superposition for Fixed Domains",
        "authors": [
            "Matthias Horbach",
            "Christoph Weidenbach"
        ],
        "summary": "Superposition is an established decision procedure for a variety of first-order logic theories represented by sets of clauses. A satisfiable theory, saturated by superposition, implicitly defines a minimal term-generated model for the theory. Proving universal properties with respect to a saturated theory directly leads to a modification of the minimal model's term-generated domain, as new Skolem functions are introduced. For many applications, this is not desired.   Therefore, we propose the first superposition calculus that can explicitly represent existentially quantified variables and can thus compute with respect to a given domain. This calculus is sound and refutationally complete in the limit for a first-order fixed domain semantics. For saturated Horn theories and classes of positive formulas, we can even employ the calculus to prove properties of the minimal model itself, going beyond the scope of known superposition-based approaches.",
        "published": "2008-09-04T21:58:58Z",
        "link": "http://arxiv.org/abs/0809.0922v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; F.4.1"
        ]
    },
    {
        "title": "Bounded Underapproximations",
        "authors": [
            "Pierre Ganty",
            "Rupak Majumdar",
            "Benjamin Monmege"
        ],
        "summary": "We show a new and constructive proof of the following language-theoretic result: for every context-free language L, there is a bounded context-free language L' included in L which has the same Parikh (commutative) image as L. Bounded languages, introduced by Ginsburg and Spanier, are subsets of regular languages of the form w1*w2*...wk* for some finite words w1,...,wk. In particular bounded subsets of context-free languages have nice structural and decidability properties. Our proof proceeds in two parts. First, using Newton's iterations on the language semiring, we construct a context-free subset Ls of L that can be represented as a sequence of substitutions on a linear language and has the same Parikh image as L. Second, we inductively construct a Parikh-equivalent bounded context-free subset of Ls.   We show two applications of this result in model checking: to underapproximate the reachable state space of multithreaded procedural programs and to underapproximate the reachable state space of recursive counter programs. The bounded language constructed above provides a decidable underapproximation for the original problems. By iterating the construction, we get a semi-algorithm for the original problems that constructs a sequence of underapproximations such that no two underapproximations of the sequence can be compared. This provides a progress guarantee: every word w in L is in some underapproximation of the sequence. In addition, we show that our approach subsumes context-bounded reachability for multithreaded programs.",
        "published": "2008-09-07T22:21:33Z",
        "link": "http://arxiv.org/abs/0809.1236v4",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A computer verified, monadic, functional implementation of the integral",
        "authors": [
            "Russell O'Connor",
            "Bas Spitters"
        ],
        "summary": "We provide a computer verified exact monadic functional implementation of the Riemann integral in type theory. Together with previous work by O'Connor, this may be seen as the beginning of the realization of Bishop's vision to use constructive mathematics as a programming language for exact analysis.",
        "published": "2008-09-08T20:54:36Z",
        "link": "http://arxiv.org/abs/0809.1552v2",
        "categories": [
            "cs.LO",
            "cs.NA"
        ]
    },
    {
        "title": "Probabilistic Systems with LimSup and LimInf Objectives",
        "authors": [
            "Krishnendu Chatterjee",
            "Thomas A. Henzinger"
        ],
        "summary": "We give polynomial-time algorithms for computing the values of Markov decision processes (MDPs) with limsup and liminf objectives. A real-valued reward is assigned to each state, and the value of an infinite path in the MDP is the limsup (resp. liminf) of all rewards along the path. The value of an MDP is the maximal expected value of an infinite path that can be achieved by resolving the decisions of the MDP. Using our result on MDPs, we show that turn-based stochastic games with limsup and liminf objectives can be solved in NP \\cap coNP.",
        "published": "2008-09-09T00:12:05Z",
        "link": "http://arxiv.org/abs/0809.1465v1",
        "categories": [
            "cs.GT",
            "cs.LO"
        ]
    },
    {
        "title": "Computing with Classical Real Numbers",
        "authors": [
            "Cezary Kaliszyk",
            "Russell O'Connor"
        ],
        "summary": "There are two incompatible Coq libraries that have a theory of the real numbers; the Coq standard library gives an axiomatic treatment of classical real numbers, while the CoRN library from Nijmegen defines constructively valid real numbers. Unfortunately, this means results about one structure cannot easily be used in the other structure. We present a way interfacing these two libraries by showing that their real number structures are isomorphic assuming the classical axioms already present in the standard library reals. This allows us to use O'Connor's decision procedure for solving ground inequalities present in CoRN to solve inequalities about the reals from the Coq standard library, and it allows theorems from the Coq standard library to apply to problem about the CoRN reals.",
        "published": "2008-09-09T19:55:03Z",
        "link": "http://arxiv.org/abs/0809.1644v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Topological Complexity of omega-Powers : Extended Abstract",
        "authors": [
            "Olivier Finkel",
            "Dominique Lecomte"
        ],
        "summary": "This is an extended abstract presenting new results on the topological complexity of omega-powers (which are included in a paper \"Classical and effective descriptive complexities of omega-powers\" available from arXiv:0708.4176) and reflecting also some open questions which were discussed during the Dagstuhl seminar on \"Topological and Game-Theoretic Aspects of Infinite Computations\" 29.06.08 - 04.07.08.",
        "published": "2008-09-10T15:13:34Z",
        "link": "http://arxiv.org/abs/0809.1812v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "Weyl's Predicative Classical Mathematics as a Logic-Enriched Type Theory",
        "authors": [
            "Robin Adams",
            "Zhaohui Luo"
        ],
        "summary": "We construct a logic-enriched type theory LTTW that corresponds closely to the predicative system of foundations presented by Hermann Weyl in Das Kontinuum. We formalise many results from that book in LTTW, including Weyl's definition of the cardinality of a set and several results from real analysis, using the proof assistant Plastic that implements the logical framework LF. This case study shows how type theory can be used to represent a non-constructive foundation for mathematics.",
        "published": "2008-09-11T17:25:28Z",
        "link": "http://arxiv.org/abs/0809.2061v3",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "On (Omega-)Regular Model Checking",
        "authors": [
            "Axel Legay",
            "Pierre Wolper"
        ],
        "summary": "Checking infinite-state systems is frequently done by encoding infinite sets of states as regular languages. Computing such a regular representation of, say, the set of reachable states of a system requires acceleration techniques that can finitely compute the effect of an unbounded number of transitions. Among the acceleration techniques that have been proposed, one finds both specific and generic techniques. Specific techniques exploit the particular type of system being analyzed, e.g. a system manipulating queues or integers, whereas generic techniques only assume that the transition relation is represented by a finite-state transducer, which has to be iterated. In this paper, we investigate the possibility of using generic techniques in cases where only specific techniques have been exploited so far. Finding that existing generic techniques are often not applicable in cases easily handled by specific techniques, we have developed a new approach to iterating transducers. This new approach builds on earlier work, but exploits a number of new conceptual and algorithmic ideas, often induced with the help of experiments, that give it a broad scope, as well as good performances.",
        "published": "2008-09-12T13:40:37Z",
        "link": "http://arxiv.org/abs/0809.2214v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Datalog and Constraint Satisfaction with Infinite Templates",
        "authors": [
            "Manuel Bodirsky",
            "Victor Dalmau"
        ],
        "summary": "On finite structures, there is a well-known connection between the expressive power of Datalog, finite variable logics, the existential pebble game, and bounded hypertree duality. We study this connection for infinite structures. This has applications for constraint satisfaction with infinite templates. If the template Gamma is omega-categorical, we present various equivalent characterizations of those Gamma such that the constraint satisfaction problem (CSP) for Gamma can be solved by a Datalog program. We also show that CSP(Gamma) can be solved in polynomial time for arbitrary omega-categorical structures Gamma if the input is restricted to instances of bounded treewidth. Finally, we characterize those omega-categorical templates whose CSP has Datalog width 1, and those whose CSP has strict Datalog width k.",
        "published": "2008-09-14T10:01:27Z",
        "link": "http://arxiv.org/abs/0809.2386v3",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Structures de réalisabilité, RAM et ultrafiltre sur N",
        "authors": [
            "Jean-Louis Krivine"
        ],
        "summary": "We show how to transform into programs the proofs in classical Analysis which use the existence of an ultrafilter on the integers. The method mixes the classical realizability introduced by the author, with the \"forcing\" of P. Cohen. The programs we obtain, use read and write instructions in random access memory.",
        "published": "2008-09-14T12:01:24Z",
        "link": "http://arxiv.org/abs/0809.2394v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Monadic Datalog over Finite Structures with Bounded Treewidth",
        "authors": [
            "Georg Gottlob",
            "Reinhard Pichler",
            "Fang Wei"
        ],
        "summary": "Bounded treewidth and Monadic Second Order (MSO) logic have proved to be key concepts in establishing fixed-parameter tractability results. Indeed, by Courcelle's Theorem we know: Any property of finite structures, which is expressible by an MSO sentence, can be decided in linear time (data complexity) if the structures have bounded treewidth.   In principle, Courcelle's Theorem can be applied directly to construct concrete algorithms by transforming the MSO evaluation problem into a tree language recognition problem. The latter can then be solved via a finite tree automaton (FTA). However, this approach has turned out to be problematical, since even relatively simple MSO formulae may lead to a ``state explosion'' of the FTA.   In this work we propose monadic datalog (i.e., datalog where all intentional predicate symbols are unary) as an alternative method to tackle this class of fixed-parameter tractable problems. We show that if some property of finite structures is expressible in MSO then this property can also be expressed by means of a monadic datalog program over the structure plus the tree decomposition.   Moreover, we show that the resulting fragment of datalog can be evaluated in linear time (both w.r.t. the program size and w.r.t. the data size). This new approach is put to work by devising new algorithms for the 3-Colorability problem of graphs and for the PRIMALITY problem of relational schemas (i.e., testing if some attribute in a relational schema is part of a key). We also report on experimental results with a prototype implementation.",
        "published": "2008-09-18T12:40:49Z",
        "link": "http://arxiv.org/abs/0809.3140v1",
        "categories": [
            "cs.DB",
            "cs.CC",
            "cs.LO",
            "F.2.2; F.4.1"
        ]
    },
    {
        "title": "Formalising the pi-calculus using nominal logic",
        "authors": [
            "Jesper Bengtson",
            "Joachim Parrow"
        ],
        "summary": "We formalise the pi-calculus using the nominal datatype package, based on ideas from the nominal logic by Pitts et al., and demonstrate an implementation in Isabelle/HOL. The purpose is to derive powerful induction rules for the semantics in order to conduct machine checkable proofs, closely following the intuitive arguments found in manual proofs. In this way we have covered many of the standard theorems of bisimulation equivalence and congruence, both late and early, and both strong and weak in a uniform manner. We thus provide one of the most extensive formalisations of a process calculus ever done inside a theorem prover.   A significant gain in our formulation is that agents are identified up to alpha-equivalence, thereby greatly reducing the arguments about bound names. This is a normal strategy for manual proofs about the pi-calculus, but that kind of hand waving has previously been difficult to incorporate smoothly in an interactive theorem prover. We show how the nominal logic formalism and its support in Isabelle accomplishes this and thus significantly reduces the tedium of conducting completely formal proofs. This improves on previous work using weak higher order abstract syntax since we do not need extra assumptions to filter out exotic terms and can keep all arguments within a familiar first-order logic.",
        "published": "2008-09-23T16:39:03Z",
        "link": "http://arxiv.org/abs/0809.3960v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Termination Criteria for Solving Concurrent Safety and Reachability   Games",
        "authors": [
            "Krishnendu Chatterjee",
            "Luca de Alfaro",
            "Thomas A. Henzinger"
        ],
        "summary": "We consider concurrent games played on graphs. At every round of a game, each player simultaneously and independently selects a move; the moves jointly determine the transition to a successor state. Two basic objectives are the safety objective to stay forever in a given set of states, and its dual, the reachability objective to reach a given set of states. We present in this paper a strategy improvement algorithm for computing the value of a concurrent safety game, that is, the maximal probability with which player~1 can enforce the safety objective. The algorithm yields a sequence of player-1 strategies which ensure probabilities of winning that converge monotonically to the value of the safety game.   Our result is significant because the strategy improvement algorithm provides, for the first time, a way to approximate the value of a concurrent safety game from below. Since a value iteration algorithm, or a strategy improvement algorithm for reachability games, can be used to approximate the same value from above, the combination of both algorithms yields a method for computing a converging sequence of upper and lower bounds for the values of concurrent reachability and safety games. Previous methods could approximate the values of these games only from one direction, and as no rates of convergence are known, they did not provide a practical way to solve these games.",
        "published": "2008-09-23T20:29:37Z",
        "link": "http://arxiv.org/abs/0809.4017v1",
        "categories": [
            "cs.GT",
            "cs.LO"
        ]
    },
    {
        "title": "Bisimilarity and Behaviour-Preserving Reconfigurations of Open Petri   Nets",
        "authors": [
            "Paolo Baldan",
            "Andrea Corradini",
            "Hartmut Ehrig",
            "Reiko Heckel",
            "Barbara König"
        ],
        "summary": "We propose a framework for the specification of behaviour-preserving reconfigurations of systems modelled as Petri nets. The framework is based on open nets, a mild generalisation of ordinary Place/Transition nets suited to model open systems which might interact with the surrounding environment and endowed with a colimit-based composition operation. We show that natural notions of bisimilarity over open nets are congruences with respect to the composition operation. The considered behavioural equivalences differ for the choice of the observations, which can be single firings or parallel steps. Additionally, we consider weak forms of such equivalences, arising in the presence of unobservable actions. We also provide an up-to technique for facilitating bisimilarity proofs. The theory is used to identify suitable classes of reconfiguration rules (in the double-pushout approach to rewriting) whose application preserves the observational semantics of the net.",
        "published": "2008-09-24T09:28:18Z",
        "link": "http://arxiv.org/abs/0809.4115v2",
        "categories": [
            "cs.LO",
            "F.4.1, D.2.2, D.3.1"
        ]
    },
    {
        "title": "Mechanistic Behavior of Single-Pass Instruction Sequences",
        "authors": [
            "Jan A. Bergstra",
            "Mark B. van der Zwaag"
        ],
        "summary": "Earlier work on program and thread algebra detailed the functional, observable behavior of programs under execution. In this article we add the modeling of unobservable, mechanistic processing, in particular processing due to jump instructions. We model mechanistic processing preceding some further behavior as a delay of that behavior; we borrow a unary delay operator from discrete time process algebra. We define a mechanistic improvement ordering on threads and observe that some threads do not have an optimal implementation.",
        "published": "2008-09-26T13:57:36Z",
        "link": "http://arxiv.org/abs/0809.4635v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.4; F.3.2; F.3.3"
        ]
    },
    {
        "title": "An Evidential Path Logic for Multi-Relational Networks",
        "authors": [
            "Marko A. Rodriguez",
            "Joe Geldart"
        ],
        "summary": "Multi-relational networks are used extensively to structure knowledge. Perhaps the most popular instance, due to the widespread adoption of the Semantic Web, is the Resource Description Framework (RDF). One of the primary purposes of a knowledge network is to reason; that is, to alter the topology of the network according to an algorithm that uses the existing topological structure as its input. There exist many such reasoning algorithms. With respect to the Semantic Web, the bivalent, monotonic reasoners of the RDF Schema (RDFS) and the Web Ontology Language (OWL) are the most prevalent. However, nothing prevents other forms of reasoning from existing in the Semantic Web. This article presents a non-bivalent, non-monotonic, evidential logic and reasoner that is an algebraic ring over a multi-relational network equipped with two binary operations that can be composed to execute various forms of inference. Given its multi-relational grounding, it is possible to use the presented evidential framework as another method for structuring knowledge and reasoning in the Semantic Web. The benefits of this framework are that it works with arbitrary, partial, and contradictory knowledge while, at the same time, it supports a tractable approximate reasoning process.",
        "published": "2008-10-08T17:49:15Z",
        "link": "http://arxiv.org/abs/0810.1481v2",
        "categories": [
            "cs.LO",
            "cs.SC",
            "I.2.4"
        ]
    },
    {
        "title": "On characterising strong bisimilarity in a fragment of CCS with   replication",
        "authors": [
            "Daniel Hirschkoff",
            "Damien Pous"
        ],
        "summary": "We provide a characterisation of strong bisimilarity in a fragment of CCS that contains only prefix, parallel composition, synchronisation and a limited form of replication. The characterisation is not an axiomatisation, but is instead presented as a rewriting system. We discuss how our method allows us to derive a new congruence result in the $\\pi$-calculus: congruence holds in the sub-calculus that does not include restriction nor sum, and features a limited form of replication. We have not formalised the latter result in all details.",
        "published": "2008-10-12T18:54:26Z",
        "link": "http://arxiv.org/abs/0810.2061v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Structural abstract interpretation, A formal study using Coq",
        "authors": [
            "Yves Bertot"
        ],
        "summary": "interpreters are tools to compute approximations for behaviors of a program. These approximations can then be used for optimisation or for error detection. In this paper, we show how to describe an abstract interpreter using the type-theory based theorem prover Coq, using inductive types for syntax and structural recursive programming for the abstract interpreter's kernel. The abstract interpreter can then be proved correct with respect to a Hoare logic for the programming language.",
        "published": "2008-10-13T09:07:38Z",
        "link": "http://arxiv.org/abs/0810.2179v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "On combinations of local theory extensions",
        "authors": [
            "Viorica Sofronie-Stokkermans"
        ],
        "summary": "In this paper we study possibilities of efficient reasoning in combinations of theories over possibly non-disjoint signatures. We first present a class of theory extensions (called local extensions) in which hierarchical reasoning is possible, and give several examples from computer science and mathematics in which such extensions occur in a natural way. We then identify situations in which combinations of local extensions of a theory are again local extensions of that theory. We thus obtain criteria both for recognizing wider classes of local theory extensions, and for modular reasoning in combinations of theories over non-disjoint signatures.",
        "published": "2008-10-15T10:26:02Z",
        "link": "http://arxiv.org/abs/0810.2653v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Sheaves and geometric logic and applications to the modular verification   of complex systems",
        "authors": [
            "Viorica Sofronie-Stokkermans"
        ],
        "summary": "In this paper we show that states, transitions and behavior of concurrent systems can often be modeled as sheaves over a suitable topological space. In this context, geometric logic can be used to describe which local properties (i.e. properties of individual systems) are preserved, at a global level, when interconnecting the systems. The main area of application is to modular verification of complex systems. We illustrate the ideas by means of an example involving a family of interacting controllers for trains on a rail track.",
        "published": "2008-10-16T09:38:51Z",
        "link": "http://arxiv.org/abs/0810.2877v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Taming Modal Impredicativity: Superlazy Reduction",
        "authors": [
            "Ugo Dal Lago",
            "Luca Roversi",
            "Luca Vercelli"
        ],
        "summary": "Pure, or type-free, Linear Logic proof nets are Turing complete once cut-elimination is considered as computation. We introduce modal impredicativity as a new form of impredicativity causing the complexity of cut-elimination to be problematic from a complexity point of view. Modal impredicativity occurs when, during reduction, the conclusion of a residual of a box b interacts with a node that belongs to the proof net inside another residual of b. Technically speaking, superlazy reduction is a new notion of reduction that allows to control modal impredicativity. More specifically, superlazy reduction replicates a box only when all its copies are opened. This makes the overall cost of reducing a proof net finite and predictable. Specifically, superlazy reduction applied to any pure proof nets takes primitive recursive time. Moreover, any primitive recursive function can be computed by a pure proof net via superlazy reduction.",
        "published": "2008-10-16T12:37:26Z",
        "link": "http://arxiv.org/abs/0810.2891v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Clone Theory: Its Syntax and Semantics, Applications to Universal   Algebra, Lambda Calculus and Algebraic Logic",
        "authors": [
            "Zhaohua Luo"
        ],
        "summary": "The primary goal of this paper is to present a unified way to transform the syntax of a logic system into certain initial algebraic structure so that it can be studied algebraically. The algebraic structures which one may choose for this purpose are various clones over a full subcategory of a category. We show that the syntax of equational logic, lambda calculus and first order logic can be represented as clones or right algebras of clones over the set of positive integers. The semantics is then represented by structures derived from left algebras of these clones.",
        "published": "2008-10-17T17:46:14Z",
        "link": "http://arxiv.org/abs/0810.3162v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A sound spatio-temporal Hoare logic for the verification of structured   interactive programs with registers and voices",
        "authors": [
            "Cezara Dragoi",
            "Gheorghe Stefanescu"
        ],
        "summary": "Interactive systems with registers and voices (shortly, \"rv-systems\") are a model for interactive computing obtained closing register machines with respect to a space-time duality transformation (\"voices\" are the time-dual counterparts of \"registers\"). In the same vain, AGAPIA v0.1, a structured programming language for rv-systems, is the space-time dual closure of classical while programs (over a specific type of data). Typical AGAPIA programs describe open processes located at various sites and having their temporal windows of adequate reaction to the environment. The language naturally supports process migration, structured interaction, and deployment of components on heterogeneous machines.   In this paper a sound Hoare-like spatio-temporal logic for the verification of AGAPIA v0.1 programs is introduced. As a case study, a formal verification proof of a popular distributed termination detection protocol is presented.",
        "published": "2008-10-19T00:05:20Z",
        "link": "http://arxiv.org/abs/0810.3332v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.1.2; F.3; D.2.4; D.3.2"
        ]
    },
    {
        "title": "Characterising Testing Preorders for Finite Probabilistic Processes",
        "authors": [
            "Yuxin Deng",
            "Matthew Hennessy",
            "Rob van Glabbeek",
            "Carroll Morgan"
        ],
        "summary": "In 1992 Wang & Larsen extended the may- and must preorders of De Nicola and Hennessy to processes featuring probabilistic as well as nondeterministic choice. They concluded with two problems that have remained open throughout the years, namely to find complete axiomatisations and alternative characterisations for these preorders. This paper solves both problems for finite processes with silent moves. It characterises the may preorder in terms of simulation, and the must preorder in terms of failure simulation. It also gives a characterisation of both preorders using a modal logic. Finally it axiomatises both preorders over a probabilistic version of CSP.",
        "published": "2008-10-21T04:48:18Z",
        "link": "http://arxiv.org/abs/0810.3708v2",
        "categories": [
            "cs.LO",
            "F.3.2; D.3.1"
        ]
    },
    {
        "title": "Binding bigraphs as symmetric monoidal closed theories",
        "authors": [
            "Tom Hirschowitz",
            "Aurélien Pardon"
        ],
        "summary": "Milner's bigraphs are a general framework for reasoning about distributed and concurrent programming languages. Notably, it has been designed to encompass both the pi-calculus and the Ambient calculus. This paper is only concerned with bigraphical syntax: given what we here call a bigraphical signature K, Milner constructs a (pre-) category of bigraphs BBig(K), whose main features are (1) the presence of relative pushouts (RPOs), which makes them well-behaved w.r.t. bisimulations, and that (2) the so-called structural equations become equalities. Examples of the latter include, e.g., in pi and Ambient, renaming of bound variables, associativity and commutativity of parallel composition, or scope extrusion for restricted names. Also, bigraphs follow a scoping discipline ensuring that, roughly, bound variables never escape their scope. Here, we reconstruct bigraphs using a standard categorical tool: symmetric monoidal closed (SMC) theories. Our theory enforces the same scoping discipline as bigraphs, as a direct property of SMC structure. Furthermore, it elucidates the slightly mysterious status of so-called links in bigraphs. Finally, our category is also considerably larger than the category of bigraphs, notably encompassing in the same framework terms and a flexible form of higher-order contexts.",
        "published": "2008-10-24T09:33:08Z",
        "link": "http://arxiv.org/abs/0810.4419v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Graphical Presentations of Symmetric Monoidal Closed Theories",
        "authors": [
            "Richard Garner",
            "Tom Hirschowitz",
            "Aurélien Pardon"
        ],
        "summary": "We define a notion of symmetric monoidal closed (SMC) theory, consisting of a SMC signature augmented with equations, and describe the classifying categories of such theories in terms of proof nets.",
        "published": "2008-10-24T09:35:41Z",
        "link": "http://arxiv.org/abs/0810.4420v2",
        "categories": [
            "cs.LO",
            "math.CT"
        ]
    },
    {
        "title": "Logics for XML",
        "authors": [
            "Pierre Geneves"
        ],
        "summary": "This thesis describes the theoretical and practical foundations of a system for the static analysis of XML processing languages. The system relies on a fixpoint temporal logic with converse, derived from the mu-calculus, where models are finite trees. This calculus is expressive enough to capture regular tree types along with multi-directional navigation in trees, while having a single exponential time complexity. Specifically the decidability of the logic is proved in time 2^O(n) where n is the size of the input formula.   Major XML concepts are linearly translated into the logic: XPath navigation and node selection semantics, and regular tree languages (which include DTDs and XML Schemas). Based on these embeddings, several problems of major importance in XML applications are reduced to satisfiability of the logic. These problems include XPath containment, emptiness, equivalence, overlap, coverage, in the presence or absence of regular tree type constraints, and the static type-checking of an annotated query.   The focus is then given to a sound and complete algorithm for deciding the logic, along with a detailed complexity analysis, and crucial implementation techniques for building an effective solver. Practical experiments using a full implementation of the system are presented. The system appears to be efficient in practice for several realistic scenarios.   The main application of this work is a new class of static analyzers for programming languages using both XPath expressions and XML type annotations (input and output). Such analyzers allow to ensure at compile-time valuable properties such as type-safety and optimizations, for safer and more efficient XML processing.",
        "published": "2008-10-24T13:40:11Z",
        "link": "http://arxiv.org/abs/0810.4460v2",
        "categories": [
            "cs.PL",
            "cs.DB",
            "cs.LO",
            "D.3.0; D.3.1; D.3.4; E.1; F.3.1; F.3.2; F.4.1; F.4.3; H.2.3; I.2.4;\n  I.7.2"
        ]
    },
    {
        "title": "On Finite Bases for Weak Semantics: Failures versus Impossible Futures",
        "authors": [
            "Taolue Chen",
            "Wan Fokkink",
            "Rob van Glabbeek"
        ],
        "summary": "We provide a finite basis for the (in)equational theory of the process algebra BCCS modulo the weak failures preorder and equivalence. We also give positive and negative results regarding the axiomatizability of BCCS modulo weak impossible futures semantics.",
        "published": "2008-10-27T18:48:18Z",
        "link": "http://arxiv.org/abs/0810.4904v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Automatic structures of bounded degree revisited",
        "authors": [
            "Dietrich Kuske",
            "Markus Lohrey"
        ],
        "summary": "The first-order theory of a string automatic structure is known to be decidable, but there are examples of string automatic structures with nonelementary first-order theories. We prove that the first-order theory of a string automatic structure of bounded degree is decidable in doubly exponential space (for injective automatic presentations, this holds even uniformly). This result is shown to be optimal since we also present a string automatic structure of bounded degree whose first-order theory is hard for 2EXPSPACE. We prove similar results also for tree automatic structures. These findings close the gaps left open in a previous paper of the second author by improving both, the lower and the upper bounds.",
        "published": "2008-10-28T09:49:06Z",
        "link": "http://arxiv.org/abs/0810.4998v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "P is not equal to NP",
        "authors": [
            "Sten-Ake Tarnlund"
        ],
        "summary": "SAT is not in P, is true and provable in a simply consistent extension B' of a first order theory B of computing, with a single finite axiom characterizing a universal Turing machine. Therefore, P is not equal to NP, is true and provable in a simply consistent extension B\" of B.",
        "published": "2008-10-28T16:32:58Z",
        "link": "http://arxiv.org/abs/0810.5056v2",
        "categories": [
            "cs.CC",
            "cs.LO",
            "D.1.6; F.1.3; F.4.1"
        ]
    },
    {
        "title": "Symbolic model checking of tense logics on rational Kripke models",
        "authors": [
            "Wilmari Bekker",
            "Valentin Goranko"
        ],
        "summary": "We introduce the class of rational Kripke models and study symbolic model checking of the basic tense logic Kt and some extensions of it in models from that class. Rational Kripke models are based on (generally infinite) rational graphs, with vertices labeled by the words in some regular language and transitions recognized by asynchronous two-head finite automata, also known as rational transducers. Every atomic proposition in a rational Kripke model is evaluated in a regular set of states. We show that every formula of Kt has an effectively computable regular extension in every rational Kripke model, and therefore local model checking and global model checking of Kt in rational Kripke models are decidable. These results are lifted to a number of extensions of Kt. We study and partly determine the complexity of the model checking procedures.",
        "published": "2008-10-30T15:59:53Z",
        "link": "http://arxiv.org/abs/0810.5516v1",
        "categories": [
            "cs.LO",
            "D.2.4; F.4.1"
        ]
    },
    {
        "title": "Model checking memoryful linear-time logics over one-counter automata",
        "authors": [
            "Stephane Demri",
            "Ranko Lazic",
            "Arnaud Sangnier"
        ],
        "summary": "We study complexity of the model-checking problems for LTL with registers (also known as freeze LTL) and for first-order logic with data equality tests over one-counter automata. We consider several classes of one-counter automata (mainly deterministic vs. nondeterministic) and several logical fragments (restriction on the number of registers or variables and on the use of propositional variables for control locations). The logics have the ability to store a counter value and to test it later against the current counter value. We show that model checking over deterministic one-counter automata is PSPACE-complete with infinite and finite accepting runs. By constrast, we prove that model checking freeze LTL in which the until operator is restricted to the eventually operator over nondeterministic one-counter automata is undecidable even if only one register is used and with no propositional variable. As a corollary of our proof, this also holds for first-order logic with data equality tests restricted to two variables. This makes a difference with the facts that several verification problems for one-counter automata are known to be decidable with relatively low complexity, and that finitary satisfiability for the two logics are decidable. Our results pave the way for model-checking memoryful (linear-time) logics over other classes of operational models, such as reversal-bounded counter machines.",
        "published": "2008-10-30T16:04:06Z",
        "link": "http://arxiv.org/abs/0810.5517v2",
        "categories": [
            "cs.LO",
            "F.1.1; F.4.1"
        ]
    },
    {
        "title": "A triangle-based logic for affine-invariant querying of spatial and   spatio-temporal data",
        "authors": [
            "Sofie Haesevoets",
            "Bart Kuijpers"
        ],
        "summary": "In spatial databases, incompatibilities often arise due to different choices of origin or unit of measurement (e.g., centimeters versus inches). By representing and querying the data in an affine-invariant manner, we can avoid these incompatibilities.   In practice, spatial (resp., spatio-temporal) data is often represented as a finite union of triangles (resp., moving triangles). As two arbitrary triangles are equal up to a unique affinity of the plane, they seem perfect candidates as basic units for an affine-invariant query language.   We propose a so-called \"triangle logic\", a query language that is affine-generic and has triangles as basic elements. We show that this language has the same expressive power as the affine-generic fragment of first-order logic over the reals on triangle databases. We illustrate that the proposed language is simple and intuitive. It can also serve as a first step towards a \"moving-triangle logic\" for spatio-temporal data.",
        "published": "2008-10-31T16:16:30Z",
        "link": "http://arxiv.org/abs/0810.5725v2",
        "categories": [
            "cs.LO",
            "cs.DB",
            "H.2.3; H.2.8; F.4.0"
        ]
    },
    {
        "title": "Multi-Objective Model Checking of Markov Decision Processes",
        "authors": [
            "Kousha Etessami",
            "Marta Kwiatkowska",
            "Moshe Y. Vardi",
            "Mihalis Yannakakis"
        ],
        "summary": "We study and provide efficient algorithms for multi-objective model checking problems for Markov Decision Processes (MDPs). Given an MDP, M, and given multiple linear-time (\\omega -regular or LTL) properties \\varphi\\_i, and probabilities r\\_i \\epsilon [0,1], i=1,...,k, we ask whether there exists a strategy \\sigma for the controller such that, for all i, the probability that a trajectory of M controlled by \\sigma satisfies \\varphi\\_i is at least r\\_i. We provide an algorithm that decides whether there exists such a strategy and if so produces it, and which runs in time polynomial in the size of the MDP. Such a strategy may require the use of both randomization and memory. We also consider more general multi-objective \\omega -regular queries, which we motivate with an application to assume-guarantee compositional reasoning for probabilistic systems.   Note that there can be trade-offs between different properties: satisfying property \\varphi\\_1 with high probability may necessitate satisfying \\varphi\\_2 with low probability. Viewing this as a multi-objective optimization problem, we want information about the \"trade-off curve\" or Pareto curve for maximizing the probabilities of different properties. We show that one can compute an approximate Pareto curve with respect to a set of \\omega -regular properties in time polynomial in the size of the MDP.   Our quantitative upper bounds use LP methods. We also study qualitative multi-objective model checking problems, and we show that these can be analysed by purely graph-theoretic methods, even though the strategies may still require both randomization and memory.",
        "published": "2008-10-31T16:18:14Z",
        "link": "http://arxiv.org/abs/0810.5728v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.GT",
            "G.3; F.2; F.3.1; F.4.1"
        ]
    },
    {
        "title": "Automatic Modular Abstractions for Linear Constraints",
        "authors": [
            "David Monniaux"
        ],
        "summary": "We propose a method for automatically generating abstract transformers for static analysis by abstract interpretation. The method focuses on linear constraints on programs operating on rational, real or floating-point variables and containing linear assignments and tests. In addition to loop-free code, the same method also applies for obtaining least fixed points as functions of the precondition, which permits the analysis of loops and recursive functions. Our algorithms are based on new quantifier elimination and symbolic manipulation techniques. Given the specification of an abstract domain, and a program block, our method automatically outputs an implementation of the corresponding abstract transformer. It is thus a form of program transformation. The motivation of our work is data-flow synchronous programming languages, used for building control-command embedded systems, but it also applies to imperative and functional programming.",
        "published": "2008-11-02T14:47:44Z",
        "link": "http://arxiv.org/abs/0811.0166v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "Embedding Non-Ground Logic Programs into Autoepistemic Logic for   Knowledge Base Combination",
        "authors": [
            "Jos de Bruijn",
            "Thomas Eiter",
            "Axel Polleres",
            "Hans Tompits"
        ],
        "summary": "In the context of the Semantic Web, several approaches to the combination of ontologies, given in terms of theories of classical first-order logic and rule bases, have been proposed. They either cast rules into classical logic or limit the interaction between rules and ontologies. Autoepistemic logic (AEL) is an attractive formalism which allows to overcome these limitations, by serving as a uniform host language to embed ontologies and nonmonotonic logic programs into it. For the latter, so far only the propositional setting has been considered. In this paper, we present three embeddings of normal and three embeddings of disjunctive non-ground logic programs under the stable model semantics into first-order AEL. While the embeddings all correspond with respect to objective ground atoms, differences arise when considering non-atomic formulas and combinations with first-order theories. We compare the embeddings with respect to stable expansions and autoepistemic consequences, considering the embeddings by themselves, as well as combinations with classical theories. Our results reveal differences and correspondences of the embeddings and provide useful guidance in the choice of a particular embedding for knowledge combination.",
        "published": "2008-11-03T18:42:01Z",
        "link": "http://arxiv.org/abs/0811.0359v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Instruction sequences for the production of processes",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "Single-pass instruction sequences under execution are considered to produce behaviours to be controlled by some execution environment. Threads as considered in thread algebra model such behaviours: upon each action performed by a thread, a reply from its execution environment determines how the thread proceeds. Threads in turn can be looked upon as producing processes as considered in process algebra. We show that, by apposite choice of basic instructions, all processes that can only be in a finite number of states can be produced by single-pass instruction sequences.",
        "published": "2008-11-04T07:24:12Z",
        "link": "http://arxiv.org/abs/0811.0436v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.4; F.1.1; F.1.2; F.3.2"
        ]
    },
    {
        "title": "First-Order and Temporal Logics for Nested Words",
        "authors": [
            "Rajeev Alur",
            "Marcelo Arenas",
            "Pablo Barcelo",
            "Kousha Etessami",
            "Neil Immerman",
            "Leonid Libkin"
        ],
        "summary": "Nested words are a structured model of execution paths in procedural programs, reflecting their call and return nesting structure. Finite nested words also capture the structure of parse trees and other tree-structured data, such as XML. We provide new temporal logics for finite and infinite nested words, which are natural extensions of LTL, and prove that these logics are first-order expressively-complete. One of them is based on adding a \"within\" modality, evaluating a formula on a subword, to a logic CaRet previously studied in the context of verifying properties of recursive state machines (RSMs). The other logic, NWTL, is based on the notion of a summary path that uses both the linear and nesting structures. For NWTL we show that satisfiability is EXPTIME-complete, and that model-checking can be done in time polynomial in the size of the RSM model and exponential in the size of the NWTL formula (and is also EXPTIME-complete). Finally, we prove that first-order logic over nested words has the three-variable property, and we present a temporal logic for nested words which is complete for the two-variable fragment of first-order.",
        "published": "2008-11-04T15:30:12Z",
        "link": "http://arxiv.org/abs/0811.0537v3",
        "categories": [
            "cs.LO",
            "F.1.1, F.3.1, F.4.1"
        ]
    },
    {
        "title": "When are two algorithms the same?",
        "authors": [
            "Andreas Blass",
            "Nachum Dershowitz",
            "Yuri Gurevich"
        ],
        "summary": "People usually regard algorithms as more abstract than the programs that implement them. The natural way to formalize this idea is that algorithms are equivalence classes of programs with respect to a suitable equivalence relation. We argue that no such equivalence relation exists.",
        "published": "2008-11-05T20:38:22Z",
        "link": "http://arxiv.org/abs/0811.0811v1",
        "categories": [
            "cs.GL",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "Persistent Queries",
        "authors": [
            "Andreas Blass",
            "Yuri Gurevich"
        ],
        "summary": "We propose a syntax and semantics for interactive abstract state machines to deal with the following situation. A query is issued during a certain step, but the step ends before any reply is received. Later, a reply arrives, and later yet the algorithm makes use of this reply. By a persistent query, we mean a query for which a late reply might be used. Syntactically, our proposal involves issuing, along with a persistent query, a location where a late reply is to be stored. Semantically, it involves only a minor modification of the existing theory of interactive small-step abstract state machines.",
        "published": "2008-11-05T21:10:33Z",
        "link": "http://arxiv.org/abs/0811.0819v1",
        "categories": [
            "cs.PL",
            "cs.LO"
        ]
    },
    {
        "title": "The Complexity of Propositional Implication",
        "authors": [
            "Olaf Beyersdorff",
            "Arne Meier",
            "Michael Thomas",
            "Heribert Vollmer"
        ],
        "summary": "The question whether a set of formulae G implies a formula f is fundamental. The present paper studies the complexity of the above implication problem for propositional formulae that are built from a systematically restricted set of Boolean connectives. We give a complete complexity classification for all sets of Boolean functions in the meaning of Post's lattice and show that the implication problem is efficentily solvable only if the connectives are definable using the constants {false,true} and only one of {and,or,xor}. The problem remains coNP-complete in all other cases. We also consider the restriction of G to singletons.",
        "published": "2008-11-06T14:44:57Z",
        "link": "http://arxiv.org/abs/0811.0959v3",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "One useful logic that defines its own truth",
        "authors": [
            "Andreas Blass",
            "Yuri Gurevich"
        ],
        "summary": "Existential fixed point logic (EFPL) is a natural fit for some applications, and the purpose of this talk is to attract attention to EFPL. The logic is also interesting in its own right as it has attractive properties. One of those properties is rather unusual: truth of formulas can be defined (given appropriate syntactic apparatus) in the logic. We mentioned that property elsewhere, and we use this opportunity to provide the proof.",
        "published": "2008-11-06T15:09:43Z",
        "link": "http://arxiv.org/abs/0811.0964v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Two Forms of One Useful Logic: Existential Fixed Point Logic and Liberal   Datalog",
        "authors": [
            "Andreas Blass",
            "Yuri Gurevich"
        ],
        "summary": "A natural liberalization of Datalog is used in the Distributed Knowledge Authorization Language (DKAL). We show that the expressive power of this liberal Datalog is that of existential fixed-point logic. The exposition is self-contained.",
        "published": "2008-11-06T15:58:17Z",
        "link": "http://arxiv.org/abs/0811.0977v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Resolution Trees with Lemmas: Resolution Refinements that Characterize   DLL Algorithms with Clause Learning",
        "authors": [
            "Samuel R. Buss",
            "Jan Hoffmann",
            "Jan Johannsen"
        ],
        "summary": "Resolution refinements called w-resolution trees with lemmas (WRTL) and with input lemmas (WRTI) are introduced. Dag-like resolution is equivalent to both WRTL and WRTI when there is no regularity condition. For regular proofs, an exponential separation between regular dag-like resolution and both regular WRTL and regular WRTI is given.   It is proved that DLL proof search algorithms that use clause learning based on unit propagation can be polynomially simulated by regular WRTI. More generally, non-greedy DLL algorithms with learning by unit propagation are equivalent to regular WRTI. A general form of clause learning, called DLL-Learn, is defined that is equivalent to regular WRTL.   A variable extension method is used to give simulations of resolution by regular WRTI, using a simplified form of proof trace extensions. DLL-Learn and non-greedy DLL algorithms with learning by unit propagation can use variable extensions to simulate general resolution without doing restarts.   Finally, an exponential lower bound for WRTL where the lemmas are restricted to short clauses is shown.",
        "published": "2008-11-07T15:31:58Z",
        "link": "http://arxiv.org/abs/0811.1075v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "Action Theory Evolution",
        "authors": [
            "Ivan Varzinczak"
        ],
        "summary": "Like any other logical theory, domain descriptions in reasoning about actions may evolve, and thus need revision methods to adequately accommodate new information about the behavior of actions. The present work is about changing action domain descriptions in propositional dynamic logic. Its contribution is threefold: first we revisit the semantics of action theory contraction that has been done in previous work, giving more robust operators that express minimal change based on a notion of distance between Kripke-models. Second we give algorithms for syntactical action theory contraction and establish their correctness w.r.t. our semantics. Finally we state postulates for action theory contraction and assess the behavior of our operators w.r.t. them. Moreover, we also address the revision counterpart of action theory change, showing that it benefits from our semantics for contraction.",
        "published": "2008-11-12T12:05:55Z",
        "link": "http://arxiv.org/abs/0811.1878v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "A TLA+ Proof System",
        "authors": [
            "Kaustuv C. Chaudhuri",
            "Damien Doligez",
            "Leslie Lamport",
            "Stephan Merz"
        ],
        "summary": "We describe an extension to the TLA+ specification language with constructs for writing proofs and a proof environment, called the Proof Manager (PM), to checks those proofs. The language and the PM support the incremental development and checking of hierarchically structured proofs. The PM translates a proof into a set of independent proof obligations and calls upon a collection of back-end provers to verify them. Different provers can be used to verify different obligations. The currently supported back-ends are the tableau prover Zenon and Isabelle/TLA+, an axiomatisation of TLA+ in Isabelle/Pure. The proof obligations for a complete TLA+ proof can also be used to certify the theorem in Isabelle/TLA+.",
        "published": "2008-11-12T15:00:22Z",
        "link": "http://arxiv.org/abs/0811.1914v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Coalgebraic Automata Theory: Basic Results",
        "authors": [
            "C. Kupke",
            "Y. Venema"
        ],
        "summary": "We generalize some of the central results in automata theory to the abstraction level of coalgebras and thus lay out the foundations of a universal theory of automata operating on infinite objects.   Let F be any set functor that preserves weak pullbacks. We show that the class of recognizable languages of F-coalgebras is closed under taking unions, intersections, and projections. We also prove that if a nondeterministic F-automaton accepts some coalgebra it accepts a finite one of the size of the automaton. Our main technical result concerns an explicit construction which transforms a given alternating F-automaton into an equivalent nondeterministic one, whose size is exponentially bound by the size of the original automaton.",
        "published": "2008-11-12T18:53:13Z",
        "link": "http://arxiv.org/abs/0811.1976v2",
        "categories": [
            "cs.LO",
            "F.1.1; F.4.3; F.3.2"
        ]
    },
    {
        "title": "Compactly accessible categories and quantum key distribution",
        "authors": [
            "Chris Heunen"
        ],
        "summary": "Compact categories have lately seen renewed interest via applications to quantum physics. Being essentially finite-dimensional, they cannot accomodate (co)limit-based constructions. For example, they cannot capture protocols such as quantum key distribution, that rely on the law of large numbers. To overcome this limitation, we introduce the notion of a compactly accessible category, relying on the extra structure of a factorisation system. This notion allows for infinite dimension while retaining key properties of compact categories: the main technical result is that the choice-of-duals functor on the compact part extends canonically to the whole compactly accessible category. As an example, we model a quantum key distribution protocol and prove its correctness categorically.",
        "published": "2008-11-13T13:58:48Z",
        "link": "http://arxiv.org/abs/0811.2113v3",
        "categories": [
            "cs.LO",
            "cs.PL",
            "quant-ph",
            "F.3.2"
        ]
    },
    {
        "title": "The Church Problem for Countable Ordinals",
        "authors": [
            "Alexander Rabinovich"
        ],
        "summary": "A fundamental theorem of Buchi and Landweber shows that the Church synthesis problem is computable. Buchi and Landweber reduced the Church Problem to problems about &#969;-games and used the determinacy of such games as one of the main tools to show its computability. We consider a natural generalization of the Church problem to countable ordinals and investigate games of arbitrary countable length. We prove that determinacy and decidability parts of the Bu}chi and Landweber theorem hold for all countable ordinals and that its full extension holds for all ordinals < \\omega\\^\\omega.",
        "published": "2008-11-13T18:47:27Z",
        "link": "http://arxiv.org/abs/0811.2198v4",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Phase transition for Local Search on planted SAT",
        "authors": [
            "Andrei A. Bulatov",
            "Evgeny S. Skvortsov"
        ],
        "summary": "The Local Search algorithm (or Hill Climbing, or Iterative Improvement) is one of the simplest heuristics to solve the Satisfiability and Max-Satisfiability problems. It is a part of many satisfiability and max-satisfiability solvers, where it is used to find a good starting point for a more sophisticated heuristics, and to improve a candidate solution. In this paper we give an analysis of Local Search on random planted 3-CNF formulas. We show that if there is k<7/6 such that the clause-to-variable ratio is less than k ln(n) (n is the number of variables in a CNF) then Local Search whp does not find a satisfying assignment, and if there is k>7/6 such that the clause-to-variable ratio is greater than k ln(n)$ then the local search whp finds a satisfying assignment. As a byproduct we also show that for any constant r there is g such that Local Search applied to a random (not necessarily planted) 3-CNF with clause-to-variable ratio r produces an assignment that satisfies at least gn clauses less than the maximal number of satisfiable clauses.",
        "published": "2008-11-16T01:41:15Z",
        "link": "http://arxiv.org/abs/0811.2546v1",
        "categories": [
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "A Rational Deconstruction of Landin's SECD Machine with the J Operator",
        "authors": [
            "Olivier Danvy",
            "Kevin Millikin"
        ],
        "summary": "Landin's SECD machine was the first abstract machine for applicative expressions, i.e., functional programs. Landin's J operator was the first control operator for functional languages, and was specified by an extension of the SECD machine. We present a family of evaluation functions corresponding to this extension of the SECD machine, using a series of elementary transformations (transformation into continu-ation-passing style (CPS) and defunctionalization, chiefly) and their left inverses (transformation into direct style and refunctionalization). To this end, we modernize the SECD machine into a bisimilar one that operates in lockstep with the original one but that (1) does not use a data stack and (2) uses the caller-save rather than the callee-save convention for environments. We also identify that the dump component of the SECD machine is managed in a callee-save way. The caller-save counterpart of the modernized SECD machine precisely corresponds to Thielecke's double-barrelled continuations and to Felleisen's encoding of J in terms of call/cc. We then variously characterize the J operator in terms of CPS and in terms of delimited-control operators in the CPS hierarchy. As a byproduct, we also present several reduction semantics for applicative expressions with the J operator, based on Curien's original calculus of explicit substitutions. These reduction semantics mechanically correspond to the modernized versions of the SECD machine and to the best of our knowledge, they provide the first syntactic theories of applicative expressions with the J operator.",
        "published": "2008-11-19T22:31:34Z",
        "link": "http://arxiv.org/abs/0811.3231v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.1, D.3.3, F.1.1"
        ]
    },
    {
        "title": "A Cloning Pushout Approach to Term-Graph Transformation",
        "authors": [
            "Dominique Duval",
            "Rachid Echahed",
            "Frédéric Prost"
        ],
        "summary": "We address the problem of cyclic termgraph rewriting. We propose a new framework where rewrite rules are tuples of the form $(L,R,\\tau,\\sigma)$ such that $L$ and $R$ are termgraphs representing the left-hand and the right-hand sides of the rule, $\\tau$ is a mapping from the nodes of $L$ to those of $R$ and $\\sigma$ is a partial function from nodes of $R$ to nodes of $L$. $\\tau$ describes how incident edges of the nodes in $L$ are connected in $R$. $\\tau$ is not required to be a graph morphism as in classical algebraic approaches of graph transformation. The role of $\\sigma$ is to indicate the parts of $L$ to be cloned (copied). Furthermore, we introduce a new notion of \\emph{cloning pushout} and define rewrite steps as cloning pushouts in a given category. Among the features of the proposed rewrite systems, we quote the ability to perform local and global redirection of pointers, addition and deletion of nodes as well as cloning and collapsing substructures.",
        "published": "2008-11-20T19:39:51Z",
        "link": "http://arxiv.org/abs/0811.3400v1",
        "categories": [
            "cs.LO",
            "F.4.2"
        ]
    },
    {
        "title": "Craig Interpolation for Quantifier-Free Presburger Arithmetic",
        "authors": [
            "Angelo Brillout",
            "Daniel Kroening",
            "Thomas Wahl"
        ],
        "summary": "Craig interpolation has become a versatile algorithmic tool for improving software verification. Interpolants can, for instance, accelerate the convergence of fixpoint computations for infinite-state systems. They also help improve the refinement of iteratively computed lazy abstractions. Efficient interpolation procedures have been presented only for a few theories. In this paper, we introduce a complete interpolation method for the full range of quantifier-free Presburger arithmetic formulas. We propose a novel convex variable projection for integer inequalities and a technique to combine them with equalities. The derivation of the interpolant has complexity low-degree polynomial in the size of the refutation proof and is typically fast in practice.",
        "published": "2008-11-21T11:44:22Z",
        "link": "http://arxiv.org/abs/0811.3521v1",
        "categories": [
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Highly Undecidable Problems about Recognizability by Tiling Systems",
        "authors": [
            "Olivier Finkel"
        ],
        "summary": "Altenbernd, Thomas and W\\\"ohrle have considered acceptance of languages of infinite two-dimensional words (infinite pictures) by finite tiling systems, with usual acceptance conditions, such as the B\\\"uchi and Muller ones [1]. It was proved in [9] that it is undecidable whether a B\\\"uchi-recognizable language of infinite pictures is E-recognizable (respectively, A-recognizable). We show here that these two decision problems are actually $\\Pi_2^1$-complete, hence located at the second level of the analytical hierarchy, and \"highly undecidable\". We give the exact degree of numerous other undecidable problems for B\\\"uchi-recognizable languages of infinite pictures. In particular, the non-emptiness and the infiniteness problems are $\\Sigma^1_1$-complete, and the universality problem, the inclusion problem, the equivalence problem, the determinizability problem, the complementability problem, are all $\\Pi^1_2$-complete. It is also $\\Pi^1_2$-complete to determine whether a given B\\\"uchi recognizable language of infinite pictures can be accepted row by row using an automaton model over ordinal words of length $\\omega^2$.",
        "published": "2008-11-22T17:41:28Z",
        "link": "http://arxiv.org/abs/0811.3704v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "Hybrid: A Definitional Two-Level Approach to Reasoning with Higher-Order   Abstract Syntax",
        "authors": [
            "Amy Felty",
            "Alberto Momigliano"
        ],
        "summary": "Combining higher-order abstract syntax and (co)induction in a logical framework is well known to be problematic. Previous work described the implementation of a tool called Hybrid, within Isabelle HOL, which aims to address many of these difficulties. It allows object logics to be represented using higher-order abstract syntax, and reasoned about using tactical theorem proving and principles of (co)induction. In this paper we describe how to use it in a multi-level reasoning fashion, similar in spirit to other meta-logics such as Twelf. By explicitly referencing provability in a middle layer called a specification logic, we solve the problem of reasoning by (co)induction in the presence of non-stratifiable hypothetical judgments, which allow very elegant and succinct specifications of object logic inference rules.",
        "published": "2008-11-26T17:04:30Z",
        "link": "http://arxiv.org/abs/0811.4367v2",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Homomorphism Preservation on Quasi-Wide Classes",
        "authors": [
            "Anuj Dawar"
        ],
        "summary": "A class of structures is said to have the homomorphism-preservation property just in case every first-order formula that is preserved by homomorphisms on this class is equivalent to an existential-positive formula. It is known by a result of Rossman that the class of finite structures has this property and by previous work of Atserias et al. that various of its subclasses do. We extend the latter results by introducing the notion of a quasi-wide class and showing that any quasi-wide class that is closed under taking substructures and disjoint unions has the homomorphism-preservation property. We show, in particular, that classes of structures of bounded expansion and that locally exclude minors are quasi-wide. We also construct an example of a class of finite structures which is closed under substructures and disjoint unions but does not admit the homomorphism-preservation property.",
        "published": "2008-11-27T09:50:22Z",
        "link": "http://arxiv.org/abs/0811.4497v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Compact Labelings For Efficient First-Order Model-Checking",
        "authors": [
            "Bruno Courcelle",
            "Cyril Gavoille",
            "Mamadou Moustapha Kanté"
        ],
        "summary": "We consider graph properties that can be checked from labels, i.e., bit sequences, of logarithmic length attached to vertices. We prove that there exists such a labeling for checking a first-order formula with free set variables in the graphs of every class that is \\emph{nicely locally cwd-decomposable}. This notion generalizes that of a \\emph{nicely locally tree-decomposable} class. The graphs of such classes can be covered by graphs of bounded \\emph{clique-width} with limited overlaps. We also consider such labelings for \\emph{bounded} first-order formulas on graph classes of \\emph{bounded expansion}. Some of these results are extended to counting queries.",
        "published": "2008-11-28T13:29:15Z",
        "link": "http://arxiv.org/abs/0811.4713v2",
        "categories": [
            "cs.DS",
            "cs.LO",
            "68R05, 68R10, 05C78, 05C85",
            "F.0; G.2.2"
        ]
    },
    {
        "title": "Automated Induction for Complex Data Structures",
        "authors": [
            "Adel Bouhoula",
            "Florent Jacquemard"
        ],
        "summary": "We propose a procedure for automated implicit inductive theorem proving for equational specifications made of rewrite rules with conditions and constraints. The constraints are interpreted over constructor terms (representing data values), and may express syntactic equality, disequality, ordering and also membership in a fixed tree language. Constrained equational axioms between constructor terms are supported and can be used in order to specify complex data structures like sets, sorted lists, trees, powerlists...   Our procedure is based on tree grammars with constraints, a formalism which can describe exactly the initial model of the given specification (when it is sufficiently complete and terminating). They are used in the inductive proofs first as an induction scheme for the generation of subgoals at induction steps, second for checking validity and redundancy criteria by reduction to an emptiness problem, and third for defining and solving membership constraints.   We show that the procedure is sound and refutationally complete. It generalizes former test set induction techniques and yields natural proofs for several non-trivial examples presented in the paper, these examples are difficult to specify and carry on automatically with related induction procedures.",
        "published": "2008-11-28T13:58:46Z",
        "link": "http://arxiv.org/abs/0811.4720v1",
        "categories": [
            "cs.LO",
            "cs.SC",
            "F.4.1; F.4.2; F.4.3"
        ]
    },
    {
        "title": "Probabilistic reasoning with answer sets",
        "authors": [
            "Chitta Baral",
            "Michael Gelfond",
            "Nelson Rushton"
        ],
        "summary": "This paper develops a declarative language, P-log, that combines logical and probabilistic arguments in its reasoning. Answer Set Prolog is used as the logical foundation, while causal Bayes nets serve as a probabilistic foundation. We give several non-trivial examples and illustrate the use of P-log for knowledge representation and updating of knowledge. We argue that our approach to updates is more appealing than existing approaches. We give sufficiency conditions for the coherency of P-log programs and show that Bayes nets can be easily mapped to coherent P-log programs.",
        "published": "2008-12-03T06:36:16Z",
        "link": "http://arxiv.org/abs/0812.0659v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Unary finite automata vs. arithmetic progressions",
        "authors": [
            "Anthony Widjaja To"
        ],
        "summary": "We point out a subtle error in the proof of Chrobak's theorem that every unary NFA can be represented as a union of arithmetic progressions that is at most quadratically large. We propose a correction for this and show how Martinez's polynomial time algorithm, which realizes Chrobak's theorem, can be made correct accordingly. We also show that Martinez's algorithm cannot be improved to have logarithmic space, unless L = NL.",
        "published": "2008-12-06T14:18:05Z",
        "link": "http://arxiv.org/abs/0812.1291v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Graph Polynomials: From Recursive Definitions To Subset Expansion   Formulas",
        "authors": [
            "Benny Godlin",
            "Emilia Katz",
            "Johann A. Makowsky"
        ],
        "summary": "Many graph polynomials, such as the Tutte polynomial, the interlace polynomial and the matching polynomial, have both a recursive definition and a defining subset expansion formula. In this paper we present a general, logic-based framework which gives a precise meaning to recursive definitions of graph polynomials. We then prove that in this framework every recursive definition of a graph polynomial can be converted into a subset expansion formula.",
        "published": "2008-12-07T16:50:53Z",
        "link": "http://arxiv.org/abs/0812.1364v1",
        "categories": [
            "cs.LO",
            "cs.DM"
        ]
    },
    {
        "title": "The Wadge Hierarchy of Deterministic Tree Languages",
        "authors": [
            "Filip Murlak"
        ],
        "summary": "We provide a complete description of the Wadge hierarchy for deterministically recognisable sets of infinite trees. In particular we give an elementary procedure to decide if one deterministic tree language is continuously reducible to another. This extends Wagner's results on the hierarchy of omega-regular languages of words to the case of trees.",
        "published": "2008-12-09T16:14:05Z",
        "link": "http://arxiv.org/abs/0812.1729v2",
        "categories": [
            "cs.LO",
            "F.4.3; F.4.1; F.1.1; F.1.3"
        ]
    },
    {
        "title": "Dynamic Complexity of Formal Languages",
        "authors": [
            "Wouter Gelade",
            "Marcel Marquardt",
            "Thomas Schwentick"
        ],
        "summary": "The paper investigates the power of the dynamic complexity classes DynFO, DynQF and DynPROP over string languages. The latter two classes contain problems that can be maintained using quantifier-free first-order updates, with and without auxiliary functions, respectively. It is shown that the languages maintainable in DynPROP exactly are the regular languages, even when allowing arbitrary precomputation. This enables lower bounds for DynPROP and separates DynPROP from DynQF and DynFO. Further, it is shown that any context-free language can be maintained in DynFO and a number of specific context-free languages, for example all Dyck-languages, are maintainable in DynQF. Furthermore, the dynamic complexity of regular tree languages is investigated and some results concerning arbitrary structures are obtained: there exist first-order definable properties which are not maintainable in DynPROP. On the other hand any existential first-order property can be maintained in DynQF when allowing precomputation.",
        "published": "2008-12-10T14:13:57Z",
        "link": "http://arxiv.org/abs/0812.1915v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "The convex hull of a regular set of integer vectors is polyhedral and   effectively computable",
        "authors": [
            "Alain Finkel",
            "Jérôme Leroux"
        ],
        "summary": "Number Decision Diagrams (NDD) provide a natural finite symbolic representation for regular set of integer vectors encoded as strings of digit vectors (least or most significant digit first). The convex hull of the set of vectors represented by a NDD is proved to be an effectively computable convex polyhedron.",
        "published": "2008-12-10T16:26:36Z",
        "link": "http://arxiv.org/abs/0812.1951v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "Decomposition of Decidable First-Order Logics over Integers and Reals",
        "authors": [
            "Florent Bouchy",
            "Alain Finkel",
            "Jérôme Leroux"
        ],
        "summary": "We tackle the issue of representing infinite sets of real- valued vectors. This paper introduces an operator for combining integer and real sets. Using this operator, we decompose three well-known logics extending Presburger with reals. Our decomposition splits a logic into two parts : one integer, and one decimal (i.e. on the interval [0,1]). We also give a basis for an implementation of our representation.",
        "published": "2008-12-10T17:08:45Z",
        "link": "http://arxiv.org/abs/0812.1967v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Completeness for Flat Modal Fixpoint Logics",
        "authors": [
            "Luigi Santocanale",
            "Yde Venema"
        ],
        "summary": "This paper exhibits a general and uniform method to prove completeness for certain modal fixpoint logics. Given a set \\Gamma of modal formulas of the form \\gamma(x, p1, . . ., pn), where x occurs only positively in \\gamma, the language L\\sharp (\\Gamma) is obtained by adding to the language of polymodal logic a connective \\sharp\\_\\gamma for each \\gamma \\epsilon. The term \\sharp\\_\\gamma (\\varphi1, . . ., \\varphin) is meant to be interpreted as the least fixed point of the functional interpretation of the term \\gamma(x, \\varphi 1, . . ., \\varphi n). We consider the following problem: given \\Gamma, construct an axiom system which is sound and complete with respect to the concrete interpretation of the language L\\sharp (\\Gamma) on Kripke frames. We prove two results that solve this problem. First, let K\\sharp (\\Gamma) be the logic obtained from the basic polymodal K by adding a Kozen-Park style fixpoint axiom and a least fixpoint rule, for each fixpoint connective \\sharp\\_\\gamma. Provided that each indexing formula \\gamma satisfies the syntactic criterion of being untied in x, we prove this axiom system to be complete. Second, addressing the general case, we prove the soundness and completeness of an extension K+ (\\Gamma) of K\\_\\sharp (\\Gamma). This extension is obtained via an effective procedure that, given an indexing formula \\gamma as input, returns a finite set of axioms and derivation rules for \\sharp\\_\\gamma, of size bounded by the length of \\gamma. Thus the axiom system K+ (\\Gamma) is finite whenever \\Gamma is finite.",
        "published": "2008-12-12T15:04:57Z",
        "link": "http://arxiv.org/abs/0812.2390v1",
        "categories": [
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "On the Expressive Power of 2-Stack Visibly Pushdown Automata",
        "authors": [
            "Benedikt Bollig"
        ],
        "summary": "Visibly pushdown automata are input-driven pushdown automata that recognize some non-regular context-free languages while preserving the nice closure and decidability properties of finite automata. Visibly pushdown automata with multiple stacks have been considered recently by La Torre, Madhusudan, and Parlato, who exploit the concept of visibility further to obtain a rich automata class that can even express properties beyond the class of context-free languages. At the same time, their automata are closed under boolean operations, have a decidable emptiness and inclusion problem, and enjoy a logical characterization in terms of a monadic second-order logic over words with an additional nesting structure. These results require a restricted version of visibly pushdown automata with multiple stacks whose behavior can be split up into a fixed number of phases. In this paper, we consider 2-stack visibly pushdown automata (i.e., visibly pushdown automata with two stacks) in their unrestricted form. We show that they are expressively equivalent to the existential fragment of monadic second-order logic. Furthermore, it turns out that monadic second-order quantifier alternation forms an infinite hierarchy wrt words with multiple nestings. Combining these results, we conclude that 2-stack visibly pushdown automata are not closed under complementation. Finally, we discuss the expressive power of B\\\"{u}chi 2-stack visibly pushdown automata running on infinite (nested) words. Extending the logic by an infinity quantifier, we can likewise establish equivalence to existential monadic second-order logic.",
        "published": "2008-12-12T16:43:48Z",
        "link": "http://arxiv.org/abs/0812.2423v2",
        "categories": [
            "cs.LO",
            "F.4.3"
        ]
    },
    {
        "title": "Standard Logics Are Valuation-Nonmonotonic",
        "authors": [
            "Mladen Pavicic",
            "Norman D. Megill"
        ],
        "summary": "It has recently been discovered that both quantum and classical propositional logics can be modelled by classes of non-orthomodular and thus non-distributive lattices that properly contain standard orthomodular and Boolean classes, respectively. In this paper we prove that these logics are complete even for those classes of the former lattices from which the standard orthomodular lattices and Boolean algebras are excluded. We also show that neither quantum nor classical computers can be founded on the latter models. It follows that logics are \"valuation-nonmonotonic\" in the sense that their possible models (corresponding to their possible hardware implementations) and the valuations for them drastically change when we add new conditions to their defining conditions. These valuations can even be completely separated by putting them into disjoint lattice classes by a technique presented in the paper.",
        "published": "2008-12-15T00:49:12Z",
        "link": "http://arxiv.org/abs/0812.2702v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "quant-ph"
        ]
    },
    {
        "title": "Branching Bisimilarity with Explicit Divergence",
        "authors": [
            "Rob van Glabbeek",
            "Bas Luttik",
            "Nikola Trcka"
        ],
        "summary": "We consider the relational characterisation of branching bisimilarity with explicit divergence. We prove that it is an equivalence and that it coincides with the original definition of branching bisimilarity with explicit divergence in terms of coloured traces. We also establish a correspondence with several variants of an action-based modal logic with until- and divergence modalities.",
        "published": "2008-12-16T14:36:43Z",
        "link": "http://arxiv.org/abs/0812.3068v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "XML Static Analyzer User Manual",
        "authors": [
            "Pierre Geneves",
            "Nabil Layaida"
        ],
        "summary": "This document describes how to use the XML static analyzer in practice. It provides informal documentation for using the XML reasoning solver implementation. The solver allows automated verification of properties that are expressed as logical formulas over trees. A logical formula may for instance express structural constraints or navigation properties (like e.g. path existence and node selection) in finite trees. Logical formulas can be expressed using the syntax of XPath expressions, DTD, XML Schemas, and Relax NG definitions.",
        "published": "2008-12-18T15:22:46Z",
        "link": "http://arxiv.org/abs/0812.3550v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "cs.LO",
            "cs.SE",
            "D.3.0; D.3.1; D.3.4; E.1; F.3.1; F.3.2; F.4.1; F.4.3; H.2.3; I.2.4;\n  I.7.2"
        ]
    },
    {
        "title": "Bootstrapping Inductive and Coinductive Types in HasCASL",
        "authors": [
            "Lutz Schröder"
        ],
        "summary": "We discuss the treatment of initial datatypes and final process types in the wide-spectrum language HasCASL. In particular, we present specifications that illustrate how datatypes and process types arise as bootstrapped concepts using HasCASL's type class mechanism, and we describe constructions of types of finite and infinite trees that establish the conservativity of datatype and process type declarations adhering to certain reasonable formats. The latter amounts to modifying known constructions from HOL to avoid unique choice; in categorical terminology, this means that we establish that quasitoposes with an internal natural numbers object support initial algebras and final coalgebras for a range of polynomial functors, thereby partially generalising corresponding results from topos theory. Moreover, we present similar constructions in categories of internal complete partial orders in quasitoposes.",
        "published": "2008-12-19T16:22:15Z",
        "link": "http://arxiv.org/abs/0812.3836v2",
        "categories": [
            "cs.LO",
            "cs.SE",
            "D.2.1; E.1; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "I, Quantum Robot: Quantum Mind control on a Quantum Computer",
        "authors": [
            "Paola Zizzi"
        ],
        "summary": "The logic which describes quantum robots is not orthodox quantum logic, but a deductive calculus which reproduces the quantum tasks (computational processes, and actions) taking into account quantum superposition and quantum entanglement. A way toward the realization of intelligent quantum robots is to adopt a quantum metalanguage to control quantum robots. A physical implementation of a quantum metalanguage might be the use of coherent states in brain signals.",
        "published": "2008-12-25T16:31:05Z",
        "link": "http://arxiv.org/abs/0812.4614v2",
        "categories": [
            "quant-ph",
            "cs.AI",
            "cs.LO",
            "cs.RO"
        ]
    },
    {
        "title": "Induction and Co-induction in Sequent Calculus",
        "authors": [
            "Alwen Tiu",
            "Alberto Momigliano"
        ],
        "summary": "Proof search has been used to specify a wide range of computation systems. In order to build a framework for reasoning about such specifications, we make use of a sequent calculus involving induction and co-induction. These proof principles are based on a proof theoretic (rather than set-theoretic) notion of definition. Definitions are akin to (stratified) logic programs, where the left and right rules for defined atoms allow one to view theories as \"closed\" or defining fixed points. The use of definitions makes it possible to reason intensionally about syntax, in particular enforcing free equality via unification. We add in a consistent way rules for pre and post fixed points, thus allowing the user to reason inductively and co-inductively about properties of computational system making full use of higher-order abstract syntax. Consistency is guaranteed via cut-elimination, where we give the first, to our knowledge, cut-elimination procedure in the presence of general inductive and co-inductive definitions.",
        "published": "2008-12-27T09:29:16Z",
        "link": "http://arxiv.org/abs/0812.4727v3",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Nominalistic Logic (Extended Abstract)",
        "authors": [
            "Jørgen Villadsen"
        ],
        "summary": "Nominalistic Logic (NL) is a new presentation of Paul Gilmore's Intensional Type Theory (ITT) as a sequent calculus together with a succinct nominalization axiom (N) that permits names of predicates as individuals in certain cases. The logic has a flexible comprehension axiom, but no extensionality axiom and no infinity axiom, although axiom N is the key to the derivation of Peano's postulates for the natural numbers.",
        "published": "2008-12-28T12:45:05Z",
        "link": "http://arxiv.org/abs/0812.4814v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "The Complexity of Generalized Satisfiability for Linear Temporal Logic",
        "authors": [
            "Michael Bauland",
            "Thomas Schneider",
            "Henning Schnoor",
            "Ilka Schnoor",
            "Heribert Vollmer"
        ],
        "summary": "In a seminal paper from 1985, Sistla and Clarke showed that satisfiability for Linear Temporal Logic (LTL) is either NP-complete or PSPACE-complete, depending on the set of temporal operators used. If, in contrast, the set of propositional operators is restricted, the complexity may decrease. This paper undertakes a systematic study of satisfiability for LTL formulae over restricted sets of propositional and temporal operators. Since every propositional operator corresponds to a Boolean function, there exist infinitely many propositional operators. In order to systematically cover all possible sets of them, we use Post's lattice. With its help, we determine the computational complexity of LTL satisfiability for all combinations of temporal operators and all but two classes of propositional functions. Each of these infinitely many problems is shown to be either PSPACE-complete, NP-complete, or in P.",
        "published": "2008-12-28T21:10:06Z",
        "link": "http://arxiv.org/abs/0812.4848v3",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Formalizing common sense for scalable inconsistency-robust information   integration using Direct Logic(TM) reasoning and the Actor Model",
        "authors": [
            "Carl Hewitt"
        ],
        "summary": "Because contemporary large software systems are pervasively inconsistent, it is not safe to reason about them using classical logic. The goal of Direct Logic is to be a minimal fix to classical mathematical logic that meets the requirements of large-scale Internet applications (including sense making for natural language) by addressing the following issues: inconsistency robustness, contrapositive inference bug, and direct argumentation.   Direct Logic makes the following contributions over previous work:   * Direct Inference (no contrapositive bug for inference)   * Direct Argumentation (inference directly expressed)   * Inconsistency-robust deduction without artifices such as indices (labels) on propositions or restrictions on reiteration   * Intuitive inferences hold including the following:   * Boolean Equivalences   * Reasoning by splitting for disjunctive cases   * Soundness   * Inconsistency-robust Proof by Contradiction   Since the global state model of computation (first formalized by Turing) is inadequate to the needs of modern large-scale Internet applications the Actor Model was developed to meet this need. Using, the Actor Model, this paper proves that Logic Programming is not computationally universal in that there are computations that cannot be implemented using logical inference. Consequently the Logic Programming paradigm is strictly less general than the Procedural Embedding of Knowledge paradigm.",
        "published": "2008-12-28T21:37:23Z",
        "link": "http://arxiv.org/abs/0812.4852v103",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Symmetric and Asymmetric Asynchronous Interaction",
        "authors": [
            "Rob van Glabbeek",
            "Ursula Goltz",
            "Jens-Wolfhard Schicke"
        ],
        "summary": "We investigate classes of systems based on different interaction patterns with the aim of achieving distributability. As our system model we use Petri nets. In Petri nets, an inherent concept of simultaneity is built in, since when a transition has more than one preplace, it can be crucial that tokens are removed instantaneously. When modelling a system which is intended to be implemented in a distributed way by a Petri net, this built-in concept of synchronous interaction may be problematic. To investigate this we consider asynchronous implementations of nets, in which removing tokens from places can no longer be considered as instantaneous. We model this by inserting silent (unobservable) transitions between transitions and some of their preplaces. We investigate three such implementations, differing in the selection of preplaces of a transition from which the removal of a token is considered time consuming, and the possibility of collecting the tokens in a given order.   We investigate the effect of these different transformations of instantaneous interaction into asynchronous interaction patterns by comparing the behaviours of nets before and after insertion of the silent transitions. We exhibit for which classes of Petri nets we obtain equivalent behaviour with respect to failures equivalence.   It turns out that the resulting hierarchy of Petri net classes can be described by semi-structural properties. For two of the classes we obtain precise characterisations; for the remaining class we obtain lower and upper bounds.   We briefly comment on possible applications of our results to Message Sequence Charts.",
        "published": "2008-12-31T03:43:25Z",
        "link": "http://arxiv.org/abs/0901.0043v1",
        "categories": [
            "cs.LO",
            "cs.DC",
            "F.1.2; B.4.3"
        ]
    },
    {
        "title": "On Synchronous and Asynchronous Interaction in Distributed Systems",
        "authors": [
            "Rob van Glabbeek",
            "Ursula Goltz",
            "Jens-Wolfhard Schicke"
        ],
        "summary": "When considering distributed systems, it is a central issue how to deal with interactions between components. In this paper, we investigate the paradigms of synchronous and asynchronous interaction in the context of distributed systems. We investigate to what extent or under which conditions synchronous interaction is a valid concept for specification and implementation of such systems. We choose Petri nets as our system model and consider different notions of distribution by associating locations to elements of nets. First, we investigate the concept of simultaneity which is inherent in the semantics of Petri nets when transitions have multiple input places. We assume that tokens may only be taken instantaneously by transitions on the same location. We exhibit a hierarchy of `asynchronous' Petri net classes by different assumptions on possible distributions. Alternatively, we assume that the synchronisations specified in a Petri net are crucial system properties. Hence transitions and their preplaces may no longer placed on separate locations. We then answer the question which systems may be implemented in a distributed way without restricting concurrency, assuming that locations are inherently sequential. It turns out that in both settings we find semi-structural properties of Petri nets describing exactly the problematic situations for interactions in distributed systems.",
        "published": "2008-12-31T04:13:35Z",
        "link": "http://arxiv.org/abs/0901.0048v1",
        "categories": [
            "cs.LO",
            "cs.DC",
            "F.1.2; B.4.3"
        ]
    },
    {
        "title": "Pareto and Boltzmann-Gibbs behaviors in a deterministic multi-agent   system",
        "authors": [
            "J. Gonzalez-Estevez",
            "M. G. Cosenza",
            "R. Lopez-Ruiz",
            "J. R. Sanchez"
        ],
        "summary": "A deterministic system of interacting agents is considered as a model for economic dynamics. The dynamics of the system is described by a coupled map lattice with near neighbor interactions. The evolution of each agent results from the competition between two factors: the agent's own tendency to grow and the environmental influence that moderates this growth. Depending on the values of the parameters that control these factors, the system can display Pareto or Boltzmann-Gibbs statistical behaviors in its asymptotic dynamical regime. The regions where these behaviors appear are calculated on the space of parameters of the system. Other statistical properties, such as the mean wealth, the standard deviation, and the Gini coefficient characterizing the degree of equity in the wealth distribution are also calculated on the space of parameters of the system.",
        "published": "2008-01-07T13:15:16Z",
        "link": "http://arxiv.org/abs/0801.0969v1",
        "categories": [
            "q-fin.GN",
            "cond-mat.stat-mech",
            "cs.MA",
            "nlin.AO",
            "nlin.CD",
            "physics.comp-ph",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Computational Solutions for Today's Navy",
        "authors": [
            "Frank W. Bentrem",
            "John T. Sample",
            "Michael M. Harris"
        ],
        "summary": "New methods are being employed to meet the Navy's changing software-development environment.",
        "published": "2008-01-10T16:46:04Z",
        "link": "http://arxiv.org/abs/0801.1630v3",
        "categories": [
            "cs.MA",
            "cs.GL",
            "I.2.11; J.2"
        ]
    },
    {
        "title": "Computational approach to the emergence and evolution of language -   evolutionary naming game model",
        "authors": [
            "Adam Lipowski",
            "Dorota Lipowska"
        ],
        "summary": "Computational modelling with multi-agent systems is becoming an important technique of studying language evolution. We present a brief introduction into this rapidly developing field, as well as our own contributions that include an analysis of the evolutionary naming-game model. In this model communicating agents, that try to establish a common vocabulary, are equipped with an evolutionarily selected learning ability. Such a coupling of biological and linguistic ingredients results in an abrupt transition: upon a small change of the model control parameter a poorly communicating group of linguistically unskilled agents transforms into almost perfectly communicating group with large learning abilities. Genetic imprinting of the learning abilities proceeds via Baldwin effect: initially unskilled communicating agents learn a language and that creates a niche in which there is an evolutionary pressure for the increase of learning ability. Under the assumption that communication intensity increases continuously with finite speed, the transition is split into several transition-like changes. It shows that the speed of cultural changes, that sets an additional characteristic timescale, might be yet another factor affecting the evolution of language. In our opinion, this model shows that linguistic and biological processes have a strong influence on each other and this effect certainly has contributed to an explosive development of our species.",
        "published": "2008-01-10T19:45:25Z",
        "link": "http://arxiv.org/abs/0801.1658v3",
        "categories": [
            "physics.soc-ph",
            "cs.CL",
            "cs.MA"
        ]
    },
    {
        "title": "Human Heuristics for Autonomous Agents",
        "authors": [
            "Franco Bagnoli",
            "Andrea Guazzini",
            "Pietro Lio'"
        ],
        "summary": "We investigate the problem of autonomous agents processing pieces of information that may be corrupted (tainted). Agents have the option of contacting a central database for a reliable check of the status of the message, but this procedure is costly and therefore should be used with parsimony. Agents have to evaluate the risk of being infected, and decide if and when communicating partners are affordable. Trustability is implemented as a personal (one-to-one) record of past contacts among agents, and as a mean-field monitoring of the level of message corruption. Moreover, this information is slowly forgotten in time, so that at the end everybody is checked against the database. We explore the behavior of a homogeneous system in the case of a fixed pool of spreaders of corrupted messages, and in the case of spontaneous appearance of corrupted messages.",
        "published": "2008-01-19T19:36:13Z",
        "link": "http://arxiv.org/abs/0801.3048v1",
        "categories": [
            "cs.MA",
            "cs.HC",
            "cs.NI"
        ]
    },
    {
        "title": "Shallow Models for Non-Iterative Modal Logics",
        "authors": [
            "Lutz Schröder",
            "Dirk Patinson"
        ],
        "summary": "The methods used to establish PSPACE-bounds for modal logics can roughly be grouped into two classes: syntax driven methods establish that exhaustive proof search can be performed in polynomial space whereas semantic approaches directly construct shallow models. In this paper, we follow the latter approach and establish generic PSPACE-bounds for a large and heterogeneous class of modal logics in a coalgebraic framework. In particular, no complete axiomatisation of the logic under scrutiny is needed. This does not only complement our earlier, syntactic, approach conceptually, but also covers a wide variety of new examples which are difficult to harness by purely syntactic means. Apart from re-proving known complexity bounds for a large variety of structurally different logics, we apply our method to obtain previously unknown PSPACE-bounds for Elgesem's logic of agency and for graded modal logic over reflexive frames.",
        "published": "2008-02-01T13:11:09Z",
        "link": "http://arxiv.org/abs/0802.0116v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "cs.MA",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "Les Agents comme des interpréteurs Scheme : Spécification dynamique   par la communication",
        "authors": [
            "Clément Jonquet",
            "Stefano A. Cerri"
        ],
        "summary": "We proposed in previous papers an extension and an implementation of the STROBE model, which regards the Agents as Scheme interpreters. These Agents are able to interpret messages in a dedicated environment including an interpreter that learns from the current conversation therefore representing evolving meta-level Agent's knowledge. When the Agent's interpreter is a nondeterministic one, the dialogues may consist of subsequent refinements of specifications in the form of constraint sets. The paper presents a worked out example of dynamic service generation - such as necessary on Grids - by exploiting STROBE Agents equipped with a nondeterministic interpreter. It shows how enabling dynamic specification of a problem. Then it illustrates how these principles could be effective for other applications. Details of the implementation are not provided here, but are available.",
        "published": "2008-02-11T08:55:46Z",
        "link": "http://arxiv.org/abs/0802.1393v1",
        "categories": [
            "cs.MA",
            "cs.AI"
        ]
    },
    {
        "title": "On the Complexity of Nash Equilibria of Action-Graph Games",
        "authors": [
            "Constantinos Daskalakis",
            "Grant Schoenebeck",
            "Gregory Valiant",
            "Paul Valiant"
        ],
        "summary": "We consider the problem of computing Nash Equilibria of action-graph games (AGGs). AGGs, introduced by Bhat and Leyton-Brown, is a succinct representation of games that encapsulates both \"local\" dependencies as in graphical games, and partial indifference to other agents' identities as in anonymous games, which occur in many natural settings. This is achieved by specifying a graph on the set of actions, so that the payoff of an agent for selecting a strategy depends only on the number of agents playing each of the neighboring strategies in the action graph. We present a Polynomial Time Approximation Scheme for computing mixed Nash equilibria of AGGs with constant treewidth and a constant number of agent types (and an arbitrary number of strategies), together with hardness results for the cases when either the treewidth or the number of agent types is unconstrained. In particular, we show that even if the action graph is a tree, but the number of agent-types is unconstrained, it is NP-complete to decide the existence of a pure-strategy Nash equilibrium and PPAD-complete to compute a mixed Nash equilibrium (even an approximate one); similarly for symmetric AGGs (all agents belong to a single type), if we allow arbitrary treewidth. These hardness results suggest that, in some sense, our PTAS is as strong of a positive result as one can expect.",
        "published": "2008-02-12T09:45:36Z",
        "link": "http://arxiv.org/abs/0802.1604v1",
        "categories": [
            "cs.GT",
            "cs.MA"
        ]
    },
    {
        "title": "A Universal In-Place Reconfiguration Algorithm for Sliding Cube-Shaped   Robots in a Quadratic Number of Moves",
        "authors": [
            "Zachary Abel",
            "Hugo A. Akitaya",
            "Scott Duke Kominers",
            "Matias Korman",
            "Frederick Stock"
        ],
        "summary": "In the modular robot reconfiguration problem, we are given $n$ cube-shaped modules (or robots) as well as two configurations, i.e., placements of the $n$ modules so that their union is face-connected. The goal is to find a sequence of moves that reconfigures the modules from one configuration to the other using \"sliding moves,\" in which a module slides over the face or edge of a neighboring module, maintaining connectivity of the configuration at all times.   For many years it has been known that certain module configurations in this model require at least $\\Omega(n^2)$ moves to reconfigure between them. In this paper, we introduce the first universal reconfiguration algorithm -- i.e., we show that any $n$-module configuration can reconfigure itself into any specified $n$-module configuration using just sliding moves. Our algorithm achieves reconfiguration in $O(n^2)$ moves, making it asymptotically tight. We also present a variation that reconfigures in-place, it ensures that throughout the reconfiguration process, all modules, except for one, will be contained in the union of the bounding boxes of the start and end configuration.",
        "published": "2008-02-23T00:54:13Z",
        "link": "http://arxiv.org/abs/0802.3414v4",
        "categories": [
            "cs.CG",
            "cs.MA",
            "cs.RO"
        ]
    },
    {
        "title": "A Study On Distributed Model Predictive Consensus",
        "authors": [
            "Tamas Keviczky",
            "Karl Henrik Johansson"
        ],
        "summary": "We investigate convergence properties of a proposed distributed model predictive control (DMPC) scheme, where agents negotiate to compute an optimal consensus point using an incremental subgradient method based on primal decomposition as described in Johansson et al. [2006, 2007]. The objective of the distributed control strategy is to agree upon and achieve an optimal common output value for a group of agents in the presence of constraints on the agent dynamics using local predictive controllers. Stability analysis using a receding horizon implementation of the distributed optimal consensus scheme is performed. Conditions are given under which convergence can be obtained even if the negotiations do not reach full consensus.",
        "published": "2008-02-29T19:05:31Z",
        "link": "http://arxiv.org/abs/0802.4450v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "In-depth analysis of the Naming Game dynamics: the homogeneous mixing   case",
        "authors": [
            "Andrea Baronchelli",
            "Vittorio Loreto",
            "Luc Steels"
        ],
        "summary": "Language emergence and evolution has recently gained growing attention through multi-agent models and mathematical frameworks to study their behavior. Here we investigate further the Naming Game, a model able to account for the emergence of a shared vocabulary of form-meaning associations through social/cultural learning. Due to the simplicity of both the structure of the agents and their interaction rules, the dynamics of this model can be analyzed in great detail using numerical simulations and analytical arguments. This paper first reviews some existing results and then presents a new overall understanding.",
        "published": "2008-03-04T13:35:28Z",
        "link": "http://arxiv.org/abs/0803.0398v1",
        "categories": [
            "physics.soc-ph",
            "cond-mat.stat-mech",
            "cs.GT",
            "cs.MA"
        ]
    },
    {
        "title": "Using Intelligent Agents to understand organisational behaviour",
        "authors": [
            "Helen Celia",
            "Christopher Clegg",
            "Mark Robinson",
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Christine Sprigg"
        ],
        "summary": "This paper introduces two ongoing research projects which seek to apply computer modelling techniques in order to simulate human behaviour within organisations. Previous research in other disciplines has suggested that complex social behaviours are governed by relatively simple rules which, when identified, can be used to accurately model such processes using computer technology. The broad objective of our research is to develop a similar capability within organisational psychology.",
        "published": "2008-03-11T14:19:33Z",
        "link": "http://arxiv.org/abs/0803.1596v1",
        "categories": [
            "cs.NE",
            "cs.MA"
        ]
    },
    {
        "title": "Using Intelligent Agents to Understand Management Practices and Retail   Productivity",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Helen Celia",
            "Christopher Clegg"
        ],
        "summary": "Intelligent agents offer a new and exciting way of understanding the world of work. In this paper we apply agent-based modeling and simulation to investigate a set of problems in a retail context. Specifically, we are working to understand the relationship between human resource management practices and retail productivity. Despite the fact we are working within a relatively novel and complex domain, it is clear that intelligent agents could offer potential for fostering sustainable organizational capabilities in the future. The project is still at an early stage. So far we have conducted a case study in a UK department store to collect data and capture impressions about operations and actors within departments. Furthermore, based on our case study we have built and tested our first version of a retail branch simulator which we will present in this paper.",
        "published": "2008-03-11T14:55:58Z",
        "link": "http://arxiv.org/abs/0803.1604v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "cs.MA"
        ]
    },
    {
        "title": "An Agent-Based Simulation of In-Store Customer Experiences",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Helen Celia",
            "Christopher Clegg"
        ],
        "summary": "Agent-based modelling and simulation offers a new and exciting way of understanding the world of work. In this paper we describe the development of an agent-based simulation model, designed to help to understand the relationship between human resource management practices and retail productivity. We report on the current development of our simulation model which includes new features concerning the evolution of customers over time. To test some of these features we have conducted a series of experiments dealing with customer pool sizes, standard and noise reduction modes, and the spread of the word of mouth. Our multi-disciplinary research team draws upon expertise from work psychologists and computer scientists. Despite the fact we are working within a relatively novel and complex domain, it is clear that intelligent agents offer potential for fostering sustainable organisational capabilities in the future.",
        "published": "2008-03-11T16:11:34Z",
        "link": "http://arxiv.org/abs/0803.1621v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "cs.MA"
        ]
    },
    {
        "title": "Tableau-based decision procedures for logics of strategic ability in   multi-agent systems",
        "authors": [
            "Valentin Goranko",
            "Dmitry Shkatov"
        ],
        "summary": "We develop an incremental tableau-based decision procedures for the   Alternating-time temporal logic ATL and some of its variants.   While running within the theoretically established complexity upper bound, we claim that our tableau is practically more efficient in the average case than other decision procedures for ATL known so far. Besides, the ease of its adaptation to variants of ATL demonstrates the flexibility of the proposed procedure.",
        "published": "2008-03-15T16:22:53Z",
        "link": "http://arxiv.org/abs/0803.2306v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.MA",
            "F.4.1; I.2.3; I.2.4; I.2.11"
        ]
    },
    {
        "title": "Introduction to Multi-Agent Simulation",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin"
        ],
        "summary": "When designing systems that are complex, dynamic and stochastic in nature, simulation is generally recognised as one of the best design support technologies, and a valuable aid in the strategic and tactical decision making process. A simulation model consists of a set of rules that define how a system changes over time, given its current state. Unlike analytical models, a simulation model is not solved but is run and the changes of system states can be observed at any point in time. This provides an insight into system dynamics rather than just predicting the output of a system based on specific inputs. Simulation is not a decision making tool but a decision support tool, allowing better informed decisions to be made. Due to the complexity of the real world, a simulation model can only be an approximation of the target system. The essence of the art of simulation modelling is abstraction and simplification. Only those characteristics that are important for the study and analysis of the target system should be included in the simulation model.",
        "published": "2008-03-27T12:38:17Z",
        "link": "http://arxiv.org/abs/0803.3905v1",
        "categories": [
            "cs.NE",
            "cs.MA"
        ]
    },
    {
        "title": "Artificial Immune Systems Tutorial",
        "authors": [
            "Uwe Aickelin",
            "Dipankar Dasgupta"
        ],
        "summary": "The biological immune system is a robust, complex, adaptive system that defends the body from foreign pathogens. It is able to categorize all cells (or molecules) within the body as self-cells or non-self cells. It does this with the help of a distributed task force that has the intelligence to take action from a local and also a global perspective using its network of chemical messengers for communication. There are two major branches of the immune system. The innate immune system is an unchanging mechanism that detects and destroys certain invading organisms, whilst the adaptive immune system responds to previously unknown foreign cells and builds a response to them that can remain in the body over a long period of time. This remarkable information processing biological system has caught the attention of computer science in recent years. A novel computational intelligence technique, inspired by immunology, has emerged, called Artificial Immune Systems. Several concepts from the immune have been extracted and applied for solution to real world science and engineering problems. In this tutorial, we briefly describe the immune system metaphors that are relevant to existing Artificial Immune Systems methods. We will then show illustrative real-world problems suitable for Artificial Immune Systems and give a step-by-step algorithm walkthrough for one such problem. A comparison of the Artificial Immune Systems to other well-known algorithms, areas for future work, tips & tricks and a list of resources will round this tutorial off. It should be noted that as Artificial Immune Systems is still a young and evolving field, there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from time to time and from those examples given here.",
        "published": "2008-03-27T12:55:59Z",
        "link": "http://arxiv.org/abs/0803.3912v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Moore and more and symmetry",
        "authors": [
            "Tobias Kretz",
            "Michael Schreckenberg"
        ],
        "summary": "In any spatially discrete model of pedestrian motion which uses a regular lattice as basis, there is the question of how the symmetry between the different directions of motion can be restored as far as possible but with limited computational effort. This question is equivalent to the question ''How important is the orientation of the axis of discretization for the result of the simulation?'' An optimization in terms of symmetry can be combined with the implementation of higher and heterogeniously distributed walking speeds by representing different walking speeds via different amounts of cells an agent may move during one round. Therefore all different possible neighborhoods for speeds up to v = 10 (cells per round) will be examined for the amount of deviation from radial symmetry. Simple criteria will be stated which will allow find an optimal neighborhood for each speed. It will be shown that following these criteria even the best mixture of steps in Moore and von Neumann neighborhoods is unable to reproduce the optimal neighborhood for a speed as low as 4.",
        "published": "2008-04-02T09:47:04Z",
        "link": "http://arxiv.org/abs/0804.0318v1",
        "categories": [
            "cs.MA",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Distributed Consensus over Wireless Sensor Networks Affected by   Multipath Fading",
        "authors": [
            "Gesualdo Scutari",
            "Sergio Barbarossa"
        ],
        "summary": "The design of sensor networks capable of reaching a consensus on a globally optimal decision test, without the need for a fusion center, is a problem that has received considerable attention in the last years. Many consensus algorithms have been proposed, with convergence conditions depending on the graph describing the interaction among the nodes. In most works, the graph is undirected and there are no propagation delays. Only recently, the analysis has been extended to consensus algorithms incorporating propagation delays. In this work, we propose a consensus algorithm able to converge to a globally optimal decision statistic, using a wideband wireless network, governed by a fairly simple MAC mechanism, where each link is a multipath, frequency-selective, channel. The main contribution of the paper is to derive necessary and sufficient conditions on the network topology and sufficient conditions on the channel transfer functions guaranteeing the exponential convergence of the consensus algorithm to a globally optimal decision value, for any bounded delay condition.",
        "published": "2008-04-03T09:22:41Z",
        "link": "http://arxiv.org/abs/0804.0506v1",
        "categories": [
            "cs.DC",
            "cs.MA"
        ]
    },
    {
        "title": "A $O(\\log m)$, deterministic, polynomial-time computable approximation   of Lewis Carroll's scoring rule",
        "authors": [
            "Jason Covey",
            "Christopher Homan"
        ],
        "summary": "We provide deterministic, polynomial-time computable voting rules that approximate Dodgson's and (the ``minimization version'' of) Young's scoring rules to within a logarithmic factor. Our approximation of Dodgson's rule is tight up to a constant factor, as Dodgson's rule is $\\NP$-hard to approximate to within some logarithmic factor. The ``maximization version'' of Young's rule is known to be $\\NP$-hard to approximate by any constant factor. Both approximations are simple, and natural as rules in their own right: Given a candidate we wish to score, we can regard either its Dodgson or Young score as the edit distance between a given set of voter preferences and one in which the candidate to be scored is the Condorcet winner. (The difference between the two scoring rules is the type of edits allowed.) We regard the marginal cost of a sequence of edits to be the number of edits divided by the number of reductions (in the candidate's deficit against any of its opponents in the pairwise race against that opponent) that the edits yield. Over a series of rounds, our scoring rules greedily choose a sequence of edits that modify exactly one voter's preferences and whose marginal cost is no greater than any other such single-vote-modifying sequence.",
        "published": "2008-04-09T07:12:29Z",
        "link": "http://arxiv.org/abs/0804.1421v1",
        "categories": [
            "cs.GT",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "The F.A.S.T.-Model",
        "authors": [
            "Tobias Kretz",
            "Michael Schreckenberg"
        ],
        "summary": "A discrete model of pedestrian motion is presented that is implemented in the Floor field- and Agentbased Simulation Tool (F.A.S.T.) which has already been applicated to a variety of real life scenarios.",
        "published": "2008-04-11T13:33:42Z",
        "link": "http://arxiv.org/abs/0804.1893v1",
        "categories": [
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Parimutuel Betting on Permutations",
        "authors": [
            "Shipra Agrawal",
            "Zizhuo Wang",
            "Yinyu Ye"
        ],
        "summary": "We focus on a permutation betting market under parimutuel call auction model where traders bet on the final ranking of n candidates. We present a Proportional Betting mechanism for this market. Our mechanism allows the traders to bet on any subset of the n x n 'candidate-rank' pairs, and rewards them proportionally to the number of pairs that appear in the final outcome. We show that market organizer's decision problem for this mechanism can be formulated as a convex program of polynomial size. More importantly, the formulation yields a set of n x n unique marginal prices that are sufficient to price the bets in this mechanism, and are computable in polynomial-time. The marginal prices reflect the traders' beliefs about the marginal distributions over outcomes. We also propose techniques to compute the joint distribution over n! permutations from these marginal distributions. We show that using a maximum entropy criterion, we can obtain a concise parametric form (with only n x n parameters) for the joint distribution which is defined over an exponentially large state space. We then present an approximation algorithm for computing the parameters of this distribution. In fact, the algorithm addresses the generic problem of finding the maximum entropy distribution over permutations that has a given mean, and may be of independent interest.",
        "published": "2008-04-15T00:20:17Z",
        "link": "http://arxiv.org/abs/0804.2288v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DS",
            "cs.MA",
            "I.2.1; F.2"
        ]
    },
    {
        "title": "On the Expressiveness and Complexity of ATL",
        "authors": [
            "Francois Laroussinie",
            "Nicolas Markey",
            "Ghassan Oreiby"
        ],
        "summary": "ATL is a temporal logic geared towards the specification and verification of properties in multi-agents systems. It allows to reason on the existence of strategies for coalitions of agents in order to enforce a given property. In this paper, we first precisely characterize the complexity of ATL model-checking over Alternating Transition Systems and Concurrent Game Structures when the number of agents is not fixed. We prove that it is \\Delta^P_2 - and \\Delta^P_?_3-complete, depending on the underlying multi-agent model (ATS and CGS resp.). We also consider the same problems for some extensions of ATL. We then consider expressiveness issues. We show how ATS and CGS are related and provide translations between these models w.r.t. alternating bisimulation. We also prove that the standard definition of ATL (built on modalities \"Next\", \"Always\" and \"Until\") cannot express the duals of its modalities: it is necessary to explicitely add the modality \"Release\".",
        "published": "2008-04-15T17:18:46Z",
        "link": "http://arxiv.org/abs/0804.2435v3",
        "categories": [
            "cs.LO",
            "cs.GT",
            "cs.MA",
            "F.1.1; F.3.1"
        ]
    },
    {
        "title": "Comparison of Various Methods for the Calculation of the Distance   Potential Field",
        "authors": [
            "Tobias Kretz",
            "Cornelia Bönisch",
            "Peter Vortisch"
        ],
        "summary": "The distance from a given position toward one or more destinations, exits, and way points is a more or less important input variable in most models of pedestrian dynamics. Except for the special case when there are no obstacles in a concave scenario -- i.e. each position is visible from any other -- the calculation of these distances is a non-trivial task. This isn't that big a problem, as long as the model only demands the distances to be stored in a Static Floor Field also called Potential Field, which never changes throughout the whole simulation. In this case a pre-calculation once before the simulation starts is sufficient. But if one wants to allow changes of the geometry during a simulation run -- imagine doors or the blocking of a corridor due to some hazard -- in the Distance Potential Field, calculation time matters strongly. This contribution gives an overview over existing and new exact and approximate methods to calculate a potential field, analytical investigations for their exactness, and tests of their computation speed. The advantages and drawbacks of the methods are discussed.",
        "published": "2008-04-24T09:13:08Z",
        "link": "http://arxiv.org/abs/0804.3868v1",
        "categories": [
            "physics.comp-ph",
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Counterflow Extension for the F.A.S.T.-Model",
        "authors": [
            "Tobias Kretz",
            "Maike Kaufman",
            "Michael Schreckenberg"
        ],
        "summary": "The F.A.S.T. (Floor field and Agent based Simulation Tool) model is a microscopic model of pedestrian dynamics, which is discrete in space and time. It was developed in a number of more or less consecutive steps from a simple CA model. This contribution is a summary of a study on an extension of the F.A.S.T-model for counterflow situations. The extensions will be explained and it will be shown that the extended F.A.S.T.-model is capable of handling various counterflow situations and to reproduce the well known lane formation effect.",
        "published": "2008-04-28T07:26:31Z",
        "link": "http://arxiv.org/abs/0804.4336v1",
        "categories": [
            "cs.MA",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Prediction and Mitigation of Crush Conditions in Emergency Evacuations",
        "authors": [
            "Peter J. Harding",
            "Martyn Amos",
            "Steve Gwynne"
        ],
        "summary": "Several simulation environments exist for the simulation of large-scale evacuations of buildings, ships, or other enclosed spaces. These offer sophisticated tools for the study of human behaviour, the recreation of environmental factors such as fire or smoke, and the inclusion of architectural or structural features, such as elevators, pillars and exits. Although such simulation environments can provide insights into crowd behaviour, they lack the ability to examine potentially dangerous forces building up within a crowd. These are commonly referred to as crush conditions, and are a common cause of death in emergency evacuations.   In this paper, we describe a methodology for the prediction and mitigation of crush conditions. The paper is organised as follows. We first establish the need for such a model, defining the main factors that lead to crush conditions, and describing several exemplar case studies. We then examine current methods for studying crush, and describe their limitations. From this, we develop a three-stage hybrid approach, using a combination of techniques. We conclude with a brief discussion of the potential benefits of our approach.",
        "published": "2008-05-03T13:00:42Z",
        "link": "http://arxiv.org/abs/0805.0360v1",
        "categories": [
            "cs.CE",
            "cs.MA"
        ]
    },
    {
        "title": "AGNOSCO - Identification of Infected Nodes with artificial Ant Colonies",
        "authors": [
            "Michael Hilker",
            "Christoph Schommer"
        ],
        "summary": "If a computer node is infected by a virus, worm or a backdoor, then this is a security risk for the complete network structure where the node is associated. Existing Network Intrusion Detection Systems (NIDS) provide a certain amount of support for the identification of such infected nodes but suffer from the need of plenty of communication and computational power. In this article, we present a novel approach called AGNOSCO to support the identification of infected nodes through the usage of artificial ant colonies. It is shown that AGNOSCO overcomes the communication and computational power problem while identifying infected nodes properly.",
        "published": "2008-05-06T19:47:11Z",
        "link": "http://arxiv.org/abs/0805.0785v2",
        "categories": [
            "cs.AI",
            "cs.MA",
            "C.2.0"
        ]
    },
    {
        "title": "SANA - Network Protection through artificial Immunity",
        "authors": [
            "Michael Hilker",
            "Christoph Schommer"
        ],
        "summary": "Current network protection systems use a collection of intelligent components - e.g. classifiers or rule-based firewall systems to detect intrusions and anomalies and to secure a network against viruses, worms, or trojans. However, these network systems rely on individuality and support an architecture with less collaborative work of the protection components. They give less administration support for maintenance, but offer a large number of individual single points of failures - an ideal situation for network attacks to succeed. In this work, we discuss the required features, the performance, and the problems of a distributed protection system called SANA. It consists of a cooperative architecture, it is motivated by the human immune system, where the components correspond to artificial immune cells that are connected for their collaborative work. SANA promises a better protection against intruders than common known protection systems through an adaptive self-management while keeping the resources efficiently by an intelligent reduction of redundant tasks. We introduce a library of several novel and common used protection components and evaluate the performance of SANA by a proof-of-concept implementation.",
        "published": "2008-05-07T07:13:32Z",
        "link": "http://arxiv.org/abs/0805.0849v1",
        "categories": [
            "cs.CR",
            "cs.MA",
            "C.2.0"
        ]
    },
    {
        "title": "SANA - Security Analysis in Internet Traffic through Artificial Immune   Systems",
        "authors": [
            "Michael Hilker",
            "Christoph Schommer"
        ],
        "summary": "The Attacks done by Viruses, Worms, Hackers, etc. are a Network Security-Problem in many Organisations. Current Intrusion Detection Systems have significant Disadvantages, e.g. the need of plenty of Computational Power or the Local Installation. Therefore, we introduce a novel Framework for Network Security which is called SANA. SANA contains an artificial Immune System with artificial Cells which perform certain Tasks in order to to support existing systems to better secure the Network against Intrusions. The Advantages of SANA are that it is efficient, adaptive, autonomous, and massively-distributed. In this Article, we describe the Architecture of the artificial Immune System and the Functionality of the Components. We explain briefly the Implementation and discuss Results.",
        "published": "2008-05-07T09:31:18Z",
        "link": "http://arxiv.org/abs/0805.0909v1",
        "categories": [
            "cs.CR",
            "cs.MA",
            "C.2.0"
        ]
    },
    {
        "title": "Swarm-Based Spatial Sorting",
        "authors": [
            "Martyn Amos",
            "Oliver Don"
        ],
        "summary": "Purpose: To present an algorithm for spatially sorting objects into an annular structure. Design/Methodology/Approach: A swarm-based model that requires only stochastic agent behaviour coupled with a pheromone-inspired \"attraction-repulsion\" mechanism. Findings: The algorithm consistently generates high-quality annular structures, and is particularly powerful in situations where the initial configuration of objects is similar to those observed in nature. Research limitations/implications: Experimental evidence supports previous theoretical arguments about the nature and mechanism of spatial sorting by insects. Practical implications: The algorithm may find applications in distributed robotics. Originality/value: The model offers a powerful minimal algorithmic framework, and also sheds further light on the nature of attraction-repulsion algorithms and underlying natural processes.",
        "published": "2008-05-12T19:46:29Z",
        "link": "http://arxiv.org/abs/0805.1727v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Distributed Self Management for Distributed Security Systems",
        "authors": [
            "Michael Hilker"
        ],
        "summary": "Distributed system as e.g. artificial immune systems, complex adaptive systems, or multi-agent systems are widely used in Computer Science, e.g. for network security, optimisations, or simulations. In these systems, small entities move through the network and perform certain tasks. At some time, the entities move to another place and require therefore information where to move is most profitable. Common used systems do not provide any information or use a centralised approach where a center delegates the entities. This article discusses whether small information about the neighbours enhances the performance of the overall system or not. Therefore, two information-protocols are introduced and analysed. In addition, the protocols are implemented and tested using the artificial immune system SANA that protects a network against intrusions.",
        "published": "2008-05-13T06:32:12Z",
        "link": "http://arxiv.org/abs/0805.1785v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "Next Challenges in Bringing Artificial Immune Systems to Production in   Network Security",
        "authors": [
            "Michael Hilker"
        ],
        "summary": "The human immune system protects the human body against various pathogens like e.g. biological viruses and bacteria. Artificial immune systems reuse the architecture, organization, and workflows of the human immune system for various problems in computer science. In the network security, the artificial immune system is used to secure a network and its nodes against intrusions like viruses, worms, and trojans. However, these approaches are far away from production where they are academic proof-of-concept implementations or use only a small part to protect against a certain intrusion. This article discusses the required steps to bring artificial immune systems into production in the network security domain. It furthermore figures out the challenges and provides the description and results of the prototype of an artificial immune system, which is SANA called.",
        "published": "2008-05-13T06:40:39Z",
        "link": "http://arxiv.org/abs/0805.1786v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "A Network Protection Framework through Artificial Immunity",
        "authors": [
            "Michael Hilker",
            "Christoph Schommer"
        ],
        "summary": "Current network protection systems use a collection of intelligent components - e.g. classifiers or rule-based firewall systems to detect intrusions and anomalies and to secure a network against viruses, worms, or trojans. However, these network systems rely on individuality and support an architecture with less collaborative work of the protection components. They give less administration support for maintenance, but offer a large number of individual single points of failures - an ideal situation for network attacks to succeed. In this work, we discuss the required features, the performance, and the problems of a distributed protection system called {\\it SANA}. It consists of a cooperative architecture, it is motivated by the human immune system, where the components correspond to artificial immune cells that are connected for their collaborative work. SANA promises a better protection against intruders than common known protection systems through an adaptive self-management while keeping the resources efficiently by an intelligent reduction of redundancies. We introduce a library of several novel and common used protection components and evaluate the performance of SANA by a proof-of-concept implementation.",
        "published": "2008-05-13T06:51:35Z",
        "link": "http://arxiv.org/abs/0805.1787v1",
        "categories": [
            "cs.MA",
            "cs.CR",
            "C.2.0"
        ]
    },
    {
        "title": "Pedestrian Flow at Bottlenecks - Validation and Calibration of Vissim's   Social Force Model of Pedestrian Traffic and its Empirical Foundations",
        "authors": [
            "Tobias Kretz",
            "Stefan Hengst",
            "Peter Vortisch"
        ],
        "summary": "In this contribution first results of experiments on pedestrian flow through bottlenecks are presented and then compared to simulation results obtained with the Social Force Model in the Vissim simulation framework. Concerning the experiments it is argued that the basic dependence between flow and bottleneck width is not a step function but that it is linear and modified by the effect of a psychological phenomenon. The simulation results as well show a linear dependence and the parameters can be calibrated such that the absolute values for flow and time fit to range of experimental results.",
        "published": "2008-05-13T07:24:32Z",
        "link": "http://arxiv.org/abs/0805.1788v1",
        "categories": [
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Sincere-Strategy Preference-Based Approval Voting Fully Resists   Constructive Control and Broadly Resists Destructive Control",
        "authors": [
            "Gabor Erdelyi",
            "Markus Nowak",
            "Joerg Rothe"
        ],
        "summary": "We study sincere-strategy preference-based approval voting (SP-AV), a system proposed by Brams and Sanver [Electoral Studies, 25(2):287-305, 2006], and here adjusted so as to coerce admissibility of the votes (rather than excluding inadmissible votes a priori), with respect to procedural control. In such control scenarios, an external agent seeks to change the outcome of an election via actions such as adding/deleting/partitioning either candidates or voters. SP-AV combines the voters' preference rankings with their approvals of candidates, where in elections with at least two candidates the voters' approval strategies are adjusted--if needed--to approve of their most-preferred candidate and to disapprove of their least-preferred candidate. This rule coerces admissibility of the votes even in the presence of control actions, and hybridizes, in effect, approval with pluralitiy voting.   We prove that this system is computationally resistant (i.e., the corresponding control problems are NP-hard) to 19 out of 22 types of constructive and destructive control. Thus, SP-AV has more resistances to control than is currently known for any other natural voting system with a polynomial-time winner problem. In particular, SP-AV is (after Copeland voting, see Faliszewski et al. [AAIM-2008, Springer LNCS 5034, pp. 165-176, 2008]) the second natural voting system with an easy winner-determination procedure that is known to have full resistance to constructive control, and unlike Copeland voting it in addition displays broad resistance to destructive control.",
        "published": "2008-06-03T13:27:16Z",
        "link": "http://arxiv.org/abs/0806.0535v5",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Steganographic Routing in Multi Agent System Environment",
        "authors": [
            "Krzysztof Szczypiorski",
            "Igor Margasinski",
            "Wojciech Mazurczyk"
        ],
        "summary": "In this paper we present an idea of trusted communication platform for Multi-Agent Systems (MAS) called TrustMAS. Based on analysis of routing protocols suitable for MAS we have designed a new proactive hidden routing. Proposed steg-agents discovery procedure, as well as further routes updates and hidden communication, are cryptographically independent. Steganographic exchange can cover heterogeneous and geographically outlying environments using available cross-layer covert channels. Finally we have specified rules that agents have to follow to benefit the TrustMAS distributed router platform.",
        "published": "2008-06-03T16:15:16Z",
        "link": "http://arxiv.org/abs/0806.0576v1",
        "categories": [
            "cs.CR",
            "cs.MA"
        ]
    },
    {
        "title": "Collaborative model of interaction and Unmanned Vehicle Systems'   interface",
        "authors": [
            "Sylvie Saget",
            "Francois Legras",
            "Gilles Coppin"
        ],
        "summary": "The interface for the next generation of Unmanned Vehicle Systems should be an interface with multi-modal displays and input controls. Then, the role of the interface will not be restricted to be a support of the interactions between the ground operator and vehicles. Interface must take part in the interaction management too. In this paper, we show that recent works in pragmatics and philosophy provide a suitable theoretical framework for the next generation of UV System's interface. We concentrate on two main aspects of the collaborative model of interaction based on acceptance: multi-strategy approach for communicative act generation and interpretation and communicative alignment.",
        "published": "2008-06-04T14:22:38Z",
        "link": "http://arxiv.org/abs/0806.0784v1",
        "categories": [
            "cs.AI",
            "cs.HC",
            "cs.MA"
        ]
    },
    {
        "title": "Development of Hybrid Intelligent Systems and their Applications from   Engineering Systems to Complex Systems",
        "authors": [
            "Hamed Owladeghaffari"
        ],
        "summary": "In this study, we introduce general frame of MAny Connected Intelligent Particles Systems (MACIPS). Connections and interconnections between particles get a complex behavior of such merely simple system (system in system).Contribution of natural computing, under information granulation theory, are the main topic of this spacious skeleton. Upon this clue, we organize different algorithms involved a few prominent intelligent computing and approximate reasoning methods such as self organizing feature map (SOM)[9], Neuro- Fuzzy Inference System[10], Rough Set Theory (RST)[11], collaborative clustering, Genetic Algorithm and Ant Colony System. Upon this, we have employed our algorithms on the several engineering systems, especially emerged systems in Civil and Mineral processing. In other process, we investigated how our algorithms can be taken as a linkage of government-society interaction, where government catches various fashions of behavior: solid (absolute) or flexible. So, transition of such society, by changing of connectivity parameters (noise) from order to disorder is inferred. Add to this, one may find an indirect mapping among finical systems and eventual market fluctuations with MACIPS. In the following sections, we will mention the main topics of the suggested proposal, briefly Details of the proposed algorithms can be found in the references.",
        "published": "2008-06-14T03:44:35Z",
        "link": "http://arxiv.org/abs/0806.2356v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Frequency of Correctness versus Average-Case Polynomial Time and   Generalized Juntas",
        "authors": [
            "Gabor Erdelyi",
            "Lane A. Hemaspaandra",
            "Joerg Rothe",
            "Holger Spakowski"
        ],
        "summary": "We prove that every distributional problem solvable in polynomial time on the average with respect to the uniform distribution has a frequently self-knowingly correct polynomial-time algorithm. We also study some features of probability weight of correctness with respect to generalizations of Procaccia and Rosenschein's junta distributions [PR07b].",
        "published": "2008-06-16T12:03:37Z",
        "link": "http://arxiv.org/abs/0806.2555v1",
        "categories": [
            "cs.CC",
            "cs.GT",
            "cs.MA",
            "F.1.3; F.2.2; I.2.11"
        ]
    },
    {
        "title": "Multi Site Coordination using a Multi-Agent System",
        "authors": [
            "Thibaud Monteiro",
            "Daniel Roy",
            "Didier Anciaux"
        ],
        "summary": "A new approach of coordination of decisions in a multi site system is proposed. It is based this approach on a multi-agent concept and on the principle of distributed network of enterprises. For this purpose, each enterprise is defined as autonomous and performs simultaneously at the local and global levels. The basic component of our approach is a so-called Virtual Enterprise Node (VEN), where the enterprise network is represented as a set of tiers (like in a product breakdown structure). Within the network, each partner constitutes a VEN, which is in contact with several customers and suppliers. Exchanges between the VENs ensure the autonomy of decision, and guarantiee the consistency of information and material flows. Only two complementary VEN agents are necessary: one for external interactions, the Negotiator Agent (NA) and one for the planning of internal decisions, the Planner Agent (PA). If supply problems occur in the network, two other agents are defined: the Tier Negotiator Agent (TNA) working at the tier level only and the Supply Chain Mediator Agent (SCMA) working at the level of the enterprise network. These two agents are only active when the perturbation occurs. Otherwise, the VENs process the flow of information alone. With this new approach, managing enterprise network becomes much more transparent and looks like managing a simple enterprise in the network. The use of a Multi-Agent System (MAS) allows physical distribution of the decisional system, and procures a heterarchical organization structure with a decentralized control that guaranties the autonomy of each entity and the flexibility of the network.",
        "published": "2008-06-18T15:46:10Z",
        "link": "http://arxiv.org/abs/0806.3031v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Multi-agents architecture for supply chain management",
        "authors": [
            "Daniel Roy",
            "Didier Anciaux",
            "Thibaud Monteiro",
            "Latifa Ouzizi"
        ],
        "summary": "The purpose of this paper is to propose a new approach for the supply chain management. This approach is based on the virtual enterprise paradigm and the used of multi-agent concept. Each entity (like enterprise) is autonomous and must perform local and global goals in relation with its environment. The base component of our approach is a Virtual Enterprise Node (VEN). The supply chain is viewed as a set of tiers (corresponding to the levels of production), in which each partner of the supply chain (VEN) is in relation with several customers and suppliers. Each VEN belongs to one tier. The main customer gives global objectives (quantity, cost and delay) to the supply chain. The Mediator Agent (MA) is in charge to manage the supply chain in order to respect those objectives as global level. Those objectives are taking over to Negotiator Agent at the tier level (NAT). These two agents are only active if a perturbation occurs; otherwise information flows are only exchange between VENs. This architecture allows supply chains management which is completely transparent seen from simple enterprise of the supply chain. The used of Multi-Agent System (MAS) allows physical distribution of the decisional system. Moreover, the hierarchical organizational structure with a decentralized control guaranties, in the same time, the autonomy of each entity and the whole flexibility.",
        "published": "2008-06-18T15:46:29Z",
        "link": "http://arxiv.org/abs/0806.3032v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Separability in the Ambient Logic",
        "authors": [
            "Daniel Hirschkoff",
            "Etienne Lozes",
            "Davide Sangiorgi"
        ],
        "summary": "The \\it{Ambient Logic} (AL) has been proposed for expressing properties of process mobility in the calculus of Mobile Ambients (MA), and as a basis for query languages on semistructured data. We study some basic questions concerning the discriminating power of AL, focusing on the equivalence on processes induced by the logic $(=_L>)$. As underlying calculi besides MA we consider a subcalculus in which an image-finiteness condition holds and that we prove to be Turing complete. Synchronous variants of these calculi are studied as well. In these calculi, we provide two operational characterisations of $_=L$: a coinductive one (as a form of bisimilarity) and an inductive one (based on structual properties of processes). After showing $_=L$ to be stricly finer than barbed congruence, we establish axiomatisations of $_=L$ on the subcalculus of MA (both the asynchronous and the synchronous version), enabling us to relate $_=L$ to structural congruence. We also present some (un)decidability results that are related to the above separation properties for AL: the undecidability of $_=L$ on MA and its decidability on the subcalculus.",
        "published": "2008-06-24T10:00:00Z",
        "link": "http://arxiv.org/abs/0806.3849v2",
        "categories": [
            "cs.LO",
            "cs.MA",
            "cs.PL",
            "F.3.2; F.4.1"
        ]
    },
    {
        "title": "Cooperation with Complement is Better",
        "authors": [
            "Ilker Yildirim",
            "Haluk Bingol"
        ],
        "summary": "In a setting where heterogeneous agents interact to accomplish a given set of goals, cooperation is of utmost importance, especially when agents cannot achieve their individual goals by exclusive use of their own efforts. Even when we consider friendly environments and benevolent agents, cooperation involves several issues: with whom to cooperate, reciprocation, how to address credit assignment and complex division of gains, etc. We propose a model where heterogeneous agents cooperate by forming groups and formation of larger groups is promoted. Benefit of agents is proportional to the performance and the size of the group. There is a time pressure to form a group. We investigate how preferring similar or complement agents in group formation affects an agent's success. Preferring complement in group formation is found to be better, yet there is no need to push the strategy to the extreme since the effect of complementing partners is saturated.",
        "published": "2008-06-24T17:38:05Z",
        "link": "http://arxiv.org/abs/0806.3938v1",
        "categories": [
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "On Krause's multi-agent consensus model with state-dependent   connectivity (Extended version)",
        "authors": [
            "Vincent D. Blondel",
            "Julien M. Hendrickx",
            "John N. Tsitsiklis"
        ],
        "summary": "We study a model of opinion dynamics introduced by Krause: each agent has an opinion represented by a real number, and updates its opinion by averaging all agent opinions that differ from its own by less than 1. We give a new proof of convergence into clusters of agents, with all agents in the same cluster holding the same opinion. We then introduce a particular notion of equilibrium stability and provide lower bounds on the inter-cluster distances at a stable equilibrium. To better understand the behavior of the system when the number of agents is large, we also introduce and study a variant involving a continuum of agents, obtaining partial convergence results and lower bounds on inter-cluster distances, under some mild assumptions.",
        "published": "2008-07-13T12:33:28Z",
        "link": "http://arxiv.org/abs/0807.2028v4",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Logic Engines as Interactors",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "We introduce a new programming language construct, Interactors, supporting the agent-oriented view that programming is a dialog between simple, self-contained, autonomous building blocks.   We define Interactors as an abstraction of answer generation and refinement in Logic Engines resulting in expressive language extension and metaprogramming patterns, including emulation of Prolog's dynamic database.   A mapping between backtracking based answer generation in the callee and \"forward\" recursion in the caller enables interaction between different branches of the callee's search process and provides simplified design patterns for algorithms involving combinatorial generation and infinite answer streams.   Interactors extend language constructs like Ruby, Python and C#'s multiple coroutining block returns through yield statements and they can emulate the action of monadic constructs and catamorphisms in functional languages.   Keywords: generalized iterators, logic engines, agent oriented programming language constructs, interoperation with stateful objects, metaprogramming",
        "published": "2008-08-05T05:48:32Z",
        "link": "http://arxiv.org/abs/0808.0556v1",
        "categories": [
            "cs.PL",
            "cs.MA"
        ]
    },
    {
        "title": "Personal Semantic Web Through A Space Based Computing Environment",
        "authors": [
            "Ian Oliver",
            "Jukka Honkola"
        ],
        "summary": "The Semantic Web through technologies such to support the canonical representation information and presenting it to users in a method by which its meaning can be understood or at least communi- cated and interpreted by all parties. As the Semantic Web evolves into more of a computing platform rather than an information platform more dynamic structures, interactions and behaviours will evolve leading to systems which localise and personalise this Dynamic Semantic Web.",
        "published": "2008-08-11T07:13:43Z",
        "link": "http://arxiv.org/abs/0808.1455v1",
        "categories": [
            "cs.NI",
            "cs.MA"
        ]
    },
    {
        "title": "Offloading Cognition onto Cognitive Technology",
        "authors": [
            "Itiel Dror",
            "Stevan Harnad"
        ],
        "summary": "\"Cognizing\" (e.g., thinking, understanding, and knowing) is a mental state. Systems without mental states, such as cognitive technology, can sometimes contribute to human cognition, but that does not make them cognizers. Cognizers can offload some of their cognitive functions onto cognitive technology, thereby extending their performance capacity beyond the limits of their own brain power. Language itself is a form of cognitive technology that allows cognizers to offload some of their cognitive functions onto the brains of other cognizers. Language also extends cognizers' individual and joint performance powers, distributing the load through interactive and collaborative cognition. Reading, writing, print, telecommunications and computing further extend cognizers' capacities. And now the web, with its network of cognizers, digital databases and software agents, all accessible anytime, anywhere, has become our 'Cognitive Commons,' in which distributed cognizers and cognitive technology can interoperate globally with a speed, scope and degree of interactivity inconceivable through local individual cognition alone. And as with language, the cognitive tool par excellence, such technological changes are not merely instrumental and quantitative: they can have profound effects on how we think and encode information, on how we communicate with one another, on our mental states, and on our very nature.",
        "published": "2008-08-26T19:15:24Z",
        "link": "http://arxiv.org/abs/0808.3569v3",
        "categories": [
            "cs.MA",
            "cs.CL"
        ]
    },
    {
        "title": "TrustMAS: Trusted Communication Platform for Multi-Agent Systems",
        "authors": [
            "Krzysztof Szczypiorski",
            "Igor Margasinski",
            "Wojciech Mazurczyk",
            "Krzysztof Cabaj",
            "Pawel Radziszewski"
        ],
        "summary": "The paper presents TrustMAS - Trusted Communication Platform for Multi-Agent Systems, which provides trust and anonymity for mobile agents. The platform includes anonymous technique based on random-walk algorithm for providing general purpose anonymous communication for agents. All agents, which take part in the proposed platform, benefit from trust and anonymity that is provided for their interactions. Moreover, in TrustMAS there are StegAgents (SA) that are able to perform various steganographic communication. To achieve that goal, SAs may use methods in different layers of TCP/IP model or specialized middleware enabling steganography that allows hidden communication through all layers of mentioned model. In TrustMAS steganographic channels are used to exchange routing tables between StegAgents. Thus all StegAgents in TrustMAS with their ability to exchange information by using hidden channels form distributed steganographic router (Stegrouter).",
        "published": "2008-08-29T10:02:20Z",
        "link": "http://arxiv.org/abs/0808.4060v1",
        "categories": [
            "cs.CR",
            "cs.MA"
        ]
    },
    {
        "title": "Tableau-based decision procedure for the multi-agent epistemic logic   with operators of common and distributed knowledge",
        "authors": [
            "Valentin Goranko",
            "Dmitry Shkatov"
        ],
        "summary": "We develop an incremental-tableau-based decision procedure for the multi-agent epistemic logic MAEL(CD) (aka S5_n (CD)), whose language contains operators of individual knowledge for a finite set Ag of agents, as well as operators of distributed and common knowledge among all agents in Ag. Our tableau procedure works in (deterministic) exponential time, thus establishing an upper bound for MAEL(CD)-satisfiability that matches the (implicit) lower-bound known from earlier results, which implies ExpTime-completeness of MAEL(CD)-satisfiability. Therefore, our procedure provides a complexity-optimal algorithm for checking MAEL(CD)-satisfiability, which, however, in most cases is much more efficient. We prove soundness and completeness of the procedure, and illustrate it with an example.",
        "published": "2008-08-29T17:09:06Z",
        "link": "http://arxiv.org/abs/0808.4133v1",
        "categories": [
            "cs.LO",
            "cs.MA",
            "F.4.1; I.2.4; I.2.11"
        ]
    },
    {
        "title": "Distributed Parameter Estimation in Sensor Networks: Nonlinear   Observation Models and Imperfect Communication",
        "authors": [
            "Soummya Kar",
            "Jose M. F. Moura",
            "Kavita Ramanan"
        ],
        "summary": "The paper studies distributed static parameter (vector) estimation in sensor networks with nonlinear observation models and noisy inter-sensor communication. It introduces \\emph{separably estimable} observation models that generalize the observability condition in linear centralized estimation to nonlinear distributed estimation. It studies two distributed estimation algorithms in separably estimable models, the $\\mathcal{NU}$ (with its linear counterpart $\\mathcal{LU}$) and the $\\mathcal{NLU}$. Their update rule combines a \\emph{consensus} step (where each sensor updates the state by weight averaging it with its neighbors' states) and an \\emph{innovation} step (where each sensor processes its local current observation.) This makes the three algorithms of the \\textit{consensus + innovations} type, very different from traditional consensus. The paper proves consistency (all sensors reach consensus almost surely and converge to the true parameter value,) efficiency, and asymptotic unbiasedness. For $\\mathcal{LU}$ and $\\mathcal{NU}$, it proves asymptotic normality and provides convergence rate guarantees. The three algorithms are characterized by appropriately chosen decaying weight sequences. Algorithms $\\mathcal{LU}$ and $\\mathcal{NU}$ are analyzed in the framework of stochastic approximation theory; algorithm $\\mathcal{NLU}$ exhibits mixed time-scale behavior and biased perturbations, and its analysis requires a different approach that is developed in the paper.",
        "published": "2008-08-29T20:32:48Z",
        "link": "http://arxiv.org/abs/0809.0009v2",
        "categories": [
            "cs.MA",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "ECOLANG - Communications Language for Ecological Simulations Network",
        "authors": [
            "Antonio Pereira"
        ],
        "summary": "This document describes the communication language used in one multiagent system environment for ecological simulations, based on EcoDynamo simulator application linked with several intelligent agents and visualisation applications, and extends the initial definition of the language. The agents actions and perceptions are translated into messages exchanged with the simulator application and other agents. The concepts and definitions used follow the BNF notation (Backus et al. 1960) and is inspired in the Coach Unilang language (Reis and Lau 2002).",
        "published": "2008-09-09T17:46:17Z",
        "link": "http://arxiv.org/abs/0809.1618v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Agent-based Ecological Model Calibration - on the Edge of a New Approach",
        "authors": [
            "Antonio Pereira",
            "Pedro Duarte",
            "Luis Paulo Reis"
        ],
        "summary": "The purpose of this paper is to present a new approach to ecological model calibration -- an agent-based software. This agent works on three stages: 1- It builds a matrix that synthesizes the inter-variable relationships; 2- It analyses the steady-state sensitivity of different variables to different parameters; 3- It runs the model iteratively and measures model lack of fit, adequacy and reliability. Stage 3 continues until some convergence criteria are attained. At each iteration, the agent knows from stages 1 and 2, which parameters are most likely to produce the desired shift on predicted results.",
        "published": "2008-09-09T22:12:37Z",
        "link": "http://arxiv.org/abs/0809.1686v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "The Potluck Problem",
        "authors": [
            "Prabodh K. Enumula",
            "Shrisha Rao"
        ],
        "summary": "This paper proposes the Potluck Problem as a model for the behavior of independent producers and consumers under standard economic assumptions, as a problem of resource allocation in a multi-agent system in which there is no explicit communication among the agents.",
        "published": "2008-09-12T05:54:50Z",
        "link": "http://arxiv.org/abs/0809.2136v2",
        "categories": [
            "cs.GT",
            "cs.MA",
            "I.2.11; I.6.1"
        ]
    },
    {
        "title": "Fairness in Combinatorial Auctioning Systems",
        "authors": [
            "Megha Saini",
            "Shrisha Rao"
        ],
        "summary": "One of the Multi-Agent Systems that is widely used by various government agencies, buyers and sellers in a market economy, in such a manner so as to attain optimized resource allocation, is the Combinatorial Auctioning System (CAS). We study another important aspect of resource allocations in CAS, namely fairness. We present two important notions of fairness in CAS, extended fairness and basic fairness. We give an algorithm that works by incorporating a metric to ensure fairness in a CAS that uses the Vickrey-Clark-Groves (VCG) mechanism, and uses an algorithm of Sandholm to achieve optimality. Mathematical formulations are given to represent measures of extended fairness and basic fairness.",
        "published": "2008-09-12T09:31:47Z",
        "link": "http://arxiv.org/abs/0809.2168v1",
        "categories": [
            "cs.GT",
            "cs.MA",
            "I.2.11; J.4; F.2.2"
        ]
    },
    {
        "title": "Mathematical Tool of Discrete Dynamic Modeling of Complex Systems in   Control Loop",
        "authors": [
            "Armen Bagdasaryan"
        ],
        "summary": "In this paper we present a method of discrete modeling and analysis of multi-level dynamics of complex large-scale hierarchical dynamic systems subject to external dynamic control mechanism. In a model each state describes parallel dynamics and simultaneous trends of changes in system parameters. The essence of the approach is in analysis of system state dynamics while it is in the control loop.",
        "published": "2008-09-16T11:14:20Z",
        "link": "http://arxiv.org/abs/0809.2680v1",
        "categories": [
            "cs.MA",
            "cs.CE"
        ]
    },
    {
        "title": "Mathematical and computer tools of discrete dynamic modeling and   analysis of complex systems in control loop",
        "authors": [
            "Armen Bagdasaryan"
        ],
        "summary": "We present a method of discrete modeling and analysis of multilevel dynamics of complex large-scale hierarchical dynamic systems subject to external dynamic control mechanism. Architectural model of information system supporting simulation and analysis of dynamic processes and development scenarios (strategies) of complex large-scale hierarchical systems is also proposed.",
        "published": "2008-09-22T12:03:52Z",
        "link": "http://arxiv.org/abs/0809.3688v1",
        "categories": [
            "cs.CE",
            "cs.MA"
        ]
    },
    {
        "title": "Llull and Copeland Voting Computationally Resist Bribery and Control",
        "authors": [
            "Piotr Faliszewski",
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "The only systems previously known to be resistant to all the standard control types were highly artificial election systems created by hybridization. We study a parameterized version of Copeland voting, denoted by Copeland^\\alpha, where the parameter \\alpha is a rational number between 0 and 1 that specifies how ties are valued in the pairwise comparisons of candidates. We prove that Copeland^{0.5}, the system commonly referred to as \"Copeland voting,\" provides full resistance to constructive control, and we prove the same for Copeland^\\alpha, for all rational \\alpha, 0 < \\alpha < 1. Copeland voting is the first natural election system proven to have full resistance to constructive control. We also prove that both Copeland^1 (Llull elections) and Copeland^0 are resistant to all standard types of constructive control other than one variant of addition of candidates. Moreover, we show that for each rational \\alpha, 0 \\leq \\alpha \\leq 1, Copeland^\\alpha voting is fully resistant to bribery attacks, and we establish fixed-parameter tractability of bounded-case control for Copeland^\\alpha. We also study Copeland^\\alpha elections under more flexible models such as microbribery and extended control and we integrate the potential irrationality of voter preferences into many of our results.",
        "published": "2008-09-25T19:49:38Z",
        "link": "http://arxiv.org/abs/0809.4484v2",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "A Computational Study on Emotions and Temperament in Multi-Agent Systems",
        "authors": [
            "Luis Paulo Reis",
            "Daria Barteneva",
            "Nuno Lau"
        ],
        "summary": "Recent advances in neurosciences and psychology have provided evidence that affective phenomena pervade intelligence at many levels, being inseparable from the cognitionaction loop. Perception, attention, memory, learning, decisionmaking, adaptation, communication and social interaction are some of the aspects influenced by them. This work draws its inspirations from neurobiology, psychophysics and sociology to approach the problem of building autonomous robots capable of interacting with each other and building strategies based on temperamental decision mechanism. Modelling emotions is a relatively recent focus in artificial intelligence and cognitive modelling. Such models can ideally inform our understanding of human behavior. We may see the development of computational models of emotion as a core research focus that will facilitate advances in the large array of computational systems that model, interpret or influence human behavior. We propose a model based on a scalable, flexible and modular approach to emotion which allows runtime evaluation between emotional quality and performance. The results achieved showed that the strategies based on temperamental decision mechanism strongly influence the system performance and there are evident dependency between emotional state of the agents and their temperamental type, as well as the dependency between the team performance and the temperamental configuration of the team members, and this enable us to conclude that the modular approach to emotional programming based on temperamental theory is the good choice to develop computational mind models for emotional behavioral Multi-Agent systems.",
        "published": "2008-09-27T16:33:34Z",
        "link": "http://arxiv.org/abs/0809.4784v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "cs.RO"
        ]
    },
    {
        "title": "Three New Complexity Results for Resource Allocation Problems",
        "authors": [
            "Bart de Keijzer"
        ],
        "summary": "We prove the following results for task allocation of indivisible resources:   - The problem of finding a leximin-maximal resource allocation is in P if the agents have max-utility functions and atomic demands.   - Deciding whether a resource allocation is Pareto-optimal is coNP-complete for agents with (1-)additive utility functions.   - Deciding whether there exists a Pareto-optimal and envy-free resource allocation is Sigma_2^p-complete for agents with (1-)additive utility functions.",
        "published": "2008-10-02T20:32:52Z",
        "link": "http://arxiv.org/abs/0810.0532v2",
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.CC",
            "cs.GT"
        ]
    },
    {
        "title": "A static theory of promises",
        "authors": [
            "Jan A. Bergstra",
            "Mark Burgess"
        ],
        "summary": "We discuss for the concept of promises within a framework that can be applied to either humans or technology. We compare promises to the more established notion of obligations and find promises to be both simpler and more effective at reducing uncertainty in behavioural outcomes.",
        "published": "2008-10-18T07:41:26Z",
        "link": "http://arxiv.org/abs/0810.3294v5",
        "categories": [
            "cs.MA",
            "cs.SE"
        ]
    },
    {
        "title": "Language structure in the n-object naming game",
        "authors": [
            "Adam Lipowski",
            "Dorota Lipowska"
        ],
        "summary": "We examine a naming game with two agents trying to establish a common vocabulary for n objects. Such efforts lead to the emergence of language that allows for an efficient communication and exhibits some degree of homonymy and synonymy. Although homonymy reduces the communication efficiency, it seems to be a dynamical trap that persists for a long, and perhaps indefinite, time. On the other hand, synonymy does not reduce the efficiency of communication, but appears to be only a transient feature of the language. Thus, in our model the role of synonymy decreases and in the long-time limit it becomes negligible. A similar rareness of synonymy is observed in present natural languages. The role of noise, that distorts the communicated words, is also examined. Although, in general, the noise reduces the communication efficiency, it also regroups the words so that they are more evenly distributed within the available \"verbal\" space.",
        "published": "2008-10-19T23:59:37Z",
        "link": "http://arxiv.org/abs/0810.3442v2",
        "categories": [
            "cs.CL",
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Social Learning Methods in Board Games",
        "authors": [
            "Vukosi N. Marivate",
            "Tshilidzi Marwala"
        ],
        "summary": "This paper discusses the effects of social learning in training of game playing agents. The training of agents in a social context instead of a self-play environment is investigated. Agents that use the reinforcement learning algorithms are trained in social settings. This mimics the way in which players of board games such as scrabble and chess mentor each other in their clubs. A Round Robin tournament and a modified Swiss tournament setting are used for the training. The agents trained using social settings are compared to self play agents and results indicate that more robust agents emerge from the social training setting. Higher state space games can benefit from such settings as diverse set of agents will have multiple strategies that increase the chances of obtaining more experienced players at the end of training. The Social Learning trained agents exhibit better playing experience than self play agents. The modified Swiss playing style spawns a larger number of better playing agents as the population size increases.",
        "published": "2008-10-20T07:04:30Z",
        "link": "http://arxiv.org/abs/0810.3474v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "A Novel Clustering Algorithm Based on a Modified Model of Random Walk",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "summary": "We introduce a modified model of random walk, and then develop two novel clustering algorithms based on it. In the algorithms, each data point in a dataset is considered as a particle which can move at random in space according to the preset rules in the modified model. Further, this data point may be also viewed as a local control subsystem, in which the controller adjusts its transition probability vector in terms of the feedbacks of all data points, and then its transition direction is identified by an event-generating function. Finally, the positions of all data points are updated. As they move in space, data points collect gradually and some separating parts emerge among them automatically. As a consequence, data points that belong to the same class are located at a same position, whereas those that belong to different classes are away from one another. Moreover, the experimental results have demonstrated that data points in the test datasets are clustered reasonably and efficiently, and the comparison with other algorithms also provides an indication of the effectiveness of the proposed algorithms.",
        "published": "2008-10-30T13:26:31Z",
        "link": "http://arxiv.org/abs/0810.5484v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Conjectural Equilibrium in Water-filling Games",
        "authors": [
            "Yi Su",
            "Mihaela van der Schaar"
        ],
        "summary": "This paper considers a non-cooperative game in which competing users sharing a frequency-selective interference channel selfishly optimize their power allocation in order to improve their achievable rates. Previously, it was shown that a user having the knowledge of its opponents' channel state information can make foresighted decisions and substantially improve its performance compared with the case in which it deploys the conventional iterative water-filling algorithm, which does not exploit such knowledge. This paper discusses how a foresighted user can acquire this knowledge by modeling its experienced interference as a function of its own power allocation. To characterize the outcome of the multi-user interaction, the conjectural equilibrium is introduced, and the existence of this equilibrium for the investigated water-filling game is proved. Interestingly, both the Nash equilibrium and the Stackelberg equilibrium are shown to be special cases of the generalization of conjectural equilibrium. We develop practical algorithms to form accurate beliefs and search desirable power allocation strategies. Numerical simulations indicate that a foresighted user without any a priori knowledge of its competitors' private information can effectively learn the required information, and induce the entire system to an operating point that improves both its own achievable rate as well as the rates of the other participants in the water-filling game.",
        "published": "2008-10-31T23:59:07Z",
        "link": "http://arxiv.org/abs/0811.0048v1",
        "categories": [
            "cs.GT",
            "cs.MA"
        ]
    },
    {
        "title": "A Bayesian Framework for Opinion Updates",
        "authors": [
            "Andre C. R. Martins"
        ],
        "summary": "Opinion Dynamics lacks a theoretical basis. In this article, I propose to use a decision-theoretic framework, based on the updating of subjective probabilities, as that basis. We will see we get a basic tool for a better understanding of the interaction between the agents in Opinion Dynamics problems and for creating new models. I will review the few existing applications of Bayesian update rules to both discrete and continuous opinion problems and show that several traditional models can be obtained as special cases or approximations from these Bayesian models. The empirical basis and useful properties of the framework will be discussed and examples of how the framework can be used to describe different problems given.",
        "published": "2008-11-01T20:22:18Z",
        "link": "http://arxiv.org/abs/0811.0113v2",
        "categories": [
            "physics.soc-ph",
            "cs.MA",
            "nlin.AO"
        ]
    },
    {
        "title": "A computational model of affects",
        "authors": [
            "Mika Turkia"
        ],
        "summary": "This article provides a simple logical structure, in which affective concepts (i.e. concepts related to emotions and feelings) can be defined. The set of affects defined is similar to the set of emotions covered in the OCC model (Ortony A., Collins A., and Clore G. L.: The Cognitive Structure of Emotions. Cambridge University Press, 1988), but the model presented in this article is fully computationally defined.",
        "published": "2008-11-02T03:38:59Z",
        "link": "http://arxiv.org/abs/0811.0123v2",
        "categories": [
            "cs.AI",
            "cs.MA",
            "I.2.11; J.4"
        ]
    },
    {
        "title": "Cooperative interface of a swarm of UAVs",
        "authors": [
            "Sylvie Saget",
            "Francois Legras",
            "Gilles Coppin"
        ],
        "summary": "After presenting the broad context of authority sharing, we outline how introducing more natural interaction in the design of the ground operator interface of UV systems should help in allowing a single operator to manage the complexity of his/her task. Introducing new modalities is one one of the means in the realization of our vision of next- generation GOI. A more fundamental aspect resides in the interaction manager which should help balance the workload of the operator between mission and interaction, notably by applying a multi-strategy approach to generation and interpretation. We intend to apply these principles to the context of the Smaart prototype, and in this perspective, we illustrate how to characterize the workload associated with a particular operational situation.",
        "published": "2008-11-03T16:54:10Z",
        "link": "http://arxiv.org/abs/0811.0335v1",
        "categories": [
            "cs.AI",
            "cs.HC",
            "cs.MA"
        ]
    },
    {
        "title": "Modeling Cultural Dynamics",
        "authors": [
            "Liane Gabora"
        ],
        "summary": "EVOC (for EVOlution of Culture) is a computer model of culture that enables us to investigate how various factors such as barriers to cultural diffusion, the presence and choice of leaders, or changes in the ratio of innovation to imitation affect the diversity and effectiveness of ideas. It consists of neural network based agents that invent ideas for actions, and imitate neighbors' actions. The model is based on a theory of culture according to which what evolves through culture is not memes or artifacts, but the internal models of the world that give rise to them, and they evolve not through a Darwinian process of competitive exclusion but a Lamarckian process involving exchange of innovation protocols. EVOC shows an increase in mean fitness of actions over time, and an increase and then decrease in the diversity of actions. Diversity of actions is positively correlated with population size and density, and with barriers between populations. Slowly eroding borders increase fitness without sacrificing diversity by fostering specialization followed by sharing of fit actions. Introducing a leader that broadcasts its actions throughout the population increases the fitness of actions but reduces diversity of actions. Increasing the number of leaders reduces this effect. Efforts are underway to simulate the conditions under which an agent immigrating from one culture to another contributes new ideas while still fitting in.",
        "published": "2008-11-16T03:35:56Z",
        "link": "http://arxiv.org/abs/0811.2551v3",
        "categories": [
            "cs.MA",
            "cs.AI",
            "q-bio.NC"
        ]
    },
    {
        "title": "A Novel Clustering Algorithm Based on Quantum Games",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "summary": "Enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum game with the problem of data clustering, and then develop a quantum-game-based clustering algorithm, in which data points in a dataset are considered as players who can make decisions and implement quantum strategies in quantum games. After each round of a quantum game, each player's expected payoff is calculated. Later, he uses a link-removing-and-rewiring (LRR) function to change his neighbors and adjust the strength of links connecting to them in order to maximize his payoff. Further, algorithms are discussed and analyzed in two cases of strategies, two payoff matrixes and two LRR functions. Consequently, the simulation results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms have fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.",
        "published": "2008-12-03T15:46:03Z",
        "link": "http://arxiv.org/abs/0812.0743v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.GT",
            "cs.MA",
            "cs.NE",
            "quant-ph"
        ]
    },
    {
        "title": "Complex Agent Networks explaining the HIV epidemic among homosexual men   in Amsterdam",
        "authors": [
            "Shan Mei",
            "P. M. A Sloot",
            "Rick Quax",
            "Yifan Zhu",
            "Weiping Wang"
        ],
        "summary": "Simulating the evolution of the Human Immunodeficiency Virus (HIV) epidemic requires a detailed description of the population network, especially for small populations in which individuals can be represented in detail and accuracy. In this paper, we introduce the concept of a Complex Agent Network(CAN) to model the HIV epidemics by combining agent-based modelling and complex networks, in which agents represent individuals that have sexual interactions. The applicability of CANs is demonstrated by constructing and executing a detailed HIV epidemic model for men who have sex with men (MSM) in Amsterdam, including a distinction between steady and casual relationships. We focus on MSM contacts because they play an important role in HIV epidemics and have been tracked in Amsterdam for a long time. Our experiments show good correspondence between the historical data of the Amsterdam cohort and the simulation results.",
        "published": "2008-12-05T15:18:38Z",
        "link": "http://arxiv.org/abs/0812.1155v2",
        "categories": [
            "cs.MA",
            "q-bio.PE"
        ]
    },
    {
        "title": "Multi-Agent Reinforcement Learning and Genetic Policy Sharing",
        "authors": [
            "Jake Ellowitz"
        ],
        "summary": "The effects of policy sharing between agents in a multi-agent dynamical system has not been studied extensively. I simulate a system of agents optimizing the same task using reinforcement learning, to study the effects of different population densities and policy sharing. I demonstrate that sharing policies decreases the time to reach asymptotic behavior, and results in improved asymptotic behavior.",
        "published": "2008-12-09T16:13:33Z",
        "link": "http://arxiv.org/abs/0812.1599v1",
        "categories": [
            "cs.MA",
            "cs.AI"
        ]
    },
    {
        "title": "Emergence of Spontaneous Order Through Neighborhood Formation in   Peer-to-Peer Recommender Systems",
        "authors": [
            "Ernesto Diaz-Aviles",
            "Lars Schmidt-Thieme",
            "Cai-Nicolas Ziegler"
        ],
        "summary": "The advent of the Semantic Web necessitates paradigm shifts away from centralized client/server architectures towards decentralization and peer-to-peer computation, making the existence of central authorities superfluous and even impossible. At the same time, recommender systems are gaining considerable impact in e-commerce, providing people with recommendations that are personalized and tailored to their very needs. These recommender systems have traditionally been deployed with stark centralized scenarios in mind, operating in closed communities detached from their host network's outer perimeter. We aim at marrying these two worlds, i.e., decentralized peer-to-peer computing and recommender systems, in one agent-based framework. Our architecture features an epidemic-style protocol maintaining neighborhoods of like-minded peers in a robust, selforganizing fashion. In order to demonstrate our architecture's ability to retain scalability, robustness and to allow for convergence towards high-quality recommendations, we conduct offline experiments on top of the popular MovieLens dataset.",
        "published": "2008-12-23T23:26:27Z",
        "link": "http://arxiv.org/abs/0812.4460v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "cs.MA",
            "C.2.4; H.3.3"
        ]
    },
    {
        "title": "Corpus sp{é}cialis{é} et ressource de sp{é}cialit{é}",
        "authors": [
            "Bernard Jacquemin",
            "Sabine Ploux"
        ],
        "summary": "\"Semantic Atlas\" is a mathematic and statistic model to visualise word senses according to relations between words. The model, that has been applied to proximity relations from a corpus, has shown its ability to distinguish word senses as the corpus' contributors comprehend them. We propose to use the model and a specialised corpus in order to create automatically a specialised dictionary relative to the corpus' domain. A morpho-syntactic analysis performed on the corpus makes it possible to create the dictionary from syntactic relations between lexical units. The semantic resource can be used to navigate semantically - and not only lexically - through the corpus, to create classical dictionaries or for diachronic studies of the language.",
        "published": "2008-01-08T08:21:26Z",
        "link": "http://arxiv.org/abs/0801.1179v2",
        "categories": [
            "cs.IR",
            "cs.CL"
        ]
    },
    {
        "title": "The emerging field of language dynamics",
        "authors": [
            "S. Wichmann"
        ],
        "summary": "A simple review by a linguist, citing many articles by physicists: Quantitative methods, agent-based computer simulations, language dynamics, language typology, historical linguistics",
        "published": "2008-01-09T12:34:40Z",
        "link": "http://arxiv.org/abs/0801.1415v1",
        "categories": [
            "cs.CL",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Computational approach to the emergence and evolution of language -   evolutionary naming game model",
        "authors": [
            "Adam Lipowski",
            "Dorota Lipowska"
        ],
        "summary": "Computational modelling with multi-agent systems is becoming an important technique of studying language evolution. We present a brief introduction into this rapidly developing field, as well as our own contributions that include an analysis of the evolutionary naming-game model. In this model communicating agents, that try to establish a common vocabulary, are equipped with an evolutionarily selected learning ability. Such a coupling of biological and linguistic ingredients results in an abrupt transition: upon a small change of the model control parameter a poorly communicating group of linguistically unskilled agents transforms into almost perfectly communicating group with large learning abilities. Genetic imprinting of the learning abilities proceeds via Baldwin effect: initially unskilled communicating agents learn a language and that creates a niche in which there is an evolutionary pressure for the increase of learning ability. Under the assumption that communication intensity increases continuously with finite speed, the transition is split into several transition-like changes. It shows that the speed of cultural changes, that sets an additional characteristic timescale, might be yet another factor affecting the evolution of language. In our opinion, this model shows that linguistic and biological processes have a strong influence on each other and this effect certainly has contributed to an explosive development of our species.",
        "published": "2008-01-10T19:45:25Z",
        "link": "http://arxiv.org/abs/0801.1658v3",
        "categories": [
            "physics.soc-ph",
            "cs.CL",
            "cs.MA"
        ]
    },
    {
        "title": "A Comparison of natural (english) and artificial (esperanto) languages.   A Multifractal method based analysis",
        "authors": [
            "J. Gillet",
            "M. Ausloos"
        ],
        "summary": "We present a comparison of two english texts, written by Lewis Carroll, one (Alice in wonderland) and the other (Through a looking glass), the former translated into esperanto, in order to observe whether natural and artificial languages significantly differ from each other. We construct one dimensional time series like signals using either word lengths or word frequencies. We use the multifractal ideas for sorting out correlations in the writings. In order to check the robustness of the methods we also write the corresponding shuffled texts. We compare characteristic functions and e.g. observe marked differences in the (far from parabolic) f(alpha) curves, differences which we attribute to Tsallis non extensive statistical features in the ''frequency time series'' and ''length time series''. The esperanto text has more extreme vallues. A very rough approximation consists in modeling the texts as a random Cantor set if resulting from a binomial cascade of long and short words (or words and blanks). This leads to parameters characterizing the text style, and most likely in fine the author writings.",
        "published": "2008-01-16T14:07:33Z",
        "link": "http://arxiv.org/abs/0801.2510v1",
        "categories": [
            "cs.CL",
            "physics.data-an"
        ]
    },
    {
        "title": "Online-concordance \"Perekhresni stezhky\" (\"The Cross-Paths\"), a novel by   Ivan Franko",
        "authors": [
            "Solomiya Buk",
            "Andrij Rovenchak"
        ],
        "summary": "In the article, theoretical principles and practical realization for the compilation of the concordance to \"Perekhresni stezhky\" (\"The Cross-Paths\"), a novel by Ivan Franko, are described. Two forms for the context presentation are proposed. The electronic version of this lexicographic work is available online.",
        "published": "2008-01-21T17:41:57Z",
        "link": "http://arxiv.org/abs/0801.3239v1",
        "categories": [
            "cs.CL",
            "cs.DL"
        ]
    },
    {
        "title": "Robustness Evaluation of Two CCG, a PCFG and a Link Grammar Parsers",
        "authors": [
            "Tuomo Kakkonen"
        ],
        "summary": "Robustness in a parser refers to an ability to deal with exceptional phenomena. A parser is robust if it deals with phenomena outside its normal range of inputs. This paper reports on a series of robustness evaluations of state-of-the-art parsers in which we concentrated on one aspect of robustness: its ability to parse sentences containing misspelled words. We propose two measures for robustness evaluation based on a comparison of a parser's output for grammatical input sentences and their noisy counterparts. In this paper, we use these measures to compare the overall robustness of the four evaluated parsers, and we present an analysis of the decline in parser performance with increasing error levels. Our results indicate that performance typically declines tens of percentage units when parsers are presented with texts containing misspellings. When it was tested on our purpose-built test set of 443 sentences, the best parser in the experiment (C&C parser) was able to return exactly the same parse tree for the grammatical and ungrammatical sentences for 60.8%, 34.0% and 14.9% of the sentences with one, two or three misspelled words respectively.",
        "published": "2008-01-24T18:41:01Z",
        "link": "http://arxiv.org/abs/0801.3817v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Between conjecture and memento: shaping a collective emotional   perception of the future",
        "authors": [
            "Alberto Pepe",
            "Johan Bollen"
        ],
        "summary": "Large scale surveys of public mood are costly and often impractical to perform. However, the web is awash with material indicative of public mood such as blogs, emails, and web queries. Inexpensive content analysis on such extensive corpora can be used to assess public mood fluctuations. The work presented here is concerned with the analysis of the public mood towards the future. Using an extension of the Profile of Mood States questionnaire, we have extracted mood indicators from 10,741 emails submitted in 2006 to futureme.org, a web service that allows its users to send themselves emails to be delivered at a later date. Our results indicate long-term optimism toward the future, but medium-term apprehension and confusion.",
        "published": "2008-01-25T01:09:47Z",
        "link": "http://arxiv.org/abs/0801.3864v1",
        "categories": [
            "cs.CL",
            "cs.GL"
        ]
    },
    {
        "title": "Methods to integrate a language model with semantic information for a   word prediction component",
        "authors": [
            "Tonio Wandmacher",
            "Jean-Yves Antoine"
        ],
        "summary": "Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4-gram baseline, and most of them to a simple cache model as well.",
        "published": "2008-01-30T17:10:24Z",
        "link": "http://arxiv.org/abs/0801.4716v1",
        "categories": [
            "cs.CL",
            "K.4.2; I.2.7; H.5.2; I.2.1"
        ]
    },
    {
        "title": "Concerning Olga, the Beautiful Little Street Dancer (Adjectives as   Higher-Order Polymorphic Functions)",
        "authors": [
            "Walid S. Saba"
        ],
        "summary": "In this paper we suggest a typed compositional seman-tics for nominal compounds of the form [Adj Noun] that models adjectives as higher-order polymorphic functions, and where types are assumed to represent concepts in an ontology that reflects our commonsense view of the world and the way we talk about it in or-dinary language. In addition to [Adj Noun] compounds our proposal seems also to suggest a plausible explana-tion for well known adjective ordering restrictions.",
        "published": "2008-01-30T19:40:45Z",
        "link": "http://arxiv.org/abs/0801.4746v5",
        "categories": [
            "cs.CL",
            "cs.LO"
        ]
    },
    {
        "title": "Textual Fingerprinting with Texts from Parkin, Bassewitz, and Leander",
        "authors": [
            "Christoph Schommer",
            "Conny Uhde"
        ],
        "summary": "Current research in author profiling to discover a legal author's fingerprint does not only follow examinations based on statistical parameters only but include more and more dynamic methods that can learn and that react adaptable to the specific behavior of an author. But the question on how to appropriately represent a text is still one of the fundamental tasks, and the problem of which attribute should be used to fingerprint the author's style is still not exactly defined. In this work, we focus on linguistic selection of attributes to fingerprint the style of the authors Parkin, Bassewitz and Leander. We use texts of the genre Fairy Tale as it has a clear style and texts of a shorter size with a straightforward story-line and a simple language.",
        "published": "2008-02-15T16:14:09Z",
        "link": "http://arxiv.org/abs/0802.2234v1",
        "categories": [
            "cs.CL",
            "cs.CR",
            "I.7.5; I.5.4; K.6.5"
        ]
    },
    {
        "title": "Hubs in Languages: Scale Free Networks of Synonyms",
        "authors": [
            "Hanna E. Makaruk",
            "Robert Owczarek"
        ],
        "summary": "Natural languages are described in this paper in terms of networks of synonyms: a word is identified with a node, and synonyms are connected by undirected links. Our statistical analysis of the network of synonyms in Polish language showed it is scale-free; similar to what is known for English. The statistical properties of the networks are also similar. Thus, the statistical aspects of the networks are good candidates for culture independent elements of human language. We hypothesize that optimization for robustness and efficiency is responsible for this universality. Despite the statistical similarity, there is no one-to-one mapping between networks of these two languages. Although many hubs in Polish are translated into similarly highly connected hubs in English, there are also hubs specific to one of these languages only: a single word in one language is equivalent to many different and disconnected words in the other, in accordance with the Whorf hypothesis about language relativity. Identifying language-specific hubs is vitally important for automatic translation, and for understanding contextual, culturally related messages that are frequently missed or twisted in a naive, literary translation.",
        "published": "2008-02-28T00:15:54Z",
        "link": "http://arxiv.org/abs/0802.4112v1",
        "categories": [
            "physics.soc-ph",
            "cs.CL",
            "physics.data-an"
        ]
    },
    {
        "title": "Equilibrium (Zipf) and Dynamic (Grasseberg-Procaccia) method based   analyses of human texts. A comparison of natural (english) and artificial   (esperanto) languages",
        "authors": [
            "M. Ausloos"
        ],
        "summary": "A comparison of two english texts from Lewis Carroll, one (Alice in wonderland), also translated into esperanto, the other (Through a looking glass) are discussed in order to observe whether natural and artificial languages significantly differ from each other. One dimensional time series like signals are constructed using only word frequencies (FTS) or word lengths (LTS). The data is studied through (i) a Zipf method for sorting out correlations in the FTS and (ii) a Grassberger-Procaccia (GP) technique based method for finding correlations in LTS. Features are compared : different power laws are observed with characteristic exponents for the ranking properties, and the {\\it phase space attractor dimensionality}. The Zipf exponent can take values much less than unity ($ca.$ 0.50 or 0.30) depending on how a sentence is defined. This non-universality is conjectured to be a measure of the author $style$. Moreover the attractor dimension $r$ is a simple function of the so called phase space dimension $n$, i.e., $r = n^{\\lambda}$, with $\\lambda = 0.79$. Such an exponent should also conjecture to be a measure of the author $creativity$. However, even though there are quantitative differences between the original english text and its esperanto translation, the qualitative differences are very minutes, indicating in this case a translation relatively well respecting, along our analysis lines, the content of the author writing.",
        "published": "2008-02-28T11:49:48Z",
        "link": "http://arxiv.org/abs/0802.4215v1",
        "categories": [
            "physics.soc-ph",
            "cs.CL",
            "physics.data-an"
        ]
    },
    {
        "title": "Some properties of the Ukrainian writing system",
        "authors": [
            "Solomija Buk",
            "Ján Mačutek",
            "Andrij Rovenchak"
        ],
        "summary": "We investigate the grapheme-phoneme relation in Ukrainian and some properties of the Ukrainian version of the Cyrillic alphabet.",
        "published": "2008-02-28T12:58:49Z",
        "link": "http://arxiv.org/abs/0802.4198v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "The Generation of Textual Entailment with NLML in an Intelligent   Dialogue system for Language Learning CSIEC",
        "authors": [
            "Jiyou Jia"
        ],
        "summary": "This research report introduces the generation of textual entailment within the project CSIEC (Computer Simulation in Educational Communication), an interactive web-based human-computer dialogue system with natural language for English instruction. The generation of textual entailment (GTE) is critical to the further improvement of CSIEC project. Up to now we have found few literatures related with GTE. Simulating the process that a human being learns English as a foreign language we explore our naive approach to tackle the GTE problem and its algorithm within the framework of CSIEC, i.e. rule annotation in NLML, pattern recognition (matching), and entailment transformation. The time and space complexity of our algorithm is tested with some entailment examples. Further works include the rules annotation based on the English textbooks and a GUI interface for normal users to edit the entailment rules.",
        "published": "2008-02-29T06:16:29Z",
        "link": "http://arxiv.org/abs/0802.4326v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CY"
        ]
    },
    {
        "title": "Figuring out Actors in Text Streams: Using Collocations to establish   Incremental Mind-maps",
        "authors": [
            "T. Rothenberger",
            "S. Oez",
            "E. Tahirovic",
            "C. Schommer"
        ],
        "summary": "The recognition, involvement, and description of main actors influences the story line of the whole text. This is of higher importance as the text per se represents a flow of words and expressions that once it is read it is lost. In this respect, the understanding of a text and moreover on how the actor exactly behaves is not only a major concern: as human beings try to store a given input on short-term memory while associating diverse aspects and actors with incidents, the following approach represents a virtual architecture, where collocations are concerned and taken as the associative completion of the actors' acting. Once that collocations are discovered, they become managed in separated memory blocks broken down by the actors. As for human beings, the memory blocks refer to associative mind-maps. We then present several priority functions to represent the actual temporal situation inside a mind-map to enable the user to reconstruct the recent events from the discovered temporal results.",
        "published": "2008-03-19T18:00:19Z",
        "link": "http://arxiv.org/abs/0803.2856v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.7.5; I.5.4; H.2.4"
        ]
    },
    {
        "title": "Effects of High-Order Co-occurrences on Word Semantic Similarities",
        "authors": [
            "Benoît Lemaire",
            "Guy Denhière"
        ],
        "summary": "A computational model of the construction of word meaning through exposure to texts is built in order to simulate the effects of co-occurrence values on word semantic similarities, paragraph by paragraph. Semantic similarity is here viewed as association. It turns out that the similarity between two words W1 and W2 strongly increases with a co-occurrence, decreases with the occurrence of W1 without W2 or W2 without W1, and slightly increases with high-order co-occurrences. Therefore, operationalizing similarity as a frequency of co-occurrence probably introduces a bias: first, there are cases in which there is similarity without co-occurrence and, second, the frequency of co-occurrence overestimates similarity.",
        "published": "2008-04-01T11:33:39Z",
        "link": "http://arxiv.org/abs/0804.0143v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Parts-of-Speech Tagger Errors Do Not Necessarily Degrade Accuracy in   Extracting Information from Biomedical Text",
        "authors": [
            "Maurice HT Ling",
            "Christophe Lefevre",
            "Kevin R. Nicholas"
        ],
        "summary": "A recent study reported development of Muscorian, a generic text processing tool for extracting protein-protein interactions from text that achieved comparable performance to biomedical-specific text processing tools. This result was unexpected since potential errors from a series of text analysis processes is likely to adversely affect the outcome of the entire process. Most biomedical entity relationship extraction tools have used biomedical-specific parts-of-speech (POS) tagger as errors in POS tagging and are likely to affect subsequent semantic analysis of the text, such as shallow parsing. This study aims to evaluate the parts-of-speech (POS) tagging accuracy and attempts to explore whether a comparable performance is obtained when a generic POS tagger, MontyTagger, was used in place of MedPost, a tagger trained in biomedical text. Our results demonstrated that MontyTagger, Muscorian's POS tagger, has a POS tagging accuracy of 83.1% when tested on biomedical text. Replacing MontyTagger with MedPost did not result in a significant improvement in entity relationship extraction from text; precision of 55.6% from MontyTagger versus 56.8% from MedPost on directional relationships and 86.1% from MontyTagger compared to 81.8% from MedPost on nondirectional relationships. This is unexpected as the potential for poor POS tagging by MontyTagger is likely to affect the outcome of the information extraction. An analysis of POS tagging errors demonstrated that 78.5% of tagging errors are being compensated by shallow parsing. Thus, despite 83.1% tagging accuracy, MontyTagger has a functional tagging accuracy of 94.6%.",
        "published": "2008-04-02T09:34:13Z",
        "link": "http://arxiv.org/abs/0804.0317v1",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "A Semi-Automatic Framework to Discover Epistemic Modalities in   Scientific Articles",
        "authors": [
            "Sviatlana Danilava",
            "Christoph Schommer"
        ],
        "summary": "Documents in scientific newspapers are often marked by attitudes and opinions of the author and/or other persons, who contribute with objective and subjective statements and arguments as well. In this respect, the attitude is often accomplished by a linguistic modality. As in languages like english, french and german, the modality is expressed by special verbs like can, must, may, etc. and the subjunctive mood, an occurrence of modalities often induces that these verbs take over the role of modality. This is not correct as it is proven that modality is the instrument of the whole sentence where both the adverbs, modal particles, punctuation marks, and the intonation of a sentence contribute. Often, a combination of all these instruments are necessary to express a modality. In this work, we concern with the finding of modal verbs in scientific texts as a pre-step towards the discovery of the attitude of an author. Whereas the input will be an arbitrary text, the output consists of zones representing modalities.",
        "published": "2008-04-07T14:13:27Z",
        "link": "http://arxiv.org/abs/0804.1033v1",
        "categories": [
            "cs.CL",
            "cs.LO",
            "J.5; I.2.7; F.4.3"
        ]
    },
    {
        "title": "Information filtering based on wiki index database",
        "authors": [
            "A. V. Smirnov",
            "A. A. Krizhanovsky"
        ],
        "summary": "In this paper we present a profile-based approach to information filtering by an analysis of the content of text documents. The Wikipedia index database is created and used to automatically generate the user profile from the user document collection. The problem-oriented Wikipedia subcorpora are created (using knowledge extracted from the user profile) for each topic of user interests. The index databases of these subcorpora are applied to filtering information flow (e.g., mails, news). Thus, the analyzed texts are classified into several topics explicitly presented in the user profile. The paper concentrates on the indexing part of the approach. The architecture of an application implementing the Wikipedia indexing is described. The indexing method is evaluated using the Russian and Simple English Wikipedia.",
        "published": "2008-04-15T11:05:59Z",
        "link": "http://arxiv.org/abs/0804.2354v2",
        "categories": [
            "cs.IR",
            "cs.CL",
            "I.7.2; I.7.3; I.7.5; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Phoneme recognition in TIMIT with BLSTM-CTC",
        "authors": [
            "Santiago Fernández",
            "Alex Graves",
            "Juergen Schmidhuber"
        ],
        "summary": "We compare the performance of a recurrent neural network with the best results published so far on phoneme recognition in the TIMIT database. These published results have been obtained with a combination of classifiers. However, in this paper we apply a single recurrent neural network to the same task. Our recurrent neural network attains an error rate of 24.6%. This result is not significantly different from that obtained by the other best methods, but they rely on a combination of classifiers for achieving comparable performance.",
        "published": "2008-04-21T15:38:45Z",
        "link": "http://arxiv.org/abs/0804.3269v1",
        "categories": [
            "cs.CL",
            "cs.NE",
            "I.2.7; I.5.4"
        ]
    },
    {
        "title": "Respect My Authority! HITS Without Hyperlinks, Utilizing Cluster-Based   Language Models",
        "authors": [
            "Oren Kurland",
            "Lillian Lee"
        ],
        "summary": "We present an approach to improving the precision of an initial document ranking wherein we utilize cluster information within a graph-based framework. The main idea is to perform re-ranking based on centrality within bipartite graphs of documents (on one side) and clusters (on the other side), on the premise that these are mutually reinforcing entities. Links between entities are created via consideration of language models induced from them.   We find that our cluster-document graphs give rise to much better retrieval performance than previously proposed document-only graphs do. For example, authority-based re-ranking of documents via a HITS-style cluster-based approach outperforms a previously-proposed PageRank-inspired algorithm applied to solely-document graphs. Moreover, we also show that computing authority scores for clusters constitutes an effective method for identifying clusters containing a large percentage of relevant documents.",
        "published": "2008-04-22T20:02:14Z",
        "link": "http://arxiv.org/abs/0804.3599v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Feature Unification in TAG Derivation Trees",
        "authors": [
            "Sylvain Schmitz",
            "Joseph Le Roux"
        ],
        "summary": "The derivation trees of a tree adjoining grammar provide a first insight into the sentence semantics, and are thus prime targets for generation systems. We define a formalism, feature-based regular tree grammars, and a translation from feature based tree adjoining grammars into this new formalism. The translation preserves the derivation structures of the original grammar, and accounts for feature unification.",
        "published": "2008-04-29T11:39:18Z",
        "link": "http://arxiv.org/abs/0804.4584v1",
        "categories": [
            "cs.CL",
            "F.4.2; I.2.7"
        ]
    },
    {
        "title": "Decomposition Techniques for Subgraph Matching",
        "authors": [
            "Stephane Zampelli",
            "Martin Mann",
            "Yves Deville",
            "Rolf Backofen"
        ],
        "summary": "In the constraint programming framework, state-of-the-art static and dynamic decomposition techniques are hard to apply to problems with complete initial constraint graphs. For such problems, we propose a hybrid approach of these techniques in the presence of global constraints. In particular, we solve the subgraph isomorphism problem. Further we design specific heuristics for this hard problem, exploiting its special structure to achieve decomposition. The underlying idea is to precompute a static heuristic on a subset of its constraint network, to follow this static ordering until a first problem decomposition is available, and to switch afterwards to a fully propagated, dynamically decomposing search. Experimental results show that, for sparse graphs, our decomposition method solves more instances than dedicated, state-of-the-art matching algorithms or standard constraint programming approaches.",
        "published": "2008-05-07T17:41:47Z",
        "link": "http://arxiv.org/abs/0805.1030v1",
        "categories": [
            "cs.CC",
            "cs.CL"
        ]
    },
    {
        "title": "Graph Algorithms for Improving Type-Logical Proof Search",
        "authors": [
            "Richard Moot"
        ],
        "summary": "Proof nets are a graph theoretical representation of proofs in various fragments of type-logical grammar. In spite of this basis in graph theory, there has been relatively little attention to the use of graph theoretic algorithms for type-logical proof search. In this paper we will look at several ways in which standard graph theoretic algorithms can be used to restrict the search space. In particular, we will provide an O(n4) algorithm for selecting an optimal axiom link at any stage in the proof search as well as a O(kn3) algorithm for selecting the k best proof candidates.",
        "published": "2008-05-15T13:30:08Z",
        "link": "http://arxiv.org/abs/0805.2303v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "A toolkit for a generative lexicon",
        "authors": [
            "Patrick Henry",
            "Christian Bassac"
        ],
        "summary": "In this paper we describe the conception of a software toolkit designed for the construction, maintenance and collaborative use of a Generative Lexicon. In order to ease its portability and spreading use, this tool was built with free and open source products. We eventually tested the toolkit and showed it filters the adequate form of anaphoric reference to the modifier in endocentric compounds.",
        "published": "2008-05-16T13:58:44Z",
        "link": "http://arxiv.org/abs/0805.2537v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Computational Representation of Linguistic Structures using   Domain-Specific Languages",
        "authors": [
            "Fabian Steeg",
            "Christoph Benden",
            "Paul O. Samuelsdorff"
        ],
        "summary": "We describe a modular system for generating sentences from formal definitions of underlying linguistic structures using domain-specific languages. The system uses Java in general, Prolog for lexical entries and custom domain-specific languages based on Functional Grammar and Functional Discourse Grammar notation, implemented using the ANTLR parser generator. We show how linguistic and technological parts can be brought together in a natural language processing system and how domain-specific languages can be used as a tool for consistent formal notation in linguistic description.",
        "published": "2008-05-21T23:44:06Z",
        "link": "http://arxiv.org/abs/0805.3366v1",
        "categories": [
            "cs.CL",
            "I.2.4; I.2.7"
        ]
    },
    {
        "title": "Exploring a type-theoretic approach to accessibility constraint   modelling",
        "authors": [
            "Sylvain Pogodalla"
        ],
        "summary": "The type-theoretic modelling of DRT that [degroote06] proposed features continuations for the management of the context in which a clause has to be interpreted. This approach, while keeping the standard definitions of quantifier scope, translates the rules of the accessibility constraints of discourse referents inside the semantic recipes. In this paper, we deal with additional rules for these accessibility constraints. In particular in the case of discourse referents introduced by proper nouns, that negation does not block, and in the case of rhetorical relations that structure discourses. We show how this continuation-based approach applies to those accessibility constraints and how we can consider the parallel management of various principles.",
        "published": "2008-05-22T08:48:28Z",
        "link": "http://arxiv.org/abs/0805.3410v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Goal-oriented Dialog as a Collaborative Subordinated Activity involving   Collective Acceptance",
        "authors": [
            "Sylvie Saget",
            "Marc Guyomard"
        ],
        "summary": "Modeling dialog as a collaborative activity consists notably in specifying the content of the Conversational Common Ground and the kind of social mental state involved. In previous work (Saget, 2006), we claim that Collective Acceptance is the proper social attitude for modeling Conversational Common Ground in the particular case of goal-oriented dialog. In this paper, a formalization of Collective Acceptance is shown, besides elements in order to integrate this attitude in a rational model of dialog are provided; and finally, a model of referential acts as being part of a collaborative activity is presented. The particular case of reference has been chosen in order to exemplify our claims.",
        "published": "2008-05-27T12:16:12Z",
        "link": "http://arxiv.org/abs/0805.4101v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "A semantic space for modeling children's semantic memory",
        "authors": [
            "Guy Denhière",
            "Benoît Lemaire",
            "Cédrick Bellissens",
            "Sandra Jhean"
        ],
        "summary": "The goal of this paper is to present a model of children's semantic memory, which is based on a corpus reproducing the kinds of texts children are exposed to. After presenting the literature in the development of the semantic memory, a preliminary French corpus of 3.2 million words is described. Similarities in the resulting semantic space are compared to human data on four tests: association norms, vocabulary test, semantic judgments and memory tasks. A second corpus is described, which is composed of subcorpora corresponding to various ages. This stratified corpus is intended as a basis for developmental studies. Finally, two applications of these models of semantic memory are presented: the first one aims at tracing the development of semantic similarities paragraph by paragraph; the second one describes an implementation of a model of text comprehension derived from the Construction-integration model (Kintsch, 1988, 1998) and based on such models of semantic memory.",
        "published": "2008-05-28T14:56:18Z",
        "link": "http://arxiv.org/abs/0805.4369v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Textual Entailment Recognizing by Theorem Proving Approach",
        "authors": [
            "Doina Tatar",
            "Militon Frentiu"
        ],
        "summary": "In this paper we present two original methods for recognizing textual inference. First one is a modified resolution method such that some linguistic considerations are introduced in the unification of two atoms. The approach is possible due to the recent methods of transforming texts in logic formulas. Second one is based on semantic relations in text, as presented in WordNet. Some similarities between these two methods are remarked.",
        "published": "2008-05-29T11:53:39Z",
        "link": "http://arxiv.org/abs/0805.4521v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "La fiabilité des informations sur le web",
        "authors": [
            "Bernard Jacquemin",
            "Aurélien Lauf",
            "Céline Poudat",
            "Martine Hurault-Plantet",
            "Nicolas Auray"
        ],
        "summary": "Online IR tools have to take into account new phenomena linked to the appearance of blogs, wiki and other collaborative publications. Among these collaborative sites, Wikipedia represents a crucial source of information. However, the quality of this information has been recently questionned. A better knowledge of the contributors' behaviors should help users navigate through information whose quality may vary from one source to another. In order to explore this idea, we present an analysis of the role of different types of contributors in the control of the publication of conflictual articles.",
        "published": "2008-05-30T10:45:42Z",
        "link": "http://arxiv.org/abs/0805.4722v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "cs.CY"
        ]
    },
    {
        "title": "Managing conflicts between users in Wikipedia",
        "authors": [
            "Bernard Jacquemin",
            "Aurélien Lauf",
            "Céline Poudat",
            "Martine Hurault-Plantet",
            "Nicolas Auray"
        ],
        "summary": "Wikipedia is nowadays a widely used encyclopedia, and one of the most visible sites on the Internet. Its strong principle of collaborative work and free editing sometimes generates disputes due to disagreements between users. In this article we study how the wikipedian community resolves the conflicts and which roles do wikipedian choose in this process. We observed the users behavior both in the article talk pages, and in the Arbitration Committee pages specifically dedicated to serious disputes. We first set up a users typology according to their involvement in conflicts and their publishing and management activity in the encyclopedia. We then used those user types to describe users behavior in contributing to articles that are tagged by the wikipedian community as being in conflict with the official guidelines of Wikipedia, or conversely as being well featured.",
        "published": "2008-05-30T13:20:42Z",
        "link": "http://arxiv.org/abs/0805.4754v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "A chain dictionary method for Word Sense Disambiguation and applications",
        "authors": [
            "Doina Tatar",
            "Gabriela Serban",
            "Andreea Mihis",
            "Mihaiela Lupea",
            "Dana Lupsa",
            "Militon Frentiu"
        ],
        "summary": "A large class of unsupervised algorithms for Word Sense Disambiguation (WSD) is that of dictionary-based methods. Various algorithms have as the root Lesk's algorithm, which exploits the sense definitions in the dictionary directly. Our approach uses the lexical base WordNet for a new algorithm originated in Lesk's, namely \"chain algorithm for disambiguation of all words\", CHAD. We show how translation from a language into another one and also text entailment verification could be accomplished by this disambiguation.",
        "published": "2008-06-16T13:48:55Z",
        "link": "http://arxiv.org/abs/0806.2581v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "How Is Meaning Grounded in Dictionary Definitions?",
        "authors": [
            "A. Blondin Masse",
            "G. Chicoisne",
            "Y. Gargouri",
            "S. Harnad",
            "O. Picard",
            "O. Marcotte"
        ],
        "summary": "Meaning cannot be based on dictionary definitions all the way down: at some point the circularity of definitions must be broken in some way, by grounding the meanings of certain words in sensorimotor categories learned from experience or shaped by evolution. This is the \"symbol grounding problem.\" We introduce the concept of a reachable set -- a larger vocabulary whose meanings can be learned from a smaller vocabulary through definition alone, as long as the meanings of the smaller vocabulary are themselves already grounded. We provide simple algorithms to compute reachable sets for any given dictionary.",
        "published": "2008-06-23T15:53:05Z",
        "link": "http://arxiv.org/abs/0806.3710v2",
        "categories": [
            "cs.CL",
            "cs.DB",
            "A.2; H.3.1; I.2.7; I.2.0; I.2.4; H.3.2; I.5.4"
        ]
    },
    {
        "title": "Computational Approaches to Measuring the Similarity of Short Contexts :   A Review of Applications and Methods",
        "authors": [
            "Ted Pedersen"
        ],
        "summary": "Measuring the similarity of short written contexts is a fundamental problem in Natural Language Processing. This article provides a unifying framework by which short context problems can be categorized both by their intended application and proposed solution. The goal is to show that various problems and methodologies that appear quite different on the surface are in fact very closely related. The axes by which these categorizations are made include the format of the contexts (headed versus headless), the way in which the contexts are to be measured (first-order versus second-order similarity), and the information used to represent the features in the contexts (micro versus macro views). The unifying thread that binds together many short context applications and methods is the fact that similarity decisions must be made between contexts that share few (if any) words in common.",
        "published": "2008-06-23T23:27:20Z",
        "link": "http://arxiv.org/abs/0806.3787v2",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "About the creation of a parallel bilingual corpora of web-publications",
        "authors": [
            "D. V. Lande",
            "V. V. Zhygalo"
        ],
        "summary": "The algorithm of the creation texts parallel corpora was presented. The algorithm is based on the use of \"key words\" in text documents, and on the means of their automated translation. Key words were singled out by means of using Russian and Ukrainian morphological dictionaries, as well as dictionaries of the translation of nouns for the Russian and Ukrainianlanguages. Besides, to calculate the weights of the terms in the documents, empiric-statistic rules were used. The algorithm under consideration was realized in the form of a program complex, integrated into the content-monitoring InfoStream system. As a result, a parallel bilingual corpora of web-publications containing about 30 thousand documents, was created",
        "published": "2008-07-02T09:49:14Z",
        "link": "http://arxiv.org/abs/0807.0311v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Music, Complexity, Information",
        "authors": [
            "Damian H. Zanette"
        ],
        "summary": "These are the preparatory notes for a Science & Music essay, \"Playing by numbers\", appeared in Nature 453 (2008) 988-989.",
        "published": "2008-07-03T13:42:00Z",
        "link": "http://arxiv.org/abs/0807.0565v1",
        "categories": [
            "physics.soc-ph",
            "cs.CL"
        ]
    },
    {
        "title": "Scientific Paper Summarization Using Citation Summary Networks",
        "authors": [
            "Vahed Qazvinian",
            "Dragomir R. Radev"
        ],
        "summary": "Quickly moving to a new area of research is painful for researchers due to the vast amount of scientific literature in each field of study. One possible way to overcome this problem is to summarize a scientific topic. In this paper, we propose a model of summarizing a single article, which can be further used to summarize an entire topic. Our model is based on analyzing others' viewpoint of the target article's contributions and the study of its citation summary network using a clustering approach.",
        "published": "2008-07-10T00:01:20Z",
        "link": "http://arxiv.org/abs/0807.1560v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3; H.3.1; I.2.7; G.2.2"
        ]
    },
    {
        "title": "TuLiPA: Towards a Multi-Formalism Parsing Environment for Grammar   Engineering",
        "authors": [
            "Laura Kallmeyer",
            "Timm Lichte",
            "Wolfgang Maier",
            "Yannick Parmentier",
            "Johannes Dellert",
            "Kilian Evang"
        ],
        "summary": "In this paper, we present an open-source parsing environment (Tuebingen Linguistic Parsing Architecture, TuLiPA) which uses Range Concatenation Grammar (RCG) as a pivot formalism, thus opening the way to the parsing of several mildly context-sensitive formalisms. This environment currently supports tree-based grammars (namely Tree-Adjoining Grammars, TAG) and Multi-Component Tree-Adjoining Grammars with Tree Tuples (TT-MCTAG)) and allows computation not only of syntactic structures, but also of the corresponding semantic representations. It is used for the development of a tree-based grammar for German.",
        "published": "2008-07-23T09:05:50Z",
        "link": "http://arxiv.org/abs/0807.3622v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Formal semantics of language and the Richard-Berry paradox",
        "authors": [
            "Stefano Crespi Reghizzi"
        ],
        "summary": "The classical logical antinomy known as Richard-Berry paradox is combined with plausible assumptions about the size i.e. the descriptional complexity of Turing machines formalizing certain sentences, to show that formalization of language leads to contradiction.",
        "published": "2008-07-24T10:31:55Z",
        "link": "http://arxiv.org/abs/0807.3845v1",
        "categories": [
            "cs.CL",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Logics for the Relational Syllogistic",
        "authors": [
            "Ian Pratt-Hartmann",
            "Lawrence S. Moss"
        ],
        "summary": "The Aristotelian syllogistic cannot account for the validity of many inferences involving relational facts. In this paper, we investigate the prospects for providing a relational syllogistic. We identify several fragments based on (a) whether negation is permitted on all nouns, including those in the subject of a sentence; and (b) whether the subject noun phrase may contain a relative clause. The logics we present are extensions of the classical syllogistic, and we pay special attention to the question of whether reductio ad absurdum is needed. Thus our main goal is to derive results on the existence (or non-existence) of syllogistic proof systems for relational fragments. We also determine the computational complexity of all our fragments.",
        "published": "2008-08-04T22:26:38Z",
        "link": "http://arxiv.org/abs/0808.0521v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "cs.CL",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Commonsense Knowledge, Ontology and Ordinary Language",
        "authors": [
            "Walid S. Saba"
        ],
        "summary": "Over two decades ago a \"quite revolution\" overwhelmingly replaced knowledgebased approaches in natural language processing (NLP) by quantitative (e.g., statistical, corpus-based, machine learning) methods. Although it is our firm belief that purely quantitative approaches cannot be the only paradigm for NLP, dissatisfaction with purely engineering approaches to the construction of large knowledge bases for NLP are somewhat justified. In this paper we hope to demonstrate that both trends are partly misguided and that the time has come to enrich logical semantics with an ontological structure that reflects our commonsense view of the world and the way we talk about in ordinary language. In this paper it will be demonstrated that assuming such an ontological structure a number of challenges in the semantics of natural language (e.g., metonymy, intensionality, copredication, nominal compounds, etc.) can be properly and uniformly addressed.",
        "published": "2008-08-08T14:37:45Z",
        "link": "http://arxiv.org/abs/0808.1211v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Index wiki database: design and experiments",
        "authors": [
            "A. A. Krizhanovsky"
        ],
        "summary": "With the fantastic growth of Internet usage, information search in documents of a special type called a \"wiki page\" that is written using a simple markup language, has become an important problem. This paper describes the software architectural model for indexing wiki texts in three languages (Russian, English, and German) and the interaction between the software components (GATE, Lemmatizer, and Synarcher). The inverted file index database was designed using visual tool DBDesigner. The rules for parsing Wikipedia texts are illustrated by examples. Two index databases of Russian Wikipedia (RW) and Simple English Wikipedia (SEW) are built and compared. The size of RW is by order of magnitude higher than SEW (number of words, lexemes), though the growth rate of number of pages in SEW was found to be 14% higher than in Russian, and the rate of acquisition of new words in SEW lexicon was 7% higher during a period of five months (from September 2007 to February 2008). The Zipf's law was tested with both Russian and Simple Wikipedias. The entire source code of the indexing software and the generated index databases are freely available under GPL (GNU General Public License).",
        "published": "2008-08-12T23:47:21Z",
        "link": "http://arxiv.org/abs/0808.1753v2",
        "categories": [
            "cs.IR",
            "cs.CL",
            "I.7.2; I.7.3; I.7.5; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Investigation of the Zipf-plot of the extinct Meroitic language",
        "authors": [
            "Reginald D. Smith"
        ],
        "summary": "The ancient and extinct language Meroitic is investigated using Zipf's Law. In particular, since Meroitic is still undeciphered, the Zipf law analysis allows us to assess the quality of current texts and possible avenues for future investigation using statistical techniques.",
        "published": "2008-08-21T10:54:54Z",
        "link": "http://arxiv.org/abs/0808.2904v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "What It Feels Like To Hear Voices: Fond Memories of Julian Jaynes",
        "authors": [
            "Stevan Harnad"
        ],
        "summary": "Julian Jaynes's profound humanitarian convictions not only prevented him from going to war, but would have prevented him from ever kicking a dog. Yet according to his theory, not only are language-less dogs unconscious, but so too were the speaking/hearing Greeks in the Bicameral Era, when they heard gods' voices telling them what to do rather than thinking for themselves. I argue that to be conscious is to be able to feel, and that all mammals (and probably lower vertebrates and invertebrates too) feel, hence are conscious. Julian Jaynes's brilliant analysis of our concepts of consciousness nevertheless keeps inspiring ever more inquiry and insights into the age-old mind/body problem and its relation to cognition and language.",
        "published": "2008-08-26T18:17:44Z",
        "link": "http://arxiv.org/abs/0808.3563v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Offloading Cognition onto Cognitive Technology",
        "authors": [
            "Itiel Dror",
            "Stevan Harnad"
        ],
        "summary": "\"Cognizing\" (e.g., thinking, understanding, and knowing) is a mental state. Systems without mental states, such as cognitive technology, can sometimes contribute to human cognition, but that does not make them cognizers. Cognizers can offload some of their cognitive functions onto cognitive technology, thereby extending their performance capacity beyond the limits of their own brain power. Language itself is a form of cognitive technology that allows cognizers to offload some of their cognitive functions onto the brains of other cognizers. Language also extends cognizers' individual and joint performance powers, distributing the load through interactive and collaborative cognition. Reading, writing, print, telecommunications and computing further extend cognizers' capacities. And now the web, with its network of cognizers, digital databases and software agents, all accessible anytime, anywhere, has become our 'Cognitive Commons,' in which distributed cognizers and cognitive technology can interoperate globally with a speed, scope and degree of interactivity inconceivable through local individual cognition alone. And as with language, the cognitive tool par excellence, such technological changes are not merely instrumental and quantitative: they can have profound effects on how we think and encode information, on how we communicate with one another, on our mental states, and on our very nature.",
        "published": "2008-08-26T19:15:24Z",
        "link": "http://arxiv.org/abs/0808.3569v3",
        "categories": [
            "cs.MA",
            "cs.CL"
        ]
    },
    {
        "title": "Constructing word similarities in Meroitic as an aid to decipherment",
        "authors": [
            "Reginald D. Smith"
        ],
        "summary": "Meroitic is the still undeciphered language of the ancient civilization of Kush. Over the years, various techniques for decipherment such as finding a bilingual text or cognates from modern or other ancient languages in the Sudan and surrounding areas has not been successful. Using techniques borrowed from information theory and natural language statistics, similar words are paired and attempts are made to use currently defined words to extract at least partial meaning from unknown words.",
        "published": "2008-08-27T02:02:40Z",
        "link": "http://arxiv.org/abs/0808.3616v3",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Open architecture for multilingual parallel texts",
        "authors": [
            "M. T. Carrasco Benitez"
        ],
        "summary": "Multilingual parallel texts (abbreviated to parallel texts) are linguistic versions of the same content (\"translations\"); e.g., the Maastricht Treaty in English and Spanish are parallel texts. This document is about creating an open architecture for the whole Authoring, Translation and Publishing Chain (ATP-chain) for the processing of parallel texts.",
        "published": "2008-08-28T11:59:34Z",
        "link": "http://arxiv.org/abs/0808.3889v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Swapping Lemmas for Regular and Context-Free Languages",
        "authors": [
            "Tomoyuki Yamakami"
        ],
        "summary": "In formal language theory, one of the most fundamental tools, known as pumping lemmas, is extremely useful for regular and context-free languages. However, there are natural properties for which the pumping lemmas are of little use. One of such examples concerns a notion of advice, which depends only on the size of an underlying input. A standard pumping lemma encounters difficulty in proving that a given language is not regular in the presence of advice. We develop its substitution, called a swapping lemma for regular languages, to demonstrate the non-regularity of a target language with advice. For context-free languages, we also present a similar form of swapping lemma, which serves as a technical tool to show that certain languages are not context-free with advice.",
        "published": "2008-08-29T16:09:08Z",
        "link": "http://arxiv.org/abs/0808.4122v2",
        "categories": [
            "cs.CC",
            "cs.CL",
            "cs.FL"
        ]
    },
    {
        "title": "On the nature of long-range letter correlations in texts",
        "authors": [
            "Dmitrii Y. Manin"
        ],
        "summary": "The origin of long-range letter correlations in natural texts is studied using random walk analysis and Jensen-Shannon divergence. It is concluded that they result from slow variations in letter frequency distribution, which are a consequence of slow variations in lexical composition within the text. These correlations are preserved by random letter shuffling within a moving window. As such, they do reflect structural properties of the text, but in a very indirect manner.",
        "published": "2008-08-31T06:08:15Z",
        "link": "http://arxiv.org/abs/0809.0103v1",
        "categories": [
            "cs.CL",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks have been treated independently, using a wide variety of algorithms. These four semantic classes, however, are a tiny sample of the full range of semantic phenomena, and we cannot afford to create ad hoc algorithms for each semantic phenomenon; we need to seek a unified approach. We propose to subsume a broad range of phenomena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonym questions, ESL synonym-antonym questions, and similar-associated-both questions from cognitive psychology.",
        "published": "2008-08-31T14:00:26Z",
        "link": "http://arxiv.org/abs/0809.0124v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "The Complexity of Enriched Mu-Calculi",
        "authors": [
            "Piero A. Bonatti",
            "Carsten Lutz",
            "Aniello Murano",
            "Moshe Y. Vardi"
        ],
        "summary": "The fully enriched &mu;-calculus is the extension of the propositional &mu;-calculus with inverse programs, graded modalities, and nominals. While satisfiability in several expressive fragments of the fully enriched &mu;-calculus is known to be decidable and ExpTime-complete, it has recently been proved that the full calculus is undecidable. In this paper, we study the fragments of the fully enriched &mu;-calculus that are obtained by dropping at least one of the additional constructs. We show that, in all fragments obtained in this way, satisfiability is decidable and ExpTime-complete. Thus, we identify a family of decidable logics that are maximal (and incomparable) in expressive power. Our results are obtained by introducing two new automata models, showing that their emptiness problems are ExpTime-complete, and then reducing satisfiability in the relevant logics to these problems. The automata models we introduce are two-way graded alternating parity automata over infinite trees (2GAPTs) and fully enriched automata (FEAs) over infinite forests. The former are a common generalization of two incomparable automata models from the literature. The latter extend alternating automata in a similar way as the fully enriched &mu;-calculus extends the standard &mu;-calculus.",
        "published": "2008-09-02T07:51:04Z",
        "link": "http://arxiv.org/abs/0809.0360v2",
        "categories": [
            "cs.LO",
            "cs.CL",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Using descriptive mark-up to formalize translation quality assessment",
        "authors": [
            "Andrey Kutuzov"
        ],
        "summary": "The paper deals with using descriptive mark-up to emphasize translation mistakes. The author postulates the necessity to develop a standard and formal XML-based way of describing translation mistakes. It is considered to be important for achieving impersonal translation quality assessment. Marked-up translations can be used in corpus translation studies; moreover, automatic translation assessment based on marked-up mistakes is possible. The paper concludes with setting up guidelines for further activity within the described field.",
        "published": "2008-09-18T20:48:13Z",
        "link": "http://arxiv.org/abs/0809.3250v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Mining Meaning from Wikipedia",
        "authors": [
            "Olena Medelyan",
            "David Milne",
            "Catherine Legg",
            "Ian H. Witten"
        ],
        "summary": "Wikipedia is a goldmine of information; not just for its many readers, but also for the growing community of researchers who recognize it as a resource of exceptional scale and utility. It represents a vast investment of manual effort and judgment: a huge, constantly evolving tapestry of concepts and relations that is being applied to a host of tasks.   This article provides a comprehensive description of this work. It focuses on research that extracts and makes use of the concepts, relations, facts and descriptions found in Wikipedia, and organizes the work into four broad categories: applying Wikipedia to natural language processing; using it to facilitate information retrieval and information extraction; and as a resource for ontology building. The article addresses how Wikipedia is being used as is, how it is being improved and adapted, and how it is being combined with other structures to create entirely new resources. We identify the research groups and individuals involved, and how their work has developed in the last few years. We provide a comprehensive list of the open-source software they have produced.",
        "published": "2008-09-26T04:47:19Z",
        "link": "http://arxiv.org/abs/0809.4530v2",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.IR",
            "I.2.6; I.2.7; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Distribution of complexities in the Vai script",
        "authors": [
            "Andrij Rovenchak",
            "Ján Mačutek",
            "Charles Riley"
        ],
        "summary": "In the paper, we analyze the distribution of complexities in the Vai script, an indigenous syllabic writing system from Liberia. It is found that the uniformity hypothesis for complexities fails for this script. The models using Poisson distribution for the number of components and hyper-Poisson distribution for connections provide good fits in the case of the Vai script.",
        "published": "2008-10-01T15:53:36Z",
        "link": "http://arxiv.org/abs/0810.0200v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Une grammaire formelle du créole martiniquais pour la génération   automatique",
        "authors": [
            "Pascal Vaillant"
        ],
        "summary": "In this article, some first elements of a computational modelling of the grammar of the Martiniquese French Creole dialect are presented. The sources of inspiration for the modelling is the functional description given by Damoiseau (1984), and Pinalie's & Bernabe's (1999) grammar manual. Based on earlier works in text generation (Vaillant, 1997), a unification grammar formalism, namely Tree Adjoining Grammars (TAG), and a modelling of lexical functional categories based on syntactic and semantic properties, are used to implement a grammar of Martiniquese Creole which is used in a prototype of text generation system. One of the main applications of the system could be its use as a tool software supporting the task of learning Creole as a second language. -- Nous pr\\'esenterons dans cette communication les premiers travaux de mod\\'elisation informatique d'une grammaire de la langue cr\\'eole martiniquaise, en nous inspirant des descriptions fonctionnelles de Damoiseau (1984) ainsi que du manuel de Pinalie & Bernab\\'e (1999). Prenant appui sur des travaux ant\\'erieurs en g\\'en\\'eration de texte (Vaillant, 1997), nous utilisons un formalisme de grammaires d'unification, les grammaires d'adjonction d'arbres (TAG d'apr\\`es l'acronyme anglais), ainsi qu'une mod\\'elisation de cat\\'egories lexicales fonctionnelles \\`a base syntaxico-s\\'emantique, pour mettre en oeuvre une grammaire du cr\\'eole martiniquais utilisable dans une maquette de syst\\`eme de g\\'en\\'eration automatique. L'un des int\\'er\\^ets principaux de ce syst\\`eme pourrait \\^etre son utilisation comme logiciel outil pour l'aide \\`a l'apprentissage du cr\\'eole en tant que langue seconde.",
        "published": "2008-10-07T14:40:19Z",
        "link": "http://arxiv.org/abs/0810.1199v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A Layered Grammar Model: Using Tree-Adjoining Grammars to Build a Common   Syntactic Kernel for Related Dialects",
        "authors": [
            "Pascal Vaillant"
        ],
        "summary": "This article describes the design of a common syntactic description for the core grammar of a group of related dialects. The common description does not rely on an abstract sub-linguistic structure like a metagrammar: it consists in a single FS-LTAG where the actual specific language is included as one of the attributes in the set of attribute types defined for the features. When the lang attribute is instantiated, the selected subset of the grammar is equivalent to the grammar of one dialect. When it is not, we have a model of a hybrid multidialectal linguistic system. This principle is used for a group of creole languages of the West-Atlantic area, namely the French-based Creoles of Haiti, Guadeloupe, Martinique and French Guiana.",
        "published": "2008-10-07T14:50:59Z",
        "link": "http://arxiv.org/abs/0810.1207v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Analyse spectrale des textes: détection automatique des frontières   de langue et de discours",
        "authors": [
            "Pascal Vaillant",
            "Richard Nock",
            "Claudia Henry"
        ],
        "summary": "We propose a theoretical framework within which information on the vocabulary of a given corpus can be inferred on the basis of statistical information gathered on that corpus. Inferences can be made on the categories of the words in the vocabulary, and on their syntactical properties within particular languages. Based on the same statistical data, it is possible to build matrices of syntagmatic similarity (bigram transition matrices) or paradigmatic similarity (probability for any pair of words to share common contexts). When clustered with respect to their syntagmatic similarity, words tend to group into sublanguage vocabularies, and when clustered with respect to their paradigmatic similarity, into syntactic or semantic classes. Experiments have explored the first of these two possibilities. Their results are interpreted in the frame of a Markov chain modelling of the corpus' generative processe(s): we show that the results of a spectral analysis of the transition matrix can be interpreted as probability distributions of words within clusters. This method yields a soft clustering of the vocabulary into sublanguages which contribute to the generation of heterogeneous corpora. As an application, we show how multilingual texts can be visually segmented into linguistically homogeneous segments. Our method is specifically useful in the case of related languages which happened to be mixed in corpora.",
        "published": "2008-10-07T15:25:31Z",
        "link": "http://arxiv.org/abs/0810.1212v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Soft Uncoupling of Markov Chains for Permeable Language Distinction: A   New Algorithm",
        "authors": [
            "Richard Nock",
            "Pascal Vaillant",
            "Frank Nielsen",
            "Claudia Henry"
        ],
        "summary": "Without prior knowledge, distinguishing different languages may be a hard task, especially when their borders are permeable. We develop an extension of spectral clustering -- a powerful unsupervised classification toolbox -- that is shown to resolve accurately the task of soft language distinction. At the heart of our approach, we replace the usual hard membership assignment of spectral clustering by a soft, probabilistic assignment, which also presents the advantage to bypass a well-known complexity bottleneck of the method. Furthermore, our approach relies on a novel, convenient construction of a Markov chain out of a corpus. Extensive experiments with a readily available system clearly display the potential of the method, which brings a visually appealing soft distinction of languages that may define altogether a whole corpus.",
        "published": "2008-10-07T18:09:07Z",
        "link": "http://arxiv.org/abs/0810.1261v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "On the Vocabulary of Grammar-Based Codes and the Logical Consistency of   Texts",
        "authors": [
            "Łukasz Dębowski"
        ],
        "summary": "The article presents a new interpretation for Zipf-Mandelbrot's law in natural language which rests on two areas of information theory. Firstly, we construct a new class of grammar-based codes and, secondly, we investigate properties of strongly nonergodic stationary processes. The motivation for the joint discussion is to prove a proposition with a simple informal statement: If a text of length $n$ describes $n^\\beta$ independent facts in a repetitive way then the text contains at least $n^\\beta/\\log n$ different words, under suitable conditions on $n$. In the formal statement, two modeling postulates are adopted. Firstly, the words are understood as nonterminal symbols of the shortest grammar-based encoding of the text. Secondly, the text is assumed to be emitted by a finite-energy strongly nonergodic source whereas the facts are binary IID variables predictable in a shift-invariant way.",
        "published": "2008-10-17T16:32:17Z",
        "link": "http://arxiv.org/abs/0810.3125v5",
        "categories": [
            "cs.IT",
            "cs.CL",
            "math.IT",
            "94A29, 60G10, 94A17",
            "E.4; G.3; I.2.7"
        ]
    },
    {
        "title": "Text as Statistical Mechanics Object",
        "authors": [
            "K. Koroutchev",
            "E. Korutcheva"
        ],
        "summary": "In this article we present a model of human written text based on statistical mechanics approach by deriving the potential energy for different parts of the text using large text corpus. We have checked the results numerically and found that the specific heat parameter effectively separates the closed class words from the specific terms used in the text.",
        "published": "2008-10-19T17:34:33Z",
        "link": "http://arxiv.org/abs/0810.3416v1",
        "categories": [
            "cs.CL",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Language structure in the n-object naming game",
        "authors": [
            "Adam Lipowski",
            "Dorota Lipowska"
        ],
        "summary": "We examine a naming game with two agents trying to establish a common vocabulary for n objects. Such efforts lead to the emergence of language that allows for an efficient communication and exhibits some degree of homonymy and synonymy. Although homonymy reduces the communication efficiency, it seems to be a dynamical trap that persists for a long, and perhaps indefinite, time. On the other hand, synonymy does not reduce the efficiency of communication, but appears to be only a transient feature of the language. Thus, in our model the role of synonymy decreases and in the long-time limit it becomes negligible. A similar rareness of synonymy is observed in present natural languages. The role of noise, that distorts the communicated words, is also examined. Although, in general, the noise reduces the communication efficiency, it also regroups the words so that they are more evenly distributed within the available \"verbal\" space.",
        "published": "2008-10-19T23:59:37Z",
        "link": "http://arxiv.org/abs/0810.3442v2",
        "categories": [
            "cs.CL",
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Assembling Actor-based Mind-Maps from Text Stream",
        "authors": [
            "Claudine Brucks",
            "Christoph Schommer"
        ],
        "summary": "For human beings, the processing of text streams of unknown size leads generally to problems because e.g. noise must be selected out, information be tested for its relevance or redundancy, and linguistic phenomenon like ambiguity or the resolution of pronouns be advanced. Putting this into simulation by using an artificial mind-map is a challenge, which offers the gate for a wide field of applications like automatic text summarization or punctual retrieval. In this work we present a framework that is a first step towards an automatic intellect. It aims at assembling a mind-map based on incoming text streams and on a subject-verb-object strategy, having the verb as an interconnection between the adjacent nouns. The mind-map's performance is enriched by a pronoun resolution engine that bases on the work of D. Klein, and C. D. Manning.",
        "published": "2008-10-25T16:00:08Z",
        "link": "http://arxiv.org/abs/0810.4616v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "I.2.4; H.3.1"
        ]
    },
    {
        "title": "Computational modelling of evolution: ecosystems and language",
        "authors": [
            "Adam Lipowski",
            "Dorota Lipowska"
        ],
        "summary": "Recently, computational modelling became a very important research tool that enables us to study problems that for decades evaded scientific analysis. Evolutionary systems are certainly examples of such problems: they are composed of many units that might reproduce, diffuse, mutate, die, or in some cases for example communicate. These processes might be of some adaptive value, they influence each other and occur on various time scales. That is why such systems are so difficult to study. In this paper we briefly review some computational approaches, as well as our contributions, to the evolution of ecosystems and language. We start from Lotka-Volterra equations and the modelling of simple two-species prey-predator systems. Such systems are canonical example for studying oscillatory behaviour in competitive populations. Then we describe various approaches to study long-term evolution of multi-species ecosystems. We emphasize the need to use models that take into account both ecological and evolutionary processes. Finally, we address the problem of the emergence and development of language. It is becoming more and more evident that any theory of language origin and development must be consistent with darwinian principles of evolution. Consequently, a number of techniques developed for modelling evolution of complex ecosystems are being applied to the problem of language. We briefly review some of these approaches.",
        "published": "2008-10-27T23:20:11Z",
        "link": "http://arxiv.org/abs/0810.4952v1",
        "categories": [
            "q-bio.PE",
            "cs.CL",
            "physics.soc-ph"
        ]
    },
    {
        "title": "CoZo+ - A Content Zoning Engine for textual documents",
        "authors": [
            "Cynthia Wagner",
            "Christoph Schommer"
        ],
        "summary": "Content zoning can be understood as a segmentation of textual documents into zones. This is inspired by [6] who initially proposed an approach for the argumentative zoning of textual documents. With the prototypical CoZo+ engine, we focus on content zoning towards an automatic processing of textual streams while considering only the actors as the zones. We gain information that can be used to realize an automatic recognition of content for pre-defined actors. We understand CoZo+ as a necessary pre-step towards an automatic generation of summaries and to make intellectual ownership of documents detectable.",
        "published": "2008-11-04T09:08:32Z",
        "link": "http://arxiv.org/abs/0811.0453v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.3; H.3.1"
        ]
    },
    {
        "title": "UNL-French deconversion as transfer & generation from an interlingua   with possible quality enhancement through offline human interaction",
        "authors": [
            "Gilles sérasset",
            "Christian Boitet"
        ],
        "summary": "We present the architecture of the UNL-French deconverter, which \"generates\" from the UNL interlingua by first\"localizing\" the UNL form for French, within UNL, and then applying slightly adapted but classical transfer and generation techniques, implemented in GETA's Ariane-G5 environment, supplemented by some UNL-specific tools. Online interaction can be used during deconversion to enhance output quality and is now used for development purposes. We show how interaction could be delayed and embedded in the postedition phase, which would then interact not directly with the output text, but indirectly with several components of the deconverter. Interacting online or offline can improve the quality not only of the utterance at hand, but also of the utterances processed later, as various preferences may be automatically changed to let the deconverter \"learn\".",
        "published": "2008-11-04T19:31:58Z",
        "link": "http://arxiv.org/abs/0811.0579v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "The Application of Fuzzy Logic to Collocation Extraction",
        "authors": [
            "Raj Kishor Bisht",
            "H. S. Dhami"
        ],
        "summary": "Collocations are important for many tasks of Natural language processing such as information retrieval, machine translation, computational lexicography etc. So far many statistical methods have been used for collocation extraction. Almost all the methods form a classical crisp set of collocation. We propose a fuzzy logic approach of collocation extraction to form a fuzzy set of collocations in which each word combination has a certain grade of membership for being collocation. Fuzzy logic provides an easy way to express natural language into fuzzy logic rules. Two existing methods; Mutual information and t-test have been utilized for the input of the fuzzy inference system. The resulting membership function could be easily seen and demonstrated. To show the utility of the fuzzy logic some word pairs have been examined as an example. The working data has been based on a corpus of about one million words contained in different novels constituting project Gutenberg available on www.gutenberg.org. The proposed method has all the advantages of the two methods, while overcoming their drawbacks. Hence it provides a better result than the two methods.",
        "published": "2008-11-08T10:44:43Z",
        "link": "http://arxiv.org/abs/0811.1260v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Prospective Study for Semantic Inter-Media Fusion in Content-Based   Medical Image Retrieval",
        "authors": [
            "Roxana Teodorescu",
            "Daniel Racoceanu",
            "Wee-Kheng Leow",
            "Vladimir Cretu"
        ],
        "summary": "One important challenge in modern Content-Based Medical Image Retrieval (CBMIR) approaches is represented by the semantic gap, related to the complexity of the medical knowledge. Among the methods that are able to close this gap in CBMIR, the use of medical thesauri/ontologies has interesting perspectives due to the possibility of accessing on-line updated relevant webservices and to extract real-time medical semantic structured information. The CBMIR approach proposed in this paper uses the Unified Medical Language System's (UMLS) Metathesaurus to perform a semantic indexing and fusion of medical media. This fusion operates before the query processing (retrieval) and works at an UMLS-compliant conceptual indexing level. Our purpose is to study various techniques related to semantic data alignment, preprocessing, fusion, clustering and retrieval, by evaluating the various techniques and highlighting future research directions. The alignment and the preprocessing are based on partial text/image retrieval feedback and on the data structure. We analyze various probabilistic, fuzzy and evidence-based approaches for the fusion process and different similarity functions for the retrieval process. All the proposed methods are evaluated on the Cross Language Evaluation Forum's (CLEF) medical image retrieval benchmark, by focusing also on a more homogeneous component medical image database: the Pathology Education Instructional Resource (PEIR).",
        "published": "2008-11-28T13:30:23Z",
        "link": "http://arxiv.org/abs/0811.4717v1",
        "categories": [
            "cs.IR",
            "cs.CL"
        ]
    },
    {
        "title": "A Computational Model to Disentangle Semantic Information Embedded in   Word Association Norms",
        "authors": [
            "J. Borge",
            "A. Arenas"
        ],
        "summary": "Two well-known databases of semantic relationships between pairs of words used in psycholinguistics, feature-based and association-based, are studied as complex networks. We propose an algorithm to disentangle feature based relationships from free association semantic networks. The algorithm uses the rich topology of the free association semantic network to produce a new set of relationships between words similar to those observed in feature production norms.",
        "published": "2008-12-16T14:24:23Z",
        "link": "http://arxiv.org/abs/0812.3070v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "physics.data-an",
            "physics.soc-ph"
        ]
    },
    {
        "title": "The Latent Relation Mapping Engine: Algorithm and Experiments",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Many AI researchers and cognitive scientists have argued that analogy is the core of cognition. The most influential work on computational modeling of analogy-making is Structure Mapping Theory (SMT) and its implementation in the Structure Mapping Engine (SME). A limitation of SME is the requirement for complex hand-coded representations. We introduce the Latent Relation Mapping Engine (LRME), which combines ideas from SME and Latent Relational Analysis (LRA) in order to remove the requirement for hand-coded representations. LRME builds analogical mappings between lists of words, using a large corpus of raw text to automatically discover the semantic relations among the words. We evaluate LRME on a set of twenty analogical mapping problems, ten based on scientific analogies and ten based on common metaphors. LRME achieves human-level performance on the twenty problems. We compare LRME with a variety of alternative approaches and find that they are not able to reach the same level of performance.",
        "published": "2008-12-23T20:08:53Z",
        "link": "http://arxiv.org/abs/0812.4446v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "H.3.1, I.2.6, I.2.7"
        ]
    },
    {
        "title": "Spam: It's Not Just for Inboxes and Search Engines! Making Hirsch   h-index Robust to Scientospam",
        "authors": [
            "Dimitrios Katsaros",
            "Leonidas Akritidis",
            "Panayiotis Bozanis"
        ],
        "summary": "What is the 'level of excellence' of a scientist and the real impact of his/her work upon the scientific thinking and practising? How can we design a fair, an unbiased metric -- and most importantly -- a metric robust to manipulation?",
        "published": "2008-01-02T13:06:37Z",
        "link": "http://arxiv.org/abs/0801.0386v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Modeling Online Reviews with Multi-grain Topic Models",
        "authors": [
            "Ivan Titov",
            "Ryan McDonald"
        ],
        "summary": "In this paper we present a novel framework for extracting the ratable aspects of objects from online user reviews. Extracting such aspects is an important challenge in automatically mining product opinions from the web and in generating opinion-based summaries of user reviews. Our models are based on extensions to standard topic modeling methods such as LDA and PLSA to induce multi-grain topics. We argue that multi-grain models are more appropriate for our task since standard models tend to produce topics that correspond to global properties of objects (e.g., the brand of a product type) rather than the aspects of an object that tend to be rated by a user. The models we present not only extract ratable aspects, but also cluster them into coherent topics, e.g., `waitress' and `bartender' are part of the same topic `staff' for restaurants. This differentiates it from much of the previous work which extracts aspects through term frequency analysis with minimal clustering. We evaluate the multi-grain models both qualitatively and quantitatively to show that they improve significantly upon standard topic models.",
        "published": "2008-01-07T17:01:34Z",
        "link": "http://arxiv.org/abs/0801.1063v1",
        "categories": [
            "cs.IR",
            "cs.DB",
            "H.2.8; H.3.1; H.4"
        ]
    },
    {
        "title": "Corpus sp{é}cialis{é} et ressource de sp{é}cialit{é}",
        "authors": [
            "Bernard Jacquemin",
            "Sabine Ploux"
        ],
        "summary": "\"Semantic Atlas\" is a mathematic and statistic model to visualise word senses according to relations between words. The model, that has been applied to proximity relations from a corpus, has shown its ability to distinguish word senses as the corpus' contributors comprehend them. We propose to use the model and a specialised corpus in order to create automatically a specialised dictionary relative to the corpus' domain. A morpho-syntactic analysis performed on the corpus makes it possible to create the dictionary from syntactic relations between lexical units. The semantic resource can be used to navigate semantically - and not only lexically - through the corpus, to create classical dictionaries or for diachronic studies of the language.",
        "published": "2008-01-08T08:21:26Z",
        "link": "http://arxiv.org/abs/0801.1179v2",
        "categories": [
            "cs.IR",
            "cs.CL"
        ]
    },
    {
        "title": "String algorithms and data structures",
        "authors": [
            "Paolo Ferragina"
        ],
        "summary": "The string-matching field has grown at a such complicated stage that various issues come into play when studying it: data structure and algorithmic design, database principles, compression techniques, architectural features, cache and prefetching policies. The expertise nowadays required to design good string data structures and algorithms is therefore transversal to many computer science fields and much more study on the orchestration of known, or novel, techniques is needed to make progress in this fascinating topic. This survey is aimed at illustrating the key ideas which should constitute, in our opinion, the current background of every index designer. We also discuss the positive features and drawback of known indexing schemes and algorithms, and devote much attention to detail research issues and open problems both on the theoretical and the experimental side.",
        "published": "2008-01-15T20:54:18Z",
        "link": "http://arxiv.org/abs/0801.2378v1",
        "categories": [
            "cs.DS",
            "cs.IR"
        ]
    },
    {
        "title": "Survey of Technologies for Web Application Development",
        "authors": [
            "Barry Doyle",
            "Cristina Videira Lopes"
        ],
        "summary": "Web-based application developers face a dizzying array of platforms, languages, frameworks and technical artifacts to choose from. We survey, classify, and compare technologies supporting Web application development. The classification is based on (1) foundational technologies; (2)integration with other information sources; and (3) dynamic content generation. We further survey and classify software engineering techniques and tools that have been adopted from traditional programming into Web programming. We conclude that, although the infrastructure problems of the Web have largely been solved, the cacophony of technologies for Web-based applications reflects the lack of a solid model tailored for this domain.",
        "published": "2008-01-17T05:06:44Z",
        "link": "http://arxiv.org/abs/0801.2618v1",
        "categories": [
            "cs.SE",
            "cs.IR",
            "cs.NI",
            "A.1; D.1.0; D.1.1; D.2.11; H.3.5; H.5.4"
        ]
    },
    {
        "title": "Balancing transparency, efficiency and security in pervasive systems",
        "authors": [
            "Mark Wenstrom",
            "Eloisa Bentivegna",
            "Ali Hurson"
        ],
        "summary": "This chapter will survey pervasive computing with a look at how its constraint for transparency affects issues of resource management and security. The goal of pervasive computing is to render computing transparent, such that computing resources are ubiquitously offered to the user and services are proactively performed for a user without his or her intervention. The task of integrating computing infrastructure into everyday life without making it excessively invasive brings about tradeoffs between flexibility and robustness, efficiency and effectiveness, as well as autonomy and reliability. As the feasibility of ubiquitous computing and its real potential for mass applications are still a matter of controversy, this chapter will look into the underlying issues of resource management and authentication to discover how these can be handled in a least invasive fashion. The discussion will be closed by an overview of the solutions proposed by current pervasive computing efforts, both in the area of generic platforms and for dedicated applications such as pervasive education and healthcare.",
        "published": "2008-01-20T19:15:50Z",
        "link": "http://arxiv.org/abs/0801.3102v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.m"
        ]
    },
    {
        "title": "Descent methods for Nonnegative Matrix Factorization",
        "authors": [
            "Ngoc-Diep Ho",
            "Paul Van Dooren",
            "Vincent D. Blondel"
        ],
        "summary": "In this paper, we present several descent methods that can be applied to nonnegative matrix factorization and we analyze a recently developped fast block coordinate method called Rank-one Residue Iteration (RRI). We also give a comparison of these different methods and show that the new block coordinate method has better properties in terms of approximation error and complexity. By interpreting this method as a rank-one approximation of the residue matrix, we prove that it \\emph{converges} and also extend it to the nonnegative tensor factorization and introduce some variants of the method by imposing some additional controllable constraints such as: sparsity, discreteness and smoothness.",
        "published": "2008-01-21T15:46:43Z",
        "link": "http://arxiv.org/abs/0801.3199v3",
        "categories": [
            "cs.NA",
            "cs.IR",
            "math.OC"
        ]
    },
    {
        "title": "Encoding changing country codes for the Semantic Web with ISO 3166 and   SKOS",
        "authors": [
            "Jakob Voss"
        ],
        "summary": "This paper shows how authority files can be encoded for the Semantic Web with the Simple Knowledge Organisation System (SKOS). In particular the application of SKOS for encoding the structure, management, and utilization of country codes as defined in ISO 3166 is demonstrated. The proposed encoding gives a use case for SKOS that includes features that have only been discussed little so far, such as multiple notations, nested concept schemes, changes by versioning.",
        "published": "2008-01-25T10:40:27Z",
        "link": "http://arxiv.org/abs/0801.3908v1",
        "categories": [
            "cs.IR",
            "H.3.1"
        ]
    },
    {
        "title": "On quantum statistics in data analysis",
        "authors": [
            "Dusko Pavlovic"
        ],
        "summary": "Originally, quantum probability theory was developed to analyze statistical phenomena in quantum systems, where classical probability theory does not apply, because the lattice of measurable sets is not necessarily distributive. On the other hand, it is well known that the lattices of concepts, that arise in data analysis, are in general also non-distributive, albeit for completely different reasons. In his recent book, van Rijsbergen argues that many of the logical tools developed for quantum systems are also suitable for applications in information retrieval. I explore the mathematical support for this idea on an abstract vector space model, covering several forms of data analysis (information retrieval, data mining, collaborative filtering, formal concept analysis...), and roughly based on an idea from categorical quantum mechanics. It turns out that quantum (i.e., noncommutative) probability distributions arise already in this rudimentary mathematical framework. We show that a Bell-type inequality must be satisfied by the standard similarity measures, if they are used for preference predictions. The fact that already a very general, abstract version of the vector space model yields simple counterexamples for such inequalities seems to be an indicator of a genuine need for quantum statistics in data analysis.",
        "published": "2008-02-10T01:42:31Z",
        "link": "http://arxiv.org/abs/0802.1296v3",
        "categories": [
            "cs.IR",
            "math.CT",
            "quant-ph",
            "H.3.3"
        ]
    },
    {
        "title": "Network as a computer: ranking paths to find flows",
        "authors": [
            "Dusko Pavlovic"
        ],
        "summary": "We explore a simple mathematical model of network computation, based on Markov chains. Similar models apply to a broad range of computational phenomena, arising in networks of computers, as well as in genetic, and neural nets, in social networks, and so on. The main problem of interaction with such spontaneously evolving computational systems is that the data are not uniformly structured. An interesting approach is to try to extract the semantical content of the data from their distribution among the nodes. A concept is then identified by finding the community of nodes that share it. The task of data structuring is thus reduced to the task of finding the network communities, as groups of nodes that together perform some non-local data processing. Towards this goal, we extend the ranking methods from nodes to paths. This allows us to extract some information about the likely flow biases from the available static information about the network.",
        "published": "2008-02-10T05:33:37Z",
        "link": "http://arxiv.org/abs/0802.1306v1",
        "categories": [
            "cs.IR",
            "cs.AI",
            "math.CT",
            "H.3.3; I.2.4; I.2.6"
        ]
    },
    {
        "title": "Characterising through Erasing: A Theoretical Framework for Representing   Documents Inspired by Quantum Theory",
        "authors": [
            "Álvaro Francisco Huertas-Rosero",
            "Leif Azzopardi",
            "C. J. van Rijsbergen"
        ],
        "summary": "The problem of representing text documents within an Information Retrieval system is formulated as an analogy to the problem of representing the quantum states of a physical system. Lexical measurements of text are proposed as a way of representing documents which are akin to physical measurements on quantum states. Consequently, the representation of the text is only known after measurements have been made, and because the process of measuring may destroy parts of the text, the document is characterised through erasure. The mathematical foundations of such a quantum representation of text are provided in this position paper as a starting point for indexing and retrieval within a ``quantum like'' Information Retrieval system.",
        "published": "2008-02-12T23:43:20Z",
        "link": "http://arxiv.org/abs/0802.1738v2",
        "categories": [
            "cs.IR",
            "quant-ph",
            "H.3.1"
        ]
    },
    {
        "title": "Use of Rapid Probabilistic Argumentation for Ranking on Large Complex   Networks",
        "authors": [
            "Burak Cetin",
            "Haluk Bingol"
        ],
        "summary": "We introduce a family of novel ranking algorithms called ERank which run in linear/near linear time and build on explicitly modeling a network as uncertain evidence. The model uses Probabilistic Argumentation Systems (PAS) which are a combination of probability theory and propositional logic, and also a special case of Dempster-Shafer Theory of Evidence. ERank rapidly generates approximate results for the NP-complete problem involved enabling the use of the technique in large networks. We use a previously introduced PAS model for citation networks generalizing it for all networks. We propose a statistical test to be used for comparing the performances of different ranking algorithms based on a clustering validity test. Our experimentation using this test on a real-world network shows ERank to have the best performance in comparison to well-known algorithms including PageRank, closeness, and betweenness.",
        "published": "2008-02-22T11:49:16Z",
        "link": "http://arxiv.org/abs/0802.3293v1",
        "categories": [
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Time Warp Edit Distance",
        "authors": [
            "Pierre-François Marteau"
        ],
        "summary": "This technical report details a family of time warp distances on the set of discrete time series. This family is constructed as an editing distance whose elementary operations apply on linear segments. A specific parameter allows controlling the stiffness of the elastic matching. It is well suited for the processing of event data for which each data sample is associated with a timestamp, not necessarily obtained according to a constant sampling rate. Some properties verified by these distances are proposed and proved in this report.",
        "published": "2008-02-24T17:18:50Z",
        "link": "http://arxiv.org/abs/0802.3522v5",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Information Hiding Techniques: A Tutorial Review",
        "authors": [
            "Sabu M. Thampi"
        ],
        "summary": "The purpose of this tutorial is to present an overview of various information hiding techniques. A brief history of steganography is provided along with techniques that were used to hide information. Text, image and audio based information hiding techniques are discussed. This paper also provides a basic introduction to digital watermarking.",
        "published": "2008-02-26T05:22:30Z",
        "link": "http://arxiv.org/abs/0802.3746v1",
        "categories": [
            "cs.CR",
            "cs.IR"
        ]
    },
    {
        "title": "Mobile Agents for Content-Based WWW Distributed Image Retrieval",
        "authors": [
            "Sabu M. Thampi",
            "K. Chandra Sekaran"
        ],
        "summary": "At present, the de-facto standard for providing contents in the Internet is the World Wide Web. A technology, which is now emerging on the Web, is Content-Based Image Retrieval (CBIR). CBIR applies methods and algorithms from computer science to analyse and index images based on their visual content. Mobile agents push the flexibility of distributed systems to their limits since not only computations are dynamically distributed but also the code that performs them. The current commercial applet-based methodologies for accessing image database systems offer limited flexibility, scalability and robustness. In this paper the author proposes a new framework for content-based WWW distributed image retrieval based on Java-based mobile agents. The implementation of the framework shows that its performance is comparable to, and in some cases outperforms, the current approach.",
        "published": "2008-03-01T08:27:59Z",
        "link": "http://arxiv.org/abs/0803.0053v1",
        "categories": [
            "cs.DC",
            "cs.IR"
        ]
    },
    {
        "title": "Multi-dimensional sparse time series: feature extraction",
        "authors": [
            "Marco Franciosi",
            "Giulia Menconi"
        ],
        "summary": "We show an analysis of multi-dimensional time series via entropy and statistical linguistic techniques. We define three markers encoding the behavior of the series, after it has been translated into a multi-dimensional symbolic sequence. The leading component and the trend of the series with respect to a mobile window analysis result from the entropy analysis and label the dynamical evolution of the series. The diversification formalizes the differentiation in the use of recurrent patterns, from a Zipf law point of view. These markers are the starting point of further analysis such as classification or clustering of large database of multi-dimensional time series, prediction of future behavior and attribution of new data. We also present an application to economic data. We deal with measurements of money investments of some business companies in advertising market for different media sources.",
        "published": "2008-03-04T10:27:59Z",
        "link": "http://arxiv.org/abs/0803.0405v1",
        "categories": [
            "cs.MM",
            "cs.IR"
        ]
    },
    {
        "title": "Website Optimization through Mining User Navigational Pattern",
        "authors": [
            "Biswajit Biswal"
        ],
        "summary": "With the World Wide Web's ubiquity increase and the rapid development of various online businesses, the complexity of web sites grow. The analysis of web user's navigational pattern within a web site can provide useful information for server performance enhancements, restructuring a website and direct marketing in e-commerce etc. In this paper, an algorithm is proposed for mining such navigation patterns. The key insight is that users access information of interest and follow a certain path while navigating a web site. If they don't find it, they would backtrack and choose among the alternate paths till they reach the destination. The point they backtrack is the Intermediate Reference Location. Identifying such Intermediate locations and destinations out of the pattern will be the main endeavor in the rest of this report.",
        "published": "2008-03-06T10:01:08Z",
        "link": "http://arxiv.org/abs/0803.0822v1",
        "categories": [
            "cs.IR",
            "H.3.4"
        ]
    },
    {
        "title": "Citation Counting, Citation Ranking, and h-Index of Human-Computer   Interaction Researchers: A Comparison between Scopus and Web of Science",
        "authors": [
            "Lokman I. Meho",
            "Yvonne Rogers"
        ],
        "summary": "This study examines the differences between Scopus and Web of Science in the citation counting, citation ranking, and h-index of 22 top human-computer interaction (HCI) researchers from EQUATOR--a large British Interdisciplinary Research Collaboration project. Results indicate that Scopus provides significantly more coverage of HCI literature than Web of Science, primarily due to coverage of relevant ACM and IEEE peer-reviewed conference proceedings. No significant differences exist between the two databases if citations in journals only are compared. Although broader coverage of the literature does not significantly alter the relative citation ranking of individual researchers, Scopus helps distinguish between the researchers in a more nuanced fashion than Web of Science in both citation counting and h-index. Scopus also generates significantly different maps of citation networks of individual scholars than those generated by Web of Science. The study also presents a comparison of h-index scores based on Google Scholar with those based on the union of Scopus and Web of Science. The study concludes that Scopus can be used as a sole data source for citation-based research and evaluation in HCI, especially if citations in conference proceedings are sought and that h scores should be manually calculated instead of relying on system calculations.",
        "published": "2008-03-12T08:09:19Z",
        "link": "http://arxiv.org/abs/0803.1716v1",
        "categories": [
            "cs.HC",
            "cs.IR"
        ]
    },
    {
        "title": "The Anatomy of Mitos Web Search Engine",
        "authors": [
            "Panagiotis Papadakos",
            "Giorgos Vasiliadis",
            "Yannis Theoharis",
            "Nikos Armenatzoglou",
            "Stella Kopidaki",
            "Yannis Marketakis",
            "Manos Daskalakis",
            "Kostas Karamaroudis",
            "Giorgos Linardakis",
            "Giannis Makrydakis",
            "Vangelis Papathanasiou",
            "Lefteris Sardis",
            "Petros Tsialiamanis",
            "Georgia Troullinou",
            "Kostas Vandikas",
            "Dimitris Velegrakis",
            "Yannis Tzitzikas"
        ],
        "summary": "Engineering a Web search engine offering effective and efficient information retrieval is a challenging task. This document presents our experiences from designing and developing a Web search engine offering a wide spectrum of functionalities and we report some interesting experimental results. A rather peculiar design choice of the engine is that its index is based on a DBMS, while some of the distinctive functionalities that are offered include advanced Greek language stemming, real time result clustering, and advanced link analysis techniques (also for spam page detection).",
        "published": "2008-03-14T19:18:15Z",
        "link": "http://arxiv.org/abs/0803.2220v2",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Succinct Data Structures for Retrieval and Approximate Membership",
        "authors": [
            "Martin Dietzfelbinger",
            "Rasmus Pagh"
        ],
        "summary": "The retrieval problem is the problem of associating data with keys in a set. Formally, the data structure must store a function f: U ->{0,1}^r that has specified values on the elements of a given set S, a subset of U, |S|=n, but may have any value on elements outside S. Minimal perfect hashing makes it possible to avoid storing the set S, but this induces a space overhead of Theta(n) bits in addition to the nr bits needed for function values. In this paper we show how to eliminate this overhead. Moreover, we show that for any k query time O(k) can be achieved using space that is within a factor 1+e^{-k} of optimal, asymptotically for large n. If we allow logarithmic evaluation time, the additive overhead can be reduced to O(log log n) bits whp. The time to construct the data structure is O(n), expected. A main technical ingredient is to utilize existing tight bounds on the probability of almost square random matrices with rows of low weight to have full row rank. In addition to direct constructions, we point out a close connection between retrieval structures and hash tables where keys are stored in an array and some kind of probing scheme is used. Further, we propose a general reduction that transfers the results on retrieval into analogous results on approximate membership, a problem traditionally addressed using Bloom filters. Again, we show how to eliminate the space overhead present in previously known methods, and get arbitrarily close to the lower bound. The evaluation procedures of our data structures are extremely simple (similar to a Bloom filter). For the results stated above we assume free access to fully random hash functions. However, we show how to justify this assumption using extra space o(n) to simulate full randomness on a RAM.",
        "published": "2008-03-26T10:53:49Z",
        "link": "http://arxiv.org/abs/0803.3693v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "Parts-of-Speech Tagger Errors Do Not Necessarily Degrade Accuracy in   Extracting Information from Biomedical Text",
        "authors": [
            "Maurice HT Ling",
            "Christophe Lefevre",
            "Kevin R. Nicholas"
        ],
        "summary": "A recent study reported development of Muscorian, a generic text processing tool for extracting protein-protein interactions from text that achieved comparable performance to biomedical-specific text processing tools. This result was unexpected since potential errors from a series of text analysis processes is likely to adversely affect the outcome of the entire process. Most biomedical entity relationship extraction tools have used biomedical-specific parts-of-speech (POS) tagger as errors in POS tagging and are likely to affect subsequent semantic analysis of the text, such as shallow parsing. This study aims to evaluate the parts-of-speech (POS) tagging accuracy and attempts to explore whether a comparable performance is obtained when a generic POS tagger, MontyTagger, was used in place of MedPost, a tagger trained in biomedical text. Our results demonstrated that MontyTagger, Muscorian's POS tagger, has a POS tagging accuracy of 83.1% when tested on biomedical text. Replacing MontyTagger with MedPost did not result in a significant improvement in entity relationship extraction from text; precision of 55.6% from MontyTagger versus 56.8% from MedPost on directional relationships and 86.1% from MontyTagger compared to 81.8% from MedPost on nondirectional relationships. This is unexpected as the potential for poor POS tagging by MontyTagger is likely to affect the outcome of the information extraction. An analysis of POS tagging errors demonstrated that 78.5% of tagging errors are being compensated by shallow parsing. Thus, despite 83.1% tagging accuracy, MontyTagger has a functional tagging accuracy of 94.6%.",
        "published": "2008-04-02T09:34:13Z",
        "link": "http://arxiv.org/abs/0804.0317v1",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "Comparing and Combining Methods for Automatic Query Expansion",
        "authors": [
            "José R. Pérez-Agüera",
            "Lourdes Araujo"
        ],
        "summary": "Query expansion is a well known method to improve the performance of information retrieval systems. In this work we have tested different approaches to extract the candidate query terms from the top ranked documents returned by the first-pass retrieval.   One of them is the cooccurrence approach, based on measures of cooccurrence of the candidate and the query terms in the retrieved documents. The other one, the probabilistic approach, is based on the probability distribution of terms in the collection and in the top ranked set.   We compare the retrieval improvement achieved by expanding the query with terms obtained with different methods belonging to both approaches. Besides, we have developed a na\\\"ive combination of both kinds of method, with which we have obtained results that improve those obtained with any of them separately. This result confirms that the information provided by each approach is of a different nature and, therefore, can be used in a combined manner.",
        "published": "2008-04-13T11:38:28Z",
        "link": "http://arxiv.org/abs/0804.2057v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Information filtering based on wiki index database",
        "authors": [
            "A. V. Smirnov",
            "A. A. Krizhanovsky"
        ],
        "summary": "In this paper we present a profile-based approach to information filtering by an analysis of the content of text documents. The Wikipedia index database is created and used to automatically generate the user profile from the user document collection. The problem-oriented Wikipedia subcorpora are created (using knowledge extracted from the user profile) for each topic of user interests. The index databases of these subcorpora are applied to filtering information flow (e.g., mails, news). Thus, the analyzed texts are classified into several topics explicitly presented in the user profile. The paper concentrates on the indexing part of the approach. The architecture of an application implementing the Wikipedia indexing is described. The indexing method is evaluated using the Russian and Simple English Wikipedia.",
        "published": "2008-04-15T11:05:59Z",
        "link": "http://arxiv.org/abs/0804.2354v2",
        "categories": [
            "cs.IR",
            "cs.CL",
            "I.7.2; I.7.3; I.7.5; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Respect My Authority! HITS Without Hyperlinks, Utilizing Cluster-Based   Language Models",
        "authors": [
            "Oren Kurland",
            "Lillian Lee"
        ],
        "summary": "We present an approach to improving the precision of an initial document ranking wherein we utilize cluster information within a graph-based framework. The main idea is to perform re-ranking based on centrality within bipartite graphs of documents (on one side) and clusters (on the other side), on the premise that these are mutually reinforcing entities. Links between entities are created via consideration of language models induced from them.   We find that our cluster-document graphs give rise to much better retrieval performance than previously proposed document-only graphs do. For example, authority-based re-ranking of documents via a HITS-style cluster-based approach outperforms a previously-proposed PageRank-inspired algorithm applied to solely-document graphs. Moreover, we also show that computing authority scores for clusters constitutes an effective method for identifying clusters containing a large percentage of relevant documents.",
        "published": "2008-04-22T20:02:14Z",
        "link": "http://arxiv.org/abs/0804.3599v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Constructions for Clumps Statistics",
        "authors": [
            "Frederique Bassino",
            "Julien Clement",
            "Julien Fayolle",
            "Pierre Nicodeme"
        ],
        "summary": "We consider a component of the word statistics known as clump; starting from a finite set of words, clumps are maximal overlapping sets of these occurrences. This parameter has first been studied by Schbath with the aim of counting the number of occurrences of words in random texts. Later work with similar probabilistic approach used the Chen-Stein approximation for a compound Poisson distribution, where the number of clumps follows a law close to Poisson. Presently there is no combinatorial counterpart to this approach, and we fill the gap here. We emphasize the fact that, in contrast with the probabilistic approach which only provides asymptotic results, the combinatorial approach provides exact results that are useful when considering short sequences.",
        "published": "2008-04-23T10:38:15Z",
        "link": "http://arxiv.org/abs/0804.3671v1",
        "categories": [
            "cs.DM",
            "cs.IR"
        ]
    },
    {
        "title": "An Algorigtm for Singular Value Decomposition of Matrices in Blocks",
        "authors": [
            "Alvaro Francisco Huertas-Rosero"
        ],
        "summary": "Two methods to decompose block matrices analogous to Singular Matrix Decomposition are proposed, one yielding the so called economy decomposition, and other yielding the full decomposition. This method is devised to avoid handling matrices bigger than the biggest blocks, so it is particularly appropriate when a limitation on the size of matrices exists. The method is tested on a document-term matrix (17780x3204) divided in 4 blocks, the upper-left corner being 215x215.",
        "published": "2008-04-27T23:29:07Z",
        "link": "http://arxiv.org/abs/0804.4305v2",
        "categories": [
            "math.NA",
            "cs.IR",
            "math.AC",
            "15A18"
        ]
    },
    {
        "title": "Dependence Structure Estimation via Copula",
        "authors": [
            "Jian Ma",
            "Zengqi Sun"
        ],
        "summary": "Dependence strucuture estimation is one of the important problems in machine learning domain and has many applications in different scientific areas. In this paper, a theoretical framework for such estimation based on copula and copula entropy -- the probabilistic theory of representation and measurement of statistical dependence, is proposed. Graphical models are considered as a special case of the copula framework. A method of the framework for estimating maximum spanning copula is proposed. Due to copula, the method is irrelevant to the properties of individual variables, insensitive to outlier and able to deal with non-Gaussianity. Experiments on both simulated data and real dataset demonstrated the effectiveness of the proposed method.",
        "published": "2008-04-28T17:14:53Z",
        "link": "http://arxiv.org/abs/0804.4451v2",
        "categories": [
            "cs.LG",
            "cs.IR",
            "stat.ME"
        ]
    },
    {
        "title": "Nonnegative Matrix Factorization via Rank-One Downdate",
        "authors": [
            "Michael Biggs",
            "Ali Ghodsi",
            "Stephen Vavasis"
        ],
        "summary": "Nonnegative matrix factorization (NMF) was popularized as a tool for data mining by Lee and Seung in 1999. NMF attempts to approximate a matrix with nonnegative entries by a product of two low-rank matrices, also with nonnegative entries. We propose an algorithm called rank-one downdate (R1D) for computing a NMF that is partly motivated by singular value decomposition. This algorithm computes the dominant singular values and vectors of adaptively determined submatrices of a matrix. On each iteration, R1D extracts a rank-one submatrix from the dataset according to an objective function. We establish a theoretical result that maximizing this objective function corresponds to correctly classifying articles in a nearly separable corpus. We also provide computational experiments showing the success of this method in identifying features in realistic datasets.",
        "published": "2008-05-01T17:59:44Z",
        "link": "http://arxiv.org/abs/0805.0120v1",
        "categories": [
            "cs.IR",
            "cs.NA",
            "I.2.7; H.3.3; G.1.3"
        ]
    },
    {
        "title": "Semantic Analysis of Tag Similarity Measures in Collaborative Tagging   Systems",
        "authors": [
            "Ciro Cattuto",
            "Dominik Benz",
            "Andreas Hotho",
            "Gerd Stumme"
        ],
        "summary": "Social bookmarking systems allow users to organise collections of resources on the Web in a collaborative fashion. The increasing popularity of these systems as well as first insights into their emergent semantics have made them relevant to disciplines like knowledge extraction and ontology learning. The problem of devising methods to measure the semantic relatedness between tags and characterizing it semantically is still largely open. Here we analyze three measures of tag relatedness: tag co-occurrence, cosine similarity of co-occurrence distributions, and FolkRank, an adaptation of the PageRank algorithm to folksonomies. Each measure is computed on tags from a large-scale dataset crawled from the social bookmarking system del.icio.us. To provide a semantic grounding of our findings, a connection to WordNet (a semantic lexicon for the English language) is established by mapping tags into synonym sets of WordNet, and applying there well-known metrics of semantic similarity. Our results clearly expose different characteristics of the selected measures of relatedness, making them applicable to different subtasks of knowledge extraction such as synonym detection or discovery of concept hierarchies.",
        "published": "2008-05-14T14:10:02Z",
        "link": "http://arxiv.org/abs/0805.2045v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.5; G.2.2; H.1.2; H.1.m; H.5.3"
        ]
    },
    {
        "title": "LCSH, SKOS and Linked Data",
        "authors": [
            "Ed Summers",
            "Antoine Isaac",
            "Clay Redding",
            "Dan Krech"
        ],
        "summary": "A technique for converting Library of Congress Subject Headings MARCXML to Simple Knowledge Organization System (SKOS) RDF is described. Strengths of the SKOS vocabulary are highlighted, as well as possible points for extension, and the integration of other semantic web vocabularies such as Dublin Core. An application for making the vocabulary available as linked-data on the Web is also described.",
        "published": "2008-05-19T13:11:41Z",
        "link": "http://arxiv.org/abs/0805.2855v3",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.7"
        ]
    },
    {
        "title": "Modeling Loosely Annotated Images with Imagined Annotations",
        "authors": [
            "Hong Tang",
            "Nozha Boujemma",
            "Yunhao Chen"
        ],
        "summary": "In this paper, we present an approach to learning latent semantic analysis models from loosely annotated images for automatic image annotation and indexing. The given annotation in training images is loose due to: (1) ambiguous correspondences between visual features and annotated keywords; (2) incomplete lists of annotated keywords. The second reason motivates us to enrich the incomplete annotation in a simple way before learning topic models. In particular, some imagined keywords are poured into the incomplete annotation through measuring similarity between keywords. Then, both given and imagined annotations are used to learning probabilistic topic models for automatically annotating new images. We conduct experiments on a typical Corel dataset of images and loose annotations, and compare the proposed method with state-of-the-art discrete annotation methods (using a set of discrete blobs to represent an image). The proposed method improves word-driven probability Latent Semantic Analysis (PLSA-words) up to a comparable performance with the best discrete annotation method, while a merit of PLSA-words is still kept, i.e., a wider semantic range.",
        "published": "2008-05-29T10:35:29Z",
        "link": "http://arxiv.org/abs/0805.4508v1",
        "categories": [
            "cs.IR",
            "cs.AI",
            "H.3.3"
        ]
    },
    {
        "title": "La fiabilité des informations sur le web",
        "authors": [
            "Bernard Jacquemin",
            "Aurélien Lauf",
            "Céline Poudat",
            "Martine Hurault-Plantet",
            "Nicolas Auray"
        ],
        "summary": "Online IR tools have to take into account new phenomena linked to the appearance of blogs, wiki and other collaborative publications. Among these collaborative sites, Wikipedia represents a crucial source of information. However, the quality of this information has been recently questionned. A better knowledge of the contributors' behaviors should help users navigate through information whose quality may vary from one source to another. In order to explore this idea, we present an analysis of the role of different types of contributors in the control of the publication of conflictual articles.",
        "published": "2008-05-30T10:45:42Z",
        "link": "http://arxiv.org/abs/0805.4722v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "cs.CY"
        ]
    },
    {
        "title": "Managing conflicts between users in Wikipedia",
        "authors": [
            "Bernard Jacquemin",
            "Aurélien Lauf",
            "Céline Poudat",
            "Martine Hurault-Plantet",
            "Nicolas Auray"
        ],
        "summary": "Wikipedia is nowadays a widely used encyclopedia, and one of the most visible sites on the Internet. Its strong principle of collaborative work and free editing sometimes generates disputes due to disagreements between users. In this article we study how the wikipedian community resolves the conflicts and which roles do wikipedian choose in this process. We observed the users behavior both in the article talk pages, and in the Arbitration Committee pages specifically dedicated to serious disputes. We first set up a users typology according to their involvement in conflicts and their publishing and management activity in the encyclopedia. We then used those user types to describe users behavior in contributing to articles that are tagged by the wikipedian community as being in conflict with the official guidelines of Wikipedia, or conversely as being well featured.",
        "published": "2008-05-30T13:20:42Z",
        "link": "http://arxiv.org/abs/0805.4754v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "Analysis of Social Voting Patterns on Digg",
        "authors": [
            "Kristina Lerman",
            "Aram Galstyan"
        ],
        "summary": "The social Web is transforming the way information is created and distributed. Blog authoring tools enable users to publish content, while sites such as Digg and Del.icio.us are used to distribute content to a wider audience. With content fast becoming a commodity, interest in using social networks to promote and find content has grown, both on the side of content producers (viral marketing) and consumers (recommendation). Here we study the role of social networks in promoting content on Digg, a social news aggregator that allows users to submit links to and vote on news stories. Digg's goal is to feature the most interesting stories on its front page, and it aggregates opinions of its many users to identify them. Like other social networking sites, Digg allows users to designate other users as ``friends'' and see what stories they found interesting. We studied the spread of interest in news stories submitted to Digg in June 2006. Our results suggest that pattern of the spread of interest in a story on the network is indicative of how popular the story will become. Stories that spread mainly outside of the submitter's neighborhood go on to be very popular, while stories that spread mainly through submitter's social neighborhood prove not to be very popular. This effect is visible already in the early stages of voting, and one can make a prediction about the potential audience of a story simply by analyzing where the initial votes come from.",
        "published": "2008-06-11T17:53:45Z",
        "link": "http://arxiv.org/abs/0806.1918v1",
        "categories": [
            "cs.CY",
            "cs.IR"
        ]
    },
    {
        "title": "Cross-concordances: terminology mapping and its effectiveness for   information retrieval",
        "authors": [
            "Philipp Mayr",
            "Vivien Petras"
        ],
        "summary": "The German Federal Ministry for Education and Research funded a major terminology mapping initiative, which found its conclusion in 2007. The task of this terminology mapping initiative was to organize, create and manage 'cross-concordances' between controlled vocabularies (thesauri, classification systems, subject heading lists) centred around the social sciences but quickly extending to other subject areas. 64 crosswalks with more than 500,000 relations were established. In the final phase of the project, a major evaluation effort to test and measure the effectiveness of the vocabulary mappings in an information system environment was conducted. The paper reports on the cross-concordance work and evaluation results.",
        "published": "2008-06-23T20:37:10Z",
        "link": "http://arxiv.org/abs/0806.3765v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Interprétation vague des contraintes structurelles pour la RI dans des   corpus de documents XML - Évaluation d'une méthode approchée de RI   structurée",
        "authors": [
            "Eugen Popovici",
            "Gilbas Ménier",
            "Pierre-François Marteau"
        ],
        "summary": "We propose specific data structures designed to the indexing and retrieval of information elements in heterogeneous XML data bases. The indexing scheme is well suited to the management of various contextual searches, expressed either at a structural level or at an information content level. The approximate search mechanisms are based on a modified Levenshtein editing distance and information fusion heuristics. The implementation described highlights the mixing of structured information presented as field/value instances and free text elements. The retrieval performances of the proposed approach are evaluated within the INEX 2005 evaluation campaign. The evaluation results rank the proposed approach among the best evaluated XML IR systems for the VVCAS task.",
        "published": "2008-06-30T15:25:30Z",
        "link": "http://arxiv.org/abs/0806.4921v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Automatic Metadata Generation using Associative Networks",
        "authors": [
            "Marko A. Rodriguez",
            "Johan Bollen",
            "Herbert Van de Sompel"
        ],
        "summary": "In spite of its tremendous value, metadata is generally sparse and incomplete, thereby hampering the effectiveness of digital information services. Many of the existing mechanisms for the automated creation of metadata rely primarily on content analysis which can be costly and inefficient. The automatic metadata generation system proposed in this article leverages resource relationships generated from existing metadata as a medium for propagation from metadata-rich to metadata-poor resources. Because of its independence from content analysis, it can be applied to a wide variety of resource media types and is shown to be computationally inexpensive. The proposed method operates through two distinct phases. Occurrence and co-occurrence algorithms first generate an associative network of repository resources leveraging existing repository metadata. Second, using the associative network as a substrate, metadata associated with metadata-rich resources is propagated to metadata-poor resources by means of a discrete-form spreading activation algorithm. This article discusses the general framework for building associative networks, an algorithm for disseminating metadata through such networks, and the results of an experiment and validation of the proposed method using a standard bibliographic dataset.",
        "published": "2008-06-30T21:23:28Z",
        "link": "http://arxiv.org/abs/0807.0023v2",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.1; H.3.7"
        ]
    },
    {
        "title": "Quantitative Paradigm of Software Reliability as Content Relevance",
        "authors": [
            "Yuri Arkhipkin"
        ],
        "summary": "This paper presents a quantitative approach to software reliability and content relevance definitions validated by the systems' potential reliability law.Thus it is argued for the unified math nature or quantitative paradigm of software reliability and content relevance.",
        "published": "2008-07-01T05:29:07Z",
        "link": "http://arxiv.org/abs/0807.0070v1",
        "categories": [
            "cs.SE",
            "cs.IR",
            "D.2.5; D.2.8; D.2.9; H.3.1; D.1.m"
        ]
    },
    {
        "title": "Unveiling the mystery of visual information processing in human brain",
        "authors": [
            "Emanuel Diamant"
        ],
        "summary": "It is generally accepted that human vision is an extremely powerful information processing system that facilitates our interaction with the surrounding world. However, despite extended and extensive research efforts, which encompass many exploration fields, the underlying fundamentals and operational principles of visual information processing in human brain remain unknown. We still are unable to figure out where and how along the path from eyes to the cortex the sensory input perceived by the retina is converted into a meaningful object representation, which can be consciously manipulated by the brain. Studying the vast literature considering the various aspects of brain information processing, I was surprised to learn that the respected scholarly discussion is totally indifferent to the basic keynote question: \"What is information?\" in general or \"What is visual information?\" in particular. In the old days, it was assumed that any scientific research approach has first to define its basic departure points. Why was it overlooked in brain information processing research remains a conundrum. In this paper, I am trying to find a remedy for this bizarre situation. I propose an uncommon definition of \"information\", which can be derived from Kolmogorov's Complexity Theory and Chaitin's notion of Algorithmic Information. Embracing this new definition leads to an inevitable revision of traditional dogmas that shape the state of the art of brain information processing research. I hope this revision would better serve the challenging goal of human visual information processing modeling.",
        "published": "2008-07-02T12:33:48Z",
        "link": "http://arxiv.org/abs/0807.0337v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "cs.IT",
            "math.IT",
            "q-bio.NC"
        ]
    },
    {
        "title": "Scientific Paper Summarization Using Citation Summary Networks",
        "authors": [
            "Vahed Qazvinian",
            "Dragomir R. Radev"
        ],
        "summary": "Quickly moving to a new area of research is painful for researchers due to the vast amount of scientific literature in each field of study. One possible way to overcome this problem is to summarize a scientific topic. In this paper, we propose a model of summarizing a single article, which can be further used to summarize an entire topic. Our model is based on analyzing others' viewpoint of the target article's contributions and the study of its citation summary network using a clustering approach.",
        "published": "2008-07-10T00:01:20Z",
        "link": "http://arxiv.org/abs/0807.1560v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3; H.3.1; I.2.7; G.2.2"
        ]
    },
    {
        "title": "Hybrid Keyword Search Auctions",
        "authors": [
            "Ashish Goel",
            "Kamesh Munagala"
        ],
        "summary": "Search auctions have become a dominant source of revenue generation on the Internet. Such auctions have typically used per-click bidding and pricing. We propose the use of hybrid auctions where an advertiser can make a per-impression as well as a per-click bid, and the auctioneer then chooses one of the two as the pricing mechanism. We assume that the advertiser and the auctioneer both have separate beliefs (called priors) on the click-probability of an advertisement. We first prove that the hybrid auction is truthful, assuming that the advertisers are risk-neutral. We then show that this auction is superior to the existing per-click auction in multiple ways: 1) It takes into account the risk characteristics of the advertisers. 2) For obscure keywords, the auctioneer is unlikely to have a very sharp prior on the click-probabilities. In such situations, the hybrid auction can result in significantly higher revenue. 3) An advertiser who believes that its click-probability is much higher than the auctioneer's estimate can use per-impression bids to correct the auctioneer's prior without incurring any extra cost. 4) The hybrid auction can allow the advertiser and auctioneer to implement complex dynamic programming strategies. As Internet commerce matures, we need more sophisticated pricing models to exploit all the information held by each of the participants. We believe that hybrid auctions could be an important step in this direction.",
        "published": "2008-07-16T04:04:32Z",
        "link": "http://arxiv.org/abs/0807.2496v2",
        "categories": [
            "cs.GT",
            "cs.DS",
            "cs.IR"
        ]
    },
    {
        "title": "Text Data Mining: Theory and Methods",
        "authors": [
            "Jeffrey Solka"
        ],
        "summary": "This paper provides the reader with a very brief introduction to some of the theory and methods of text data mining. The intent of this article is to introduce the reader to some of the current methodologies that are employed within this discipline area while at the same time making the reader aware of some of the interesting challenges that remain to be solved within the area. Finally, the articles serves as a very rudimentary tutorial on some of techniques while also providing the reader with a list of references for additional study.",
        "published": "2008-07-16T13:39:32Z",
        "link": "http://arxiv.org/abs/0807.2569v1",
        "categories": [
            "stat.ML",
            "cs.IR",
            "stat.CO",
            "62-01 (Primary) 62A01 (Secondary)"
        ]
    },
    {
        "title": "The rank convergence of HITS can be slow",
        "authors": [
            "Enoch Peserico",
            "Luca Pretto"
        ],
        "summary": "We prove that HITS, to \"get right\" h of the top k ranked nodes of an N>=2k node graph, can require h^(Omega(N h/k)) iterations (i.e. a substantial Omega(N h log(h)/k) matrix multiplications even with a \"squaring trick\"). Our proof requires no algebraic tools and is entirely self-contained.",
        "published": "2008-07-18T16:42:57Z",
        "link": "http://arxiv.org/abs/0807.3006v1",
        "categories": [
            "cs.DS",
            "cs.IR",
            "F.2.2; H.3.3"
        ]
    },
    {
        "title": "Approximating Document Frequency with Term Count Values",
        "authors": [
            "Martin Klein",
            "Michael L. Nelson"
        ],
        "summary": "For bounded datasets such as the TREC Web Track (WT10g) the computation of term frequency (TF) and inverse document frequency (IDF) is not difficult. However, when the corpus is the entire web, direct IDF calculation is impossible and values must instead be estimated. Most available datasets provide values for term count (TC) meaning the number of times a certain term occurs in the entire corpus. Intuitively this value is different from document frequency (DF), the number of documents (e.g., web pages) a certain term occurs in. We conduct a comparison study between TC and DF values within the Web as Corpus (WaC). We found a very strong correlation with Spearman's rho >0.8 (p<0.005) which makes us confident in claiming that for such recently created corpora the TC and DF values can be used interchangeably to compute IDF values. These results are useful for the generation of accurate lexical signatures based on the TF-IDF scheme.",
        "published": "2008-07-23T21:44:46Z",
        "link": "http://arxiv.org/abs/0807.3755v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.0"
        ]
    },
    {
        "title": "Schroedinger-like PageRank equation and localization in the WWW",
        "authors": [
            "Nicola Perra",
            "Vinko Zlatic",
            "Alessandro Chessa",
            "Claudio Conti",
            "Debora Donato",
            "Guido Caldarelli"
        ],
        "summary": "The WorldWide Web is one of the most important communication systems we use in our everyday life. Despite its central role, the growth and the development of the WWW is not controlled by any central authority. This situation has created a huge ensemble of connections whose complexity can be fruitfully described and quantified by network theory. One important application that allows to sort out the information present in these connections is given by the PageRank alghorithm. Computation of this quantity is usually made iteratively with a large use of computational time. In this paper we show that the PageRank can be expressed in terms of a wave function obeying a Schroedinger-like equation. In particular the topological disorder given by the unbalance of outgoing and ingoing links between pages, induces wave function and potential structuring. This allows to directly localize the pages with the largest score. Through this new representation we can now compute the PageRank without iterative techniques. For most of the cases of interest our method is faster than the original one. Our results also clarify the role of topology in the diffusion of information within complex networks. The whole approach opens the possibility to novel techniques inspired by quantum physics for the analysis of the WWW properties.",
        "published": "2008-07-28T18:31:17Z",
        "link": "http://arxiv.org/abs/0807.4325v1",
        "categories": [
            "physics.soc-ph",
            "cond-mat.stat-mech",
            "cs.IR",
            "physics.data-an"
        ]
    },
    {
        "title": "I'm sorry to say, but your understanding of image processing   fundamentals is absolutely wrong",
        "authors": [
            "Emanuel Diamant"
        ],
        "summary": "The ongoing discussion whether modern vision systems have to be viewed as visually-enabled cognitive systems or cognitively-enabled vision systems is groundless, because perceptual and cognitive faculties of vision are separate components of human (and consequently, artificial) information processing system modeling.",
        "published": "2008-08-01T04:45:17Z",
        "link": "http://arxiv.org/abs/0808.0056v1",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.IR",
            "cs.RO",
            "q-bio.NC"
        ]
    },
    {
        "title": "Text Modeling using Unsupervised Topic Models and Concept Hierarchies",
        "authors": [
            "Chaitanya Chemudugunta",
            "Padhraic Smyth",
            "Mark Steyvers"
        ],
        "summary": "Statistical topic models provide a general data-driven framework for automated discovery of high-level knowledge from large collections of text documents. While topic models can potentially discover a broad range of themes in a data set, the interpretability of the learned topics is not always ideal. Human-defined concepts, on the other hand, tend to be semantically richer due to careful selection of words to define concepts but they tend not to cover the themes in a data set exhaustively. In this paper, we propose a probabilistic framework to combine a hierarchy of human-defined semantic concepts with statistical topic models to seek the best of both worlds. Experimental results using two different sources of concept hierarchies and two collections of text documents indicate that this combination leads to systematic improvements in the quality of the associated language models as well as enabling new techniques for inferring and visualizing the semantics of a document.",
        "published": "2008-08-07T07:59:29Z",
        "link": "http://arxiv.org/abs/0808.0973v1",
        "categories": [
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Index wiki database: design and experiments",
        "authors": [
            "A. A. Krizhanovsky"
        ],
        "summary": "With the fantastic growth of Internet usage, information search in documents of a special type called a \"wiki page\" that is written using a simple markup language, has become an important problem. This paper describes the software architectural model for indexing wiki texts in three languages (Russian, English, and German) and the interaction between the software components (GATE, Lemmatizer, and Synarcher). The inverted file index database was designed using visual tool DBDesigner. The rules for parsing Wikipedia texts are illustrated by examples. Two index databases of Russian Wikipedia (RW) and Simple English Wikipedia (SEW) are built and compared. The size of RW is by order of magnitude higher than SEW (number of words, lexemes), though the growth rate of number of pages in SEW was found to be 14% higher than in Russian, and the rate of acquisition of new words in SEW lexicon was 7% higher during a period of five months (from September 2007 to February 2008). The Zipf's law was tested with both Russian and Simple Wikipedias. The entire source code of the indexing software and the generated index databases are freely available under GPL (GNU General Public License).",
        "published": "2008-08-12T23:47:21Z",
        "link": "http://arxiv.org/abs/0808.1753v2",
        "categories": [
            "cs.IR",
            "cs.CL",
            "I.7.2; I.7.3; I.7.5; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Solving the apparent diversity-accuracy dilemma of recommender systems",
        "authors": [
            "Tao Zhou",
            "Zoltan Kuscsik",
            "Jian-Guo Liu",
            "Matus Medo",
            "Joseph R. Wakeling",
            "Yi-Cheng Zhang"
        ],
        "summary": "Recommender systems use data on past user preferences to predict possible future likes and interests. A key challenge is that while the most useful individual recommendations are to be found among diverse niche objects, the most reliably accurate results are obtained by methods that recommend objects based on user or object similarity. In this paper we introduce a new algorithm specifically to address the challenge of diversity and show how it can be used to resolve this apparent dilemma when combined in an elegant hybrid with an accuracy-focused algorithm. By tuning the hybrid appropriately we are able to obtain, without relying on any semantic or context-specific information, simultaneous gains in both accuracy and diversity of recommendations.",
        "published": "2008-08-19T23:17:40Z",
        "link": "http://arxiv.org/abs/0808.2670v3",
        "categories": [
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Highly accurate recommendation algorithm based on high-order   similarities",
        "authors": [
            "Jian-Guo Liu",
            "Tao Zhou",
            "Bing-Hong Wang",
            "Yi-Cheng Zhang"
        ],
        "summary": "In this Letter, we introduce a modified collaborative filtering (MCF) algorithm, which has remarkably higher accuracy than the standard collaborative filtering. In the MCF, instead of the standard Pearson coefficient, the user-user similarities are obtained by a diffusion process. Furthermore, by considering the second order similarities, we design an effective algorithm that depresses the influence of mainstream preferences. The corresponding algorithmic accuracy, measured by the ranking score, is further improved by 24.9% in the optimal case. In addition, two significant criteria of algorithmic performance, diversity and popularity, are also taken into account. Numerical results show that the algorithm based on second order similarity can outperform the MCF simultaneously in all three criteria.",
        "published": "2008-08-27T15:42:02Z",
        "link": "http://arxiv.org/abs/0808.3726v2",
        "categories": [
            "physics.data-an",
            "cs.IR"
        ]
    },
    {
        "title": "A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks have been treated independently, using a wide variety of algorithms. These four semantic classes, however, are a tiny sample of the full range of semantic phenomena, and we cannot afford to create ad hoc algorithms for each semantic phenomenon; we need to seek a unified approach. We propose to subsume a broad range of phenomena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonym questions, ESL synonym-antonym questions, and similar-associated-both questions from cognitive psychology.",
        "published": "2008-08-31T14:00:26Z",
        "link": "http://arxiv.org/abs/0809.0124v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "The Prolog Interface to the Unstructured Information Management   Architecture",
        "authors": [
            "Paul Fodor",
            "Adam Lally",
            "David Ferrucci"
        ],
        "summary": "In this paper we describe the design and implementation of the Prolog interface to the Unstructured Information Management Architecture (UIMA) and some of its applications in natural language processing. The UIMA Prolog interface translates unstructured data and the UIMA Common Analysis Structure (CAS) into a Prolog knowledge base, over which, the developers write rules and use resolution theorem proving to search and generate new annotations over the unstructured data. These rules can explore all the previous UIMA annotations (such as, the syntactic structure, parsing statistics) and external Prolog knowledge bases (such as, Prolog WordNet and Extended WordNet) to implement a variety of tasks for the natural language analysis. We also describe applications of this logic programming interface in question analysis (such as, focus detection, answer-type and other constraints detection), shallow parsing (such as, relations in the syntactic structure), and answer selection.",
        "published": "2008-09-03T17:38:32Z",
        "link": "http://arxiv.org/abs/0809.0680v1",
        "categories": [
            "cs.SE",
            "cs.IR"
        ]
    },
    {
        "title": "A Simple Mechanism for Focused Web-harvesting",
        "authors": [
            "Z. Akbar",
            "L. T. Handoko"
        ],
        "summary": "The focused web-harvesting is deployed to realize an automated and comprehensive index databases as an alternative way for virtual topical data integration. The web-harvesting has been implemented and extended by not only specifying the targeted URLs, but also predefining human-edited harvesting parameters to improve the speed and accuracy. The harvesting parameter set comprises three main components. First, the depth-scale of being harvested final pages containing desired information counted from the first page at the targeted URLs. Secondly, the focus-point number to determine the exact box containing relevant information. Lastly, the combination of keywords to recognize encountered hyperlinks of relevant images or full-texts embedded in those final pages. All parameters are accessible and fully customizable for each target by the administrators of participating institutions over an integrated web interface. A real implementation to the Indonesian Scientific Index which covers all scientific information across Indonesia is also briefly introduced.",
        "published": "2008-09-03T23:53:29Z",
        "link": "http://arxiv.org/abs/0809.0723v1",
        "categories": [
            "cs.IR",
            "cs.CY"
        ]
    },
    {
        "title": "Normalized Information Distance",
        "authors": [
            "Paul M. B. Vitanyi",
            "Frank J. Balbach",
            "Rudi L. Cilibrasi",
            "Ming Li"
        ],
        "summary": "The normalized information distance is a universal distance measure for objects of all kinds. It is based on Kolmogorov complexity and thus uncomputable, but there are ways to utilize it. First, compression algorithms can be used to approximate the Kolmogorov complexity if the objects have a string representation. Second, for names and abstract concepts, page count statistics from the World Wide Web can be used. These practical realizations of the normalized information distance can then be applied to machine learning tasks, expecially clustering, to perform feature-free and parameter-free data mining. This chapter discusses the theoretical foundations of the normalized information distance and both practical realizations. It presents numerous examples of successful real-world applications based on these distance measures, ranging from bioinformatics to music clustering to machine translation.",
        "published": "2008-09-15T15:33:11Z",
        "link": "http://arxiv.org/abs/0809.2553v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "An Exploratory Study of Calendar Use",
        "authors": [
            "Manas Tungare",
            "Manuel Perez-Quinones",
            "Alyssa Sams"
        ],
        "summary": "In this paper, we report on findings from an ethnographic study of how people use their calendars for personal information management (PIM). Our participants were faculty, staff and students who were not required to use or contribute to any specific calendaring solution, but chose to do so anyway. The study was conducted in three parts: first, an initial survey provided broad insights into how calendars were used; second, this was followed up with personal interviews of a few participants which were transcribed and content-analyzed; and third, examples of calendar artifacts were collected to inform our analysis. Findings from our study include the use of multiple reminder alarms, the reliance on paper calendars even among regular users of electronic calendars, and wide use of calendars for reporting and life-archival purposes. We conclude the paper with a discussion of what these imply for designers of interactive calendar systems and future work in PIM research.",
        "published": "2008-09-19T19:56:15Z",
        "link": "http://arxiv.org/abs/0809.3447v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.5.2"
        ]
    },
    {
        "title": "Mining Meaning from Wikipedia",
        "authors": [
            "Olena Medelyan",
            "David Milne",
            "Catherine Legg",
            "Ian H. Witten"
        ],
        "summary": "Wikipedia is a goldmine of information; not just for its many readers, but also for the growing community of researchers who recognize it as a resource of exceptional scale and utility. It represents a vast investment of manual effort and judgment: a huge, constantly evolving tapestry of concepts and relations that is being applied to a host of tasks.   This article provides a comprehensive description of this work. It focuses on research that extracts and makes use of the concepts, relations, facts and descriptions found in Wikipedia, and organizes the work into four broad categories: applying Wikipedia to natural language processing; using it to facilitate information retrieval and information extraction; and as a resource for ontology building. The article addresses how Wikipedia is being used as is, how it is being improved and adapted, and how it is being combined with other structures to create entirely new resources. We identify the research groups and individuals involved, and how their work has developed in the last few years. We provide a comprehensive list of the open-source software they have produced.",
        "published": "2008-09-26T04:47:19Z",
        "link": "http://arxiv.org/abs/0809.4530v2",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.IR",
            "I.2.6; I.2.7; H.3.1; H.3.3"
        ]
    },
    {
        "title": "Faceted Ranking of Egos in Collaborative Tagging Systems",
        "authors": [
            "Jose Ignacio Orlicki",
            "Pablo Ignacio Fierens",
            "José Ignacio Alvarez-Hamelin"
        ],
        "summary": "Multimedia uploaded content is tagged and recommended by users of collaborative systems, resulting in informal classifications also known as folksonomies. Faceted web ranking has been proved a reasonable alternative to a single ranking which does not take into account a personalized context. In this paper we analyze the online computation of rankings of users associated to facets made up of multiple tags. Possible applications are user reputation evaluation (ego-ranking) and improvement of content quality in case of retrieval. We propose a solution based on PageRank as centrality measure: (i) a ranking for each tag is computed offline on the basis of the corresponding tag-dependent subgraph; (ii) a faceted order is generated by merging rankings corresponding to all the tags in the facet. The fundamental assumption, validated by empirical observations, is that step (i) is scalable. We also present algorithms for part (ii) having time complexity O(k), where k is the number of tags in the facet, well suited to online computation.",
        "published": "2008-09-26T16:26:50Z",
        "link": "http://arxiv.org/abs/0809.4668v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Relevance Feedback in Conceptual Image Retrieval: A User Evaluation",
        "authors": [
            "Jose Torres",
            "Luis Paulo Reis"
        ],
        "summary": "The Visual Object Information Retrieval (VOIR) system described in this paper implements an image retrieval approach that combines two layers, the conceptual and the visual layer. It uses terms from a textual thesaurus to represent the conceptual information and also works with image regions, the visual information. The terms are related with the image regions through a weighted association enabling the execution of concept-level queries. VOIR uses region-based relevance feedback to improve the quality of the results in each query session and to discover new associations between text and image. This paper describes a user-centred and task-oriented comparative evaluation of VOIR which was undertaken considering three distinct versions of VOIR: a full-fledge version; one supporting relevance feedback only at image level; and a third version not supporting relevance feedback at all. The evaluation performed showed the usefulness of region based relevance feedback in the context of VOIR prototype.",
        "published": "2008-09-28T10:17:20Z",
        "link": "http://arxiv.org/abs/0809.4834v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Evaluation of Authors and Journals",
        "authors": [
            "Joseph B. Keller"
        ],
        "summary": "A method is presented for evaluating authors on the basis of citations. It assigns to each author a citation score which depends upon the number of times he is cited, and upon the scores of the citers. The scores are found to be the components of an eigenvector of a normalized citation matrix. The same method can be applied to citation of journals by other journals, to evaluating teams in a league [1], etc.",
        "published": "2008-10-05T20:36:19Z",
        "link": "http://arxiv.org/abs/0810.0852v1",
        "categories": [
            "math.HO",
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Analyse spectrale des textes: détection automatique des frontières   de langue et de discours",
        "authors": [
            "Pascal Vaillant",
            "Richard Nock",
            "Claudia Henry"
        ],
        "summary": "We propose a theoretical framework within which information on the vocabulary of a given corpus can be inferred on the basis of statistical information gathered on that corpus. Inferences can be made on the categories of the words in the vocabulary, and on their syntactical properties within particular languages. Based on the same statistical data, it is possible to build matrices of syntagmatic similarity (bigram transition matrices) or paradigmatic similarity (probability for any pair of words to share common contexts). When clustered with respect to their syntagmatic similarity, words tend to group into sublanguage vocabularies, and when clustered with respect to their paradigmatic similarity, into syntactic or semantic classes. Experiments have explored the first of these two possibilities. Their results are interpreted in the frame of a Markov chain modelling of the corpus' generative processe(s): we show that the results of a spectral analysis of the transition matrix can be interpreted as probability distributions of words within clusters. This method yields a soft clustering of the vocabulary into sublanguages which contribute to the generation of heterogeneous corpora. As an application, we show how multilingual texts can be visually segmented into linguistically homogeneous segments. Our method is specifically useful in the case of related languages which happened to be mixed in corpora.",
        "published": "2008-10-07T15:25:31Z",
        "link": "http://arxiv.org/abs/0810.1212v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Soft Uncoupling of Markov Chains for Permeable Language Distinction: A   New Algorithm",
        "authors": [
            "Richard Nock",
            "Pascal Vaillant",
            "Frank Nielsen",
            "Claudia Henry"
        ],
        "summary": "Without prior knowledge, distinguishing different languages may be a hard task, especially when their borders are permeable. We develop an extension of spectral clustering -- a powerful unsupervised classification toolbox -- that is shown to resolve accurately the task of soft language distinction. At the heart of our approach, we replace the usual hard membership assignment of spectral clustering by a soft, probabilistic assignment, which also presents the advantage to bypass a well-known complexity bottleneck of the method. Furthermore, our approach relies on a novel, convenient construction of a Markov chain out of a corpus. Extensive experiments with a readily available system clearly display the potential of the method, which brings a visually appealing soft distinction of languages that may define altogether a whole corpus.",
        "published": "2008-10-07T18:09:07Z",
        "link": "http://arxiv.org/abs/0810.1261v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Introduction to Searching with Regular Expressions",
        "authors": [
            "Christopher M. Frenz"
        ],
        "summary": "The explosive rate of information growth and availability often makes it increasingly difficult to locate information pertinent to your needs. These problems are often compounded when keyword based search methodologies are not adequate for describing the information you seek. In many instances, information such as Web site URLs, phone numbers, etc. can often be better identified through the use of a textual pattern than by keyword. For example, many more phone numbers could be picked up by a search for the pattern (XXX) XXX-XXXX, where X could be any digit, than would be by a search for any specific phone number (i.e. the keyword approach). Programming languages typically allow for the matching of textual patterns via the usage of regular expressions. This tutorial will provide an introduction to the basics of programming regular expressions as well as provide an introduction to how regular expressions can be applied to data processing tasks such as information extraction and search refinement.",
        "published": "2008-10-09T19:57:31Z",
        "link": "http://arxiv.org/abs/0810.1732v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Efficient Pattern Matching on Binary Strings",
        "authors": [
            "Simone Faro",
            "Thierry Lecroq"
        ],
        "summary": "The binary string matching problem consists in finding all the occurrences of a pattern in a text where both strings are built on a binary alphabet. This is an interesting problem in computer science, since binary data are omnipresent in telecom and computer network applications. Moreover the problem finds applications also in the field of image processing and in pattern matching on compressed texts. Recently it has been shown that adaptations of classical exact string matching algorithms are not very efficient on binary data. In this paper we present two efficient algorithms for the problem adapted to completely avoid any reference to bits allowing to process pattern and text byte by byte. Experimental results show that the new algorithms outperform existing solutions in most cases.",
        "published": "2008-10-14T08:44:27Z",
        "link": "http://arxiv.org/abs/0810.2390v2",
        "categories": [
            "cs.DS",
            "cs.IR",
            "F.2.2; H.3.3; E.4"
        ]
    },
    {
        "title": "A Simple Linear Ranking Algorithm Using Query Dependent Intercept   Variables",
        "authors": [
            "Nir Ailon"
        ],
        "summary": "The LETOR website contains three information retrieval datasets used as a benchmark for testing machine learning ideas for ranking. Algorithms participating in the challenge are required to assign score values to search results for a collection of queries, and are measured using standard IR ranking measures (NDCG, precision, MAP) that depend only the relative score-induced order of the results. Similarly to many of the ideas proposed in the participating algorithms, we train a linear classifier. In contrast with other participating algorithms, we define an additional free variable (intercept, or benchmark) for each query. This allows expressing the fact that results for different queries are incomparable for the purpose of determining relevance. The cost of this idea is the addition of relatively few nuisance parameters. Our approach is simple, and we used a standard logistic regression library to test it. The results beat the reported participating algorithms. Hence, it seems promising to combine our approach with other more complex ideas.",
        "published": "2008-10-15T19:03:10Z",
        "link": "http://arxiv.org/abs/0810.2764v1",
        "categories": [
            "cs.IR",
            "cs.LG"
        ]
    },
    {
        "title": "Combining Advanced Visualization and Automatized Reasoning for   Webometrics: A Test Study",
        "authors": [
            "Claire François",
            "Jean-Charles Lamirel",
            "Shadi Al Shehabi"
        ],
        "summary": "This paper presents a first attempt at performing a precise and automatic identification of the linking behaviour in a scientific domain through the analysis of the communication of the related academic institutions on the web. The proposed approach is based on the paradigm of multiple viewpoint data analysis (MVDA) than can be fruitfully exploited to highlight relationships between data, like websites, carrying several kinds of description. It uses the MultiSOM clustering and mapping method. The domain that has been chosen for this study is the domain of Computer Science in Germany. The analysis is conduced on a set of 438 websites of this domain using all together, thematic, geographic and linking information. It highlights interesting results concerning both global and local linking behaviour.",
        "published": "2008-10-28T15:43:45Z",
        "link": "http://arxiv.org/abs/0810.5057v2",
        "categories": [
            "cs.IR",
            "cs.DL"
        ]
    },
    {
        "title": "Quasi-metrics, Similarities and Searches: aspects of geometry of protein   datasets",
        "authors": [
            "Aleksandar Stojmirovic"
        ],
        "summary": "A quasi-metric is a distance function which satisfies the triangle inequality but is not symmetric: it can be thought of as an asymmetric metric. The central result of this thesis, developed in Chapter 3, is that a natural correspondence exists between similarity measures between biological (nucleotide or protein) sequences and quasi-metrics.   Chapter 2 presents basic concepts of the theory of quasi-metric spaces and introduces a new examples of them: the universal countable rational quasi-metric space and its bicompletion, the universal bicomplete separable quasi-metric space. Chapter 4 is dedicated to development of a notion of the quasi-metric space with Borel probability measure, or pq-space. The main result of this chapter indicates that `a high dimensional quasi-metric space is close to being a metric space'.   Chapter 5 investigates the geometric aspects of the theory of database similarity search in the context of quasi-metrics. The results about $pq$-spaces are used to produce novel theoretical bounds on performance of indexing schemes.   Finally, the thesis presents some biological applications. Chapter 6 introduces FSIndex, an indexing scheme that significantly accelerates similarity searches of short protein fragment datasets. Chapter 7 presents the prototype of the system for discovery of short functional protein motifs called PFMFind, which relies on FSIndex for similarity searches.",
        "published": "2008-10-30T03:14:17Z",
        "link": "http://arxiv.org/abs/0810.5407v1",
        "categories": [
            "cs.IR",
            "math.GN",
            "q-bio.QM"
        ]
    },
    {
        "title": "Relating Web pages to enable information-gathering tasks",
        "authors": [
            "Amitabha Bagchi",
            "Garima Lahoti"
        ],
        "summary": "We argue that relationships between Web pages are functions of the user's intent. We identify a class of Web tasks - information-gathering - that can be facilitated by a search engine that provides links to pages which are related to the page the user is currently viewing. We define three kinds of intentional relationships that correspond to whether the user is a) seeking sources of information, b) reading pages which provide information, or c) surfing through pages as part of an extended information-gathering process. We show that these three relationships can be productively mined using a combination of textual and link information and provide three scoring mechanisms that correspond to them: {\\em SeekRel}, {\\em FactRel} and {\\em SurfRel}. These scoring mechanisms incorporate both textual and link information. We build a set of capacitated subnetworks - each corresponding to a particular keyword - that mirror the interconnection structure of the World Wide Web. The scores are computed by computing flows on these subnetworks. The capacities of the links are derived from the {\\em hub} and {\\em authority} values of the nodes they connect, following the work of Kleinberg (1998) on assigning authority to pages in hyperlinked environments. We evaluated our scoring mechanism by running experiments on four data sets taken from the Web. We present user evaluations of the relevance of the top results returned by our scoring mechanisms and compare those to the top results returned by Google's Similar Pages feature, and the {\\em Companion} algorithm proposed by Dean and Henzinger (1999).",
        "published": "2008-10-30T07:17:49Z",
        "link": "http://arxiv.org/abs/0810.5428v2",
        "categories": [
            "cs.IR",
            "cs.DS",
            "H.3.3"
        ]
    },
    {
        "title": "Predicting the popularity of online content",
        "authors": [
            "Gabor Szabo",
            "Bernardo A. Huberman"
        ],
        "summary": "We present a method for accurately predicting the long time popularity of online content from early measurements of user access. Using two content sharing portals, Youtube and Digg, we show that by modeling the accrual of views and votes on content offered by these services we can predict the long-term dynamics of individual submissions from initial data. In the case of Digg, measuring access to given stories during the first two hours allows us to forecast their popularity 30 days ahead with remarkable accuracy, while downloads of Youtube videos need to be followed for 10 days to attain the same performance. The differing time scales of the predictions are shown to be due to differences in how content is consumed on the two portals: Digg stories quickly become outdated, while Youtube videos are still found long after they are initially submitted to the portal. We show that predictions are more accurate for submissions for which attention decays quickly, whereas predictions for evergreen content will be prone to larger errors.",
        "published": "2008-11-04T05:38:58Z",
        "link": "http://arxiv.org/abs/0811.0405v1",
        "categories": [
            "cs.CY",
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "CoZo+ - A Content Zoning Engine for textual documents",
        "authors": [
            "Cynthia Wagner",
            "Christoph Schommer"
        ],
        "summary": "Content zoning can be understood as a segmentation of textual documents into zones. This is inspired by [6] who initially proposed an approach for the argumentative zoning of textual documents. With the prototypical CoZo+ engine, we focus on content zoning towards an automatic processing of textual streams while considering only the actors as the zones. We gain information that can be used to realize an automatic recognition of content for pre-defined actors. We understand CoZo+ as a necessary pre-step towards an automatic generation of summaries and to make intellectual ownership of documents detectable.",
        "published": "2008-11-04T09:08:32Z",
        "link": "http://arxiv.org/abs/0811.0453v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.3; H.3.1"
        ]
    },
    {
        "title": "Query Refinement by Multi Word Term expansions and semantic synonymy",
        "authors": [
            "Veronila Lux-Pogodalla",
            "Eric San Juan"
        ],
        "summary": "We developed a system, TermWatch (https://stid-bdd.iut.univ-metz.fr/TermWatch/index.pl), which combines a linguistic extraction of terms, their structuring into a terminological network with a clustering algorithm. In this paper we explore its ability in integrating the most promising aspects of the studies on query refinement: choice of meaningful text units to cluster (domain terms), choice of tight semantic relations with which to cluster terms, structuring of terms in a network enabling abetter perception of domain concepts. We have run this experiment on the 367 645 English abstracts of PASCAL 2005-2006 bibliographic database (http://www.inist.fr) and compared the structured terminological resource automatically build by TermWarch to the English segment of TermScience resource (http://termsciences.inist.fr/) containing 88 211 terms.",
        "published": "2008-11-04T20:43:29Z",
        "link": "http://arxiv.org/abs/0811.0603v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Visualization of association graphs for assisting the interpretation of   classifications",
        "authors": [
            "Eric San Juan",
            "Ivana Roche"
        ],
        "summary": "Given a query on the PASCAL database maintained by the INIST, we design user interfaces to visualize and browse two types of graphs extracted from abstracts: 1) the graph of all associations between authors (co-author graph), 2) the graph of strong associations between authors and terms automatically extracted from abstracts and grouped using linguistic variations. We adapt for this purpose the TermWatch system that comprises a term extractor, a relation identifier which yields the terminological network and a clustering module. The results are output on two interfaces: a graphic one mapping the clusters in a 2D space and a terminological hypertext network allowing the user to interactively explore results and return to source texts.",
        "published": "2008-11-05T12:58:38Z",
        "link": "http://arxiv.org/abs/0811.0717v1",
        "categories": [
            "stat.AP",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Web Usage Analysis: New Science Indicators and Co-usage",
        "authors": [
            "Xavier Polanco",
            "Ivana Roche",
            "Dominique Besagni"
        ],
        "summary": "A new type of statistical analysis of the science and technical information (STI) in the Web context is produced. We propose a set of indicators about Web users, visualized bibliographic records, and e-commercial transactions. In addition, we introduce two Web usage factors. Finally, we give an overview of the co-usage analysis. For these tasks, we introduce a computer based system, called Miri@d, which produces descriptive statistical information about the Web users' searching behaviour, and what is effectively used from a free access digital bibliographical database. The system is conceived as a server of statistical data which are carried out beforehand, and as an interactive server for online statistical work. The results will be made available to analysts, who can use this descriptive statistical information as raw data for their indicator design tasks, and as input for multivariate data analysis, clustering analysis, and mapping. Managers also can exploit the results in order to improve management and decision-making.",
        "published": "2008-11-05T13:00:52Z",
        "link": "http://arxiv.org/abs/0811.0719v1",
        "categories": [
            "cs.IR",
            "stat.AP"
        ]
    },
    {
        "title": "Adaptive Base Class Boost for Multi-class Classification",
        "authors": [
            "Ping Li"
        ],
        "summary": "We develop the concept of ABC-Boost (Adaptive Base Class Boost) for multi-class classification and present ABC-MART, a concrete implementation of ABC-Boost. The original MART (Multiple Additive Regression Trees) algorithm has been very successful in large-scale applications. For binary classification, ABC-MART recovers MART. For multi-class classification, ABC-MART considerably improves MART, as evaluated on several public data sets.",
        "published": "2008-11-08T23:23:08Z",
        "link": "http://arxiv.org/abs/0811.1250v1",
        "categories": [
            "cs.LG",
            "cs.IR"
        ]
    },
    {
        "title": "Search Result Clustering via Randomized Partitioning of Query-Induced   Subgraphs",
        "authors": [
            "Aleksandar Bradic"
        ],
        "summary": "In this paper, we present an approach to search result clustering, using partitioning of underlying link graph. We define the notion of \"query-induced subgraph\" and formulate the problem of search result clustering as a problem of efficient partitioning of given subgraph into topic-related clusters. Also, we propose a novel algorithm for approximative partitioning of such graph, which results in cluster quality comparable to the one obtained by deterministic algorithms, while operating in more efficient computation time, suitable for practical implementations. Finally, we present a practical clustering search engine developed as a part of this research and use it to get results about real-world performance of proposed concepts.",
        "published": "2008-11-25T23:11:55Z",
        "link": "http://arxiv.org/abs/0811.4186v1",
        "categories": [
            "cs.IR",
            "cs.DS",
            "H.3.3; I.1.2; E.1; G.3"
        ]
    },
    {
        "title": "Frozen Footprints",
        "authors": [
            "Massimo Franceschet"
        ],
        "summary": "Bibliometrics has the ambitious goal of measuring science. To this end, it exploits the way science is disseminated trough scientific publications and the resulting citation network of scientific papers. We survey the main historical contributions to the field, the most interesting bibliometric indicators, and the most popular bibliometric data sources. Moreover, we discuss distributions commonly used to model bibliometric phenomena and give an overview of methods to build bibliometric maps of science.",
        "published": "2008-11-27T18:12:28Z",
        "link": "http://arxiv.org/abs/0811.4603v2",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Prospective Study for Semantic Inter-Media Fusion in Content-Based   Medical Image Retrieval",
        "authors": [
            "Roxana Teodorescu",
            "Daniel Racoceanu",
            "Wee-Kheng Leow",
            "Vladimir Cretu"
        ],
        "summary": "One important challenge in modern Content-Based Medical Image Retrieval (CBMIR) approaches is represented by the semantic gap, related to the complexity of the medical knowledge. Among the methods that are able to close this gap in CBMIR, the use of medical thesauri/ontologies has interesting perspectives due to the possibility of accessing on-line updated relevant webservices and to extract real-time medical semantic structured information. The CBMIR approach proposed in this paper uses the Unified Medical Language System's (UMLS) Metathesaurus to perform a semantic indexing and fusion of medical media. This fusion operates before the query processing (retrieval) and works at an UMLS-compliant conceptual indexing level. Our purpose is to study various techniques related to semantic data alignment, preprocessing, fusion, clustering and retrieval, by evaluating the various techniques and highlighting future research directions. The alignment and the preprocessing are based on partial text/image retrieval feedback and on the data structure. We analyze various probabilistic, fuzzy and evidence-based approaches for the fusion process and different similarity functions for the retrieval process. All the proposed methods are evaluated on the Cross Language Evaluation Forum's (CLEF) medical image retrieval benchmark, by focusing also on a more homogeneous component medical image database: the Pathology Education Instructional Resource (PEIR).",
        "published": "2008-11-28T13:30:23Z",
        "link": "http://arxiv.org/abs/0811.4717v1",
        "categories": [
            "cs.IR",
            "cs.CL"
        ]
    },
    {
        "title": "Emergent Community Structure in Social Tagging Systems",
        "authors": [
            "Ciro Cattuto",
            "Andrea Baldassarri",
            "Vito D. P. Servedio",
            "Vittorio Loreto"
        ],
        "summary": "A distributed classification paradigm known as collaborative tagging has been widely adopted in new Web applications designed to manage and share online resources. Users of these applications organize resources (Web pages, digital photographs, academic papers) by associating with them freely chosen text labels, or tags. Here we leverage the social aspects of collaborative tagging and introduce a notion of resource distance based on the collective tagging activity of users. We collect data from a popular system and perform experiments showing that our definition of distance can be used to build a weighted network of resources with a detectable community structure. We show that this community structure clearly exposes the semantic relations among resources. The communities of resources that we observe are a genuinely emergent feature, resulting from the uncoordinated activity of a large number of users, and their detection paves the way for mapping emergent semantics in social tagging systems.",
        "published": "2008-12-03T11:40:46Z",
        "link": "http://arxiv.org/abs/0812.0698v1",
        "categories": [
            "cs.IR",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "Adaptive Spam Detection Inspired by a Cross-Regulation Model of Immune   Dynamics: A Study of Concept Drift",
        "authors": [
            "Alaa Abi-Haidar",
            "Luis M. Rocha"
        ],
        "summary": "This paper proposes a novel solution to spam detection inspired by a model of the adaptive immune system known as the crossregulation model. We report on the testing of a preliminary algorithm on six e-mail corpora. We also compare our results statically and dynamically with those obtained by the Naive Bayes classifier and another binary classification method we developed previously for biomedical text-mining applications. We show that the cross-regulation model is competitive against those and thus promising as a bio-inspired algorithm for spam detection in particular, and binary classification in general.",
        "published": "2008-12-04T20:40:32Z",
        "link": "http://arxiv.org/abs/0812.1014v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "nlin.AO"
        ]
    },
    {
        "title": "Uncovering protein interaction in abstracts and text using a novel   linear model and word proximity networks",
        "authors": [
            "Alaa Abi-Haidar",
            "Jasleen Kaur",
            "Ana G. Maguitman",
            "Predrag Radivojac",
            "Andreas Retchsteiner",
            "Karin Verspoor",
            "Zhiping Wang",
            "Luis M. Rocha"
        ],
        "summary": "We participated in three of the protein-protein interaction subtasks of the Second BioCreative Challenge: classification of abstracts relevant for protein-protein interaction (IAS), discovery of protein pairs (IPS) and text passages characterizing protein interaction (ISS) in full text documents. We approached the abstract classification task with a novel, lightweight linear model inspired by spam-detection techniques, as well as an uncertainty-based integration scheme. We also used a Support Vector Machine and the Singular Value Decomposition on the same features for comparison purposes. Our approach to the full text subtasks (protein pair and passage identification) includes a feature expansion method based on word-proximity networks. Our approach to the abstract classification task (IAS) was among the top submissions for this task in terms of the measures of performance used in the challenge evaluation (accuracy, F-score and AUC). We also report on a web-tool we produced using our approach: the Protein Interaction Abstract Relevance Evaluator (PIARE). Our approach to the full text tasks resulted in one of the highest recall rates as well as mean reciprocal rank of correct passages. Our approach to abstract classification shows that a simple linear model, using relatively few features, is capable of generalizing and uncovering the conceptual nature of protein-protein interaction from the bibliome. Since the novel approach is based on a very lightweight linear model, it can be easily ported and applied to similar problems. In full text problems, the expansion of word features with word-proximity networks is shown to be useful, though the need for some improvements is discussed.",
        "published": "2008-12-04T21:37:35Z",
        "link": "http://arxiv.org/abs/0812.1029v1",
        "categories": [
            "cs.IR",
            "cs.LG"
        ]
    },
    {
        "title": "Conceptual approach through an annotation process for the representation   and the information contents enhancement in economic intelligence (EI)",
        "authors": [
            "Sahbi Sidhom"
        ],
        "summary": "In the era of the information society, the impact of the information systems on the economy of material and immaterial is certainly perceptible. With regards to the information resources of an organization, the annotation involved to enrich informational content, to track the intellectual activities on a document and to set the added value on information for the benefit of solving a decision-making problem in the context of economic intelligence. Our contribution is distinguished by the representation of an annotation process and its inherent concepts to lead the decisionmaker to an anticipated decision: the provision of relevant and annotated information. Such information in the system is made easy by taking into account the diversity of resources and those that are well annotated so formally and informally by the EI actors. A capital research framework consist of integrating in the decision-making process the annotator activity, the software agent (or the reasoning mechanisms) and the information resources enhancement.",
        "published": "2008-12-07T20:07:37Z",
        "link": "http://arxiv.org/abs/0812.1394v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Stability of graph communities across time scales",
        "authors": [
            "J. -C. Delvenne",
            "S. N. Yaliraki",
            "M. Barahona"
        ],
        "summary": "The complexity of biological, social and engineering networks makes it desirable to find natural partitions into communities that can act as simplified descriptions and provide insight into the structure and function of the overall system. Although community detection methods abound, there is a lack of consensus on how to quantify and rank the quality of partitions. We show here that the quality of a partition can be measured in terms of its stability, defined in terms of the clustered autocovariance of a Markov process taking place on the graph. Because the stability has an intrinsic dependence on time scales of the graph, it allows us to compare and rank partitions at each time and also to establish the time spans over which partitions are optimal. Hence the Markov time acts effectively as an intrinsic resolution parameter that establishes a hierarchy of increasingly coarser clusterings. Within our framework we can then provide a unifying view of several standard partitioning measures: modularity and normalized cut size can be interpreted as one-step time measures, whereas Fiedler's spectral clustering emerges at long times. We apply our method to characterize the relevance and persistence of partitions over time for constructive and real networks, including hierarchical graphs and social networks. We also obtain reduced descriptions for atomic level protein structures over different time scales.",
        "published": "2008-12-09T22:32:02Z",
        "link": "http://arxiv.org/abs/0812.1811v4",
        "categories": [
            "physics.soc-ph",
            "cs.IR",
            "physics.data-an"
        ]
    },
    {
        "title": "Content-based and Algorithmic Classifications of Journals: Perspectives   on the Dynamics of Scientific Communication and Indexer Effects",
        "authors": [
            "Ismael Rafols",
            "Loet Leydesdorff"
        ],
        "summary": "The aggregated journal-journal citation matrix -based on the Journal Citation Reports (JCR) of the Science Citation Index- can be decomposed by indexers and/or algorithmically. In this study, we test the results of two recently available algorithms for the decomposition of large matrices against two content-based classifications of journals: the ISI Subject Categories and the field/subfield classification of Glaenzel & Schubert (2003). The content-based schemes allow for the attribution of more than a single category to a journal, whereas the algorithms maximize the ratio of within-category citations over between-category citations in the aggregated category-category citation matrix. By adding categories, indexers generate between-category citations, which may enrich the database, for example, in the case of inter-disciplinary developments. The consequent indexer effects are significant in sparse areas of the matrix more than in denser ones. Algorithmic decompositions, on the other hand, are more heavily skewed towards a relatively small number of categories, while this is deliberately counter-acted upon in the case of content-based classifications. Because of the indexer effects, science policy studies and the sociology of science should be careful when using content-based classifications, which are made for bibliographic disclosure, and not for the purpose of analyzing latent structures in scientific communications. Despite the large differences among them, the four classification schemes enable us to generate surprisingly similar maps of science at the global level. Erroneous classifications are cancelled as noise at the aggregate level, but may disturb the evaluation locally.",
        "published": "2008-12-23T03:45:51Z",
        "link": "http://arxiv.org/abs/0812.4332v1",
        "categories": [
            "physics.data-an",
            "cs.DL",
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Mining User Profiles to Support Structure and Explanation in Open Social   Networking",
        "authors": [
            "Avare Stewart",
            "Ernesto Diaz-Aviles",
            "Wolfgang Nejdl"
        ],
        "summary": "The proliferation of media sharing and social networking websites has brought with it vast collections of site-specific user generated content. The result is a Social Networking Divide in which the concepts and structure common across different sites are hidden. The knowledge and structures from one social site are not adequately exploited to provide new information and resources to the same or different users in comparable social sites. For music bloggers, this latent structure, forces bloggers to select sub-optimal blogrolls. However, by integrating the social activities of music bloggers and listeners, we are able to overcome this limitation: improving the quality of the blogroll neighborhoods, in terms of similarity, by 85 percent when using tracks and by 120 percent when integrating tags from another site.",
        "published": "2008-12-23T23:20:44Z",
        "link": "http://arxiv.org/abs/0812.4461v1",
        "categories": [
            "cs.IR",
            "H.3.3; H.3.5"
        ]
    },
    {
        "title": "Emergence of Spontaneous Order Through Neighborhood Formation in   Peer-to-Peer Recommender Systems",
        "authors": [
            "Ernesto Diaz-Aviles",
            "Lars Schmidt-Thieme",
            "Cai-Nicolas Ziegler"
        ],
        "summary": "The advent of the Semantic Web necessitates paradigm shifts away from centralized client/server architectures towards decentralization and peer-to-peer computation, making the existence of central authorities superfluous and even impossible. At the same time, recommender systems are gaining considerable impact in e-commerce, providing people with recommendations that are personalized and tailored to their very needs. These recommender systems have traditionally been deployed with stark centralized scenarios in mind, operating in closed communities detached from their host network's outer perimeter. We aim at marrying these two worlds, i.e., decentralized peer-to-peer computing and recommender systems, in one agent-based framework. Our architecture features an epidemic-style protocol maintaining neighborhoods of like-minded peers in a robust, selforganizing fashion. In order to demonstrate our architecture's ability to retain scalability, robustness and to allow for convergence towards high-quality recommendations, we conduct offline experiments on top of the popular MovieLens dataset.",
        "published": "2008-12-23T23:26:27Z",
        "link": "http://arxiv.org/abs/0812.4460v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "cs.MA",
            "C.2.4; H.3.3"
        ]
    },
    {
        "title": "Assessing scientific research performance and impact with single indices",
        "authors": [
            "John Panaretos",
            "Chrisovaladis Malesios"
        ],
        "summary": "We provide a comprehensive and critical review of the h-index and its most important modifications proposed in the literature, as well as of other similar indicators measuring research output and impact. Extensions of some of these indices are presented and illustrated.",
        "published": "2008-12-24T15:41:48Z",
        "link": "http://arxiv.org/abs/0812.4542v3",
        "categories": [
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Regular Expression Subtyping for XML Query and Update Languages",
        "authors": [
            "James Cheney"
        ],
        "summary": "XML database query languages such as XQuery employ regular expression types with structural subtyping. Subtyping systems typically have two presentations, which should be equivalent: a declarative version in which the subsumption rule may be used anywhere, and an algorithmic version in which the use of subsumption is limited in order to make typechecking syntax-directed and decidable. However, the XQuery standard type system circumvents this issue by using imprecise typing rules for iteration constructs and defining only algorithmic typechecking, and another extant proposal provides more precise types for iteration constructs but ignores subtyping. In this paper, we consider a core XQuery-like language with a subsumption rule and prove the completeness of algorithmic typechecking; this is straightforward for XQuery proper but requires some care in the presence of more precise iteration typing disciplines. We extend this result to an XML update language we have introduced in earlier work.",
        "published": "2008-01-04T18:13:48Z",
        "link": "http://arxiv.org/abs/0801.0714v1",
        "categories": [
            "cs.PL",
            "cs.DB"
        ]
    },
    {
        "title": "Call-by-value Termination in the Untyped lambda-calculus",
        "authors": [
            "Neil D. Jones",
            "Nina Bohr"
        ],
        "summary": "A fully-automated algorithm is developed able to show that evaluation of a given untyped lambda-expression will terminate under CBV (call-by-value). The ``size-change principle'' from first-order programs is extended to arbitrary untyped lambda-expressions in two steps. The first step suffices to show CBV termination of a single, stand-alone lambda;-expression. The second suffices to show CBV termination of any member of a regular set of lambda-expressions, defined by a tree grammar. (A simple example is a minimum function, when applied to arbitrary Church numerals.) The algorithm is sound and proven so in this paper. The Halting Problem's undecidability implies that any sound algorithm is necessarily incomplete: some lambda-expressions may in fact terminate under CBV evaluation, but not be recognised as terminating.   The intensional power of the termination algorithm is reasonably high. It certifies as terminating many interesting and useful general recursive algorithms including programs with mutual recursion and parameter exchanges, and Colson's ``minimum'' algorithm. Further, our type-free approach allows use of the Y combinator, and so can identify as terminating a substantial subset of PCF.",
        "published": "2008-01-06T19:01:02Z",
        "link": "http://arxiv.org/abs/0801.0882v2",
        "categories": [
            "cs.PL",
            "F.3.2; D.3.1"
        ]
    },
    {
        "title": "DSL development based on target meta-models. Using AST transformations   for automating semantic analysis in a textual DSL framework",
        "authors": [
            "Andrey Breslav"
        ],
        "summary": "This paper describes an approach to creating textual syntax for Do- main-Specific Languages (DSL). We consider target meta-model to be the main artifact and hence to be developed first. The key idea is to represent analysis of textual syntax as a sequence of transformations. This is made by explicit opera- tions on abstract syntax trees (ATS), for which a simple language is proposed. Text-to-model transformation is divided into two parts: text-to-AST (developed by openArchitectureWare [1]) and AST-to-model (proposed by this paper). Our approach simplifies semantic analysis and helps to generate as much as possi- ble.",
        "published": "2008-01-08T12:28:18Z",
        "link": "http://arxiv.org/abs/0801.1219v1",
        "categories": [
            "cs.PL",
            "D.3.4"
        ]
    },
    {
        "title": "Generative Unbinding of Names",
        "authors": [
            "Andrew M. Pitts",
            "Mark R. Shinwell"
        ],
        "summary": "This paper is concerned with the form of typed name binding used by the FreshML family of languages. Its characteristic feature is that a name binding is represented by an abstract (name,value)-pair that may only be deconstructed via the generation of fresh bound names. The paper proves a new result about what operations on names can co-exist with this construct. In FreshML the only observation one can make of names is to test whether or not they are equal. This restricted amount of observation was thought necessary to ensure that there is no observable difference between alpha-equivalent name binders. Yet from an algorithmic point of view it would be desirable to allow other operations and relations on names, such as a total ordering. This paper shows that, contrary to expectations, one may add not just ordering, but almost any relation or numerical function on names without disturbing the fundamental correctness result about this form of typed name binding (that object-level alpha-equivalence precisely corresponds to contextual equivalence at the programming meta-level), so long as one takes the state of dynamically created names into account.",
        "published": "2008-01-08T15:04:56Z",
        "link": "http://arxiv.org/abs/0801.1251v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.3.1; D.3.3; F.3.2"
        ]
    },
    {
        "title": "Programming an interpreter using molecular dynamics",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "PGA (ProGram Algebra) is an algebra of programs which concerns programs in their simplest form: sequences of instructions. Molecular dynamics is a simple model of computation developed in the setting of PGA, which bears on the use of dynamic data structures in programming. We consider the programming of an interpreter for a program notation that is close to existing assembly languages using PGA with the primitives of molecular dynamics as basic instructions. It happens that, although primarily meant for explaining programming language features relating to the use of dynamic data structures, the collection of primitives of molecular dynamics in itself is suited to our programming wants.",
        "published": "2008-01-15T07:56:12Z",
        "link": "http://arxiv.org/abs/0801.2226v1",
        "categories": [
            "cs.PL",
            "D.1.4; D.3.1; D.3.4; F.1.1; F.3.2"
        ]
    },
    {
        "title": "Policies of System Level Pipeline Modeling",
        "authors": [
            "Ed Harcourt"
        ],
        "summary": "Pipelining is a well understood and often used implementation technique for increasing the performance of a hardware system. We develop several SystemC/C++ modeling techniques that allow us to quickly model, simulate, and evaluate pipelines. We employ a small domain specific language (DSL) based on resource usage patterns that automates the drudgery of boilerplate code needed to configure connectivity in simulation models. The DSL is embedded directly in the host modeling language SystemC/C++. Additionally we develop several techniques for parameterizing a pipeline's behavior based on policies of function, communication, and timing (performance modeling).",
        "published": "2008-01-15T15:44:28Z",
        "link": "http://arxiv.org/abs/0801.2201v1",
        "categories": [
            "cs.AR",
            "cs.PL"
        ]
    },
    {
        "title": "Ensuring Spreadsheet Integrity with Model Master",
        "authors": [
            "Jocelyn Paine"
        ],
        "summary": "We have developed the Model Master (MM) language for describing spreadsheets, and tools for converting MM programs to and from spreadsheets. The MM decompiler translates a spreadsheet into an MM program which gives a concise summary of its calculations, layout, and styling. This is valuable when trying to understand spreadsheets one has not seen before, and when checking for errors. The MM compiler goes the other way, translating an MM program into a spreadsheet. This makes possible a new style of development, in which spreadsheets are generated from textual specifications. This can reduce error rates compared to working directly with the raw spreadsheet, and gives important facilities for code reuse. MM programs also offer advantages over Excel files for the interchange of spreadsheets.",
        "published": "2008-01-24T00:32:29Z",
        "link": "http://arxiv.org/abs/0801.3690v1",
        "categories": [
            "cs.PL",
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.9"
        ]
    },
    {
        "title": "Modular Compilation of a Synchronous Language",
        "authors": [
            "Annie Ressouche",
            "Daniel Gaffé",
            "Valérie Roy"
        ],
        "summary": "Synchronous languages rely on formal methods to ease the development of applications in an efficient and reusable way. Formal methods have been advocated as a means of increasing the reliability of systems, especially those which are safety or business critical. It is still difficult to develop automatic specification and verification tools due to limitations like state explosion, undecidability, etc... In this work, we design a new specification model based on a reactive synchronous approach. Then, we benefit from a formal framework well suited to perform compilation and formal validation of systems. In practice, we design and implement a special purpose language (LE) and its two semantics: the ehavioral semantics helps us to define a program by the set of its behaviors and avoid ambiguousness in programs' interpretation; the execution equational semantics allows the modular compilation of programs into software and hardware targets (c code, vhdl code, fpga synthesis, observers). Our approach is pertinent considering the two main requirements of critical realistic applications: the modular compilation allows us to deal with large systems, the model-based approach provides us with formal validation.",
        "published": "2008-01-24T15:24:46Z",
        "link": "http://arxiv.org/abs/0801.3715v1",
        "categories": [
            "cs.PL",
            "cs.LO"
        ]
    },
    {
        "title": "Quantum entanglement analysis based on abstract interpretation",
        "authors": [
            "Simon Perdrix"
        ],
        "summary": "Entanglement is a non local property of quantum states which has no classical counterpart and plays a decisive role in quantum information theory. Several protocols, like the teleportation, are based on quantum entangled states. Moreover, any quantum algorithm which does not create entanglement can be efficiently simulated on a classical computer. The exact role of the entanglement is nevertheless not well understood. Since an exact analysis of entanglement evolution induces an exponential slowdown, we consider approximative analysis based on the framework of abstract interpretation. In this paper, a concrete quantum semantics based on superoperators is associated with a simple quantum programming language. The representation of entanglement, i.e. the design of the abstract domain is a key issue. A representation of entanglement as a partition of the memory is chosen. An abstract semantics is introduced, and the soundness of the approximation is proven.",
        "published": "2008-01-28T10:45:47Z",
        "link": "http://arxiv.org/abs/0801.4230v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "quant-ph"
        ]
    },
    {
        "title": "Spreadsheet Debugging",
        "authors": [
            "Yirsaw Ayalew",
            "Roland Mittermeir"
        ],
        "summary": "Spreadsheet programs, artifacts developed by non-programmers, are used for a variety of important tasks and decisions. Yet a significant proportion of them have severe quality problems. To address this issue, our previous work presented an interval-based testing methodology for spreadsheets. Interval-based testing rests on the observation that spreadsheets are mainly used for numerical computations. It also incorporates ideas from symbolic testing and interval analysis. This paper addresses the issue of efficiently debugging spreadsheets. Based on the interval-based testing methodology, this paper presents a technique for tracing faults in spreadsheet programs. The fault tracing technique proposed uses the dataflow information and cell marks to identify the most influential faulty cell(s) for a given formula cell containing a propagated fault.",
        "published": "2008-01-28T14:07:58Z",
        "link": "http://arxiv.org/abs/0801.4280v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "Independence and concurrent separation logic",
        "authors": [
            "Jonathan Hayman",
            "Glynn Winskel"
        ],
        "summary": "A compositional Petri net-based semantics is given to a simple language allowing pointer manipulation and parallelism. The model is then applied to give a notion of validity to the judgements made by concurrent separation logic that emphasizes the process-environment duality inherent in such rely-guarantee reasoning. Soundness of the rules of concurrent separation logic with respect to this definition of validity is shown. The independence information retained by the Petri net model is then exploited to characterize the independence of parallel processes enforced by the logic. This is shown to permit a refinement operation capable of changing the granularity of atomic actions.",
        "published": "2008-02-06T15:39:20Z",
        "link": "http://arxiv.org/abs/0802.0820v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.2; F.3.1; D.3.1; F.1.2"
        ]
    },
    {
        "title": "Thread extraction for polyadic instruction sequences",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "In this paper, we study the phenomenon that instruction sequences are split into fragments which somehow produce a joint behaviour. In order to bring this phenomenon better into the picture, we formalize a simple mechanism by which several instruction sequence fragments can produce a joint behaviour. We also show that, even in the case of this simple mechanism, it is a non-trivial matter to explain by means of a translation into a single instruction sequence what takes place on execution of a collection of instruction sequence fragments.",
        "published": "2008-02-12T07:49:27Z",
        "link": "http://arxiv.org/abs/0802.1578v3",
        "categories": [
            "cs.PL",
            "D.3.1; D.3.3; F.1.1; F.3.2; F.3.3"
        ]
    },
    {
        "title": "Software graphs and programmer awareness",
        "authors": [
            "G. J. Baxter",
            "M. R. Frean"
        ],
        "summary": "Dependencies between types in object-oriented software can be viewed as directed graphs, with types as nodes and dependencies as edges. The in-degree and out-degree distributions of such graphs have quite different forms, with the former resembling a power-law distribution and the latter an exponential distribution. This effect appears to be independent of application or type relationship. A simple generative model is proposed to explore the proposition that the difference arises because the programmer is aware of the out-degree of a type but not of its in-degree. The model reproduces the two distributions, and compares reasonably well to those observed in 14 different type relationships across 12 different Java applications.",
        "published": "2008-02-16T03:38:03Z",
        "link": "http://arxiv.org/abs/0802.2306v1",
        "categories": [
            "cs.SE",
            "cs.PL"
        ]
    },
    {
        "title": "The RDF Virtual Machine",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "The Resource Description Framework (RDF) is a semantic network data model that is used to create machine-understandable descriptions of the world and is the basis of the Semantic Web. This article discusses the application of RDF to the representation of computer software and virtual computing machines. The Semantic Web is posited as not only a web of data, but also as a web of programs and processes.",
        "published": "2008-02-24T05:28:52Z",
        "link": "http://arxiv.org/abs/0802.3492v2",
        "categories": [
            "cs.PL",
            "F.1.2; I.2.4; E.1"
        ]
    },
    {
        "title": "Algebraic Pattern Matching in Join Calculus",
        "authors": [
            "Qin Ma",
            "Luc Maranget"
        ],
        "summary": "We propose an extension of the join calculus with pattern matching on algebraic data types. Our initial motivation is twofold: to provide an intuitive semantics of the interaction between concurrency and pattern matching; to define a practical compilation scheme from extended join definitions into ordinary ones plus ML pattern matching. To assess the correctness of our compilation scheme, we develop a theory of the applied join calculus, a calculus with value passing and value matching. We implement this calculus as an extension of the current JoCaml system.",
        "published": "2008-02-27T13:21:51Z",
        "link": "http://arxiv.org/abs/0802.4018v2",
        "categories": [
            "cs.PL",
            "cs.DC",
            "D.1.3; D.3.3; F.3.2"
        ]
    },
    {
        "title": "Automated Termination Proofs for Logic Programs by Term Rewriting",
        "authors": [
            "P. Schneider-Kamp",
            "J. Giesl",
            "A. Serebrenik",
            "R. Thiemann"
        ],
        "summary": "There are two kinds of approaches for termination analysis of logic programs: \"transformational\" and \"direct\" ones. Direct approaches prove termination directly on the basis of the logic program. Transformational approaches transform a logic program into a term rewrite system (TRS) and then analyze termination of the resulting TRS instead. Thus, transformational approaches make all methods previously developed for TRSs available for logic programs as well. However, the applicability of most existing transformations is quite restricted, as they can only be used for certain subclasses of logic programs. (Most of them are restricted to well-moded programs.) In this paper we improve these transformations such that they become applicable for any definite logic program. To simulate the behavior of logic programs by TRSs, we slightly modify the notion of rewriting by permitting infinite terms. We show that our transformation results in TRSs which are indeed suitable for automated termination analysis. In contrast to most other methods for termination of logic programs, our technique is also sound for logic programming without occur check, which is typically used in practice. We implemented our approach in the termination prover AProVE and successfully evaluated it on a large collection of examples.",
        "published": "2008-03-02T14:53:01Z",
        "link": "http://arxiv.org/abs/0803.0014v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL",
            "F.3.1; D.1.6; I.2.2; I.2.3"
        ]
    },
    {
        "title": "The Abella Interactive Theorem Prover (System Description)",
        "authors": [
            "Andrew Gacek"
        ],
        "summary": "Abella is an interactive system for reasoning about aspects of object languages that have been formally presented through recursive rules based on syntactic structure. Abella utilizes a two-level logic approach to specification and reasoning. One level is defined by a specification logic which supports a transparent encoding of structural semantics rules and also enables their execution. The second level, called the reasoning logic, embeds the specification logic and allows the development of proofs of properties about specifications. An important characteristic of both logics is that they exploit the lambda tree syntax approach to treating binding in object languages. Amongst other things, Abella has been used to prove normalizability properties of the lambda calculus, cut admissibility for a sequent calculus and type uniqueness and subject reduction properties. This paper discusses the logical foundations of Abella, outlines the style of theorem proving that it supports and finally describes some of its recent applications.",
        "published": "2008-03-15T16:15:10Z",
        "link": "http://arxiv.org/abs/0803.2305v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Concurrent Composition and Algebras of Events, Actions, and Processes",
        "authors": [
            "Mark Burgin a",
            "Marc L. Smith"
        ],
        "summary": "There are many different models of concurrent processes. The goal of this work is to introduce a common formalized framework for current research in this area and to eliminate shortcomings of existing models of concurrency. Following up the previous research of the authors and other researchers on concurrency, here we build a high-level metamodel EAP (event-action-process) for concurrent processes. This metamodel comprises a variety of other models of concurrent processes. We shape mathematical models for, and study events, actions, and processes in relation to important practical problems, such as communication in networks, concurrent programming, and distributed computations. In the third section of the work, a three-level algebra of events, actions and processes is constructed and studied as a new stage of algebra for concurrent processes. Relations between EAP process algebra and other models of concurrency are considered in the fourth section of this work.",
        "published": "2008-03-21T00:27:38Z",
        "link": "http://arxiv.org/abs/0803.3099v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.4.1"
        ]
    },
    {
        "title": "A Type System for Data-Flow Integrity on Windows Vista",
        "authors": [
            "Avik Chaudhuri",
            "Prasad Naldurg",
            "Sriram Rajamani"
        ],
        "summary": "The Windows Vista operating system implements an interesting model of multi-level integrity. We observe that in this model, trusted code can be blamed for any information-flow attack; thus, it is possible to eliminate such attacks by static analysis of trusted code. We formalize this model by designing a type system that can efficiently enforce data-flow integrity on Windows Vista. Typechecking guarantees that objects whose contents are statically trusted never contain untrusted values, regardless of what untrusted code runs in the environment. Some of Windows Vista's runtime access checks are necessary for soundness; others are redundant and can be optimized away.",
        "published": "2008-03-21T21:28:16Z",
        "link": "http://arxiv.org/abs/0803.3230v2",
        "categories": [
            "cs.CR",
            "cs.OS",
            "cs.PL",
            "D.4.6; D.2.4; F.3.1"
        ]
    },
    {
        "title": "Structure and Interpretation of Computer Programs",
        "authors": [
            "Ganesh M. Narayan",
            "K. Gopinath",
            "V. Sridhar"
        ],
        "summary": "Call graphs depict the static, caller-callee relation between \"functions\" in a program. With most source/target languages supporting functions as the primitive unit of composition, call graphs naturally form the fundamental control flow representation available to understand/develop software. They are also the substrate on which various interprocedural analyses are performed and are integral part of program comprehension/testing. Given their universality and usefulness, it is imperative to ask if call graphs exhibit any intrinsic graph theoretic features -- across versions, program domains and source languages. This work is an attempt to answer these questions: we present and investigate a set of meaningful graph measures that help us understand call graphs better; we establish how these measures correlate, if any, across different languages and program domains; we also assess the overall, language independent software quality by suitably interpreting these measures.",
        "published": "2008-03-27T22:58:43Z",
        "link": "http://arxiv.org/abs/0803.4025v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.8; D.2.3; D.2.5; D.2.10"
        ]
    },
    {
        "title": "Semi-continuous Sized Types and Termination",
        "authors": [
            "Andreas Abel"
        ],
        "summary": "Some type-based approaches to termination use sized types: an ordinal bound for the size of a data structure is stored in its type. A recursive function over a sized type is accepted if it is visible in the type system that recursive calls occur just at a smaller size. This approach is only sound if the type of the recursive function is admissible, i.e., depends on the size index in a certain way. To explore the space of admissible functions in the presence of higher-kinded data types and impredicative polymorphism, a semantics is developed where sized types are interpreted as functions from ordinals into sets of strongly normalizing terms. It is shown that upper semi-continuity of such functions is a sufficient semantic criterion for admissibility. To provide a syntactical criterion, a calculus for semi-continuous functions is developed.",
        "published": "2008-04-05T22:27:29Z",
        "link": "http://arxiv.org/abs/0804.0876v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "Testing data types implementations from algebraic specifications",
        "authors": [
            "Marie-Claude Gaudel",
            "Pascale Le Gall"
        ],
        "summary": "Algebraic specifications of data types provide a natural basis for testing data types implementations. In this framework, the conformance relation is based on the satisfaction of axioms. This makes it possible to formally state the fundamental concepts of testing: exhaustive test set, testability hypotheses, oracle. Various criteria for selecting finite test sets have been proposed. They depend on the form of the axioms, and on the possibilities of observation of the implementation under test. This last point is related to the well-known oracle problem. As the main interest of algebraic specifications is data type abstraction, testing a concrete implementation raises the issue of the gap between the abstract description and the concrete representation. The observational semantics of algebraic specifications bring solutions on the basis of the so-called observable contexts. After a description of testing methods based on algebraic specifications, the chapter gives a brief presentation of some tools and case studies, and presents some applications to other formal methods involving datatypes.",
        "published": "2008-04-07T06:35:44Z",
        "link": "http://arxiv.org/abs/0804.0970v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "A Survey of Quantum Programming Languages: History, Methods, and Tools",
        "authors": [
            "Donald A. Sofge"
        ],
        "summary": "Quantum computer programming is emerging as a new subject domain from multidisciplinary research in quantum computing, computer science, mathematics (especially quantum logic, lambda calculi, and linear logic), and engineering attempts to build the first non-trivial quantum computer. This paper briefly surveys the history, methods, and proposed tools for programming quantum computers circa late 2007. It is intended to provide an extensive but non-exhaustive look at work leading up to the current state-of-the-art in quantum computer programming. Further, it is an attempt to analyze the needed programming tools for quantum programmers, to use this analysis to predict the direction in which the field is moving, and to make recommendations for further development of quantum programming language tools.",
        "published": "2008-04-07T19:48:31Z",
        "link": "http://arxiv.org/abs/0804.1118v1",
        "categories": [
            "cs.PL",
            "D.3.2"
        ]
    },
    {
        "title": "The Geometry of Interaction of Differential Interaction Nets",
        "authors": [
            "Marc de Falco"
        ],
        "summary": "The Geometry of Interaction purpose is to give a semantic of proofs or programs accounting for their dynamics. The initial presentation, translated as an algebraic weighting of paths in proofnets, led to a better characterization of the lambda-calculus optimal reduction. Recently Ehrhard and Regnier have introduced an extension of the Multiplicative Exponential fragment of Linear Logic (MELL) that is able to express non-deterministic behaviour of programs and a proofnet-like calculus: Differential Interaction Nets. This paper constructs a proper Geometry of Interaction (GoI) for this extension. We consider it both as an algebraic theory and as a concrete reversible computation. We draw links between this GoI and the one of MELL. As a by-product we give for the first time an equational theory suitable for the GoI of the Multiplicative Additive fragment of Linear Logic.",
        "published": "2008-04-09T08:57:47Z",
        "link": "http://arxiv.org/abs/0804.1435v1",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "A classification of invasive patterns in AOP",
        "authors": [
            "Freddy Munoz",
            "Benoit Baudry",
            "Olivier Barais"
        ],
        "summary": "Aspect-Oriented Programming (AOP) improves modularity by encapsulating crosscutting concerns into aspects. Some mechanisms to compose aspects allow invasiveness as a mean to integrate concerns. Invasiveness means that AOP languages have unrestricted access to program properties. Such kind of languages are interesting because they allow performing complex operations and better introduce functionalities. In this report we present a classification of invasive patterns in AOP. This classification characterizes the aspects invasive behavior and allows developers to abstract about the aspect incidence over the program they crosscut.",
        "published": "2008-04-10T13:21:32Z",
        "link": "http://arxiv.org/abs/0804.1696v2",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "A Logic Programming Framework for Combinational Circuit Synthesis",
        "authors": [
            "Paul Tarau",
            "Brenda Luderman"
        ],
        "summary": "Logic Programming languages and combinational circuit synthesis tools share a common \"combinatorial search over logic formulae\" background. This paper attempts to reconnect the two fields with a fresh look at Prolog encodings for the combinatorial objects involved in circuit synthesis. While benefiting from Prolog's fast unification algorithm and built-in backtracking mechanism, efficiency of our search algorithm is ensured by using parallel bitstring operations together with logic variable equality propagation, as a mapping mechanism from primary inputs to the leaves of candidate Leaf-DAGs implementing a combinational circuit specification. After an exhaustive expressiveness comparison of various minimal libraries, a surprising first-runner, Strict Boolean Inequality \"<\" together with constant function \"1\" also turns out to have small transistor-count implementations, competitive to NAND-only or NOR-only libraries. As a practical outcome, a more realistic circuit synthesizer is implemented that combines rewriting-based simplification of (<,1) circuits with exhaustive Leaf-DAG circuit search.   Keywords: logic programming and circuit design, combinatorial object generation, exact combinational circuit synthesis, universal boolean logic libraries, symbolic rewriting, minimal transistor-count circuit synthesis",
        "published": "2008-04-14T02:40:31Z",
        "link": "http://arxiv.org/abs/0804.2095v1",
        "categories": [
            "cs.LO",
            "cs.CE",
            "cs.DM",
            "cs.PL"
        ]
    },
    {
        "title": "Reasoning in Abella about Structural Operational Semantics   Specifications",
        "authors": [
            "Andrew Gacek",
            "Dale Miller",
            "Gopalan Nadathur"
        ],
        "summary": "The approach to reasoning about structural operational semantics style specifications supported by the Abella system is discussed. This approach uses lambda tree syntax to treat object language binding and encodes binding related properties in generic judgments. Further, object language specifications are embedded directly into the reasoning framework through recursive definitions. The treatment of binding via generic judgments implicitly enforces distinctness and atomicity in the names used for bound variables. These properties must, however, be made explicit in reasoning tasks. This objective can be achieved by allowing recursive definitions to also specify generic properties of atomic predicates. The utility of these various logical features in the Abella system is demonstrated through actual reasoning tasks. Brief comparisons with a few other logic based approaches are also made.",
        "published": "2008-04-24T15:22:02Z",
        "link": "http://arxiv.org/abs/0804.3914v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Design and Implementation of a Tracer Driver: Easy and Efficient Dynamic   Analyses of Constraint Logic Programs",
        "authors": [
            "Ludovic Langevine",
            "Mireille Ducasse"
        ],
        "summary": "Tracers provide users with useful information about program executions. In this article, we propose a ``tracer driver''. From a single tracer, it provides a powerful front-end enabling multiple dynamic analysis tools to be easily implemented, while limiting the overhead of the trace generation. The relevant execution events are specified by flexible event patterns and a large variety of trace data can be given either systematically or ``on demand''. The proposed tracer driver has been designed in the context of constraint logic programming; experiments have been made within GNU-Prolog. Execution views provided by existing tools have been easily emulated with a negligible overhead. Experimental measures show that the flexibility and power of the described architecture lead to good performance. The tracer driver overhead is inversely proportional to the average time between two traced events. Whereas the principles of the tracer driver are independent of the traced programming language, it is best suited for high-level languages, such as constraint logic programming, where each traced execution event encompasses numerous low-level execution steps. Furthermore, constraint logic programming is especially hard to debug. The current environments do not provide all the useful dynamic analysis tools. They can significantly benefit from our tracer driver which enables dynamic analyses to be integrated at a very low cost.",
        "published": "2008-04-25T14:05:36Z",
        "link": "http://arxiv.org/abs/0804.4116v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5; D.3.2"
        ]
    },
    {
        "title": "The Complexity of Coverage",
        "authors": [
            "Krishnendu Chatterjee",
            "Luca de Alfaro",
            "Rupak Majumdar"
        ],
        "summary": "We study the problem of generating a test sequence that achieves maximal coverage for a reactive system under test. We formulate the problem as a repeated game between the tester and the system, where the system state space is partitioned according to some coverage criterion and the objective of the tester is to maximize the set of partitions (or coverage goals) visited during the game. We show the complexity of the maximal coverage problem for non-deterministic systems is PSPACE-complete, but is NP-complete for deterministic systems. For the special case of non-deterministic systems with a re-initializing ``reset'' action, which represent running a new test input on a re-initialized system, we show that the complexity is again co-NP-complete. Our proof technique for reset games uses randomized testing strategies that circumvent the exponentially large memory requirement in the deterministic case.",
        "published": "2008-04-29T04:26:08Z",
        "link": "http://arxiv.org/abs/0804.4525v1",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Event Synchronization by Lightweight Message Passing",
        "authors": [
            "Avik Chaudhuri"
        ],
        "summary": "Concurrent ML's events and event combinators facilitate modular concurrent programming with first-class synchronization abstractions. A standard implementation of these abstractions relies on fairly complex manipulations of first-class continuations in the underlying language. In this paper, we present a lightweight implementation of these abstractions in Concurrent Haskell, a language that already provides first-order message passing. At the heart of our implementation is a new distributed synchronization protocol. In contrast with several previous translations of event abstractions in concurrent languages, we remain faithful to the standard semantics for events and event combinators; for example, we retain the symmetry of $\\mathtt{choose}$ for expressing selective communication.",
        "published": "2008-05-27T01:31:05Z",
        "link": "http://arxiv.org/abs/0805.4029v1",
        "categories": [
            "cs.PL",
            "cs.DC",
            "D.3.3; D.1.3; F.3.3"
        ]
    },
    {
        "title": "Logical Reasoning for Higher-Order Functions with Local State",
        "authors": [
            "Nobuko Yoshida",
            "Kohei Honda",
            "Martin Berger"
        ],
        "summary": "We introduce an extension of Hoare logic for call-by-value higher-order functions with ML-like local reference generation. Local references may be generated dynamically and exported outside their scope, may store higher-order functions and may be used to construct complex mutable data structures. This primitive is captured logically using a predicate asserting reachability of a reference name from a possibly higher-order datum and quantifiers over hidden references. We explore the logic's descriptive and reasoning power with non-trivial programming examples combining higher-order procedures and dynamically generated local state. Axioms for reachability and local invariant play a central role for reasoning about the examples.",
        "published": "2008-06-15T14:43:25Z",
        "link": "http://arxiv.org/abs/0806.2448v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.3; D.3.2; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "Data-Oblivious Stream Productivity",
        "authors": [
            "Joerg Endrullis",
            "Clemens Grabmayer",
            "Dimitri Hendriks"
        ],
        "summary": "We are concerned with demonstrating productivity of specifications of infinite streams of data, based on orthogonal rewrite rules. In general, this property is undecidable, but for restricted formats computable sufficient conditions can be obtained. The usual analysis disregards the identity of data, thus leading to approaches that we call data-oblivious. We present a method that is provably optimal among all such data-oblivious approaches. This means that in order to improve on the algorithm in this paper one has to proceed in a data-aware fashion.",
        "published": "2008-06-16T22:02:56Z",
        "link": "http://arxiv.org/abs/0806.2680v5",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "An overview of QML with a concrete implementation in Haskell",
        "authors": [
            "Jonathan Grattage"
        ],
        "summary": "This paper gives an introduction to and overview of the functional quantum programming language QML. The syntax of this language is defined and explained, along with a new QML definition of the quantum teleport algorithm. The categorical operational semantics of QML is also briefly introduced, in the form of annotated quantum circuits. This definition leads to a denotational semantics, given in terms of superoperators. Finally, an implementation in Haskell of the semantics for QML is presented as a compiler. The compiler takes QML programs as input, which are parsed into a Haskell datatype. The output from the compiler is either a quantum circuit (operational), an isometry (pure denotational) or a superoperator (impure denotational). Orthogonality judgements and problems with coproducts in QML are also discussed.",
        "published": "2008-06-17T10:02:04Z",
        "link": "http://arxiv.org/abs/0806.2735v2",
        "categories": [
            "quant-ph",
            "cs.PL"
        ]
    },
    {
        "title": "Separability in the Ambient Logic",
        "authors": [
            "Daniel Hirschkoff",
            "Etienne Lozes",
            "Davide Sangiorgi"
        ],
        "summary": "The \\it{Ambient Logic} (AL) has been proposed for expressing properties of process mobility in the calculus of Mobile Ambients (MA), and as a basis for query languages on semistructured data. We study some basic questions concerning the discriminating power of AL, focusing on the equivalence on processes induced by the logic $(=_L>)$. As underlying calculi besides MA we consider a subcalculus in which an image-finiteness condition holds and that we prove to be Turing complete. Synchronous variants of these calculi are studied as well. In these calculi, we provide two operational characterisations of $_=L$: a coinductive one (as a form of bisimilarity) and an inductive one (based on structual properties of processes). After showing $_=L$ to be stricly finer than barbed congruence, we establish axiomatisations of $_=L$ on the subcalculus of MA (both the asynchronous and the synchronous version), enabling us to relate $_=L$ to structural congruence. We also present some (un)decidability results that are related to the above separation properties for AL: the undecidability of $_=L$ on MA and its decidability on the subcalculus.",
        "published": "2008-06-24T10:00:00Z",
        "link": "http://arxiv.org/abs/0806.3849v2",
        "categories": [
            "cs.LO",
            "cs.MA",
            "cs.PL",
            "F.3.2; F.4.1"
        ]
    },
    {
        "title": "Concept-Oriented Programming",
        "authors": [
            "Alexandr Savinov"
        ],
        "summary": "Object-oriented programming (OOP) is aimed at describing the structure and behaviour of objects by hiding the mechanism of their representation and access in primitive references. In this article we describe an approach, called concept-oriented programming (COP), which focuses on modelling references assuming that they also possess application-specific structure and behaviour accounting for a great deal or even most of the overall program complexity. References in COP are completely legalized and get the same status as objects while the functions are distributed among both objects and references. In order to support this design we introduce a new programming construct, called concept, which generalizes conventional classes and concept inclusion relation generalizing class inheritance. The main advantage of COP is that it allows programmers to describe two sides of any program: explicitly used functions of objects and intermediate functionality of references having cross-cutting nature and executed implicitly behind the scenes during object access.",
        "published": "2008-06-29T10:56:41Z",
        "link": "http://arxiv.org/abs/0806.4746v2",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Flux: FunctionaL Updates for XML (extended report)",
        "authors": [
            "James Cheney"
        ],
        "summary": "XML database query languages have been studied extensively, but XML database updates have received relatively little attention, and pose many challenges to language design. We are developing an XML update language called Flux, which stands for FunctionaL Updates for XML, drawing upon ideas from functional programming languages. In prior work, we have introduced a core language for Flux with a clear operational semantics and a sound, decidable static type system based on regular expression types.   Our initial proposal had several limitations. First, it lacked support for recursive types or update procedures. Second, although a high-level source language can easily be translated to the core language, it is difficult to propagate meaningful type errors from the core language back to the source. Third, certain updates are well-formed yet contain path errors, or ``dead'' subexpressions which never do any useful work. It would be useful to detect path errors, since they often represent errors or optimization opportunities.   In this paper, we address all three limitations. Specifically, we present an improved, sound type system that handles recursion. We also formalize a source update language and give a translation to the core language that preserves and reflects typability. We also develop a path-error analysis (a form of dead-code analysis) for updates.",
        "published": "2008-07-08T17:10:51Z",
        "link": "http://arxiv.org/abs/0807.1211v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "D.3.1; H.2.3"
        ]
    },
    {
        "title": "A Non-Termination Criterion for Binary Constraint Logic Programs",
        "authors": [
            "Etienne Payet",
            "Fred Mesnard"
        ],
        "summary": "On the one hand, termination analysis of logic programs is now a fairly established research topic within the logic programming community. On the other hand, non-termination analysis seems to remain a much less attractive subject. If we divide this line of research into two kinds of approaches: dynamic versus static analysis, this paper belongs to the latter. It proposes a criterion for detecting non-terminating atomic queries with respect to binary CLP rules, which strictly generalizes our previous works on this subject. We give a generic operational definition and an implemented logical form of this criterion. Then we show that the logical form is correct and complete with respect to the operational definition.",
        "published": "2008-07-22T13:51:33Z",
        "link": "http://arxiv.org/abs/0807.3451v3",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Quantifying Timing Leaks and Cost Optimisation",
        "authors": [
            "Alessandra Di Pierro",
            "Chris Hankin",
            "Herbert Wiklicky"
        ],
        "summary": "We develop a new notion of security against timing attacks where the attacker is able to simultaneously observe the execution time of a program and the probability of the values of low variables. We then show how to measure the security of a program with respect to this notion via a computable estimate of the timing leakage and use this estimate for cost optimisation.",
        "published": "2008-07-24T13:17:19Z",
        "link": "http://arxiv.org/abs/0807.3879v1",
        "categories": [
            "cs.CR",
            "cs.PL"
        ]
    },
    {
        "title": "Unfolding in CHR",
        "authors": [
            "Maurizio Gabbrielli",
            "Maria Chiara Meo",
            "Paolo Tacchella"
        ],
        "summary": "Program transformation is an appealing technique which allows to improve run-time efficiency, space-consumption and more generally to optimize a given program. Essentially it consists of a sequence of syntactic program manipulations which preserves some kind of semantic equivalence. One of the basic operations which is used by most program transformation systems is unfolding which consists in the replacement of a procedure call by its definition. While there is a large body of literature on transformation and unfolding of sequential programs, very few papers have addressed this issue for concurrent languages and, to the best of our knowledge, no other has considered unfolding of CHR programs.   This paper defines a correct unfolding system for CHR programs. We define an unfolding rule, show its correctness and discuss some conditions which can be used to delete an unfolded rule while preserving the program meaning. We prove that confluence and termination properties are preserved by the above transformations.",
        "published": "2008-07-25T15:21:46Z",
        "link": "http://arxiv.org/abs/0807.3979v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Logic Engines as Interactors",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "We introduce a new programming language construct, Interactors, supporting the agent-oriented view that programming is a dialog between simple, self-contained, autonomous building blocks.   We define Interactors as an abstraction of answer generation and refinement in Logic Engines resulting in expressive language extension and metaprogramming patterns, including emulation of Prolog's dynamic database.   A mapping between backtracking based answer generation in the callee and \"forward\" recursion in the caller enables interaction between different branches of the callee's search process and provides simplified design patterns for algorithms involving combinatorial generation and infinite answer streams.   Interactors extend language constructs like Ruby, Python and C#'s multiple coroutining block returns through yield statements and they can emulate the action of monadic constructs and catamorphisms in functional languages.   Keywords: generalized iterators, logic engines, agent oriented programming language constructs, interoperation with stateful objects, metaprogramming",
        "published": "2008-08-05T05:48:32Z",
        "link": "http://arxiv.org/abs/0808.0556v1",
        "categories": [
            "cs.PL",
            "cs.MA"
        ]
    },
    {
        "title": "Coinductive big-step operational semantics",
        "authors": [
            "Xavier Leroy",
            "Hervé Grall"
        ],
        "summary": "Using a call-by-value functional language as an example, this article illustrates the use of coinductive definitions and proofs in big-step operational semantics, enabling it to describe diverging evaluations in addition to terminating evaluations. We formalize the connections between the coinductive big-step semantics and the standard small-step semantics, proving that both semantics are equivalent. We then study the use of coinductive big-step semantics in proofs of type soundness and proofs of semantic preservation for compilers. A methodological originality of this paper is that all results have been proved using the Coq proof assistant. We explain the proof-theoretic presentation of coinductive definitions and proofs offered by Coq, and show that it facilitates the discovery and the presentation of the results.",
        "published": "2008-08-05T14:47:32Z",
        "link": "http://arxiv.org/abs/0808.0586v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Declarative Combinatorics: Isomorphisms, Hylomorphisms and Hereditarily   Finite Data Types in Haskell",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "This paper is an exploration in a functional programming framework of {\\em isomorphisms} between elementary data types (natural numbers, sets, multisets, finite functions, permutations binary decision diagrams, graphs, hypergraphs, parenthesis languages, dyadic rationals, primes, DNA sequences etc.) and their extension to hereditarily finite universes through {\\em hylomorphisms} derived from {\\em ranking/unranking} and {\\em pairing/unpairing} operations.   An embedded higher order {\\em combinator language} provides any-to-any encodings automatically.   Besides applications to experimental mathematics, a few examples of ``free algorithms'' obtained by transferring operations between data types are shown. Other applications range from stream iterators on combinatorial objects to self-delimiting codes, succinct data representations and generation of random instances.   The paper covers 59 data types and, through the use of the embedded combinator language, provides 3540 distinct bijective transformations between them.   The self-contained source code of the paper, as generated from a literate Haskell program, is available at \\url{http://logic.csci.unt.edu/tarau/research/2008/fISO.zip}.   {\\bf Keywords}: Haskell data representations, data type isomorphisms, declarative combinatorics, computational mathematics, Ackermann encoding, G\\\"{o}del numberings, arithmetization, ranking/unranking, hereditarily finite sets, functions and permutations, encodings of binary decision diagrams, dyadic rationals, DNA encodings",
        "published": "2008-08-21T16:47:38Z",
        "link": "http://arxiv.org/abs/0808.2953v4",
        "categories": [
            "cs.PL",
            "cs.DS"
        ]
    },
    {
        "title": "Proving Noninterference by a Fully Complete Translation to the Simply   Typed lambda-calculus",
        "authors": [
            "Naokata Shikuma",
            "Atsushi Igarashi"
        ],
        "summary": "Tse and Zdancewic have formalized the notion of noninterference for Abadi et al.'s DCC in terms of logical relations and given a proof of noninterference by reduction to parametricity of System F. Unfortunately, their proof contains errors in a key lemma that their translation from DCC to System F preserves the logical relations defined for both calculi. In fact, we have found a counterexample for it. In this article, instead of DCC, we prove noninterference for sealing calculus, a new variant of DCC, by reduction to the basic lemma of a logical relation for the simply typed lambda-calculus, using a fully complete translation to the simply typed lambda-calculus. Full completeness plays an important role in showing preservation of the two logical relations through the translation. Also, we investigate relationship among sealing calculus, DCC, and an extension of DCC by Tse and Zdancewic and show that the first and the last of the three are equivalent.",
        "published": "2008-08-25T06:56:05Z",
        "link": "http://arxiv.org/abs/0808.3307v2",
        "categories": [
            "cs.PL",
            "cs.CR",
            "D.3.1; F.3.2; F.3.3"
        ]
    },
    {
        "title": "Realizing Fast, Scalable and Reliable Scientific Computations in Grid   Environments",
        "authors": [
            "Yong Zhao",
            "Ioan Raicu",
            "Ian Foster",
            "Mihael Hategan",
            "Veronika Nefedova",
            "Mike Wilde"
        ],
        "summary": "The practical realization of managing and executing large scale scientific computations efficiently and reliably is quite challenging. Scientific computations often involve thousands or even millions of tasks operating on large quantities of data, such data are often diversely structured and stored in heterogeneous physical formats, and scientists must specify and run such computations over extended periods on collections of compute, storage and network resources that are heterogeneous, distributed and may change constantly. We present the integration of several advanced systems: Swift, Karajan, and Falkon, to address the challenges in running various large scale scientific applications in Grid environments. Swift is a parallel programming tool for rapid and reliable specification, execution, and management of large-scale science and engineering workflows. Swift consists of a simple scripting language called SwiftScript and a powerful runtime system that is based on the CoG Karajan workflow engine and integrates the Falkon light-weight task execution service that uses multi-level scheduling and a streamlined dispatcher. We showcase the scalability, performance and reliability of the integrated system using application examples drawn from astronomy, cognitive neuroscience and molecular dynamics, which all comprise large number of fine-grained jobs. We show that Swift is able to represent dynamic workflows whose structures can only be determined during runtime and reduce largely the code size of various workflow representations using SwiftScript; schedule the execution of hundreds of thousands of parallel computations via the Karajan engine; and achieve up to 90% reduction in execution time when compared to traditional batch schedulers.",
        "published": "2008-08-26T16:15:42Z",
        "link": "http://arxiv.org/abs/0808.3548v1",
        "categories": [
            "cs.DC",
            "cs.PL",
            "D.1.3; D.4.7"
        ]
    },
    {
        "title": "How applicable is Python as first computer language for teaching   programming in a pre-university educational environment, from a teacher's   point of view?",
        "authors": [
            "Fotis Georgatos"
        ],
        "summary": "This project report attempts to evaluate the educational properties of the Python computer language, in practice. This is done by examining computer language evolution history, related scientific background work, the existing educational research on computer languages and Python's experimental application in higher secondary education in Greece, during first half of year 2002. This Thesis Report was delivered in advance of a thesis defense for a Masters/Doctorandus (MSc/Drs) title with the Amstel Institute/Universiteit van Amsterdam, during the same year.",
        "published": "2008-09-09T14:39:57Z",
        "link": "http://arxiv.org/abs/0809.1437v1",
        "categories": [
            "cs.PL",
            "cs.CY",
            "D.3; K.3.2; I.2.6"
        ]
    },
    {
        "title": "Mechanistic Behavior of Single-Pass Instruction Sequences",
        "authors": [
            "Jan A. Bergstra",
            "Mark B. van der Zwaag"
        ],
        "summary": "Earlier work on program and thread algebra detailed the functional, observable behavior of programs under execution. In this article we add the modeling of unobservable, mechanistic processing, in particular processing due to jump instructions. We model mechanistic processing preceding some further behavior as a delay of that behavior; we borrow a unary delay operator from discrete time process algebra. We define a mechanistic improvement ordering on threads and observe that some threads do not have an optimal implementation.",
        "published": "2008-09-26T13:57:36Z",
        "link": "http://arxiv.org/abs/0809.4635v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.4; F.3.2; F.3.3"
        ]
    },
    {
        "title": "Optimizing Binary Code Produced by Valgrind (Project Report on Virtual   Execution Environments Course - AVExe)",
        "authors": [
            "Filipe Cabecinhas",
            "Nuno Lopes",
            "Renato Crisostomo",
            "Luis Veiga"
        ],
        "summary": "Valgrind is a widely used framework for dynamic binary instrumentation and its mostly known by its memcheck tool. Valgrind's code generation module is far from producing optimal code. In addition it has many backends for different CPU architectures, which difficults code optimization in an architecture independent way. Our work focused on identifying sub-optimal code produced by Valgrind and optimizing it.",
        "published": "2008-10-02T09:41:52Z",
        "link": "http://arxiv.org/abs/0810.0372v1",
        "categories": [
            "cs.PL",
            "cs.OS"
        ]
    },
    {
        "title": "Definition and Implementation of a Points-To Analysis for C-like   Languages",
        "authors": [
            "Stefano Soffia"
        ],
        "summary": "The points-to problem is the problem of determining the possible run-time targets of pointer variables and is usually considered part of the more general aliasing problem, which consists in establishing whether and when different expressions can refer to the same memory address. Aliasing information is essential to every tool that needs to reason about the semantics of programs. However, due to well-known undecidability results, for all interesting languages that admit aliasing, the exact solution of nontrivial aliasing problems is not generally computable. This work focuses on approximated solutions to this problem by presenting a store-based, flow-sensitive points-to analysis, for applications in the field of automated software verification. In contrast to software testing procedures, which heuristically check the program against a finite set of executions, the methods considered in this work are static analyses, where the computed results are valid for all the possible executions of the analyzed program. We present a simplified programming language and its execution model; then an approximated execution model is developed using the ideas of abstract interpretation theory. Finally, the soundness of the approximation is formally proved. The aim of developing a realistic points-to analysis is pursued by presenting some extensions to the initial simplified model and discussing the correctness of their formulation. This work contains original contributions to the issue of points-to analysis, as it provides a formulation of a filter operation on the points-to abstract domain and a formal proof of the soundness of the defined abstract operations: these, as far as we now, are lacking from the previous literature.",
        "published": "2008-10-04T10:34:09Z",
        "link": "http://arxiv.org/abs/0810.0753v1",
        "categories": [
            "cs.PL",
            "F.3.1"
        ]
    },
    {
        "title": "On the expressiveness of single-pass instruction sequences",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "We perceive programs as single-pass instruction sequences. A single-pass instruction sequence under execution is considered to produce a behaviour to be controlled by some execution environment. Threads as considered in basic thread algebra model such behaviours. We show that all regular threads, i.e. threads that can only be in a finite number of states, can be produced by single-pass instruction sequences without jump instructions if use can be made of Boolean registers. We also show that, in the case where goto instructions are used instead of jump instructions, a bound to the number of labels restricts the expressiveness.",
        "published": "2008-10-07T06:51:53Z",
        "link": "http://arxiv.org/abs/0810.1106v3",
        "categories": [
            "cs.PL",
            "D.1.4; D.3.3; F.1.1; F.3.3"
        ]
    },
    {
        "title": "Periodic Single-Pass Instruction Sequences",
        "authors": [
            "Jan A. Bergstra",
            "Alban Ponse"
        ],
        "summary": "A program is a finite piece of data that produces a (possibly infinite) sequence of primitive instructions. From scratch we develop a linear notation for sequential, imperative programs, using a familiar class of primitive instructions and so-called repeat instructions, a particular type of control instructions. The resulting mathematical structure is a semigroup. We relate this set of programs to program algebra (PGA) and show that a particular subsemigroup is a carrier for PGA by providing axioms for single-pass congruence, structural congruence, and thread extraction. This subsemigroup characterizes periodic single-pass instruction sequences and provides a direct basis for PGA's toolset.",
        "published": "2008-10-07T13:55:21Z",
        "link": "http://arxiv.org/abs/0810.1151v2",
        "categories": [
            "cs.PL",
            "D.3.1; F.3.2"
        ]
    },
    {
        "title": "A sound spatio-temporal Hoare logic for the verification of structured   interactive programs with registers and voices",
        "authors": [
            "Cezara Dragoi",
            "Gheorghe Stefanescu"
        ],
        "summary": "Interactive systems with registers and voices (shortly, \"rv-systems\") are a model for interactive computing obtained closing register machines with respect to a space-time duality transformation (\"voices\" are the time-dual counterparts of \"registers\"). In the same vain, AGAPIA v0.1, a structured programming language for rv-systems, is the space-time dual closure of classical while programs (over a specific type of data). Typical AGAPIA programs describe open processes located at various sites and having their temporal windows of adequate reaction to the environment. The language naturally supports process migration, structured interaction, and deployment of components on heterogeneous machines.   In this paper a sound Hoare-like spatio-temporal logic for the verification of AGAPIA v0.1 programs is introduced. As a case study, a formal verification proof of a popular distributed termination detection protocol is presented.",
        "published": "2008-10-19T00:05:20Z",
        "link": "http://arxiv.org/abs/0810.3332v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.1.2; F.3; D.2.4; D.3.2"
        ]
    },
    {
        "title": "A Call-Graph Profiler for GNU Octave",
        "authors": [
            "Muthiah Annamalai",
            "Leela Velusamy"
        ],
        "summary": "We report the design and implementation of a call-graph profiler for GNU Octave, a numerical computing platform. GNU Octave simplifies matrix computation for use in modeling or simulation. Our work provides a call-graph profiler, which is an improvement on the flat profiler. We elaborate design constraints of building a profiler for numerical computation, and benchmark the profiler by comparing it to the rudimentary timer start-stop (tic-toc) measurements, for a similar set of programs. The profiler code provides clean interfaces to internals of GNU Octave, for other (newer) profiling tools on GNU Octave.",
        "published": "2008-10-20T08:29:21Z",
        "link": "http://arxiv.org/abs/0810.3468v1",
        "categories": [
            "cs.PF",
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Binding bigraphs as symmetric monoidal closed theories",
        "authors": [
            "Tom Hirschowitz",
            "Aurélien Pardon"
        ],
        "summary": "Milner's bigraphs are a general framework for reasoning about distributed and concurrent programming languages. Notably, it has been designed to encompass both the pi-calculus and the Ambient calculus. This paper is only concerned with bigraphical syntax: given what we here call a bigraphical signature K, Milner constructs a (pre-) category of bigraphs BBig(K), whose main features are (1) the presence of relative pushouts (RPOs), which makes them well-behaved w.r.t. bisimulations, and that (2) the so-called structural equations become equalities. Examples of the latter include, e.g., in pi and Ambient, renaming of bound variables, associativity and commutativity of parallel composition, or scope extrusion for restricted names. Also, bigraphs follow a scoping discipline ensuring that, roughly, bound variables never escape their scope. Here, we reconstruct bigraphs using a standard categorical tool: symmetric monoidal closed (SMC) theories. Our theory enforces the same scoping discipline as bigraphs, as a direct property of SMC structure. Furthermore, it elucidates the slightly mysterious status of so-called links in bigraphs. Finally, our category is also considerably larger than the category of bigraphs, notably encompassing in the same framework terms and a flexible form of higher-order contexts.",
        "published": "2008-10-24T09:33:08Z",
        "link": "http://arxiv.org/abs/0810.4419v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Logics for XML",
        "authors": [
            "Pierre Geneves"
        ],
        "summary": "This thesis describes the theoretical and practical foundations of a system for the static analysis of XML processing languages. The system relies on a fixpoint temporal logic with converse, derived from the mu-calculus, where models are finite trees. This calculus is expressive enough to capture regular tree types along with multi-directional navigation in trees, while having a single exponential time complexity. Specifically the decidability of the logic is proved in time 2^O(n) where n is the size of the input formula.   Major XML concepts are linearly translated into the logic: XPath navigation and node selection semantics, and regular tree languages (which include DTDs and XML Schemas). Based on these embeddings, several problems of major importance in XML applications are reduced to satisfiability of the logic. These problems include XPath containment, emptiness, equivalence, overlap, coverage, in the presence or absence of regular tree type constraints, and the static type-checking of an annotated query.   The focus is then given to a sound and complete algorithm for deciding the logic, along with a detailed complexity analysis, and crucial implementation techniques for building an effective solver. Practical experiments using a full implementation of the system are presented. The system appears to be efficient in practice for several realistic scenarios.   The main application of this work is a new class of static analyzers for programming languages using both XPath expressions and XML type annotations (input and output). Such analyzers allow to ensure at compile-time valuable properties such as type-safety and optimizations, for safer and more efficient XML processing.",
        "published": "2008-10-24T13:40:11Z",
        "link": "http://arxiv.org/abs/0810.4460v2",
        "categories": [
            "cs.PL",
            "cs.DB",
            "cs.LO",
            "D.3.0; D.3.1; D.3.4; E.1; F.3.1; F.3.2; F.4.1; F.4.3; H.2.3; I.2.4;\n  I.7.2"
        ]
    },
    {
        "title": "The Mob core language and abstract machine (rev 0.2)",
        "authors": [
            "Herve Paulino",
            "Luis Lopes"
        ],
        "summary": "Most current mobile agent systems are based on programming languages whose semantics are difficult to prove correct as they lack an adequate underlying formal theory. In recent years, the development of the theory of concurrent systems, namely of process calculi, has allowed for the first time the modeling of mobile agent systems.Languages directly based on process calculi are, however, very low-level and it is desirable to provide the programmer with higher level abstractions, while keeping the semantics of the base calculus.   In this technical report we present the syntax and the semantics of a scripting language for programming mobile agents called Mob. We describe the language's syntax and semantics. Mob is service-oriented, meaning that agents act both as servers and as clients of services and that this coupling is done dynamically at run-time. The language is implemented on top of a process calculus which allows us to prove that the framework is sound by encoding its semantics into the underlying calculus. This provides a form of language security not available to other mobile agent languages developed using a more ah-doc approach.",
        "published": "2008-10-24T15:02:09Z",
        "link": "http://arxiv.org/abs/0810.4451v1",
        "categories": [
            "cs.PL",
            "cs.DC"
        ]
    },
    {
        "title": "Detection of parallel steps in programs with arrays",
        "authors": [
            "R. Nuriyev"
        ],
        "summary": "The problem of detecting of information and logically independent (DILD) steps in programs is a key for equivalent program transformations. Here we are considering the problem of independence of loop iterations, the concentration of massive data processing and hence the most challenge construction for parallelizing. We introduced a separated form of loops when loop's body is a sequence of procedures each of them are used array's elements selected in a previous procedure. We prove that any loop may be algorithmically represented in this form and number of such procedures is invariant. We show that for this form of loop the steps connections are determined with some integer equations and hence the independence problem is algorithmically unsolvable if index expressions are more complex than cubical. We suggest a modification of index semantics that made connection equations trivial and loops iterations can be executed in parallel.",
        "published": "2008-10-30T20:48:49Z",
        "link": "http://arxiv.org/abs/0810.5575v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Automatic Modular Abstractions for Linear Constraints",
        "authors": [
            "David Monniaux"
        ],
        "summary": "We propose a method for automatically generating abstract transformers for static analysis by abstract interpretation. The method focuses on linear constraints on programs operating on rational, real or floating-point variables and containing linear assignments and tests. In addition to loop-free code, the same method also applies for obtaining least fixed points as functions of the precondition, which permits the analysis of loops and recursive functions. Our algorithms are based on new quantifier elimination and symbolic manipulation techniques. Given the specification of an abstract domain, and a program block, our method automatically outputs an implementation of the corresponding abstract transformer. It is thus a form of program transformation. The motivation of our work is data-flow synchronous programming languages, used for building control-command embedded systems, but it also applies to imperative and functional programming.",
        "published": "2008-11-02T14:47:44Z",
        "link": "http://arxiv.org/abs/0811.0166v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "Instruction sequences for the production of processes",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "summary": "Single-pass instruction sequences under execution are considered to produce behaviours to be controlled by some execution environment. Threads as considered in thread algebra model such behaviours: upon each action performed by a thread, a reply from its execution environment determines how the thread proceeds. Threads in turn can be looked upon as producing processes as considered in process algebra. We show that, by apposite choice of basic instructions, all processes that can only be in a finite number of states can be produced by single-pass instruction sequences.",
        "published": "2008-11-04T07:24:12Z",
        "link": "http://arxiv.org/abs/0811.0436v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.4; F.1.1; F.1.2; F.3.2"
        ]
    },
    {
        "title": "Persistent Queries",
        "authors": [
            "Andreas Blass",
            "Yuri Gurevich"
        ],
        "summary": "We propose a syntax and semantics for interactive abstract state machines to deal with the following situation. A query is issued during a certain step, but the step ends before any reply is received. Later, a reply arrives, and later yet the algorithm makes use of this reply. By a persistent query, we mean a query for which a late reply might be used. Syntactically, our proposal involves issuing, along with a persistent query, a location where a late reply is to be stored. Semantically, it involves only a minor modification of the existing theory of interactive small-step abstract state machines.",
        "published": "2008-11-05T21:10:33Z",
        "link": "http://arxiv.org/abs/0811.0819v1",
        "categories": [
            "cs.PL",
            "cs.LO"
        ]
    },
    {
        "title": "Compactly accessible categories and quantum key distribution",
        "authors": [
            "Chris Heunen"
        ],
        "summary": "Compact categories have lately seen renewed interest via applications to quantum physics. Being essentially finite-dimensional, they cannot accomodate (co)limit-based constructions. For example, they cannot capture protocols such as quantum key distribution, that rely on the law of large numbers. To overcome this limitation, we introduce the notion of a compactly accessible category, relying on the extra structure of a factorisation system. This notion allows for infinite dimension while retaining key properties of compact categories: the main technical result is that the choice-of-duals functor on the compact part extends canonically to the whole compactly accessible category. As an example, we model a quantum key distribution protocol and prove its correctness categorically.",
        "published": "2008-11-13T13:58:48Z",
        "link": "http://arxiv.org/abs/0811.2113v3",
        "categories": [
            "cs.LO",
            "cs.PL",
            "quant-ph",
            "F.3.2"
        ]
    },
    {
        "title": "A Transformation--Based Approach for the Design of Parallel/Distributed   Scientific Software: the FFT",
        "authors": [
            "Harry B. Hunt",
            "Lenore R. Mullin",
            "Daniel J. Rosenkrantz",
            "James E. Raynolds"
        ],
        "summary": "We describe a methodology for designing efficient parallel and distributed scientific software. This methodology utilizes sequences of mechanizable algebra--based optimizing transformations. In this study, we apply our methodology to the FFT, starting from a high--level algebraic algorithm description. Abstract multiprocessor plans are developed and refined to specify which computations are to be done by each processor. Templates are then created that specify the locations of computations and data on the processors, as well as data flow among processors. Templates are developed in both the MPI and OpenMP programming styles.   Preliminary experiments comparing code constructed using our methodology with code from several standard scientific libraries show that our code is often competitive and sometimes performs better. Interestingly, our code handled a larger range of problem sizes on one target architecture.",
        "published": "2008-11-15T22:32:59Z",
        "link": "http://arxiv.org/abs/0811.2535v1",
        "categories": [
            "cs.SE",
            "cs.PL"
        ]
    },
    {
        "title": "A Rational Deconstruction of Landin's SECD Machine with the J Operator",
        "authors": [
            "Olivier Danvy",
            "Kevin Millikin"
        ],
        "summary": "Landin's SECD machine was the first abstract machine for applicative expressions, i.e., functional programs. Landin's J operator was the first control operator for functional languages, and was specified by an extension of the SECD machine. We present a family of evaluation functions corresponding to this extension of the SECD machine, using a series of elementary transformations (transformation into continu-ation-passing style (CPS) and defunctionalization, chiefly) and their left inverses (transformation into direct style and refunctionalization). To this end, we modernize the SECD machine into a bisimilar one that operates in lockstep with the original one but that (1) does not use a data stack and (2) uses the caller-save rather than the callee-save convention for environments. We also identify that the dump component of the SECD machine is managed in a callee-save way. The caller-save counterpart of the modernized SECD machine precisely corresponds to Thielecke's double-barrelled continuations and to Felleisen's encoding of J in terms of call/cc. We then variously characterize the J operator in terms of CPS and in terms of delimited-control operators in the CPS hierarchy. As a byproduct, we also present several reduction semantics for applicative expressions with the J operator, based on Curien's original calculus of explicit substitutions. These reduction semantics mechanically correspond to the modernized versions of the SECD machine and to the best of our knowledge, they provide the first syntactic theories of applicative expressions with the J operator.",
        "published": "2008-11-19T22:31:34Z",
        "link": "http://arxiv.org/abs/0811.3231v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.1, D.3.3, F.1.1"
        ]
    },
    {
        "title": "Ensuring Query Compatibility with Evolving XML Schemas",
        "authors": [
            "Pierre Genevès",
            "Nabil Layaïda",
            "Vincent Quint"
        ],
        "summary": "During the life cycle of an XML application, both schemas and queries may change from one version to another. Schema evolutions may affect query results and potentially the validity of produced data. Nowadays, a challenge is to assess and accommodate the impact of theses changes in rapidly evolving XML applications.   This article proposes a logical framework and tool for verifying forward/backward compatibility issues involving schemas and queries. First, it allows analyzing relations between schemas. Second, it allows XML designers to identify queries that must be reformulated in order to produce the expected results across successive schema versions. Third, it allows examining more precisely the impact of schema changes over queries, therefore facilitating their reformulation.",
        "published": "2008-11-26T14:37:01Z",
        "link": "http://arxiv.org/abs/0811.4324v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.0; D.3.1; D.3.4; E.1; F.3.1; F.3.2; F.4.1; F.4.3; H.2.3; I.2.4;\n  I.7.2"
        ]
    },
    {
        "title": "Provenance Traces",
        "authors": [
            "James Cheney",
            "Umut Acar",
            "Amal Ahmed"
        ],
        "summary": "Provenance is information about the origin, derivation, ownership, or history of an object. It has recently been studied extensively in scientific databases and other settings due to its importance in helping scientists judge data validity, quality and integrity. However, most models of provenance have been stated as ad hoc definitions motivated by informal concepts such as \"comes from\", \"influences\", \"produces\", or \"depends on\". These models lack clear formalizations describing in what sense the definitions capture these intuitive concepts. This makes it difficult to compare approaches, evaluate their effectiveness, or argue about their validity.   We introduce provenance traces, a general form of provenance for the nested relational calculus (NRC), a core database query language. Provenance traces can be thought of as concrete data structures representing the operational semantics derivation of a computation; they are related to the traces that have been used in self-adjusting computation, but differ in important respects. We define a tracing operational semantics for NRC queries that produces both an ordinary result and a trace of the execution. We show that three pre-existing forms of provenance for the NRC can be extracted from provenance traces. Moreover, traces satisfy two semantic guarantees: consistency, meaning that the traces describe what actually happened during execution, and fidelity, meaning that the traces \"explain\" how the expression would behave if the input were changed. These guarantees are much stronger than those contemplated for previous approaches to provenance; thus, provenance traces provide a general semantic foundation for comparing and unifying models of provenance in databases.",
        "published": "2008-12-02T18:17:23Z",
        "link": "http://arxiv.org/abs/0812.0564v1",
        "categories": [
            "cs.PL",
            "cs.DB"
        ]
    },
    {
        "title": "Justifications for Logic Programs under Answer Set Semantics",
        "authors": [
            "Enrico Pontelli",
            "Tran Cao Son",
            "Omar Elkhatib"
        ],
        "summary": "The paper introduces the notion of off-line justification for Answer Set Programming (ASP). Justifications provide a graph-based explanation of the truth value of an atom w.r.t. a given answer set. The paper extends also this notion to provide justification of atoms during the computation of an answer set (on-line justification), and presents an integration of on-line justifications within the computation model of Smodels. Off-line and on-line justifications provide useful tools to enhance understanding of ASP, and they offer a basic data structure to support methodologies and tools for debugging answer set programs. A preliminary implementation has been developed in ASP-PROLOG.   (To appear in Theory and Practice of Logic Programming (TPLP))",
        "published": "2008-12-03T20:10:00Z",
        "link": "http://arxiv.org/abs/0812.0790v1",
        "categories": [
            "cs.AI",
            "cs.PL"
        ]
    },
    {
        "title": "Control software analysis, part II: Closed-loop analysis",
        "authors": [
            "Eric Feron",
            "Fernando Alegre"
        ],
        "summary": "The analysis and proper documentation of the properties of closed-loop control software presents many distinct aspects from the analysis of the same software running open-loop. Issues of physical system representations arise, and it is desired that such representations remain independent from the representations of the control program. For that purpose, a concurrent program representation of the plant and the control processes is proposed, although the closed-loop system is sufficiently serialized to enable a sequential analysis. While dealing with closed-loop system properties, it is also shown by means of examples how special treatment of nonlinearities extends from the analysis of control specifications to code analysis.",
        "published": "2008-12-10T17:57:14Z",
        "link": "http://arxiv.org/abs/0812.1986v1",
        "categories": [
            "cs.SE",
            "cs.PL"
        ]
    },
    {
        "title": "New parallel programming language design: a bridge between brain models   and multi-core/many-core computers?",
        "authors": [
            "Gheorghe Stefanescu",
            "Camelia Chira"
        ],
        "summary": "The recurrent theme of this paper is that sequences of long temporal patterns as opposed to sequences of simple statements are to be fed into computation devices, being them (new proposed) models for brain activity or multi-core/many-core computers. In such models, parts of these long temporal patterns are already committed while other are predicted. This combination of matching patterns and making predictions appears as a key element in producing intelligent processing in brain models and getting efficient speculative execution on multi-core/many-core computers. A bridge between these far-apart models of computation could be provided by appropriate design of massively parallel, interactive programming languages. Agapia is a recently proposed language of this kind, where user controlled long high-level temporal structures occur at the interaction interfaces of processes. In this paper Agapia is used to link HTMs brain models with TRIPS multi-core/many-core architectures.",
        "published": "2008-12-15T22:55:19Z",
        "link": "http://arxiv.org/abs/0812.2926v1",
        "categories": [
            "cs.PL",
            "cs.AI"
        ]
    },
    {
        "title": "XML Static Analyzer User Manual",
        "authors": [
            "Pierre Geneves",
            "Nabil Layaida"
        ],
        "summary": "This document describes how to use the XML static analyzer in practice. It provides informal documentation for using the XML reasoning solver implementation. The solver allows automated verification of properties that are expressed as logical formulas over trees. A logical formula may for instance express structural constraints or navigation properties (like e.g. path existence and node selection) in finite trees. Logical formulas can be expressed using the syntax of XPath expressions, DTD, XML Schemas, and Relax NG definitions.",
        "published": "2008-12-18T15:22:46Z",
        "link": "http://arxiv.org/abs/0812.3550v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "cs.LO",
            "cs.SE",
            "D.3.0; D.3.1; D.3.4; E.1; F.3.1; F.3.2; F.4.1; F.4.3; H.2.3; I.2.4;\n  I.7.2"
        ]
    },
    {
        "title": "Formalizing common sense for scalable inconsistency-robust information   integration using Direct Logic(TM) reasoning and the Actor Model",
        "authors": [
            "Carl Hewitt"
        ],
        "summary": "Because contemporary large software systems are pervasively inconsistent, it is not safe to reason about them using classical logic. The goal of Direct Logic is to be a minimal fix to classical mathematical logic that meets the requirements of large-scale Internet applications (including sense making for natural language) by addressing the following issues: inconsistency robustness, contrapositive inference bug, and direct argumentation.   Direct Logic makes the following contributions over previous work:   * Direct Inference (no contrapositive bug for inference)   * Direct Argumentation (inference directly expressed)   * Inconsistency-robust deduction without artifices such as indices (labels) on propositions or restrictions on reiteration   * Intuitive inferences hold including the following:   * Boolean Equivalences   * Reasoning by splitting for disjunctive cases   * Soundness   * Inconsistency-robust Proof by Contradiction   Since the global state model of computation (first formalized by Turing) is inadequate to the needs of modern large-scale Internet applications the Actor Model was developed to meet this need. Using, the Actor Model, this paper proves that Logic Programming is not computationally universal in that there are computations that cannot be implemented using logical inference. Consequently the Logic Programming paradigm is strictly less general than the Procedural Embedding of Knowledge paradigm.",
        "published": "2008-12-28T21:37:23Z",
        "link": "http://arxiv.org/abs/0812.4852v103",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "A Simple, Linear-Time Algorithm for x86 Jump Encoding",
        "authors": [
            "Neil G. Dickson"
        ],
        "summary": "The problem of space-optimal jump encoding in the x86 instruction set, also known as branch displacement optimization, is described, and a linear-time algorithm is given that uses no complicated data structures, no recursion, and no randomization. The only assumption is that there are no array declarations whose size depends on the negative of the size of a section of code (Hyde 2006), which is reasonable for real code.",
        "published": "2008-12-29T21:07:52Z",
        "link": "http://arxiv.org/abs/0812.4973v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "On the Maximum Span of Fixed-Angle Chains",
        "authors": [
            "Nadia Benbernou",
            "Joseph O'Rourke"
        ],
        "summary": "Soss proved that it is NP-hard to find the maximum 2D span of a fixed-angle polygonal chain: the largest distance achievable between the endpoints in a planar embedding. These fixed-angle chains can serve as models of protein backbones. The corresponding problem in 3D is open. We show that three special cases of particular relevance to the protein model are solvable in polynomial time. When all link lengths and all angles are equal, the maximum 3D span is achieved in a flat configuration and can be computed in constant time. When all angles are equal and the chain is simple (non-self-crossing), the maximum flat span can be found in linear time. In 3D, when all angles are equal to 90 deg (but the link lengths arbitrary), the maximum 3D span is in general nonplanar but can be found in quadratic time.",
        "published": "2008-01-01T04:17:20Z",
        "link": "http://arxiv.org/abs/0801.0258v2",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "On sign conditions over real multivariate polynomials",
        "authors": [
            "Gabriela Jeronimo",
            "Daniel Perrucci",
            "Juan Sabia"
        ],
        "summary": "We present a new probabilistic algorithm to find a finite set of points intersecting the closure of each connected component of the realization of every sign condition over a family of real polynomials defining regular hypersurfaces that intersect transversally. This enables us to show a probabilistic procedure to list all feasible sign conditions over the polynomials. In addition, we extend these results to the case of closed sign conditions over an arbitrary family of real multivariate polynomials. The complexity bounds for these procedures improve the known ones.",
        "published": "2008-01-03T20:03:05Z",
        "link": "http://arxiv.org/abs/0801.0586v2",
        "categories": [
            "math.AG",
            "cs.CG",
            "cs.SC",
            "14P10; 14Q20; 68W30"
        ]
    },
    {
        "title": "Almost 2-SAT is Fixed-Parameter Tractable",
        "authors": [
            "Igor Razgon",
            "Barry O'Sullivan"
        ],
        "summary": "We consider the following problem. Given a 2-CNF formula, is it possible to remove at most $k$ clauses so that the resulting 2-CNF formula is satisfiable? This problem is known to different research communities in Theoretical Computer Science under the names 'Almost 2-SAT', 'All-but-$k$ 2-SAT', '2-CNF deletion', '2-SAT deletion'. The status of fixed-parameter tractability of this problem is a long-standing open question in the area of Parameterized Complexity. We resolve this open question by proposing an algorithm which solves this problem in $O(15^k*k*m^3)$ and thus we show that this problem is fixed-parameter tractable.",
        "published": "2008-01-08T19:04:14Z",
        "link": "http://arxiv.org/abs/0801.1300v4",
        "categories": [
            "cs.DS",
            "cs.CG",
            "cs.LO"
        ]
    },
    {
        "title": "Algorithms for eps-approximations of Terrains",
        "authors": [
            "Jeff M. Phillips"
        ],
        "summary": "Consider a point set D with a measure function w : D -> R. Let A be the set of subsets of D induced by containment in a shape from some geometric family (e.g. axis-aligned rectangles, half planes, balls, k-oriented polygons). We say a range space (D, A) has an eps-approximation P if max {R \\in A} | w(R \\cap P)/w(P) - w(R \\cap D)/w(D) | <= eps. We describe algorithms for deterministically constructing discrete eps-approximations for continuous point sets such as distributions or terrains. Furthermore, for certain families of subsets A, such as those described by axis-aligned rectangles, we reduce the size of the eps-approximations by almost a square root from O(1/eps^2 log 1/eps) to O(1/eps polylog 1/eps). This is often the first step in transforming a continuous problem into a discrete one for which combinatorial techniques can be applied. We describe applications of this result in geo-spatial analysis, biosurveillance, and sensor networks.",
        "published": "2008-01-18T01:19:44Z",
        "link": "http://arxiv.org/abs/0801.2793v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Shallow, Low, and Light Trees, and Tight Lower Bounds for Euclidean   Spanners",
        "authors": [
            "Yefim Dinitz",
            "Michael Elkin",
            "Shay Solomon"
        ],
        "summary": "We show that for every $n$-point metric space $M$ there exists a spanning tree $T$ with unweighted diameter $O(\\log n)$ and weight $\\omega(T) = O(\\log n) \\cdot \\omega(MST(M))$. Moreover, there is a designated point $rt$ such that for every point $v$, $dist_T(rt,v) \\le (1+\\epsilon) \\cdot dist_M(rt,v)$, for an arbitrarily small constant $\\epsilon > 0$. We extend this result, and provide a tradeoff between unweighted diameter and weight, and prove that this tradeoff is \\emph{tight up to constant factors} in the entire range of parameters. These results enable us to settle a long-standing open question in Computational Geometry. In STOC'95 Arya et al. devised a construction of Euclidean Spanners with unweighted diameter $O(\\log n)$ and weight $O(\\log n) \\cdot \\omega(MST(M))$. Ten years later in SODA'05 Agarwal et al. showed that this result is tight up to a factor of $O(\\log \\log n)$. We close this gap and show that the result of Arya et al. is tight up to constant factors.",
        "published": "2008-01-23T13:57:00Z",
        "link": "http://arxiv.org/abs/0801.3581v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2; F.2.3; G.2.2"
        ]
    },
    {
        "title": "Spanners of Additively Weighted Point Sets",
        "authors": [
            "Prosenjit Bose",
            "Paz Carmi",
            "Mathieu Couture"
        ],
        "summary": "We study the problem of computing geometric spanners for (additively) weighted point sets. A weighted point set is a set of pairs $(p,r)$ where $p$ is a point in the plane and $r$ is a real number. The distance between two points $(p_i,r_i)$ and $(p_j,r_j)$ is defined as $|p_ip_j|-r_i-r_j$. We show that in the case where all $r_i$ are positive numbers and $|p_ip_j|\\geq r_i+r_j$ for all $i,j$ (in which case the points can be seen as non-intersecting disks in the plane), a variant of the Yao graph is a $(1+\\epsilon)$-spanner that has a linear number of edges. We also show that the Additively Weighted Delaunay graph (the face-dual of the Additively Weighted Voronoi diagram) has constant spanning ratio. The straight line embedding of the Additively Weighted Delaunay graph may not be a plane graph. We show how to compute a plane embedding that also has a constant spanning ratio.",
        "published": "2008-01-25T19:43:09Z",
        "link": "http://arxiv.org/abs/0801.4013v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "A Class of Convex Polyhedra with Few Edge Unfoldings",
        "authors": [
            "Alex Benton",
            "Joseph O'Rourke"
        ],
        "summary": "We construct a sequence of convex polyhedra on n vertices with the property that, as n -> infinity, the fraction of its edge unfoldings that avoid overlap approaches 0, and so the fraction that overlap approaches 1. Nevertheless, each does have (several) nonoverlapping edge unfoldings.",
        "published": "2008-01-25T20:22:04Z",
        "link": "http://arxiv.org/abs/0801.4019v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "A Locked Orthogonal Tree",
        "authors": [
            "David Charlton",
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Gregory Price",
            "Yaa-Lirng Tu"
        ],
        "summary": "We give a counterexample to a conjecture of Poon [Poo06] that any orthogonal tree in two dimensions can always be flattened by a continuous motion that preserves edge lengths and avoids self-intersection. We show our example is locked by extending results on strongly locked self-touching linkages due to Connelly, Demaine and Rote [CDR02] to allow zero-length edges as defined in [ADG07], which may be of independent interest. Our results also yield a locked tree with only eleven edges, which is the smallest known example of a locked tree.",
        "published": "2008-01-29T00:39:37Z",
        "link": "http://arxiv.org/abs/0801.4405v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Untangling polygons and graphs",
        "authors": [
            "Josef Cibulka"
        ],
        "summary": "Untangling is a process in which some vertices of a planar graph are moved to obtain a straight-line plane drawing. The aim is to move as few vertices as possible. We present an algorithm that untangles the cycle graph C_n while keeping at least \\Omega(n^{2/3}) vertices fixed. For any graph G, we also present an upper bound on the number of fixed vertices in the worst case. The bound is a function of the number of vertices, maximum degree and diameter of G. One of its consequences is the upper bound O((n log n)^{2/3}) for all 3-vertex-connected planar graphs.",
        "published": "2008-02-10T09:28:15Z",
        "link": "http://arxiv.org/abs/0802.1312v2",
        "categories": [
            "cs.CG",
            "cs.DM"
        ]
    },
    {
        "title": "Guarding curvilinear art galleries with edge or mobile guards via   2-dominance of triangulation graphs",
        "authors": [
            "Menelaos I. Karavelas"
        ],
        "summary": "We consider the problem of monitoring an art gallery modeled as a polygon, the edges of which are arcs of curves, with edge or mobile guards. Our focus is on piecewise-convex polygons, i.e., polygons that are locally convex, except possibly at the vertices, and their edges are convex arcs. We transform the problem of monitoring a piecewise-convex polygon to the problem of 2-dominating a properly defined triangulation graph with edges or diagonals, where 2-dominance requires that every triangle in the triangulation graph has at least two of its vertices in its 2-dominating set. We show that $\\lfloor\\frac{n+1}{3}\\rfloor$ diagonal guards or $\\lfloor\\frac{2n+1}{5}\\rfloor$ edge guards are always sufficient and sometimes necessary, in order to 2-dominate a triangulation graph. Furthermore, we show how to compute: a diagonal 2-dominating set of size $\\lfloor\\frac{n+1}{3}\\rfloor$ in linear time, an edge 2-dominating set of size $\\lfloor\\frac{2n+1}{5}\\rfloor$ in $O(n^2)$ time, and an edge 2-dominating set of size $\\lfloor\\frac{3n}{7}\\rfloor$ in O(n) time. Based on the above-mentioned results, we prove that, for piecewise-convex polygons, we can compute: a mobile guard set of size $\\lfloor\\frac{n+1}{3}\\rfloor$ in $O(n\\log{}n)$ time, an edge guard set of size $\\lfloor\\frac{2n+1}{5}\\rfloor$ in $O(n^2)$ time, and an edge guard set of size $\\lfloor\\frac{3n}{7}\\rfloor$ in $O(n\\log{}n)$ time. Finally, we show that $\\lfloor\\frac{n}{3}\\rfloor$ mobile or $\\lceil\\frac{n}{3}\\rceil$ edge guards are sometimes necessary. When restricting our attention to monotone piecewise-convex polygons, the bounds mentioned above drop: $\\lceil\\frac{n+1}{4}\\rceil$ edge or mobile guards are always sufficient and sometimes necessary; such an edge or mobile guard set, of size at most $\\lceil\\frac{n+1}{4}\\rceil$, can be computed in O(n) time.",
        "published": "2008-02-11T00:40:37Z",
        "link": "http://arxiv.org/abs/0802.1361v2",
        "categories": [
            "cs.CG",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Minimal Committee Problem for Inconsistent Systems of Linear   Inequalities on the Plane",
        "authors": [
            "K. S. Kobylkin"
        ],
        "summary": "A representation of an arbitrary system of strict linear inequalities in R^n as a system of points is proposed. The representation is obtained by using a so-called polarity. Based on this representation an algorithm for constructing a committee solution of an inconsistent plane system of linear inequalities is given. A solution of two problems on minimal committee of a plane system is proposed. The obtained solutions to these problems can be found by means of the proposed algorithm.",
        "published": "2008-02-11T19:50:56Z",
        "link": "http://arxiv.org/abs/0802.1514v3",
        "categories": [
            "cs.DM",
            "cs.CG"
        ]
    },
    {
        "title": "Discrete Complex Structure on Surfel Surfaces",
        "authors": [
            "Christian Mercat"
        ],
        "summary": "This paper defines a theory of conformal parametrization of digital surfaces made of surfels equipped with a normal vector. The main idea is to locally project each surfel to the tangent plane, therefore deforming its aspect-ratio. It is a generalization of the theory known for polyhedral surfaces. The main difference is that the conformal ratios that appear are no longer real in general. It yields a generalization of the standard Laplacian on weighted graphs.",
        "published": "2008-02-12T11:06:38Z",
        "link": "http://arxiv.org/abs/0802.1617v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "math.CV"
        ]
    },
    {
        "title": "Well-Centered Triangulation",
        "authors": [
            "Evan VanderZee",
            "Anil N. Hirani",
            "Damrong Guoy",
            "Edgar Ramos"
        ],
        "summary": "Meshes composed of well-centered simplices have nice orthogonal dual meshes (the dual Voronoi diagram). This is useful for certain numerical algorithms that prefer such primal-dual mesh pairs. We prove that well-centered meshes also have optimality properties and relationships to Delaunay and minmax angle triangulations. We present an iterative algorithm that seeks to transform a given triangulation in two or three dimensions into a well-centered one by minimizing a cost function and moving the interior vertices while keeping the mesh connectivity and boundary vertices fixed. The cost function is a direct result of a new characterization of well-centeredness in arbitrary dimensions that we present. Ours is the first optimization-based heuristic for well-centeredness, and the first one that applies in both two and three dimensions. We show the results of applying our algorithm to small and large two-dimensional meshes, some with a complex boundary, and obtain a well-centered tetrahedralization of the cube. We also show numerical evidence that our algorithm preserves gradation and that it improves the maximum and minimum angles of acute triangulations created by the best known previous method.",
        "published": "2008-02-14T23:04:07Z",
        "link": "http://arxiv.org/abs/0802.2108v3",
        "categories": [
            "cs.CG",
            "cs.NA",
            "I.3.5"
        ]
    },
    {
        "title": "Minimizing the Maximum Interference is Hard",
        "authors": [
            "Kevin Buchin"
        ],
        "summary": "We consider the following interference model for wireless sensor and ad hoc networks: the receiver interference of a node is the number of transmission ranges it lies in. We model transmission ranges as disks. For this case we show that choosing transmission radii which minimize the maximum interference while maintaining a connected symmetric communication graph is NP-complete.",
        "published": "2008-02-15T03:25:37Z",
        "link": "http://arxiv.org/abs/0802.2134v2",
        "categories": [
            "cs.NI",
            "cs.CG"
        ]
    },
    {
        "title": "Guarding curvilinear art galleries with vertex or point guards",
        "authors": [
            "Menelaos I. Karavelas",
            "Elias P. Tsigaridas"
        ],
        "summary": "One of the earliest and most well known problems in computational geometry is the so-called art gallery problem. The goal is to compute the minimum possible number guards placed on the vertices of a simple polygon in such a way that they cover the interior of the polygon.   In this paper we consider the problem of guarding an art gallery which is modeled as a polygon with curvilinear walls. Our main focus is on polygons the edges of which are convex arcs pointing towards the exterior or interior of the polygon (but not both), named piecewise-convex and piecewise-concave polygons. We prove that, in the case of piecewise-convex polygons, if we only allow vertex guards, $\\lfloor\\frac{4n}{7}\\rfloor-1$ guards are sometimes necessary, and $\\lfloor\\frac{2n}{3}\\rfloor$ guards are always sufficient. Moreover, an $O(n\\log{}n)$ time and O(n) space algorithm is described that produces a vertex guarding set of size at most $\\lfloor\\frac{2n}{3}\\rfloor$. When we allow point guards the afore-mentioned lower bound drops down to $\\lfloor\\frac{n}{2}\\rfloor$. In the special case of monotone piecewise-convex polygons we can show that $\\lfloor\\frac{n}{2}\\rfloor$ vertex guards are always sufficient and sometimes necessary; these bounds remain valid even if we allow point guards.   In the case of piecewise-concave polygons, we show that $2n-4$ point guards are always sufficient and sometimes necessary, whereas it might not be possible to guard such polygons by vertex guards. We conclude with bounds for other types of curvilinear polygons and future work.",
        "published": "2008-02-19T06:10:17Z",
        "link": "http://arxiv.org/abs/0802.2594v1",
        "categories": [
            "cs.CG",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Geodesic Fréchet Distance Inside a Simple Polygon",
        "authors": [
            "Atlas F. Cook IV",
            "Carola Wenk"
        ],
        "summary": "We unveil an alluring alternative to parametric search that applies to both the non-geodesic and geodesic Fr\\'echet optimization problems. This randomized approach is based on a variant of red-blue intersections and is appealing due to its elegance and practical efficiency when compared to parametric search. We present the first algorithm for the geodesic Fr\\'echet distance between two polygonal curves $A$ and $B$ inside a simple bounding polygon $P$. The geodesic Fr\\'echet decision problem is solved almost as fast as its non-geodesic sibling and requires $O(N^{2\\log k)$ time and $O(k+N)$ space after $O(k)$ preprocessing, where $N$ is the larger of the complexities of $A$ and $B$ and $k$ is the complexity of $P$. The geodesic Fr\\'echet optimization problem is solved by a randomized approach in $O(k+N^{2\\log kN\\log N)$ expected time and $O(k+N^{2)$ space. This runtime is only a logarithmic factor larger than the standard non-geodesic Fr\\'echet algorithm (Alt and Godau 1995). Results are also presented for the geodesic Fr\\'echet distance in a polygonal domain with obstacles and the geodesic Hausdorff distance for sets of points or sets of line segments inside a simple polygon $P$.",
        "published": "2008-02-20T14:21:19Z",
        "link": "http://arxiv.org/abs/0802.2846v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "Geometric Set Cover and Hitting Sets for Polytopes in $R^3$",
        "authors": [
            "Sören Laue"
        ],
        "summary": "Suppose we are given a finite set of points $P$ in $\\R^3$ and a collection of polytopes $\\mathcal{T}$ that are all translates of the same polytope $T$. We consider two problems in this paper. The first is the set cover problem where we want to select a minimal number of polytopes from the collection $\\mathcal{T}$ such that their union covers all input points $P$. The second problem that we consider is finding a hitting set for the set of polytopes $\\mathcal{T}$, that is, we want to select a minimal number of points from the input points $P$ such that every given polytope is hit by at least one point. We give the first constant-factor approximation algorithms for both problems. We achieve this by providing an epsilon-net for translates of a polytope in $R^3$ of size $\\bigO(\\frac{1{\\epsilon)$.",
        "published": "2008-02-20T14:32:26Z",
        "link": "http://arxiv.org/abs/0802.2861v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Quantifying Homology Classes",
        "authors": [
            "Chao Chen",
            "Daniel Freedman"
        ],
        "summary": "We develop a method for measuring homology classes. This involves three problems. First, we define the size of a homology class, using ideas from relative homology. Second, we define an optimal basis of a homology group to be the basis whose elements' size have the minimal sum. We provide a greedy algorithm to compute the optimal basis and measure classes in it. The algorithm runs in $O(\\beta^4 n^3 \\log^2 n)$ time, where $n$ is the size of the simplicial complex and $\\beta$ is the Betti number of the homology group. Third, we discuss different ways of localizing homology classes and prove some hardness results.",
        "published": "2008-02-20T14:37:07Z",
        "link": "http://arxiv.org/abs/0802.2865v1",
        "categories": [
            "cs.CG",
            "cs.DM"
        ]
    },
    {
        "title": "A Universal In-Place Reconfiguration Algorithm for Sliding Cube-Shaped   Robots in a Quadratic Number of Moves",
        "authors": [
            "Zachary Abel",
            "Hugo A. Akitaya",
            "Scott Duke Kominers",
            "Matias Korman",
            "Frederick Stock"
        ],
        "summary": "In the modular robot reconfiguration problem, we are given $n$ cube-shaped modules (or robots) as well as two configurations, i.e., placements of the $n$ modules so that their union is face-connected. The goal is to find a sequence of moves that reconfigures the modules from one configuration to the other using \"sliding moves,\" in which a module slides over the face or edge of a neighboring module, maintaining connectivity of the configuration at all times.   For many years it has been known that certain module configurations in this model require at least $\\Omega(n^2)$ moves to reconfigure between them. In this paper, we introduce the first universal reconfiguration algorithm -- i.e., we show that any $n$-module configuration can reconfigure itself into any specified $n$-module configuration using just sliding moves. Our algorithm achieves reconfiguration in $O(n^2)$ moves, making it asymptotically tight. We also present a variation that reconfigures in-place, it ensures that throughout the reconfiguration process, all modules, except for one, will be contained in the union of the bounding boxes of the start and end configuration.",
        "published": "2008-02-23T00:54:13Z",
        "link": "http://arxiv.org/abs/0802.3414v4",
        "categories": [
            "cs.CG",
            "cs.MA",
            "cs.RO"
        ]
    },
    {
        "title": "A Simple Yao-Yao-Based Spanner of Bounded Degree",
        "authors": [
            "Mirela Damian"
        ],
        "summary": "It is a standing open question to decide whether the Yao-Yao structure for unit disk graphs (UDGs) is a length spanner of not. This question is highly relevant to the topology control problem for wireless ad hoc networks. In this paper we make progress towards resolving this question by showing that the Yao-Yao structure is a length spanner for UDGs of bounded aspect ratio. We also propose a new local algorithm, called Yao-Sparse-Sink, based on the Yao-Sink method introduced by Li, Wan, Wang and Frieder, that computes a (1+e)-spanner of bounded degree for a given UDG and for given e > 0. The Yao-Sparse-Sink method enables an efficient local computation of sparse sink trees. Finally, we show that all these structures for UDGs -- Yao, Yao-Yao, Yao-Sink and Yao-Sparse-Sink -- have arbitrarily large weight.",
        "published": "2008-02-29T14:39:59Z",
        "link": "http://arxiv.org/abs/0802.4325v2",
        "categories": [
            "cs.CG",
            "cs.DS",
            "C.2.1; F.2.2"
        ]
    },
    {
        "title": "Staged Self-Assembly:Nanomanufacture of Arbitrary Shapes with O(1) Glues",
        "authors": [
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Sandor P. Fekete",
            "Mashhood Ishaque",
            "Eynat Rafalin",
            "Robert T. Schweller",
            "Diane Souvaine"
        ],
        "summary": "We introduce staged self-assembly of Wang tiles, where tiles can be added dynamically in sequence and where intermediate constructions can be stored for later mixing. This model and its various constraints and performance measures are motivated by a practical nanofabrication scenario through protein-based bioengineering. Staging allows us to break through the traditional lower bounds in tile self-assembly by encoding the shape in the staging algorithm instead of the tiles. All of our results are based on the practical assumption that only a constant number of glues, and thus only a constant number of tiles, can be engineered, as each new glue type requires significant biochemical research and experiments. Under this assumption, traditional tile self-assembly cannot even manufacture an n*n square; in contrast, we show how staged assembly enables manufacture of arbitrary orthogonal shapes in a variety of precise formulations of the model.",
        "published": "2008-03-03T20:20:52Z",
        "link": "http://arxiv.org/abs/0803.0316v1",
        "categories": [
            "cs.CG",
            "F.1.1; J.3"
        ]
    },
    {
        "title": "Untangling planar graphs from a specified vertex position - Hard cases",
        "authors": [
            "Mihyun Kang",
            "Oleg Pikhurko",
            "Alexander Ravsky",
            "Mathias Schacht",
            "Oleg Verbitsky"
        ],
        "summary": "Given a planar graph $G$, we consider drawings of $G$ in the plane where edges are represented by straight line segments (which possibly intersect). Such a drawing is specified by an injective embedding $\\pi$ of the vertex set of $G$ into the plane. We prove that a wheel graph $W_n$ admits a drawing $\\pi$ such that, if one wants to eliminate edge crossings by shifting vertices to new positions in the plane, then at most $(2+o(1))\\sqrt n$ of all $n$ vertices can stay fixed. Moreover, such a drawing $\\pi$ exists even if it is presupposed that the vertices occupy any prescribed set of points in the plane. Similar questions are discussed for other families of planar graphs.",
        "published": "2008-03-06T13:08:41Z",
        "link": "http://arxiv.org/abs/0803.0858v5",
        "categories": [
            "cs.DM",
            "cs.CG"
        ]
    },
    {
        "title": "On the Computation of the Topology of a Non-Reduced Implicit Space Curve",
        "authors": [
            "Daouda Niang Diatta",
            "Bernard Mourrain",
            "Olivier Ruatta"
        ],
        "summary": "An algorithm is presented for the computation of the topology of a non-reduced space curve defined as the intersection of two implicit algebraic surfaces. It computes a Piecewise Linear Structure (PLS) isotopic to the original space curve. The algorithm is designed to provide the exact result for all inputs. It's a symbolic-numeric algorithm based on subresultant computation. Simple algebraic criteria are given to certify the output of the algorithm. The algorithm uses only one projection of the non-reduced space curve augmented with adjacency information around some \"particular points\" of the space curve. The algorithm is implemented with the Mathemagix Computer Algebra System (CAS) using the SYNAPS library as a backend.",
        "published": "2008-03-07T15:28:52Z",
        "link": "http://arxiv.org/abs/0803.1110v1",
        "categories": [
            "math.AC",
            "cs.CG",
            "cs.SC"
        ]
    },
    {
        "title": "On the Topology of the Restricted Delaunay Triangulation and Witness   Complex in Higher Dimensions",
        "authors": [
            "Steve Y. Oudot"
        ],
        "summary": "It is a well-known fact that, under mild sampling conditions, the restricted Delaunay triangulation provides good topological approximations of 1- and 2-manifolds. We show that this is not the case for higher-dimensional manifolds, even under stronger sampling conditions. Specifically, it is not true that, for any compact closed submanifold M of R^n, and any sufficiently dense uniform sampling L of M, the Delaunay triangulation of L restricted to M is homeomorphic to M, or even homotopy equivalent to it. Besides, it is not true either that, for any sufficiently dense set W of witnesses, the witness complex of L relative to M contains or is contained in the restricted Delaunay triangulation of L.",
        "published": "2008-03-09T13:47:43Z",
        "link": "http://arxiv.org/abs/0803.1296v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Trajectory Networks and Their Topological Changes Induced by   Geographical Infiltration",
        "authors": [
            "Luciano da Fontoura Costa"
        ],
        "summary": "In this article we investigate the topological changes undergone by trajectory networks as a consequence of progressive geographical infiltration. Trajectory networks, a type of knitted network, are obtained by establishing paths between geographically distributed nodes while following an associated vector field. For instance, the nodes could correspond to neurons along the cortical surface and the vector field could correspond to the gradient of neurotrophic factors, or the nodes could represent towns while the vector fields would be given by economical and/or geographical gradients. Therefore trajectory networks are natural models of a large number of geographical structures. The geographical infiltrations correspond to the addition of new local connections between nearby existing nodes. As such, these infiltrations could be related to several real-world processes such as contaminations, diseases, attacks, parasites, etc. The way in which progressive geographical infiltrations affect trajectory networks is investigated in terms of the degree, clustering coefficient, size of the largest component and the lengths of the existing chains measured along the infiltrations. It is shown that the maximum infiltration distance plays a critical role in the intensity of the induced topological changes. For large enough values of this parameter, the chains intrinsic to the trajectory networks undergo a collapse which is shown not to be related to the percolation of the network also implied by the infiltrations.",
        "published": "2008-03-17T13:52:39Z",
        "link": "http://arxiv.org/abs/0803.2447v2",
        "categories": [
            "cs.DM",
            "cs.CG"
        ]
    },
    {
        "title": "Complexity and algorithms for computing Voronoi cells of lattices",
        "authors": [
            "Mathieu Dutour Sikiric",
            "Achill Schuermann",
            "Frank Vallentin"
        ],
        "summary": "In this paper we are concerned with finding the vertices of the Voronoi cell of a Euclidean lattice. Given a basis of a lattice, we prove that computing the number of vertices is a #P-hard problem. On the other hand we describe an algorithm for this problem which is especially suited for low dimensional (say dimensions at most 12) and for highly-symmetric lattices. We use our implementation, which drastically outperforms those of current computer algebra systems, to find the vertices of Voronoi cells and quantizer constants of some prominent lattices.",
        "published": "2008-03-31T22:02:47Z",
        "link": "http://arxiv.org/abs/0804.0036v4",
        "categories": [
            "math.MG",
            "cs.CG",
            "cs.IT",
            "math.IT",
            "math.NT",
            "11H56, 11H06, 11B1, 03D15, 52B55, 52B12"
        ]
    },
    {
        "title": "Spacetime Meshing for Discontinuous Galerkin Methods",
        "authors": [
            "Shripad Thite"
        ],
        "summary": "Spacetime discontinuous Galerkin (SDG) finite element methods are used to solve such PDEs involving space and time variables arising from wave propagation phenomena in important applications in science and engineering.   To support an accurate and efficient solution procedure using SDG methods and to exploit the flexibility of these methods, we give a meshing algorithm to construct an unstructured simplicial spacetime mesh over an arbitrary simplicial space domain. Our algorithm is the first spacetime meshing algorithm suitable for efficient solution of nonlinear phenomena in anisotropic media using novel discontinuous Galerkin finite element methods for implicit solutions directly in spacetime. Given a triangulated d-dimensional Euclidean space domain M (a simplicial complex) and initial conditions of the underlying hyperbolic spacetime PDE, we construct an unstructured simplicial mesh of the (d+1)-dimensional spacetime domain M x [0,infinity). Our algorithm uses a near-optimal number of spacetime elements, each with bounded temporal aspect ratio for any finite prefix M x [0,T] of spacetime. Our algorithm is an advancing front procedure that constructs the spacetime mesh incrementally, an extension of the Tent Pitcher algorithm of Ungor and Sheffer (2000).   In 2DxTime, our algorithm simultaneously adapts the size and shape of spacetime tetrahedra to a spacetime error indicator. We are able to incorporate more general front modification operations, such as edge flips and limited mesh smoothing. Our algorithm represents recent progress towards a meshing algorithm in 2DxTime to track moving domain boundaries and other singular surfaces such as shock fronts.",
        "published": "2008-04-07T00:44:42Z",
        "link": "http://arxiv.org/abs/0804.0942v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Efficient Spacetime Meshing with Nonlocal Cone Constraints",
        "authors": [
            "Shripad Thite"
        ],
        "summary": "Spacetime Discontinuous Galerkin (DG) methods are used to solve hyperbolic PDEs describing wavelike physical phenomena. When the PDEs are nonlinear, the speed of propagation of the phenomena, called the wavespeed, at any point in the spacetime domain is computed as part of the solution. We give an advancing front algorithm to construct a simplicial mesh of the spacetime domain suitable for DG solutions. Given a simplicial mesh of a bounded linear or planar space domain M, we incrementally construct a mesh of the spacetime domain M x [0,infinity) such that the solution can be computed in constant time per element. We add a patch of spacetime elements to the mesh at every step. The boundary of every patch is causal which means that the elements in the patch can be solved immediately and that the patches in the mesh are partially ordered by dependence. The elements in a single patch are coupled because they share implicit faces; however, the number of elements in each patch is bounded. The main contribution of this paper is sufficient constraints on the progress in time made by the algorithm at each step which guarantee that a new patch with causal boundary can be added to the mesh at every step even when the wavespeed is increasing discontinuously. Our algorithm adapts to the local gradation of the space mesh as well as the wavespeed that most constrains progress at each step. Previous algorithms have been restricted at each step by the maximum wavespeed throughout the entire spacetime domain.",
        "published": "2008-04-07T01:26:39Z",
        "link": "http://arxiv.org/abs/0804.0946v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Cauchy's Arm Lemma on a Growing Sphere",
        "authors": [
            "Zachary Abel",
            "David Charlton",
            "Sebastien Collette",
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Stefan Langerman",
            "Joseph O'Rourke",
            "Val Pinciu",
            "Godfried Toussaint"
        ],
        "summary": "We propose a variant of Cauchy's Lemma, proving that when a convex chain on one sphere is redrawn (with the same lengths and angles) on a larger sphere, the distance between its endpoints increases. The main focus of this work is a comparison of three alternate proofs, to show the links between Toponogov's Comparison Theorem, Legendre's Theorem and Cauchy's Arm Lemma.",
        "published": "2008-04-07T08:49:47Z",
        "link": "http://arxiv.org/abs/0804.0986v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "On the Stretch Factor of Convex Delaunay Graphs",
        "authors": [
            "Prosenjit Bose",
            "Paz Carmi",
            "Sebastien Collette",
            "Michiel Smid"
        ],
        "summary": "Let C be a compact and convex set in the plane that contains the origin in its interior, and let S be a finite set of points in the plane. The Delaunay graph DG_C(S) of S is defined to be the dual of the Voronoi diagram of S with respect to the convex distance function defined by C. We prove that DG_C(S) is a t-spanner for S, for some constant t that depends only on the shape of the set C. Thus, for any two points p and q in S, the graph DG_C(S) contains a path between p and q whose Euclidean length is at most t times the Euclidean distance between p and q.",
        "published": "2008-04-07T14:33:04Z",
        "link": "http://arxiv.org/abs/0804.1041v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Discrete schemes for Gaussian curvature and their convergence",
        "authors": [
            "Zhiqiang Xu",
            "Guoliang Xu"
        ],
        "summary": "In this paper, several discrete schemes for Gaussian curvature are surveyed. The convergence property of a modified discrete scheme for the Gaussian curvature is considered. Furthermore, a new discrete scheme for Gaussian curvature is resented. We prove that the new scheme converges at the regular vertex with valence not less than 5. By constructing a counterexample, we also show that it is impossible for building a discrete scheme for Gaussian curvature which converges over the regular vertex with valence 4. Finally, asymptotic errors of several discrete scheme for Gaussian curvature are compared.",
        "published": "2008-04-07T14:47:03Z",
        "link": "http://arxiv.org/abs/0804.1046v1",
        "categories": [
            "cs.CV",
            "cs.CG",
            "cs.GR",
            "cs.NA"
        ]
    },
    {
        "title": "A Lower Bound on the Area of a 3-Coloured Disk Packing",
        "authors": [
            "Peter Brass",
            "Ferran Hurtado",
            "Benjamin Lafreniere",
            "Anna Lubiw"
        ],
        "summary": "Given a set of unit-disks in the plane with union area $A$, what fraction of $A$ can be covered by selecting a pairwise disjoint subset of the disks? Rado conjectured 1/4 and proved $1/4.41$. Motivated by the problem of channel-assignment for wireless access points, in which use of 3 channels is a standard practice, we consider a variant where the selected subset of disks must be 3-colourable with disks of the same colour pairwise-disjoint. For this variant of the problem, we conjecture that it is always possible to cover at least $1/1.41$ of the union area and prove $1/2.09$. We also provide an $O(n^2)$ algorithm to select a subset achieving a $1/2.77$ bound.",
        "published": "2008-04-08T07:26:04Z",
        "link": "http://arxiv.org/abs/0804.1173v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Natural pseudo-distance and optimal matching between reduced size   functions",
        "authors": [
            "M. d'Amico",
            "P. Frosini",
            "C. Landi"
        ],
        "summary": "This paper studies the properties of a new lower bound for the natural pseudo-distance. The natural pseudo-distance is a dissimilarity measure between shapes, where a shape is viewed as a topological space endowed with a real-valued continuous function. Measuring dissimilarity amounts to minimizing the change in the functions due to the application of homeomorphisms between topological spaces, with respect to the $L_\\infty$-norm. In order to obtain the lower bound, a suitable metric between size functions, called matching distance, is introduced. It compares size functions by solving an optimal matching problem between countable point sets. The matching distance is shown to be resistant to perturbations, implying that it is always smaller than the natural pseudo-distance. We also prove that the lower bound so obtained is sharp and cannot be improved by any other distance between size functions.",
        "published": "2008-04-22T11:25:11Z",
        "link": "http://arxiv.org/abs/0804.3500v1",
        "categories": [
            "cs.CG",
            "cs.CV"
        ]
    },
    {
        "title": "Isotropic PCA and Affine-Invariant Clustering",
        "authors": [
            "S. Charles Brubaker",
            "Santosh S. Vempala"
        ],
        "summary": "We present a new algorithm for clustering points in R^n. The key property of the algorithm is that it is affine-invariant, i.e., it produces the same partition for any affine transformation of the input. It has strong guarantees when the input is drawn from a mixture model. For a mixture of two arbitrary Gaussians, the algorithm correctly classifies the sample assuming only that the two components are separable by a hyperplane, i.e., there exists a halfspace that contains most of one Gaussian and almost none of the other in probability mass. This is nearly the best possible, improving known results substantially. For k > 2 components, the algorithm requires only that there be some (k-1)-dimensional subspace in which the emoverlap in every direction is small. Here we define overlap to be the ratio of the following two quantities: 1) the average squared distance between a point and the mean of its component, and 2) the average squared distance between a point and the mean of the mixture. The main result may also be stated in the language of linear discriminant analysis: if the standard Fisher discriminant is small enough, labels are not needed to estimate the optimal subspace for projection. Our main tools are isotropic transformation, spectral projection and a simple reweighting technique. We call this combination isotropic PCA.",
        "published": "2008-04-22T17:59:03Z",
        "link": "http://arxiv.org/abs/0804.3575v2",
        "categories": [
            "cs.LG",
            "cs.CG"
        ]
    },
    {
        "title": "On the metric distortion of nearest-neighbour graphs on random point   sets",
        "authors": [
            "Amitabha Bagchi",
            "Sohit Bansal"
        ],
        "summary": "We study the graph constructed on a Poisson point process in $d$ dimensions by connecting each point to the $k$ points nearest to it. This graph a.s. has an infinite cluster if $k > k_c(d)$ where $k_c(d)$, known as the critical value, depends only on the dimension $d$. This paper presents an improved upper bound of 188 on the value of $k_c(2)$. We also show that if $k \\geq 188$ the infinite cluster of $\\NN(2,k)$ has an infinite subset of points with the property that the distance along the edges of the graphs between these points is at most a constant multiplicative factor larger than their Euclidean distance. Finally we discuss in detail the relevance of our results to the study of multi-hop wireless sensor networks.",
        "published": "2008-04-23T19:04:18Z",
        "link": "http://arxiv.org/abs/0804.3784v2",
        "categories": [
            "cs.NI",
            "cs.CG"
        ]
    },
    {
        "title": "On Computing the Shadows and Slices of Polytopes",
        "authors": [
            "Hans Raj Tiwary"
        ],
        "summary": "We study the complexity of computing the projection of an arbitrary $d$-polytope along $k$ orthogonal vectors for various input and output forms. We show that if $d$ and $k$ are part of the input (i.e. not a constant) and we are interested in output-sensitive algorithms, then in most forms the problem is equivalent to enumerating vertices of polytopes, except in two where it is NP-hard. In two other forms the problem is trivial. We also review the complexity of computing projections when the projection directions are in some sense non-degenerate. For full-dimensional polytopes containing origin in the interior, projection is an operation dual to intersecting the polytope with a suitable linear subspace and so the results in this paper can be dualized by interchanging vertices with facets and projection with intersection. To compare the complexity of projection and vertex enumeration, we define new complexity classes based on the complexity of Vertex Enumeration.",
        "published": "2008-04-25T17:05:16Z",
        "link": "http://arxiv.org/abs/0804.4150v2",
        "categories": [
            "cs.CC",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Eppstein's bound on intersecting triangles revisited",
        "authors": [
            "Gabriel Nivasch",
            "Micha Sharir"
        ],
        "summary": "Let S be a set of n points in the plane, and let T be a set of m triangles with vertices in S. Then there exists a point in the plane contained in Omega(m^3 / (n^6 log^2 n)) triangles of T. Eppstein (1993) gave a proof of this claim, but there is a problem with his proof. Here we provide a correct proof by slightly modifying Eppstein's argument.",
        "published": "2008-04-28T14:09:41Z",
        "link": "http://arxiv.org/abs/0804.4415v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Stabbing simplices by points and flats",
        "authors": [
            "Boris Bukh",
            "Jiří Matoušek",
            "Gabriel Nivasch"
        ],
        "summary": "The following result was proved by Barany in 1982: For every d >= 1 there exists c_d > 0 such that for every n-point set S in R^d there is a point p in R^d contained in at least c_d n^{d+1} - O(n^d) of the simplices spanned by S.   We investigate the largest possible value of c_d. It was known that c_d <= 1/(2^d(d+1)!) (this estimate actually holds for every point set S). We construct sets showing that c_d <= (d+1)^{-(d+1)}, and we conjecture this estimate to be tight. The best known lower bound, due to Wagner, is c_d >= gamma_d := (d^2+1)/((d+1)!(d+1)^{d+1}); in his method, p can be chosen as any centerpoint of S. We construct n-point sets with a centerpoint that is contained in no more than gamma_d n^{d+1}+O(n^d) simplices spanned by S, thus showing that the approach using an arbitrary centerpoint cannot be further improved.   We also prove that for every n-point set S in R^d there exists a (d-2)-flat that stabs at least c_{d,d-2} n^3 - O(n^2) of the triangles spanned by S, with c_{d,d-2}>=(1/24)(1- 1/(2d-1)^2). To this end, we establish an equipartition result of independent interest (generalizing planar results of Buck and Buck and of Ceder): Every mass distribution in R^d can be divided into 4d-2 equal parts by 2d-1 hyperplanes intersecting in a common (d-2)-flat.",
        "published": "2008-04-28T19:58:04Z",
        "link": "http://arxiv.org/abs/0804.4464v2",
        "categories": [
            "math.CO",
            "cs.CG",
            "52C10, 52C35, 52A35, 54C99"
        ]
    },
    {
        "title": "An Affine-invariant Time-dependent Triangulation of Spatio-temporal Data",
        "authors": [
            "Sofie Haesevoets",
            "Bart Kuijpers"
        ],
        "summary": "In the geometric data model for spatio-temporal data, introduced by Chomicki and Revesz, spatio-temporal data are modelled as a finite collection of triangles that are transformed by time-dependent affinities of the plane. To facilitate querying and animation of spatio-temporal data, we present a normal form for data in the geometric data model. We propose an algorithm for constructing this normal form via a spatio-temporal triangulation of geometric data objects. This triangulation algorithm generates new geometric data objects that partition the given objects both in space and in time. A particular property of the proposed partition is that it is invariant under time-dependent affine transformations, and hence independent of the particular choice of coordinate system used to describe he spatio-temporal data in. We can show that our algorithm works correctly and has a polynomial time complexity (of reasonably low degree in the number of input triangles and the maximal degree of the polynomial functions that describe the transformation functions). We also discuss several possible applications of this spatio-temporal triangulation.",
        "published": "2008-04-30T06:02:56Z",
        "link": "http://arxiv.org/abs/0804.4740v1",
        "categories": [
            "cs.CG",
            "cs.DB",
            "H.2.8; I.3.5"
        ]
    },
    {
        "title": "Straight Skeletons of Three-Dimensional Polyhedra",
        "authors": [
            "Gill Barequet",
            "David Eppstein",
            "Michael T. Goodrich",
            "Amir Vaxman"
        ],
        "summary": "This paper studies the straight skeleton of polyhedra in three dimensions. We first address voxel-based polyhedra (polycubes), formed as the union of a collection of cubical (axis-aligned) voxels. We analyze the ways in which the skeleton may intersect each voxel of the polyhedron, and show that the skeleton may be constructed by a simple voxel-sweeping algorithm taking constant time per voxel. In addition, we describe a more complex algorithm for straight skeletons of voxel-based polyhedra, which takes time proportional to the area of the surfaces of the straight skeleton rather than the volume of the polyhedron. We also consider more general polyhedra with axis-parallel edges and faces, and show that any n-vertex polyhedron of this type has a straight skeleton with O(n^2) features. We provide algorithms for constructing the straight skeleton, with running time O(min(n^2 log n, k log^{O(1)} n)) where k is the output complexity. Next, we discuss the straight skeleton of a general nonconvex polyhedron. We show that it has an ambiguity issue, and suggest a consistent method to resolve it. We prove that the straight skeleton of a general polyhedron has a superquadratic complexity in the worst case. Finally, we report on an implementation of a simple algorithm for the general case.",
        "published": "2008-04-30T22:52:34Z",
        "link": "http://arxiv.org/abs/0805.0022v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Morphing of Triangular Meshes in Shape Space",
        "authors": [
            "Stefanie Wuhrer",
            "Prosenjit Bose",
            "Chang Shu",
            "Joseph O'Rourke",
            "Alan Brunton"
        ],
        "summary": "We present a novel approach to morph between two isometric poses of the same non-rigid object given as triangular meshes. We model the morphs as linear interpolations in a suitable shape space $\\mathcal{S}$. For triangulated 3D polygons, we prove that interpolating linearly in this shape space corresponds to the most isometric morph in $\\mathbb{R}^3$. We then extend this shape space to arbitrary triangulations in 3D using a heuristic approach and show the practical use of the approach using experiments. Furthermore, we discuss a modified shape space that is useful for isometric skeleton morphing. All of the newly presented approaches solve the morphing problem without the need to solve a minimization problem.",
        "published": "2008-05-01T23:08:30Z",
        "link": "http://arxiv.org/abs/0805.0162v2",
        "categories": [
            "cs.CG",
            "cs.GR"
        ]
    },
    {
        "title": "3D Building Model Fitting Using A New Kinetic Framework",
        "authors": [
            "Mathieu Brédif",
            "Dider Boldo",
            "Marc Pierrot-Deseilligny",
            "Henri Maître"
        ],
        "summary": "We describe a new approach to fit the polyhedron describing a 3D building model to the point cloud of a Digital Elevation Model (DEM). We introduce a new kinetic framework that hides to its user the combinatorial complexity of determining or maintaining the polyhedron topology, allowing the design of a simple variational optimization. This new kinetic framework allows the manipulation of a bounded polyhedron with simple faces by specifying the target plane equations of each of its faces. It proceeds by evolving continuously from the polyhedron defined by its initial topology and its initial plane equations to a polyhedron that is as topologically close as possible to the initial polyhedron but with the new plane equations. This kinetic framework handles internally the necessary topological changes that may be required to keep the faces simple and the polyhedron bounded. For each intermediate configurations where the polyhedron looses the simplicity of its faces or its boundedness, the simplest topological modification that is able to reestablish the simplicity and the boundedness is performed.",
        "published": "2008-05-06T06:34:31Z",
        "link": "http://arxiv.org/abs/0805.0648v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Searching for Frequent Colors in Rectangles",
        "authors": [
            "Marek Karpinski",
            "Yakov Nekrich"
        ],
        "summary": "We study a new variant of colored orthogonal range searching problem: given a query rectangle $Q$ all colors $c$, such that at least a fraction $\\tau$ of all points in $Q$ are of color $c$, must be reported. We describe several data structures for that problem that use pseudo-linear space and answer queries in poly-logarithmic time.",
        "published": "2008-05-09T13:47:55Z",
        "link": "http://arxiv.org/abs/0805.1348v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "Approximation Algorithms for Shortest Descending Paths in Terrains",
        "authors": [
            "Mustaq Ahmed",
            "Sandip Das",
            "Sachin Lodha",
            "Anna Lubiw",
            "Anil Maheshwari",
            "Sasanka Roy"
        ],
        "summary": "A path from s to t on a polyhedral terrain is descending if the height of a point p never increases while we move p along the path from s to t. No efficient algorithm is known to find a shortest descending path (SDP) from s to t in a polyhedral terrain. We give two approximation algorithms (more precisely, FPTASs) that solve the SDP problem on general terrains. Both algorithms are simple, robust and easy to implement.",
        "published": "2008-05-09T19:39:19Z",
        "link": "http://arxiv.org/abs/0805.1401v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "An optimization problem on the sphere",
        "authors": [
            "Andreas Maurer"
        ],
        "summary": "We prove existence and uniqueness of the minimizer for the average geodesic distance to the points of a geodesically convex set on the sphere. This implies a corresponding existence and uniqueness result for an optimal algorithm for halfspace learning, when data and target functions are drawn from the uniform distribution.",
        "published": "2008-05-15T17:25:03Z",
        "link": "http://arxiv.org/abs/0805.2362v1",
        "categories": [
            "cs.LG",
            "cs.CG"
        ]
    },
    {
        "title": "Canonical polygon Queries on the plane: a New Approach",
        "authors": [
            "Spyros Sioutas",
            "Dimitrios Sofotassios",
            "Kostas Tsichlas",
            "Dimitrios Sotiropoulos",
            "Panayiotis Vlamos"
        ],
        "summary": "The polygon retrieval problem on points is the problem of preprocessing a set of $n$ points on the plane, so that given a polygon query, the subset of points lying inside it can be reported efficiently.   It is of great interest in areas such as Computer Graphics, CAD applications, Spatial Databases and GIS developing tasks. In this paper we study the problem of canonical $k$-vertex polygon queries on the plane. A canonical $k$-vertex polygon query always meets the following specific property: a point retrieval query can be transformed into a linear number (with respect to the number of vertices) of point retrievals for orthogonal objects such as rectangles and triangles (throughout this work we call a triangle orthogonal iff two of its edges are axis-parallel).   We present two new algorithms for this problem. The first one requires $O(n\\log^2{n})$ space and $O(k\\frac{log^3n}{loglogn}+A)$ query time. A simple modification scheme on first algorithm lead us to a second solution, which consumes $O(n^2)$ space and $O(k \\frac{logn}{loglogn}+A)$ query time, where $A$ denotes the size of the answer and $k$ is the number of vertices.   The best previous solution for the general polygon retrieval problem uses $O(n^2)$ space and answers a query in $O(k\\log{n}+A)$ time, where $k$ is the number of vertices. It is also very complicated and difficult to be implemented in a standard imperative programming language such as C or C++.",
        "published": "2008-05-17T16:00:09Z",
        "link": "http://arxiv.org/abs/0805.2681v2",
        "categories": [
            "cs.CG",
            "cs.DS",
            "E.1; E.5"
        ]
    },
    {
        "title": "Three-dimensional Random Voronoi Tessellations: From Cubic Crystal   Lattices to Poisson Point Processes",
        "authors": [
            "Valerio Lucarini"
        ],
        "summary": "We perturb the SC, BCC, and FCC crystal structures with a spatial Gaussian noise whose adimensional strength is controlled by the parameter a, and analyze the topological and metrical properties of the resulting Voronoi Tessellations (VT). The topological properties of the VT of the SC and FCC crystals are unstable with respect to the introduction of noise, because the corresponding polyhedra are geometrically degenerate, whereas the tessellation of the BCC crystal is topologically stable even against noise of small but finite intensity. For weak noise, the mean area of the perturbed BCC and FCC crystals VT increases quadratically with a. In the case of perturbed SCC crystals, there is an optimal amount of noise that minimizes the mean area of the cells. Already for a moderate noise (a>0.5), the properties of the three perturbed VT are indistinguishable, and for intense noise (a>2), results converge to the Poisson-VT limit. Notably, 2-parameter gamma distributions are an excellent model for the empirical of of all considered properties. The VT of the perturbed BCC and FCC structures are local maxima for the isoperimetric quotient, which measures the degre of sphericity of the cells, among space filling VT. In the BCC case, this suggests a weaker form of the recentluy disproved Kelvin conjecture. Due to the fluctuations of the shape of the cells, anomalous scalings with exponents >3/2 is observed between the area and the volumes of the cells, and, except for the FCC case, also for a->0. In the Poisson-VT limit, the exponent is about 1.67. As the number of faces is positively correlated with the sphericity of the cells, the anomalous scaling is heavily reduced when we perform powerlaw fits separately on cells with a specific number of faces.",
        "published": "2008-05-18T00:36:52Z",
        "link": "http://arxiv.org/abs/0805.2705v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cond-mat.other",
            "cs.CG",
            "math-ph",
            "math.MP",
            "nlin.PS",
            "physics.data-an"
        ]
    },
    {
        "title": "Succinct Geometric Indexes Supporting Point Location Queries",
        "authors": [
            "Prosenjit Bose",
            "Eric Y. Chen",
            "Meng He",
            "Anil Maheshwari",
            "Pat Morin"
        ],
        "summary": "We propose to design data structures called succinct geometric indexes of negligible space (more precisely, o(n) bits) that, by taking advantage of the n points in the data set permuted and stored elsewhere as a sequence, to support geometric queries in optimal time. Our first and main result is a succinct geometric index that can answer point location queries, a fundamental problem in computational geometry, on planar triangulations in O(lg n) time. We also design three variants of this index. The first supports point location using $\\lg n + 2\\sqrt{\\lg n} + O(\\lg^{1/4} n)$ point-line comparisons. The second supports point location in o(lg n) time when the coordinates are integers bounded by U. The last variant can answer point location in O(H+1) expected time, where H is the entropy of the query distribution. These results match the query efficiency of previous point location structures that use O(n) words or O(n lg n) bits, while saving drastic amounts of space.   We then generalize our succinct geometric index to planar subdivisions, and design indexes for other types of queries. Finally, we apply our techniques to design the first implicit data structures that support point location in $O(\\lg^2 n)$ time.",
        "published": "2008-05-27T15:15:05Z",
        "link": "http://arxiv.org/abs/0805.4147v1",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Succinct Greedy Graph Drawing in the Hyperbolic Plane",
        "authors": [
            "David Eppstein",
            "Michael T. Goodrich"
        ],
        "summary": "We describe an efficient method for drawing any n-vertex simple graph G in the hyperbolic plane. Our algorithm produces greedy drawings, which support greedy geometric routing, so that a message M between any pair of vertices may be routed geometrically, simply by having each vertex that receives M pass it along to any neighbor that is closer in the hyperbolic metric to the message's eventual destination. More importantly, for networking applications, our algorithm produces succinct drawings, in that each of the vertex positions in one of our embeddings can be represented using O(log n) bits and the calculation of which neighbor to send a message to may be performed efficiently using these representations. These properties are useful, for example, for routing in sensor networks, where storage and bandwidth are limited.",
        "published": "2008-06-02T17:35:58Z",
        "link": "http://arxiv.org/abs/0806.0341v1",
        "categories": [
            "cs.CG",
            "F.2.2; C.2.1"
        ]
    },
    {
        "title": "On collinear sets in straight line drawings",
        "authors": [
            "Alexander Ravsky",
            "Oleg Verbitsky"
        ],
        "summary": "We consider straight line drawings of a planar graph $G$ with possible edge crossings. The \\emph{untangling problem} is to eliminate all edge crossings by moving as few vertices as possible to new positions. Let $fix(G)$ denote the maximum number of vertices that can be left fixed in the worst case. In the \\emph{allocation problem}, we are given a planar graph $G$ on $n$ vertices together with an $n$-point set $X$ in the plane and have to draw $G$ without edge crossings so that as many vertices as possible are located in $X$. Let $fit(G)$ denote the maximum number of points fitting this purpose in the worst case. As $fix(G)\\le fit(G)$, we are interested in upper bounds for the latter and lower bounds for the former parameter.   For each $\\epsilon>0$, we construct an infinite sequence of graphs with $fit(G)=O(n^{\\sigma+\\epsilon})$, where $\\sigma<0.99$ is a known graph-theoretic constant, namely the shortness exponent for the class of cubic polyhedral graphs. To the best of our knowledge, this is the first example of graphs with $fit(G)=o(n)$. On the other hand, we prove that $fix(G)\\ge\\sqrt{n/30}$ for all $G$ with tree-width at most 2. This extends the lower bound obtained by Goaoc et al. [Discrete and Computational Geometry 42:542-569 (2009)] for outerplanar graphs.   Our upper bound for $fit(G)$ is based on the fact that the constructed graphs can have only few collinear vertices in any crossing-free drawing. To prove the lower bound for $fix(G)$, we show that graphs of tree-width 2 admit drawings that have large sets of collinear vertices with some additional special properties.",
        "published": "2008-06-02T18:01:35Z",
        "link": "http://arxiv.org/abs/0806.0253v5",
        "categories": [
            "cs.CG",
            "cs.DM"
        ]
    },
    {
        "title": "Drawing (Complete) Binary Tanglegrams: Hardness, Approximation,   Fixed-Parameter Tractability",
        "authors": [
            "Kevin Buchin",
            "Maike Buchin",
            "Jaroslaw Byrka",
            "Martin Nöllenburg",
            "Yoshio Okamoto",
            "Rodrigo I. Silveira",
            "Alexander Wolff"
        ],
        "summary": "A \\emph{binary tanglegram} is a drawing of a pair of rooted binary trees whose leaf sets are in one-to-one correspondence; matching leaves are connected by inter-tree edges. For applications, for example, in phylogenetics, it is essential that both trees are drawn without edge crossings and that the inter-tree edges have as few crossings as possible. It is known that finding a tanglegram with the minimum number of crossings is NP-hard and that the problem is fixed-parameter tractable with respect to that number.   We prove that under the Unique Games Conjecture there is no constant-factor approximation for binary trees. We show that the problem is NP-hard even if both trees are complete binary trees. For this case we give an $O(n^3)$-time 2-approximation and a new, simple fixed-parameter algorithm. We show that the maximization version of the dual problem for binary trees can be reduced to a version of MaxCut for which the algorithm of Goemans and Williamson yields a 0.878-approximation.",
        "published": "2008-06-05T09:31:57Z",
        "link": "http://arxiv.org/abs/0806.0920v3",
        "categories": [
            "cs.CG",
            "cs.CC"
        ]
    },
    {
        "title": "Drawing Binary Tanglegrams: An Experimental Evaluation",
        "authors": [
            "Martin Nöllenburg",
            "Danny Holten",
            "Markus Völker",
            "Alexander Wolff"
        ],
        "summary": "A binary tanglegram is a pair <S,T> of binary trees whose leaf sets are in one-to-one correspondence; matching leaves are connected by inter-tree edges. For applications, for example in phylogenetics or software engineering, it is required that the individual trees are drawn crossing-free. A natural optimization problem, denoted tanglegram layout problem, is thus to minimize the number of crossings between inter-tree edges.   The tanglegram layout problem is NP-hard and is currently considered both in application domains and theory. In this paper we present an experimental comparison of a recursive algorithm of Buchin et al., our variant of their algorithm, the algorithm hierarchy sort of Holten and van Wijk, and an integer quadratic program that yields optimal solutions.",
        "published": "2008-06-05T11:00:33Z",
        "link": "http://arxiv.org/abs/0806.0928v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "Steiner trees and spanning trees in six-pin soap films",
        "authors": [
            "Prasun Dutta",
            "S. Pratik Khastgir",
            "Anushree Roy"
        ],
        "summary": "We have studied the Steiner tree problem using six-pin soap films in detail. We extend the existing method of experimental realisation of Steiner trees in $n$-terminal problem through soap films to observe new non-minimal Steiner trees. We also produced spanning tree configurations for the first time by our method. Experimentally, by varying the pin diameter, we have achieved these new stable soap film configurations. A new algorithm is presented for creating these Steiner trees theoretically.   Exact lengths of these Steiner tree configurations are calculated using a geometrical method. An exact two-parameter empirical formula is proposed for estimating the lengths of these soap film configurations in six-pin soap film problem.",
        "published": "2008-06-08T16:58:31Z",
        "link": "http://arxiv.org/abs/0806.1340v1",
        "categories": [
            "cs.CG",
            "cond-mat.other",
            "physics.class-ph"
        ]
    },
    {
        "title": "Highway Hull Revisited",
        "authors": [
            "Greg Aloupis",
            "Jean Cardinal",
            "Sebastien Collette",
            "Ferran Hurtado",
            "Stefan Langerman",
            "Joseph O'Rourke",
            "Belen Palop"
        ],
        "summary": "A highway H is a line in the plane on which one can travel at a greater speed than in the remaining plane. One can choose to enter and exit H at any point. The highway time distance between a pair of points is the minimum time required to move from one point to the other, with optional use of H.   The highway hull HH(S,H) of a point set S is the minimal set containing S as well as the shortest paths between all pairs of points in HH(S,H), using the highway time distance.   We provide a Theta(n log n) worst-case time algorithm to find the highway hull under the L_1 metric, as well as an O(n log^2 n) time algorithm for the L_2 metric which improves the best known result of O(n^2).   We also define and construct the useful region of the plane: the region that a highway must intersect in order that the shortest path between at least one pair of points uses the highway.",
        "published": "2008-06-09T10:22:38Z",
        "link": "http://arxiv.org/abs/0806.1416v3",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "The injectivity of the global function of a cellular automaton in the   hyperbolic plane is undecidable",
        "authors": [
            "Margenstern Maurice"
        ],
        "summary": "In this paper, we look at the following question. We consider cellular automata in the hyperbolic plane and we consider the global function defined on all possible configurations. Is the injectivity of this function undecidable? The problem was answered positively in the case of the Euclidean plane by Jarkko Kari, in 1994. In the present paper, we show that the answer is also positive for the hyperbolic plane: the problem is undecidable.",
        "published": "2008-06-10T10:58:30Z",
        "link": "http://arxiv.org/abs/0806.1602v2",
        "categories": [
            "cs.CG",
            "cs.LO",
            "F.2.2"
        ]
    },
    {
        "title": "Self-overlapping Curves Revisited",
        "authors": [
            "David Eppstein",
            "Elena Mumford"
        ],
        "summary": "A surface embedded in space, in such a way that each point has a neighborhood within which the surface is a terrain, projects to an immersed surface in the plane, the boundary of which is a self-intersecting curve. Under what circumstances can we reverse these mappings algorithmically? Shor and van Wyk considered one such problem, determining whether a curve is the boundary of an immersed disk; they showed that the self-overlapping curves defined in this way can be recognized in polynomial time. We show that several related problems are more difficult: it is NP-complete to determine whether an immersed disk is the projection of a surface embedded in space, or whether a curve is the boundary of an immersed surface in the plane that is not constrained to be a disk. However, when a casing is supplied with a self-intersecting curve, describing which component of the curve lies above and which below at each crossing, we may determine in time linear in the number of crossings whether the cased curve forms the projected boundary of a surface in space. As a related result, we show that an immersed surface with a single boundary curve that crosses itself n times has at most 2^{n/2} combinatorially distinct spatial embeddings, and we discuss the existence of fixed-parameter tractable algorithms for related problems.",
        "published": "2008-06-10T18:50:53Z",
        "link": "http://arxiv.org/abs/0806.1724v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Finding the theta-Guarded Region",
        "authors": [
            "Domagoj Matijević",
            "Ralf Osbild"
        ],
        "summary": "We are given a finite set of n points (guards) G in the plane R^2 and an angle 0 < theta < 2 pi. A theta-cone is a cone with apex angle theta. We call a theta-cone empty (with respect to G) if it does not contain any point of G. A point p in R^2 is called theta-guarded if every theta-cone with its apex located at p is non-empty. Furthermore, the set of all theta-guarded points is called the theta-guarded region, or the theta-region for short.   We present several results on this topic. The main contribution of our work is to describe the theta-region with O(n/theta) circular arcs, and we give an algorithm to compute it. We prove a tight O(n) worst-case bound on the complexity of the theta-region for theta >= pi/2. In case theta is bounded from below by a positive constant, we prove an almost linear bound O(n^(1+epsilon)) for any epsilon > 0 on the complexity. Moreover, we show that there is a sequence of inputs such that the asymptotic bound on the complexity of their theta-region is Omega(n^2). In addition we point out gaps in the proofs of a recent publication that claims an O(n) bound on the complexity for any constant angle theta.",
        "published": "2008-06-12T14:42:48Z",
        "link": "http://arxiv.org/abs/0806.2090v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Triangulation of Simple 3D Shapes with Well-Centered Tetrahedra",
        "authors": [
            "Evan VanderZee",
            "Anil N. Hirani",
            "Damrong Guoy"
        ],
        "summary": "A completely well-centered tetrahedral mesh is a triangulation of a three dimensional domain in which every tetrahedron and every triangle contains its circumcenter in its interior. Such meshes have applications in scientific computing and other fields. We show how to triangulate simple domains using completely well-centered tetrahedra. The domains we consider here are space, infinite slab, infinite rectangular prism, cube and regular tetrahedron. We also demonstrate single tetrahedra with various combinations of the properties of dihedral acuteness, 2-well-centeredness and 3-well-centeredness.",
        "published": "2008-06-13T20:54:14Z",
        "link": "http://arxiv.org/abs/0806.2332v2",
        "categories": [
            "cs.CG",
            "cs.NA",
            "I.3.5"
        ]
    },
    {
        "title": "Existence of a polyhedron which does not have a non-overlapping   pseudo-edge unfolding",
        "authors": [
            "Alexey S Tarasov"
        ],
        "summary": "There exists a surface of a convex polyhedron P and a partition L of P into geodesic convex polygons such that there are no connected \"edge\" unfoldings of P without self-intersections (whose spanning tree is a subset of the edge skeleton of L).",
        "published": "2008-06-14T06:57:50Z",
        "link": "http://arxiv.org/abs/0806.2360v3",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Biased Range Trees",
        "authors": [
            "Vida Dujmovic",
            "John Howat",
            "Pat Morin"
        ],
        "summary": "A data structure, called a biased range tree, is presented that preprocesses a set S of n points in R^2 and a query distribution D for 2-sided orthogonal range counting queries. The expected query time for this data structure, when queries are drawn according to D, matches, to within a constant factor, that of the optimal decision tree for S and D. The memory and preprocessing requirements of the data structure are O(n log n).",
        "published": "2008-06-17T15:18:40Z",
        "link": "http://arxiv.org/abs/0806.2707v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "I.3.5"
        ]
    },
    {
        "title": "On Computing the Vertex Centroid of a Polyhedron",
        "authors": [
            "Khaled Elbassioni",
            "Hans Raj Tiwary"
        ],
        "summary": "Let $\\mathcal{P}$ be an $\\mathcal{H}$-polytope in $\\mathbb{R}^d$ with vertex set $V$. The vertex centroid is defined as the average of the vertices in $V$. We prove that computing the vertex centroid of an $\\mathcal{H}$-polytope is #P-hard. Moreover, we show that even just checking whether the vertex centroid lies in a given halfspace is already #P-hard for $\\mathcal{H}$-polytopes. We also consider the problem of approximating the vertex centroid by finding a point within an $\\epsilon$ distance from it and prove this problem to be #P-easy by showing that given an oracle for counting the number of vertices of an $\\mathcal{H}$-polytope, one can approximate the vertex centroid in polynomial time. We also show that any algorithm approximating the vertex centroid to \\emph{any} ``sufficiently'' non-trivial (for example constant) distance, can be used to construct a fully polynomial approximation scheme for approximating the centroid and also an output-sensitive polynomial algorithm for the Vertex Enumeration problem. Finally, we show that for unbounded polyhedra the vertex centroid can not be approximated to a distance of $d^{{1/2}-\\delta}$ for any fixed constant $\\delta>0$.",
        "published": "2008-06-20T20:56:19Z",
        "link": "http://arxiv.org/abs/0806.3456v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "An Efficient Algorithm for 2D Euclidean 2-Center with Outliers",
        "authors": [
            "Pankaj K. Agarwal",
            "Jeff M. Phillips"
        ],
        "summary": "For a set P of n points in R^2, the Euclidean 2-center problem computes a pair of congruent disks of the minimal radius that cover P. We extend this to the (2,k)-center problem where we compute the minimal radius pair of congruent disks to cover n-k points of P. We present a randomized algorithm with O(n k^7 log^3 n) expected running time for the (2,k)-center problem. We also study the (p,k)-center problem in R}^2 under the \\ell_\\infty-metric. We give solutions for p=4 in O(k^{O(1)} n log n) time and for p=5 in O(k^{O(1)} n log^5 n) time.",
        "published": "2008-06-26T15:10:28Z",
        "link": "http://arxiv.org/abs/0806.4326v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Space Efficient Multi-Dimensional Range Reporting",
        "authors": [
            "Marek Karpinski",
            "Yakov Nekrich"
        ],
        "summary": "We present a data structure that supports three-dimensional range reporting queries in $O(\\log \\log U + (\\log \\log n)^3+k)$ time and uses $O(n\\log^{1+\\eps} n)$ space, where $U$ is the size of the universe, $k$ is the number of points in the answer,and $\\eps$ is an arbitrary constant. This result improves over the data structure of Alstrup, Brodal, and Rauhe (FOCS 2000) that uses $O(n\\log^{1+\\eps} n)$ space and supports queries in $O(\\log n+k)$ time,the data structure of Nekrich (SoCG'07) that uses $O(n\\log^{3} n)$ space and supports queries in $O(\\log \\log U + (\\log \\log n)^2 + k)$ time, and the data structure of Afshani (ESA'08) that uses $O(n\\log^{3} n)$ space and also supports queries in $O(\\log \\log U + (\\log \\log n)^2 + k)$ time but relies on randomization during the preprocessing stage. Our result allows us to significantly reduce the space usage of the fastest previously known static and incremental $d$-dimensional data structures, $d\\geq 3$, at a cost of increasing the query time by a negligible $O(\\log \\log n)$ factor.",
        "published": "2008-06-26T16:32:57Z",
        "link": "http://arxiv.org/abs/0806.4361v2",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "Locality and Bounding-Box Quality of Two-Dimensional Space-Filling   Curves",
        "authors": [
            "Herman Haverkort",
            "Freek van Walderveen"
        ],
        "summary": "Space-filling curves can be used to organise points in the plane into bounding-box hierarchies (such as R-trees). We develop measures of the bounding-box quality of space-filling curves that express how effective different space-filling curves are for this purpose. We give general lower bounds on the bounding-box quality measures and on locality according to Gotsman and Lindenbaum for a large class of space-filling curves. We describe a generic algorithm to approximate these and similar quality measures for any given curve. Using our algorithm we find good approximations of the locality and the bounding-box quality of several known and new space-filling curves. Surprisingly, some curves with relatively bad locality by Gotsman and Lindenbaum's measure, have good bounding-box quality, while the curve with the best-known locality has relatively bad bounding-box quality.",
        "published": "2008-06-29T21:47:15Z",
        "link": "http://arxiv.org/abs/0806.4787v2",
        "categories": [
            "cs.CG",
            "cs.DB"
        ]
    },
    {
        "title": "On stars and Steiner stars. II",
        "authors": [
            "Adrian Dumitrescu",
            "Csaba D. Tóth",
            "Guangwu Xu"
        ],
        "summary": "A {\\em Steiner star} for a set $P$ of $n$ points in $\\RR^d$ connects an arbitrary center point to all points of $P$, while a {\\em star} connects a point $p\\in P$ to the remaining $n-1$ points of $P$. All connections are realized by straight line segments. Fekete and Meijer showed that the minimum star is at most $\\sqrt{2}$ times longer than the minimum Steiner star for any finite point configuration in $\\RR^d$. The maximum ratio between them, over all finite point configurations in $\\RR^d$, is called the {\\em star Steiner ratio} in $\\RR^d$. It is conjectured that this ratio is $4/\\pi = 1.2732...$ in the plane and $4/3=1.3333...$ in three dimensions. Here we give upper bounds of 1.3631 in the plane, and 1.3833 in 3-space, thereby substantially improving recent upper bounds of 1.3999, and $\\sqrt{2}-10^{-4}$, respectively. Our results also imply improved bounds on the maximum ratios between the minimum star and the maximum matching in two and three dimensions.",
        "published": "2008-06-30T19:46:43Z",
        "link": "http://arxiv.org/abs/0806.4858v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Hardness of embedding simplicial complexes in $\\R^d$",
        "authors": [
            "Jiří Matoušek",
            "Martin Tancer",
            "Uli Wagner"
        ],
        "summary": "Let EMBED(k,d) be the following algorithmic problem: Given a finite simplicial complex K of dimension at most k, does there exist a (piecewise linear) embedding of K into R^d? Known results easily imply polynomiality of EMBED(k,2) (k=1,2; the case k=1, d=2 is graph planarity) and of EMBED(k,2k) for all k>2 (even if k is not considered fixed).   We observe that the celebrated result of Novikov on the algorithmic unsolvability of recognizing the 5-sphere implies that EMBED(d,d) and EMBED(d-1,d) are undecidable for each d>4. Our main result is NP-hardness of EMBED(2,4) and, more generally, of EMBED(k,d) for all k,d with d>3 and d\\geq k \\geq (2d-2)/3. These dimensions fall outside the so-called metastable range of a theorem of Haefliger and Weber, which characterizes embeddability using the deleted product obstruction. Our reductions are based on examples, due to Segal, Spie\\.z, Freedman, Krushkal, Teichner, and Skopenkov, showing that outside the metastable range the deleted product obstruction is not sufficient to characterize embeddability.",
        "published": "2008-07-02T14:00:47Z",
        "link": "http://arxiv.org/abs/0807.0336v2",
        "categories": [
            "cs.CG",
            "math.GT"
        ]
    },
    {
        "title": "Improved bounds and new techniques for Davenport-Schinzel sequences and   their generalizations",
        "authors": [
            "Gabriel Nivasch"
        ],
        "summary": "Let lambda_s(n) denote the maximum length of a Davenport-Schinzel sequence of order s on n symbols. For s=3 it is known that lambda_3(n) = Theta(n alpha(n)) (Hart and Sharir, 1986). For general s>=4 there are almost-tight upper and lower bounds, both of the form n * 2^poly(alpha(n)) (Agarwal, Sharir, and Shor, 1989). Our first result is an improvement of the upper-bound technique of Agarwal et al. We obtain improved upper bounds for s>=6, which are tight for even s up to lower-order terms in the exponent. More importantly, we also present a new technique for deriving upper bounds for lambda_s(n). With this new technique we: (1) re-derive the upper bound of lambda_3(n) <= 2n alpha(n) + O(n sqrt alpha(n)) (first shown by Klazar, 1999); (2) re-derive our own new upper bounds for general s; and (3) obtain improved upper bounds for the generalized Davenport-Schinzel sequences considered by Adamec, Klazar, and Valtr (1992). Regarding lower bounds, we show that lambda_3(n) >= 2n alpha(n) - O(n), and therefore, the coefficient 2 is tight. We also present a simpler version of the construction of Agarwal, Sharir, and Shor that achieves the known lower bounds for even s>=4.",
        "published": "2008-07-03T05:08:29Z",
        "link": "http://arxiv.org/abs/0807.0484v3",
        "categories": [
            "cs.DM",
            "cs.CG"
        ]
    },
    {
        "title": "Decomposition of Multiple Coverings into More Parts",
        "authors": [
            "G. Aloupis",
            "J. Cardinal",
            "S. Collette",
            "S. Langerman",
            "D. Orden",
            "P. Ramos"
        ],
        "summary": "We prove that for every centrally symmetric convex polygon Q, there exists a constant alpha such that any alpha*k-fold covering of the plane by translates of Q can be decomposed into k coverings. This improves on a quadratic upper bound proved by Pach and Toth (SoCG'07). The question is motivated by a sensor network problem, in which a region has to be monitored by sensors with limited battery lifetime.",
        "published": "2008-07-03T13:03:47Z",
        "link": "http://arxiv.org/abs/0807.0552v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Line Transversals of Convex Polyhedra in $\\reals^3$",
        "authors": [
            "Haim Kaplan",
            "Natan Rubin",
            "Micha Sharir"
        ],
        "summary": "We establish a bound of $O(n^2k^{1+\\eps})$, for any $\\eps>0$, on the combinatorial complexity of the set $\\T$ of line transversals of a collection $\\P$ of $k$ convex polyhedra in $\\reals^3$ with a total of $n$ facets, and present a randomized algorithm which computes the boundary of $\\T$ in comparable expected time. Thus, when $k\\ll n$, the new bounds on the complexity (and construction cost) of $\\T$ improve upon the previously best known bounds, which are nearly cubic in $n$.   To obtain the above result, we study the set $\\TL$ of line transversals which emanate from a fixed line $\\ell_0$, establish an almost tight bound of $O(nk^{1+\\eps})$ on the complexity of $\\TL$, and provide a randomized algorithm which computes $\\TL$ in comparable expected time. Slightly improved combinatorial bounds for the complexity of $\\TL$, and comparable improvements in the cost of constructing this set, are established for two special cases, both assuming that the polyhedra of $\\P$ are pairwise disjoint: the case where $\\ell_0$ is disjoint from the polyhedra of $\\P$, and the case where the polyhedra of $\\P$ are unbounded in a direction parallel to $\\ell_0$.",
        "published": "2008-07-08T11:55:45Z",
        "link": "http://arxiv.org/abs/0807.1221v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "The Johnson-Lindenstrauss lemma almost characterizes Hilbert space, but   not quite",
        "authors": [
            "William B. Johnson",
            "Assaf Naor"
        ],
        "summary": "Let $X$ be a normed space that satisfies the Johnson-Lindenstrauss lemma (J-L lemma, in short) in the sense that for any integer $n$ and any $x_1,\\ldots,x_n\\in X$ there exists a linear mapping $L:X\\to F$, where $F\\subseteq X$ is a linear subspace of dimension $O(\\log n)$, such that $\\|x_i-x_j\\|\\le\\|L(x_i)-L(x_j)\\|\\le O(1)\\cdot\\|x_i-x_j\\|$ for all $i,j\\in \\{1,\\ldots, n\\}$. We show that this implies that $X$ is almost Euclidean in the following sense: Every $n$-dimensional subspace of $X$ embeds into Hilbert space with distortion $2^{2^{O(\\log^*n)}}$. On the other hand, we show that there exists a normed space $Y$ which satisfies the J-L lemma, but for every $n$ there exists an $n$-dimensional subspace $E_n\\subseteq Y$ whose Euclidean distortion is at least $2^{\\Omega(\\alpha(n))}$, where $\\alpha$ is the inverse Ackermann function.",
        "published": "2008-07-11T19:39:07Z",
        "link": "http://arxiv.org/abs/0807.1919v1",
        "categories": [
            "math.FA",
            "cs.CG",
            "math.MG"
        ]
    },
    {
        "title": "Ranking Unit Squares with Few Visibilities",
        "authors": [
            "Bernd Gärtner"
        ],
        "summary": "Given a set of n unit squares in the plane, the goal is to rank them in space in such a way that only few squares see each other vertically. We prove that ranking the squares according to the lexicographic order of their centers results in at most 3n-7 pairwise visibilities for n at least 4. We also show that this bound is best possible, by exhibiting a set of n squares with at least 3n-7 pairwise visibilities under any ranking.",
        "published": "2008-07-14T15:23:52Z",
        "link": "http://arxiv.org/abs/0807.2178v1",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Isometric Diamond Subgraphs",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We describe polynomial time algorithms for determining whether an undirected graph may be embedded in a distance-preserving way into the hexagonal tiling of the plane, the diamond structure in three dimensions, or analogous structures in higher dimensions. The graphs that may be embedded in this way form an interesting subclass of the partial cubes.",
        "published": "2008-07-14T18:01:19Z",
        "link": "http://arxiv.org/abs/0807.2218v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Polygon Exploration with Time-Discrete Vision",
        "authors": [
            "Sandor P. Fekete",
            "Christiane Schmidt"
        ],
        "summary": "With the advent of autonomous robots with two- and three-dimensional scanning capabilities, classical visibility-based exploration methods from computational geometry have gained in practical importance. However, real-life laser scanning of useful accuracy does not allow the robot to scan continuously while in motion; instead, it has to stop each time it surveys its environment. This requirement was studied by Fekete, Klein and Nuechter for the subproblem of looking around a corner, but until now has not been considered in an online setting for whole polygonal regions.   We give the first algorithmic results for this important algorithmic problem that combines stationary art gallery-type aspects with watchman-type issues in an online scenario: We demonstrate that even for orthoconvex polygons, a competitive strategy can be achieved only for limited aspect ratio A (the ratio of the maximum and minimum edge length of the polygon), i.e., for a given lower bound on the size of an edge; we give a matching upper bound by providing an O(log A)-competitive strategy for simple rectilinear polygons, using the assumption that each edge of the polygon has to be fully visible from some scan point.",
        "published": "2008-07-15T12:10:08Z",
        "link": "http://arxiv.org/abs/0807.2358v2",
        "categories": [
            "cs.CG",
            "cs.RO",
            "F.2.2; I.2.9"
        ]
    },
    {
        "title": "Inapproximability for metric embeddings into R^d",
        "authors": [
            "Jiri Matousek",
            "Anastasios Sidiropoulos"
        ],
        "summary": "We consider the problem of computing the smallest possible distortion for embedding of a given n-point metric space into R^d, where d is fixed (and small). For d=1, it was known that approximating the minimum distortion with a factor better than roughly n^(1/12) is NP-hard. From this result we derive inapproximability with factor roughly n^(1/(22d-10)) for every fixed d\\ge 2, by a conceptually very simple reduction. However, the proof of correctness involves a nontrivial result in geometric topology (whose current proof is based on ideas due to Jussi Vaisala).   For d\\ge 3, we obtain a stronger inapproximability result by a different reduction: assuming P \\ne NP, no polynomial-time algorithm can distinguish between spaces embeddable in R^d with constant distortion from spaces requiring distortion at least n^(c/d), for a constant c>0. The exponent c/d has the correct order of magnitude, since every n-point metric space can be embedded in R^d with distortion O(n^{2/d}\\log^{3/2}n) and such an embedding can be constructed in polynomial time by random projection.   For d=2, we give an example of a metric space that requires a large distortion for embedding in R^2, while all not too large subspaces of it embed almost isometrically.",
        "published": "2008-07-15T19:21:09Z",
        "link": "http://arxiv.org/abs/0807.2472v1",
        "categories": [
            "cs.CG",
            "cs.CC"
        ]
    },
    {
        "title": "Modularity clustering is force-directed layout",
        "authors": [
            "Andreas Noack"
        ],
        "summary": "Two natural and widely used representations for the community structure of networks are clusterings, which partition the vertex set into disjoint subsets, and layouts, which assign the vertices to positions in a metric space. This paper unifies prominent characterizations of layout quality and clustering quality, by showing that energy models of pairwise attraction and repulsion subsume Newman and Girvan's modularity measure. Layouts with optimal energy are relaxations of, and are thus consistent with, clusterings with optimal modularity, which is of practical relevance because both representations are complementary and often used together.",
        "published": "2008-07-25T19:56:08Z",
        "link": "http://arxiv.org/abs/0807.4052v2",
        "categories": [
            "cs.DM",
            "cs.CG",
            "physics.soc-ph",
            "G.2.2; G.2.3; I.5.3"
        ]
    },
    {
        "title": "Eigenvalue bounds, spectral partitioning, and metrical deformations via   flows",
        "authors": [
            "Punyashloka Biswal",
            "James R. Lee",
            "Satish Rao"
        ],
        "summary": "We present a new method for upper bounding the second eigenvalue of the Laplacian of graphs. Our approach uses multi-commodity flows to deform the geometry of the graph; we embed the resulting metric into Euclidean space to recover a bound on the Rayleigh quotient. Using this, we show that every $n$-vertex graph of genus $g$ and maximum degree $d$ satisfies $\\lambda_2(G) = O((g+1)^3 d/n)$. This recovers the $O(d/n)$ bound of Spielman and Teng for planar graphs, and compares to Kelner's bound of $O((g+1) poly(d)/n)$, but our proof does not make use of conformal mappings or circle packings. We are thus able to extend this to resolve positively a conjecture of Spielman and Teng, by proving that $\\lambda_2(G) = O(d h^6 \\log h/n)$ whenever $G$ is $K_h$-minor free. This shows, in particular, that spectral partitioning can be used to recover $O(\\sqrt{n})$-sized separators in bounded degree graphs that exclude a fixed minor. We extend this further by obtaining nearly optimal bounds on $\\lambda_2$ for graphs which exclude small-depth minors in the sense of Plotkin, Rao, and Smith. Consequently, we show that spectral algorithms find small separators in a general class of geometric graphs.   Moreover, while the standard \"sweep\" algorithm applied to the second eigenvector may fail to find good quotient cuts in graphs of unbounded degree, our approach produces a vector that works for arbitrary graphs. This yields an alternate proof of the result of Alon, Seymour, and Thomas that every excluded-minor family of graphs has $O(\\sqrt{n})$-node balanced separators.",
        "published": "2008-08-01T16:45:16Z",
        "link": "http://arxiv.org/abs/0808.0148v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "math.MG",
            "math.SP"
        ]
    },
    {
        "title": "Uniqueness of certain polynomials constant on a line",
        "authors": [
            "Jiri Lebl",
            "Daniel Lichtblau"
        ],
        "summary": "We study a question with connections to linear algebra, real algebraic geometry, combinatorics, and complex analysis. Let $p(x,y)$ be a polynomial of degree $d$ with $N$ positive coefficients and no negative coefficients, such that $p=1$ when $x+y=1$. A sharp estimate $d \\leq 2N-3$ is known. In this paper we study the $p$ for which equality holds. We prove some new results about the form of these \"sharp\" polynomials. Using these new results and using two independent computational methods we give a complete classification of these polynomials up to $d=17$. The question is motivated by the problem of classification of CR maps between spheres in different dimensions.",
        "published": "2008-08-03T00:22:30Z",
        "link": "http://arxiv.org/abs/0808.0284v3",
        "categories": [
            "math.CV",
            "cs.CG",
            "math.NT",
            "32H35, 68W30, 11C08, 05E99"
        ]
    },
    {
        "title": "Dynamic Connectivity: Connecting to Networks and Geometry",
        "authors": [
            "Timothy M. Chan",
            "Mihai Patrascu",
            "Liam Roditty"
        ],
        "summary": "Dynamic connectivity is a well-studied problem, but so far the most compelling progress has been confined to the edge-update model: maintain an understanding of connectivity in an undirected graph, subject to edge insertions and deletions. In this paper, we study two more challenging, yet equally fundamental problems.   Subgraph connectivity asks to maintain an understanding of connectivity under vertex updates: updates can turn vertices on and off, and queries refer to the subgraph induced by \"on\" vertices. (For instance, this is closer to applications in networks of routers, where node faults may occur.)   We describe a data structure supporting vertex updates in O (m^{2/3}) amortized time, where m denotes the number of edges in the graph. This greatly improves over the previous result [Chan, STOC'02], which required fast matrix multiplication and had an update time of O(m^0.94). The new data structure is also simpler.   Geometric connectivity asks to maintain a dynamic set of n geometric objects, and query connectivity in their intersection graph. (For instance, the intersection graph of balls describes connectivity in a network of sensors with bounded transmission radius.)   Previously, nontrivial fully dynamic results were known only for special cases like axis-parallel line segments and rectangles. We provide similarly improved update times, O (n^{2/3}), for these special cases. Moreover, we show how to obtain sublinear update bounds for virtually all families of geometric objects which allow sublinear-time range queries, such as arbitrary 2D line segments, d-dimensional simplices, and d-dimensional balls.",
        "published": "2008-08-07T22:16:15Z",
        "link": "http://arxiv.org/abs/0808.1128v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "Spatial planning with constraints on translational distances between   geometric objects",
        "authors": [
            "Gennady Pustylnik"
        ],
        "summary": "The main constraint on relative position of geometric objects, used in spatial planning for computing the C-space maps (for example, in robotics, CAD, and packaging), is the relative non-overlapping of objects. This is the simplest constraint in which the minimum translational distance between objects is greater than zero, or more generally, than some positive value. We present a technique, based on the Minkowski operations, for generating the translational C-space maps for spatial planning with more general and more complex constraints on the relative position of geometric objects, such as constraints on various types (not only on the minimum) of the translational distances between objects. The developed technique can also be used, respectively, for spatial planning with constraints on translational distances in a given direction, and rotational distances between geometric objects, as well as for spatial planning with given dynamic geometric situation of moving objects.",
        "published": "2008-08-21T14:00:36Z",
        "link": "http://arxiv.org/abs/0808.2931v1",
        "categories": [
            "cs.CG",
            "cs.RO"
        ]
    },
    {
        "title": "Studying Geometric Graph Properties of Road Networks Through an   Algorithmic Lens",
        "authors": [
            "David Eppstein",
            "Michael T. Goodrich"
        ],
        "summary": "This paper studies real-world road networks from an algorithmic perspective, focusing on empirical studies that yield useful properties of road networks that can be exploited in the design of fast algorithms that deal with geographic data. Unlike previous approaches, our study is not based on the assumption that road networks are planar graphs. Indeed, based on the a number of experiments we have performed on the road networks of the 50 United States and District of Columbia, we provide strong empirical evidence that road networks are quite non-planar. Our approach therefore instead is directed at finding algorithmically-motivated properties of road networks as non-planar geometric graphs, focusing on alternative properties of road networks that can still lead to efficient algorithms for such problems as shortest paths and Voronoi diagrams. In particular, we study road networks as multiscale-dispersed graphs, which is a concept we formalize in terms of disk neighborhood systems. This approach allows us to develop fast algorithms for road networks without making any additional assumptions about the distribution of edge weights. In fact, our algorithms can allow for non-metric weights.",
        "published": "2008-08-27T19:00:29Z",
        "link": "http://arxiv.org/abs/0808.3694v2",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Optimizing the double description method for normal surface enumeration",
        "authors": [
            "Benjamin A. Burton"
        ],
        "summary": "Many key algorithms in 3-manifold topology involve the enumeration of normal surfaces, which is based upon the double description method for finding the vertices of a convex polytope. Typically we are only interested in a small subset of these vertices, thus opening the way for substantial optimization. Here we give an account of the vertex enumeration problem as it applies to normal surfaces, and present new optimizations that yield strong improvements in both running time and memory consumption. The resulting algorithms are tested using the freely available software package Regina.",
        "published": "2008-08-29T09:01:31Z",
        "link": "http://arxiv.org/abs/0808.4050v3",
        "categories": [
            "math.GT",
            "cs.CG",
            "math.CO",
            "52B55 (Primary) 57N10, 57N35 (Secondary)"
        ]
    },
    {
        "title": "Improved Approximations for Guarding 1.5-Dimensional Terrains",
        "authors": [
            "K. Elbassioni",
            "D. Matijevic",
            "J. Mestre",
            "D. Severdija"
        ],
        "summary": "We present a 4-approximation algorithm for the problem of placing a fewest guards on a 1.5D terrain so that every point of the terrain is seen by at least one guard. This improves on the currently best approximation factor of 5. Our method is based on rounding the linear programming relaxation of the corresponding covering problem. Besides the simplicity of the analysis, which mainly relies on decomposing the constraint matrix of the LP into totally balanced matrices, our algorithm, unlike previous work, generalizes to the weighted and partial versions of the basic problem.",
        "published": "2008-09-01T16:13:30Z",
        "link": "http://arxiv.org/abs/0809.0159v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Approximating the volume of unions and intersections of high-dimensional   geometric objects",
        "authors": [
            "Karl Bringmann",
            "Tobias Friedrich"
        ],
        "summary": "We consider the computation of the volume of the union of high-dimensional geometric objects. While showing that this problem is #P-hard already for very simple bodies (i.e., axis-parallel boxes), we give a fast FPRAS for all objects where one can: (1) test whether a given point lies inside the object, (2) sample a point uniformly, (3) calculate the volume of the object in polynomial time. All three oracles can be weak, that is, just approximate. This implies that Klee's measure problem and the hypervolume indicator can be approximated efficiently even though they are #P-hard and hence cannot be solved exactly in time polynomial in the number of dimensions unless P=NP. Our algorithm also allows to approximate efficiently the volume of the union of convex bodies given by weak membership oracles.   For the analogous problem of the intersection of high-dimensional geometric objects we prove #P-hardness for boxes and show that there is no multiplicative polynomial-time $2^{d^{1-\\epsilon}}$-approximation for certain boxes unless NP=BPP, but give a simple additive polynomial-time $\\epsilon$-approximation.",
        "published": "2008-09-04T16:14:09Z",
        "link": "http://arxiv.org/abs/0809.0835v2",
        "categories": [
            "cs.CG",
            "cs.NE"
        ]
    },
    {
        "title": "Minkowski Sum Selection and Finding",
        "authors": [
            "Cheng-Wei Luo",
            "Hsiao-Fei Liu",
            "Peng-An Chen",
            "Kun-Mao Chao"
        ],
        "summary": "For the \\textsc{Minkowski Sum Selection} problem with linear objective functions, we obtain the following results: (1) optimal $O(n\\log n)$ time algorithms for $\\lambda=1$; (2) $O(n\\log^2 n)$ time deterministic algorithms and expected $O(n\\log n)$ time randomized algorithms for any fixed $\\lambda>1$. For the \\textsc{Minkowski Sum Finding} problem with linear objective functions or objective functions of the form   $f(x,y)=\\frac{by}{ax}$, we construct optimal $O(n\\log n)$ time algorithms for any fixed $\\lambda\\geq 1$.",
        "published": "2008-09-06T14:31:49Z",
        "link": "http://arxiv.org/abs/0809.1171v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "Obtaining Exact Interpolation Multivariate Polynomial by Approximation",
        "authors": [
            "Yong Feng",
            "Jingzhong Zhang",
            "Xiaolin Qin",
            "Xun Yuan"
        ],
        "summary": "In some fields such as Mathematics Mechanization, automated reasoning and Trustworthy Computing etc., exact results are needed. Symbolic computations are used to obtain the exact results. Symbolic computations are of high complexity. In order to improve the situation, exactly interpolating methods are often proposed for the exact results and approximate interpolating methods for the approximate ones. In this paper, we study how to obtain exact interpolation polynomial with rational coefficients by approximate interpolating methods.",
        "published": "2008-09-09T02:33:30Z",
        "link": "http://arxiv.org/abs/0809.1476v1",
        "categories": [
            "cs.SC",
            "cs.CG"
        ]
    },
    {
        "title": "The fully connected N-dimensional skeleton: probing the evolution of the   cosmic web",
        "authors": [
            "T. Sousbie",
            "S. Colombi",
            "C. Pichon"
        ],
        "summary": "A method to compute the full hierarchy of the critical subsets of a density field is presented. It is based on a watershed technique and uses a probability propagation scheme to improve the quality of the segmentation by circumventing the discreteness of the sampling. It can be applied within spaces of arbitrary dimensions and geometry. This recursive segmentation of space yields, for a $d$-dimensional space, a $d-1$ succession of $n$-dimensional subspaces that fully characterize the topology of the density field. The final 1D manifold of the hierarchy is the fully connected network of the primary critical lines of the field : the skeleton. It corresponds to the subset of lines linking maxima to saddle points, and provides a definition of the filaments that compose the cosmic web as a precise physical object, which makes it possible to compute any of its properties such as its length, curvature, connectivity etc... When the skeleton extraction is applied to initial conditions of cosmological N-body simulations and their present day non linear counterparts, it is shown that the time evolution of the cosmic web, as traced by the skeleton, is well accounted for by the Zel'dovich approximation. Comparing this skeleton to the initial skeleton undergoing the Zel'dovich mapping shows that two effects are competing during the formation of the cosmic web: a general dilation of the larger filaments that is captured by a simple deformation of the skeleton of the initial conditions on the one hand, and the shrinking, fusion and disappearance of the more numerous smaller filaments on the other hand. Other applications of the N dimensional skeleton and its peak patch hierarchy are discussed.",
        "published": "2008-09-14T22:22:05Z",
        "link": "http://arxiv.org/abs/0809.2423v2",
        "categories": [
            "astro-ph",
            "cs.CG",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Largest Empty Circle Centered on a Query Line",
        "authors": [
            "John Augustine",
            "Brian Putnam",
            "Sasanka Roy"
        ],
        "summary": "The Largest Empty Circle problem seeks the largest circle centered within the convex hull of a set $P$ of $n$ points in $\\mathbb{R}^2$ and devoid of points from $P$. In this paper, we introduce a query version of this well-studied problem. In our query version, we are required to preprocess $P$ so that when given a query line $Q$, we can quickly compute the largest empty circle centered at some point on $Q$ and within the convex hull of $P$.   We present solutions for two special cases and the general case; all our queries run in $O(\\log n)$ time. We restrict the query line to be horizontal in the first special case, which we preprocess in $O(n \\alpha(n) \\log n)$ time and space, where $\\alpha(n)$ is the slow growing inverse of the Ackermann's function. When the query line is restricted to pass through a fixed point, the second special case, our preprocessing takes $O(n \\alpha(n)^{O(\\alpha(n))} \\log n)$ time and space. We use insights from the two special cases to solve the general version of the problem with preprocessing time and space in $O(n^3 \\log n)$ and $O(n^3)$ respectively.",
        "published": "2008-09-16T15:00:50Z",
        "link": "http://arxiv.org/abs/0809.2651v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "A Simple Framework to Typify Social Bibliographic Communities",
        "authors": [
            "Christoph Schommer"
        ],
        "summary": "Social Communities in bibliographic databases exist since many years, researchers share common research interests, and work and publish together. A social community may vary in type and size, being fully connected between participating members or even more expressed by a consortium of small and individual members who play individual roles in it. In this work, we focus on social communities inside the bibliographic database DBLP and characterize communities through a simple typifying description model. Generally, we understand a publication as a transaction between the associated authors. The idea therefore is to concern with directed associative relationships among them, to decompose each pattern to its fundamental structure, and to describe the communities by expressive attributes. Finally, we argue that the decomposition supports the management of discovered structures towards the use of adaptive-incremental mind-maps.",
        "published": "2008-09-16T21:25:41Z",
        "link": "http://arxiv.org/abs/0809.2818v1",
        "categories": [
            "cs.DL",
            "cs.CG",
            "H.3.1; H.3.6"
        ]
    },
    {
        "title": "Communication-Efficient Construction of the Plane Localized Delaunay   Graph",
        "authors": [
            "Prosenjit Bose",
            "Paz Carmi",
            "Michiel Smid",
            "Daming Xu"
        ],
        "summary": "Let $V$ be a finite set of points in the plane. We present a 2-local algorithm that constructs a plane $\\frac{4 \\pi \\sqrt{3}}{9}$-spanner of the unit-disk graph $\\UDG(V)$. This algorithm makes only one round of communication and each point of $V$ broadcasts at most 5 messages. This improves the previously best message-bound of 11 by Ara\\'{u}jo and Rodrigues (Fast localized Delaunay triangulation, Lecture Notes in Computer Science, volume 3544, 2004).",
        "published": "2008-09-17T16:28:06Z",
        "link": "http://arxiv.org/abs/0809.2956v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Characterizing graphs with convex and connected configuration spaces",
        "authors": [
            "Meera Sitharam",
            "Heping Gao"
        ],
        "summary": "We define and study exact, efficient representations of realization spaces Euclidean Distance Constraint Systems (EDCS), which includes Linkages and Frameworks. Each representation corresponds to a choice of Cayley parameters and yields a different parametrized configuration space. Significantly, we give purely graph-theoretic, forbidden minor characterizations that capture (i) the class of graphs that always admit efficient configuration spaces and (ii) the possible choices of representation parameters that yield efficient configuration spaces for a given graph. In addition, our results are tight: we show counterexamples to obvious extensions. This is the first step in a systematic and graded program of combinatorial characterizations of efficient configuration spaces. We discuss several future theoretical and applied research directions. Some of our proofs employ an unusual interplay of (a) classical analytic results related to positive semi-definiteness of Euclidean distance matrices, with (b) recent forbidden minor characterizations and algorithms related to the notion of d-realizability of EDCS. We further introduce a novel type of restricted edge contraction or reduction to a graph minor, a \"trick\" that we anticipate will be useful in other situations.",
        "published": "2008-09-23T15:44:54Z",
        "link": "http://arxiv.org/abs/0809.3935v6",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Perspective Drawing of Surfaces with Line Hidden Line Elimination,   Dibujando Superficies En Perspectiva Con Eliminacion De Lineas Ocultas",
        "authors": [
            "Ignacio Vega-Paez",
            "Jose Angel Ortega",
            "Georgina G. Pulido"
        ],
        "summary": "An efficient computer algorithm is described for the perspective drawing of a wide class of surfaces. The class includes surfaces corresponding lo single-valued, continuous functions which are defined over rectangular domains. The algorithm automatically computes and eliminates hidden lines. The number of computations in the algorithm grows linearly with the number of sample points on the surface to be drawn. An analysis of the algorithm is presented, and extensions lo certain multi-valued functions are indicated. The algorithm is implemented and tested on .Net 2.0 platform that left interactive use. Running times are found lo be exceedingly efficient for visualization, where interaction on-line and view-point control, enables effective and rapid examination of a surfaces from many perspectives.",
        "published": "2008-09-24T05:50:56Z",
        "link": "http://arxiv.org/abs/0809.4093v2",
        "categories": [
            "cs.GR",
            "cs.CG"
        ]
    },
    {
        "title": "Simulated annealing for weighted polygon packing",
        "authors": [
            "Yi-Chun Xu",
            "Ren-Bin Xiao",
            "Martyn Amos"
        ],
        "summary": "In this paper we present a new algorithm for a layout optimization problem: this concerns the placement of weighted polygons inside a circular container, the two objectives being to minimize imbalance of mass and to minimize the radius of the container. This problem carries real practical significance in industrial applications (such as the design of satellites), as well as being of significant theoretical interest. Previous work has dealt with circular or rectangular objects, but here we deal with the more realistic case where objects may be represented as polygons and the polygons are allowed to rotate. We present a solution based on simulated annealing and first test it on instances with known optima. Our results show that the algorithm obtains container radii that are close to optimal. We also compare our method with existing algorithms for the (special) rectangular case. Experimental results show that our approach out-performs these methods in terms of solution quality.",
        "published": "2008-09-29T16:22:28Z",
        "link": "http://arxiv.org/abs/0809.5005v1",
        "categories": [
            "cs.CG",
            "cs.AI"
        ]
    },
    {
        "title": "Planar Visibility Counting",
        "authors": [
            "Matthias Fischer",
            "Matthias Hilbig",
            "Claudius Jähn",
            "Friedhelm Meyer auf der Heide",
            "Martin Ziegler"
        ],
        "summary": "For a fixed virtual scene (=collection of simplices) S and given observer position p, how many elements of S are weakly visible (i.e. not fully occluded by others) from p? The present work explores the trade-off between query time and preprocessing space for these quantities in 2D: exactly, in the approximate deterministic, and in the probabilistic sense. We deduce the EXISTENCE of an O(m^2/n^2) space data structure for S that, given p and time O(log n), allows to approximate the ratio of occluded segments up to arbitrary constant absolute error; here m denotes the size of the Visibility Graph--which may be quadratic, but typically is just linear in the size n of the scene S. On the other hand, we present a data structure CONSTRUCTIBLE in O(n*log(n)+m^2*polylog(n)/k) preprocessing time and space with similar approximation properties and query time O(k*polylog n), where k<n is an arbitrary parameter. We describe an implementation of this approach and demonstrate the practical benefit of the parameter k to trade memory for query time in an empirical evaluation on three classes of benchmark scenes.",
        "published": "2008-10-01T01:53:52Z",
        "link": "http://arxiv.org/abs/0810.0052v3",
        "categories": [
            "cs.CG",
            "cs.DS",
            "I.3.5; F.2.2"
        ]
    },
    {
        "title": "Characterizing 1-Dof Henneberg-I graphs with efficient configuration   spaces",
        "authors": [
            "Heping Gao",
            "Meera Sitharam"
        ],
        "summary": "We define and study exact, efficient representations of realization spaces of a natural class of underconstrained 2D Euclidean Distance Constraint Systems(EDCS) or Frameworks based on 1-dof Henneberg-I graphs. Each representation corresponds to a choice of parameters and yields a different parametrized configuration space. Our notion of efficiency is based on the algebraic complexities of sampling the configuration space and of obtaining a realization from the sample (parametrized) configuration. Significantly, we give purely combinatorial characterizations that capture (i) the class of graphs that have efficient configuration spaces and (ii) the possible choices of representation parameters that yield efficient configuration spaces for a given graph. Our results automatically yield an efficient algorithm for sampling realizations, without missing extreme or boundary realizations. In addition, our results formally show that our definition of efficient configuration space is robust and that our characterizations are tight. We choose the class of 1-dof Henneberg-I graphs in order to take the next step in a systematic and graded program of combinatorial characterizations of efficient configuration spaces. In particular, the results presented here are the first characterizations that go beyond graphs that have connected and convex configuration spaces.",
        "published": "2008-10-12T20:17:21Z",
        "link": "http://arxiv.org/abs/0810.1997v2",
        "categories": [
            "cs.CG",
            "cs.RO",
            "cs.SC"
        ]
    },
    {
        "title": "Efficient Algorithmic Techniques for Several Multidimensional Geometric   Data Management and Analysis Problems",
        "authors": [
            "Mugurel Ionut Andreica"
        ],
        "summary": "In this paper I present several novel, efficient, algorithmic techniques for solving some multidimensional geometric data management and analysis problems. The techniques are based on several data structures from computational geometry (e.g. segment tree and range tree) and on the well-known sweep-line method.",
        "published": "2008-10-24T09:44:03Z",
        "link": "http://arxiv.org/abs/0810.4423v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Characterizing Graphs of Zonohedra",
        "authors": [
            "Muhammad Abdullah Adnan",
            "Masud Hasan"
        ],
        "summary": "A classic theorem by Steinitz states that a graph G is realizable by a convex polyhedron if and only if G is 3-connected planar. Zonohedra are an important subclass of convex polyhedra having the property that the faces of a zonohedron are parallelograms and are in parallel pairs. In this paper we give characterization of graphs of zonohedra. We also give a linear time algorithm to recognize such a graph. In our quest for finding the algorithm, we prove that in a zonohedron P both the number of zones and the number of faces in each zone is O(square root{n}), where n is the number of vertices of P.",
        "published": "2008-11-03T10:19:10Z",
        "link": "http://arxiv.org/abs/0811.0254v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Configuration spaces of convex and embedded polygons in the plane",
        "authors": [
            "Don Shimamoto",
            "Mary Wootters"
        ],
        "summary": "This paper studies the configuration spaces of linkages whose underlying graph is a single cycle. Assume that the edge lengths are such that there are no configurations in which all the edges lie along a line. The main results are that, modulo translations and rotations, each component of the space of convex configurations is homeomorphic to a closed Euclidean ball and each component of the space of embedded configurations is homeomorphic to a Euclidean space. This represents an elaboration on the topological information that follows from the convexification theorem of Connelly, Demaine, and Rote.",
        "published": "2008-11-10T14:24:02Z",
        "link": "http://arxiv.org/abs/0811.1365v1",
        "categories": [
            "cs.CG",
            "I.3.5"
        ]
    },
    {
        "title": "Necessary Conditions for Discontinuities of Multidimensional Size   Functions",
        "authors": [
            "Andrea Cerri",
            "Patrizio Frosini"
        ],
        "summary": "Some new results about multidimensional Topological Persistence are presented, proving that the discontinuity points of a k-dimensional size function are necessarily related to the pseudocritical or special values of the associated measuring function.",
        "published": "2008-11-12T11:58:15Z",
        "link": "http://arxiv.org/abs/0811.1868v2",
        "categories": [
            "cs.CG",
            "cs.CV",
            "math.AT"
        ]
    },
    {
        "title": "Evolutionary Construction of Geographical Networks with Nearly Optimal   Robustness and Efficient Routing Properties",
        "authors": [
            "Yukio Hayashi"
        ],
        "summary": "Robust and efficient design of networks on a realistic geographical space is one of the important issues for the realization of dependable communication systems. In this paper, based on a percolation theory and a geometric graph property, we investigate such a design from the following viewpoints: 1) network evolution according to a spatially heterogeneous population, 2) trimodal low degrees for the tolerant connectivity against both failures and attacks, and 3) decentralized routing within short paths. Furthermore, we point out the weakened tolerance by geographical constraints on local cycles, and propose a practical strategy by adding a small fraction of shortcut links between randomly chosen nodes in order to improve the robustness to a similar level to that of the optimal bimodal networks with a larger degree $O(\\sqrt{N})$ for the network size $N$. These properties will be useful for constructing future ad-hoc networks in wide-area communications.",
        "published": "2008-11-18T01:19:37Z",
        "link": "http://arxiv.org/abs/0811.2827v3",
        "categories": [
            "physics.data-an",
            "cs.CG",
            "cs.NI",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Automatic Generation of the Axial Lines of Urban Environments to Capture   What We Perceive",
        "authors": [
            "Bin Jiang",
            "Xintao Liu"
        ],
        "summary": "Based on the concepts of isovists and medial axes, we developed a set of algorithms that can automatically generate axial lines for representing individual linearly stretched parts of open space of an urban environment. Open space is the space between buildings, where people can freely move around. The generation of the axial lines has been a key aspect of space syntax research, conventionally relying on hand-drawn axial lines of an urban environment, often called axial map, for urban morphological analysis. Although various attempts have been made towards an automatic solution, few of them can produce the axial map that consists of the least number of longest visibility lines, and none of them really works for different urban environments. Our algorithms provide a better solution than existing ones. Throughout this paper, we have also argued and demonstrated that the axial lines constitute a true skeleton, superior to medial axes, in capturing what we perceive about the urban environment.   Keywords: Visibility, space syntax, topological analysis, medial axes, axial lines, isovists",
        "published": "2008-11-27T09:04:36Z",
        "link": "http://arxiv.org/abs/0811.4489v3",
        "categories": [
            "cs.CG",
            "cs.CV"
        ]
    },
    {
        "title": "Zigzag Persistence",
        "authors": [
            "Gunnar Carlsson",
            "Vin de Silva"
        ],
        "summary": "We describe a new methodology for studying persistence of topological features across a family of spaces or point-cloud data sets, called zigzag persistence. Building on classical results about quiver representations, zigzag persistence generalises the highly successful theory of persistent homology and addresses several situations which are not covered by that theory. In this paper we develop theoretical and algorithmic foundations with a view towards applications in topological statistics.",
        "published": "2008-12-01T02:08:08Z",
        "link": "http://arxiv.org/abs/0812.0197v1",
        "categories": [
            "cs.CG",
            "I.3.5"
        ]
    },
    {
        "title": "A Matlab Implementation of a Flat Norm Motivated Polygonal Edge Matching   Method using a Decomposition of Boundary into Four 1-Dimensional Currents",
        "authors": [
            "Simon P. Morgan",
            "Wotao Yin",
            "Kevin R. Vixie"
        ],
        "summary": "We describe and provide code and examples for a polygonal edge matching method.",
        "published": "2008-12-01T18:59:52Z",
        "link": "http://arxiv.org/abs/0812.0340v2",
        "categories": [
            "cs.CV",
            "cs.CG"
        ]
    },
    {
        "title": "k-means requires exponentially many iterations even in the plane",
        "authors": [
            "Andrea Vattani"
        ],
        "summary": "The k-means algorithm is a well-known method for partitioning n points that lie in the d-dimensional space into k clusters. Its main features are simplicity and speed in practice. Theoretically, however, the best known upper bound on its running time (i.e. O(n^{kd})) can be exponential in the number of points. Recently, Arthur and Vassilvitskii [3] showed a super-polynomial worst-case analysis, improving the best known lower bound from \\Omega(n) to 2^{\\Omega(\\sqrt{n})} with a construction in d=\\Omega(\\sqrt{n}) dimensions. In [3] they also conjectured the existence of superpolynomial lower bounds for any d >= 2.   Our contribution is twofold: we prove this conjecture and we improve the lower bound, by presenting a simple construction in the plane that leads to the exponential lower bound 2^{\\Omega(n)}.",
        "published": "2008-12-01T22:55:39Z",
        "link": "http://arxiv.org/abs/0812.0382v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Delaunay Triangulations in Linear Time? (Part I)",
        "authors": [
            "Kevin Buchin"
        ],
        "summary": "We present a new and simple randomized algorithm for constructing the Delaunay triangulation using nearest neighbor graphs for point location. Under suitable assumptions, it runs in linear expected time for points in the plane with polynomially bounded spread, i.e., if the ratio between the largest and smallest pointwise distance is polynomially bounded. This also holds for point sets with bounded spread in higher dimensions as long as the expected complexity of the Delaunay triangulation of a sample of the points is linear in the sample size.",
        "published": "2008-12-01T23:09:13Z",
        "link": "http://arxiv.org/abs/0812.0387v3",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Dilation, smoothed distance, and minimization diagrams of convex   functions",
        "authors": [
            "Matthew Dickerson",
            "David Eppstein",
            "Kevin A. Wortman"
        ],
        "summary": "We study Voronoi diagrams for distance functions that add together two convex functions, each taking as its argument the difference between Cartesian coordinates of two planar points. When the functions do not grow too quickly, then the Voronoi diagram has linear complexity and can be constructed in near-linear randomized expected time. Additionally, the level sets of the distances from the sites form a family of pseudocircles in the plane, all cells in the Voronoi diagram are connected, and the set of bisectors separating any one cell in the diagram from each of the others forms an arrangement of pseudolines in the plane. We apply these results to the smoothed distance or biotope transform metric, a geometric analogue of the Jaccard distance whose Voronoi diagrams can be used to determine the dilation of a star network with a given hub. For sufficiently closely spaced points in the plane, the Voronoi diagram of smoothed distance has linear complexity and can be computed efficiently. We also experiment with a variant of Lloyd's algorithm, adapted to smoothed distance, to find uniformly spaced point samples with exponentially decreasing density around a given point.",
        "published": "2008-12-02T22:09:31Z",
        "link": "http://arxiv.org/abs/0812.0607v2",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Rigid Components of Random Graphs",
        "authors": [
            "Louis Theran"
        ],
        "summary": "The planar rigidity problem asks, given a set of m pairwise distances among a set P of n unknown points, whether it is possible to reconstruct P, up to a finite set of possibilities (modulo rigid motions of the plane). The celebrated Maxwell-Laman Theorem from Rigidity Theory says that, generically, the rigidity problem has a combinatorial answer: the underlying combinatorial structure must contain a spanning minimally-rigid graph (Laman graph). In the case where the system is not rigid, its inclusion-wise maximal rigid substructures (rigid components) are also combinatorially characterized via the Maxwell-Laman theorem, and may be found efficiently.   Physicists have used planar combinatorial rigidity has been used to study the phase transition between liquid and solid in network glasses. The approach has been to generate a graph via a stochastic process and then experimentally analyze its rigidity properties. Of particular interest is the size of the largest rigid components.   In this paper, we study the emergence of rigid components in an Erdos-Renyi random graph G(n,p), using the parameterization p=c/n for a fixed constant c>0. Our first result is that for all c>0, almost surely all rigid components have size 2, 3 or Omega(n). We also show that for c>4, almost surely the largest rigid components have size at least n/10.   While the G(n,p) model is simpler than those appearing in the physics literature, these results are the first of this type where the distribution is over all graphs on n vertices and the expected number of edges is O(n).",
        "published": "2008-12-04T07:23:25Z",
        "link": "http://arxiv.org/abs/0812.0872v1",
        "categories": [
            "math.CO",
            "cs.CG",
            "05C80; 68R10"
        ]
    },
    {
        "title": "Linear-Time Algorithms for Geometric Graphs with Sublinearly Many Edge   Crossings",
        "authors": [
            "David Eppstein",
            "Michael T. Goodrich",
            "Darren Strash"
        ],
        "summary": "We provide linear-time algorithms for geometric graphs with sublinearly many crossings. That is, we provide algorithms running in O(n) time on connected geometric graphs having n vertices and k crossings, where k is smaller than n by an iterated logarithmic factor. Specific problems we study include Voronoi diagrams and single-source shortest paths. Our algorithms all run in linear time in the standard comparison-based computational model; hence, we make no assumptions about the distribution or bit complexities of edge weights, nor do we utilize unusual bit-level operations on memory words. Instead, our algorithms are based on a planarization method that \"zeroes in\" on edge crossings, together with methods for extending planar separator decompositions to geometric graphs with sublinearly many crossings. Incidentally, our planarization algorithm also solves an open computational geometry problem of Chazelle for triangulating a self-intersecting polygonal chain having n segments and k crossings in linear time, for the case when k is sublinear in n by an iterated logarithmic factor.",
        "published": "2008-12-04T10:29:00Z",
        "link": "http://arxiv.org/abs/0812.0893v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "cs.DS",
            "cs.GR",
            "F.2.2; G.2.2; G.3"
        ]
    },
    {
        "title": "Stability of Curvature Measures",
        "authors": [
            "Frédéric Chazal",
            "David Cohen-Steiner",
            "André Lieutier",
            "Boris Thibert"
        ],
        "summary": "We address the problem of curvature estimation from sampled compact sets. The main contribution is a stability result: we show that the gaussian, mean or anisotropic curvature measures of the offset of a compact set K with positive $\\mu$-reach can be estimated by the same curvature measures of the offset of a compact set K' close to K in the Hausdorff sense. We show how these curvature measures can be computed for finite unions of balls. The curvature measures of the offset of a compact set with positive $\\mu$-reach can thus be approximated by the curvature measures of the offset of a point-cloud sample. These results can also be interpreted as a framework for an effective and robust notion of curvature.",
        "published": "2008-12-07T19:58:52Z",
        "link": "http://arxiv.org/abs/0812.1390v1",
        "categories": [
            "cs.CG",
            "math.DG"
        ]
    },
    {
        "title": "The convex hull of a regular set of integer vectors is polyhedral and   effectively computable",
        "authors": [
            "Alain Finkel",
            "Jérôme Leroux"
        ],
        "summary": "Number Decision Diagrams (NDD) provide a natural finite symbolic representation for regular set of integer vectors encoded as strings of digit vectors (least or most significant digit first). The convex hull of the set of vectors represented by a NDD is proved to be an effectively computable convex polyhedron.",
        "published": "2008-12-10T16:26:36Z",
        "link": "http://arxiv.org/abs/0812.1951v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "Unfolding Convex Polyhedra via Quasigeodesic Star Unfoldings",
        "authors": [
            "Jin-ichi Itoh",
            "Joseph O'Rourke",
            "Costin Vîlcu"
        ],
        "summary": "We extend the notion of a star unfolding to be based on a simple quasigeodesic loop Q rather than on a point. This gives a new general method to unfold the surface of any convex polyhedron P to a simple, planar polygon: shortest paths from all vertices of P to Q are cut, and all but one segment of Q is cut.",
        "published": "2008-12-12T15:05:25Z",
        "link": "http://arxiv.org/abs/0812.2257v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Shape Fitting on Point Sets with Probability Distributions",
        "authors": [
            "Maarten Loffler",
            "Jeff M. Phillips"
        ],
        "summary": "A typical computational geometry problem begins: Consider a set P of n points in R^d. However, many applications today work with input that is not precisely known, for example when the data is sensed and has some known error model. What if we do not know the set P exactly, but rather we have a probability distribution mu_p governing the location of each point p in P?   Consider a set of (non-fixed) points P, and let mu_P be the probability distribution of this set. We study several measures (e.g. the radius of the smallest enclosing ball, or the area of the smallest enclosing box) with respect to mu_P. The solutions to these problems do not, as in the traditional case, consist of a single answer, but rather a distribution of answers. We describe several data structures that approximate distributions of answers for shape fitting problems.   We provide simple and efficient randomized algorithms for computing all of these data structures, which are easy to implement and practical. We provide some experimental results to assert this. We also provide more involved deterministic algorithms for some of these data structures that run in time polynomial in n and 1/eps, where eps is the approximation factor.",
        "published": "2008-12-16T04:09:38Z",
        "link": "http://arxiv.org/abs/0812.2967v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Comparison of Binary Classification Based on Signed Distance Functions   with Support Vector Machines",
        "authors": [
            "Erik M. Boczko",
            "Todd Young",
            "Minhui Zie",
            "Di Wu"
        ],
        "summary": "We investigate the performance of a simple signed distance function (SDF) based method by direct comparison with standard SVM packages, as well as K-nearest neighbor and RBFN methods. We present experimental results comparing the SDF approach with other classifiers on both synthetic geometric problems and five benchmark clinical microarray data sets. On both geometric problems and microarray data sets, the non-optimized SDF based classifiers perform just as well or slightly better than well-developed, standard SVM methods. These results demonstrate the potential accuracy of SDF-based methods on some types of problems.",
        "published": "2008-12-16T20:58:24Z",
        "link": "http://arxiv.org/abs/0812.3147v1",
        "categories": [
            "cs.LG",
            "cs.CG"
        ]
    },
    {
        "title": "Chain-Based Representations for Solid and Physical Modeling",
        "authors": [
            "Antonio DiCarlo",
            "Franco Milicchio",
            "Alberto Paoluzzi",
            "Vadim Shapiro"
        ],
        "summary": "In this paper we show that the (co)chain complex associated with a decomposition of the computational domain, commonly called a mesh in computational science and engineering, can be represented by a block-bidiagonal matrix that we call the Hasse matrix. Moreover, we show that topology-preserving mesh refinements, produced by the action of (the simplest) Euler operators, can be reduced to multilinear transformations of the Hasse matrix representing the complex. Our main result is a new representation of the (co)chain complex underlying field computations, a representation that provides new insights into the transformations induced by local mesh refinements. Our approach is based on first principles and is general in that it applies to most representational domains that can be characterized as cell complexes, without any restrictions on their type, dimension, codimension, orientability, manifoldness, connectedness.",
        "published": "2008-12-17T11:03:31Z",
        "link": "http://arxiv.org/abs/0812.3249v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Succinct Greedy Geometric Routing in the Euclidean Plane",
        "authors": [
            "Michael T. Goodrich",
            "Darren Strash"
        ],
        "summary": "In greedy geometric routing, messages are passed in a network embedded in a metric space according to the greedy strategy of always forwarding messages to nodes that are closer to the destination. We show that greedy geometric routing schemes exist for the Euclidean metric in R^2, for 3-connected planar graphs, with coordinates that can be represented succinctly, that is, with O(log n) bits, where n is the number of vertices in the graph. Moreover, our embedding strategy introduces a coordinate system for R^2 that supports distance comparisons using our succinct coordinates. Thus, our scheme can be used to significantly reduce bandwidth, space, and header size over other recently discovered greedy geometric routing implementations for R^2.",
        "published": "2008-12-19T20:42:33Z",
        "link": "http://arxiv.org/abs/0812.3893v3",
        "categories": [
            "cs.CG",
            "F.2.2; C.2.1"
        ]
    },
    {
        "title": "A Pseudopolynomial Algorithm for Alexandrov's Theorem",
        "authors": [
            "Daniel Kane",
            "Gregory N. Price",
            "Erik D. Demaine"
        ],
        "summary": "Alexandrov's Theorem states that every metric with the global topology and local geometry required of a convex polyhedron is in fact the intrinsic metric of a unique convex polyhedron. Recent work by Bobenko and Izmestiev describes a differential equation whose solution leads to the polyhedron corresponding to a given metric. We describe an algorithm based on this differential equation to compute the polyhedron to arbitrary precision given the metric, and prove a pseudopolynomial bound on its running time. Along the way, we develop pseudopolynomial algorithms for computing shortest paths and weighted Delaunay triangulations on a polyhedral surface, even when the surface edges are not shortest paths.",
        "published": "2008-12-30T08:41:29Z",
        "link": "http://arxiv.org/abs/0812.5030v3",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Lower bounds for weak epsilon-nets and stair-convexity",
        "authors": [
            "Boris Bukh",
            "Jiří Matoušek",
            "Gabriel Nivasch"
        ],
        "summary": "A set N is called a \"weak epsilon-net\" (with respect to convex sets) for a finite set X in R^d if N intersects every convex set that contains at least epsilon*|X| points of X. For every fixed d>=2 and every r>=1 we construct sets X in R^d for which every weak (1/r)-net has at least Omega(r log^{d-1} r) points; this is the first superlinear lower bound for weak epsilon-nets in a fixed dimension.   The construction is a \"stretched grid\", i.e., the Cartesian product of d suitable fast-growing finite sequences, and convexity in this grid can be analyzed using \"stair-convexity\", a new variant of the usual notion of convexity.   We also consider weak epsilon-nets for the diagonal of our stretched grid in R^d, d>=3, which is an \"intrinsically 1-dimensional\" point set. In this case we exhibit slightly superlinear lower bounds (involving the inverse Ackermann function), showing that upper bounds by Alon, Kaplan, Nivasch, Sharir, and Smorodinsky (2008) are not far from the truth in the worst case.   Using the stretched grid we also improve the known upper bound for the so-called \"second selection lemma\" in the plane by a logarithmic factor: We obtain a set T of t triangles with vertices in an n-point set in the plane such that no point is contained in more than O(t^2 / (n^3 log (n^3/t))) triangles of T.",
        "published": "2008-12-30T09:21:04Z",
        "link": "http://arxiv.org/abs/0812.5039v2",
        "categories": [
            "math.CO",
            "cs.CG",
            "52A30, 52C99, 68U05"
        ]
    },
    {
        "title": "Toward the Graphics Turing Scale on a Blue Gene Supercomputer",
        "authors": [
            "Michael McGuigan"
        ],
        "summary": "We investigate raytracing performance that can be achieved on a class of Blue Gene supercomputers. We measure a 822 times speedup over a Pentium IV on a 6144 processor Blue Gene/L. We measure the computational performance as a function of number of processors and problem size to determine the scaling performance of the raytracing calculation on the Blue Gene. We find nontrivial scaling behavior at large number of processors. We discuss applications of this technology to scientific visualization with advanced lighting and high resolution. We utilize three racks of a Blue Gene/L in our calculations which is less than three percent of the the capacity of the worlds largest Blue Gene computer.",
        "published": "2008-01-09T20:51:02Z",
        "link": "http://arxiv.org/abs/0801.1500v1",
        "categories": [
            "cs.GR"
        ]
    },
    {
        "title": "MathPSfrag 2: Convenient LaTeX Labels in Mathematica",
        "authors": [
            "Johannes Große"
        ],
        "summary": "This article introduces the next version of MathPSfrag. MathPSfrag is a Mathematica package that during export automatically replaces all expressions in a plot by corresponding LaTeX commands. The new version can also produce LaTeX independent images; e.g., PDF files for inclusion in pdfLaTeX. Moreover from these files a preview is generated and shown within Mathematica.",
        "published": "2008-01-15T18:34:44Z",
        "link": "http://arxiv.org/abs/0801.2175v1",
        "categories": [
            "cs.GR",
            "I.3.4"
        ]
    },
    {
        "title": "Multiple Uncertainties in Time-Variant Cosmological Particle Data",
        "authors": [
            "Steve Haroz",
            "Kwan-Liu Ma",
            "Katrin Heitmann"
        ],
        "summary": "Though the mediums for visualization are limited, the potential dimensions of a dataset are not. In many areas of scientific study, understanding the correlations between those dimensions and their uncertainties is pivotal to mining useful information from a dataset. Obtaining this insight can necessitate visualizing the many relationships among temporal, spatial, and other dimensionalities of data and its uncertainties. We utilize multiple views for interactive dataset exploration and selection of important features, and we apply those techniques to the unique challenges of cosmological particle datasets. We show how interactivity and incorporation of multiple visualization techniques help overcome the problem of limited visualization dimensions and allow many types of uncertainty to be seen in correlation with other variables.",
        "published": "2008-01-15T22:57:41Z",
        "link": "http://arxiv.org/abs/0801.2405v2",
        "categories": [
            "astro-ph",
            "cs.GR",
            "cs.HC"
        ]
    },
    {
        "title": "Complex Eigenvalues for Binary Subdivision Schemes",
        "authors": [
            "Christian Kuehn"
        ],
        "summary": "Convergence properties of binary stationary subdivision schemes for curves have been analyzed using the techniques of z-transforms and eigenanalysis. Eigenanalysis provides a way to determine derivative continuity at specific points based on the eigenvalues of a finite matrix. None of the well-known subdivision schemes for curves have complex eigenvalues. We prove when a convergent scheme with palindromic mask can have complex eigenvalues and that a lower limit for the size of the mask exists in this case. We find a scheme with complex eigenvalues achieving this lower bound. Furthermore we investigate this scheme numerically and explain from a geometric viewpoint why such a scheme has not yet been used in computer-aided geometric design.",
        "published": "2008-01-21T18:27:09Z",
        "link": "http://arxiv.org/abs/0801.3249v1",
        "categories": [
            "cs.GR",
            "cs.NA",
            "I.3.5"
        ]
    },
    {
        "title": "Discrete Complex Structure on Surfel Surfaces",
        "authors": [
            "Christian Mercat"
        ],
        "summary": "This paper defines a theory of conformal parametrization of digital surfaces made of surfels equipped with a normal vector. The main idea is to locally project each surfel to the tangent plane, therefore deforming its aspect-ratio. It is a generalization of the theory known for polyhedral surfaces. The main difference is that the conformal ratios that appear are no longer real in general. It yields a generalization of the standard Laplacian on weighted graphs.",
        "published": "2008-02-12T11:06:38Z",
        "link": "http://arxiv.org/abs/0802.1617v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "math.CV"
        ]
    },
    {
        "title": "PVM-Distributed Implementation of the Radiance Code",
        "authors": [
            "Francisco R. Villatoro",
            "Antonio J. Nebro",
            "Jose E. Fernández"
        ],
        "summary": "The Parallel Virtual Machine (PVM) tool has been used for a distributed implementation of Greg Ward's Radiance code. In order to generate exactly the same primary rays with both the sequential and the parallel codes, the quincunx sampling technique used in Radiance for the reduction of the number of primary rays by interpolation, must be left untouched in the parallel implementation. The octree of local ambient values used in Radiance for the indirect illumination has been shared among all the processors. Both static and dynamic image partitioning techniques which replicate the octree of the complete scene in all the processors and have load-balancing, have been developed for one frame rendering. Speedups larger than 7.5 have been achieved in a network of 8 workstations. For animation sequences, a new dynamic partitioning distribution technique with superlinear speedups has also been developed.",
        "published": "2008-02-22T17:32:17Z",
        "link": "http://arxiv.org/abs/0802.3355v1",
        "categories": [
            "cs.DC",
            "cs.GR"
        ]
    },
    {
        "title": "Realistic Haptic Rendering of Interacting Deformable Objects in Virtual   Environments",
        "authors": [
            "Christian Duriez",
            "Frédéric Dubois",
            "Abderrahmane Kheddar",
            "Claude Andriot"
        ],
        "summary": "A new computer haptics algorithm to be used in general interactive manipulations of deformable virtual objects is presented. In multimodal interactive simulations, haptic feedback computation often comes from contact forces. Subsequently, the fidelity of haptic rendering depends significantly on contact space modeling. Contact and friction laws between deformable models are often simplified in up to date methods. They do not allow a \"realistic\" rendering of the subtleties of contact space physical phenomena (such as slip and stick effects due to friction or mechanical coupling between contacts). In this paper, we use Signorini's contact law and Coulomb's friction law as a computer haptics basis. Real-time performance is made possible thanks to a linearization of the behavior in the contact space, formulated as the so-called Delassus operator, and iteratively solved by a Gauss-Seidel type algorithm. Dynamic deformation uses corotational global formulation to obtain the Delassus operator in which the mass and stiffness ratio are dissociated from the simulation time step. This last point is crucial to keep stable haptic feedback. This global approach has been packaged, implemented, and tested. Stable and realistic 6D haptic feedback is demonstrated through a clipping task experiment.",
        "published": "2008-04-03T13:49:51Z",
        "link": "http://arxiv.org/abs/0804.0561v1",
        "categories": [
            "cs.GR"
        ]
    },
    {
        "title": "Discrete schemes for Gaussian curvature and their convergence",
        "authors": [
            "Zhiqiang Xu",
            "Guoliang Xu"
        ],
        "summary": "In this paper, several discrete schemes for Gaussian curvature are surveyed. The convergence property of a modified discrete scheme for the Gaussian curvature is considered. Furthermore, a new discrete scheme for Gaussian curvature is resented. We prove that the new scheme converges at the regular vertex with valence not less than 5. By constructing a counterexample, we also show that it is impossible for building a discrete scheme for Gaussian curvature which converges over the regular vertex with valence 4. Finally, asymptotic errors of several discrete scheme for Gaussian curvature are compared.",
        "published": "2008-04-07T14:47:03Z",
        "link": "http://arxiv.org/abs/0804.1046v1",
        "categories": [
            "cs.CV",
            "cs.CG",
            "cs.GR",
            "cs.NA"
        ]
    },
    {
        "title": "Size matters: performance declines if your pixels are too big or too   small",
        "authors": [
            "Vassilis Kostakos",
            "Eamonn O'Neill"
        ],
        "summary": "We present a conceptual model that describes the effect of pixel size on target acquisition. We demonstrate the use of our conceptual model by applying it to predict and explain the results of an experiment to evaluate users' performance in a target acquisition task involving three distinct display sizes: standard desktop, small and large displays. The results indicate that users are fastest on standard desktop displays, undershoots are the most common error on small displays and overshoots are the most common error on large displays. We propose heuristics to maintain usability when changing displays. Finally, we contribute to the growing body of evidence that amplitude does affect performance in a display-based pointing task.",
        "published": "2008-04-18T21:02:19Z",
        "link": "http://arxiv.org/abs/0804.3103v1",
        "categories": [
            "cs.GR",
            "cs.HC"
        ]
    },
    {
        "title": "Morphing of Triangular Meshes in Shape Space",
        "authors": [
            "Stefanie Wuhrer",
            "Prosenjit Bose",
            "Chang Shu",
            "Joseph O'Rourke",
            "Alan Brunton"
        ],
        "summary": "We present a novel approach to morph between two isometric poses of the same non-rigid object given as triangular meshes. We model the morphs as linear interpolations in a suitable shape space $\\mathcal{S}$. For triangulated 3D polygons, we prove that interpolating linearly in this shape space corresponds to the most isometric morph in $\\mathbb{R}^3$. We then extend this shape space to arbitrary triangulations in 3D using a heuristic approach and show the practical use of the approach using experiments. Furthermore, we discuss a modified shape space that is useful for isometric skeleton morphing. All of the newly presented approaches solve the morphing problem without the need to solve a minimization problem.",
        "published": "2008-05-01T23:08:30Z",
        "link": "http://arxiv.org/abs/0805.0162v2",
        "categories": [
            "cs.CG",
            "cs.GR"
        ]
    },
    {
        "title": "Neural networks in 3D medical scan visualization",
        "authors": [
            "Dženan Zukić",
            "Andreas Elsner",
            "Zikrija Avdagić",
            "Gitta Domik"
        ],
        "summary": "For medical volume visualization, one of the most important tasks is to reveal clinically relevant details from the 3D scan (CT, MRI ...), e.g. the coronary arteries, without obscuring them with less significant parts. These volume datasets contain different materials which are difficult to extract and visualize with 1D transfer functions based solely on the attenuation coefficient. Multi-dimensional transfer functions allow a much more precise classification of data which makes it easier to separate different surfaces from each other. Unfortunately, setting up multi-dimensional transfer functions can become a fairly complex task, generally accomplished by trial and error. This paper explains neural networks, and then presents an efficient way to speed up visualization process by semi-automatic transfer function generation. We describe how to use neural networks to detect distinctive features shown in the 2D histogram of the volume data and how to use this information for data classification.",
        "published": "2008-06-18T08:36:15Z",
        "link": "http://arxiv.org/abs/0806.2925v2",
        "categories": [
            "cs.AI",
            "cs.GR",
            "I.3; I.2.6"
        ]
    },
    {
        "title": "Quasi-Mandelbrot sets for perturbed complex analytic maps: visual   patterns",
        "authors": [
            "A. V. Toporensky"
        ],
        "summary": "We consider perturbations of the complex quadratic map $ z \\to z^2 +c$ and corresponding changes in their quasi-Mandelbrot sets. Depending on particular perturbation, visual forms of quasi-Mandelbrot set changes either sharply (when the perturbation reaches some critical value) or continuously. In the latter case we have a smooth transition from the classical form of the set to some forms, constructed from mostly linear structures, as it is typical for two-dimensional real number dynamics. Two examples of continuous evolution of the quasi-Mandelbrot set are described.",
        "published": "2008-07-10T14:40:35Z",
        "link": "http://arxiv.org/abs/0807.1667v1",
        "categories": [
            "cs.GR"
        ]
    },
    {
        "title": "On the role of metaphor in information visualization",
        "authors": [
            "John S. Risch"
        ],
        "summary": "The concept of metaphor, in particular graphical (or visual) metaphor, is central to the field of information visualization. Information graphics and interactive information visualization systems employ a variety of metaphorical devices to make abstract, complex, voluminous, or otherwise difficult-to-comprehend information understandable in graphical terms. This paper explores the use of metaphor in information visualization, advancing the theory previously argued by Johnson, Lakoff, Tversky et al. that many information graphics are metaphorically understood in terms of cognitively entrenched spatial patterns known as image schemas. These patterns serve to structure and constrain abstract reasoning processes via metaphorical projection operations that are grounded in everyday perceptual experiences with phenomena such as containment, movement, and force dynamics. Building on previous research, I argue that information graphics promote comprehension of their target information through the use of graphical patterns that invoke these preexisting schematic structures. I further theorize that the degree of structural alignment of a particular graphic with one or more corresponding image schemas accounts for its perceived degree of intuitiveness. Accordingly, image schema theory can provide a powerful explanatory and predictive framework for visualization research. I review relevant theories of analogy and metaphor, and discuss the image schematic properties of several common types of information graphic. I conclude with the proposal that the inventory of image schemas culled from linguistic studies can serve as the basis for an inventory of design elements suitable for developing intuitive and effective new information visualization techniques.",
        "published": "2008-09-04T19:55:40Z",
        "link": "http://arxiv.org/abs/0809.0884v1",
        "categories": [
            "cs.HC",
            "cs.GR"
        ]
    },
    {
        "title": "Perspective Drawing of Surfaces with Line Hidden Line Elimination,   Dibujando Superficies En Perspectiva Con Eliminacion De Lineas Ocultas",
        "authors": [
            "Ignacio Vega-Paez",
            "Jose Angel Ortega",
            "Georgina G. Pulido"
        ],
        "summary": "An efficient computer algorithm is described for the perspective drawing of a wide class of surfaces. The class includes surfaces corresponding lo single-valued, continuous functions which are defined over rectangular domains. The algorithm automatically computes and eliminates hidden lines. The number of computations in the algorithm grows linearly with the number of sample points on the surface to be drawn. An analysis of the algorithm is presented, and extensions lo certain multi-valued functions are indicated. The algorithm is implemented and tested on .Net 2.0 platform that left interactive use. Running times are found lo be exceedingly efficient for visualization, where interaction on-line and view-point control, enables effective and rapid examination of a surfaces from many perspectives.",
        "published": "2008-09-24T05:50:56Z",
        "link": "http://arxiv.org/abs/0809.4093v2",
        "categories": [
            "cs.GR",
            "cs.CG"
        ]
    },
    {
        "title": "Visualization Optimization : Application to the RoboCup Rescue Domain",
        "authors": [
            "Pedro Miguel Moreira",
            "Luís Paulo Reis",
            "António Augusto de Sousa"
        ],
        "summary": "In this paper we demonstrate the use of intelligent optimization methodologies on the visualization optimization of virtual / simulated environments. The problem of automatic selection of an optimized set of views, which better describes an on-going simulation over a virtual environment is addressed in the context of the RoboCup Rescue Simulation domain. A generic architecture for optimization is proposed and described. We outline the possible extensions of this architecture and argue on how several problems within the fields of Interactive Rendering and Visualization can benefit from it.",
        "published": "2008-10-13T12:53:57Z",
        "link": "http://arxiv.org/abs/0810.2021v1",
        "categories": [
            "cs.GR",
            "cs.AI",
            "I.3.7; I.2.8"
        ]
    },
    {
        "title": "Detecting the Most Unusual Part of a Digital Image",
        "authors": [
            "K. Koroutchev",
            "E. Korutcheva"
        ],
        "summary": "The purpose of this paper is to introduce an algorithm that can detect the most unusual part of a digital image. The most unusual part of a given shape is defined as a part of the image that has the maximal distance to all non intersecting shapes with the same form.   The method can be used to scan image databases with no clear model of the interesting part or large image databases, as for example medical databases.",
        "published": "2008-10-19T18:04:51Z",
        "link": "http://arxiv.org/abs/0810.3418v1",
        "categories": [
            "cs.CV",
            "cs.GR"
        ]
    },
    {
        "title": "Interchanging Interactive 3-d Graphics for Astronomy",
        "authors": [
            "C. J. Fluke",
            "D. G. Barnes",
            "N. T. Jones"
        ],
        "summary": "We demonstrate how interactive, three-dimensional (3-d) scientific visualizations can be efficiently interchanged between a variety of mediums. Through the use of an appropriate interchange format, and a unified interaction interface, we minimize the effort to produce visualizations appropriate for undertaking knowledge discovery at the astronomer's desktop, as part of conference presentations, in digital publications or as Web content. We use examples from cosmological visualization to address some of the issues of interchange, and to describe our approach to adapting S2PLOT desktop visualizations to the Web.   Supporting demonstrations are available at http://astronomy.swin.edu.au/s2plot/interchange/",
        "published": "2008-10-23T02:58:55Z",
        "link": "http://arxiv.org/abs/0810.4201v2",
        "categories": [
            "astro-ph",
            "cs.GR"
        ]
    },
    {
        "title": "GPU-Based Interactive Visualization of Billion Point Cosmological   Simulations",
        "authors": [
            "Tamas Szalay",
            "Volker Springel",
            "Gerard Lemson"
        ],
        "summary": "Despite the recent advances in graphics hardware capabilities, a brute force approach is incapable of interactively displaying terabytes of data. We have implemented a system that uses hierarchical level-of-detailing for the results of cosmological simulations, in order to display visually accurate results without loading in the full dataset (containing over 10 billion points). The guiding principle of the program is that the user should not be able to distinguish what they are seeing from a full rendering of the original data. Furthermore, by using a tree-based system for levels of detail, the size of the underlying data is limited only by the capacity of the IO system containing it.",
        "published": "2008-11-13T09:34:42Z",
        "link": "http://arxiv.org/abs/0811.2055v2",
        "categories": [
            "cs.GR",
            "astro-ph"
        ]
    },
    {
        "title": "String Art: Circle Drawing Using Straight Lines",
        "authors": [
            "Sankar K",
            "Sarad AV"
        ],
        "summary": "An algorithm to generate the locus of a circle using the intersection points of straight lines is proposed. The pixels on the circle are plotted independent of one another and the operations involved in finding the locus of the circle from the intersection of straight lines are parallelizable. Integer only arithmetic and algorithmic optimizations are used for speedup. The proposed algorithm makes use of an envelope to form a parabolic arc which is consequent transformed into a circle. The use of parabolic arcs for the transformation results in higher pixel errors as the radius of the circle to be drawn increases. At its current state, the algorithm presented may be suitable only for generating circles for string art.",
        "published": "2008-11-25T17:12:22Z",
        "link": "http://arxiv.org/abs/0811.4121v1",
        "categories": [
            "cs.GR",
            "cs.OH",
            "I.3.3"
        ]
    },
    {
        "title": "The Good, the Bad, and the Ugly: three different approaches to break   their watermarking system",
        "authors": [
            "Gaëtan Le Guelvouit",
            "Teddy Furon",
            "François Cayre"
        ],
        "summary": "The Good is Blondie, a wandering gunman with a strong personal sense of honor. The Bad is Angel Eyes, a sadistic hitman who always hits his mark. The Ugly is Tuco, a Mexican bandit who's always only looking out for himself. Against the backdrop of the BOWS contest, they search for a watermark in gold buried in three images. Each knows only a portion of the gold's exact location, so for the moment they're dependent on each other. However, none are particularly inclined to share...",
        "published": "2008-11-28T16:31:12Z",
        "link": "http://arxiv.org/abs/0811.4681v1",
        "categories": [
            "cs.GR",
            "cs.MM"
        ]
    },
    {
        "title": "Strong Spatial Mixing and Approximating Partition Functions of Two-State   Spin Systems without Hard Constrains",
        "authors": [
            "Jinshan Zhang"
        ],
        "summary": "We prove Gibbs distribution of two-state spin systems(also known as binary Markov random fields) without hard constrains on a tree exhibits strong spatial mixing(also known as strong correlation decay), under the assumption that, for arbitrary `external field', the absolute value of `inverse temperature' is small, or the `external field' is uniformly large or small. The first condition on `inverse temperature' is tight if the distribution is restricted to ferromagnetic or antiferromagnetic Ising models.   Thanks to Weitz's self-avoiding tree, we extends the result for sparse on average graphs, which generalizes part of the recent work of Mossel and Sly\\cite{MS08}, who proved the strong spatial mixing property for ferromagnetic Ising model. Our proof yields a different approach, carefully exploiting the monotonicity of local recursion. To our best knowledge, the second condition of `external field' for strong spatial mixing in this paper is first considered and stated in term of `maximum average degree' and `interaction energy'. As an application, we present an FPTAS for partition functions of two-state spin models without hard constrains under the above assumptions in a general family of graphs including interesting bounded degree graphs.",
        "published": "2008-12-03T16:56:53Z",
        "link": "http://arxiv.org/abs/0812.0754v2",
        "categories": [
            "cs.DM",
            "cs.GR",
            "F.2.0"
        ]
    },
    {
        "title": "Linear-Time Algorithms for Geometric Graphs with Sublinearly Many Edge   Crossings",
        "authors": [
            "David Eppstein",
            "Michael T. Goodrich",
            "Darren Strash"
        ],
        "summary": "We provide linear-time algorithms for geometric graphs with sublinearly many crossings. That is, we provide algorithms running in O(n) time on connected geometric graphs having n vertices and k crossings, where k is smaller than n by an iterated logarithmic factor. Specific problems we study include Voronoi diagrams and single-source shortest paths. Our algorithms all run in linear time in the standard comparison-based computational model; hence, we make no assumptions about the distribution or bit complexities of edge weights, nor do we utilize unusual bit-level operations on memory words. Instead, our algorithms are based on a planarization method that \"zeroes in\" on edge crossings, together with methods for extending planar separator decompositions to geometric graphs with sublinearly many crossings. Incidentally, our planarization algorithm also solves an open computational geometry problem of Chazelle for triangulating a self-intersecting polygonal chain having n segments and k crossings in linear time, for the case when k is sublinear in n by an iterated logarithmic factor.",
        "published": "2008-12-04T10:29:00Z",
        "link": "http://arxiv.org/abs/0812.0893v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "cs.DS",
            "cs.GR",
            "F.2.2; G.2.2; G.3"
        ]
    },
    {
        "title": "An analysis of a random algorithm for estimating all the matchings",
        "authors": [
            "Jinshan Zhang"
        ],
        "summary": "Counting the number of all the matchings on a bipartite graph has been transformed into calculating the permanent of a matrix obtained from the extended bipartite graph by Yan Huo, and Rasmussen presents a simple approach (RM) to approximate the permanent, which just yields a critical ratio O($n\\omega(n)$) for almost all the 0-1 matrices, provided it's a simple promising practical way to compute this #P-complete problem. In this paper, the performance of this method will be shown when it's applied to compute all the matchings based on that transformation. The critical ratio will be proved to be very large with a certain probability, owning an increasing factor larger than any polynomial of $n$ even in the sense for almost all the 0-1 matrices. Hence, RM fails to work well when counting all the matchings via computing the permanent of the matrix. In other words, we must carefully utilize the known methods of estimating the permanent to count all the matchings through that transformation.",
        "published": "2008-12-05T12:16:53Z",
        "link": "http://arxiv.org/abs/0812.1119v1",
        "categories": [
            "cs.GR",
            "cs.AI",
            "F.2.0"
        ]
    },
    {
        "title": "Polyomino-Based Digital Halftoning",
        "authors": [
            "David Vanderhaeghe",
            "Victor Ostromoukhov"
        ],
        "summary": "In this work, we present a new method for generating a threshold structure. This kind of structure can be advantageously used in various halftoning algorithms such as clustered-dot or dispersed-dot dithering, error diffusion with threshold modulation, etc. The proposed method is based on rectifiable polyominoes -- a non-periodic hierarchical structure, which tiles the Euclidean plane with no gaps. Each polyomino contains a fixed number of discrete threshold values. Thanks to its inherent non-periodic nature combined with off-line optimization of threshold values, our polyomino-based threshold structure shows blue-noise spectral properties. The halftone images produced with this threshold structure have high visual quality. Although the proposed method is general, and can be applied on any polyomino tiling, we consider one particular case: tiling with G-hexominoes. We compare our polyomino-based threshold structure with the best known state-of-the-art methods for generation threshold matrices, and conclude considerable improvement achieved with our method.",
        "published": "2008-12-09T10:12:36Z",
        "link": "http://arxiv.org/abs/0812.1647v1",
        "categories": [
            "cs.GR"
        ]
    },
    {
        "title": "Implementation of perception and action at nanoscale",
        "authors": [
            "Sylvain Marlière",
            "Jean Loup Florens",
            "Florence Marchi",
            "Annie Luciani",
            "Joel Chevrier"
        ],
        "summary": "Real time combination of nanosensors and nanoactuators with virtual reality environment and multisensorial interfaces enable us to efficiently act and perceive at nanoscale. Advanced manipulation of nanoobjects and new strategies for scientific education are the key motivations. We have no existing intuitive representation of the nanoworld ruled by laws foreign to our experience. A central challenge is then the construction of nanoworld simulacrum that we can start to visit and to explore. In this nanoworld simulacrum, object identifications will be based on probed entity physical and chemical intrinsic properties, on their interactions with sensors and on the final choices made in building a multisensorial interface so that these objects become coherent elements of the human sphere of action and perception. Here we describe a 1D virtual nanomanipulator, part of the Cit\\'e des Sciences EXPO NANO in Paris, that is the first realization based on this program.",
        "published": "2008-01-04T13:38:39Z",
        "link": "http://arxiv.org/abs/0801.0678v1",
        "categories": [
            "cs.RO",
            "cs.HC"
        ]
    },
    {
        "title": "Evolution of central pattern generators for the control of a five-link   bipedal walking mechanism",
        "authors": [
            "Atilim Gunes Baydin"
        ],
        "summary": "Central pattern generators (CPGs), with a basis is neurophysiological studies, are a type of neural network for the generation of rhythmic motion. While CPGs are being increasingly used in robot control, most applications are hand-tuned for a specific task and it is acknowledged in the field that generic methods and design principles for creating individual networks for a given task are lacking. This study presents an approach where the connectivity and oscillatory parameters of a CPG network are determined by an evolutionary algorithm with fitness evaluations in a realistic simulation with accurate physics. We apply this technique to a five-link planar walking mechanism to demonstrate its feasibility and performance. In addition, to see whether results from simulation can be acceptably transferred to real robot hardware, the best evolved CPG network is also tested on a real mechanism. Our results also confirm that the biologically inspired CPG model is well suited for legged locomotion, since a diverse manifestation of networks have been observed to succeed in fitness simulations during evolution.",
        "published": "2008-01-06T00:20:25Z",
        "link": "http://arxiv.org/abs/0801.0830v9",
        "categories": [
            "cs.NE",
            "cs.RO",
            "92B20, 92B25, 70E60, 68T05, 68U20",
            "I.2.2; I.2.9"
        ]
    },
    {
        "title": "TER: A Robot for Remote Ultrasonic Examination: Experimental Evaluations",
        "authors": [
            "Jean-Jacques Banihachemi",
            "Eric Boidard",
            "Jean-Luc Bosson",
            "Luc Bressollette",
            "Ivan Bricault",
            "Philippe Cinquin",
            "Gilbert Ferretti",
            "Maud Marchal",
            "Thomas Martinelli",
            "Alexandre Moreau-Gaudry",
            "Franck Pelissier",
            "Christian Roux",
            "Dominique Saragaglia",
            "Pierre Thorel",
            "Jocelyne Troccaz",
            "Adriana Vilchis"
        ],
        "summary": "This chapter:   o Motivates the clinical use of robotic tele-echography   o Introduces the TER system   o Describes technical and clinical evaluations performed with TER",
        "published": "2008-01-28T18:39:41Z",
        "link": "http://arxiv.org/abs/0801.4355v1",
        "categories": [
            "cs.OH",
            "cs.RO"
        ]
    },
    {
        "title": "Stiffness Analysis of 3-d.o.f. Overconstrained Translational Parallel   Manipulators",
        "authors": [
            "Anatoly Pashkevich",
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The paper presents a new stiffness modelling method for overconstrained parallel manipulators, which is applied to 3-d.o.f. translational mechanisms. It is based on a multidimensional lumped-parameter model that replaces the link flexibility by localized 6-d.o.f. virtual springs. In contrast to other works, the method includes a FEA-based link stiffness evaluation and employs a new solution strategy of the kinetostatic equations, which allows computing the stiffness matrix for the overconstrained architectures and for the singular manipulator postures. The advantages of the developed technique are confirmed by application examples, which deal with comparative stiffness analysis of two translational parallel manipulators.",
        "published": "2008-02-20T09:30:13Z",
        "link": "http://arxiv.org/abs/0802.2773v1",
        "categories": [
            "cs.RO",
            "physics.class-ph"
        ]
    },
    {
        "title": "A Universal In-Place Reconfiguration Algorithm for Sliding Cube-Shaped   Robots in a Quadratic Number of Moves",
        "authors": [
            "Zachary Abel",
            "Hugo A. Akitaya",
            "Scott Duke Kominers",
            "Matias Korman",
            "Frederick Stock"
        ],
        "summary": "In the modular robot reconfiguration problem, we are given $n$ cube-shaped modules (or robots) as well as two configurations, i.e., placements of the $n$ modules so that their union is face-connected. The goal is to find a sequence of moves that reconfigures the modules from one configuration to the other using \"sliding moves,\" in which a module slides over the face or edge of a neighboring module, maintaining connectivity of the configuration at all times.   For many years it has been known that certain module configurations in this model require at least $\\Omega(n^2)$ moves to reconfigure between them. In this paper, we introduce the first universal reconfiguration algorithm -- i.e., we show that any $n$-module configuration can reconfigure itself into any specified $n$-module configuration using just sliding moves. Our algorithm achieves reconfiguration in $O(n^2)$ moves, making it asymptotically tight. We also present a variation that reconfigures in-place, it ensures that throughout the reconfiguration process, all modules, except for one, will be contained in the union of the bounding boxes of the start and end configuration.",
        "published": "2008-02-23T00:54:13Z",
        "link": "http://arxiv.org/abs/0802.3414v4",
        "categories": [
            "cs.CG",
            "cs.MA",
            "cs.RO"
        ]
    },
    {
        "title": "Quiescence of Self-stabilizing Gossiping among Mobile Agents in Graphs",
        "authors": [
            "Toshimitsu Masuzawa",
            "Sébastien Tixeuil"
        ],
        "summary": "This paper considers gossiping among mobile agents in graphs: agents move on the graph and have to disseminate their initial information to every other agent. We focus on self-stabilizing solutions for the gossip problem, where agents may start from arbitrary locations in arbitrary states. Self-stabilization requires (some of the) participating agents to keep moving forever, hinting at maximizing the number of agents that could be allowed to stop moving eventually. This paper formalizes the self-stabilizing agent gossip problem, introduces the quiescence number (i.e., the maximum number of eventually stopping agents) of self-stabilizing solutions and investigates the quiescence number with respect to several assumptions related to agent anonymity, synchrony, link duplex capacity, and whiteboard capacity.",
        "published": "2008-03-03T09:14:21Z",
        "link": "http://arxiv.org/abs/0803.0189v2",
        "categories": [
            "cs.DC",
            "cs.PF",
            "cs.RO"
        ]
    },
    {
        "title": "Non-Singular Assembly-mode Changing Motions for 3-RPR Parallel   Manipulators",
        "authors": [
            "Mazen Zein",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "When moving from one arbitrary location at another, a parallel manipulator may change its assembly-mode without crossing a singularity. Because the non-singular change of assembly-mode cannot be simply detected, the actual assembly-mode during motion is difficult to track. This paper proposes a global explanatory approach to help better understand non-singular assembly-mode changing motions for 3-RPR planar parallel manipulators. The approach consists in fixing one of the actuated joints and analyzing the configuration-space as a surface in a 3-dimensional space. Such a global description makes it possible to display all possible non-singular assembly-mode changing trajectories.",
        "published": "2008-03-08T06:54:47Z",
        "link": "http://arxiv.org/abs/0803.1221v1",
        "categories": [
            "cs.RO",
            "physics.class-ph"
        ]
    },
    {
        "title": "Genetic-Algorithm Seeding Of Idiotypic Networks For Mobile-Robot   Navigation",
        "authors": [
            "Amanda Whitbrook",
            "Uwe Aickelin",
            "Jonathan Garibaldi"
        ],
        "summary": "Robot-control designers have begun to exploit the properties of the human immune system in order to produce dynamic systems that can adapt to complex, varying, real-world tasks. Jernes idiotypic-network theory has proved the most popular artificial-immune-system (AIS) method for incorporation into behaviour-based robotics, since idiotypic selection produces highly adaptive responses. However, previous efforts have mostly focused on evolving the network connections and have often worked with a single, pre-engineered set of behaviours, limiting variability. This paper describes a method for encoding behaviours as a variable set of attributes, and shows that when the encoding is used with a genetic algorithm (GA), multiple sets of diverse behaviours can develop naturally and rapidly, providing much greater scope for flexible behaviour-selection. The algorithm is tested extensively with a simulated e-puck robot that navigates around a maze by tracking colour. Results show that highly successful behaviour sets can be generated within about 25 minutes, and that much greater diversity can be obtained when multiple autonomous populations are used, rather than a single one.",
        "published": "2008-03-11T16:26:47Z",
        "link": "http://arxiv.org/abs/0803.1626v1",
        "categories": [
            "cs.NE",
            "cs.RO"
        ]
    },
    {
        "title": "Idiotypic Immune Networks in Mobile Robot Control",
        "authors": [
            "Amanda Whitbrook",
            "Uwe Aickelin",
            "Jonathan Garibaldi"
        ],
        "summary": "Jerne's idiotypic network theory postulates that the immune response involves inter-antibody stimulation and suppression as well as matching to antigens. The theory has proved the most popular Artificial Immune System (ais) model for incorporation into behavior-based robotics but guidelines for implementing idiotypic selection are scarce. Furthermore, the direct effects of employing the technique have not been demonstrated in the form of a comparison with non-idiotypic systems. This paper aims to address these issues. A method for integrating an idiotypic ais network with a Reinforcement Learning based control system (rl) is described and the mechanisms underlying antibody stimulation and suppression are explained in detail. Some hypotheses that account for the network advantage are put forward and tested using three systems with increasing idiotypic complexity. The basic rl, a simplified hybrid ais-rl that implements idiotypic selection independently of derived concentration levels and a full hybrid ais-rl scheme are examined. The test bed takes the form of a simulated Pioneer robot that is required to navigate through maze worlds detecting and tracking door markers.",
        "published": "2008-03-20T12:24:43Z",
        "link": "http://arxiv.org/abs/0803.2981v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.RO"
        ]
    },
    {
        "title": "Towards Physarum robots: computing and manipulating on water surface",
        "authors": [
            "Andrew Adamatzky"
        ],
        "summary": "Plasmodium of Physarym polycephalum is an ideal biological substrate for implementing concurrent and parallel computation, including combinatorial geometry and optimization on graphs. We report results of scoping experiments on Physarum computing in conditions of minimal friction, on the water surface. We show that plasmodium of Physarum is capable for computing a basic spanning trees and manipulating of light-weight objects. We speculate that our results pave the pathways towards design and implementation of amorphous biological robots.",
        "published": "2008-04-13T00:42:28Z",
        "link": "http://arxiv.org/abs/0804.2036v1",
        "categories": [
            "cs.RO",
            "cs.AI"
        ]
    },
    {
        "title": "Image Processing in Optical Guidance for Autonomous Landing of Lunar   Probe",
        "authors": [
            "Ding Meng",
            "Cao Yun-feng",
            "Wu Qing-xian",
            "Zhang Zhen"
        ],
        "summary": "Because of the communication delay between earth and moon, the GNC technology of lunar probe is becoming more important than ever. Current navigation technology is not able to provide precise motion estimation for probe landing control system Computer vision offers a new approach to solve this problem. In this paper, author introduces an image process algorithm of computer vision navigation for autonomous landing of lunar probe. The purpose of the algorithm is to detect and track feature points which are factors of navigation. Firstly, fixation areas are detected as sub-images and matched. Secondly, feature points are extracted from sub-images and tracked. Computer simulation demonstrates the result of algorithm takes less computation and fulfils requests of navigation algorithm.",
        "published": "2008-04-24T08:22:00Z",
        "link": "http://arxiv.org/abs/0804.3862v1",
        "categories": [
            "cs.RO",
            "I.4.9"
        ]
    },
    {
        "title": "Hardware In The Loop Simulator in UAV Rapid Development Life Cycle",
        "authors": [
            "Widyawardana Adiprawita",
            "Adang Suwandi Ahmad",
            "Jaka Semibiring"
        ],
        "summary": "Field trial is very critical and high risk in autonomous UAV development life cycle. Hardware in the loop (HIL) simulation is a computer simulation that has the ability to simulate UAV flight characteristic, sensor modeling and actuator modeling while communicating in real time with the UAV autopilot hardware. HIL simulation can be used to test the UAV autopilot hardware reliability, test the closed loop performance of the overall system and tuning the control parameter. By rigorous testing in the HIL simulator, the risk in the field trial can be minimized.",
        "published": "2008-04-24T09:35:55Z",
        "link": "http://arxiv.org/abs/0804.3874v1",
        "categories": [
            "cs.RO",
            "B.7.2"
        ]
    },
    {
        "title": "Effects of Leaders Position and Shape on Aerodynamic Performances of V   Flight Formation",
        "authors": [
            "H. P. Thien",
            "M. A. Moelyadi",
            "H. Muhammad"
        ],
        "summary": "The influences of the leader in a group of V flight formation are dealt with. The investigation is focused on the effect of its position and shape on aerodynamics performances of a given V flight formation. Vortices generated the wing tip of the leader moves downstream forming a pair of opposite rotating line vortices. These vortices are generally undesirable because they create a downwash that increases the induced drag on leaders wing. However, this downwash is also accompanied by an upwash that can beneficial to the followers wing flying behind the leaders one, namely a favorable lift for the followers wing. How much contributions of the leaders wing to the followers wing in the V formation flight is determined by the strength of tip vortices generated by the leaders wing which is influenced by its position and shape including incidence angle, dihedral angle, aspect ratio and taper ratio. The prediction of aerodynamic performances of the V flight formation including lift, drag and moment coefficients is numerically performed by solving Navier Stokes equations with k e turbulence model. The computational domain is defined with multiblock topology to capture the complex geometry arrangement of the V flight formation.",
        "published": "2008-04-24T10:01:29Z",
        "link": "http://arxiv.org/abs/0804.3879v1",
        "categories": [
            "cs.RO",
            "J.2"
        ]
    },
    {
        "title": "Automated Flight Test and System Identification for Rotary Wing Small   Aerial Platform using Frequency Responses Analysis",
        "authors": [
            "Widyawardana Adiprawita",
            "Adang Suwandi Ahmad",
            "Jaka Semibiring"
        ],
        "summary": "This paper proposes an autopilot system that can be used to control the small scale rotorcraft during the flight test for linear-frequency-domain system identification. The input frequency swept is generated automatically as part of the autopilot control command. Therefore the bandwidth coverage and consistency of the frequency swept is guaranteed to produce high quality data for system identification. Beside that we can set the safety parameter during the flight test (maximum roll or pitch value, minimum altitude, etc) so the safety of the whole flight test is guaranteed. This autopilot for automated flight test will be tested using hardware in the loop simulator for hover flight condition.",
        "published": "2008-04-24T10:07:34Z",
        "link": "http://arxiv.org/abs/0804.3881v1",
        "categories": [
            "cs.RO",
            "B.4.4"
        ]
    },
    {
        "title": "Virtual Reality Simulation of Fire Fighting Robot Dynamic and Motion",
        "authors": [
            "Joga D. Setiawan",
            "Mochamad Subchan",
            "Agus Budiyono"
        ],
        "summary": "This paper presents one approach in designing a Fire Fighting Robot which has been contested annually in a robotic student competition in many countries following the rules initiated at the Trinity College. The approach makes use of computer simulation and animation in a virtual reality environment. In the simulation, the amount of time, starting from home until the flame is destroyed, can be confirmed. The efficacy of algorithms and parameter values employed can be easily evaluated. Rather than spending time building the real robot in a trial and error fashion, now students can explore more variation of algorithm, parameter and sensor-actuator configuration in the early stage of design. Besides providing additional excitement during learning process and enhancing students understanding to the engineering aspects of the design, this approach could become a useful tool to increase the chance of winning the contest.",
        "published": "2008-04-24T10:13:53Z",
        "link": "http://arxiv.org/abs/0804.3882v1",
        "categories": [
            "cs.RO",
            "H.5.1"
        ]
    },
    {
        "title": "Heading Lock Maneuver Testing of Autonomous Underwater Vehicle",
        "authors": [
            "K. Muljowidodo",
            "N. Sapto Adi"
        ],
        "summary": "In recent years, Autonomous Underwater Vehicle (UAV) research and development at Bandung Institute of Technology in Indonesia has achieved the testing stage in the field. This testing was still being classified as the early testing, since some of the preliminary tests were carried out in the scale of the laboratory. The paper would discuss the laboratory test and several tests that were done in the field. Discussions were stressed in the procedure and the aim that will be achieved, along with several early results. The testing was carried out in the lake with the area around 8300 Ha and the maximum depth of 50 meters. The location of the testing was chosen with consideration of minimizing the effect of the current and the wave, as well as the location that was not too far from the Laboratory. The type of testing that will be discussed in paper was Heading Lock Maneuver Testing. The vehicle was tested to move with a certain cruising speed, afterwards it was commanded by an arbitrarily selected heading direction. The response and the behavior of the vehicle were recorded as the data produced by the testing.",
        "published": "2008-04-24T10:25:41Z",
        "link": "http://arxiv.org/abs/0804.3885v1",
        "categories": [
            "cs.RO",
            "B.6.2"
        ]
    },
    {
        "title": "Development of Architectures for Internet Telerobotics Systems",
        "authors": [
            "Riyanto Bambang"
        ],
        "summary": "This paper presents our experience in developing and implementing Internet telerobotics system. Internet telerobotics system refers to a robot system controlled and monitored remotely through the Internet. A robot manipulator with five degrees of freedom, called Mentor, is employed. Client-server architecture is chosen as a platform for our Internet telerobotics system. Three generations of telerobotics systems have evolved in this research. The first generation was based on CGI and two tiered architecture, where a client presents a Graphical User Interface to the user, and utilizes the user's data entry and actions to perform requests to robot server running on a different machine. The second generation was developed using Java. We also employ Java 3D for creating and manipulating 3D geometry of manipulator links and for constructing the structures used in rendering that geometry, resulting in 3D robot movement simulation presented to the users(clients) through their web browser. Recent development in our Internet telerobotics includes object recognition through image captured by a camera, which poses challenging problem, given the undeterministic latency of the Internet. The third generation is centered around the use of CORBA for development platform of distributed internet telerobotics system, aimed at distributing task of telerobotics system.",
        "published": "2008-04-24T10:43:46Z",
        "link": "http://arxiv.org/abs/0804.3891v1",
        "categories": [
            "cs.RO",
            "I.5.5"
        ]
    },
    {
        "title": "Unmanned Aerial Vehicle Instrumentation for Rapid Aerial Photo System",
        "authors": [
            "Widyawardana Adiprawita",
            "Adang Suwandi Ahmad",
            "Jaka Semibiring"
        ],
        "summary": "This research will proposed a new kind of relatively low cost autonomous UAV that will enable farmers to make just in time mosaics of aerial photo of their crop. These mosaics of aerial photo should be able to be produced with relatively low cost and within the 24 hours of acquisition constraint. The autonomous UAV will be equipped with payload management system specifically developed for rapid aerial mapping. As mentioned before turn around time is the key factor, so accuracy is not the main focus (not orthorectified aerial mapping). This system will also be equipped with special software to post process the aerial photos to produce the mosaic aerial photo map",
        "published": "2008-04-24T10:48:13Z",
        "link": "http://arxiv.org/abs/0804.3894v1",
        "categories": [
            "cs.RO",
            "B.7.2"
        ]
    },
    {
        "title": "First Principle Approach to Modeling of Small Scale Helicopter",
        "authors": [
            "A. Budiyono",
            "T. Sudiyanto",
            "H. Lesmana"
        ],
        "summary": "The establishment of global helicopter linear model is very precious and useful for the design of the linear control laws, since it is never afforded in the published literatures. In the first principle approach, the mathematical model was developed using basic helicopter theory accounting for particular characteristic of the miniature helicopter. No formal system identification procedures are required for the proposed model structure. The relevant published literatures however did not present the linear models required for the design of linear control laws. The paper presents a step by step development of linear model for small scale helicopter based on first-principle approach. Beyond the previous work in literatures, the calculation of the stability derivatives is presented in detail. A computer program is used to solve the equilibrium conditions and then calculate the change in aerodynamics forces and moments due to the change in each degree of freedom and control input. The detail derivation allows the comprehensive analysis of relative dominance of vehicle states and input variables to force and moment components. Hence it facilitates the development of minimum complexity small scale helicopter dynamics model.",
        "published": "2008-04-24T10:52:30Z",
        "link": "http://arxiv.org/abs/0804.3895v1",
        "categories": [
            "cs.RO",
            "I.6.1"
        ]
    },
    {
        "title": "Optimal Tracking Controller Design for a Small Scale Helicopter",
        "authors": [
            "Agus Budiyono",
            "Singgih S. Wibowo"
        ],
        "summary": "A model helicopter is more difficult to control than its full scale counterparts. This is due to its greater sensitivity to control inputs and disturbances as well as higher bandwidth of dynamics. This works is focused on designing practical tracking controller for a small scale helicopter following predefined trajectories. A tracking controller based on optimal control theory is synthesized as part of the development of an autonomous helicopter. Some issues in regards to control constraints are addressed. The weighting between state tracking performance and control power expenditure is analyzed. Overall performance of the control design is evaluated based on its time domain histories of trajectories as well as control inputs.",
        "published": "2008-04-24T10:58:35Z",
        "link": "http://arxiv.org/abs/0804.3897v1",
        "categories": [
            "cs.RO",
            "C.3"
        ]
    },
    {
        "title": "Development of a peristaltic micropump for bio-medical applications   based on mini LIPCA",
        "authors": [
            "My Pham",
            "Thanh Tung Nguyen",
            "Nam Seo Goo"
        ],
        "summary": "This paper presents the design, fabrication, and experimental characterization of a peristaltic micropump. The micropump is composed of two layers fabricated from polydimethylsiloxane (PDMS) material. The first layer has a rectangular channel and two valve seals. Three rectangular mini lightweight piezo-composite actuators are integrated in the second layer, and used as actuation parts. Two layers are bonded, and covered by two polymethyl methacrylate (PMMA) plates, which help increase the stiffness of the micropump. A maximum flow rate of 900 mokroliter per min and a maximum backpressure of 1.8 kPa are recorded when water is used as pump liquid. We measured the power consumption of the micropump. The micropump is found to be a promising candidate for bio-medical application due to its bio-compatibility, portability, bidirectionality, and simple effective design.",
        "published": "2008-04-28T12:59:55Z",
        "link": "http://arxiv.org/abs/0804.4395v1",
        "categories": [
            "cs.RO",
            "B.1.2"
        ]
    },
    {
        "title": "Intelligent Unmanned Explorer for Deep Space Exploration",
        "authors": [
            "T. Kubota",
            "T. Yoshimitsu"
        ],
        "summary": "asteroids or comets have received remarkable attention in the world. In small body explorations, especially, detailed in-situ surface exploration by tiny rover is one of effective and fruitful means and is expected to make strong contributions towards scientific studies. JAXA ISAS is promoting MUSES C mission, which is the worlds first sample and return attempt to or from the near earth asteroid. Hayabusa spacecraft in MUSES C mission took the tiny rover, which was expected to perform the in-situ surface exploration by hopping. This paper describes the system design, mobility and intelligence of the developed unmanned explorer. This paper also presents the ground experimental results and the flight results.",
        "published": "2008-04-30T01:18:37Z",
        "link": "http://arxiv.org/abs/0804.4717v1",
        "categories": [
            "cs.RO",
            "I.6.1"
        ]
    },
    {
        "title": "Study of improving nano-contouring performance by employing   cross-coupling controller",
        "authors": [
            "Wen Yuh Jywe",
            "Shih Shin Chen",
            "Hung-Shu Wang",
            "Chien Hung Liu",
            "Hsin Hung Jwo",
            "Yun Feng Teng",
            "Tung Hsien Hsieh"
        ],
        "summary": "For the tracking stage path planning, we design a two-axis cross-coupling control system which uses the PI controller to compensate the contour error between axes. In this paper, the stage adoptive is designed by our laboratory (Precision Machine Center of National Formosa University). The cross-coupling controller calculates the actuating signal of each axis by combining multi-axes position error. Hence, the cross-coupling controller improves the stage tracking ability and decreases the contour error. The experiments show excellent stage motion. This finding confirms that the proposed method is a powerful and efficient tool for improving stage tracking ability. Also found were the stages tracking to minimize contour error of two types circular to approximately 25nm.",
        "published": "2008-04-30T07:57:25Z",
        "link": "http://arxiv.org/abs/0804.4749v1",
        "categories": [
            "cs.RO",
            "B.1.2"
        ]
    },
    {
        "title": "The Numerical Control Design for a Pair of Dubins Vehicles",
        "authors": [
            "Heru Tjahjana",
            "Iwan Pranoto",
            "Hari Muhammad",
            "J. Naiborhu",
            "Miswanto"
        ],
        "summary": "In this paper, a model of a pair of Dubins vehicles is considered. The vehicles move from an initial position and orientation to final position and orientation. A long the motion, the two vehicles are not allowed to collide however the two vehicles cant to far each other. The optimal control of the vehicle is found using the Pontryagins Maximum Principle (PMP). This PMP leads to a Hamiltonian system consisting of a system of differential equation and its adjoint. The originally differential equation has initial and final condition but the adjoint system doesn't have one. The classical difficulty is solved numerically by the greatest gradient descent method. Some simulation results are presented in this paper.",
        "published": "2008-04-30T08:03:05Z",
        "link": "http://arxiv.org/abs/0804.4750v1",
        "categories": [
            "cs.RO",
            "I.2.8"
        ]
    },
    {
        "title": "Simulation of Dynamic Yaw Stability Derivatives of a Bird Using CFD",
        "authors": [
            "M. A. Moelyadi",
            "G. Sachs"
        ],
        "summary": "Simulation results on dynamic yaw stability derivatives of a gull bird by means of computational fluid dynamics are presented. Two different kinds of motions are used for determining the dynamic yaw stability derivatives CNr and CNbeta . Concerning the first one, simple lateral translation and yaw rotary motions in yaw are considered. The second one consists of combined motions. To determine dynamic yaw stability derivatives of the bird, the simulation of an unsteady flow with a bird model showing a harmonic motion is performed. The unsteady flow solution for each time step is obtained by solving unsteady Euler equations based on a finite volume approach for a smaller reduced frequency. Then, an evaluation of unsteady forces and moments for one cycle is conducted using harmonic Fourier analysis. The results on the dynamic yaw stability derivatives for both simulations of the model motion show a good agreement.",
        "published": "2008-04-30T08:16:33Z",
        "link": "http://arxiv.org/abs/0804.4752v1",
        "categories": [
            "cs.RO",
            "I.2.9"
        ]
    },
    {
        "title": "Wavelet Based Iterative Learning Control with Fuzzy PD Feedback for   Position Tracking of A Pneumatic Servo System",
        "authors": [
            "C. E. Huang",
            "J. S. Chen"
        ],
        "summary": "In this paper, a wavelet-based iterative learning control (WILC) scheme with Fuzzy PD feedback is presented for a pneumatic control system with nonsmooth nonlinearities and uncertain parameters. The wavelet transform is employed to extract the learnable dynamics from measured output signal before it can be used to update the control profile. The wavelet transform is adopted to decompose the original signal into many low-resolution signals that contain the learnable and unlearnable parts. The desired control profile is then compared with the learnable part of the transformed signal. Thus, the effects from unlearnable dynamics on the controlled system can be attenuated by a Fuzzy PD feedback controller. As for the rules of Fuzzy PD controller in the feedback loop, a genetic algorithm (GA) is employed to search for the inference rules of optimization. A proportional-valve controlled pneumatic cylinder actuator system is used as the control target for simulation. Simulation results have shown a much-improved positiontracking performance.",
        "published": "2008-04-30T08:22:19Z",
        "link": "http://arxiv.org/abs/0804.4753v1",
        "categories": [
            "cs.RO",
            "I.2.6"
        ]
    },
    {
        "title": "Positive Real Synthesis of Networked Control System An LMI Approach",
        "authors": [
            "Bambang Riyanto",
            "Imam Arifin"
        ],
        "summary": "This paper presents the positive real analysis and synthesis for Networked Control Systems (NCS) in discrete time. Based on the definition of passivity, the sufficient condition of NCS is given by stochastic Lyapunov functional. The controller via state feedback is designed to guarantee the stability of NCS and closed-loop positive realness. It is shown that a mode-dependent positive real controller exists if a set of coupled linear matrix inequalities has solutions. The controller can be then constructed in terms of the solutions.",
        "published": "2008-04-30T08:25:46Z",
        "link": "http://arxiv.org/abs/0804.4754v1",
        "categories": [
            "cs.RO",
            "I.2.8"
        ]
    },
    {
        "title": "Analysis of Stability, Response and LQR Controller Design of a Small   Scale Helicopter Dynamics",
        "authors": [
            "Hardian Reza Dharmayanda",
            "Taesam Kang",
            "Young Jae Lee",
            "Sangkyung Sung"
        ],
        "summary": "This paper presents how to use feedback controller with helicopter dynamics state space model. A simplified analysis is presented for controller design using LQR of small scale helicopters for axial and forward flights. Our approach is simple and gives the basic understanding about how to develop controller for solving the stability of linear helicopter flight dynamics.",
        "published": "2008-04-30T08:32:27Z",
        "link": "http://arxiv.org/abs/0804.4757v1",
        "categories": [
            "cs.RO",
            "I.2.8"
        ]
    },
    {
        "title": "Design and control of dynamical quantum processes in ortho para H2   conversion on surfaces",
        "authors": [
            "Rifki Muhida",
            "Riza Muhida",
            "Wilson A. Dino",
            "Hiroshi Nakanishi",
            "Hideaki Kasai"
        ],
        "summary": "We present here a novel, cost-effective method for increasing and controlling the ortho para H2 (o p H2) conversion yield. First, we invoke two processes derived from fundamental, surface science insights, based on the effect of molecular orientation on the hydrogen solid surface reaction, i.e., dynamical quantum filtering and steering, and apply them to enhance the o p H2 conversion yield. Second, we find an important factor that can significantly influence the yield i.e., inhomogeneity of spin density distribution. This factor gives us a promising possibility to increase the yield and to find the best catalyst e.g., design of materials that can function as catalysts for the o p H2 conversion.",
        "published": "2008-04-30T08:38:19Z",
        "link": "http://arxiv.org/abs/0804.4759v1",
        "categories": [
            "cs.RO",
            "I.2.8"
        ]
    },
    {
        "title": "Design of Attitude Stability System for Prolate Dual-spin Satellite in   Its Inclined Elliptical Orbit",
        "authors": [
            "J. Muliadi",
            "S. D. Jenie",
            "A. Budiyono"
        ],
        "summary": "In general, most of communication satellites were designed to be operated in geostationary orbit. And many of them were designed in prolate dual-spin configuration. As a prolate dual-spin vehicle, they have to be stabilized against their internal energy dissipation effect. Several countries that located in southern hemisphere, has shown interest to use communication satellite. Because of those countries southern latitude, an idea emerged to incline the communication satellite (due to its prolate dualspin configuration) in elliptical orbit. This work is focused on designing Attitude Stability System for prolate dual-spin satellite in the effect of perturbed field of gravity due to the inclination of its elliptical orbit. DANDE (De-spin Active Nutation Damping Electronics) provides primary stabilization method for the satellite in its orbit. Classical Control Approach is used for the iteration of DANDE parameters. The control performance is evaluated based on time response analysis.",
        "published": "2008-05-22T06:17:57Z",
        "link": "http://arxiv.org/abs/0805.3390v1",
        "categories": [
            "cs.RO",
            "I.2.9"
        ]
    },
    {
        "title": "Kinematic Analysis of the vertebra of an eel like robot",
        "authors": [
            "Damien Chablat"
        ],
        "summary": "The kinematic analysis of a spherical wrist with parallel architecture is the object of this article. This study is part of a larger French project, which aims to design and to build an eel like robot to imitate the eel swimming. To implement direct and inverse kinematics on the control law of the prototype, we need to evaluate the workspace without any collisions between the different bodies. The tilt and torsion parameters are used to represent the workspace.",
        "published": "2008-06-03T09:33:53Z",
        "link": "http://arxiv.org/abs/0806.0473v1",
        "categories": [
            "cs.RO",
            "physics.class-ph"
        ]
    },
    {
        "title": "Modeling And Simulation Of Prolate Dual-Spin Satellite Dynamics In An   Inclined Elliptical Orbit: Case Study Of Palapa B2R Satellite",
        "authors": [
            "J. Muliadi",
            "S. D. Jenie",
            "A. Budiyono"
        ],
        "summary": "In response to the interest to re-use Palapa B2R satellite nearing its End of Life (EOL) time, an idea to incline the satellite orbit in order to cover a new region has emerged in the recent years. As a prolate dual-spin vehicle, Palapa B2R has to be stabilized against its internal energy dissipation effect. This work is focused on analyzing the dynamics of the reusable satellite in its inclined orbit. The study discusses in particular the stability of the prolate dual-spin satellite under the effect of perturbed field of gravitation due to the inclination of its elliptical orbit. Palapa B2R physical data was substituted into the dual-spin's equation of motion. The coefficient of zonal harmonics J2 was induced into the gravity-gradient moment term that affects the satellite attitude. The satellite's motion and attitude were then simulated in the perturbed gravitational field by J2, with the variation of orbit's eccentricity and inclination. The analysis of the satellite dynamics and its stability was conducted for designing a control system for the vehicle in its new inclined orbit.",
        "published": "2008-06-04T10:33:07Z",
        "link": "http://arxiv.org/abs/0806.0740v1",
        "categories": [
            "cs.RO",
            "I.2.9"
        ]
    },
    {
        "title": "Onboard Multivariable Controller Design for a Small Scale Helicopter   Using Coefficient Diagram Method",
        "authors": [
            "A. Budiyono"
        ],
        "summary": "A mini scale helicopter exhibits not only increased sensitivity to control inputs and disturbances, but also higher bandwidth of its dynamics. These properties make model helicopters, as a flying robot, more difficult to control. The dynamics model accuracy will determine the performance of the designed controller. It is attractive in this regards to have a controller that can accommodate the unmodeled dynamics or parameter changes and perform well in such situations. Coefficient Diagram Method (CDM) is chosen as the candidate to synthesize such a controller due to its simplicity and convenience in demonstrating integrated performance measures including equivalent time constant, stability indices and robustness. In this study, CDM is implemented for a design of multivariable controller for a small scale helicopter during hover and cruise flight. In the synthesis of MIMO CDM, good design common sense based on hands-on experience is necessary. The low level controller algorithm is designed as part of hybrid supervisory control architecture to be implemented on an onboard computer system. Its feasibility and performance are evaluated based on its robustness, desired time domain system responses and compliance to hard-real time requirements.",
        "published": "2008-06-04T10:45:49Z",
        "link": "http://arxiv.org/abs/0806.0743v1",
        "categories": [
            "cs.RO",
            "I.2.9"
        ]
    },
    {
        "title": "Design, Development and Testing of Underwater Vehicles: ITB Experience",
        "authors": [
            "Muljowidodo",
            "Said D. Jenie",
            "Agus Budiyono",
            "Sapto A. Nugroho"
        ],
        "summary": "The last decade has witnessed increasing worldwide interest in the research of underwater robotics with particular focus on the area of autonomous underwater vehicles (AUVs). The underwater robotics technology has enabled human to access the depth of the ocean to conduct environmental surveys, resources mapping as well as scientific and military missions. This capability is especially valuable for countries with major water or oceanic resources. As an archipelagic nation with more than 13,000 islands, Indonesia has one of the most abundant living and non-organic oceanic resources. The needs for the mapping, exploration, and environmental preservation of the vast marine resources are therefore imperative. The challenge of the deep water exploration has been the complex issues associated with hazardous and unstructured undersea and sea-bed environments. The paper reports the design, development and testing efforts of underwater vehicle that have been conducted at Institut Teknologi Bandung. Key technology areas have been identified and step-by-step development is presented in conjunction with the need to meet the challenge of underwater vehicle operation. A number of future research directions are also highlighted.",
        "published": "2008-06-25T03:56:40Z",
        "link": "http://arxiv.org/abs/0806.4020v1",
        "categories": [
            "cs.RO",
            "I.2.9"
        ]
    },
    {
        "title": "Linear Parameter Varying Model Identification for Control of   Rotorcraft-based UAV",
        "authors": [
            "Agus Budiyono",
            "H. Y Sutarto"
        ],
        "summary": "A rotorcraft-based unmanned aerial vehicle exhibits more complex properties compared to its full-size counterparts due to its increased sensitivity to control inputs and disturbances and higher bandwidth of its dynamics. As an aerial vehicle with vertical take-off and landing capability, the helicopter specifically poses a difficult problem of transition between forward flight and unstable hover and vice versa. The LPV control technique explicitly takes into account the change in performance due to the real-time parameter variations. The technique therefore theoretically guarantees the performance and robustness over the entire operating envelope. In this study, we investigate a new approach implementing model identification for use in the LPV control framework. The identification scheme employs recursive least square technique implemented on the LPV system represented by dynamics of helicopter during a transition. The airspeed as the scheduling of parameter trajectory is not assumed to vary slowly. The exclusion of slow parameter change requirement allows for the application of the algorithm for aggressive maneuvering capability without the need of expensive computation. The technique is tested numerically and will be validated in the autonomous flight of a small scale helicopter.",
        "published": "2008-06-25T04:09:29Z",
        "link": "http://arxiv.org/abs/0806.4021v1",
        "categories": [
            "cs.RO",
            "I.2.9"
        ]
    },
    {
        "title": "An Algebraic Approach for the MIMO Control of Small Scale Helicopter",
        "authors": [
            "A. Budiyono",
            "T. Sudiyanto"
        ],
        "summary": "The control of small-scale helicopter is a MIMO problem. To use of classical control approach to formally solve a MIMO problem, one needs to come up with multidimensional Root Locus diagram to tune the control parameters. The problem with the required dimension of the RL diagram for MIMO design has forced the design procedure of classical approach to be conducted in cascaded multi-loop SISO system starting from the innermost loop outward. To implement this control approach for a helicopter, a pitch and roll attitude control system is often subordinated to a, respectively, longitudinal and lateral velocity control system in a nested architecture. The requirement for this technique to work is that the inner attitude control loop must have a higher bandwidth than the outer velocity control loop which is not the case for high performance mini helicopter. To address the above problems, an algebraic design approach is proposed in this work. The designed control using s-CDM approach is demonstrated for hovering control of small-scale helicopter simultaneously subjected to plant parameter uncertainties and wind disturbances.",
        "published": "2008-06-28T04:36:03Z",
        "link": "http://arxiv.org/abs/0806.4648v1",
        "categories": [
            "cs.RO",
            "I.2.8"
        ]
    },
    {
        "title": "Polygon Exploration with Time-Discrete Vision",
        "authors": [
            "Sandor P. Fekete",
            "Christiane Schmidt"
        ],
        "summary": "With the advent of autonomous robots with two- and three-dimensional scanning capabilities, classical visibility-based exploration methods from computational geometry have gained in practical importance. However, real-life laser scanning of useful accuracy does not allow the robot to scan continuously while in motion; instead, it has to stop each time it surveys its environment. This requirement was studied by Fekete, Klein and Nuechter for the subproblem of looking around a corner, but until now has not been considered in an online setting for whole polygonal regions.   We give the first algorithmic results for this important algorithmic problem that combines stationary art gallery-type aspects with watchman-type issues in an online scenario: We demonstrate that even for orthoconvex polygons, a competitive strategy can be achieved only for limited aspect ratio A (the ratio of the maximum and minimum edge length of the polygon), i.e., for a given lower bound on the size of an edge; we give a matching upper bound by providing an O(log A)-competitive strategy for simple rectilinear polygons, using the assumption that each edge of the polygon has to be fully visible from some scan point.",
        "published": "2008-07-15T12:10:08Z",
        "link": "http://arxiv.org/abs/0807.2358v2",
        "categories": [
            "cs.CG",
            "cs.RO",
            "F.2.2; I.2.9"
        ]
    },
    {
        "title": "On Endogenous Reconfiguration in Mobile Robotic Networks",
        "authors": [
            "Ketan Savla",
            "Emilio Frazzoli"
        ],
        "summary": "In this paper, our focus is on certain applications for mobile robotic networks, where reconfiguration is driven by factors intrinsic to the network rather than changes in the external environment. In particular, we study a version of the coverage problem useful for surveillance applications, where the objective is to position the robots in order to minimize the average distance from a random point in a given environment to the closest robot. This problem has been well-studied for omni-directional robots and it is shown that optimal configuration for the network is a centroidal Voronoi configuration and that the coverage cost belongs to $\\Theta(m^{-1/2})$, where $m$ is the number of robots in the network. In this paper, we study this problem for more realistic models of robots, namely the double integrator (DI) model and the differential drive (DD) model. We observe that the introduction of these motion constraints in the algorithm design problem gives rise to an interesting behavior. For a \\emph{sparser} network, the optimal algorithm for these models of robots mimics that for omni-directional robots. We propose novel algorithms whose performances are within a constant factor of the optimal asymptotically (i.e., as $m \\to +\\infty$). In particular, we prove that the coverage cost for the DI and DD models of robots is of order $m^{-1/3}$. Additionally, we show that, as the network grows, these novel algorithms outperform the conventional algorithm; hence necessitating a reconfiguration in the network in order to maintain optimal quality of service.",
        "published": "2008-07-16T20:09:18Z",
        "link": "http://arxiv.org/abs/0807.2648v2",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "The NAO humanoid: a combination of performance and affordability",
        "authors": [
            "David Gouaillier",
            "Vincent Hugel",
            "Pierre Blazevic",
            "Chris Kilner",
            "Jerome Monceaux",
            "Pascal Lafourcade",
            "Brice Marnier",
            "Julien Serre",
            "Bruno Maisonnier"
        ],
        "summary": "This article presents the design of the autonomous humanoid robot called NAO that is built by the French company Aldebaran-Robotics. With its height of 0.57 m and its weight about 4.5 kg, this innovative robot is lightweight and compact. It distinguishes itself from its existing Japanese, American, and other counterparts thanks to its pelvis kinematics design, its proprietary actuation system based on brush DC motors, its electronic, computer and distributed software architectures. This robot has been designed to be affordable without sacrificing quality and performance. It is an open and easy-to-handle platform where the user can change all the embedded system software or just add some applications to make the robot adopt specific behaviours. The robot's head and forearms are modular and can be changed to promote further evolution. The comprehensive and functional design is one of the reasons that helped select NAO to replace the AIBO quadrupeds in the 2008 RoboCup standard league.",
        "published": "2008-07-21T09:28:09Z",
        "link": "http://arxiv.org/abs/0807.3223v2",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Exploiting Bird Locomotion Kinematics Data for Robotics Modeling",
        "authors": [
            "Vincent Hugel",
            "Remi Hackert",
            "Anick Abourachid"
        ],
        "summary": "We present here the results of an analysis carried out by biologists and roboticists with the aim of modeling bird locomotion kinematics for robotics purposes. The aim was to develop a bio-inspired kinematic model of the bird leg from biological data. We first acquired and processed kinematic data for sagittal and top views obtained by X-ray radiography of quails walking. Data processing involved filtering and specific data reconstruction in three dimensions, as two-dimensional views cannot be synchronized. We then designed a robotic model of a bird-like leg based on a kinematic analysis of the biological data. Angular velocity vectors were calculated to define the number of degrees of freedom (DOF) at each joint and the orientation of the rotation axes.",
        "published": "2008-07-21T09:37:48Z",
        "link": "http://arxiv.org/abs/0807.3225v3",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Avoider robot design to dim the fire with dt basic mini system",
        "authors": [
            "Eri Prasetyo",
            "Wahyu K. R.",
            "Bumi Prabu Prabowo"
        ],
        "summary": "Avoider robot is mean robot who is designed to avoid the block in around. Except that, this robot is also added by an addition application to dim the fire. This robot is made with ultrasonic sensor PING. This sensor is set on the front, right and left from robot. This sensor is used robot to look for the right street, so that robot can walk on. After the robot can look for the right street, next accomplished the robot is looking for the fire in around. And the next, dim the fire with fan. This robot is made with basic stamp 2 micro-controller. And that micro-controller can be found in dt-basic mini system module. This robot is made with servo motor on the right and left side, which is used to movement.",
        "published": "2008-07-28T02:46:45Z",
        "link": "http://arxiv.org/abs/0807.4345v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "I'm sorry to say, but your understanding of image processing   fundamentals is absolutely wrong",
        "authors": [
            "Emanuel Diamant"
        ],
        "summary": "The ongoing discussion whether modern vision systems have to be viewed as visually-enabled cognitive systems or cognitively-enabled vision systems is groundless, because perceptual and cognitive faculties of vision are separate components of human (and consequently, artificial) information processing system modeling.",
        "published": "2008-08-01T04:45:17Z",
        "link": "http://arxiv.org/abs/0808.0056v1",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.IR",
            "cs.RO",
            "q-bio.NC"
        ]
    },
    {
        "title": "A 8 bits Pipeline Analog to Digital Converter Design for High Speed   Camera Application",
        "authors": [
            "Eri Prasetyo",
            "Hamzah Afandi",
            "Nurul Huda Dominique Ginhac",
            "Michel Paindavoine"
        ],
        "summary": "- This paper describes a pipeline analog-to-digital converter is implemented for high speed camera. In the pipeline ADC design, prime factor is designing operational amplifier with high gain so ADC have been high speed. The other advantage of pipeline is simple on concept, easy to implement in layout and have flexibility to increase speed. We made design and simulation using Mentor Graphics Software with 0.6 \\mu m CMOS technology with a total power dissipation of 75.47 mW. Circuit techniques used include a precise comparator, operational amplifier and clock management. A switched capacitor is used to sample and multiplying at each stage. Simulation a worst case DNL and INL of 0.75 LSB. The design operates at 5 V dc. The ADC achieves a SNDR of 44.86 dB. keywords: pipeline, switched capacitor, clock management",
        "published": "2008-08-04T03:23:20Z",
        "link": "http://arxiv.org/abs/0808.0374v1",
        "categories": [
            "cs.RO",
            "cs.CV"
        ]
    },
    {
        "title": "Design and Implementation a 8 bits Pipeline Analog to Digital Converter   in the Technology 0.6 μm CMOS Process",
        "authors": [
            "Eri Prasetyo",
            "Dominique Ginhac",
            "Michel Paindavoine"
        ],
        "summary": "This paper describes a 8 bits, 20 Msamples/s pipeline analog-to-digital converter implemented in 0.6 \\mu m CMOS technology with a total power dissipation of 75.47 mW. Circuit techniques used include a precise comparator, operational amplifier and clock management. A switched capacitor is used to sample and multiplying at each stage. Simulation a worst case DNL and INL of 0.75 LSB. The design operate at 5 V dc. The ADC achieves a SNDR of 44.86 dB. keywords : pipeline, switched capacitor, clock management",
        "published": "2008-08-04T07:19:57Z",
        "link": "http://arxiv.org/abs/0808.0387v1",
        "categories": [
            "cs.RO",
            "cs.CV"
        ]
    },
    {
        "title": "Self-Motions of General 3-RPR Planar Parallel Robots",
        "authors": [
            "Sébastien Briot",
            "Ilian Bonev",
            "Damien Chablat",
            "Philippe Wenger",
            "Vigen Arakelian"
        ],
        "summary": "This paper studies the kinematic geometry of general 3-RPR planar parallel robots with actuated base joints. These robots, while largely overlooked, have simple direct kinematics and large singularity-free workspace. Furthermore, their kinematic geometry is the same as that of a newly developed parallel robot with SCARA-type motions. Starting from the direct and inverse kinematic model, the expressions for the singularity loci of 3-RPR planar parallel robots are determined. Then, the global behaviour at all singularities is geometrically described by studying the degeneracy of the direct kinematic model. Special cases of self-motions are then examined and the degree of freedom gained in such special configurations is kinematically interpreted. Finally, a practical example is discussed and experimental validations performed on an actual robot prototype are presented.",
        "published": "2008-08-08T17:17:23Z",
        "link": "http://arxiv.org/abs/0808.1247v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Medical robotics: where we come from, where we are and where we could go",
        "authors": [
            "Jocelyne Troccaz"
        ],
        "summary": "This short note presents a viewpoint about medical robotics.",
        "published": "2008-08-12T13:21:52Z",
        "link": "http://arxiv.org/abs/0808.1661v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Spatial planning with constraints on translational distances between   geometric objects",
        "authors": [
            "Gennady Pustylnik"
        ],
        "summary": "The main constraint on relative position of geometric objects, used in spatial planning for computing the C-space maps (for example, in robotics, CAD, and packaging), is the relative non-overlapping of objects. This is the simplest constraint in which the minimum translational distance between objects is greater than zero, or more generally, than some positive value. We present a technique, based on the Minkowski operations, for generating the translational C-space maps for spatial planning with more general and more complex constraints on the relative position of geometric objects, such as constraints on various types (not only on the minimum) of the translational distances between objects. The developed technique can also be used, respectively, for spatial planning with constraints on translational distances in a given direction, and rotational distances between geometric objects, as well as for spatial planning with given dynamic geometric situation of moving objects.",
        "published": "2008-08-21T14:00:36Z",
        "link": "http://arxiv.org/abs/0808.2931v1",
        "categories": [
            "cs.CG",
            "cs.RO"
        ]
    },
    {
        "title": "Microcontroller-based System for Modular Networked Robot",
        "authors": [
            "I. Firmansyah",
            "Z. Akbar",
            "B. Hermanto",
            "L. T. Handoko"
        ],
        "summary": "A prototype of modular networked robot for autonomous monitoring works with full control over web through wireless connection has been developed. The robot is equipped with a particular set of built-in analyzing tools and appropriate censors, depending on its main purposes, to enable self-independent and real-time data acquisition and processing. The paper is focused on the microcontroller-based system to realize the modularity. The whole system is divided into three modules : main unit, data acquisition and data processing, while the analyzed results and all aspects of control and monitoring systems are fully accessible over an integrated web-interface. This concept leads to some unique features : enhancing flexibility due to enabling partial replacement of the modules according to user needs, easy access over web for remote users, and low development and maintenance cost due to software dominated components.",
        "published": "2008-09-04T00:13:11Z",
        "link": "http://arxiv.org/abs/0809.0727v1",
        "categories": [
            "cs.RO",
            "cs.CY"
        ]
    },
    {
        "title": "Kinetostatic Performance of a Planar Parallel Mechanism with Variable   Actuation",
        "authors": [
            "Novona Rakotomanga",
            "Damien Chablat",
            "Stéphane Caro"
        ],
        "summary": "This paper deals with a new planar parallel mechanism with variable actuation and its kinetostatic performance. A drawback of parallel mechanisms is the non homogeneity of kinetostatic performance within their workspace. The common approach to solve this problem is the introduction of actuation redundancy, that involves force control algorithms. Another approach, highlighted in this paper, is to select the actuated joint in each limb with regard to the pose of the end-effector. First, the architecture of the mechanism and two kinetostatic performance indices are described. Then, the actuating modes of the mechanism are compared.",
        "published": "2008-09-18T14:13:40Z",
        "link": "http://arxiv.org/abs/0809.3044v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Kinematic and Dynamic Analyses of the Orthoglide 5-axis",
        "authors": [
            "Raza Ur-Rehman",
            "Stéphane Caro",
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "This paper deals with the kinematic and dynamic analyses of the Orthoglide 5-axis, a five-degree-of-freedom manipulator. It is derived from two manipulators: i) the Orthoglide 3-axis; a three dof translational manipulator and ii) the Agile eye; a parallel spherical wrist. First, the kinematic and dynamic models of the Orthoglide 5-axis are developed. The geometric and inertial parameters of the manipulator are determined by means of a CAD software. Then, the required motors performances are evaluated for some test trajectories. Finally, the motors are selected in the catalogue from the previous results.",
        "published": "2008-09-18T15:11:50Z",
        "link": "http://arxiv.org/abs/0809.3179v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Singularity Analysis of Limited-dof Parallel Manipulators using   Grassmann-Cayley Algebra",
        "authors": [
            "Daniel Kanaan",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This paper characterizes geometrically the singularities of limited DOF parallel manipulators. The geometric conditions associated with the dependency of six Pl\\\"ucker vector of lines (finite and infinite) constituting the rows of the inverse Jacobian matrix are formulated using Grassmann-Cayley algebra. Manipulators under consideration do not need to have a passive spherical joint somewhere in each leg. This study is illustrated with three example robots",
        "published": "2008-09-18T15:14:20Z",
        "link": "http://arxiv.org/abs/0809.3180v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Framework for Dynamic Evaluation of Muscle Fatigue in Manual Handling   Work",
        "authors": [
            "Liang Ma",
            "Fouad Bennis",
            "Damien Chablat",
            "Wei Zhang"
        ],
        "summary": "Muscle fatigue is defined as the point at which the muscle is no longer able to sustain the required force or work output level. The overexertion of muscle force and muscle fatigue can induce acute pain and chronic pain in human body. When muscle fatigue is accumulated, the functional disability can be resulted as musculoskeletal disorders (MSD). There are several posture exposure analysis methods useful for rating the MSD risks, but they are mainly based on static postures. Even in some fatigue evaluation methods, muscle fatigue evaluation is only available for static postures, but not suitable for dynamic working process. Meanwhile, some existing muscle fatigue models based on physiological models cannot be easily used in industrial ergonomic evaluations. The external dynamic load is definitely the most important factor resulting muscle fatigue, thus we propose a new fatigue model under a framework for evaluating fatigue in dynamic working processes. Under this framework, virtual reality system is taken to generate virtual working environment, which can be interacted with the work with haptic interfaces and optical motion capture system. The motion information and load information are collected and further processed to evaluate the overall work load of the worker based on dynamic muscle fatigue models and other work evaluation criterions and to give new information to characterize the penibility of the task in design process.",
        "published": "2008-09-18T15:15:58Z",
        "link": "http://arxiv.org/abs/0809.3181v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "SINGULAB - A Graphical user Interface for the Singularity Analysis of   Parallel Robots based on Grassmann-Cayley Algebra",
        "authors": [
            "Patricia Ben-Horin",
            "Moshe Shoham",
            "Stéphane Caro",
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "This paper presents SinguLab, a graphical user interface for the singularity analysis of parallel robots. The algorithm is based on Grassmann-Cayley algebra. The proposed tool is interactive and introduces the designer to the singularity analysis performed by this method, showing all the stages along the procedure and eventually showing the solution algebraically and graphically, allowing as well the singularity verification of different robot poses.",
        "published": "2008-09-18T15:19:36Z",
        "link": "http://arxiv.org/abs/0809.3182v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Changing Assembly Modes without Passing Parallel Singularities in   Non-Cuspidal 3-R\\underline{P}R Planar Parallel Robots",
        "authors": [
            "Ilian Bonev",
            "Sébastien Briot",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "This paper demonstrates that any general 3-DOF three-legged planar parallel robot with extensible legs can change assembly modes without passing through parallel singularities (configurations where the mobile platform loses its stiffness). While the results are purely theoretical, this paper questions the very definition of parallel singularities.",
        "published": "2008-09-19T14:25:17Z",
        "link": "http://arxiv.org/abs/0809.3384v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A Computational Study on Emotions and Temperament in Multi-Agent Systems",
        "authors": [
            "Luis Paulo Reis",
            "Daria Barteneva",
            "Nuno Lau"
        ],
        "summary": "Recent advances in neurosciences and psychology have provided evidence that affective phenomena pervade intelligence at many levels, being inseparable from the cognitionaction loop. Perception, attention, memory, learning, decisionmaking, adaptation, communication and social interaction are some of the aspects influenced by them. This work draws its inspirations from neurobiology, psychophysics and sociology to approach the problem of building autonomous robots capable of interacting with each other and building strategies based on temperamental decision mechanism. Modelling emotions is a relatively recent focus in artificial intelligence and cognitive modelling. Such models can ideally inform our understanding of human behavior. We may see the development of computational models of emotion as a core research focus that will facilitate advances in the large array of computational systems that model, interpret or influence human behavior. We propose a model based on a scalable, flexible and modular approach to emotion which allows runtime evaluation between emotional quality and performance. The results achieved showed that the strategies based on temperamental decision mechanism strongly influence the system performance and there are evident dependency between emotional state of the agents and their temperamental type, as well as the dependency between the team performance and the temperamental configuration of the team members, and this enable us to conclude that the modular approach to emotional programming based on temperamental theory is the good choice to develop computational mind models for emotional behavioral Multi-Agent systems.",
        "published": "2008-09-27T16:33:34Z",
        "link": "http://arxiv.org/abs/0809.4784v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "cs.RO"
        ]
    },
    {
        "title": "Stiffness Analysis Of Multi-Chain Parallel Robotic Systems",
        "authors": [
            "Anatoly Pashkevich",
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The paper presents a new stiffness modelling method for multi-chain parallel robotic manipulators with flexible links and compliant actuating joints. In contrast to other works, the method involves a FEA-based link stiffness evaluation and employs a new solution strategy of the kinetostatic equations, which allows computing the stiffness matrix for singular postures and to take into account influence of the internal forces. The advantages of the developed technique are confirmed by application examples, which deal with stiffness analysis of the Orthoglide manipulator.",
        "published": "2008-10-05T15:03:22Z",
        "link": "http://arxiv.org/abs/0810.0830v1",
        "categories": [
            "cs.RO",
            "physics.class-ph"
        ]
    },
    {
        "title": "Characterizing 1-Dof Henneberg-I graphs with efficient configuration   spaces",
        "authors": [
            "Heping Gao",
            "Meera Sitharam"
        ],
        "summary": "We define and study exact, efficient representations of realization spaces of a natural class of underconstrained 2D Euclidean Distance Constraint Systems(EDCS) or Frameworks based on 1-dof Henneberg-I graphs. Each representation corresponds to a choice of parameters and yields a different parametrized configuration space. Our notion of efficiency is based on the algebraic complexities of sampling the configuration space and of obtaining a realization from the sample (parametrized) configuration. Significantly, we give purely combinatorial characterizations that capture (i) the class of graphs that have efficient configuration spaces and (ii) the possible choices of representation parameters that yield efficient configuration spaces for a given graph. Our results automatically yield an efficient algorithm for sampling realizations, without missing extreme or boundary realizations. In addition, our results formally show that our definition of efficient configuration space is robust and that our characterizations are tight. We choose the class of 1-dof Henneberg-I graphs in order to take the next step in a systematic and graded program of combinatorial characterizations of efficient configuration spaces. In particular, the results presented here are the first characterizations that go beyond graphs that have connected and convex configuration spaces.",
        "published": "2008-10-12T20:17:21Z",
        "link": "http://arxiv.org/abs/0810.1997v2",
        "categories": [
            "cs.CG",
            "cs.RO",
            "cs.SC"
        ]
    },
    {
        "title": "Path Planner for Objects, Robots and Mannequins by Multi-Agents Systems   or Motion Captures",
        "authors": [
            "Damien Chablat"
        ],
        "summary": "In order to optimise the costs and time of design of the new products while improving their quality, concurrent engineering is based on the digital model of these products. However, in order to be able to avoid definitively physical model without loss of information, new tools must be available. Especially, a tool making it possible to check simply and quickly the maintainability of complex mechanical sets using the numerical model is necessary. Since one decade, the MCM team of IRCCyN works on the creation of tools for the generation and the analysis of trajectories of virtual mannequins. The simulation of human tasks can be carried out either by robot-like simulation or by simulation by motion capture. This paper presents some results on the both two methods. The first method is based on a multi-agent system and on a digital mock-up technology, to assess an efficient path planner for a manikin or a robot for access and visibility task taking into account ergonomic constraints or joint limits. The human operator is integrated in the process optimisation to contribute to a global perception of the environment. This operator cooperates, in real-time, with several automatic local elementary agents. In the second method, we worked with the CEA and EADS/CCR to solve the constraints related to the evolution of human virtual in its environment on the basis of data resulting from motion capture system. An approach using of the virtual guides was developed to allow to the user the realization of precise trajectory in absence of force feedback.",
        "published": "2008-10-15T11:13:12Z",
        "link": "http://arxiv.org/abs/0810.2665v2",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A Vision-based Computed Torque Control for Parallel Kinematic Machines",
        "authors": [
            "Flavien Paccot",
            "Philippe Lemoine",
            "Nicolas Andreff",
            "Damien Chablat",
            "Philippe Martinet"
        ],
        "summary": "In this paper, a novel approach for parallel kinematic machine control relying on a fast exteroceptive measure is implemented and validated on the Orthoglide robot. This approach begins with rewriting the robot models as a function of the only end-effector pose. It is shown that such an operation reduces the model complexity. Then, this approach uses a classical Cartesian space computed torque control with a fast exteroceptive measure, reducing the control schemes complexity. Simulation results are given to show the expected performance improvements and experiments prove the practical feasibility of the approach.",
        "published": "2008-10-15T11:14:15Z",
        "link": "http://arxiv.org/abs/0810.2666v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Quantum robot: structure, algorithms and applications",
        "authors": [
            "Daoyi Dong",
            "Chunlin Chen",
            "Chenbin Zhang",
            "Zonghai Chen"
        ],
        "summary": "This paper has been withdrawn.",
        "published": "2008-10-18T01:18:03Z",
        "link": "http://arxiv.org/abs/0810.3283v2",
        "categories": [
            "cs.RO",
            "cs.AI",
            "quant-ph"
        ]
    },
    {
        "title": "Modeling Microscopic Chemical Sensors in Capillaries",
        "authors": [
            "Tad Hogg"
        ],
        "summary": "Nanotechnology-based microscopic robots could provide accurate in vivo measurement of chemicals in the bloodstream for detailed biological research and as an aid to medical treatment. Quantitative performance estimates of such devices require models of how chemicals in the blood diffuse to the devices. This paper models microscopic robots and red blood cells (erythrocytes) in capillaries using realistic distorted cell shapes. The models evaluate two sensing scenarios: robots moving with the cells past a chemical source on the vessel wall, and robots attached to the wall for longer-term chemical monitoring. Using axial symmetric geometry with realistic flow speeds and diffusion coefficients, we compare detection performance with a simpler model that does not include the cells. The average chemical absorption is quantitatively similar in both models, indicating the simpler model is an adequate design guide to sensor performance in capillaries. However, determining the variation in forces and absorption as cells move requires the full model.",
        "published": "2008-11-10T18:04:47Z",
        "link": "http://arxiv.org/abs/0811.1520v1",
        "categories": [
            "cs.RO",
            "physics.bio-ph",
            "q-bio.TO"
        ]
    },
    {
        "title": "Analyse de la rigidité des machines outils 3 axes d'architecture   parallèle hyperstatique",
        "authors": [
            "Anatoly Pashkevich",
            "Damien Chablat",
            "Philippe Wenger"
        ],
        "summary": "The paper presents a new stiffness modelling method for overconstrained parallel manipulators, which is applied to 3-d.o.f. translational mechanisms. It is based on a multidimensional lumped-parameter model that replaces the link flexibility by localized 6-d.o.f. virtual springs. In contrast to other works, the method includes a FEA-based link stiffness evaluation and employs a new solution strategy of the kinetostatic equations, which allows computing the stiffness matrix for the overconstrained architectures and for the singular manipulator postures. The advantages of the developed technique are confirmed by application examples, which deal with comparative stiffness analysis of two translational parallel manipulators.",
        "published": "2008-11-21T13:47:01Z",
        "link": "http://arxiv.org/abs/0811.3536v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Kinematic Analysis of a Serial - Parallel Machine Tool: the VERNE   machine",
        "authors": [
            "Daniel Kanaan",
            "Philippe Wenger",
            "Damien Chablat"
        ],
        "summary": "The paper derives the inverse and the forward kinematic equations of a serial - parallel 5-axis machine tool: the VERNE machine. This machine is composed of a three-degree-of-freedom (DOF) parallel module and a two-DOF serial tilting table. The parallel module consists of a moving platform that is connected to a fixed base by three non-identical legs. These legs are connected in a way that the combined effects of the three legs lead to an over-constrained mechanism with complex motion. This motion is defined as a simultaneous combination of rotation and translation. In this paper we propose symbolical methods that able to calculate all kinematic solutions and identify the acceptable one by adding analytical constraint on the disposition of legs of the parallel module.",
        "published": "2008-11-28T15:32:24Z",
        "link": "http://arxiv.org/abs/0811.4733v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "An Integrated Software-based Solution for Modular and Self-independent   Networked Robot",
        "authors": [
            "I. Firmansyah",
            "Z. Akbar",
            "B. Hermanto",
            "L. T. Handoko"
        ],
        "summary": "An integrated software-based solution for a modular and self-independent networked robot is introduced. The wirelessly operatable robot has been developed mainly for autonomous monitoring works with full control over web. The integrated software solution covers three components : a) the digital signal processing unit for data retrieval and monitoring system; b) the externally executable codes for control system; and c) the web programming for interfacing the end-users with the robot. It is argued that this integrated software-based approach is crucial to realize a flexible, modular and low development cost mobile monitoring apparatus.",
        "published": "2008-11-29T12:52:54Z",
        "link": "http://arxiv.org/abs/0812.0070v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Urologic robots and future directions",
        "authors": [
            "Pierre Mozer",
            "Jocelyne Troccaz",
            "Dan Stoianovici"
        ],
        "summary": "PURPOSE OF REVIEW: Robot-assisted laparoscopic surgery in urology has gained immense popularity with the daVinci system, but a lot of research teams are working on new robots. The purpose of this study is to review current urologic robots and present future development directions. RECENT FINDINGS: Future systems are expected to advance in two directions: improvements of remote manipulation robots and developments of image-guided robots. SUMMARY: The final goal of robots is to allow safer and more homogeneous outcomes with less variability of surgeon performance, as well as new tools to perform tasks on the basis of medical transcutaneous imaging, in a less invasive way, at lower costs. It is expected that improvements for a remote system could be augmented in reality, with haptic feedback, size reduction, and development of new tools for natural orifice translumenal endoscopic surgery. The paradigm of image-guided robots is close to clinical availability and the most advanced robots are presented with end-user technical assessments. It is also notable that the potential of robots lies much further ahead than the accomplishments of the daVinci system. The integration of imaging with robotics holds a substantial promise, because this can accomplish tasks otherwise impossible. Image-guided robots have the potential to offer a paradigm shift.",
        "published": "2008-12-12T08:38:23Z",
        "link": "http://arxiv.org/abs/0812.2313v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "BiopSym: a simulator for enhanced learning of ultrasound-guided prostate   biopsy",
        "authors": [
            "Stefano Sclaverano",
            "Grégoire Chevreau",
            "Lucile Vadcard",
            "Pierre Mozer",
            "Jocelyne Troccaz"
        ],
        "summary": "This paper describes a simulator of ultrasound-guided prostate biopsies for cancer diagnosis. When performing biopsy series, the clinician has to move the ultrasound probe and to mentally integrate the real-time bi-dimensional images into a three-dimensional (3D) representation of the anatomical environment. Such a 3D representation is necessary to sample regularly the prostate in order to maximize the probability of detecting a cancer if any. To make the training of young physicians easier and faster we developed a simulator that combines images computed from three-dimensional ultrasound recorded data to haptic feedback. The paper presents the first version of this simulator.",
        "published": "2008-12-17T09:22:47Z",
        "link": "http://arxiv.org/abs/0812.3226v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "I, Quantum Robot: Quantum Mind control on a Quantum Computer",
        "authors": [
            "Paola Zizzi"
        ],
        "summary": "The logic which describes quantum robots is not orthodox quantum logic, but a deductive calculus which reproduces the quantum tasks (computational processes, and actions) taking into account quantum superposition and quantum entanglement. A way toward the realization of intelligent quantum robots is to adopt a quantum metalanguage to control quantum robots. A physical implementation of a quantum metalanguage might be the use of coherent states in brain signals.",
        "published": "2008-12-25T16:31:05Z",
        "link": "http://arxiv.org/abs/0812.4614v2",
        "categories": [
            "quant-ph",
            "cs.AI",
            "cs.LO",
            "cs.RO"
        ]
    },
    {
        "title": "Regular Expression Subtyping for XML Query and Update Languages",
        "authors": [
            "James Cheney"
        ],
        "summary": "XML database query languages such as XQuery employ regular expression types with structural subtyping. Subtyping systems typically have two presentations, which should be equivalent: a declarative version in which the subsumption rule may be used anywhere, and an algorithmic version in which the use of subsumption is limited in order to make typechecking syntax-directed and decidable. However, the XQuery standard type system circumvents this issue by using imprecise typing rules for iteration constructs and defining only algorithmic typechecking, and another extant proposal provides more precise types for iteration constructs but ignores subtyping. In this paper, we consider a core XQuery-like language with a subsumption rule and prove the completeness of algorithmic typechecking; this is straightforward for XQuery proper but requires some care in the presence of more precise iteration typing disciplines. We extend this result to an XML update language we have introduced in earlier work.",
        "published": "2008-01-04T18:13:48Z",
        "link": "http://arxiv.org/abs/0801.0714v1",
        "categories": [
            "cs.PL",
            "cs.DB"
        ]
    },
    {
        "title": "Modeling Online Reviews with Multi-grain Topic Models",
        "authors": [
            "Ivan Titov",
            "Ryan McDonald"
        ],
        "summary": "In this paper we present a novel framework for extracting the ratable aspects of objects from online user reviews. Extracting such aspects is an important challenge in automatically mining product opinions from the web and in generating opinion-based summaries of user reviews. Our models are based on extensions to standard topic modeling methods such as LDA and PLSA to induce multi-grain topics. We argue that multi-grain models are more appropriate for our task since standard models tend to produce topics that correspond to global properties of objects (e.g., the brand of a product type) rather than the aspects of an object that tend to be rated by a user. The models we present not only extract ratable aspects, but also cluster them into coherent topics, e.g., `waitress' and `bartender' are part of the same topic `staff' for restaurants. This differentiates it from much of the previous work which extracts aspects through term frequency analysis with minimal clustering. We evaluate the multi-grain models both qualitatively and quantitatively to show that they improve significantly upon standard topic models.",
        "published": "2008-01-07T17:01:34Z",
        "link": "http://arxiv.org/abs/0801.1063v1",
        "categories": [
            "cs.IR",
            "cs.DB",
            "H.2.8; H.3.1; H.4"
        ]
    },
    {
        "title": "On Breaching Enterprise Data Privacy Through Adversarial Information   Fusion",
        "authors": [
            "Srivatsava Ranjit Ganta",
            "Raj Acharya"
        ],
        "summary": "Data privacy is one of the key challenges faced by enterprises today. Anonymization techniques address this problem by sanitizing sensitive data such that individual privacy is preserved while allowing enterprises to maintain and share sensitive data. However, existing work on this problem make inherent assumptions about the data that are impractical in day-to-day enterprise data management scenarios. Further, application of existing anonymization schemes on enterprise data could lead to adversarial attacks in which an intruder could use information fusion techniques to inflict a privacy breach. In this paper, we shed light on the shortcomings of current anonymization schemes in the context of enterprise data. We define and experimentally demonstrate Web-based Information- Fusion Attack on anonymized enterprise data. We formulate the problem of Fusion Resilient Enterprise Data Anonymization and propose a prototype solution to address this problem.",
        "published": "2008-01-11T03:21:49Z",
        "link": "http://arxiv.org/abs/0801.1715v2",
        "categories": [
            "cs.DB",
            "cs.CR",
            "cs.OH"
        ]
    },
    {
        "title": "Fault-Tolerant Partial Replication in Large-Scale Database Systems",
        "authors": [
            "Pierre Sutra",
            "Marc Shapiro"
        ],
        "summary": "We investigate a decentralised approach to committing transactions in a replicated database, under partial replication. Previous protocols either re-execute transactions entirely and/or compute a total order of transactions. In contrast, ours applies update values, and orders only conflicting transactions. It results that transactions execute faster, and distributed databases commit in small committees. Both effects contribute to preserve scalability as the number of databases and transactions increase. Our algorithm ensures serializability, and is live and safe in spite of faults.",
        "published": "2008-02-01T14:47:24Z",
        "link": "http://arxiv.org/abs/0802.0137v3",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Sketch-Based Estimation of Subpopulation-Weight",
        "authors": [
            "Edith Cohen",
            "Haim Kaplan"
        ],
        "summary": "Summaries of massive data sets support approximate query processing over the original data. A basic aggregate over a set of records is the weight of subpopulations specified as a predicate over records' attributes. Bottom-k sketches are a powerful summarization format of weighted items that includes priority sampling and the classic weighted sampling without replacement. They can be computed efficiently for many representations of the data including distributed databases and data streams.   We derive novel unbiased estimators and efficient confidence bounds for subpopulation weight. Our estimators and bounds are tailored by distinguishing between applications (such as data streams) where the total weight of the sketched set can be computed by the summarization algorithm without a significant use of additional resources, and applications (such as sketches of network neighborhoods) where this is not the case.   Our rigorous derivations are based on clever applications of the Horvitz-Thompson estimator, and are complemented by efficient computational methods. We demonstrate their benefit on a wide range of Pareto distributions.",
        "published": "2008-02-23T15:25:04Z",
        "link": "http://arxiv.org/abs/0802.3448v1",
        "categories": [
            "cs.DB",
            "cs.DS",
            "cs.NI",
            "cs.PF",
            "H.3.3; H.2; F.2.2"
        ]
    },
    {
        "title": "Neural Networks and Database Systems",
        "authors": [
            "Erich Schikuta"
        ],
        "summary": "Object-oriented database systems proved very valuable at handling and administrating complex objects. In the following guidelines for embedding neural networks into such systems are presented. It is our goal to treat networks as normal data in the database system. From the logical point of view, a neural network is a complex data value and can be stored as a normal data object. It is generally accepted that rule-based reasoning will play an important role in future database applications. The knowledge base consists of facts and rules, which are both stored and handled by the underlying database system. Neural networks can be seen as representation of intensional knowledge of intelligent database systems. So they are part of a rule based knowledge pool and can be used like conventional rules. The user has a unified view about his knowledge base regardless of the origin of the unique rules.",
        "published": "2008-02-25T09:57:31Z",
        "link": "http://arxiv.org/abs/0802.3582v1",
        "categories": [
            "cs.DB",
            "cs.NE",
            "H.2.1"
        ]
    },
    {
        "title": "Hospital Case Cost Estimates Modelling - Algorithm Comparison",
        "authors": [
            "Peter Andru",
            "Alexei Botchkarev"
        ],
        "summary": "Ontario (Canada) Health System stakeholders support the idea and necessity of the integrated source of data that would include both clinical (e.g. diagnosis, intervention, length of stay, case mix group) and financial (e.g. cost per weighted case, cost per diem) characteristics of the Ontario healthcare system activities at the patient-specific level. At present, the actual patient-level case costs in the explicit form are not available in the financial databases for all hospitals. The goal of this research effort is to develop financial models that will assign each clinical case in the patient-specific data warehouse a dollar value, representing the cost incurred by the Ontario health care facility which treated the patient. Five mathematical models have been developed and verified using real dataset. All models can be classified into two groups based on their underlying method: 1. Models based on using relative intensity weights of the cases, and 2. Models based on using cost per diem.",
        "published": "2008-02-28T04:56:48Z",
        "link": "http://arxiv.org/abs/0802.4126v1",
        "categories": [
            "cs.CE",
            "cs.DB"
        ]
    },
    {
        "title": "Composition Attacks and Auxiliary Information in Data Privacy",
        "authors": [
            "Srivatsava Ranjit Ganta",
            "Shiva Prasad Kasiviswanathan",
            "Adam Smith"
        ],
        "summary": "Privacy is an increasingly important aspect of data publishing. Reasoning about privacy, however, is fraught with pitfalls. One of the most significant is the auxiliary information (also called external knowledge, background knowledge, or side information) that an adversary gleans from other channels such as the web, public records, or domain knowledge. This paper explores how one can reason about privacy in the face of rich, realistic sources of auxiliary information. Specifically, we investigate the effectiveness of current anonymization schemes in preserving privacy when multiple organizations independently release anonymized data about overlapping populations. 1. We investigate composition attacks, in which an adversary uses independent anonymized releases to breach privacy. We explain why recently proposed models of limited auxiliary information fail to capture composition attacks. Our experiments demonstrate that even a simple instance of a composition attack can breach privacy in practice, for a large class of currently proposed techniques. The class includes k-anonymity and several recent variants. 2. On a more positive note, certain randomization-based notions of privacy (such as differential privacy) provably resist composition attacks and, in fact, the use of arbitrary side information. This resistance enables stand-alone design of anonymization schemes, without the need for explicitly keeping track of other releases. We provide a precise formulation of this property, and prove that an important class of relaxations of differential privacy also satisfy the property. This significantly enlarges the class of protocols known to enable modular design.",
        "published": "2008-03-01T00:36:12Z",
        "link": "http://arxiv.org/abs/0803.0032v2",
        "categories": [
            "cs.DB",
            "cs.CR"
        ]
    },
    {
        "title": "Inferring Neuronal Network Connectivity from Spike Data: A Temporal   Datamining Approach",
        "authors": [
            "Debprakash Patnaik",
            "P. S. Sastry",
            "K. P. Unnikrishnan"
        ],
        "summary": "Understanding the functioning of a neural system in terms of its underlying circuitry is an important problem in neuroscience. Recent developments in electrophysiology and imaging allow one to simultaneously record activities of hundreds of neurons. Inferring the underlying neuronal connectivity patterns from such multi-neuronal spike train data streams is a challenging statistical and computational problem. This task involves finding significant temporal patterns from vast amounts of symbolic time series data. In this paper we show that the frequent episode mining methods from the field of temporal data mining can be very useful in this context. In the frequent episode discovery framework, the data is viewed as a sequence of events, each of which is characterized by an event type and its time of occurrence and episodes are certain types of temporal patterns in such data. Here we show that, using the set of discovered frequent episodes from multi-neuronal data, one can infer different types of connectivity patterns in the neural system that generated it. For this purpose, we introduce the notion of mining for frequent episodes under certain temporal constraints; the structure of these temporal constraints is motivated by the application. We present algorithms for discovering serial and parallel episodes under these temporal constraints. Through extensive simulation studies we demonstrate that these methods are useful for unearthing patterns of neuronal network connectivity.",
        "published": "2008-03-04T14:11:38Z",
        "link": "http://arxiv.org/abs/0803.0450v2",
        "categories": [
            "cs.DB",
            "q-bio.NC"
        ]
    },
    {
        "title": "What Can We Learn Privately?",
        "authors": [
            "Shiva Prasad Kasiviswanathan",
            "Homin K. Lee",
            "Kobbi Nissim",
            "Sofya Raskhodnikova",
            "Adam Smith"
        ],
        "summary": "Learning problems form an important category of computational tasks that generalizes many of the computations researchers apply to large real-life data sets. We ask: what concept classes can be learned privately, namely, by an algorithm whose output does not depend too heavily on any one input or specific training example? More precisely, we investigate learning algorithms that satisfy differential privacy, a notion that provides strong confidentiality guarantees in contexts where aggregate information is released about a database containing sensitive information about individuals. We demonstrate that, ignoring computational constraints, it is possible to privately agnostically learn any concept class using a sample size approximately logarithmic in the cardinality of the concept class. Therefore, almost anything learnable is learnable privately: specifically, if a concept class is learnable by a (non-private) algorithm with polynomial sample complexity and output size, then it can be learned privately using a polynomial number of samples. We also present a computationally efficient private PAC learner for the class of parity functions. Local (or randomized response) algorithms are a practical class of private algorithms that have received extensive investigation. We provide a precise characterization of local private learning algorithms. We show that a concept class is learnable by a local algorithm if and only if it is learnable in the statistical query (SQ) model. Finally, we present a separation between the power of interactive and noninteractive local learning algorithms.",
        "published": "2008-03-06T17:50:07Z",
        "link": "http://arxiv.org/abs/0803.0924v3",
        "categories": [
            "cs.LG",
            "cs.CC",
            "cs.CR",
            "cs.DB"
        ]
    },
    {
        "title": "Selective association rule generation",
        "authors": [
            "Michael Hahsler",
            "Christian Buchta",
            "Kurt Hornik"
        ],
        "summary": "Mining association rules is a popular and well researched method for discovering interesting relations between variables in large databases. A practical problem is that at medium to low support values often a large number of frequent itemsets and an even larger number of association rules are found in a database. A widely used approach is to gradually increase minimum support and minimum confidence or to filter the found rules using increasingly strict constraints on additional measures of interestingness until the set of rules found is reduced to a manageable size. In this paper we describe a different approach which is based on the idea to first define a set of ``interesting'' itemsets (e.g., by a mixture of mining and expert knowledge) and then, in a second step to selectively generate rules for only these itemsets. The main advantage of this approach over increasing thresholds or filtering rules is that the number of rules found is significantly reduced while at the same time it is not necessary to increase the support and confidence thresholds which might lead to missing important information in the database.",
        "published": "2008-03-06T19:43:35Z",
        "link": "http://arxiv.org/abs/0803.0954v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "New probabilistic interest measures for association rules",
        "authors": [
            "Michael Hahsler",
            "Kurt Hornik"
        ],
        "summary": "Mining association rules is an important technique for discovering meaningful patterns in transaction databases. Many different measures of interestingness have been proposed for association rules. However, these measures fail to take the probabilistic properties of the mined data into account. In this paper, we start with presenting a simple probabilistic framework for transaction data which can be used to simulate transaction data when no associations are present. We use such data and a real-world database from a grocery outlet to explore the behavior of confidence and lift, two popular interest measures used for rule mining. The results show that confidence is systematically influenced by the frequency of the items in the left hand side of rules and that lift performs poorly to filter random noise in transaction data. Based on the probabilistic framework we develop two new interest measures, hyper-lift and hyper-confidence, which can be used to filter or order mined association rules. The new measures show significantly better performance than lift for applications where spurious rules are problematic.",
        "published": "2008-03-06T20:17:19Z",
        "link": "http://arxiv.org/abs/0803.0966v1",
        "categories": [
            "cs.DB",
            "stat.ML"
        ]
    },
    {
        "title": "Privacy Preserving ID3 over Horizontally, Vertically and Grid   Partitioned Data",
        "authors": [
            "Bart Kuijpers",
            "Vanessa Lemmens",
            "Bart Moelans",
            "Karl Tuyls"
        ],
        "summary": "We consider privacy preserving decision tree induction via ID3 in the case where the training data is horizontally or vertically distributed. Furthermore, we consider the same problem in the case where the data is both horizontally and vertically distributed, a situation we refer to as grid partitioned data. We give an algorithm for privacy preserving ID3 over horizontally partitioned data involving more than two parties. For grid partitioned data, we discuss two different evaluation methods for preserving privacy ID3, namely, first merging horizontally and developing vertically or first merging vertically and next developing horizontally. Next to introducing privacy preserving data mining over grid-partitioned data, the main contribution of this paper is that we show, by means of a complexity analysis that the former evaluation method is the more efficient.",
        "published": "2008-03-11T11:18:52Z",
        "link": "http://arxiv.org/abs/0803.1555v1",
        "categories": [
            "cs.DB",
            "cs.LG",
            "E.1; E.3; H.2.8; H.3.3"
        ]
    },
    {
        "title": "Conditioning Probabilistic Databases",
        "authors": [
            "Christoph Koch",
            "Dan Olteanu"
        ],
        "summary": "Past research on probabilistic databases has studied the problem of answering queries on a static database. Application scenarios of probabilistic databases however often involve the conditioning of a database using additional information in the form of new evidence. The conditioning problem is thus to transform a probabilistic database of priors into a posterior probabilistic database which is materialized for subsequent query processing or further refinement. It turns out that the conditioning problem is closely related to the problem of computing exact tuple confidence values.   It is known that exact confidence computation is an NP-hard problem. This has led researchers to consider approximation techniques for confidence computation. However, neither conditioning nor exact confidence computation can be solved using such techniques.   In this paper we present efficient techniques for both problems. We study several problem decomposition methods and heuristics that are based on the most successful search techniques from constraint satisfaction, such as the Davis-Putnam algorithm. We complement this with a thorough experimental evaluation of the algorithms proposed. Our experiments show that our exact algorithms scale well to realistic database sizes and can in some scenarios compete with the most efficient previous approximation algorithms.",
        "published": "2008-03-14T17:23:34Z",
        "link": "http://arxiv.org/abs/0803.2212v2",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.1; H.2.4"
        ]
    },
    {
        "title": "Logical Queries over Views: Decidability and Expressiveness",
        "authors": [
            "James Bailey",
            "Guozhu Dong",
            "Anthony Widjaja To"
        ],
        "summary": "We study the problem of deciding satisfiability of first order logic queries over views, our aim being to delimit the boundary between the decidable and the undecidable fragments of this language. Views currently occupy a central place in database research, due to their role in applications such as information integration and data warehousing. Our main result is the identification of a decidable class of first order queries over unary conjunctive views that generalises the decidability of the classical class of first order sentences over unary relations, known as the Lowenheim class. We then demonstrate how various extensions of this class lead to undecidability and also provide some expressivity results. Besides its theoretical interest, our new decidable class is potentially interesting for use in applications such as deciding implication of complex dependencies, analysis of a restricted class of active database rules, and ontology reasoning.",
        "published": "2008-03-18T02:07:12Z",
        "link": "http://arxiv.org/abs/0803.2559v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "F.4.1; H.2.3"
        ]
    },
    {
        "title": "A Model-Based Frequency Constraint for Mining Associations from   Transaction Data",
        "authors": [
            "Michael Hahsler"
        ],
        "summary": "Mining frequent itemsets is a popular method for finding associated items in databases. For this method, support, the co-occurrence frequency of the items which form an association, is used as the primary indicator of the associations's significance. A single user-specified support threshold is used to decided if associations should be further investigated. Support has some known problems with rare items, favors shorter itemsets and sometimes produces misleading associations.   In this paper we develop a novel model-based frequency constraint as an alternative to a single, user-specified minimum support. The constraint utilizes knowledge of the process generating transaction data by applying a simple stochastic mixture model (the NB model) which allows for transaction data's typically highly skewed item frequency distribution. A user-specified precision threshold is used together with the model to find local frequency thresholds for groups of itemsets. Based on the constraint we develop the notion of NB-frequent itemsets and adapt a mining algorithm to find all NB-frequent itemsets in a database. In experiments with publicly available transaction databases we show that the new constraint provides improvements over a single minimum support threshold and that the precision threshold is more robust and easier to set and interpret by the user.",
        "published": "2008-03-21T20:39:53Z",
        "link": "http://arxiv.org/abs/0803.3224v1",
        "categories": [
            "cs.DB",
            "H.2.8"
        ]
    },
    {
        "title": "Some results on $\\mathbb{R}$-computable structures",
        "authors": [
            "Wesley Calvert",
            "John E. Porter"
        ],
        "summary": "This survey paper examines the effective model theory obtained with the BSS model of real number computation. It treats the following topics: computable ordinals, satisfaction of computable infinitary formulas, forcing as a construction technique, effective categoricity, effective topology, and relations with other models for the effective theory of uncountable structures.",
        "published": "2008-03-24T13:59:53Z",
        "link": "http://arxiv.org/abs/0803.3404v2",
        "categories": [
            "cs.DB",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "Succinct Data Structures for Retrieval and Approximate Membership",
        "authors": [
            "Martin Dietzfelbinger",
            "Rasmus Pagh"
        ],
        "summary": "The retrieval problem is the problem of associating data with keys in a set. Formally, the data structure must store a function f: U ->{0,1}^r that has specified values on the elements of a given set S, a subset of U, |S|=n, but may have any value on elements outside S. Minimal perfect hashing makes it possible to avoid storing the set S, but this induces a space overhead of Theta(n) bits in addition to the nr bits needed for function values. In this paper we show how to eliminate this overhead. Moreover, we show that for any k query time O(k) can be achieved using space that is within a factor 1+e^{-k} of optimal, asymptotically for large n. If we allow logarithmic evaluation time, the additive overhead can be reduced to O(log log n) bits whp. The time to construct the data structure is O(n), expected. A main technical ingredient is to utilize existing tight bounds on the probability of almost square random matrices with rows of low weight to have full row rank. In addition to direct constructions, we point out a close connection between retrieval structures and hash tables where keys are stored in an array and some kind of probing scheme is used. Further, we propose a general reduction that transfers the results on retrieval into analogous results on approximate membership, a problem traditionally addressed using Bloom filters. Again, we show how to eliminate the space overhead present in previously known methods, and get arbitrarily close to the lower bound. The evaluation procedures of our data structures are extremely simple (similar to a Bloom filter). For the results stated above we assume free access to fully random hash functions. However, we show how to justify this assumption using extra space o(n) to simulate full randomness on a RAM.",
        "published": "2008-03-26T10:53:49Z",
        "link": "http://arxiv.org/abs/0803.3693v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "On the `Semantics' of Differential Privacy: A Bayesian Formulation",
        "authors": [
            "Shiva Prasad Kasiviswanathan",
            "Adam Smith"
        ],
        "summary": "Differential privacy is a definition of \"privacy'\" for algorithms that analyze and publish information about statistical databases. It is often claimed that differential privacy provides guarantees against adversaries with arbitrary side information. In this paper, we provide a precise formulation of these guarantees in terms of the inferences drawn by a Bayesian adversary. We show that this formulation is satisfied by both \"vanilla\" differential privacy as well as a relaxation known as (epsilon,delta)-differential privacy. Our formulation follows the ideas originally due to Dwork and McSherry [Dwork 2006]. This paper is, to our knowledge, the first place such a formulation appears explicitly. The analysis of the relaxed definition is new to this paper, and provides some concrete guidance for setting parameters when using (epsilon,delta)-differential privacy.",
        "published": "2008-03-27T15:00:45Z",
        "link": "http://arxiv.org/abs/0803.3946v4",
        "categories": [
            "cs.CR",
            "cs.DB"
        ]
    },
    {
        "title": "Discovering More Accurate Frequent Web Usage Patterns",
        "authors": [
            "Murat Ali Bayir",
            "Ismail Hakki Toroslu",
            "Ahmet Cosar",
            "Guven Fidan"
        ],
        "summary": "Web usage mining is a type of web mining, which exploits data mining techniques to discover valuable information from navigation behavior of World Wide Web users. As in classical data mining, data preparation and pattern discovery are the main issues in web usage mining. The first phase of web usage mining is the data processing phase, which includes the session reconstruction operation from server logs. Session reconstruction success directly affects the quality of the frequent patterns discovered in the next phase. In reactive web usage mining techniques, the source data is web server logs and the topology of the web pages served by the web server domain. Other kinds of information collected during the interactive browsing of web site by user, such as cookies or web logs containing similar information, are not used. The next phase of web usage mining is discovering frequent user navigation patterns. In this phase, pattern discovery methods are applied on the reconstructed sessions obtained in the first phase in order to discover frequent user patterns. In this paper, we propose a frequent web usage pattern discovery method that can be applied after session reconstruction phase. In order to compare accuracy performance of session reconstruction phase and pattern discovery phase, we have used an agent simulator, which models behavior of web users and generates web user navigation as well as the log data kept by the web server.",
        "published": "2008-04-09T05:46:26Z",
        "link": "http://arxiv.org/abs/0804.1409v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "An Optimal Bloom Filter Replacement Based on Matrix Solving",
        "authors": [
            "Ely Porat"
        ],
        "summary": "We suggest a method for holding a dictionary data structure, which maps keys to values, in the spirit of Bloom Filters. The space requirements of the dictionary we suggest are much smaller than those of a hashtable. We allow storing n keys, each mapped to value which is a string of k bits. Our suggested method requires nk + o(n) bits space to store the dictionary, and O(n) time to produce the data structure, and allows answering a membership query in O(1) memory probes. The dictionary size does not depend on the size of the keys. However, reducing the space requirements of the data structure comes at a certain cost. Our dictionary has a small probability of a one sided error. When attempting to obtain the value for a key that is stored in the dictionary we always get the correct answer. However, when testing for membership of an element that is not stored in the dictionary, we may get an incorrect answer, and when requesting the value of such an element we may get a certain random value. Our method is based on solving equations in GF(2^k) and using several hash functions. Another significant advantage of our suggested method is that we do not require using sophisticated hash functions. We only require pairwise independent hash functions. We also suggest a data structure that requires only nk bits space, has O(n2) preprocessing time, and has a O(log n) query time. However, this data structures requires a uniform hash functions. In order replace a Bloom Filter of n elements with an error proability of 2^{-k}, we require nk + o(n) memory bits, O(1) query time, O(n) preprocessing time, and only pairwise independent hash function. Even the most advanced previously known Bloom Filter would require nk+O(n) space, and a uniform hash functions, so our method is significantly less space consuming especially when k is small.",
        "published": "2008-04-11T11:24:04Z",
        "link": "http://arxiv.org/abs/0804.1845v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "Optimization Approach for Detecting the Critical Data on a Database",
        "authors": [
            "Prashanth Alluvada"
        ],
        "summary": "Through purposeful introduction of malicious transactions (tracking transactions) into randomly select nodes of a (database) graph, soiled and clean segments are identified. Soiled and clean measures corresponding those segments are then computed. These measures are used to repose the problem of critical database elements detection as an optimization problem over the graph. This method is universally applicable over a large class of graphs (including directed, weighted, disconnected, cyclic) that occur in several contexts of databases. A generalization argument is presented which extends the critical data problem to abstract settings.",
        "published": "2008-04-20T03:23:38Z",
        "link": "http://arxiv.org/abs/0804.3171v2",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "An Affine-invariant Time-dependent Triangulation of Spatio-temporal Data",
        "authors": [
            "Sofie Haesevoets",
            "Bart Kuijpers"
        ],
        "summary": "In the geometric data model for spatio-temporal data, introduced by Chomicki and Revesz, spatio-temporal data are modelled as a finite collection of triangles that are transformed by time-dependent affinities of the plane. To facilitate querying and animation of spatio-temporal data, we present a normal form for data in the geometric data model. We propose an algorithm for constructing this normal form via a spatio-temporal triangulation of geometric data objects. This triangulation algorithm generates new geometric data objects that partition the given objects both in space and in time. A particular property of the proposed partition is that it is invariant under time-dependent affine transformations, and hence independent of the particular choice of coordinate system used to describe he spatio-temporal data in. We can show that our algorithm works correctly and has a polynomial time complexity (of reasonably low degree in the number of input triangles and the maximal degree of the polynomial functions that describe the transformation functions). We also discuss several possible applications of this spatio-temporal triangulation.",
        "published": "2008-04-30T06:02:56Z",
        "link": "http://arxiv.org/abs/0804.4740v1",
        "categories": [
            "cs.CG",
            "cs.DB",
            "H.2.8; I.3.5"
        ]
    },
    {
        "title": "Specification of an extensible and portable file format for electronic   structure and crystallographic data",
        "authors": [
            "X. Gonze",
            "C. -O. Almbladh",
            "A. Cucca",
            "D. Caliste",
            "C. Freysoldt",
            "M. A. L. Marques",
            "V. Olevano",
            "Y. Pouillon",
            "M. J. Verstraete"
        ],
        "summary": "In order to allow different software applications, in constant evolution, to interact and exchange data, flexible file formats are needed. A file format specification for different types of content has been elaborated to allow communication of data for the software developed within the European Network of Excellence \"NANOQUANTA\", focusing on first-principles calculations of materials and nanosystems. It might be used by other software as well, and is described here in detail. The format relies on the NetCDF binary input/output library, already used in many different scientific communities, that provides flexibility as well as portability accross languages and platforms. Thanks to NetCDF, the content can be accessed by keywords, ensuring the file format is extensible and backward compatible.",
        "published": "2008-05-02T08:48:26Z",
        "link": "http://arxiv.org/abs/0805.0192v1",
        "categories": [
            "cs.DL",
            "cond-mat.mtrl-sci",
            "cs.DB"
        ]
    },
    {
        "title": "Alternating Automata on Data Trees and XPath Satisfiability",
        "authors": [
            "Marcin Jurdzinski",
            "Ranko Lazic"
        ],
        "summary": "A data tree is an unranked ordered tree whose every node is labelled by a letter from a finite alphabet and an element (\"datum\") from an infinite set, where the latter can only be compared for equality. The article considers alternating automata on data trees that can move downward and rightward, and have one register for storing data. The main results are that nonemptiness over finite data trees is decidable but not primitive recursive, and that nonemptiness of safety automata is decidable but not elementary. The proofs use nondeterministic tree automata with faulty counters. Allowing upward moves, leftward moves, or two registers, each causes undecidability. As corollaries, decidability is obtained for two data-sensitive fragments of the XPath query language.",
        "published": "2008-05-03T00:12:15Z",
        "link": "http://arxiv.org/abs/0805.0330v4",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.FL",
            "F.4.1; F.1.1; H.2.3"
        ]
    },
    {
        "title": "Pruning Attribute Values From Data Cubes with Diamond Dicing",
        "authors": [
            "Hazel Webb",
            "Owen Kaser",
            "Daniel Lemire"
        ],
        "summary": "Data stored in a data warehouse are inherently multidimensional, but most data-pruning techniques (such as iceberg and top-k queries) are unidimensional. However, analysts need to issue multidimensional queries. For example, an analyst may need to select not just the most profitable stores or--separately--the most profitable products, but simultaneous sets of stores and products fulfilling some profitability constraints. To fill this need, we propose a new operator, the diamond dice. Because of the interaction between dimensions, the computation of diamonds is challenging. We present the first diamond-dicing experiments on large data sets. Experiments show that we can compute diamond cubes over fact tables containing 100 million facts in less than 35 minutes using a standard PC.",
        "published": "2008-05-06T15:45:15Z",
        "link": "http://arxiv.org/abs/0805.0747v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "A Time Efficient Indexing Scheme for Complex Spatiotemporal Retrieval",
        "authors": [
            "Lagogiannis George",
            "Lorentzos Nikos",
            "Sioutas Spyros",
            "Theodoridis Evaggelos"
        ],
        "summary": "The paper is concerned with the time efficient processing of spatiotemporal predicates, i.e. spatial predicates associated with an exact temporal constraint. A set of such predicates forms a buffer query or a Spatio-temporal Pattern (STP) Query with time. In the more general case of an STP query, the temporal dimension is introduced via the relative order of the spatial predicates (STP queries with order). Therefore, the efficient processing of a spatiotemporal predicate is crucial for the efficient implementation of more complex queries of practical interest. We propose an extension of a known approach, suitable for processing spatial predicates, which has been used for the efficient manipulation of STP queries with order. The extended method is supported by efficient indexing structures. We also provide experimental results that show the efficiency of the technique.",
        "published": "2008-05-10T17:18:32Z",
        "link": "http://arxiv.org/abs/0805.1487v1",
        "categories": [
            "cs.DB",
            "cs.DS",
            "H.2; E.1"
        ]
    },
    {
        "title": "On the Probability Distribution of Superimposed Random Codes",
        "authors": [
            "Bernd Günther"
        ],
        "summary": "A systematic study of the probability distribution of superimposed random codes is presented through the use of generating functions. Special attention is paid to the cases of either uniformly distributed but not necessarily independent or non uniform but independent bit structures. Recommendations for optimal coding strategies are derived.",
        "published": "2008-05-12T08:52:16Z",
        "link": "http://arxiv.org/abs/0805.1593v2",
        "categories": [
            "cs.DB",
            "cs.DM",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Finger Indexed Sets: New Approaches",
        "authors": [
            "Spyros Sioutas"
        ],
        "summary": "In the particular case we have insertions/deletions at the tail of a given set S of $n$ one-dimensional elements, we present a simpler and more concrete algorithm than that presented in [Anderson, 2007] achieving the same (but also amortized) upper bound of $O(\\sqrt{logd/loglogd})$ for finger searching queries, where $d$ is the number of sorted keys between the finger element and the target element we are looking for. Furthermore, in general case we have insertions/deletions anywhere we present a new randomized algorithm achieving the same expected time bounds. Even the new solutions achieve the optimal bounds in amortized or expected case, the advantage of simplicity is of great importance due to practical merits we gain.",
        "published": "2008-05-17T14:05:12Z",
        "link": "http://arxiv.org/abs/0805.2671v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "E.1; E.5"
        ]
    },
    {
        "title": "Tri de la table de faits et compression des index bitmaps avec   alignement sur les mots",
        "authors": [
            "Kamel Aouiche",
            "Daniel Lemire",
            "Owen Kaser"
        ],
        "summary": "Bitmap indexes are frequently used to index multidimensional data. They rely mostly on sequential input/output. Bitmaps can be compressed to reduce input/output costs and minimize CPU usage. The most efficient compression techniques are based on run-length encoding (RLE), such as Word-Aligned Hybrid (WAH) compression. This type of compression accelerates logical operations (AND, OR) over the bitmaps. However, run-length encoding is sensitive to the order of the facts. Thus, we propose to sort the fact tables. We review lexicographic, Gray-code, and block-wise sorting. We found that a lexicographic sort improves compression--sometimes generating indexes twice as small--and make indexes several times faster. While sorting takes time, this is partially offset by the fact that it is faster to index a sorted table. Column order is significant: it is generally preferable to put the columns having more distinct values at the beginning. A block-wise sort is much less efficient than a full sort. Moreover, we found that Gray-code sorting is not better than lexicographic sorting when using word-aligned compression.",
        "published": "2008-05-21T19:50:46Z",
        "link": "http://arxiv.org/abs/0805.3339v3",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Spiral Walk on Triangular Meshes : Adaptive Replication in Data P2P   Networks",
        "authors": [
            "Nicolas Bonnel",
            "Gilbas Ménier",
            "Pierre-François Marteau"
        ],
        "summary": "We introduce a decentralized replication strategy for peer-to-peer file exchange based on exhaustive exploration of the neighborhood of any node in the network. The replication scheme lets the replicas evenly populate the network mesh, while regulating the total number of replicas at the same time. This is achieved by self adaptation to entering or leaving of nodes. Exhaustive exploration is achieved by a spiral walk algorithm that generates a number of messages linearly proportional to the number of visited nodes. It requires a dedicated topology (a triangular mesh on a closed surface). We introduce protocols for node connection and departure that maintain the triangular mesh at low computational and bandwidth cost. Search efficiency is increased using a mechanism based on dynamically allocated super peers. We conclude with a discussion on experimental validation results.",
        "published": "2008-05-27T12:49:55Z",
        "link": "http://arxiv.org/abs/0805.4107v1",
        "categories": [
            "cs.NI",
            "cs.DB"
        ]
    },
    {
        "title": "Design and Implementation Aspects of a novel Java P2P Simulator with GUI",
        "authors": [
            "V. Chrissikopoulos",
            "G. Papaloukopoulos",
            "E. Sakkopoulos",
            "S. Sioutas"
        ],
        "summary": "Peer-to-peer networks consist of thousands or millions of nodes that might join and leave arbitrarily. The evaluation of new protocols in real environments is many times practically impossible, especially at design and testing stages. The purpose of this paper is to describe the implementation aspects of a new Java based P2P simulator that has been developed to support scalability in the evaluation of such P2P dynamic environments. Evolving the functionality presented by previous solutions, we provide a friendly graphical user interface through which the high-level theoretic researcher/designer of a P2P system can easily construct an overlay with the desirable number of nodes and evaluate its operations using a number of key distributions. Furthermore, the simulator has built-in ability to produce statistics about the distributed structure. Emphasis was given to the parametrical configuration of the simulator. As a result the developed tool can be utilized in the simulation and evaluation procedures of a variety of different protocols, with only few changes in the Java code.",
        "published": "2008-05-27T14:36:20Z",
        "link": "http://arxiv.org/abs/0805.4134v1",
        "categories": [
            "cs.NI",
            "cs.DB",
            "cs.DC",
            "D.2; H.2; E.1"
        ]
    },
    {
        "title": "An Experimental Investigation of XML Compression Tools",
        "authors": [
            "Sherif Sakr"
        ],
        "summary": "This paper presents an extensive experimental study of the state-of-the-art of XML compression tools. The study reports the behavior of nine XML compressors using a large corpus of XML documents which covers the different natures and scales of XML documents. In addition to assessing and comparing the performance characteristics of the evaluated XML compression tools, the study tries to assess the effectiveness and practicality of using these tools in the real world. Finally, we provide some guidelines and recommen- dations which are useful for helping developers and users for making an effective decision for selecting the most suitable XML compression tool for their needs.",
        "published": "2008-05-31T14:49:00Z",
        "link": "http://arxiv.org/abs/0806.0075v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Histograms and Wavelets on Probabilistic Data",
        "authors": [
            "Graham Cormode",
            "Minos Garofalakis"
        ],
        "summary": "There is a growing realization that uncertain information is a first-class citizen in modern database management. As such, we need techniques to correctly and efficiently process uncertain data in database systems. In particular, data reduction techniques that can produce concise, accurate synopses of large probabilistic relations are crucial. Similar to their deterministic relation counterparts, such compact probabilistic data synopses can form the foundation for human understanding and interactive data exploration, probabilistic query planning and optimization, and fast approximate query processing in probabilistic database systems.   In this paper, we introduce definitions and algorithms for building histogram- and wavelet-based synopses on probabilistic data. The core problem is to choose a set of histogram bucket boundaries or wavelet coefficients to optimize the accuracy of the approximate representation of a collection of probabilistic tuples under a given error metric. For a variety of different error metrics, we devise efficient algorithms that construct optimal or near optimal B-term histogram and wavelet synopses. This requires careful analysis of the structure of the probability distributions, and novel extensions of known dynamic-programming-based techniques for the deterministic domain. Our experiments show that this approach clearly outperforms simple ideas, such as building summaries for samples drawn from the data distribution, while taking equal or less time.",
        "published": "2008-06-05T23:19:56Z",
        "link": "http://arxiv.org/abs/0806.1071v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Cardinality heterogeneities in Web service composition: Issues and   solutions",
        "authors": [
            "M. Mrissa",
            "Ph. Thiran",
            "J-M. Jacquet",
            "D. Benslimane",
            "Z. Maamar"
        ],
        "summary": "Data exchanges between Web services engaged in a composition raise several heterogeneities. In this paper, we address the problem of data cardinality heterogeneity in a composition. Firstly, we build a theoretical framework to describe different aspects of Web services that relate to data cardinality, and secondly, we solve this problem by developing a solution for cardinality mediation based on constraint logic programming.",
        "published": "2008-06-11T09:05:21Z",
        "link": "http://arxiv.org/abs/0806.1816v1",
        "categories": [
            "cs.SE",
            "cs.DB"
        ]
    },
    {
        "title": "Using rational numbers to key nested sets",
        "authors": [
            "Dan Hazel"
        ],
        "summary": "This report details the generation and use of tree node ordering keys in a single relational database table. The keys for each node are calculated from the keys of its parent, in such a way that the sort order places every node in the tree before all of its descendants and after all siblings having a lower index. The calculation from parent keys to child keys is simple, and reversible in the sense that the keys of every ancestor of a node can be calculated from that node's keys without having to consult the database.   Proofs of the above properties of the key encoding process and of its correspondence to a finite continued fraction form are provided.",
        "published": "2008-06-19T02:06:14Z",
        "link": "http://arxiv.org/abs/0806.3115v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "How Is Meaning Grounded in Dictionary Definitions?",
        "authors": [
            "A. Blondin Masse",
            "G. Chicoisne",
            "Y. Gargouri",
            "S. Harnad",
            "O. Picard",
            "O. Marcotte"
        ],
        "summary": "Meaning cannot be based on dictionary definitions all the way down: at some point the circularity of definitions must be broken in some way, by grounding the meanings of certain words in sensorimotor categories learned from experience or shaped by evolution. This is the \"symbol grounding problem.\" We introduce the concept of a reachable set -- a larger vocabulary whose meanings can be learned from a smaller vocabulary through definition alone, as long as the meanings of the smaller vocabulary are themselves already grounded. We provide simple algorithms to compute reachable sets for any given dictionary.",
        "published": "2008-06-23T15:53:05Z",
        "link": "http://arxiv.org/abs/0806.3710v2",
        "categories": [
            "cs.CL",
            "cs.DB",
            "A.2; H.3.1; I.2.7; I.2.0; I.2.4; H.3.2; I.5.4"
        ]
    },
    {
        "title": "Challenging More Updates: Towards Anonymous Re-publication of Fully   Dynamic Datasets",
        "authors": [
            "Feng Li",
            "Shuigeng Zhou"
        ],
        "summary": "Most existing anonymization work has been done on static datasets, which have no update and need only one-time publication. Recent studies consider anonymizing dynamic datasets with external updates: the datasets are updated with record insertions and/or deletions. This paper addresses a new problem: anonymous re-publication of datasets with internal updates, where the attribute values of each record are dynamically updated. This is an important and challenging problem for attribute values of records are updating frequently in practice and existing methods are unable to deal with such a situation.   We initiate a formal study of anonymous re-publication of dynamic datasets with internal updates, and show the invalidation of existing methods. We introduce theoretical definition and analysis of dynamic datasets, and present a general privacy disclosure framework that is applicable to all anonymous re-publication problems. We propose a new counterfeited generalization principle alled m-Distinct to effectively anonymize datasets with both external updates and internal updates. We also develop an algorithm to generalize datasets to meet m-Distinct. The experiments conducted on real-world data demonstrate the effectiveness of the proposed solution.",
        "published": "2008-06-28T16:24:03Z",
        "link": "http://arxiv.org/abs/0806.4703v2",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Nested Ordered Sets and their Use for Data Modelling",
        "authors": [
            "Alexandr Savinov"
        ],
        "summary": "In this paper we present a new approach to data modelling, called the concept-oriented model (CoM), and describe its main features and characteristics including data semantics and operations. The distinguishing feature of this model is that it is based on the formalism of nested ordered sets where any element participates in two structures simultaneously: hierarchical (nested) and multi-dimensional (ordered). An element of the model is postulated to consist of two parts, called identity and entity, and the whole approach can be naturally broken into two branches: identity modelling and entity modelling. We also propose a new query language with the main construct, called concept, defined as a pair of two classes: identity class and entity class. We describe how its operations of projection, de-projection and product can be used to solve typical data modelling tasks.",
        "published": "2008-06-29T11:38:06Z",
        "link": "http://arxiv.org/abs/0806.4749v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Locality and Bounding-Box Quality of Two-Dimensional Space-Filling   Curves",
        "authors": [
            "Herman Haverkort",
            "Freek van Walderveen"
        ],
        "summary": "Space-filling curves can be used to organise points in the plane into bounding-box hierarchies (such as R-trees). We develop measures of the bounding-box quality of space-filling curves that express how effective different space-filling curves are for this purpose. We give general lower bounds on the bounding-box quality measures and on locality according to Gotsman and Lindenbaum for a large class of space-filling curves. We describe a generic algorithm to approximate these and similar quality measures for any given curve. Using our algorithm we find good approximations of the locality and the bounding-box quality of several known and new space-filling curves. Surprisingly, some curves with relatively bad locality by Gotsman and Lindenbaum's measure, have good bounding-box quality, while the curve with the best-known locality has relatively bad bounding-box quality.",
        "published": "2008-06-29T21:47:15Z",
        "link": "http://arxiv.org/abs/0806.4787v2",
        "categories": [
            "cs.CG",
            "cs.DB"
        ]
    },
    {
        "title": "Conception et Evaluation de XQuery dans une architecture de médiation   \"Tout-XML\"",
        "authors": [
            "Tuyet-Tram Dang-Ngoc",
            "Georges Gardarin"
        ],
        "summary": "XML has emerged as the leading language for representing and exchanging data not only on the Web, but also in general in the enterprise. XQuery is emerging as the standard query language for XML. Thus, tools are required to mediate between XML queries and heterogeneous data sources to integrate data in XML. This paper presents the XMedia mediator, a unique tool for integrating and querying disparate heterogeneous information as unified XML views. It describes the mediator architecture and focuses on the unique distributed query processing technology implemented in this component. Query evaluation is based on an original XML algebra simply extending classical operators to process tuples of tree elements. Further, we present a set of performance evaluation on a relational benchmark, which leads to discuss possible performance enhancements.",
        "published": "2008-06-30T15:23:20Z",
        "link": "http://arxiv.org/abs/0806.4920v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "SP2Bench: A SPARQL Performance Benchmark",
        "authors": [
            "Michael Schmidt",
            "Thomas Hornung",
            "Georg Lausen",
            "Christoph Pinkel"
        ],
        "summary": "Recently, the SPARQL query language for RDF has reached the W3C recommendation status. In response to this emerging standard, the database community is currently exploring efficient storage techniques for RDF data and evaluation strategies for SPARQL queries. A meaningful analysis and comparison of these approaches necessitates a comprehensive and universal benchmark platform. To this end, we have developed SP^2Bench, a publicly available, language-specific SPARQL performance benchmark. SP^2Bench is settled in the DBLP scenario and comprises both a data generator for creating arbitrarily large DBLP-like documents and a set of carefully designed benchmark queries. The generated documents mirror key characteristics and social-world distributions encountered in the original DBLP data set, while the queries implement meaningful requests on top of this data, covering a variety of SPARQL operator constellations and RDF access patterns. As a proof of concept, we apply SP^2Bench to existing engines and discuss their strengths and weaknesses that follow immediately from the benchmark results.",
        "published": "2008-06-30T15:31:26Z",
        "link": "http://arxiv.org/abs/0806.4627v2",
        "categories": [
            "cs.DB",
            "cs.PF"
        ]
    },
    {
        "title": "Flux: FunctionaL Updates for XML (extended report)",
        "authors": [
            "James Cheney"
        ],
        "summary": "XML database query languages have been studied extensively, but XML database updates have received relatively little attention, and pose many challenges to language design. We are developing an XML update language called Flux, which stands for FunctionaL Updates for XML, drawing upon ideas from functional programming languages. In prior work, we have introduced a core language for Flux with a clear operational semantics and a sound, decidable static type system based on regular expression types.   Our initial proposal had several limitations. First, it lacked support for recursive types or update procedures. Second, although a high-level source language can easily be translated to the core language, it is difficult to propagate meaningful type errors from the core language back to the source. Third, certain updates are well-formed yet contain path errors, or ``dead'' subexpressions which never do any useful work. It would be useful to detect path errors, since they often represent errors or optimization opportunities.   In this paper, we address all three limitations. Specifically, we present an improved, sound type system that handles recursion. We also formalize a source update language and give a translation to the core language that preserves and reflects typability. We also develop a path-error analysis (a form of dead-code analysis) for updates.",
        "published": "2008-07-08T17:10:51Z",
        "link": "http://arxiv.org/abs/0807.1211v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "D.3.1; H.2.3"
        ]
    },
    {
        "title": "Faster Sequential Search with a Two-Pass Dynamic-Time-Warping Lower   Bound",
        "authors": [
            "Daniel Lemire"
        ],
        "summary": "The Dynamic Time Warping (DTW) is a popular similarity measure between time series. The DTW fails to satisfy the triangle inequality and its computation requires quadratic time. Hence, to find closest neighbors quickly, we use bounding techniques. We can avoid most DTW computations with an inexpensive lower bound (LB_Keogh). We compare LB_Keogh with a tighter lower bound (LB_Improved). We find that LB_Improved-based search is faster for sequential search. As an example, our approach is 3 times faster over random-walk and shape time series. We also review some of the mathematical properties of the DTW. We derive a tight triangle inequality for the DTW. We show that the DTW becomes the l_1 distance when time series are separated by a constant.",
        "published": "2008-07-10T20:26:31Z",
        "link": "http://arxiv.org/abs/0807.1734v5",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Eigenfactor : Does the Principle of Repeated Improvement Result in   Better Journal Impact Estimates than Raw Citation Counts?",
        "authors": [
            "Philip M. Davis"
        ],
        "summary": "Eigenfactor.org, a journal evaluation tool which uses an iterative algorithm to weight citations (similar to the PageRank algorithm used for Google) has been proposed as a more valid method for calculating the impact of journals. The purpose of this brief communication is to investigate whether the principle of repeated improvement provides different rankings of journals than does a simple unweighted citation count (the method used by ISI).",
        "published": "2008-07-17T01:01:59Z",
        "link": "http://arxiv.org/abs/0807.2678v3",
        "categories": [
            "cs.DL",
            "cs.DB"
        ]
    },
    {
        "title": "DescribeX: A Framework for Exploring and Querying XML Web Collections",
        "authors": [
            "Flavio Rizzolo"
        ],
        "summary": "This thesis introduces DescribeX, a powerful framework that is capable of describing arbitrarily complex XML summaries of web collections, providing support for more efficient evaluation of XPath workloads. DescribeX permits the declarative description of document structure using all axes and language constructs in XPath, and generalizes many of the XML indexing and summarization approaches in the literature. DescribeX supports the construction of heterogeneous summaries where different document elements sharing a common structure can be declaratively defined and refined by means of path regular expressions on axes, or axis path regular expression (AxPREs). DescribeX can significantly help in the understanding of both the structure of complex, heterogeneous XML collections and the behaviour of XPath queries evaluated on them.   Experimental results demonstrate the scalability of DescribeX summary refinements and stabilizations (the key enablers for tailoring summaries) with multi-gigabyte web collections. A comparative study suggests that using a DescribeX summary created from a given workload can produce query evaluation times orders of magnitude better than using existing summaries. DescribeX's light-weight approach of combining summaries with a file-at-a-time XPath processor can be a very competitive alternative, in terms of performance, to conventional fully-fledged XML query engines that provide DB-like functionality such as security, transaction processing, and native storage.",
        "published": "2008-07-18T14:12:02Z",
        "link": "http://arxiv.org/abs/0807.2972v1",
        "categories": [
            "cs.DB",
            "H.2.5; H.2.8"
        ]
    },
    {
        "title": "Relational Lattice Axioms",
        "authors": [
            "Marshall Spight",
            "Vadim Tropashko"
        ],
        "summary": "Relational lattice is a formal mathematical model for Relational algebra. It reduces the set of six classic relational algebra operators to two: natural join and inner union. We continue to investigate Relational lattice properties with emphasis onto axiomatic definition. New results include additional axioms, equational definition for set difference (more generally anti-join), and case study demonstrating application of the relational lattice theory for query transformations.",
        "published": "2008-07-24T05:24:34Z",
        "link": "http://arxiv.org/abs/0807.3795v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "A Logical Model and Data Placement Strategies for MEMS Storage Devices",
        "authors": [
            "Yi-Reun Kim",
            "Kyu-Young Whang",
            "Min-Soo Kim",
            "Il-Yeol Song"
        ],
        "summary": "MEMS storage devices are new non-volatile secondary storages that have outstanding advantages over magnetic disks. MEMS storage devices, however, are much different from magnetic disks in the structure and access characteristics. They have thousands of heads called probe tips and provide the following two major access facilities: (1) flexibility: freely selecting a set of probe tips for accessing data, (2) parallelism: simultaneously reading and writing data with the set of probe tips selected. Due to these characteristics, it is nontrivial to find data placements that fully utilize the capability of MEMS storage devices. In this paper, we propose a simple logical model called the Region-Sector (RS) model that abstracts major characteristics affecting data retrieval performance, such as flexibility and parallelism, from the physical MEMS storage model. We also suggest heuristic data placement strategies based on the RS model and derive new data placements for relational data and two-dimensional spatial data by using those strategies. Experimental results show that the proposed data placements improve the data retrieval performance by up to 4.0 times for relational data and by up to 4.8 times for two-dimensional spatial data of approximately 320 Mbytes compared with those of existing data placements. Further, these improvements are expected to be more marked as the database size grows.",
        "published": "2008-07-29T06:18:34Z",
        "link": "http://arxiv.org/abs/0807.4580v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "A Compositional Query Algebra for Second-Order Logic and Uncertain   Databases",
        "authors": [
            "Christoph Koch"
        ],
        "summary": "World-set algebra is a variable-free query language for uncertain databases. It constitutes the core of the query language implemented in MayBMS, an uncertain database system. This paper shows that world-set algebra captures exactly second-order logic over finite structures, or equivalently, the polynomial hierarchy. The proofs also imply that world-set algebra is closed under composition, a previously open problem.",
        "published": "2008-07-29T11:22:01Z",
        "link": "http://arxiv.org/abs/0807.4620v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2.3"
        ]
    },
    {
        "title": "Building a terminology network for search: the KoMoHe project",
        "authors": [
            "Philipp Mayr",
            "Vivien Petras"
        ],
        "summary": "The paper reports about results on the GESIS-IZ project \"Competence Center Modeling and Treatment of Semantic Heterogeneity\" (KoMoHe). KoMoHe supervised a terminology mapping effort, in which 'cross-concordances' between major controlled vocabularies were organized, created and managed. In this paper we describe the establishment and implementation of cross-concordances for search in a digital library (DL).",
        "published": "2008-08-04T22:11:45Z",
        "link": "http://arxiv.org/abs/0808.0518v1",
        "categories": [
            "cs.DL",
            "cs.DB"
        ]
    },
    {
        "title": "Histogram-Aware Sorting for Enhanced Word-Aligned Compression in Bitmap   Indexes",
        "authors": [
            "Owen Kaser",
            "Daniel Lemire",
            "Kamel Aouiche"
        ],
        "summary": "Bitmap indexes must be compressed to reduce input/output costs and minimize CPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use techniques based on run-length encoding (RLE), such as Word-Aligned Hybrid (WAH) compression. These techniques are sensitive to the order of the rows: a simple lexicographical sort can divide the index size by 9 and make indexes several times faster. We investigate reordering heuristics based on computed attribute-value histograms. Simply permuting the columns of the table based on these histograms can increase the sorting efficiency by 40%.",
        "published": "2008-08-15T03:14:55Z",
        "link": "http://arxiv.org/abs/0808.2083v3",
        "categories": [
            "cs.DB",
            "H.3.2; E.1"
        ]
    },
    {
        "title": "Confirmation Bias and the Open Access Advantage: Some Methodological   Suggestions for the Davis Citation Study",
        "authors": [
            "Stevan Harnad"
        ],
        "summary": "Davis (2008) analyzes citations from 2004-2007 in 11 biomedical journals. 15% of authors paid to make them Open Access (OA). The outcome is a significant OA citation Advantage, but a small one (21%). The author infers that the OA advantage has been shrinking yearly, but the data suggest the opposite. Further analyses are necessary:   (1) Not just author-choice (paid) OA but Free OA self-archiving needs to be taken into account rather than being counted as non-OA.   (2) proportion of OA articles per journal per year needs to be reported and taken into account.   (3) The Journal Impact Factor and the relation between the size of the OA Advantage article 'citation-bracket' need to be taken into account.   (4) The sample-size for the highest-impact, largest-sample journal analyzed, PNAS, is restricted and excluded from some of the analyses. The full PNAS dataset is needed.   (5) The interaction between OA and time, 2004-2007, is based on retrospective data from a June 2008 total cumulative citation count. The dates of both the cited articles and the citing articles need to be taken into account.   The author proposes that author self-selection bias for is the primary cause of the observed OA Advantage, but this study does not test this or of any of the other potential causal factors. The author suggests that paid OA is not worth the cost, per extra citation. But with OA self-archiving both the OA and the extra citations are free.",
        "published": "2008-08-25T03:36:14Z",
        "link": "http://arxiv.org/abs/0808.3296v2",
        "categories": [
            "cs.DL",
            "cs.DB"
        ]
    },
    {
        "title": "Conditional probability based significance tests for sequential patterns   in multi-neuronal spike trains",
        "authors": [
            "P. S. Sastry",
            "K. P. Unnikrishnan"
        ],
        "summary": "In this paper we consider the problem of detecting statistically significant sequential patterns in multi-neuronal spike trains. These patterns are characterized by ordered sequences of spikes from different neurons with specific delays between spikes. We have previously proposed a data mining scheme to efficiently discover such patterns which are frequent in the sense that the count of non-overlapping occurrences of the pattern in the data stream is above a threshold. Here we propose a method to determine the statistical significance of these repeating patterns and to set the thresholds automatically. The novelty of our approach is that we use a compound null hypothesis that includes not only models of independent neurons but also models where neurons have weak dependencies. The strength of interaction among the neurons is represented in terms of certain pair-wise conditional probabilities. We specify our null hypothesis by putting an upper bound on all such conditional probabilities. We construct a probabilistic model that captures the counting process and use this to calculate the mean and variance of the count for any pattern. Using this we derive a test of significance for rejecting such a null hypothesis. This also allows us to rank-order different significant patterns. We illustrate the effectiveness of our approach using spike trains generated from a non-homogeneous Poisson model with embedded dependencies.",
        "published": "2008-08-26T13:28:43Z",
        "link": "http://arxiv.org/abs/0808.3511v2",
        "categories": [
            "q-bio.NC",
            "cond-mat.dis-nn",
            "cs.DB",
            "q-bio.QM",
            "stat.ME"
        ]
    },
    {
        "title": "Toward Expressive and Scalable Sponsored Search Auctions",
        "authors": [
            "David J. Martin",
            "Johannes Gehrke",
            "Joseph Y. Halpern"
        ],
        "summary": "Internet search results are a growing and highly profitable advertising platform. Search providers auction advertising slots to advertisers on their search result pages. Due to the high volume of searches and the users' low tolerance for search result latency, it is imperative to resolve these auctions fast. Current approaches restrict the expressiveness of bids in order to achieve fast winner determination, which is the problem of allocating slots to advertisers so as to maximize the expected revenue given the advertisers' bids. The goal of our work is to permit more expressive bidding, thus allowing advertisers to achieve complex advertising goals, while still providing fast and scalable techniques for winner determination.",
        "published": "2008-08-31T12:24:38Z",
        "link": "http://arxiv.org/abs/0809.0116v1",
        "categories": [
            "cs.DB",
            "K.4.4"
        ]
    },
    {
        "title": "Consistent Query Answers in the Presence of Universal Constraints",
        "authors": [
            "Slawomir Staworko",
            "Jan Chomicki"
        ],
        "summary": "The framework of consistent query answers and repairs has been introduced to alleviate the impact of inconsistent data on the answers to a query. A repair is a minimally different consistent instance and an answer is consistent if it is present in every repair. In this article we study the complexity of consistent query answers and repair checking in the presence of universal constraints.   We propose an extended version of the conflict hypergraph which allows to capture all repairs w.r.t. a set of universal constraints. We show that repair checking is in PTIME for the class of full tuple-generating dependencies and denial constraints, and we present a polynomial repair algorithm. This algorithm is sound, i.e. always produces a repair, but also complete, i.e. every repair can be constructed. Next, we present a polynomial-time algorithm computing consistent answers to ground quantifier-free queries in the presence of denial constraints, join dependencies, and acyclic full-tuple generating dependencies. Finally, we show that extending the class of constraints leads to intractability. For arbitrary full tuple-generating dependencies consistent query answering becomes coNP-complete. For arbitrary universal constraints consistent query answering is \\Pi_2^p-complete and repair checking coNP-complete.",
        "published": "2008-09-09T15:04:52Z",
        "link": "http://arxiv.org/abs/0809.1551v2",
        "categories": [
            "cs.DB",
            "H.2.8"
        ]
    },
    {
        "title": "Materialized View Selection by Query Clustering in XML Data Warehouses",
        "authors": [
            "Hadj Mahboubi",
            "Kamel Aouiche",
            "Jérôme Darmont"
        ],
        "summary": "XML data warehouses form an interesting basis for decision-support applications that exploit complex data. However, native XML database management systems currently bear limited performances and it is necessary to design strategies to optimize them. In this paper, we propose an automatic strategy for the selection of XML materialized views that exploits a data mining technique, more precisely the clustering of the query workload. To validate our strategy, we implemented an XML warehouse modeled along the XCube specifications. We executed a workload of XQuery decision-support queries on this warehouse, with and without using our strategy. Our experimental results demonstrate its efficiency, even when queries are complex.",
        "published": "2008-09-11T11:59:00Z",
        "link": "http://arxiv.org/abs/0809.1963v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Dynamic index selection in data warehouses",
        "authors": [
            "Stéphane Azefack",
            "Kamel Aouiche",
            "Jérôme Darmont"
        ],
        "summary": "Analytical queries defined on data warehouses are complex and use several join operations that are very costly, especially when run on very large data volumes. To improve response times, data warehouse administrators casually use indexing techniques. This task is nevertheless complex and fastidious. In this paper, we present an automatic, dynamic index selection method for data warehouses that is based on incremental frequent itemset mining from a given query workload. The main advantage of this approach is that it helps update the set of selected indexes when workload evolves instead of recreating it from scratch. Preliminary experimental results illustrate the efficiency of this approach, both in terms of performance enhancement and overhead.",
        "published": "2008-09-11T12:01:34Z",
        "link": "http://arxiv.org/abs/0809.1965v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Knowledge and Metadata Integration for Warehousing Complex Data",
        "authors": [
            "Jean-Christian Ralaivao",
            "Jérôme Darmont"
        ],
        "summary": "With the ever-growing availability of so-called complex data, especially on the Web, decision-support systems such as data warehouses must store and process data that are not only numerical or symbolic. Warehousing and analyzing such data requires the joint exploitation of metadata and domain-related knowledge, which must thereby be integrated. In this paper, we survey the types of knowledge and metadata that are needed for managing complex data, discuss the issue of knowledge and metadata integration, and propose a CWM-compliant integration solution that we incorporate into an XML complex data warehousing framework we previously designed.",
        "published": "2008-09-11T12:20:00Z",
        "link": "http://arxiv.org/abs/0809.1971v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "A Join Index for XML Data Warehouses",
        "authors": [
            "Hadj Mahboubi",
            "Kamel Aouiche",
            "Jérôme Darmont"
        ],
        "summary": "XML data warehouses form an interesting basis for decision-support applications that exploit complex data. However, native-XML database management systems (DBMSs) currently bear limited performances and it is necessary to research for ways to optimize them. In this paper, we propose a new join index that is specifically adapted to the multidimensional architecture of XML warehouses. It eliminates join operations while preserving the information contained in the original warehouse. A theoretical study and experimental results demonstrate the efficiency of our join index. They also show that native XML DBMSs can compete with XML-compatible, relational DBMSs when warehousing and analyzing XML data.",
        "published": "2008-09-11T12:44:10Z",
        "link": "http://arxiv.org/abs/0809.1981v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Multidimensional Visualization of Oracle Performance Using Barry007",
        "authors": [
            "Tanel Poder",
            "Neil J. Gunther"
        ],
        "summary": "Most generic performance tools display only system-level performance data using 2-dimensional plots or diagrams and this limits the informational detail that can be displayed. Moreover, a modern relational database system, like Oracle, can concurrently serve thousands of client processes with different workload characteristics, so that generic performance-data displays inevitably hide important information. Drawing on our previous work, this paper demonstrates the application of Barry007 multidimensional visualization to the analysis of Oracle end-user, session-level, performance data, showing both collective trends and individual performance anomalies.",
        "published": "2008-09-15T14:11:34Z",
        "link": "http://arxiv.org/abs/0809.2532v1",
        "categories": [
            "cs.PF",
            "cs.DB",
            "B.8; C.4; D.4.8; H.2.4; H.2.7; H.3.4; H.5.1; K.8.1"
        ]
    },
    {
        "title": "An MAS-Based ETL Approach for Complex Data",
        "authors": [
            "Omar Boussaïd",
            "Fadila Bentayeb",
            "Jérôme Darmont"
        ],
        "summary": "In a data warehousing process, the phase of data integration is crucial. Many methods for data integration have been published in the literature. However, with the development of the Internet, the availability of various types of data (images, texts, sounds, videos, databases...) has increased, and structuring such data is a difficult task. We name these data, which may be structured or unstructured, \"complex data\". In this paper, we propose a new approach for complex data integration, based on a Multi-Agent System (MAS), in association to a data warehousing approach. Our objective is to take advantage of the MAS to perform the integration phase for complex data. We indeed consider the different tasks of the data integration process as services offered by agents. To validate this approach, we have actually developed an MAS for complex data integration.",
        "published": "2008-09-16T11:42:42Z",
        "link": "http://arxiv.org/abs/0809.2686v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Frequent itemsets mining for database auto-administration",
        "authors": [
            "Kamel Aouiche",
            "Jérôme Darmont",
            "Le Gruenwald"
        ],
        "summary": "With the wide development of databases in general and data warehouses in particular, it is important to reduce the tasks that a database administrator must perform manually. The aim of auto-administrative systems is to administrate and adapt themselves automatically without loss (or even with a gain) in performance. The idea of using data mining techniques to extract useful knowledge for administration from the data themselves has existed for some years. However, little research has been achieved. This idea nevertheless remains a very promising approach, notably in the field of data warehousing, where queries are very heterogeneous and cannot be interpreted easily. The aim of this study is to search for a way of extracting useful knowledge from stored data themselves to automatically apply performance optimization techniques, and more particularly indexing techniques. We have designed a tool that extracts frequent itemsets from a given workload to compute an index configuration that helps optimizing data access time. The experiments we performed showed that the index configurations generated by our tool allowed performance gains of 15% to 25% on a test database and a test data warehouse.",
        "published": "2008-09-16T11:44:39Z",
        "link": "http://arxiv.org/abs/0809.2687v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "A Complex Data Warehouse for Personalized, Anticipative Medicine",
        "authors": [
            "Jérôme Darmont",
            "Emerson Olivier"
        ],
        "summary": "With the growing use of new technologies, healthcare is nowadays undergoing significant changes. Information-based medicine has to exploit medical decision-support systems and requires the analysis of various, heterogeneous data, such as patient records, medical images, biological analysis results, etc. In this paper, we present the design of the complex data warehouse relating to high-level athletes. It is original in two ways. First, it is aimed at storing complex medical data. Second, it is designed to allow innovative and quite different kinds of analyses to support: (1) personalized and anticipative medicine (in opposition to curative medicine) for well-identified patients; (2) broad-band statistical studies over a given population of patients. Furthermore, the system includes data relating to several medical fields. It is also designed to be evolutionary to take into account future advances in medical research.",
        "published": "2008-09-16T11:47:58Z",
        "link": "http://arxiv.org/abs/0809.2688v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Expressing OLAP operators with the TAX XML algebra",
        "authors": [
            "Marouane Hachicha",
            "Hadj Mahboubi",
            "Jérôme Darmont"
        ],
        "summary": "With the rise of XML as a standard for representing business data, XML data warehouses appear as suitable solutions for Web-based decision-support applications. In this context, it is necessary to allow OLAP analyses over XML data cubes (XOLAP). Thus, XQuery extensions are needed. To help define a formal framework and allow much-needed performance optimizations on analytical queries expressed in XQuery, having an algebra at one's disposal is desirable. However, XOLAP approaches and algebras from the literature still largely rely on the relational model and/or only feature a small number of OLAP operators. In opposition, we propose in this paper to express a broad set of OLAP operators with the TAX XML algebra.",
        "published": "2008-09-16T12:12:15Z",
        "link": "http://arxiv.org/abs/0809.2691v1",
        "categories": [
            "cs.DB",
            "H.2"
        ]
    },
    {
        "title": "Finding links and initiators: a graph reconstruction problem",
        "authors": [
            "Heikki Mannila",
            "Evimaria Terzi"
        ],
        "summary": "Consider a 0-1 observation matrix M, where rows correspond to entities and columns correspond to signals; a value of 1 (or 0) in cell (i,j) of M indicates that signal j has been observed (or not observed) in entity i. Given such a matrix we study the problem of inferring the underlying directed links between entities (rows) and finding which entries in the matrix are initiators.   We formally define this problem and propose an MCMC framework for estimating the links and the initiators given the matrix of observations M. We also show how this framework can be extended to incorporate a temporal aspect; instead of considering a single observation matrix M we consider a sequence of observation matrices M1,..., Mt over time.   We show the connection between our problem and several problems studied in the field of social-network analysis. We apply our method to paleontological and ecological data and show that our algorithms work well in practice and give reasonable results.",
        "published": "2008-09-17T22:28:29Z",
        "link": "http://arxiv.org/abs/0809.3027v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "physics.soc-ph",
            "H.2.8"
        ]
    },
    {
        "title": "Monadic Datalog over Finite Structures with Bounded Treewidth",
        "authors": [
            "Georg Gottlob",
            "Reinhard Pichler",
            "Fang Wei"
        ],
        "summary": "Bounded treewidth and Monadic Second Order (MSO) logic have proved to be key concepts in establishing fixed-parameter tractability results. Indeed, by Courcelle's Theorem we know: Any property of finite structures, which is expressible by an MSO sentence, can be decided in linear time (data complexity) if the structures have bounded treewidth.   In principle, Courcelle's Theorem can be applied directly to construct concrete algorithms by transforming the MSO evaluation problem into a tree language recognition problem. The latter can then be solved via a finite tree automaton (FTA). However, this approach has turned out to be problematical, since even relatively simple MSO formulae may lead to a ``state explosion'' of the FTA.   In this work we propose monadic datalog (i.e., datalog where all intentional predicate symbols are unary) as an alternative method to tackle this class of fixed-parameter tractable problems. We show that if some property of finite structures is expressible in MSO then this property can also be expressed by means of a monadic datalog program over the structure plus the tree decomposition.   Moreover, we show that the resulting fragment of datalog can be evaluated in linear time (both w.r.t. the program size and w.r.t. the data size). This new approach is put to work by devising new algorithms for the 3-Colorability problem of graphs and for the PRIMALITY problem of relational schemas (i.e., testing if some attribute in a relational schema is part of a key). We also report on experimental results with a prototype implementation.",
        "published": "2008-09-18T12:40:49Z",
        "link": "http://arxiv.org/abs/0809.3140v1",
        "categories": [
            "cs.DB",
            "cs.CC",
            "cs.LO",
            "F.2.2; F.4.1"
        ]
    },
    {
        "title": "A global physician-oriented medical information system",
        "authors": [
            "Axel Boldt",
            "Michael Janich"
        ],
        "summary": "We propose to improve medical decision making and reduce global health care costs by employing a free Internet-based medical information system with two main target groups: practicing physicians and medical researchers. After acquiring patients' consent, physicians enter medical histories, physiological data and symptoms or disorders into the system; an integrated expert system can then assist in diagnosis and statistical software provides a list of the most promising treatment options and medications, tailored to the patient. Physicians later enter information about the outcomes of the chosen treatments, data the system uses to optimize future treatment recommendations. Medical researchers can analyze the aggregate data to compare various drugs or treatments in defined patient populations on a large scale.",
        "published": "2008-10-11T02:01:45Z",
        "link": "http://arxiv.org/abs/0810.1991v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "Dynamic Approaches to In-Network Aggregation",
        "authors": [
            "Oliver Kennedy",
            "Christoph Koch",
            "Al Demers"
        ],
        "summary": "Collaboration between small-scale wireless devices hinges on their ability to infer properties shared across multiple nearby nodes. Wireless-enabled mobile devices in particular create a highly dynamic environment not conducive to distributed reasoning about such global properties. This paper addresses a specific instance of this problem: distributed aggregation. We present extensions to existing unstructured aggregation protocols that enable estimation of count, sum, and average aggregates in highly dynamic environments. With the modified protocols, devices with only limited connectivity can maintain estimates of the aggregate, despite \\textit{unexpected} peer departures and arrivals. Our analysis of these aggregate maintenance extensions demonstrates their effectiveness in unstructured environments despite high levels of node mobility.",
        "published": "2008-10-17T19:48:38Z",
        "link": "http://arxiv.org/abs/0810.3227v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Logics for XML",
        "authors": [
            "Pierre Geneves"
        ],
        "summary": "This thesis describes the theoretical and practical foundations of a system for the static analysis of XML processing languages. The system relies on a fixpoint temporal logic with converse, derived from the mu-calculus, where models are finite trees. This calculus is expressive enough to capture regular tree types along with multi-directional navigation in trees, while having a single exponential time complexity. Specifically the decidability of the logic is proved in time 2^O(n) where n is the size of the input formula.   Major XML concepts are linearly translated into the logic: XPath navigation and node selection semantics, and regular tree languages (which include DTDs and XML Schemas). Based on these embeddings, several problems of major importance in XML applications are reduced to satisfiability of the logic. These problems include XPath containment, emptiness, equivalence, overlap, coverage, in the presence or absence of regular tree type constraints, and the static type-checking of an annotated query.   The focus is then given to a sound and complete algorithm for deciding the logic, along with a detailed complexity analysis, and crucial implementation techniques for building an effective solver. Practical experiments using a full implementation of the system are presented. The system appears to be efficient in practice for several realistic scenarios.   The main application of this work is a new class of static analyzers for programming languages using both XPath expressions and XML type annotations (input and output). Such analyzers allow to ensure at compile-time valuable properties such as type-safety and optimizations, for safer and more efficient XML processing.",
        "published": "2008-10-24T13:40:11Z",
        "link": "http://arxiv.org/abs/0810.4460v2",
        "categories": [
            "cs.PL",
            "cs.DB",
            "cs.LO",
            "D.3.0; D.3.1; D.3.4; E.1; F.3.1; F.3.2; F.4.1; F.4.3; H.2.3; I.2.4;\n  I.7.2"
        ]
    },
    {
        "title": "XQuery Join Graph Isolation",
        "authors": [
            "T. Grust",
            "M. Mayr",
            "J. Rittinger"
        ],
        "summary": "A purely relational account of the true XQuery semantics can turn any relational database system into an XQuery processor. Compiling nested expressions of the fully compositional XQuery language, however, yields odd algebraic plan shapes featuring scattered distributions of join operators that currently overwhelm commercial SQL query optimizers.   This work rewrites such plans before submission to the relational database back-end. Once cast into the shape of join graphs, we have found off-the-shelf relational query optimizers--the B-tree indexing subsystem and join tree planner, in particular--to cope and even be autonomously capable of \"reinventing\" advanced processing strategies that have originally been devised specifically for the XQuery domain, e.g., XPath step reordering, axis reversal, and path stitching. Performance assessments provide evidence that relational query engines are among the most versatile and efficient XQuery processors readily available today.",
        "published": "2008-10-27T13:41:48Z",
        "link": "http://arxiv.org/abs/0810.4809v1",
        "categories": [
            "cs.DB",
            "H.2.3; H.2.4"
        ]
    },
    {
        "title": "Anonymizing Graphs",
        "authors": [
            "Tomas Feder",
            "Shubha U. Nabar",
            "Evimaria Terzi"
        ],
        "summary": "Motivated by recently discovered privacy attacks on social networks, we study the problem of anonymizing the underlying graph of interactions in a social network. We call a graph (k,l)-anonymous if for every node in the graph there exist at least k other nodes that share at least l of its neighbors. We consider two combinatorial problems arising from this notion of anonymity in graphs. More specifically, given an input graph we ask for the minimum number of edges to be added so that the graph becomes (k,l)-anonymous. We define two variants of this minimization problem and study their properties. We show that for certain values of k and l the problems are polynomial-time solvable, while for others they become NP-hard. Approximation algorithms for the latter cases are also given.",
        "published": "2008-10-30T21:12:25Z",
        "link": "http://arxiv.org/abs/0810.5578v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "A triangle-based logic for affine-invariant querying of spatial and   spatio-temporal data",
        "authors": [
            "Sofie Haesevoets",
            "Bart Kuijpers"
        ],
        "summary": "In spatial databases, incompatibilities often arise due to different choices of origin or unit of measurement (e.g., centimeters versus inches). By representing and querying the data in an affine-invariant manner, we can avoid these incompatibilities.   In practice, spatial (resp., spatio-temporal) data is often represented as a finite union of triangles (resp., moving triangles). As two arbitrary triangles are equal up to a unique affinity of the plane, they seem perfect candidates as basic units for an affine-invariant query language.   We propose a so-called \"triangle logic\", a query language that is affine-generic and has triangles as basic elements. We show that this language has the same expressive power as the affine-generic fragment of first-order logic over the reals on triangle databases. We illustrate that the proposed language is simple and intuitive. It can also serve as a first step towards a \"moving-triangle logic\" for spatio-temporal data.",
        "published": "2008-10-31T16:16:30Z",
        "link": "http://arxiv.org/abs/0810.5725v2",
        "categories": [
            "cs.LO",
            "cs.DB",
            "H.2.3; H.2.8; F.4.0"
        ]
    },
    {
        "title": "Anonymizing Unstructured Data",
        "authors": [
            "Rajeev Motwani",
            "Shubha U. Nabar"
        ],
        "summary": "In this paper we consider the problem of anonymizing datasets in which each individual is associated with a set of items that constitute private information about the individual. Illustrative datasets include market-basket datasets and search engine query logs. We formalize the notion of k-anonymity for set-valued data as a variant of the k-anonymity model for traditional relational datasets. We define an optimization problem that arises from this definition of anonymity and provide O(klogk) and O(1)-approximation algorithms for the same. We demonstrate applicability of our algorithms to the America Online query log dataset.",
        "published": "2008-10-31T19:25:02Z",
        "link": "http://arxiv.org/abs/0810.5582v2",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Data Mining-based Fragmentation of XML Data Warehouses",
        "authors": [
            "Hadj Mahboubi",
            "Jérôme Darmont"
        ],
        "summary": "With the multiplication of XML data sources, many XML data warehouse models have been proposed to handle data heterogeneity and complexity in a way relational data warehouses fail to achieve. However, XML-native database systems currently suffer from limited performances, both in terms of manageable data volume and response time. Fragmentation helps address both these issues. Derived horizontal fragmentation is typically used in relational data warehouses and can definitely be adapted to the XML context. However, the number of fragments produced by classical algorithms is difficult to control. In this paper, we propose the use of a k-means-based fragmentation approach that allows to master the number of fragments through its $k$ parameter. We experimentally compare its efficiency to classical derived horizontal fragmentation algorithms adapted to XML data warehouses and show its superiority.",
        "published": "2008-11-05T15:00:32Z",
        "link": "http://arxiv.org/abs/0811.0741v1",
        "categories": [
            "cs.DB",
            "H.2"
        ]
    },
    {
        "title": "A role-free approach to indexing large RDF data sets in secondary memory   for efficient SPARQL evaluation",
        "authors": [
            "George H. L. Fletcher",
            "Peter W. Beck"
        ],
        "summary": "Massive RDF data sets are becoming commonplace. RDF data is typically generated in social semantic domains (such as personal information management) wherein a fixed schema is often not available a priori. We propose a simple Three-way Triple Tree (TripleT) secondary-memory indexing technique to facilitate efficient SPARQL query evaluation on such data sets. The novelty of TripleT is that (1) the index is built over the atoms occurring in the data set, rather than at a coarser granularity, such as whole triples occurring in the data set; and (2) the atoms are indexed regardless of the roles (i.e., subjects, predicates, or objects) they play in the triples of the data set. We show through extensive empirical evaluation that TripleT exhibits multiple orders of magnitude improvement over the state of the art on RDF indexing, in terms of both storage and query processing costs.",
        "published": "2008-11-07T05:08:41Z",
        "link": "http://arxiv.org/abs/0811.1083v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Disjunctive Databases for Representing Repairs",
        "authors": [
            "Cristian Molinaro",
            "Jan Chomicki",
            "Jerzy Marcinkowski"
        ],
        "summary": "This paper addresses the problem of representing the set of repairs of a possibly inconsistent database by means of a disjunctive database. Specifically, the class of denial constraints is considered. We show that, given a database and a set of denial constraints, there exists a (unique) disjunctive database, called canonical, which represents the repairs of the database w.r.t. the constraints and is contained in any other disjunctive database with the same set of minimal models. We propose an algorithm for computing the canonical disjunctive database. Finally, we study the size of the canonical disjunctive database in the presence of functional dependencies for both repairs and cardinality-based repairs.",
        "published": "2008-11-13T14:12:57Z",
        "link": "http://arxiv.org/abs/0811.2117v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Semantics and Evaluation of Top-k Queries in Probabilistic Databases",
        "authors": [
            "Xi Zhang",
            "Jan Chomicki"
        ],
        "summary": "We study here fundamental issues involved in top-k query evaluation in probabilistic databases. We consider simple probabilistic databases in which probabilities are associated with individual tuples, and general probabilistic databases in which, additionally, exclusivity relationships between tuples can be represented. In contrast to other recent research in this area, we do not limit ourselves to injective scoring functions. We formulate three intuitive postulates that the semantics of top-k queries in probabilistic databases should satisfy, and introduce a new semantics, Global-Topk, that satisfies those postulates to a large degree. We also show how to evaluate queries under the Global-Topk semantics. For simple databases we design dynamic-programming based algorithms, and for general databases we show polynomial-time reductions to the simple cases. For example, we demonstrate that for a fixed k the time complexity of top-k query evaluation is as low as linear, under the assumption that probabilistic databases are simple and scoring functions are injective.",
        "published": "2008-11-14T01:47:14Z",
        "link": "http://arxiv.org/abs/0811.2250v2",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Universally Utility-Maximizing Privacy Mechanisms",
        "authors": [
            "Arpita Ghosh",
            "Tim Roughgarden",
            "Mukund Sundararajan"
        ],
        "summary": "A mechanism for releasing information about a statistical database with sensitive data must resolve a trade-off between utility and privacy. Privacy can be rigorously quantified using the framework of {\\em differential privacy}, which requires that a mechanism's output distribution is nearly the same whether or not a given database row is included or excluded. The goal of this paper is strong and general utility guarantees, subject to differential privacy.   We pursue mechanisms that guarantee near-optimal utility to every potential user, independent of its side information (modeled as a prior distribution over query results) and preferences (modeled via a loss function).   Our main result is: for each fixed count query and differential privacy level, there is a {\\em geometric mechanism} $M^*$ -- a discrete variant of the simple and well-studied Laplace mechanism -- that is {\\em simultaneously expected loss-minimizing} for every possible user, subject to the differential privacy constraint. This is an extremely strong utility guarantee: {\\em every} potential user $u$, no matter what its side information and preferences, derives as much utility from $M^*$ as from interacting with a differentially private mechanism $M_u$ that is optimally tailored to $u$.",
        "published": "2008-11-18T05:59:39Z",
        "link": "http://arxiv.org/abs/0811.2841v3",
        "categories": [
            "cs.DB",
            "cs.GT"
        ]
    },
    {
        "title": "Secondary Indexing in One Dimension: Beyond B-trees and Bitmap Indexes",
        "authors": [
            "Rasmus Pagh",
            "S. Srinivasa Rao"
        ],
        "summary": "Let S be a finite, ordered alphabet, and let x = x_1 x_2 ... x_n be a string over S. A \"secondary index\" for x answers alphabet range queries of the form: Given a range [a_l,a_r] over S, return the set I_{[a_l;a_r]} = {i |x_i \\in [a_l; a_r]}. Secondary indexes are heavily used in relational databases and scientific data analysis. It is well-known that the obvious solution, storing a dictionary for the position set associated with each character, does not always give optimal query time. In this paper we give the first theoretically optimal data structure for the secondary indexing problem. In the I/O model, the amount of data read when answering a query is within a constant factor of the minimum space needed to represent I_{[a_l;a_r]}, assuming that the size of internal memory is (|S| log n)^{delta} blocks, for some constant delta > 0. The space usage of the data structure is O(n log |S|) bits in the worst case, and we further show how to bound the size of the data structure in terms of the 0-th order entropy of x. We show how to support updates achieving various time-space trade-offs.   We also consider an approximate version of the basic secondary indexing problem where a query reports a superset of I_{[a_l;a_r]} containing each element not in I_{[a_l;a_r]} with probability at most epsilon, where epsilon > 0 is the false positive probability. For this problem the amount of data that needs to be read by the query algorithm is reduced to O(|I_{[a_l;a_r]}| log(1/epsilon)) bits.",
        "published": "2008-11-18T13:31:05Z",
        "link": "http://arxiv.org/abs/0811.2904v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Faster Retrieval with a Two-Pass Dynamic-Time-Warping Lower Bound",
        "authors": [
            "Daniel Lemire"
        ],
        "summary": "The Dynamic Time Warping (DTW) is a popular similarity measure between time series. The DTW fails to satisfy the triangle inequality and its computation requires quadratic time. Hence, to find closest neighbors quickly, we use bounding techniques. We can avoid most DTW computations with an inexpensive lower bound (LB Keogh). We compare LB Keogh with a tighter lower bound (LB Improved). We find that LB Improved-based search is faster. As an example, our approach is 2-3 times faster over random-walk and shape time series.",
        "published": "2008-11-20T16:22:05Z",
        "link": "http://arxiv.org/abs/0811.3301v2",
        "categories": [
            "cs.DB",
            "cs.CV"
        ]
    },
    {
        "title": "Temporal Support of Regular Expressions in Sequential Pattern Mining",
        "authors": [
            "Leticia Gomez",
            "Bart Kuijpers",
            "Alejandro Vaisman"
        ],
        "summary": "Classic algorithms for sequential pattern discovery, return all frequent sequences present in a database, but, in general, only a few ones are interesting for the user. Languages based on regular expressions (RE) have been proposed to restrict frequent sequences to the ones that satisfy user-specified constraints. Although the support of a sequence is computed as the number of data-sequences satisfying a pattern with respect to the total number of data-sequences in the database, once regular expressions come into play, new approaches to the concept of support are needed. For example, users may be interested in computing the support of the RE as a whole, in addition to the one of a particular pattern. Also, when the items are frequently updated, the traditional way of counting support in sequential pattern mining may lead to incorrect (or, at least incomplete), conclusions. The problem gets more involved if we are interested in categorical sequential patterns. In light of the above, in this paper we propose to revise the classic notion of support in sequential pattern mining, introducing the concept of temporal support of regular expressions, intuitively defined as the number of sequences satisfying a target pattern, out of the total number of sequences that could have possibly matched such pattern, where the pattern is defined as a RE over complex items (i.e., not only item identifiers, but also attributes and functions).",
        "published": "2008-11-22T15:22:40Z",
        "link": "http://arxiv.org/abs/0811.3691v1",
        "categories": [
            "cs.DB",
            "H.2.8"
        ]
    },
    {
        "title": "Dynamic Indexability: The Query-Update Tradeoff for One-Dimensional   Range Queries",
        "authors": [
            "Ke Yi"
        ],
        "summary": "The B-tree is a fundamental secondary index structure that is widely used for answering one-dimensional range reporting queries. Given a set of $N$ keys, a range query can be answered in $O(\\log_B \\nm + \\frac{K}{B})$ I/Os, where $B$ is the disk block size, $K$ the output size, and $M$ the size of the main memory buffer. When keys are inserted or deleted, the B-tree is updated in $O(\\log_B N)$ I/Os, if we require the resulting changes to be committed to disk right away. Otherwise, the memory buffer can be used to buffer the recent updates, and changes can be written to disk in batches, which significantly lowers the amortized update cost. A systematic way of batching up updates is to use the logarithmic method, combined with fractional cascading, resulting in a dynamic B-tree that supports insertions in $O(\\frac{1}{B}\\log\\nm)$ I/Os and queries in $O(\\log\\nm + \\frac{K}{B})$ I/Os. Such bounds have also been matched by several known dynamic B-tree variants in the database literature.   In this paper, we prove that for any dynamic one-dimensional range query index structure with query cost $O(q+\\frac{K}{B})$ and amortized insertion cost $O(u/B)$, the tradeoff $q\\cdot \\log(u/q) = \\Omega(\\log B)$ must hold if $q=O(\\log B)$. For most reasonable values of the parameters, we have $\\nm = B^{O(1)}$, in which case our query-insertion tradeoff implies that the bounds mentioned above are already optimal. Our lower bounds hold in a dynamic version of the {\\em indexability model}, which is of independent interests.",
        "published": "2008-11-26T15:36:14Z",
        "link": "http://arxiv.org/abs/0811.4346v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "An Introduction to Knowledge Management",
        "authors": [
            "Sabu M. Thampi"
        ],
        "summary": "Knowledge has been lately recognized as one of the most important assets of organizations. Managing knowledge has grown to be imperative for the success of a company. This paper presents an overview of Knowledge Management and various aspects of secure knowledge management. A case study of knowledge management activities at Tata Steel is also discussed",
        "published": "2008-12-02T09:00:27Z",
        "link": "http://arxiv.org/abs/0812.0438v1",
        "categories": [
            "cs.DB",
            "cs.CR"
        ]
    },
    {
        "title": "Provenance Traces",
        "authors": [
            "James Cheney",
            "Umut Acar",
            "Amal Ahmed"
        ],
        "summary": "Provenance is information about the origin, derivation, ownership, or history of an object. It has recently been studied extensively in scientific databases and other settings due to its importance in helping scientists judge data validity, quality and integrity. However, most models of provenance have been stated as ad hoc definitions motivated by informal concepts such as \"comes from\", \"influences\", \"produces\", or \"depends on\". These models lack clear formalizations describing in what sense the definitions capture these intuitive concepts. This makes it difficult to compare approaches, evaluate their effectiveness, or argue about their validity.   We introduce provenance traces, a general form of provenance for the nested relational calculus (NRC), a core database query language. Provenance traces can be thought of as concrete data structures representing the operational semantics derivation of a computation; they are related to the traces that have been used in self-adjusting computation, but differ in important respects. We define a tracing operational semantics for NRC queries that produces both an ordinary result and a trace of the execution. We show that three pre-existing forms of provenance for the NRC can be extracted from provenance traces. Moreover, traces satisfy two semantic guarantees: consistency, meaning that the traces describe what actually happened during execution, and fidelity, meaning that the traces \"explain\" how the expression would behave if the input were changed. These guarantees are much stronger than those contemplated for previous approaches to provenance; thus, provenance traces provide a general semantic foundation for comparing and unifying models of provenance in databases.",
        "published": "2008-12-02T18:17:23Z",
        "link": "http://arxiv.org/abs/0812.0564v1",
        "categories": [
            "cs.PL",
            "cs.DB"
        ]
    },
    {
        "title": "Consensus Answers for Queries over Probabilistic Databases",
        "authors": [
            "Jian Li",
            "Amol Deshpande"
        ],
        "summary": "We address the problem of finding a \"best\" deterministic query answer to a query over a probabilistic database. For this purpose, we propose the notion of a consensus world (or a consensus answer) which is a deterministic world (answer) that minimizes the expected distance to the possible worlds (answers). This problem can be seen as a generalization of the well-studied inconsistent information aggregation problems (e.g. rank aggregation) to probabilistic databases. We consider this problem for various types of queries including SPJ queries, \\Topk queries, group-by aggregate queries, and clustering. For different distance metrics, we obtain polynomial time optimal or approximation algorithms for computing the consensus answers (or prove NP-hardness). Most of our results are for a general probabilistic database model, called {\\em and/xor tree model}, which significantly generalizes previous probabilistic database models like x-tuples and block-independent disjoint models, and is of independent interest.",
        "published": "2008-12-10T23:20:17Z",
        "link": "http://arxiv.org/abs/0812.2049v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Equivalence of SQL Queries in Presence of Embedded Dependencies",
        "authors": [
            "Rada Chirkova",
            "Michael Genesereth"
        ],
        "summary": "We consider the problem of finding equivalent minimal-size reformulations of SQL queries in presence of embedded dependencies [1]. Our focus is on select-project-join (SPJ) queries with equality comparisons, also known as safe conjunctive (CQ) queries, possibly with grouping and aggregation. For SPJ queries, the semantics of the SQL standard treat query answers as multisets (a.k.a. bags), whereas the stored relations may be treated either as sets, which is called bag-set semantics for query evaluation, or as bags, which is called bag semantics. (Under set semantics, both query answers and stored relations are treated as sets.)   In the context of the above Query-Reformulation Problem, we develop a comprehensive framework for equivalence of CQ queries under bag and bag-set semantics in presence of embedded dependencies, and make a number of conceptual and technical contributions. Specifically, we develop equivalence tests for CQ queries in presence of arbitrary sets of embedded dependencies under bag and bag-set semantics, under the condition that chase [9] under set semantics (set-chase) on the inputs terminates. We also present equivalence tests for aggregate CQ queries in presence of embedded dependencies. We use our equivalence tests to develop sound and complete (whenever set-chase on the inputs terminates) algorithms for solving instances of the Query-Reformulation Problem with CQ queries under each of bag and bag-set semantics, as well as for instances of the problem with aggregate queries.",
        "published": "2008-12-11T17:25:55Z",
        "link": "http://arxiv.org/abs/0812.2195v3",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "A Data Model for Integrating Heterogeneous Medical Data in the   Health-e-Child Project",
        "authors": [
            "Andrew Branson",
            "Tamas Hauer",
            "Richard McClatchey",
            "Dmitry Rogulin",
            "Jetendr Shamdasani"
        ],
        "summary": "There has been much research activity in recent times about providing the data infrastructures needed for the provision of personalised healthcare. In particular the requirement of integrating multiple, potentially distributed, heterogeneous data sources in the medical domain for the use of clinicians has set challenging goals for the healthgrid community. The approach advocated in this paper surrounds the provision of an Integrated Data Model plus links to/from ontologies to homogenize biomedical (from genomic, through cellular, disease, patient and population-related) data in the context of the EC Framework 6 Health-e-Child project. Clinical requirements are identified, the design approach in constructing the model is detailed and the integrated model described in the context of examples taken from that project. Pointers are given to future work relating the model to medical ontologies and challenges to the use of fully integrated models and ontologies are identified.",
        "published": "2008-12-15T18:25:51Z",
        "link": "http://arxiv.org/abs/0812.2874v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Ontology Assisted Query Reformulation Using Semantic and Assertion   Capabilities of OWL-DL Ontologies",
        "authors": [
            "Kamran Munir",
            "Mohammed Odeh",
            "Richard McClatchey"
        ],
        "summary": "End users of recent biomedical information systems are often unaware of the storage structure and access mechanisms of the underlying data sources and can require simplified mechanisms for writing domain specific complex queries. This research aims to assist users and their applications in formulating queries without requiring complete knowledge of the information structure of underlying data sources. To achieve this, query reformulation techniques and algorithms have been developed that can interpret ontology-based search criteria and associated domain knowledge in order to reformulate a relational query. These query reformulation algorithms exploit the semantic relationships and assertion capabilities of OWL-DL based domain ontologies for query reformulation. In this paper, this approach is applied to the integrated database schema of the EU funded Health-e-Child (HeC) project with the aim of providing ontology assisted query reformulation techniques to simplify the global access that is needed to millions of medical records across the UK and Europe.",
        "published": "2008-12-15T18:34:44Z",
        "link": "http://arxiv.org/abs/0812.2879v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "XML Static Analyzer User Manual",
        "authors": [
            "Pierre Geneves",
            "Nabil Layaida"
        ],
        "summary": "This document describes how to use the XML static analyzer in practice. It provides informal documentation for using the XML reasoning solver implementation. The solver allows automated verification of properties that are expressed as logical formulas over trees. A logical formula may for instance express structural constraints or navigation properties (like e.g. path existence and node selection) in finite trees. Logical formulas can be expressed using the syntax of XPath expressions, DTD, XML Schemas, and Relax NG definitions.",
        "published": "2008-12-18T15:22:46Z",
        "link": "http://arxiv.org/abs/0812.3550v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "cs.LO",
            "cs.SE",
            "D.3.0; D.3.1; D.3.4; E.1; F.3.1; F.3.2; F.4.1; F.4.3; H.2.3; I.2.4;\n  I.7.2"
        ]
    },
    {
        "title": "Business processes integration and performance indicators in a PLM",
        "authors": [
            "Aurélie Bissay",
            "Philippe Pernelle",
            "Arnaud Lefebvre",
            "Abdelaziz Bouras"
        ],
        "summary": "In an economic environment more and more competitive, the effective management of information and knowledge is a strategic issue for industrial enterprises. In the global marketplace, companies must use reactive strategies and reduce their products development cycle. In this context, the PLM (Product Lifecycle Management) is considered as a key component of the information system. The aim of this paper is to present an approach to integrate Business Processes in a PLM system. This approach is implemented in automotive sector with second-tier subcontractor",
        "published": "2008-12-19T07:52:46Z",
        "link": "http://arxiv.org/abs/0812.3715v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Foundations of SPARQL Query Optimization",
        "authors": [
            "Michael Schmidt",
            "Michael Meier",
            "Georg Lausen"
        ],
        "summary": "The SPARQL query language is a recent W3C standard for processing RDF data, a format that has been developed to encode information in a machine-readable way. We investigate the foundations of SPARQL query optimization and (a) provide novel complexity results for the SPARQL evaluation problem, showing that the main source of complexity is operator OPTIONAL alone; (b) propose a comprehensive set of algebraic query rewriting rules; (c) present a framework for constraint-based SPARQL optimization based upon the well-known chase procedure for Conjunctive Query minimization. In this line, we develop two novel termination conditions for the chase. They subsume the strongest conditions known so far and do not increase the complexity of the recognition problem, thus making a larger class of both Conjunctive and SPARQL queries amenable to constraint-based optimization. Our results are of immediate practical interest and might empower any SPARQL query optimizer.",
        "published": "2008-12-19T13:51:57Z",
        "link": "http://arxiv.org/abs/0812.3788v2",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "An Array Algebra",
        "authors": [
            "Albrecht Schmidt"
        ],
        "summary": "This is a proposal of an algebra which aims at distributed array processing. The focus lies on re-arranging and distributing array data, which may be multi-dimensional. The context of the work is scientific processing; thus, the core science operations are assumed to be taken care of in external libraries or languages. A main design driver is the desire to carry over some of the strategies of the relational algebra into the array domain.",
        "published": "2008-12-29T23:14:00Z",
        "link": "http://arxiv.org/abs/0812.4986v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Evolution of central pattern generators for the control of a five-link   bipedal walking mechanism",
        "authors": [
            "Atilim Gunes Baydin"
        ],
        "summary": "Central pattern generators (CPGs), with a basis is neurophysiological studies, are a type of neural network for the generation of rhythmic motion. While CPGs are being increasingly used in robot control, most applications are hand-tuned for a specific task and it is acknowledged in the field that generic methods and design principles for creating individual networks for a given task are lacking. This study presents an approach where the connectivity and oscillatory parameters of a CPG network are determined by an evolutionary algorithm with fitness evaluations in a realistic simulation with accurate physics. We apply this technique to a five-link planar walking mechanism to demonstrate its feasibility and performance. In addition, to see whether results from simulation can be acceptably transferred to real robot hardware, the best evolved CPG network is also tested on a real mechanism. Our results also confirm that the biologically inspired CPG model is well suited for legged locomotion, since a diverse manifestation of networks have been observed to succeed in fitness simulations during evolution.",
        "published": "2008-01-06T00:20:25Z",
        "link": "http://arxiv.org/abs/0801.0830v9",
        "categories": [
            "cs.NE",
            "cs.RO",
            "92B20, 92B25, 70E60, 68T05, 68U20",
            "I.2.2; I.2.9"
        ]
    },
    {
        "title": "D-optimal Bayesian Interrogation for Parameter and Noise Identification   of Recurrent Neural Networks",
        "authors": [
            "Barnabas Poczos",
            "Andras Lorincz"
        ],
        "summary": "We introduce a novel online Bayesian method for the identification of a family of noisy recurrent neural networks (RNNs). We develop Bayesian active learning technique in order to optimize the interrogating stimuli given past experiences. In particular, we consider the unknown parameters as stochastic variables and use the D-optimality principle, also known as `\\emph{infomax method}', to choose optimal stimuli. We apply a greedy technique to maximize the information gain concerning network parameters at each time step. We also derive the D-optimal estimation of the additive noise that perturbs the dynamical system of the RNN. Our analytical results are approximation-free. The analytic derivation gives rise to attractive quadratic update rules.",
        "published": "2008-01-12T08:02:12Z",
        "link": "http://arxiv.org/abs/0801.1883v1",
        "categories": [
            "cs.NE",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Analysis of Estimation of Distribution Algorithms and Genetic Algorithms   on NK Landscapes",
        "authors": [
            "Martin Pelikan"
        ],
        "summary": "This study analyzes performance of several genetic and evolutionary algorithms on randomly generated NK fitness landscapes with various values of n and k. A large number of NK problem instances are first generated for each n and k, and the global optimum of each instance is obtained using the branch-and-bound algorithm. Next, the hierarchical Bayesian optimization algorithm (hBOA), the univariate marginal distribution algorithm (UMDA), and the simple genetic algorithm (GA) with uniform and two-point crossover operators are applied to all generated instances. Performance of all algorithms is then analyzed and compared, and the results are discussed.",
        "published": "2008-01-21T00:20:50Z",
        "link": "http://arxiv.org/abs/0801.3111v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.6; I.2.8; G.1.6"
        ]
    },
    {
        "title": "iBOA: The Incremental Bayesian Optimization Algorithm",
        "authors": [
            "Martin Pelikan",
            "Kumara Sastry",
            "David E. Goldberg"
        ],
        "summary": "This paper proposes the incremental Bayesian optimization algorithm (iBOA), which modifies standard BOA by removing the population of solutions and using incremental updates of the Bayesian network. iBOA is shown to be able to learn and exploit unrestricted Bayesian networks using incremental techniques for updating both the structure as well as the parameters of the probabilistic model. This represents an important step toward the design of competent incremental estimation of distribution algorithms that can solve difficult nearly decomposable problems scalably and reliably.",
        "published": "2008-01-21T00:34:55Z",
        "link": "http://arxiv.org/abs/0801.3113v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.6; I.2.8; G.1.6"
        ]
    },
    {
        "title": "A Pyramidal Evolutionary Algorithm with Different Inter-Agent Partnering   Strategies for Scheduling Problems",
        "authors": [
            "Uwe Aickelin"
        ],
        "summary": "This paper combines the idea of a hierarchical distributed genetic algorithm with different inter-agent partnering strategies. Cascading clusters of sub-populations are built from bottom up, with higher-level sub-populations optimising larger parts of the problem. Hence higher-level sub-populations search a larger search space with a lower resolution whilst lower-level sub-populations search a smaller search space with a higher resolution. The effects of different partner selection schemes amongst the agents on solution quality are examined for two multiple-choice optimisation problems. It is shown that partnering strategies that exploit problem-specific knowledge are superior and can counter inappropriate (sub-) fitness measurements.",
        "published": "2008-01-21T15:55:22Z",
        "link": "http://arxiv.org/abs/0801.3209v2",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "On the Effects of Idiotypic Interactions for Recommendation Communities   in Artificial Immune Systems",
        "authors": [
            "Steve Cayzer",
            "Uwe Aickelin"
        ],
        "summary": "It has previously been shown that a recommender based on immune system idiotypic principles can out perform one based on correlation alone. This paper reports the results of work in progress, where we undertake some investigations into the nature of this beneficial effect. The initial findings are that the immune system recommender tends to produce different neighbourhoods, and that the superior performance of this recommender is due partly to the different neighbourhoods, and partly to the way that the idiotypic effect is used to weight each neighbours recommendations.",
        "published": "2008-01-23T09:59:06Z",
        "link": "http://arxiv.org/abs/0801.3539v3",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A Recommender System based on the Immune Network",
        "authors": [
            "Steve Cazyer",
            "Uwe Aickelin"
        ],
        "summary": "The immune system is a complex biological system with a highly distributed, adaptive and self-organising nature. This paper presents an artificial immune system (AIS) that exploits some of these characteristics and is applied to the task of film recommendation by collaborative filtering (CF). Natural evolution and in particular the immune system have not been designed for classical optimisation. However, for this problem, we are not interested in finding a single optimum. Rather we intend to identify a sub-set of good matches on which recommendations can be based. It is our hypothesis that an AIS built on two central aspects of the biological immune system will be an ideal candidate to achieve this: Antigen - antibody interaction for matching and antibody - antibody interaction for diversity. Computational results are presented in support of this conjecture and compared to those found by other CF techniques.",
        "published": "2008-01-23T10:42:49Z",
        "link": "http://arxiv.org/abs/0801.3547v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "The Danger Theory and Its Application to Artificial Immune Systems",
        "authors": [
            "Uwe Aickelin",
            "Steve Cayzer"
        ],
        "summary": "Over the last decade, a new idea challenging the classical self-non-self viewpoint has become popular amongst immunologists. It is called the Danger Theory. In this conceptual paper, we look at this theory from the perspective of Artificial Immune System practitioners. An overview of the Danger Theory is presented with particular emphasis on analogies in the Artificial Immune Systems world. A number of potential application areas are then used to provide a framing for a critical assessment of the concept, and its relevance for Artificial Immune Systems.",
        "published": "2008-01-23T11:01:31Z",
        "link": "http://arxiv.org/abs/0801.3549v3",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CR"
        ]
    },
    {
        "title": "Partnering Strategies for Fitness Evaluation in a Pyramidal Evolutionary   Algorithm",
        "authors": [
            "Uwe Aickelin",
            "Larry Bull"
        ],
        "summary": "This paper combines the idea of a hierarchical distributed genetic algorithm with different inter-agent partnering strategies. Cascading clusters of sub-populations are built from bottom up, with higher-level sub-populations optimising larger parts of the problem. Hence higher-level sub-populations search a larger search space with a lower resolution whilst lower-level sub-populations search a smaller search space with a higher resolution. The effects of different partner selection schemes for (sub-)fitness evaluation purposes are examined for two multiple-choice optimisation problems. It is shown that random partnering strategies perform best by providing better sampling and more diversity.",
        "published": "2008-01-23T11:12:39Z",
        "link": "http://arxiv.org/abs/0801.3550v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A Bayesian Optimisation Algorithm for the Nurse Scheduling Problem",
        "authors": [
            "Jingpeng Li",
            "Uwe Aickelin"
        ],
        "summary": "A Bayesian optimization algorithm for the nurse scheduling problem is presented, which involves choosing a suitable scheduling rule from a set for each nurses assignment. Unlike our previous work that used Gas to implement implicit learning, the learning in the proposed algorithm is explicit, ie. Eventually, we will be able to identify and mix building blocks directly. The Bayesian optimization algorithm is applied to implement such explicit learning by building a Bayesian network of the joint distribution of solutions. The conditional probability of each variable in the network is computed according to an initial set of promising solutions. Subsequently, each new instance for each variable is generated, ie in our case, a new rule string has been obtained. Another set of rule strings will be generated in this way, some of which will replace previous strings based on fitness selection. If stopping conditions are not met, the conditional probabilities for all nodes in the Bayesian network are updated again using the current set of promising rule strings. Computational results from 52 real data instances demonstrate the success of this approach. It is also suggested that the learning mechanism in the proposed approach might be suitable for other scheduling problems.",
        "published": "2008-01-25T16:07:25Z",
        "link": "http://arxiv.org/abs/0801.3971v3",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Movie Recommendation Systems Using An Artificial Immune System",
        "authors": [
            "Qi Chen",
            "Uwe Aickelin"
        ],
        "summary": "We apply the Artificial Immune System (AIS) technology to the Collaborative Filtering (CF) technology when we build the movie recommendation system. Two different affinity measure algorithms of AIS, Kendall tau and Weighted Kappa, are used to calculate the correlation coefficients for this movie recommendation system. From the testing we think that Weighted Kappa is more suitable than Kendall tau for movie problems.",
        "published": "2008-01-28T14:19:12Z",
        "link": "http://arxiv.org/abs/0801.4287v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "On Affinity Measures for Artificial Immune System Movie Recommenders",
        "authors": [
            "Uwe Aickelin",
            "Qi Chen"
        ],
        "summary": "We combine Artificial Immune Systems 'AIS', technology with Collaborative Filtering 'CF' and use it to build a movie recommendation system. We already know that Artificial Immune Systems work well as movie recommenders from previous work by Cayzer and Aickelin 3, 4, 5. Here our aim is to investigate the effect of different affinity measure algorithms for the AIS. Two different affinity measures, Kendalls Tau and Weighted Kappa, are used to calculate the correlation coefficients for the movie recommender. We compare the results with those published previously and show that Weighted Kappa is more suitable than others for movie problems. We also show that AIS are generally robust movie recommenders and that, as long as a suitable affinity measure is chosen, results are good.",
        "published": "2008-01-28T15:14:45Z",
        "link": "http://arxiv.org/abs/0801.4307v3",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CY"
        ]
    },
    {
        "title": "Investigating Artificial Immune Systems For Job Shop Rescheduling In   Changing Environments",
        "authors": [
            "Uwe Aickelin",
            "Edmund Burke",
            "Aniza Din"
        ],
        "summary": "Artificial immune system can be used to generate schedules in changing environments and it has been proven to be more robust than schedules developed using a genetic algorithm. Good schedules can be produced especially when the number of the antigens is increased. However, an increase in the range of the antigens had somehow affected the fitness of the immune system. In this research, we are trying to improve the result of the system by rescheduling the same problem using the same method while at the same time maintaining the robustness of the schedules.",
        "published": "2008-01-28T15:26:59Z",
        "link": "http://arxiv.org/abs/0801.4312v3",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Artificial Immune Systems (AIS) - A New Paradigm for Heuristic Decision   Making",
        "authors": [
            "Uwe Aickelin"
        ],
        "summary": "Over the last few years, more and more heuristic decision making techniques have been inspired by nature, e.g. evolutionary algorithms, ant colony optimisation and simulated annealing. More recently, a novel computational intelligence technique inspired by immunology has emerged, called Artificial Immune Systems (AIS). This immune system inspired technique has already been useful in solving some computational problems. In this keynote, we will very briefly describe the immune system metaphors that are relevant to AIS. We will then give some illustrative real-world problems suitable for AIS use and show a step-by-step algorithm walkthrough. A comparison of AIS to other well-known algorithms and areas for future work will round this keynote off. It should be noted that as AIS is still a young and evolving field, there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from the examples given here.",
        "published": "2008-01-28T15:32:05Z",
        "link": "http://arxiv.org/abs/0801.4314v3",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Strategic Alert Throttling for Intrusion Detection Systems",
        "authors": [
            "Gianni Tedesco",
            "Uwe Aickelin"
        ],
        "summary": "Network intrusion detection systems are themselves becoming targets of attackers. Alert flood attacks may be used to conceal malicious activity by hiding it among a deluge of false alerts sent by the attacker. Although these types of attacks are very hard to stop completely, our aim is to present techniques that improve alert throughput and capacity to such an extent that the resources required to successfully mount the attack become prohibitive. The key idea presented is to combine a token bucket filter with a realtime correlation algorithm. The proposed algorithm throttles alert output from the IDS when an attack is detected. The attack graph used in the correlation algorithm is used to make sure that alerts crucial to forming strategies are not discarded by throttling.",
        "published": "2008-01-28T15:36:56Z",
        "link": "http://arxiv.org/abs/0801.4119v3",
        "categories": [
            "cs.NE",
            "cs.CR"
        ]
    },
    {
        "title": "Multi-Layer Perceptrons and Symbolic Data",
        "authors": [
            "Fabrice Rossi",
            "Brieuc Conan-Guez"
        ],
        "summary": "In some real world situations, linear models are not sufficient to represent accurately complex relations between input variables and output variables of a studied system. Multilayer Perceptrons are one of the most successful non-linear regression tool but they are unfortunately restricted to inputs and outputs that belong to a normed vector space. In this chapter, we propose a general recoding method that allows to use symbolic data both as inputs and outputs to Multilayer Perceptrons. The recoding is quite simple to implement and yet provides a flexible framework that allows to deal with almost all practical cases. The proposed method is illustrated on a real world data set.",
        "published": "2008-02-02T15:09:42Z",
        "link": "http://arxiv.org/abs/0802.0251v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Accélération des cartes auto-organisatrices sur tableau de   dissimilarités par séparation et évaluation",
        "authors": [
            "Brieuc Conan-Guez",
            "Fabrice Rossi"
        ],
        "summary": "In this paper, a new implementation of the adaptation of Kohonen self-organising maps (SOM) to dissimilarity matrices is proposed. This implementation relies on the branch and bound principle to reduce the algorithm running time. An important property of this new approach is that the obtained algorithm produces exactly the same results as the standard algorithm.",
        "published": "2008-02-02T15:10:35Z",
        "link": "http://arxiv.org/abs/0802.0252v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "A data-driven functional projection approach for the selection of   feature ranges in spectra with ICA or cluster analysis",
        "authors": [
            "Catherine Krier",
            "Fabrice Rossi",
            "Damien François",
            "Michel Verleysen"
        ],
        "summary": "Prediction problems from spectra are largely encountered in chemometry. In addition to accurate predictions, it is often needed to extract information about which wavelengths in the spectra contribute in an effective way to the quality of the prediction. This implies to select wavelengths (or wavelength intervals), a problem associated to variable selection. In this paper, it is shown how this problem may be tackled in the specific case of smooth (for example infrared) spectra. The functional character of the spectra (their smoothness) is taken into account through a functional variable projection procedure. Contrarily to standard approaches, the projection is performed on a basis that is driven by the spectra themselves, in order to best fit their characteristics. The methodology is illustrated by two examples of functional projection, using Independent Component Analysis and functional variable clustering, respectively. The performances on two standard infrared spectra benchmarks are illustrated.",
        "published": "2008-02-03T19:02:49Z",
        "link": "http://arxiv.org/abs/0802.0287v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Using Bayesian Blocks to Partition Self-Organizing Maps",
        "authors": [
            "Paul R. Gazis",
            "Jeffrey D. Scargle"
        ],
        "summary": "Self organizing maps (SOMs) are widely-used for unsupervised classification. For this application, they must be combined with some partitioning scheme that can identify boundaries between distinct regions in the maps they produce. We discuss a novel partitioning scheme for SOMs based on the Bayesian Blocks segmentation algorithm of Scargle [1998]. This algorithm minimizes a cost function to identify contiguous regions over which the values of the attributes can be represented as approximately constant. Because this cost function is well-defined and largely independent of assumptions regarding the number and structure of clusters in the original sample space, this partitioning scheme offers significant advantages over many conventional methods. Sample code is available.",
        "published": "2008-02-06T18:50:16Z",
        "link": "http://arxiv.org/abs/0802.0861v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Extreme Learning Machine for land cover classification",
        "authors": [
            "Mahesh Pal"
        ],
        "summary": "This paper explores the potential of extreme learning machine based supervised classification algorithm for land cover classification. In comparison to a backpropagation neural network, which requires setting of several user-defined parameters and may produce local minima, extreme learning machine require setting of one parameter and produce a unique solution. ETM+ multispectral data set (England) was used to judge the suitability of extreme learning machine for remote sensing classifications. A back propagation neural network was used to compare its performance in term of classification accuracy and computational cost. Results suggest that the extreme learning machine perform equally well to back propagation neural network in term of classification accuracy with this data set. The computational cost using extreme learning machine is very small in comparison to back propagation neural network.",
        "published": "2008-02-11T11:12:06Z",
        "link": "http://arxiv.org/abs/0802.1412v1",
        "categories": [
            "cs.NE",
            "cs.CV"
        ]
    },
    {
        "title": "Exploiting problem structure in a genetic algorithm approach to a nurse   rostering problem",
        "authors": [
            "Uwe Aickelin",
            "Kathryn Dowsland"
        ],
        "summary": "There is considerable interest in the use of genetic algorithms to solve problems arising in the areas of scheduling and timetabling. However, the classical genetic algorithm paradigm is not well equipped to handle the conflict between objectives and constraints that typically occurs in such problems. In order to overcome this, successful implementations frequently make use of problem specific knowledge. This paper is concerned with the development of a GA for a nurse rostering problem at a major UK hospital. The structure of the constraints is used as the basis for a co-evolutionary strategy using co-operating sub-populations. Problem specific knowledge is also used to define a system of incentives and disincentives, and a complementary mutation operator. Empirical results based on 52 weeks of live data show how these features are able to improve an unsuccessful canonical GA to the point where it is able to provide a practical solution to the problem",
        "published": "2008-02-14T11:25:37Z",
        "link": "http://arxiv.org/abs/0802.2001v3",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Support Vector classifiers for Land Cover Classification",
        "authors": [
            "Mahesh Pal",
            "Paul M. Mather"
        ],
        "summary": "Support vector machines represent a promising development in machine learning research that is not widely used within the remote sensing community. This paper reports the results of Multispectral(Landsat-7 ETM+) and Hyperspectral DAIS)data in which multi-class SVMs are compared with maximum likelihood and artificial neural network methods in terms of classification accuracy. Our results show that the SVM achieves a higher level of classification accuracy than either the maximum likelihood or the neural classifier, and that the support vector machine can be used with small training datasets and high-dimensional data.",
        "published": "2008-02-15T04:53:33Z",
        "link": "http://arxiv.org/abs/0802.2138v1",
        "categories": [
            "cs.NE",
            "cs.CV"
        ]
    },
    {
        "title": "Multiclass Approaches for Support Vector Machine Based Land Cover   Classification",
        "authors": [
            "Mahesh Pal"
        ],
        "summary": "SVMs were initially developed to perform binary classification; though, applications of binary classification are very limited. Most of the practical applications involve multiclass classification, especially in remote sensing land cover classification. A number of methods have been proposed to implement SVMs to produce multiclass classification. A number of methods to generate multiclass SVMs from binary SVMs have been proposed by researchers and is still a continuing research topic. This paper compares the performance of six multi-class approaches to solve classification problem with remote sensing data in term of classification accuracy and computational cost. One vs. one, one vs. rest, Directed Acyclic Graph (DAG), and Error Corrected Output Coding (ECOC) based multiclass approaches creates many binary classifiers and combines their results to determine the class label of a test pixel. Another catogery of multi class approach modify the binary class objective function and allows simultaneous computation of multiclass classification by solving a single optimisation problem. Results from this study conclude the usefulness of One vs. One multi class approach in term of accuracy and computational cost over other multi class approaches.",
        "published": "2008-02-18T03:47:45Z",
        "link": "http://arxiv.org/abs/0802.2411v1",
        "categories": [
            "cs.NE",
            "cs.CV"
        ]
    },
    {
        "title": "Characterization of the convergence of stationary Fokker-Planck learning",
        "authors": [
            "Arturo Berrones"
        ],
        "summary": "The convergence properties of the stationary Fokker-Planck algorithm for the estimation of the asymptotic density of stochastic search processes is studied. Theoretical and empirical arguments for the characterization of convergence of the estimation in the case of separable and nonseparable nonlinear optimization problems are given. Some implications of the convergence of stationary Fokker-Planck learning for the inference of parameters in artificial neural network models are outlined.",
        "published": "2008-02-21T23:41:09Z",
        "link": "http://arxiv.org/abs/0802.3235v3",
        "categories": [
            "cs.NE",
            "cond-mat.dis-nn",
            "cs.AI"
        ]
    },
    {
        "title": "Neural Networks and Database Systems",
        "authors": [
            "Erich Schikuta"
        ],
        "summary": "Object-oriented database systems proved very valuable at handling and administrating complex objects. In the following guidelines for embedding neural networks into such systems are presented. It is our goal to treat networks as normal data in the database system. From the logical point of view, a neural network is a complex data value and can be stored as a normal data object. It is generally accepted that rule-based reasoning will play an important role in future database applications. The knowledge base consists of facts and rules, which are both stored and handled by the underlying database system. Neural networks can be seen as representation of intensional knowledge of intelligent database systems. So they are part of a rule based knowledge pool and can be used like conventional rules. The user has a unified view about his knowledge base regardless of the origin of the unique rules.",
        "published": "2008-02-25T09:57:31Z",
        "link": "http://arxiv.org/abs/0802.3582v1",
        "categories": [
            "cs.DB",
            "cs.NE",
            "H.2.1"
        ]
    },
    {
        "title": "Are complex systems hard to evolve?",
        "authors": [
            "Andy Adamatzky",
            "Larry Bull"
        ],
        "summary": "Evolutionary complexity is here measured by the number of trials/evaluations needed for evolving a logical gate in a non-linear medium. Behavioural complexity of the gates evolved is characterised in terms of cellular automata behaviour. We speculate that hierarchies of behavioural and evolutionary complexities are isomorphic up to some degree, subject to substrate specificity of evolution and the spectrum of evolution parameters.",
        "published": "2008-02-26T19:07:53Z",
        "link": "http://arxiv.org/abs/0802.3875v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Sensing Danger: Innate Immunology for Intrusion Detection",
        "authors": [
            "Uwe Aickelin",
            "Julie Greensmith"
        ],
        "summary": "The immune system provides an ideal metaphor for anomaly detection in general and computer security in particular. Based on this idea, artificial immune systems have been used for a number of years for intrusion detection, unfortunately so far with little success. However, these previous systems were largely based on immunological theory from the 1970s and 1980s and over the last decade our understanding of immunological processes has vastly improved. In this paper we present two new immune inspired algorithms based on the latest immunological discoveries, such as the behaviour of Dendritic Cells. The resultant algorithms are applied to real world intrusion problems and show encouraging results. Overall, we believe there is a bright future for these next generation artificial immune algorithms.",
        "published": "2008-02-27T12:15:08Z",
        "link": "http://arxiv.org/abs/0802.4002v3",
        "categories": [
            "cs.NE",
            "cs.CR"
        ]
    },
    {
        "title": "Brain architecture: A design for natural computation",
        "authors": [
            "Marcus Kaiser"
        ],
        "summary": "Fifty years ago, John von Neumann compared the architecture of the brain with that of computers that he invented and which is still in use today. In those days, the organisation of computers was based on concepts of brain organisation. Here, we give an update on current results on the global organisation of neural systems. For neural systems, we outline how the spatial and topological architecture of neuronal and cortical networks facilitates robustness against failures, fast processing, and balanced network activation. Finally, we discuss mechanisms of self-organization for such architectures. After all, the organization of the brain might again inspire computer architecture.",
        "published": "2008-02-27T13:00:38Z",
        "link": "http://arxiv.org/abs/0802.4010v1",
        "categories": [
            "q-bio.NC",
            "cs.AI",
            "cs.NE",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Dempster-Shafer for Anomaly Detection",
        "authors": [
            "Qi Chen",
            "Uwe Aickelin"
        ],
        "summary": "In this paper, we implement an anomaly detection system using the Dempster-Shafer method. Using two standard benchmark problems we show that by combining multiple signals it is possible to achieve better results than by using a single signal. We further show that by applying this approach to a real-world email dataset the algorithm works for email worm detection. Dempster-Shafer can be a promising method for anomaly detection problems with multiple features (data sources), and two or more classes.",
        "published": "2008-03-11T12:39:01Z",
        "link": "http://arxiv.org/abs/0803.1568v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CR"
        ]
    },
    {
        "title": "Simulation Optimization of the Crossdock Door Assignment Problem",
        "authors": [
            "Uwe Aickelin",
            "Adrian Adewunmi"
        ],
        "summary": "The purpose of this report is to present the Crossdock Door Assignment Problem, which involves assigning destinations to outbound dock doors of Crossdock centres such that travel distance by material handling equipment is minimized. We propose a two fold solution; simulation and optimization of the simulation model simulation optimization. The novel aspect of our solution approach is that we intend to use simulation to derive a more realistic objective function and use Memetic algorithms to find an optimal solution. The main advantage of using Memetic algorithms is that it combines a local search with Genetic Algorithms. The Crossdock Door Assignment Problem is a new domain application to Memetic Algorithms and it is yet unknown how it will perform.",
        "published": "2008-03-11T12:56:51Z",
        "link": "http://arxiv.org/abs/0803.1576v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Using Intelligent Agents to understand organisational behaviour",
        "authors": [
            "Helen Celia",
            "Christopher Clegg",
            "Mark Robinson",
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Christine Sprigg"
        ],
        "summary": "This paper introduces two ongoing research projects which seek to apply computer modelling techniques in order to simulate human behaviour within organisations. Previous research in other disciplines has suggested that complex social behaviours are governed by relatively simple rules which, when identified, can be used to accurately model such processes using computer technology. The broad objective of our research is to develop a similar capability within organisational psychology.",
        "published": "2008-03-11T14:19:33Z",
        "link": "http://arxiv.org/abs/0803.1596v1",
        "categories": [
            "cs.NE",
            "cs.MA"
        ]
    },
    {
        "title": "A Multi-Agent Simulation of Retail Management Practices",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Helen Celia",
            "Christopher Clegg"
        ],
        "summary": "We apply Agent-Based Modeling and Simulation (ABMS) to investigate a set of problems in a retail context. Specifically, we are working to understand the relationship between human resource management practices and retail productivity. Despite the fact we are working within a relatively novel and complex domain, it is clear that intelligent agents do offer potential for developing organizational capabilities in the future. Our multi-disciplinary research team has worked with a UK department store to collect data and capture perceptions about operations from actors within departments. Based on this case study work, we have built a simulator that we present in this paper. We then use the simulator to gather empirical evidence regarding two specific management practices: empowerment and employee development.",
        "published": "2008-03-11T14:37:40Z",
        "link": "http://arxiv.org/abs/0803.1598v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Understanding Retail Productivity by Simulating Management Practise",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Helen Celia",
            "Christopher Clegg"
        ],
        "summary": "Intelligent agents offer a new and exciting way of understanding the world of work. In this paper we apply agent-based modeling and simulation to investigate a set of problems in a retail context. Specifically, we are working to understand the relationship between human resource management practices and retail productivity. Despite the fact we are working within a relatively novel and complex domain, it is clear that intelligent agents could offer potential for fostering sustainable organizational capabilities in the future. Our research so far has led us to conduct case study work with a top ten UK retailer, collecting data in four departments in two stores. Based on our case study data we have built and tested a first version of a department store simulator. In this paper we will report on the current development of our simulator which includes new features concerning more realistic data on the pattern of footfall during the day and the week, a more differentiated view of customers, and the evolution of customers over time. This allows us to investigate more complex scenarios and to analyze the impact of various management practices.",
        "published": "2008-03-11T14:46:10Z",
        "link": "http://arxiv.org/abs/0803.1600v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Using Intelligent Agents to Understand Management Practices and Retail   Productivity",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Helen Celia",
            "Christopher Clegg"
        ],
        "summary": "Intelligent agents offer a new and exciting way of understanding the world of work. In this paper we apply agent-based modeling and simulation to investigate a set of problems in a retail context. Specifically, we are working to understand the relationship between human resource management practices and retail productivity. Despite the fact we are working within a relatively novel and complex domain, it is clear that intelligent agents could offer potential for fostering sustainable organizational capabilities in the future. The project is still at an early stage. So far we have conducted a case study in a UK department store to collect data and capture impressions about operations and actors within departments. Furthermore, based on our case study we have built and tested our first version of a retail branch simulator which we will present in this paper.",
        "published": "2008-03-11T14:55:58Z",
        "link": "http://arxiv.org/abs/0803.1604v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "cs.MA"
        ]
    },
    {
        "title": "An Agent-Based Simulation of In-Store Customer Experiences",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Helen Celia",
            "Christopher Clegg"
        ],
        "summary": "Agent-based modelling and simulation offers a new and exciting way of understanding the world of work. In this paper we describe the development of an agent-based simulation model, designed to help to understand the relationship between human resource management practices and retail productivity. We report on the current development of our simulation model which includes new features concerning the evolution of customers over time. To test some of these features we have conducted a series of experiments dealing with customer pool sizes, standard and noise reduction modes, and the spread of the word of mouth. Our multi-disciplinary research team draws upon expertise from work psychologists and computer scientists. Despite the fact we are working within a relatively novel and complex domain, it is clear that intelligent agents offer potential for fostering sustainable organisational capabilities in the future.",
        "published": "2008-03-11T16:11:34Z",
        "link": "http://arxiv.org/abs/0803.1621v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "cs.MA"
        ]
    },
    {
        "title": "Genetic-Algorithm Seeding Of Idiotypic Networks For Mobile-Robot   Navigation",
        "authors": [
            "Amanda Whitbrook",
            "Uwe Aickelin",
            "Jonathan Garibaldi"
        ],
        "summary": "Robot-control designers have begun to exploit the properties of the human immune system in order to produce dynamic systems that can adapt to complex, varying, real-world tasks. Jernes idiotypic-network theory has proved the most popular artificial-immune-system (AIS) method for incorporation into behaviour-based robotics, since idiotypic selection produces highly adaptive responses. However, previous efforts have mostly focused on evolving the network connections and have often worked with a single, pre-engineered set of behaviours, limiting variability. This paper describes a method for encoding behaviours as a variable set of attributes, and shows that when the encoding is used with a genetic algorithm (GA), multiple sets of diverse behaviours can develop naturally and rapidly, providing much greater scope for flexible behaviour-selection. The algorithm is tested extensively with a simulated e-puck robot that navigates around a maze by tracking colour. Results show that highly successful behaviour sets can be generated within about 25 minutes, and that much greater diversity can be obtained when multiple autonomous populations are used, rather than a single one.",
        "published": "2008-03-11T16:26:47Z",
        "link": "http://arxiv.org/abs/0803.1626v1",
        "categories": [
            "cs.NE",
            "cs.RO"
        ]
    },
    {
        "title": "Investigating a Hybrid Metaheuristic For Job Shop Rescheduling",
        "authors": [
            "Salwani Abdullah",
            "Uwe Aickelin",
            "Edmund Burke",
            "Aniza Din",
            "Rong Qu"
        ],
        "summary": "Previous research has shown that artificial immune systems can be used to produce robust schedules in a manufacturing environment. The main goal is to develop building blocks (antibodies) of partial schedules that can be used to construct backup solutions (antigens) when disturbances occur during production. The building blocks are created based upon underpinning ideas from artificial immune systems and evolved using a genetic algorithm (Phase I). Each partial schedule (antibody) is assigned a fitness value and the best partial schedules are selected to be converted into complete schedules (antigens). We further investigate whether simulated annealing and the great deluge algorithm can improve the results when hybridised with our artificial immune system (Phase II). We use ten fixed solutions as our target and measure how well we cover these specific scenarios.",
        "published": "2008-03-12T09:26:47Z",
        "link": "http://arxiv.org/abs/0803.1728v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Improved evolutionary generation of XSLT stylesheets",
        "authors": [
            "Pablo Garcia-Sanchez",
            "J. L. J. Laredo",
            "J. P. Sevilla",
            "Pedro Castillo",
            "J. J. Merelo"
        ],
        "summary": "This paper introduces a procedure based on genetic programming to evolve XSLT programs (usually called stylesheets or logicsheets). XSLT is a general purpose, document-oriented functional language, generally used to transform XML documents (or, in general, solve any problem that can be coded as an XML document). The proposed solution uses a tree representation for the stylesheets as well as diverse specific operators in order to obtain, in the studied cases and a reasonable time, a XSLT stylesheet that performs the transformation. Several types of representation have been compared, resulting in different performance and degree of success.",
        "published": "2008-03-13T11:43:25Z",
        "link": "http://arxiv.org/abs/0803.1926v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "An Investigation of the Sequential Sampling Method for Crossdocking   Simulation Output Variance Reduction",
        "authors": [
            "Adrian Adewunmi",
            "Uwe Aickelin",
            "Mike Byrne"
        ],
        "summary": "This paper investigates the reduction of variance associated with a simulation output performance measure, using the Sequential Sampling method while applying minimum simulation replications, for a class of JIT (Just in Time) warehousing system called crossdocking. We initially used the Sequential Sampling method to attain a desired 95% confidence interval half width of plus/minus 0.5 for our chosen performance measure (Total usage cost, given the mean maximum level of 157,000 pounds and a mean minimum level of 149,000 pounds). From our results, we achieved a 95% confidence interval half width of plus/minus 2.8 for our chosen performance measure (Total usage cost, with an average mean value of 115,000 pounds). However, the Sequential Sampling method requires a huge number of simulation replications to reduce variance for our simulation output value to the target level. Arena (version 11) simulation software was used to conduct this study.",
        "published": "2008-03-13T15:02:48Z",
        "link": "http://arxiv.org/abs/0803.1985v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Improved Squeaky Wheel Optimisation for Driver Scheduling",
        "authors": [
            "Uwe Aickelin",
            "Edmund Burke",
            "Jingpeng Li"
        ],
        "summary": "This paper presents a technique called Improved Squeaky Wheel Optimisation for driver scheduling problems. It improves the original Squeaky Wheel Optimisations effectiveness and execution speed by incorporating two additional steps of Selection and Mutation which implement evolution within a single solution. In the ISWO, a cycle of Analysis-Selection-Mutation-Prioritization-Construction continues until stopping conditions are reached. The Analysis step first computes the fitness of a current solution to identify troublesome components. The Selection step then discards these troublesome components probabilistically by using the fitness measure, and the Mutation step follows to further discard a small number of components at random. After the above steps, an input solution becomes partial and thus the resulting partial solution needs to be repaired. The repair is carried out by using the Prioritization step to first produce priorities that determine an order by which the following Construction step then schedules the remaining components. Therefore, the optimisation in the ISWO is achieved by solution disruption, iterative improvement and an iterative constructive repair process performed. Encouraging experimental results are reported.",
        "published": "2008-03-13T15:28:06Z",
        "link": "http://arxiv.org/abs/0803.1993v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "The Application of Bayesian Optimization and Classifier Systems in Nurse   Scheduling",
        "authors": [
            "Jingpeng Li",
            "Uwe Aickelin"
        ],
        "summary": "Two ideas taken from Bayesian optimization and classifier systems are presented for personnel scheduling based on choosing a suitable scheduling rule from a set for each persons assignment. Unlike our previous work of using genetic algorithms whose learning is implicit, the learning in both approaches is explicit, i.e. we are able to identify building blocks directly. To achieve this target, the Bayesian optimization algorithm builds a Bayesian network of the joint probability distribution of the rules used to construct solutions, while the adapted classifier system assigns each rule a strength value that is constantly updated according to its usefulness in the current situation. Computational results from 52 real data instances of nurse scheduling demonstrate the success of both approaches. It is also suggested that the learning mechanism in the proposed approaches might be suitable for other scheduling problems.",
        "published": "2008-03-13T15:43:34Z",
        "link": "http://arxiv.org/abs/0803.1994v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Danger Theory: The Link between AIS and IDS?",
        "authors": [
            "Uwe Aickelin",
            "Peter Bentley",
            "Steve Cayzer",
            "Kim Jungwon",
            "Julie McLeod"
        ],
        "summary": "We present ideas about creating a next generation Intrusion Detection System based on the latest immunological theories. The central challenge with computer security is determining the difference between normal and potentially harmful activity. For half a century, developers have protected their systems by coding rules that identify and block specific events. However, the nature of current and future threats in conjunction with ever larger IT systems urgently requires the development of automated and adaptive defensive tools. A promising solution is emerging in the form of Artificial Immune Systems. The Human Immune System can detect and defend against harmful and previously unseen invaders, so can we not build a similar Intrusion Detection System for our computers.",
        "published": "2008-03-13T16:01:10Z",
        "link": "http://arxiv.org/abs/0803.1997v2",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CR"
        ]
    },
    {
        "title": "Problem Solving and Complex Systems",
        "authors": [
            "Frédéric Guinand",
            "Yoann Pigné"
        ],
        "summary": "The observation and modeling of natural Complex Systems (CSs) like the human nervous system, the evolution or the weather, allows the definition of special abilities and models reusable to solve other problems. For instance, Genetic Algorithms or Ant Colony Optimizations are inspired from natural CSs to solve optimization problems. This paper proposes the use of ant-based systems to solve various problems with a non assessing approach. This means that solutions to some problem are not evaluated. They appear as resultant structures from the activity of the system. Problems are modeled with graphs and such structures are observed directly on these graphs. Problems of Multiple Sequences Alignment and Natural Language Processing are addressed with this approach.",
        "published": "2008-03-15T18:07:49Z",
        "link": "http://arxiv.org/abs/0803.2314v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Digital Ecosystems: Self-Organisation of Evolving Agent Populations",
        "authors": [
            "Gerard Briscoe",
            "Philippe De Wilde"
        ],
        "summary": "A primary motivation for our research in Digital Ecosystems is the desire to exploit the self-organising properties of biological ecosystems. Ecosystems are thought to be robust, scalable architectures that can automatically solve complex, dynamic problems. Self-organisation is perhaps one of the most desirable features in the systems that we engineer, and it is important for us to be able to measure self-organising behaviour. We investigate the self-organising aspects of Digital Ecosystems, created through the application of evolutionary computing to Multi-Agent Systems (MASs), aiming to determine a macroscopic variable to characterise the self-organisation of the evolving agent populations within. We study a measure for the self-organisation called Physical Complexity; based on statistical physics, automata theory, and information theory, providing a measure of information relative to the randomness in an organism's genome, by calculating the entropy in a population. We investigate an extension to include populations of variable length, and then built upon this to construct an efficiency measure to investigate clustering within evolving agent populations. Overall an insight has been achieved into where and how self-organisation occurs in our Digital Ecosystem, and how it can be quantified.",
        "published": "2008-03-18T16:59:12Z",
        "link": "http://arxiv.org/abs/0803.2675v4",
        "categories": [
            "cs.NE",
            "cs.CC",
            "C.2.4; D.2.11; H.1.0"
        ]
    },
    {
        "title": "KohonAnts: A Self-Organizing Ant Algorithm for Clustering and Pattern   Classification",
        "authors": [
            "C. Fernandes",
            "A. M. Mora",
            "J. J. Merelo",
            "V. Ramos",
            "J. L. J. Laredo"
        ],
        "summary": "In this paper we introduce a new ant-based method that takes advantage of the cooperative self-organization of Ant Colony Systems to create a naturally inspired clustering and pattern recognition method. The approach considers each data item as an ant, which moves inside a grid changing the cells it goes through, in a fashion similar to Kohonen's Self-Organizing Maps. The resulting algorithm is conceptually more simple, takes less free parameters than other ant-based clustering algorithms, and, after some parameter tuning, yields very good results on some benchmark problems.",
        "published": "2008-03-18T18:27:14Z",
        "link": "http://arxiv.org/abs/0803.2695v1",
        "categories": [
            "cs.NE",
            "cs.CV"
        ]
    },
    {
        "title": "Equivalence of Probabilistic Tournament and Polynomial Ranking Selection",
        "authors": [
            "Kassel Hingee",
            "Marcus Hutter"
        ],
        "summary": "Crucial to an Evolutionary Algorithm's performance is its selection scheme. We mathematically investigate the relation between polynomial rank and probabilistic tournament methods which are (respectively) generalisations of the popular linear ranking and tournament selection schemes. We show that every probabilistic tournament is equivalent to a unique polynomial rank scheme. In fact, we derived explicit operators for translating between these two types of selection. Of particular importance is that most linear and most practical quadratic rank schemes are probabilistic tournaments.",
        "published": "2008-03-20T03:50:53Z",
        "link": "http://arxiv.org/abs/0803.2925v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Enhanced Direct and Indirect Genetic Algorithm Approaches for a Mall   Layout and Tenant Selection Problem",
        "authors": [
            "Uwe Aickelin",
            "Kathryn Dowsland"
        ],
        "summary": "During our earlier research, it was recognised that in order to be successful with an indirect genetic algorithm approach using a decoder, the decoder has to strike a balance between being an optimiser in its own right and finding feasible solutions. Previously this balance was achieved manually. Here we extend this by presenting an automated approach where the genetic algorithm itself, simultaneously to solving the problem, sets weights to balance the components out. Subsequently we were able to solve a complex and non-linear scheduling problem better than with a standard direct genetic algorithm implementation.",
        "published": "2008-03-20T10:19:01Z",
        "link": "http://arxiv.org/abs/0803.2957v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "An Indirect Genetic Algorithm for Set Covering Problems",
        "authors": [
            "Uwe Aickelin"
        ],
        "summary": "This paper presents a new type of genetic algorithm for the set covering problem. It differs from previous evolutionary approaches first because it is an indirect algorithm, i.e. the actual solutions are found by an external decoder function. The genetic algorithm itself provides this decoder with permutations of the solution variables and other parameters. Second, it will be shown that results can be further improved by adding another indirect optimisation layer. The decoder will not directly seek out low cost solutions but instead aims for good exploitable solutions. These are then post optimised by another hill-climbing algorithm. Although seemingly more complicated, we will show that this three-stage approach has advantages in terms of solution quality, speed and adaptability to new types of problems over more direct approaches. Extensive computational results are presented and compared to the latest evolutionary and other heuristic approaches to the same data instances.",
        "published": "2008-03-20T10:58:32Z",
        "link": "http://arxiv.org/abs/0803.2965v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "On the Application of Hierarchical Coevolutionary Genetic Algorithms:   Recombination and Evaluation Partners",
        "authors": [
            "Uwe Aickelin",
            "Larry Bull"
        ],
        "summary": "This paper examines the use of a hierarchical coevolutionary genetic algorithm under different partnering strategies. Cascading clusters of sub-populations are built from the bottom up, with higher-level sub-populations optimising larger parts of the problem. Hence higher-level sub-populations potentially search a larger search space with a lower resolution whilst lower-level sub-populations search a smaller search space with a higher resolution. The effects of different partner selection schemes amongst the sub-populations on solution quality are examined for two constrained optimisation problems. We examine a number of recombination partnering strategies in the construction of higher-level individuals and a number of related schemes for evaluating sub-solutions. It is shown that partnering strategies that exploit problem-specific knowledge are superior and can counter inappropriate (sub)fitness measurements.",
        "published": "2008-03-20T11:09:39Z",
        "link": "http://arxiv.org/abs/0803.2966v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Building Better Nurse Scheduling Algorithms",
        "authors": [
            "Uwe Aickelin",
            "Paul White"
        ],
        "summary": "The aim of this research is twofold: Firstly, to model and solve a complex nurse scheduling problem with an integer programming formulation and evolutionary algorithms. Secondly, to detail a novel statistical method of comparing and hence building better scheduling algorithms by identifying successful algorithm modifications. The comparison method captures the results of algorithms in a single figure that can then be compared using traditional statistical techniques. Thus, the proposed method of comparing algorithms is an objective procedure designed to assist in the process of improving an algorithm. This is achieved even when some results are non-numeric or missing due to infeasibility. The final algorithm outperforms all previous evolutionary algorithms, which relied on human expertise for modification.",
        "published": "2008-03-20T11:15:37Z",
        "link": "http://arxiv.org/abs/0803.2967v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "An Indirect Genetic Algorithm for a Nurse Scheduling Problem",
        "authors": [
            "Uwe Aickelin",
            "Kathryn Dowsland"
        ],
        "summary": "This paper describes a Genetic Algorithms approach to a manpower-scheduling problem arising at a major UK hospital. Although Genetic Algorithms have been successfully used for similar problems in the past, they always had to overcome the limitations of the classical Genetic Algorithms paradigm in handling the conflict between objectives and constraints. The approach taken here is to use an indirect coding based on permutations of the nurses, and a heuristic decoder that builds schedules from these permutations. Computational experiments based on 52 weeks of live data are used to evaluate three different decoders with varying levels of intelligence, and four well-known crossover operators. Results are further enhanced by introducing a hybrid crossover operator and by making use of simple bounds to reduce the size of the solution space. The results reveal that the proposed algorithm is able to find high quality solutions and is both faster and more flexible than a recently published Tabu Search approach.",
        "published": "2008-03-20T11:21:19Z",
        "link": "http://arxiv.org/abs/0803.2969v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "A Recommender System based on Idiotypic Artificial Immune Networks",
        "authors": [
            "Steve Cayzer",
            "Uwe Aickelin"
        ],
        "summary": "The immune system is a complex biological system with a highly distributed, adaptive and self-organising nature. This paper presents an Artificial Immune System (AIS) that exploits some of these characteristics and is applied to the task of film recommendation by Collaborative Filtering (CF). Natural evolution and in particular the immune system have not been designed for classical optimisation. However, for this problem, we are not interested in finding a single optimum. Rather we intend to identify a sub-set of good matches on which recommendations can be based. It is our hypothesis that an AIS built on two central aspects of the biological immune system will be an ideal candidate to achieve this: Antigen-antibody interaction for matching and idiotypic antibody-antibody interaction for diversity. Computational results are presented in support of this conjecture and compared to those found by other CF techniques.",
        "published": "2008-03-20T11:27:56Z",
        "link": "http://arxiv.org/abs/0803.2970v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Rule Generalisation in Intrusion Detection Systems using Snort",
        "authors": [
            "Uwe Aickelin",
            "Jamie Twycross",
            "Thomas Hesketh-Roberts"
        ],
        "summary": "Intrusion Detection Systems (ids)provide an important layer of security for computer systems and networks, and are becoming more and more necessary as reliance on Internet services increases and systems with sensitive data are more commonly open to Internet access. An ids responsibility is to detect suspicious or unacceptable system and network activity and to alert a systems administrator to this activity. The majority of ids use a set of signatures that define what suspicious traffic is, and Snort is one popular and actively developing open-source ids that uses such a set of signatures known as Snort rules. Our aim is to identify a way in which Snort could be developed further by generalising rules to identify novel attacks. In particular, we attempted to relax and vary the conditions and parameters of current Snort rules, using a similar approach to classic rule learning operators such as generalisation and specialisation. We demonstrate the effectiveness of our approach through experiments with standard datasets and show that we are able to detect previously undeleted variants of various attacks. We conclude by discussing the general effectiveness and appropriateness of generalisation in Snort based ids rule processing.",
        "published": "2008-03-20T11:59:27Z",
        "link": "http://arxiv.org/abs/0803.2973v2",
        "categories": [
            "cs.NE",
            "cs.CR"
        ]
    },
    {
        "title": "An Estimation of Distribution Algorithm for Nurse Scheduling",
        "authors": [
            "Uwe Aickelin",
            "Jingpeng Li"
        ],
        "summary": "Schedules can be built in a similar way to a human scheduler by using a set of rules that involve domain knowledge. This paper presents an Estimation of Distribution Algorithm (eda) for the nurse scheduling problem, which involves choosing a suitable scheduling rule from a set for the assignment of each nurse. Unlike previous work that used Genetic Algorithms (ga) to implement implicit learning, the learning in the proposed algorithm is explicit, i.e. we identify and mix building blocks directly. The eda is applied to implement such explicit learning by building a Bayesian network of the joint distribution of solutions. The conditional probability of each variable in the network is computed according to an initial set of promising solutions. Subsequently, each new instance for each variable is generated by using the corresponding conditional probabilities, until all variables have been generated, i.e. in our case, a new rule string has been obtained. Another set of rule strings will be generated in this way, some of which will replace previous strings based on fitness selection. If stopping conditions are not met, the conditional probabilities for all nodes in the Bayesian network are updated again using the current set of promising rule strings. Computational results from 52 real data instances demonstrate the success of this approach. It is also suggested that the learning mechanism in the proposed approach might be suitable for other scheduling problems.",
        "published": "2008-03-20T12:07:26Z",
        "link": "http://arxiv.org/abs/0803.2975v2",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Idiotypic Immune Networks in Mobile Robot Control",
        "authors": [
            "Amanda Whitbrook",
            "Uwe Aickelin",
            "Jonathan Garibaldi"
        ],
        "summary": "Jerne's idiotypic network theory postulates that the immune response involves inter-antibody stimulation and suppression as well as matching to antigens. The theory has proved the most popular Artificial Immune System (ais) model for incorporation into behavior-based robotics but guidelines for implementing idiotypic selection are scarce. Furthermore, the direct effects of employing the technique have not been demonstrated in the form of a comparison with non-idiotypic systems. This paper aims to address these issues. A method for integrating an idiotypic ais network with a Reinforcement Learning based control system (rl) is described and the mechanisms underlying antibody stimulation and suppression are explained in detail. Some hypotheses that account for the network advantage are put forward and tested using three systems with increasing idiotypic complexity. The basic rl, a simplified hybrid ais-rl that implements idiotypic selection independently of derived concentration levels and a full hybrid ais-rl scheme are examined. The test bed takes the form of a simulated Pioneer robot that is required to navigate through maze worlds detecting and tracking door markers.",
        "published": "2008-03-20T12:24:43Z",
        "link": "http://arxiv.org/abs/0803.2981v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.RO"
        ]
    },
    {
        "title": "Towards a human eye behavior model by applying Data Mining Techniques on   Gaze Information from IEC",
        "authors": [
            "Denis Pallez",
            "Laurent Brisson",
            "Thierry Baccino"
        ],
        "summary": "In this paper, we firstly present what is Interactive Evolutionary Computation (IEC) and rapidly how we have combined this artificial intelligence technique with an eye-tracker for visual optimization. Next, in order to correctly parameterize our application, we present results from applying data mining techniques on gaze information coming from experiments conducted on about 80 human individuals.",
        "published": "2008-03-21T15:38:25Z",
        "link": "http://arxiv.org/abs/0803.3186v1",
        "categories": [
            "cs.HC",
            "cs.NE"
        ]
    },
    {
        "title": "Reinforcement Learning by Value Gradients",
        "authors": [
            "Michael Fairbank"
        ],
        "summary": "The concept of the value-gradient is introduced and developed in the context of reinforcement learning. It is shown that by learning the value-gradients exploration or stochastic behaviour is no longer needed to find locally optimal trajectories. This is the main motivation for using value-gradients, and it is argued that learning value-gradients is the actual objective of any value-function learning algorithm for control problems. It is also argued that learning value-gradients is significantly more efficient than learning just the values, and this argument is supported in experiments by efficiency gains of several orders of magnitude, in several problem domains. Once value-gradients are introduced into learning, several analyses become possible. For example, a surprising equivalence between a value-gradient learning algorithm and a policy-gradient learning algorithm is proven, and this provides a robust convergence proof for control problems using a value function with a general function approximator.",
        "published": "2008-03-25T11:57:15Z",
        "link": "http://arxiv.org/abs/0803.3539v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Cluster Approach to the Domains Formation",
        "authors": [
            "Leonid B. Litinskii"
        ],
        "summary": "As a rule, a quadratic functional depending on a great number of binary variables has a lot of local minima. One of approaches allowing one to find in averaged deeper local minima is aggregation of binary variables into larger blocks/domains. To minimize the functional one has to change the states of aggregated variables (domains). In the present publication we discuss methods of domains formation. It is shown that the best results are obtained when domains are formed by variables that are strongly connected with each other.",
        "published": "2008-03-26T15:14:33Z",
        "link": "http://arxiv.org/abs/0803.3746v1",
        "categories": [
            "cs.NE",
            "cs.DS"
        ]
    },
    {
        "title": "Recorded Step Directional Mutation for Faster Convergence",
        "authors": [
            "Ted Dunning"
        ],
        "summary": "Two meta-evolutionary optimization strategies described in this paper accelerate the convergence of evolutionary programming algorithms while still retaining much of their ability to deal with multi-modal problems. The strategies, called directional mutation and recorded step in this paper, can operate independently but together they greatly enhance the ability of evolutionary programming algorithms to deal with fitness landscapes characterized by long narrow valleys. The directional mutation aspect of this combined method uses correlated meta-mutation but does not introduce a full covariance matrix. These new methods are thus much more economical in terms of storage for problems with high dimensionality. Additionally, directional mutation is rotationally invariant which is a substantial advantage over self-adaptive methods which use a single variance per coordinate for problems where the natural orientation of the problem is not oriented along the axes.",
        "published": "2008-03-26T22:49:40Z",
        "link": "http://arxiv.org/abs/0803.3838v2",
        "categories": [
            "cs.NE",
            "cs.LG",
            "D.1.m; G.4"
        ]
    },
    {
        "title": "A Component Based Heuristic Search method with Adaptive Perturbations   for Hospital Personnel Scheduling",
        "authors": [
            "Jingpeng Li",
            "Uwe Aickelin",
            "Edmund Burke"
        ],
        "summary": "Nurse rostering is a complex scheduling problem that affects hospital personnel on a daily basis all over the world. This paper presents a new component-based approach with adaptive perturbations, for a nurse scheduling problem arising at a major UK hospital. The main idea behind this technique is to decompose a schedule into its components (i.e. the allocated shift pattern of each nurse), and then mimic a natural evolutionary process on these components to iteratively deliver better schedules. The worthiness of all components in the schedule has to be continuously demonstrated in order for them to remain there. This demonstration employs a dynamic evaluation function which evaluates how well each component contributes towards the final objective. Two perturbation steps are then applied: the first perturbation eliminates a number of components that are deemed not worthy to stay in the current schedule; the second perturbation may also throw out, with a low level of probability, some worthy components. The eliminated components are replenished with new ones using a set of constructive heuristics using local optimality criteria. Computational results using 52 data instances demonstrate the applicability of the proposed approach in solving real-world problems.",
        "published": "2008-03-27T12:15:43Z",
        "link": "http://arxiv.org/abs/0803.3900v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Introduction to Multi-Agent Simulation",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin"
        ],
        "summary": "When designing systems that are complex, dynamic and stochastic in nature, simulation is generally recognised as one of the best design support technologies, and a valuable aid in the strategic and tactical decision making process. A simulation model consists of a set of rules that define how a system changes over time, given its current state. Unlike analytical models, a simulation model is not solved but is run and the changes of system states can be observed at any point in time. This provides an insight into system dynamics rather than just predicting the output of a system based on specific inputs. Simulation is not a decision making tool but a decision support tool, allowing better informed decisions to be made. Due to the complexity of the real world, a simulation model can only be an approximation of the target system. The essence of the art of simulation modelling is abstraction and simplification. Only those characteristics that are important for the study and analysis of the target system should be included in the simulation model.",
        "published": "2008-03-27T12:38:17Z",
        "link": "http://arxiv.org/abs/0803.3905v1",
        "categories": [
            "cs.NE",
            "cs.MA"
        ]
    },
    {
        "title": "Artificial Immune Systems Tutorial",
        "authors": [
            "Uwe Aickelin",
            "Dipankar Dasgupta"
        ],
        "summary": "The biological immune system is a robust, complex, adaptive system that defends the body from foreign pathogens. It is able to categorize all cells (or molecules) within the body as self-cells or non-self cells. It does this with the help of a distributed task force that has the intelligence to take action from a local and also a global perspective using its network of chemical messengers for communication. There are two major branches of the immune system. The innate immune system is an unchanging mechanism that detects and destroys certain invading organisms, whilst the adaptive immune system responds to previously unknown foreign cells and builds a response to them that can remain in the body over a long period of time. This remarkable information processing biological system has caught the attention of computer science in recent years. A novel computational intelligence technique, inspired by immunology, has emerged, called Artificial Immune Systems. Several concepts from the immune have been extracted and applied for solution to real world science and engineering problems. In this tutorial, we briefly describe the immune system metaphors that are relevant to existing Artificial Immune Systems methods. We will then show illustrative real-world problems suitable for Artificial Immune Systems and give a step-by-step algorithm walkthrough for one such problem. A comparison of the Artificial Immune Systems to other well-known algorithms, areas for future work, tips & tricks and a list of resources will round this tutorial off. It should be noted that as Artificial Immune Systems is still a young and evolving field, there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from time to time and from those examples given here.",
        "published": "2008-03-27T12:55:59Z",
        "link": "http://arxiv.org/abs/0803.3912v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Neutral Fitness Landscape in the Cellular Automata Majority Problem",
        "authors": [
            "Sébastien Verel",
            "Philippe Collard",
            "Marco Tomassini",
            "Leonardo Vanneschi"
        ],
        "summary": "We study in detail the fitness landscape of a difficult cellular automata computational task: the majority problem. Our results show why this problem landscape is so hard to search, and we quantify the large degree of neutrality found in various ways. We show that a particular subspace of the solution space, called the \"Olympus\", is where good solutions concentrate, and give measures to quantitatively characterize this subspace.",
        "published": "2008-03-29T07:50:24Z",
        "link": "http://arxiv.org/abs/0803.4240v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Evolving Dynamic Change and Exchange of Genotype Encoding in Genetic   Algorithms for Difficult Optimization Problems",
        "authors": [
            "Maroun Bercachi",
            "Philippe Collard",
            "Manuel Clergue",
            "Sébastien Verel"
        ],
        "summary": "The application of genetic algorithms (GAs) to many optimization problems in organizations often results in good performance and high quality solutions. For successful and efficient use of GAs, it is not enough to simply apply simple GAs (SGAs). In addition, it is necessary to find a proper representation for the problem and to develop appropriate search operators that fit well to the properties of the genotype encoding. The representation must at least be able to encode all possible solutions of an optimization problem, and genetic operators such as crossover and mutation should be applicable to it. In this paper, serial alternation strategies between two codings are formulated in the framework of dynamic change of genotype encoding in GAs for function optimization. Likewise, a new variant of GAs for difficult optimization problems denoted {\\it Split-and-Merge} GA (SM-GA) is developed using a parallel implementation of an SGA and evolving a dynamic exchange of individual representation in the context of Dual Coding concept. Numerical experiments show that the evolved SM-GA significantly outperforms an SGA with static single coding.",
        "published": "2008-03-29T07:51:18Z",
        "link": "http://arxiv.org/abs/0803.4241v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "From Cells to Islands: An unified Model of Cellular Parallel Genetic   Algorithms",
        "authors": [
            "David Simoncini",
            "Philippe Collard",
            "Sébastien Verel",
            "Manuel Clergue"
        ],
        "summary": "This paper presents the Anisotropic selection scheme for cellular Genetic Algorithms (cGA). This new scheme allows to enhance diversity and to control the selective pressure which are two important issues in Genetic Algorithms, especially when trying to solve difficult optimization problems. Varying the anisotropic degree of selection allows swapping from a cellular to an island model of parallel genetic algorithm. Measures of performances and diversity have been performed on one well-known problem: the Quadratic Assignment Problem which is known to be difficult to optimize. Experiences show that, tuning the anisotropic degree, we can find the accurate trade-off between cGA and island models to optimize performances of parallel evolutionary algorithms. This trade-off can be interpreted as the suitable degree of migration among subpopulations in a parallel Genetic Algorithm.",
        "published": "2008-03-29T08:20:58Z",
        "link": "http://arxiv.org/abs/0803.4248v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Permeability Analysis based on information granulation theory",
        "authors": [
            "M. Sharifzadeh",
            "H. Owladeghaffari",
            "K. Shahriar",
            "E. Bakhtavar"
        ],
        "summary": "This paper describes application of information granulation theory, on the analysis of \"lugeon data\". In this manner, using a combining of Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and fuzzy granules are obtained. Balancing of crisp granules and sub- fuzzy granules, within non fuzzy information (initial granulation), is rendered in open-close iteration. Using two criteria, \"simplicity of rules \"and \"suitable adaptive threshold error level\", stability of algorithm is guaranteed. In other part of paper, rough set theory (RST), to approximate analysis, has been employed >.Validation of the proposed methods, on the large data set of in-situ permeability in rock masses, in the Shivashan dam, Iran, has been highlighted. By the implementation of the proposed algorithm on the lugeon data set, was proved the suggested method, relating the approximate analysis on the permeability, could be applied.",
        "published": "2008-04-02T13:45:51Z",
        "link": "http://arxiv.org/abs/0804.0352v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "F.4.1; K.3.2"
        ]
    },
    {
        "title": "Graphical Estimation of Permeability Using RST&NFIS",
        "authors": [
            "H. Owladeghaffari",
            "K. Shahriar W. Pedrycz"
        ],
        "summary": "This paper pursues some applications of Rough Set Theory (RST) and neural-fuzzy model to analysis of \"lugeon data\". In the manner, using Self Organizing Map (SOM) as a pre-processing the data are scaled and then the dominant rules by RST, are elicited. Based on these rules variations of permeability in the different levels of Shivashan dam, Iran has been highlighted. Then, via using a combining of SOM and an adaptive Neuro-Fuzzy Inference System (NFIS) another analysis on the data was carried out. Finally, a brief comparison between the obtained results of RST and SOM-NFIS (briefly SONFIS) has been rendered.",
        "published": "2008-04-02T13:56:10Z",
        "link": "http://arxiv.org/abs/0804.0353v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Bayesian Optimisation Algorithm for Nurse Scheduling",
        "authors": [
            "Jingpeng Li",
            "Uwe Aickelin"
        ],
        "summary": "Our research has shown that schedules can be built mimicking a human scheduler by using a set of rules that involve domain knowledge. This chapter presents a Bayesian Optimization Algorithm (BOA) for the nurse scheduling problem that chooses such suitable scheduling rules from a set for each nurses assignment. Based on the idea of using probabilistic models, the BOA builds a Bayesian network for the set of promising solutions and samples these networks to generate new candidate solutions. Computational results from 52 real data instances demonstrate the success of this approach. It is also suggested that the learning mechanism in the proposed algorithm may be suitable for other scheduling problems.",
        "published": "2008-04-03T11:14:11Z",
        "link": "http://arxiv.org/abs/0804.0524v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "An Artificial Immune System as a Recommender System for Web Sites",
        "authors": [
            "Tom Morrison",
            "Uwe Aickelin"
        ],
        "summary": "Artificial Immune Systems have been used successfully to build recommender systems for film databases. In this research, an attempt is made to extend this idea to web site recommendation. A collection of more than 1000 individuals web profiles (alternatively called preferences / favourites / bookmarks file) will be used. URLs will be classified using the DMOZ (Directory Mozilla) database of the Open Directory Project as our ontology. This will then be used as the data for the Artificial Immune Systems rather than the actual addresses. The first attempt will involve using a simple classification code number coupled with the number of pages within that classification code. However, this implementation does not make use of the hierarchical tree-like structure of DMOZ. Consideration will then be given to the construction of a similarity measure for web profiles that makes use of this hierarchical information to build a better-informed Artificial Immune System.",
        "published": "2008-04-03T14:43:44Z",
        "link": "http://arxiv.org/abs/0804.0573v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Explicit Learning: an Effort towards Human Scheduling Algorithms",
        "authors": [
            "Jingpeng Li",
            "Uwe Aickelin"
        ],
        "summary": "Scheduling problems are generally NP-hard combinatorial problems, and a lot of research has been done to solve these problems heuristically. However, most of the previous approaches are problem-specific and research into the development of a general scheduling algorithm is still in its infancy.   Mimicking the natural evolutionary process of the survival of the fittest, Genetic Algorithms (GAs) have attracted much attention in solving difficult scheduling problems in recent years. Some obstacles exist when using GAs: there is no canonical mechanism to deal with constraints, which are commonly met in most real-world scheduling problems, and small changes to a solution are difficult. To overcome both difficulties, indirect approaches have been presented (in [1] and [2]) for nurse scheduling and driver scheduling, where GAs are used by mapping the solution space, and separate decoding routines then build solutions to the original problem.",
        "published": "2008-04-03T15:31:52Z",
        "link": "http://arxiv.org/abs/0804.0580v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Prospective Algorithms for Quantum Evolutionary Computation",
        "authors": [
            "Donald A. Sofge"
        ],
        "summary": "This effort examines the intersection of the emerging field of quantum computing and the more established field of evolutionary computation. The goal is to understand what benefits quantum computing might offer to computational intelligence and how computational intelligence paradigms might be implemented as quantum programs to be run on a future quantum computer. We critically examine proposed algorithms and methods for implementing computational intelligence paradigms, primarily focused on heuristic optimization methods including and related to evolutionary computation, with particular regard for their potential for eventual implementation on quantum computing hardware.",
        "published": "2008-04-07T20:11:24Z",
        "link": "http://arxiv.org/abs/0804.1133v1",
        "categories": [
            "cs.NE",
            "I.2.8"
        ]
    },
    {
        "title": "Immune System Approaches to Intrusion Detection - A Review",
        "authors": [
            "Jungwon Kim",
            "Peter J. Bentley",
            "Uwe Aickelin",
            "Julie Greensmith",
            "Gianni Tedesco",
            "Jamie Twycross"
        ],
        "summary": "The use of artificial immune systems in intrusion detection is an appealing concept for two reasons. Firstly, the human immune system provides the human body with a high level of protection from invading pathogens, in a robust, self-organised and distributed manner. Secondly, current techniques used in computer security are not able to cope with the dynamic and increasingly complex nature of computer systems and their security. It is hoped that biologically inspired approaches in this area, including the use of immune-based systems will be able to meet this challenge. Here we review the algorithms used, the development of the systems and the outcome of their implementation. We provide an introduction and analysis of the key developments within this field, in addition to making suggestions for future research.",
        "published": "2008-04-08T11:44:38Z",
        "link": "http://arxiv.org/abs/0804.1266v1",
        "categories": [
            "cs.NE",
            "cs.CR"
        ]
    },
    {
        "title": "Data Reduction in Intrusion Alert Correlation",
        "authors": [
            "Gianni Tedesco",
            "Uwe Aickelin"
        ],
        "summary": "Network intrusion detection sensors are usually built around low level models of network traffic. This means that their output is of a similarly low level and as a consequence, is difficult to analyze. Intrusion alert correlation is the task of automating some of this analysis by grouping related alerts together. Attack graphs provide an intuitive model for such analysis. Unfortunately alert flooding attacks can still cause a loss of service on sensors, and when performing attack graph correlation, there can be a large number of extraneous alerts included in the output graph. This obscures the fine structure of genuine attacks and makes them more difficult for human operators to discern. This paper explores modified correlation algorithms which attempt to minimize the impact of this attack.",
        "published": "2008-04-08T14:15:34Z",
        "link": "http://arxiv.org/abs/0804.1281v2",
        "categories": [
            "cs.CR",
            "cs.NE"
        ]
    },
    {
        "title": "Phoneme recognition in TIMIT with BLSTM-CTC",
        "authors": [
            "Santiago Fernández",
            "Alex Graves",
            "Juergen Schmidhuber"
        ],
        "summary": "We compare the performance of a recurrent neural network with the best results published so far on phoneme recognition in the TIMIT database. These published results have been obtained with a combination of classifiers. However, in this paper we apply a single recurrent neural network to the same task. Our recurrent neural network attains an error rate of 24.6%. This result is not significantly different from that obtained by the other best methods, but they rely on a combination of classifiers for achieving comparable performance.",
        "published": "2008-04-21T15:38:45Z",
        "link": "http://arxiv.org/abs/0804.3269v1",
        "categories": [
            "cs.CL",
            "cs.NE",
            "I.2.7; I.5.4"
        ]
    },
    {
        "title": "Logic Mining Using Neural Networks",
        "authors": [
            "Saratha Sathasivam",
            "Wan Ahmad Tajuddin Wan Abdullah"
        ],
        "summary": "Knowledge could be gained from experts, specialists in the area of interest, or it can be gained by induction from sets of data. Automatic induction of knowledge from data sets, usually stored in large databases, is called data mining. Data mining methods are important in the management of complex systems. There are many technologies available to data mining practitioners, including Artificial Neural Networks, Regression, and Decision Trees. Neural networks have been successfully applied in wide range of supervised and unsupervised learning applications. Neural network methods are not commonly used for data mining tasks, because they often produce incomprehensible models, and require long training times. One way in which the collective properties of a neural network may be used to implement a computational task is by way of the concept of energy minimization. The Hopfield network is well-known example of such an approach. The Hopfield network is useful as content addressable memory or an analog computer for solving combinatorial-type optimization problems. Wan Abdullah [1] proposed a method of doing logic programming on a Hopfield neural network. Optimization of logical inconsistency is carried out by the network after the connection strengths are defined from the logic program; the network relaxes to neural states corresponding to a valid interpretation. In this article, we describe how Hopfield network is able to induce logical rules from large database by using reverse analysis method: given the values of the connections of a network, we can hope to know what logical rules are entrenched in the database.",
        "published": "2008-04-25T09:30:28Z",
        "link": "http://arxiv.org/abs/0804.4071v1",
        "categories": [
            "cs.LO",
            "cs.NE"
        ]
    },
    {
        "title": "Logic Learning in Hopfield Networks",
        "authors": [
            "Saratha Sathasivam",
            "Wan Ahmad Tajuddin Wan Abdullah"
        ],
        "summary": "Synaptic weights for neurons in logic programming can be calculated either by using Hebbian learning or by Wan Abdullah's method. In other words, Hebbian learning for governing events corresponding to some respective program clauses is equivalent with learning using Wan Abdullah's method for the same respective program clauses. In this paper we will evaluate experimentally the equivalence between these two types of learning through computer simulations.",
        "published": "2008-04-25T09:46:46Z",
        "link": "http://arxiv.org/abs/0804.4075v1",
        "categories": [
            "cs.LO",
            "cs.NE"
        ]
    },
    {
        "title": "Explaining the Logical Nature of Electrical Solitons in Neural Circuits",
        "authors": [
            "John Robert Burger"
        ],
        "summary": "Neurons are modeled electrically based on ferroelectric membranes thin enough to permit charge transfer, conjectured to be the tunneling result of thermally energetic ions and random electrons. These membranes can be triggered to produce electrical solitons, the main signals for brain associative memory and logical processing. Dendritic circuits are modeled, and electrical solitons are simulated to demonstrate the nature of soliton propagation, soliton reflection, the collision of solitons, as well as soliton OR gates, AND gates, XOR gates and NOT gates.",
        "published": "2008-04-26T16:55:59Z",
        "link": "http://arxiv.org/abs/0804.4237v1",
        "categories": [
            "cs.NE",
            "q-bio.NC",
            "F.1.1"
        ]
    },
    {
        "title": "Fast Density Codes for Image Data",
        "authors": [
            "Pierre Courrieu"
        ],
        "summary": "Recently, a new method for encoding data sets in the form of \"Density Codes\" was proposed in the literature (Courrieu, 2006). This method allows to compare sets of points belonging to every multidimensional space, and to build shape spaces invariant to a wide variety of affine and non-affine transformations. However, this general method does not take advantage of the special properties of image data, resulting in a quite slow encoding process that makes this tool practically unusable for processing large image databases with conventional computers. This paper proposes a very simple variant of the density code method that directly works on the image function, which is thousands times faster than the original Parzen window based method, without loss of its useful properties.",
        "published": "2008-04-29T14:25:30Z",
        "link": "http://arxiv.org/abs/0804.4622v3",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Network Structure and Dynamics, and Emergence of Robustness by   Stabilizing Selection in an Artificial Genome",
        "authors": [
            "Thimo Rohlf",
            "Chris Winkler"
        ],
        "summary": "Genetic regulation is a key component in development, but a clear understanding of the structure and dynamics of genetic networks is not yet at hand. In this work we investigate these properties within an artificial genome model originally introduced by Reil. We analyze statistical properties of randomly generated genomes both on the sequence- and network level, and show that this model correctly predicts the frequency of genes in genomes as found in experimental data. Using an evolutionary algorithm based on stabilizing selection for a phenotype, we show that robustness against single base mutations, as well as against random changes in initial network states that mimic stochastic fluctuations in environmental conditions, can emerge in parallel. Evolved genomes exhibit characteristic patterns on both sequence and network level.",
        "published": "2008-04-30T01:05:33Z",
        "link": "http://arxiv.org/abs/0804.4714v1",
        "categories": [
            "q-bio.MN",
            "cond-mat.dis-nn",
            "cs.NE",
            "q-bio.GN",
            "q-bio.PE"
        ]
    },
    {
        "title": "Solving Time of Least Square Systems in Sigma-Pi Unit Networks",
        "authors": [
            "Pierre Courrieu"
        ],
        "summary": "The solving of least square systems is a useful operation in neurocomputational modeling of learning, pattern matching, and pattern recognition. In these last two cases, the solution must be obtained on-line, thus the time required to solve a system in a plausible neural architecture is critical. This paper presents a recurrent network of Sigma-Pi neurons, whose solving time increases at most like the logarithm of the system size, and of its condition number, which provides plausible computation times for biological systems.",
        "published": "2008-04-30T12:23:05Z",
        "link": "http://arxiv.org/abs/0804.4808v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Fast Computation of Moore-Penrose Inverse Matrices",
        "authors": [
            "Pierre Courrieu"
        ],
        "summary": "Many neural learning algorithms require to solve large least square systems in order to obtain synaptic weights. Moore-Penrose inverse matrices allow for solving such systems, even with rank deficiency, and they provide minimum-norm vectors of synaptic weights, which contribute to the regularization of the input-output mapping. It is thus of interest to develop fast and accurate algorithms for computing Moore-Penrose inverse matrices. In this paper, an algorithm based on a full rank Cholesky factorization is proposed. The resulting pseudoinverse matrices are similar to those provided by other algorithms. However the computation time is substantially shorter, particularly for large systems.",
        "published": "2008-04-30T12:24:54Z",
        "link": "http://arxiv.org/abs/0804.4809v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Flatness of the Energy Landscape for Horn Clauses",
        "authors": [
            "Saratha Sathasivam",
            "Wan Ahmad Tajuddin Wan Abdullah"
        ],
        "summary": "The Little-Hopfield neural network programmed with Horn clauses is studied. We argue that the energy landscape of the system, corresponding to the inconsistency function for logical interpretations of the sets of Horn clauses, has minimal ruggedness. This is supported by computer simulations.",
        "published": "2008-05-02T09:20:11Z",
        "link": "http://arxiv.org/abs/0805.0197v1",
        "categories": [
            "cond-mat.dis-nn",
            "cs.NE"
        ]
    },
    {
        "title": "CMA-ES with Two-Point Step-Size Adaptation",
        "authors": [
            "Nikolaus Hansen"
        ],
        "summary": "We combine a refined version of two-point step-size adaptation with the covariance matrix adaptation evolution strategy (CMA-ES). Additionally, we suggest polished formulae for the learning rate of the covariance matrix and the recombination weights. In contrast to cumulative step-size adaptation or to the 1/5-th success rule, the refined two-point adaptation (TPA) does not rely on any internal model of optimality. In contrast to conventional self-adaptation, the TPA will achieve a better target step-size in particular with large populations. The disadvantage of TPA is that it relies on two additional objective function",
        "published": "2008-05-02T13:55:37Z",
        "link": "http://arxiv.org/abs/0805.0231v4",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Stochastic Optimization Approaches for Solving Sudoku",
        "authors": [
            "Meir Perez",
            "Tshilidzi Marwala"
        ],
        "summary": "In this paper the Sudoku problem is solved using stochastic search techniques and these are: Cultural Genetic Algorithm (CGA), Repulsive Particle Swarm Optimization (RPSO), Quantum Simulated Annealing (QSA) and the Hybrid method that combines Genetic Algorithm with Simulated Annealing (HGASA). The results obtained show that the CGA, QSA and HGASA are able to solve the Sudoku puzzle with CGA finding a solution in 28 seconds, while QSA finding a solution in 65 seconds and HGASA in 1.447 seconds. This is mainly because HGASA combines the parallel searching of GA with the flexibility of SA. The RPSO was found to be unable to solve the puzzle.",
        "published": "2008-05-06T11:06:49Z",
        "link": "http://arxiv.org/abs/0805.0697v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Contact state analysis using NFIS and SOM",
        "authors": [
            "H. Owladeghaffari"
        ],
        "summary": "This paper reports application of neuro- fuzzy inference system (NFIS) and self organizing feature map neural networks (SOM) on detection of contact state in a block system. In this manner, on a simple system, the evolution of contact states, by parallelization of DDA, has been investigated. So, a comparison between NFIS and SOM results has been presented. The results show applicability of the proposed methods, by different accuracy, on detection of contact's distribution.",
        "published": "2008-05-08T12:10:21Z",
        "link": "http://arxiv.org/abs/0805.1153v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Clustering of scientific citations in Wikipedia",
        "authors": [
            "Finn Aarup Nielsen"
        ],
        "summary": "The instances of templates in Wikipedia form an interesting data set of structured information. Here I focus on the cite journal template that is primarily used for citation to articles in scientific journals. These citations can be extracted and analyzed: Non-negative matrix factorization is performed on a (article x journal) matrix resulting in a soft clustering of Wikipedia articles and scientific journals, each cluster more or less representing a scientific topic.",
        "published": "2008-05-08T12:29:36Z",
        "link": "http://arxiv.org/abs/0805.1154v2",
        "categories": [
            "cs.DL",
            "cs.NE",
            "G.1.10; G.2.3; H.2.8"
        ]
    },
    {
        "title": "A Simple Dynamic Mind-map Framework To Discover Associative   Relationships in Transactional Data Streams",
        "authors": [
            "Christoph Schommer"
        ],
        "summary": "In this paper, we informally introduce dynamic mind-maps that represent a new approach on the basis of a dynamic construction of connectionist structures during the processing of a data stream. This allows the representation and processing of recursively defined structures and avoids the problem of a more traditional, fixed-size architecture with the processing of input structures of unknown size. For a data stream analysis with association discovery, the incremental analysis of data leads to results on demand. Here, we describe a framework that uses symbolic cells to calculate associations based on transactional data streams as it exists in e.g. bibliographic databases. We follow a natural paradigm of applying simple operations on cells yielding on a mind-map structure that adapts over time.",
        "published": "2008-05-09T08:10:09Z",
        "link": "http://arxiv.org/abs/0805.1296v1",
        "categories": [
            "cs.NE",
            "cs.SC",
            "I.2.6; H.2.8"
        ]
    },
    {
        "title": "Emergence, Competition and Dynamical Stabilization of Dissipative   Rotating Spiral Waves in an Excitable Medium: A Computational Model Based on   Cellular Automata",
        "authors": [
            "S. D. Makovetskiy",
            "D. N. Makovetskii"
        ],
        "summary": "We report some qualitatively new features of emergence, competition and dynamical stabilization of dissipative rotating spiral waves (RSWs) in the cellular-automaton model of laser-like excitable media proposed in arXiv:cond-mat/0410460v2 ; arXiv:cond-mat/0602345 . Part of the observed features are caused by unusual mechanism of excitation vorticity when the RSW's core get into the surface layer of an active medium. Instead of the well known scenario of RSW collapse, which takes place after collision of RSW's core with absorbing boundary, we observed complicated transformations of the core leading to regeneration (nonlinear \"reflection\" from the boundary) of the RSW or even to birth of several new RSWs in the surface layer. Computer experiments on bottlenecked evolution of such the RSW-ensembles (vortex matter) are reported and a possible explanation of real experiments on spin-lattice relaxation in dilute paramagnets is proposed on the basis of an analysis of the RSWs dynamics. Chimera states in RSW-ensembles are revealed and compared with analogous states in ensembles of nonlocally coupled oscillators. Generally, our computer experiments have shown that vortex matter states in laser-like excitable media have some important features of aggregate states of the usual matter.",
        "published": "2008-05-09T10:15:28Z",
        "link": "http://arxiv.org/abs/0805.1319v1",
        "categories": [
            "nlin.CG",
            "cs.NE",
            "nlin.AO"
        ]
    },
    {
        "title": "Grammatical Evolution with Restarts for Fast Fractal Generation",
        "authors": [
            "Manuel Cebrian",
            "Manuel Alfonseca",
            "Alfonso Ortega"
        ],
        "summary": "In a previous work, the authors proposed a Grammatical Evolution algorithm to automatically generate Lindenmayer Systems which represent fractal curves with a pre-determined fractal dimension. This paper gives strong statistical evidence that the probability distributions of the execution time of that algorithm exhibits a heavy tail with an hyperbolic probability decay for long executions, which explains the erratic performance of different executions of the algorithm. Three different restart strategies have been incorporated in the algorithm to mitigate the problems associated to heavy tail distributions: the first assumes full knowledge of the execution time probability distribution, the second and third assume no knowledge. These strategies exploit the fact that the probability of finding a solution in short executions is non-negligible and yield a severe reduction, both in the expected execution time (up to one order of magnitude) and in its variance, which is reduced from an infinite to a finite value.",
        "published": "2008-05-12T17:55:59Z",
        "link": "http://arxiv.org/abs/0805.1696v2",
        "categories": [
            "cs.NE",
            "cs.SC"
        ]
    },
    {
        "title": "Cognitive Architecture for Direction of Attention Founded on Subliminal   Memory Searches, Pseudorandom and Nonstop",
        "authors": [
            "J. R. Burger"
        ],
        "summary": "By way of explaining how a brain works logically, human associative memory is modeled with logical and memory neurons, corresponding to standard digital circuits. The resulting cognitive architecture incorporates basic psychological elements such as short term and long term memory. Novel to the architecture are memory searches using cues chosen pseudorandomly from short term memory. Recalls alternated with sensory images, many tens per second, are analyzed subliminally as an ongoing process, to determine a direction of attention in short term memory.",
        "published": "2008-05-20T17:37:31Z",
        "link": "http://arxiv.org/abs/0805.3126v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.0; C.1.3"
        ]
    },
    {
        "title": "An Evolutionary-Based Approach to Learning Multiple Decision Models from   Underrepresented Data",
        "authors": [
            "Vitaly Schetinin",
            "Dayou Li",
            "Carsten Maple"
        ],
        "summary": "The use of multiple Decision Models (DMs) enables to enhance the accuracy in decisions and at the same time allows users to evaluate the confidence in decision making. In this paper we explore the ability of multiple DMs to learn from a small amount of verified data. This becomes important when data samples are difficult to collect and verify. We propose an evolutionary-based approach to solving this problem. The proposed technique is examined on a few clinical problems presented by a small amount of data.",
        "published": "2008-05-24T23:37:51Z",
        "link": "http://arxiv.org/abs/0805.3800v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Neural network learning of optimal Kalman prediction and control",
        "authors": [
            "Ralph Linsker"
        ],
        "summary": "Although there are many neural network (NN) algorithms for prediction and for control, and although methods for optimal estimation (including filtering and prediction) and for optimal control in linear systems were provided by Kalman in 1960 (with nonlinear extensions since then), there has been, to my knowledge, no NN algorithm that learns either Kalman prediction or Kalman control (apart from the special case of stationary control). Here we show how optimal Kalman prediction and control (KPC), as well as system identification, can be learned and executed by a recurrent neural network composed of linear-response nodes, using as input only a stream of noisy measurement data.   The requirements of KPC appear to impose significant constraints on the allowed NN circuitry and signal flows. The NN architecture implied by these constraints bears certain resemblances to the local-circuit architecture of mammalian cerebral cortex. We discuss these resemblances, as well as caveats that limit our current ability to draw inferences for biological function. It has been suggested that the local cortical circuit (LCC) architecture may perform core functions (as yet unknown) that underlie sensory, motor,and other cortical processing. It is reasonable to conjecture that such functions may include prediction, the estimation or inference of missing or noisy sensory data, and the goal-driven generation of control signals. The resemblances found between the KPC NN architecture and that of the LCC are consistent with this conjecture.",
        "published": "2008-05-28T01:57:11Z",
        "link": "http://arxiv.org/abs/0805.4247v1",
        "categories": [
            "cs.NE",
            "C.1.3; C.3"
        ]
    },
    {
        "title": "MultiKulti Algorithm: Migrating the Most Different Genotypes in an   Island Model",
        "authors": [
            "Lourdes Araujo",
            "Juan J. Merelo Guervos",
            "Carlos Cotta",
            "Francisco Fernandez de Vega"
        ],
        "summary": "Migration policies in distributed evolutionary algorithms has not been an active research area until recently. However, in the same way as operators have an impact on performance, the choice of migrants is due to have an impact too. In this paper we propose a new policy (named multikulti) for choosing the individuals that are going to be sent to other nodes, based on multiculturality: the individual sent should be as different as possible to the receiving population. We have checked this policy on different discrete optimization problems, and found that, in average or in median, this policy outperforms classical ones like sending the best or a random individual.",
        "published": "2008-06-17T17:19:13Z",
        "link": "http://arxiv.org/abs/0806.2843v2",
        "categories": [
            "cs.NE",
            "cs.DC"
        ]
    },
    {
        "title": "Round Trip Time Prediction Using the Symbolic Function Network Approach",
        "authors": [
            "George S. Eskander",
            "Amir Atiya",
            "Kil To Chong",
            "Hyongsuk Kim",
            "Sung Goo Yoo"
        ],
        "summary": "In this paper, we develop a novel approach to model the Internet round trip time using a recently proposed symbolic type neural network model called symbolic function network. The developed predictor is shown to have good generalization performance and simple representation compared to the multilayer perceptron based predictors.",
        "published": "2008-06-23T10:04:14Z",
        "link": "http://arxiv.org/abs/0806.3646v2",
        "categories": [
            "cs.NE",
            "cs.SC"
        ]
    },
    {
        "title": "Structural Damage Detection Using Randomized Trained Neural Networks",
        "authors": [
            "Ismoyo Haryanto",
            "Joga Dharma Setiawan",
            "Agus Budiyono"
        ],
        "summary": "A computationally method on damage detection problems in structures was conducted using neural networks. The problem that is considered in this works consists of estimating the existence, location and extent of stiffness reduction in structure which is indicated by the changes of the structural static parameters such as deflection and strain. The neural network was trained to recognize the behaviour of static parameter of the undamaged structure as well as of the structure with various possible damage extent and location which were modelled as random states. The proposed techniques were applied to detect damage in a simply supported beam. The structure was analyzed using finite-element-method (FEM) and the damage identification was conducted by a back-propagation neural network using the change of the structural strain and displacement. The results showed that using proposed method the strain is more efficient for identification of damage than the displacement.",
        "published": "2008-06-28T04:43:40Z",
        "link": "http://arxiv.org/abs/0806.4650v1",
        "categories": [
            "cs.NE",
            "I.2.8"
        ]
    },
    {
        "title": "A First-Order Non-Homogeneous Markov Model for the Response of Spiking   Neurons Stimulated by Small Phase-Continuous Signals",
        "authors": [
            "J. Tapson",
            "C. Jin",
            "A. van Schaik",
            "R. Etienne-Cummings"
        ],
        "summary": "We present a first-order non-homogeneous Markov model for the interspike-interval density of a continuously stimulated spiking neuron. The model allows the conditional interspike-interval density and the stationary interspike-interval density to be expressed as products of two separate functions, one of which describes only the neuron characteristics, and the other of which describes only the signal characteristics. This allows the use of this model to predict the response when the underlying neuron model is not known or well determined. The approximation shows particularly clearly that signal autocorrelations and cross-correlations arise as natural features of the interspike-interval density, and are particularly clear for small signals and moderate noise. We show that this model simplifies the design of spiking neuron cross-correlation systems, and describe a four-neuron mutual inhibition network that generates a cross-correlation output for two input signals.",
        "published": "2008-07-09T18:36:48Z",
        "link": "http://arxiv.org/abs/0807.1513v1",
        "categories": [
            "q-bio.NC",
            "cs.NE"
        ]
    },
    {
        "title": "Hardware/Software Co-Design for Spike Based Recognition",
        "authors": [
            "Arfan Ghani",
            "Martin McGinnity",
            "Liam Maguire",
            "Jim Harkin"
        ],
        "summary": "The practical applications based on recurrent spiking neurons are limited due to their non-trivial learning algorithms. The temporal nature of spiking neurons is more favorable for hardware implementation where signals can be represented in binary form and communication can be done through the use of spikes. This work investigates the potential of recurrent spiking neurons implementations on reconfigurable platforms and their applicability in temporal based applications. A theoretical framework of reservoir computing is investigated for hardware/software implementation. In this framework, only readout neurons are trained which overcomes the burden of training at the network level. These recurrent neural networks are termed as microcircuits which are viewed as basic computational units in cortical computation. This paper investigates the potential of recurrent neural reservoirs and presents a novel hardware/software strategy for their implementation on FPGAs. The design is implemented and the functionality is tested in the context of speech recognition application.",
        "published": "2008-07-14T23:44:47Z",
        "link": "http://arxiv.org/abs/0807.2282v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CE",
            "C.1.3"
        ]
    },
    {
        "title": "Visual Grouping by Neural Oscillators",
        "authors": [
            "Guoshen Yu",
            "Jean-Jacques Slotine"
        ],
        "summary": "Distributed synchronization is known to occur at several scales in the brain, and has been suggested as playing a key functional role in perceptual grouping. State-of-the-art visual grouping algorithms, however, seem to give comparatively little attention to neural synchronization analogies. Based on the framework of concurrent synchronization of dynamic systems, simple networks of neural oscillators coupled with diffusive connections are proposed to solve visual grouping problems. Multi-layer algorithms and feedback mechanisms are also studied. The same algorithm is shown to achieve promising results on several classical visual grouping problems, including point clustering, contour integration and image segmentation.",
        "published": "2008-07-18T11:23:27Z",
        "link": "http://arxiv.org/abs/0807.2928v1",
        "categories": [
            "cs.CV",
            "cs.NE"
        ]
    },
    {
        "title": "Resource Allocation of MU-OFDM Based Cognitive Radio Systems Under   Partial Channel State Information",
        "authors": [
            "Dong Huang",
            "Chunyan Miao",
            "Cyril Leung",
            "Zhiqi Shen"
        ],
        "summary": "This paper has been withdrawn by the author due to some errors.",
        "published": "2008-08-05T04:27:00Z",
        "link": "http://arxiv.org/abs/0808.0549v5",
        "categories": [
            "cs.IT",
            "cs.NE",
            "math.CO",
            "math.IT"
        ]
    },
    {
        "title": "Fitness Landscape Analysis for Dynamic Resource Allocation in Multiuser   OFDM Based Cognitive Radio Systems",
        "authors": [
            "Dong Huang",
            "Chunyan Miao",
            "Cyril Leung"
        ],
        "summary": "This paper has been withdrawn.",
        "published": "2008-08-07T11:07:22Z",
        "link": "http://arxiv.org/abs/0808.1000v3",
        "categories": [
            "cs.IT",
            "cs.NE",
            "math.CO",
            "math.IT"
        ]
    },
    {
        "title": "A Novel Symbolic Type Neural Network Model- Application to River Flow   Forecasting",
        "authors": [
            "George S. Eskander",
            "Amir F. Atiya"
        ],
        "summary": "In this paper we introduce a new symbolic type neural tree network called symbolic function network (SFN) that is based on using elementary functions to model systems in a symbolic form. The proposed formulation permits feature selection, functional selection, and flexible structure. We applied this model on the River Flow forecasting problem. The results found to be superior in both fitness and sparsity.",
        "published": "2008-08-09T22:05:48Z",
        "link": "http://arxiv.org/abs/0808.1378v1",
        "categories": [
            "cs.NE",
            "cs.SC"
        ]
    },
    {
        "title": "Principal Graphs and Manifolds",
        "authors": [
            "A. N. Gorban",
            "A. Y. Zinovyev"
        ],
        "summary": "In many physical, statistical, biological and other investigations it is desirable to approximate a system of points by objects of lower dimension and/or complexity. For this purpose, Karl Pearson invented principal component analysis in 1901 and found 'lines and planes of closest fit to system of points'. The famous k-means algorithm solves the approximation problem too, but by finite sets instead of lines and planes. This chapter gives a brief practical introduction into the methods of construction of general principal objects, i.e. objects embedded in the 'middle' of the multidimensional data set. As a basis, the unifying framework of mean squared distance approximation of finite datasets is selected. Principal graphs and manifolds are constructed as generalisations of principal components and k-means principal points. For this purpose, the family of expectation/maximisation algorithms with nearest generalisations is presented. Construction of principal graphs with controlled complexity is based on the graph grammar approach.",
        "published": "2008-09-02T18:04:53Z",
        "link": "http://arxiv.org/abs/0809.0490v2",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ]
    },
    {
        "title": "Approximating the volume of unions and intersections of high-dimensional   geometric objects",
        "authors": [
            "Karl Bringmann",
            "Tobias Friedrich"
        ],
        "summary": "We consider the computation of the volume of the union of high-dimensional geometric objects. While showing that this problem is #P-hard already for very simple bodies (i.e., axis-parallel boxes), we give a fast FPRAS for all objects where one can: (1) test whether a given point lies inside the object, (2) sample a point uniformly, (3) calculate the volume of the object in polynomial time. All three oracles can be weak, that is, just approximate. This implies that Klee's measure problem and the hypervolume indicator can be approximated efficiently even though they are #P-hard and hence cannot be solved exactly in time polynomial in the number of dimensions unless P=NP. Our algorithm also allows to approximate efficiently the volume of the union of convex bodies given by weak membership oracles.   For the analogous problem of the intersection of high-dimensional geometric objects we prove #P-hardness for boxes and show that there is no multiplicative polynomial-time $2^{d^{1-\\epsilon}}$-approximation for certain boxes unless NP=BPP, but give a simple additive polynomial-time $\\epsilon$-approximation.",
        "published": "2008-09-04T16:14:09Z",
        "link": "http://arxiv.org/abs/0809.0835v2",
        "categories": [
            "cs.CG",
            "cs.NE"
        ]
    },
    {
        "title": "State dependent computation using coupled recurrent networks",
        "authors": [
            "Ueli Rutishauser",
            "Rodney J. Douglas"
        ],
        "summary": "Although conditional branching between possible behavioural states is a hallmark of intelligent behavior, very little is known about the neuronal mechanisms that support this processing. In a step toward solving this problem we demonstrate by theoretical analysis and simulation how networks of richly inter-connected neurons, such as those observed in the superficial layers of the neocortex, can embed reliable robust finite state machines. We show how a multi-stable neuronal network containing a number of states can be created very simply, by coupling two recurrent networks whose synaptic weights have been configured for soft winner-take-all (sWTA) performance. These two sWTAs have simple, homogenous locally recurrent connectivity except for a small fraction of recurrent cross-connections between them, which are used to embed the required states. This coupling between the maps allows the network to continue to express the current state even after the input that elicted that state is withdrawn. In addition, a small number of 'transition neurons' implement the necessary input-driven transitions between the embedded states. We provide simple rules to systematically design and construct neuronal state machines of this kind. The significance of our finding is that it offers a method whereby the cortex could construct networks supporting a broad range of sophisticated processing by applying only small specializations to the same generic neuronal circuit.",
        "published": "2008-09-24T22:52:05Z",
        "link": "http://arxiv.org/abs/0809.4296v1",
        "categories": [
            "q-bio.NC",
            "cs.NE"
        ]
    },
    {
        "title": "A computational approach to the covert and overt deployment of spatial   attention",
        "authors": [
            "Jérémy Fix",
            "Nicolas P. Rougier",
            "Frédéric Alexandre"
        ],
        "summary": "Popular computational models of visual attention tend to neglect the influence of saccadic eye movements whereas it has been shown that the primates perform on average three of them per seconds and that the neural substrate for the deployment of attention and the execution of an eye movement might considerably overlap. Here we propose a computational model in which the deployment of attention with or without a subsequent eye movement emerges from local, distributed and numerical computations.",
        "published": "2008-09-26T13:12:36Z",
        "link": "http://arxiv.org/abs/0809.4622v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Hybrid Neural Network Architecture for On-Line Learning",
        "authors": [
            "Yuhua Chen",
            "Subhash Kak",
            "Lei Wang"
        ],
        "summary": "Approaches to machine intelligence based on brain models have stressed the use of neural networks for generalization. Here we propose the use of a hybrid neural network architecture that uses two kind of neural networks simultaneously: (i) a surface learning agent that quickly adapt to new modes of operation; and, (ii) a deep learning agent that is very accurate within a specific regime of operation. The two networks of the hybrid architecture perform complementary functions that improve the overall performance. The performance of the hybrid architecture has been compared with that of back-propagation perceptrons and the CC and FC networks for chaotic time-series prediction, the CATS benchmark test, and smooth function approximation. It has been shown that the hybrid architecture provides a superior performance based on the RMS error criterion.",
        "published": "2008-09-29T23:00:22Z",
        "link": "http://arxiv.org/abs/0809.5087v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "The Fundamental Problem with the Building Block Hypothesis",
        "authors": [
            "Keki Burjorjee"
        ],
        "summary": "Skepticism of the building block hypothesis (BBH) has previously been expressed on account of the weak theoretical foundations of this hypothesis and the anomalies in the empirical record of the simple genetic algorithm. In this paper we hone in on a more fundamental cause for skepticism--the extraordinary strength of some of the assumptions that undergird the BBH. Specifically, we focus on assumptions made about the distribution of fitness over the genome set, and argue that these assumptions are unacceptably strong. As most of these assumptions have been embraced by the designers of so-called \"competent\" genetic algorithms, our critique is relevant to an appraisal of such algorithms as well.",
        "published": "2008-10-19T00:38:06Z",
        "link": "http://arxiv.org/abs/0810.3356v1",
        "categories": [
            "cs.NE",
            "I.2.8"
        ]
    },
    {
        "title": "Two Remarkable Computational Competencies of the Simple Genetic   Algorithm",
        "authors": [
            "Keki M. Burjorjee"
        ],
        "summary": "Since the inception of genetic algorithmics the identification of computational efficiencies of the simple genetic algorithm (SGA) has been an important goal. In this paper we distinguish between a computational competency of the SGA--an efficient, but narrow computational ability--and a computational proficiency of the SGA--a computational ability that is both efficient and broad. Till date, attempts to deduce a computational proficiency of the SGA have been unsuccessful. It may, however, be possible to inductively infer a computational proficiency of the SGA from a set of related computational competencies that have been deduced. With this in mind we deduce two computational competencies of the SGA. These competencies, when considered together, point toward a remarkable computational proficiency of the SGA. This proficiency is pertinent to a general problem that is closely related to a well-known statistical problem at the cutting edge of computational genetics.",
        "published": "2008-10-19T01:08:00Z",
        "link": "http://arxiv.org/abs/0810.3357v2",
        "categories": [
            "cs.NE",
            "I.2.8; F.2.m"
        ]
    },
    {
        "title": "A Study of NK Landscapes' Basins and Local Optima Networks",
        "authors": [
            "Gabriela Ochoa",
            "Marco Tomassini",
            "Sébastien Verel",
            "Christian Darabos"
        ],
        "summary": "We propose a network characterization of combinatorial fitness landscapes by adapting the notion of inherent networks proposed for energy surfaces (Doye, 2002). We use the well-known family of $NK$ landscapes as an example. In our case the inherent network is the graph where the vertices are all the local maxima and edges mean basin adjacency between two maxima. We exhaustively extract such networks on representative small NK landscape instances, and show that they are 'small-worlds'. However, the maxima graphs are not random, since their clustering coefficients are much larger than those of corresponding random graphs. Furthermore, the degree distributions are close to exponential instead of Poissonian. We also describe the nature of the basins of attraction and their relationship with the local maxima network.",
        "published": "2008-10-20T08:06:40Z",
        "link": "http://arxiv.org/abs/0810.3484v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "The Connectivity of NK Landscapes' Basins: A Network Analysis",
        "authors": [
            "Sébastien Verel",
            "Gabriela Ochoa",
            "Marco Tomassini"
        ],
        "summary": "We propose a network characterization of combinatorial fitness landscapes by adapting the notion of inherent networks proposed for energy surfaces. We use the well-known family of NK landscapes as an example. In our case the inherent network is the graph where the vertices represent the local maxima in the landscape, and the edges account for the transition probabilities between their corresponding basins of attraction. We exhaustively extracted such networks on representative small NK landscape instances, and performed a statistical characterization of their properties. We found that most of these network properties can be related to the search difficulty on the underlying NK landscapes with varying values of K.",
        "published": "2008-10-20T08:36:22Z",
        "link": "http://arxiv.org/abs/0810.3492v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "To which extend is the \"neural code\" a metric ?",
        "authors": [
            "Bruno Cessac",
            "Horacio Rostro-González",
            "Juan-Carlos Vasquez",
            "Thierry Viéville"
        ],
        "summary": "Here is proposed a review of the different choices to structure spike trains, using deterministic metrics. Temporal constraints observed in biological or computational spike trains are first taken into account. The relation with existing neural codes (rate coding, rank coding, phase coding, ..) is then discussed. To which extend the \"neural code\" contained in spike trains is related to a metric appears to be a key point, a generalization of the Victor-Purpura metric family being proposed for temporal constrained causal spike trains",
        "published": "2008-10-22T07:34:25Z",
        "link": "http://arxiv.org/abs/0810.3990v1",
        "categories": [
            "physics.bio-ph",
            "cs.NE",
            "physics.data-an",
            "q-bio.NC"
        ]
    },
    {
        "title": "Introducing numerical bounds to improve event-based neural network   simulation",
        "authors": [
            "Bruno Cessac",
            "Olivier Rochel",
            "Thierry Viéville"
        ],
        "summary": "Although the spike-trains in neural networks are mainly constrained by the neural dynamics itself, global temporal constraints (refractoriness, time precision, propagation delays, ..) are also to be taken into account. These constraints are revisited in this paper in order to use them in event-based simulation paradigms.   We first review these constraints, and discuss their consequences at the simulation level, showing how event-based simulation of time-constrained networks can be simplified in this context: the underlying data-structures are strongly simplified, while event-based and clock-based mechanisms can be easily mixed. These ideas are applied to punctual conductance-based generalized integrate-and-fire neural networks simulation, while spike-response model simulations are also revisited within this framework.   As an outcome, a fast minimal complementary alternative with respect to existing simulation event-based methods, with the possibility to simulate interesting neuron models is implemented and experimented.",
        "published": "2008-10-22T08:02:47Z",
        "link": "http://arxiv.org/abs/0810.3992v2",
        "categories": [
            "nlin.AO",
            "cs.NE",
            "nlin.CD",
            "q-bio.NC"
        ]
    },
    {
        "title": "The adaptability of physiological systems optimizes performance: new   directions in augmentation",
        "authors": [
            "Bradly Alicea"
        ],
        "summary": "This paper contributes to the human-machine interface community in two ways: as a critique of the closed-loop AC (augmented cognition) approach, and as a way to introduce concepts from complex systems and systems physiology into the field. Of particular relevance is a comparison of the inverted-U (or Gaussian) model of optimal performance and multidimensional fitness landscape model. Hypothetical examples will be given from human physiology and learning and memory. In particular, a four-step model will be introduced that is proposed as a better means to characterize multivariate systems during behavioral processes with complex dynamics such as learning. Finally, the alternate approach presented herein is considered as a preferable design alternate in human-machine systems. It is within this context that future directions are discussed.",
        "published": "2008-10-27T17:42:05Z",
        "link": "http://arxiv.org/abs/0810.4884v2",
        "categories": [
            "cs.HC",
            "cs.NE"
        ]
    },
    {
        "title": "Distributed Constrained Optimization with Semicoordinate Transformations",
        "authors": [
            "William Macready",
            "David Wolpert"
        ],
        "summary": "Recent work has shown how information theory extends conventional full-rationality game theory to allow bounded rational agents. The associated mathematical framework can be used to solve constrained optimization problems. This is done by translating the problem into an iterated game, where each agent controls a different variable of the problem, so that the joint probability distribution across the agents' moves gives an expected value of the objective function. The dynamics of the agents is designed to minimize a Lagrangian function of that joint distribution. Here we illustrate how the updating of the Lagrange parameters in the Lagrangian is a form of automated annealing, which focuses the joint distribution more and more tightly about the joint moves that optimize the objective function. We then investigate the use of ``semicoordinate'' variable transformations. These separate the joint state of the agents from the variables of the optimization problem, with the two connected by an onto mapping. We present experiments illustrating the ability of such transformations to facilitate optimization. We focus on the special kind of transformation in which the statistically independent states of the agents induces a mixture distribution over the optimization variables. Computer experiment illustrate this for $k$-sat constraint satisfaction problems and for unconstrained minimization of $NK$ functions.",
        "published": "2008-11-05T21:35:57Z",
        "link": "http://arxiv.org/abs/0811.0823v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Self-organized criticality and adaptation in discrete dynamical networks",
        "authors": [
            "Thimo Rohlf",
            "Stefan Bornholdt"
        ],
        "summary": "It has been proposed that adaptation in complex systems is optimized at the critical boundary between ordered and disordered dynamical regimes. Here, we review models of evolving dynamical networks that lead to self-organization of network topology based on a local coupling between a dynamical order parameter and rewiring of network connectivity, with convergence towards criticality in the limit of large network size $N$. In particular, two adaptive schemes are discussed and compared in the context of Boolean Networks and Threshold Networks: 1) Active nodes loose links, frozen nodes aquire new links, 2) Nodes with correlated activity connect, de-correlated nodes disconnect. These simple local adaptive rules lead to co-evolution of network topology and -dynamics. Adaptive networks are strikingly different from random networks: They evolve inhomogeneous topologies and broad plateaus of homeostatic regulation, dynamical activity exhibits $1/f$ noise and attractor periods obey a scale-free distribution. The proposed co-evolutionary mechanism of topological self-organization is robust against noise and does not depend on the details of dynamical transition rules. Using finite-size scaling, it is shown that networks converge to a self-organized critical state in the thermodynamic limit. Finally, we discuss open questions and directions for future research, and outline possible applications of these models to adaptive systems in diverse areas.",
        "published": "2008-11-06T16:04:08Z",
        "link": "http://arxiv.org/abs/0811.0980v1",
        "categories": [
            "nlin.AO",
            "cond-mat.dis-nn",
            "cs.NE"
        ]
    },
    {
        "title": "A Novel Clustering Algorithm Based on Quantum Games",
        "authors": [
            "Qiang Li",
            "Yan He",
            "Jing-ping Jiang"
        ],
        "summary": "Enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum game with the problem of data clustering, and then develop a quantum-game-based clustering algorithm, in which data points in a dataset are considered as players who can make decisions and implement quantum strategies in quantum games. After each round of a quantum game, each player's expected payoff is calculated. Later, he uses a link-removing-and-rewiring (LRR) function to change his neighbors and adjust the strength of links connecting to them in order to maximize his payoff. Further, algorithms are discussed and analyzed in two cases of strategies, two payoff matrixes and two LRR functions. Consequently, the simulation results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms have fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.",
        "published": "2008-12-03T15:46:03Z",
        "link": "http://arxiv.org/abs/0812.0743v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.GT",
            "cs.MA",
            "cs.NE",
            "quant-ph"
        ]
    },
    {
        "title": "Elagage d'un perceptron multicouches : utilisation de l'analyse de la   variance de la sensibilité des paramètres",
        "authors": [
            "Philippe Thomas",
            "André Thomas"
        ],
        "summary": "The stucture determination of a neural network for the modelisation of a system remain the core of the problem. Within this framework, we propose a pruning algorithm of the network based on the use of the analysis of the sensitivity of the variance of all the parameters of the network. This algorithm will be tested on two examples of simulation and its performances will be compared with three other algorithms of pruning of the literature",
        "published": "2008-12-04T09:12:14Z",
        "link": "http://arxiv.org/abs/0812.0882v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Sélection de la structure d'un perceptron multicouches pour la   réduction dun modèle de simulation d'une scierie",
        "authors": [
            "Philippe Thomas",
            "André Thomas"
        ],
        "summary": "Simulation is often used to evaluate the relevance of a Directing Program of Production (PDP) or to evaluate its impact on detailed sc\\'enarii of scheduling. Within this framework, we propose to reduce the complexity of a model of simulation by exploiting a multilayer perceptron. A main phase of the modeling of one system using a multilayer perceptron remains the determination of the structure of the network. We propose to compare and use various pruning algorithms in order to determine the optimal structure of the network used to reduce the complexity of the model of simulation of our case of application: a sawmill.",
        "published": "2008-12-05T08:49:53Z",
        "link": "http://arxiv.org/abs/0812.1094v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Pattern Recognition and Memory Mapping using Mirroring Neural Networks",
        "authors": [
            "Dasika Ratna Deepthi",
            "K. Eswaran"
        ],
        "summary": "In this paper, we present a new kind of learning implementation to recognize the patterns using the concept of Mirroring Neural Network (MNN) which can extract information from distinct sensory input patterns and perform pattern recognition tasks. It is also capable of being used as an advanced associative memory wherein image data is associated with voice inputs in an unsupervised manner. Since the architecture is hierarchical and modular it has the potential of being used to devise learning engines of ever increasing complexity.",
        "published": "2008-12-13T09:21:31Z",
        "link": "http://arxiv.org/abs/0812.2535v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "A Growing Self-Organizing Network for Reconstructing Curves and Surfaces",
        "authors": [
            "Marco Piastra"
        ],
        "summary": "Self-organizing networks such as Neural Gas, Growing Neural Gas and many others have been adopted in actual applications for both dimensionality reduction and manifold learning. Typically, in these applications, the structure of the adapted network yields a good estimate of the topology of the unknown subspace from where the input data points are sampled. The approach presented here takes a different perspective, namely by assuming that the input space is a manifold of known dimension. In return, the new type of growing self-organizing network presented gains the ability to adapt itself in way that may guarantee the effective and stable recovery of the exact topological structure of the input manifold.",
        "published": "2008-12-16T15:59:36Z",
        "link": "http://arxiv.org/abs/0812.2969v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Finding Still Lifes with Memetic/Exact Hybrid Algorithms",
        "authors": [
            "Jose E. Gallardo",
            "Carlos Cotta",
            "Antonio J. Fernandez"
        ],
        "summary": "The maximum density still life problem (MDSLP) is a hard constraint optimization problem based on Conway's game of life. It is a prime example of weighted constrained optimization problem that has been recently tackled in the constraint-programming community. Bucket elimination (BE) is a complete technique commonly used to solve this kind of constraint satisfaction problem. When the memory required to apply BE is too high, a heuristic method based on it (denominated mini-buckets) can be used to calculate bounds for the optimal solution. Nevertheless, the curse of dimensionality makes these techniques unpractical for large size problems. In response to this situation, we present a memetic algorithm for the MDSLP in which BE is used as a mechanism for recombining solutions, providing the best possible child from the parental set. Subsequently, a multi-level model in which this exact/metaheuristic hybrid is further hybridized with branch-and-bound techniques and mini-buckets is studied. Extensive experimental results analyze the performance of these models and multi-parent recombination. The resulting algorithm consistently finds optimal patterns for up to date solved instances in less time than current approaches. Moreover, it is shown that this proposal provides new best known solutions for very large instances.",
        "published": "2008-12-22T13:09:11Z",
        "link": "http://arxiv.org/abs/0812.4170v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Driven by Compression Progress: A Simple Principle Explains Essential   Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention,   Curiosity, Creativity, Art, Science, Music, Jokes",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "I argue that data becomes temporarily interesting by itself to some self-improving, but computationally limited, subjective observer once he learns to predict or compress the data in a better way, thus making it subjectively simpler and more beautiful. Curiosity is the desire to create or discover more non-random, non-arbitrary, regular data that is novel and surprising not in the traditional sense of Boltzmann and Shannon but in the sense that it allows for compression progress because its regularity was not yet known. This drive maximizes interestingness, the first derivative of subjective beauty or compressibility, that is, the steepness of the learning curve. It motivates exploring infants, pure mathematicians, composers, artists, dancers, comedians, yourself, and (since 1990) artificial systems.",
        "published": "2008-12-23T10:14:18Z",
        "link": "http://arxiv.org/abs/0812.4360v2",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "An algorithm for finding the Independence Number of a graph",
        "authors": [
            "Omar Kettani"
        ],
        "summary": "In this paper, we prove that for every connected graph G, there exists a split graph H with the same independence number and the same order. Then we propose a first algorithm for finding this graph, given the degree sequence of the input graph G. Further, we propose a second algorithm for finding the independence number of G, given the adjacency matrix of G.",
        "published": "2008-01-03T20:51:38Z",
        "link": "http://arxiv.org/abs/0801.0590v4",
        "categories": [
            "cs.DM",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Almost 2-SAT is Fixed-Parameter Tractable",
        "authors": [
            "Igor Razgon",
            "Barry O'Sullivan"
        ],
        "summary": "We consider the following problem. Given a 2-CNF formula, is it possible to remove at most $k$ clauses so that the resulting 2-CNF formula is satisfiable? This problem is known to different research communities in Theoretical Computer Science under the names 'Almost 2-SAT', 'All-but-$k$ 2-SAT', '2-CNF deletion', '2-SAT deletion'. The status of fixed-parameter tractability of this problem is a long-standing open question in the area of Parameterized Complexity. We resolve this open question by proposing an algorithm which solves this problem in $O(15^k*k*m^3)$ and thus we show that this problem is fixed-parameter tractable.",
        "published": "2008-01-08T19:04:14Z",
        "link": "http://arxiv.org/abs/0801.1300v4",
        "categories": [
            "cs.DS",
            "cs.CG",
            "cs.LO"
        ]
    },
    {
        "title": "Fast Integer Multiplication using Modular Arithmetic",
        "authors": [
            "Anindya De",
            "Piyush P Kurur",
            "Chandan Saha",
            "Ramprasad Saptharishi"
        ],
        "summary": "We give an $O(N\\cdot \\log N\\cdot 2^{O(\\log^*N)})$ algorithm for multiplying two $N$-bit integers that improves the $O(N\\cdot \\log N\\cdot \\log\\log N)$ algorithm by Sch\\\"{o}nhage-Strassen. Both these algorithms use modular arithmetic. Recently, F\\\"{u}rer gave an $O(N\\cdot \\log N\\cdot 2^{O(\\log^*N)})$ algorithm which however uses arithmetic over complex numbers as opposed to modular arithmetic. In this paper, we use multivariate polynomial multiplication along with ideas from F\\\"{u}rer's algorithm to achieve this improvement in the modular setting. Our algorithm can also be viewed as a $p$-adic version of F\\\"{u}rer's algorithm. Thus, we show that the two seemingly different approaches to integer multiplication, modular and complex arithmetic, are similar.",
        "published": "2008-01-09T12:44:55Z",
        "link": "http://arxiv.org/abs/0801.1416v3",
        "categories": [
            "cs.SC",
            "cs.DS"
        ]
    },
    {
        "title": "Minimum Leaf Out-branching and Related Problems",
        "authors": [
            "G. Gutin",
            "I. Razgon",
            "E. J. Kim"
        ],
        "summary": "Given a digraph $D$, the Minimum Leaf Out-Branching problem (MinLOB) is the problem of finding in $D$ an out-branching with the minimum possible number of leaves, i.e., vertices of out-degree 0. We prove that MinLOB is polynomial-time solvable for acyclic digraphs. In general, MinLOB is NP-hard and we consider three parameterizations of MinLOB. We prove that two of them are NP-complete for every value of the parameter, but the third one is fixed-parameter tractable (FPT). The FPT parametrization is as follows: given a digraph $D$ of order $n$ and a positive integral parameter $k$, check whether $D$ contains an out-branching with at most $n-k$ leaves (and find such an out-branching if it exists). We find a problem kernel of order $O(k^2)$ and construct an algorithm of running time $O(2^{O(k\\log k)}+n^6),$ which is an `additive' FPT algorithm. We also consider transformations from two related problems, the minimum path covering and the maximum internal out-tree problems into MinLOB, which imply that some parameterizations of the two problems are FPT as well.",
        "published": "2008-01-13T19:33:29Z",
        "link": "http://arxiv.org/abs/0801.1979v3",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "A Nearly Linear-Time PTAS for Explicit Fractional Packing and Covering   Linear Programs",
        "authors": [
            "Christos Koufogiannakis",
            "Neal E. Young"
        ],
        "summary": "We give an approximation algorithm for packing and covering linear programs (linear programs with non-negative coefficients). Given a constraint matrix with n non-zeros, r rows, and c columns, the algorithm computes feasible primal and dual solutions whose costs are within a factor of 1+eps of the optimal cost in time O((r+c)log(n)/eps^2 + n).",
        "published": "2008-01-13T22:04:49Z",
        "link": "http://arxiv.org/abs/0801.1987v2",
        "categories": [
            "cs.DS",
            "68W25",
            "G.1.6"
        ]
    },
    {
        "title": "Le probleme de l'isomorphisme de graphes est dans P",
        "authors": [
            "Omar Kettani"
        ],
        "summary": "This paper has been withdrawn by the author, due to possible counter-examples.",
        "published": "2008-01-15T13:06:41Z",
        "link": "http://arxiv.org/abs/0801.2284v2",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "String algorithms and data structures",
        "authors": [
            "Paolo Ferragina"
        ],
        "summary": "The string-matching field has grown at a such complicated stage that various issues come into play when studying it: data structure and algorithmic design, database principles, compression techniques, architectural features, cache and prefetching policies. The expertise nowadays required to design good string data structures and algorithms is therefore transversal to many computer science fields and much more study on the orchestration of known, or novel, techniques is needed to make progress in this fascinating topic. This survey is aimed at illustrating the key ideas which should constitute, in our opinion, the current background of every index designer. We also discuss the positive features and drawback of known indexing schemes and algorithms, and devote much attention to detail research issues and open problems both on the theoretical and the experimental side.",
        "published": "2008-01-15T20:54:18Z",
        "link": "http://arxiv.org/abs/0801.2378v1",
        "categories": [
            "cs.DS",
            "cs.IR"
        ]
    },
    {
        "title": "A Truthful Mechanism for Offline Ad Slot Scheduling",
        "authors": [
            "Jon Feldman",
            "S. Muthukrishnan",
            "Evdokia Nikolova",
            "Martin Pal"
        ],
        "summary": "We consider the \"Offline Ad Slot Scheduling\" problem, where advertisers must be scheduled to \"sponsored search\" slots during a given period of time. Advertisers specify a budget constraint, as well as a maximum cost per click, and may not be assigned to more than one slot for a particular search.   We give a truthful mechanism under the utility model where bidders try to maximize their clicks, subject to their personal constraints. In addition, we show that the revenue-maximizing mechanism is not truthful, but has a Nash equilibrium whose outcome is identical to our mechanism. As far as we can tell, this is the first treatment of sponsored search that directly incorporates both multiple slots and budget constraints into an analysis of incentives.   Our mechanism employs a descending-price auction that maintains a solution to a certain machine scheduling problem whose job lengths depend on the price, and hence is variable over the auction. The price stops when the set of bidders that can afford that price pack exactly into a block of ad slots, at which point the mechanism allocates that block and continues on the remaining slots. To prove our result on the equilibrium of the revenue-maximizing mechanism, we first show that a greedy algorithm suffices to solve the revenue-maximizing linear program; we then use this insight to prove that bidders allocated in the same block of our mechanism have no incentive to deviate from bidding the fixed price of that block.",
        "published": "2008-01-18T16:34:30Z",
        "link": "http://arxiv.org/abs/0801.2931v1",
        "categories": [
            "cs.GT",
            "cs.DS"
        ]
    },
    {
        "title": "From k-SAT to k-CSP: Two Generalized Algorithms",
        "authors": [
            "Liang Li",
            "Xin Li",
            "Tian Liu",
            "Ke Xu"
        ],
        "summary": "Constraint satisfaction problems (CSPs) models many important intractable NP-hard problems such as propositional satisfiability problem (SAT). Algorithms with non-trivial upper bounds on running time for restricted SAT with bounded clause length k (k-SAT) can be classified into three styles: DPLL-like, PPSZ-like and Local Search, with local search algorithms having already been generalized to CSP with bounded constraint arity k (k-CSP). We generalize a DPLL-like algorithm in its simplest form and a PPSZ-like algorithm from k-SAT to k-CSP. As far as we know, this is the first attempt to use PPSZ-like strategy to solve k-CSP, and before little work has been focused on the DPLL-like or PPSZ-like strategies for k-CSP.",
        "published": "2008-01-21T08:07:33Z",
        "link": "http://arxiv.org/abs/0801.3147v1",
        "categories": [
            "cs.DS",
            "cs.AI",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Shallow, Low, and Light Trees, and Tight Lower Bounds for Euclidean   Spanners",
        "authors": [
            "Yefim Dinitz",
            "Michael Elkin",
            "Shay Solomon"
        ],
        "summary": "We show that for every $n$-point metric space $M$ there exists a spanning tree $T$ with unweighted diameter $O(\\log n)$ and weight $\\omega(T) = O(\\log n) \\cdot \\omega(MST(M))$. Moreover, there is a designated point $rt$ such that for every point $v$, $dist_T(rt,v) \\le (1+\\epsilon) \\cdot dist_M(rt,v)$, for an arbitrarily small constant $\\epsilon > 0$. We extend this result, and provide a tradeoff between unweighted diameter and weight, and prove that this tradeoff is \\emph{tight up to constant factors} in the entire range of parameters. These results enable us to settle a long-standing open question in Computational Geometry. In STOC'95 Arya et al. devised a construction of Euclidean Spanners with unweighted diameter $O(\\log n)$ and weight $O(\\log n) \\cdot \\omega(MST(M))$. Ten years later in SODA'05 Agarwal et al. showed that this result is tight up to a factor of $O(\\log \\log n)$. We close this gap and show that the result of Arya et al. is tight up to constant factors.",
        "published": "2008-01-23T13:57:00Z",
        "link": "http://arxiv.org/abs/0801.3581v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2; F.2.3; G.2.2"
        ]
    },
    {
        "title": "Picking up the Pieces: Self-Healing in Reconfigurable Networks",
        "authors": [
            "Jared Saia",
            "Amitabh Trehan"
        ],
        "summary": "We consider the problem of self-healing in networks that are reconfigurable in the sense that they can change their topology during an attack. Our goal is to maintain connectivity in these networks, even in the presence of repeated adversarial node deletion, by carefully adding edges after each attack. We present a new algorithm, DASH, that provably ensures that: 1) the network stays connected even if an adversary deletes up to all nodes in the network; and 2) no node ever increases its degree by more than 2 log n, where n is the number of nodes initially in the network. DASH is fully distributed; adds new edges only among neighbors of deleted nodes; and has average latency and bandwidth costs that are at most logarithmic in n. DASH has these properties irrespective of the topology of the initial network, and is thus orthogonal and complementary to traditional topology-based approaches to defending against attack.   We also prove lower-bounds showing that DASH is asymptotically optimal in terms of minimizing maximum degree increase over multiple attacks. Finally, we present empirical results on power-law graphs that show that DASH performs well in practice, and that it significantly outperforms naive algorithms in reducing maximum degree increase. We also present empirical results on performance of our algorithms and a new heuristic with regard to stretch (increase in shortest path lengths).",
        "published": "2008-01-24T07:46:50Z",
        "link": "http://arxiv.org/abs/0801.3710v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "cs.NI",
            "C.2.1; C.2.3; C.2.4; C.4; H.3.4"
        ]
    },
    {
        "title": "Solving Min-Max Problems with Applications to Games",
        "authors": [
            "Daniel Andersson"
        ],
        "summary": "We refine existing general network optimization techniques, give new characterizations for the class of problems to which they can be applied, and show that they can also be used to solve various two-player games in almost linear time. Among these is a new variant of the network interdiction problem, where the interdictor wants to destroy high-capacity paths from the source to the destination using a vertex-wise limited budget of arc removals. We also show that replacing the limit average in mean payoff games by the maximum weight results in a class of games amenable to these techniques.",
        "published": "2008-01-27T13:28:43Z",
        "link": "http://arxiv.org/abs/0801.4130v1",
        "categories": [
            "cs.GT",
            "cs.DS"
        ]
    },
    {
        "title": "Phylogenies without Branch Bounds: Contracting the Short, Pruning the   Deep",
        "authors": [
            "Constantinos Daskalakis",
            "Elchanan Mossel",
            "Sebastien Roch"
        ],
        "summary": "We introduce a new phylogenetic reconstruction algorithm which, unlike most previous rigorous inference techniques, does not rely on assumptions regarding the branch lengths or the depth of the tree. The algorithm returns a forest which is guaranteed to contain all edges that are: 1) sufficiently long and 2) sufficiently close to the leaves. How much of the true tree is recovered depends on the sequence length provided. The algorithm is distance-based and runs in polynomial time.",
        "published": "2008-01-28T05:10:22Z",
        "link": "http://arxiv.org/abs/0801.4190v2",
        "categories": [
            "q-bio.PE",
            "cs.CE",
            "cs.DS",
            "math.PR",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Algorithms for Temperature-Aware Task Scheduling in Microprocessor   Systems",
        "authors": [
            "Marek Chrobak",
            "Christoph Durr",
            "Mathilde Hurand",
            "Julien Robert"
        ],
        "summary": "We study scheduling problems motivated by recently developed techniques for microprocessor thermal management at the operating systems level. The general scenario can be described as follows. The microprocessor's temperature is controlled by the hardware thermal management system that continuously monitors the chip temperature and automatically reduces the processor's speed as soon as the thermal threshold is exceeded. Some tasks are more CPU-intensive than other and thus generate more heat during execution. The cooling system operates non-stop, reducing (at an exponential rate) the deviation of the processor's temperature from the ambient temperature. As a result, the processor's temperature, and thus the performance as well, depends on the order of the task execution. Given a variety of possible underlying architectures, models for cooling and for hardware thermal management, as well as types of tasks, this scenario gives rise to a plethora of interesting and never studied scheduling problems.   We focus on scheduling real-time jobs in a simplified model for cooling and thermal management. A collection of unit-length jobs is given, each job specified by its release time, deadline and heat contribution. If, at some time step, the temperature of the system is t and the processor executes a job with heat contribution h, then the temperature at the next step is (t+h)/2. The temperature cannot exceed the given thermal threshold T. The objective is to maximize the throughput, that is, the number of tasks that meet their deadlines. We prove that, in the offline case, computing the optimum schedule is NP-hard, even if all jobs are released at the same time. In the online case, we show a 2-competitive deterministic algorithm and a matching lower bound.",
        "published": "2008-01-28T10:47:42Z",
        "link": "http://arxiv.org/abs/0801.4238v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Bicretieria Optimization in Routing Games",
        "authors": [
            "Costas Busch",
            "Rajgopal Kannan"
        ],
        "summary": "Two important metrics for measuring the quality of routing paths are the maximum edge congestion $C$ and maximum path length $D$. Here, we study bicriteria in routing games where each player $i$ selfishly selects a path that simultaneously minimizes its maximum edge congestion $C_i$ and path length $D_i$. We study the stability and price of anarchy of two bicriteria games:   - {\\em Max games}, where the social cost is $\\max(C,D)$ and the player cost is $\\max(C_i, D_i)$. We prove that max games are stable and convergent under best-response dynamics, and that the price of anarchy is bounded above by the maximum path length in the players' strategy sets. We also show that this bound is tight in worst-case scenarios.   - {\\em Sum games}, where the social cost is $C+D$ and the player cost is $C_i+D_i$. For sum games, we first show the negative result that there are game instances that have no Nash-equilibria. Therefore, we examine an approximate game called the {\\em sum-bucket game} that is always convergent (and therefore stable). We show that the price of anarchy in sum-bucket games is bounded above by $C^* \\cdot D^* / (C^* + D^*)$ (with a poly-log factor), where $C^*$ and $D^*$ are the optimal coordinated congestion and path length. Thus, the sum-bucket game has typically superior price of anarchy bounds than the max game. In fact, when either $C^*$ or $D^*$ is small (e.g. constant) the social cost of the Nash-equilibria is very close to the coordinated optimal $C^* + D^*$ (within a poly-log factor). We also show that the price of anarchy bound is tight for cases where both $C^*$ and $D^*$ are large.",
        "published": "2008-01-31T19:29:13Z",
        "link": "http://arxiv.org/abs/0801.4851v1",
        "categories": [
            "cs.GT",
            "cs.DS"
        ]
    },
    {
        "title": "Improved Deterministic Length Reduction",
        "authors": [
            "Amihood Amir",
            "Klim Efremenko",
            "Oren Kapah",
            "Ely Porat",
            "Amir Rothschild"
        ],
        "summary": "This paper presents a new technique for deterministic length reduction. This technique improves the running time of the algorithm presented in \\cite{LR07} for performing fast convolution in sparse data. While the regular fast convolution of vectors $V_1,V_2$ whose sizes are $N_1,N_2$ respectively, takes $O(N_1 \\log N_2)$ using FFT, using the new technique for length reduction, the algorithm proposed in \\cite{LR07} performs the convolution in $O(n_1 \\log^3 n_1)$, where $n_1$ is the number of non-zero values in $V_1$. The algorithm assumes that $V_1$ is given in advance, and $V_2$ is given in running time. The novel technique presented in this paper improves the convolution time to $O(n_1 \\log^2 n_1)$ {\\sl deterministically}, which equals the best running time given achieved by a {\\sl randomized} algorithm.   The preprocessing time of the new technique remains the same as the preprocessing time of \\cite{LR07}, which is $O(n_1^2)$. This assumes and deals the case where $N_1$ is polynomial in $n_1$. In the case where $N_1$ is exponential in $n_1$, a reduction to a polynomial case can be used. In this paper we also improve the preprocessing time of this reduction from $O(n_1^4)$ to $O(n_1^3{\\rm polylog}(n_1))$.",
        "published": "2008-01-31T21:59:33Z",
        "link": "http://arxiv.org/abs/0802.0017v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "On Approximating Frequency Moments of Data Streams with Skewed   Projections",
        "authors": [
            "Ping Li"
        ],
        "summary": "We propose skewed stable random projections for approximating the pth frequency moments of dynamic data streams (0<p<=2), which has been frequently studied in theoretical computer science and database communities. Our method significantly (or even infinitely when p->1) improves previous methods based on (symmetric) stable random projections.   Our proposed method is applicable to data streams that are (a) insertion only (the cash-register model); or (b) always non-negative (the strict Turnstile model), or (c) eventually non-negative at check points. This is only a minor restriction for practical applications.   Our method works particularly well when p = 1+/- \\Delta and \\Delta is small, which is a practically important scenario. For example, \\Delta may be the decay rate or interest rate, which are usually small. Of course, when \\Delta = 0, one can compute the 1th frequent moment (i.e., the sum) essentially error-free using a simple couter. Our method may be viewed as a ``genearlized counter'' in that it can count the total value in the future, taking in account of the effect of decaying or interest accruement.   In a summary, our contributions are two-fold. (A) This is the first propsal of skewed stable random projections. (B) Based on first principle, we develop various statistical estimators for skewed stable distributions, including their variances and error (tail) probability bounds, and consequently the sample complexity bounds.",
        "published": "2008-02-06T13:56:51Z",
        "link": "http://arxiv.org/abs/0802.0802v1",
        "categories": [
            "cs.DS",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Bit-Optimal Lempel-Ziv compression",
        "authors": [
            "Paolo Ferragina",
            "Igor Nitto",
            "Rossano Venturini"
        ],
        "summary": "One of the most famous and investigated lossless data-compression scheme is the one introduced by Lempel and Ziv about 40 years ago. This compression scheme is known as \"dictionary-based compression\" and consists of squeezing an input string by replacing some of its substrings with (shorter) codewords which are actually pointers to a dictionary of phrases built as the string is processed. Surprisingly enough, although many fundamental results are nowadays known about upper bounds on the speed and effectiveness of this compression process and references therein), ``we are not aware of any parsing scheme that achieves optimality when the LZ77-dictionary is in use under any constraint on the codewords other than being of equal length'' [N. Rajpoot and C. Sahinalp. Handbook of Lossless Data Compression, chapter Dictionary-based data compression. Academic Press, 2002. pag. 159]. Here optimality means to achieve the minimum number of bits in compressing each individual input string, without any assumption on its generating source. In this paper we provide the first LZ-based compressor which computes the bit-optimal parsing of any input string in efficient time and optimal space, for a general class of variable-length codeword encodings which encompasses most of the ones typically used in data compression and in the design of search engines and compressed indexes.",
        "published": "2008-02-06T16:31:54Z",
        "link": "http://arxiv.org/abs/0802.0835v1",
        "categories": [
            "cs.DS",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "An Empirical Study of Cache-Oblivious Priority Queues and their   Application to the Shortest Path Problem",
        "authors": [
            "Benjamin Sach",
            "Raphaël Clifford"
        ],
        "summary": "In recent years the Cache-Oblivious model of external memory computation has provided an attractive theoretical basis for the analysis of algorithms on massive datasets. Much progress has been made in discovering algorithms that are asymptotically optimal or near optimal. However, to date there are still relatively few successful experimental studies. In this paper we compare two different Cache-Oblivious priority queues based on the Funnel and Bucket Heap and apply them to the single source shortest path problem on graphs with positive edge weights. Our results show that when RAM is limited and data is swapping to external storage, the Cache-Oblivious priority queues achieve orders of magnitude speedups over standard internal memory techniques. However, for the single source shortest path problem both on simulated and real world graph data, these speedups are markedly lower due to the time required to access the graph adjacency list itself.",
        "published": "2008-02-07T18:02:11Z",
        "link": "http://arxiv.org/abs/0802.1026v1",
        "categories": [
            "cs.DS",
            "cs.SE"
        ]
    },
    {
        "title": "Average-Case Analysis of Online Topological Ordering",
        "authors": [
            "Deepak Ajwani",
            "Tobias Friedrich"
        ],
        "summary": "Many applications like pointer analysis and incremental compilation require maintaining a topological ordering of the nodes of a directed acyclic graph (DAG) under dynamic updates. All known algorithms for this problem are either only analyzed for worst-case insertion sequences or only evaluated experimentally on random DAGs. We present the first average-case analysis of online topological ordering algorithms. We prove an expected runtime of O(n^2 polylog(n)) under insertion of the edges of a complete DAG in a random order for the algorithms of Alpern et al. (SODA, 1990), Katriel and Bodlaender (TALG, 2006), and Pearce and Kelly (JEA, 2006). This is much less than the best known worst-case bound O(n^{2.75}) for this problem.",
        "published": "2008-02-07T20:27:17Z",
        "link": "http://arxiv.org/abs/0802.1059v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Minimum Entropy Orientations",
        "authors": [
            "Jean Cardinal",
            "Samuel Fiorini",
            "Gwenaël Joret"
        ],
        "summary": "We study graph orientations that minimize the entropy of the in-degree sequence. The problem of finding such an orientation is an interesting special case of the minimum entropy set cover problem previously studied by Halperin and Karp [Theoret. Comput. Sci., 2005] and by the current authors [Algorithmica, to appear]. We prove that the minimum entropy orientation problem is NP-hard even if the graph is planar, and that there exists a simple linear-time algorithm that returns an approximate solution with an additive error guarantee of 1 bit. This improves on the only previously known algorithm which has an additive error guarantee of log_2 e bits (approx. 1.4427 bits).",
        "published": "2008-02-09T01:38:06Z",
        "link": "http://arxiv.org/abs/0802.1237v2",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Some results on (a:b)-choosability",
        "authors": [
            "Shai Gutner",
            "Michael Tarsi"
        ],
        "summary": "A solution to a problem of Erd\\H{o}s, Rubin and Taylor is obtained by showing that if a graph $G$ is $(a:b)$-choosable, and $c/d > a/b$, then $G$ is not necessarily $(c:d)$-choosable. Applying probabilistic methods, an upper bound for the $k^{th}$ choice number of a graph is given. We also prove that a directed graph with maximum outdegree $d$ and no odd directed cycle is $(k(d+1):k)$-choosable for every $k \\geq 1$. Other results presented in this article are related to the strong choice number of graphs (a generalization of the strong chromatic number). We conclude with complexity analysis of some decision problems related to graph choosability.",
        "published": "2008-02-10T17:46:54Z",
        "link": "http://arxiv.org/abs/0802.1338v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Approximating General Metric Distances Between a Pattern and a Text",
        "authors": [
            "Klim Efremenko",
            "Ely Porat"
        ],
        "summary": "Let $T=t_0 ... t_{n-1}$ be a text and $P = p_0 ... p_{m-1}$ a pattern taken from some finite alphabet set $\\Sigma$, and let $\\dist$ be a metric on $\\Sigma$. We consider the problem of calculating the sum of distances between the symbols of $P$ and the symbols of substrings of $T$ of length $m$ for all possible offsets. We present an $\\epsilon$-approximation algorithm for this problem which runs in time $O(\\frac{1}{\\epsilon^2}n\\cdot \\mathrm{polylog}(n,\\abs{\\Sigma}))$",
        "published": "2008-02-11T12:36:31Z",
        "link": "http://arxiv.org/abs/0802.1427v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Error-Correcting Data Structures",
        "authors": [
            "Ronald de Wolf"
        ],
        "summary": "We study data structures in the presence of adversarial noise. We want to encode a given object in a succinct data structure that enables us to efficiently answer specific queries about the object, even if the data structure has been corrupted by a constant fraction of errors. This new model is the common generalization of (static) data structures and locally decodable error-correcting codes. The main issue is the tradeoff between the space used by the data structure and the time (number of probes) needed to answer a query about the encoded object. We prove a number of upper and lower bounds on various natural error-correcting data structure problems. In particular, we show that the optimal length of error-correcting data structures for the Membership problem (where we want to store subsets of size s from a universe of size n) is closely related to the optimal length of locally decodable codes for s-bit strings.",
        "published": "2008-02-11T16:35:49Z",
        "link": "http://arxiv.org/abs/0802.1471v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Generalized Whac-a-Mole",
        "authors": [
            "Marcin Bienkowski",
            "Marek Chrobak",
            "Christoph Durr",
            "Mathilde Hurand",
            "Artur Jez",
            "Lukasz Jez",
            "Jakub Lopuszanski",
            "Grzegorz Stachowiak"
        ],
        "summary": "We consider online competitive algorithms for the problem of collecting weighted items from a dynamic set S, when items are added to or deleted from S over time. The objective is to maximize the total weight of collected items. We study the general version, as well as variants with various restrictions, including the following: the uniform case, when all items have the same weight, the decremental sets, when all items are present at the beginning and only deletion operations are allowed, and dynamic queues, where the dynamic set is ordered and only its prefixes can be deleted (with no restriction on insertions). The dynamic queue case is a generalization of bounded-delay packet scheduling (also referred to as buffer management). We present several upper and lower bounds on the competitive ratio for these variants.",
        "published": "2008-02-12T18:41:46Z",
        "link": "http://arxiv.org/abs/0802.1685v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Parameterized Algorithms for Partial Cover Problems",
        "authors": [
            "Omid Amini",
            "Fedor V. Fomin",
            "Saket Saurabh"
        ],
        "summary": "Covering problems are fundamental classical problems in optimization, computer science and complexity theory. Typically an input to these problems is a family of sets over a finite universe and the goal is to cover the elements of the universe with as few sets of the family as possible.   The variations of covering problems include well known problems like Set Cover, Vertex Cover, Dominating Set and Facility Location to name a few. Recently there has been a lot of study on partial covering problems, a natural generalization of covering problems. Here, the goal is not to cover all the elements but to cover the specified number of elements with the minimum number of sets.   In this paper we study partial covering problems in graphs in the realm of parameterized complexity. Classical (non-partial) version of all these problems have been intensively studied in planar graphs and in graphs excluding a fixed graph $H$ as a minor. However, the techniques developed for parameterized version of non-partial covering problems cannot be applied directly to their partial counterparts. The approach we use, to show that various partial covering problems are fixed parameter tractable on planar graphs, graphs of bounded local treewidth and graph excluding some graph as a minor, is quite different from previously known techniques. The main idea behind our approach is the concept of implicit branching. We find implicit branching technique to be interesting on its own and believe that it can be used for some other problems.",
        "published": "2008-02-12T21:19:40Z",
        "link": "http://arxiv.org/abs/0802.1722v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "To Broad-Match or Not to Broad-Match : An Auctioneer's Dilemma ?",
        "authors": [
            "Sudhir Kumar Singh",
            "Vwani P. Roychowdhury"
        ],
        "summary": "We initiate the study of an interesting aspect of sponsored search advertising, namely the consequences of broad match-a feature where an ad of an advertiser can be mapped to a broader range of relevant queries, and not necessarily to the particular keyword(s) that ad is associated with. Starting with a very natural setting for strategies available to the advertisers, and via a careful look through the algorithmic lens, we first propose solution concepts for the game originating from the strategic behavior of advertisers as they try to optimize their budget allocation across various keywords. Next, we consider two broad match scenarios based on factors such as information asymmetry between advertisers and the auctioneer, and the extent of auctioneer's control on the budget splitting. In the first scenario, the advertisers have the full information about broad match and relevant parameters, and can reapportion their own budgets to utilize the extra information; in particular, the auctioneer has no direct control over budget splitting. We show that, the same broad match may lead to different equilibria, one leading to a revenue improvement, whereas another to a revenue loss. This leaves the auctioneer in a dilemma - whether to broad-match or not. This motivates us to consider another broad match scenario, where the advertisers have information only about the current scenario, and the allocation of the budgets unspent in the current scenario is in the control of the auctioneer. We observe that the auctioneer can always improve his revenue by judiciously using broad match. Thus, information seems to be a double-edged sword for the auctioneer.",
        "published": "2008-02-14T03:45:07Z",
        "link": "http://arxiv.org/abs/0802.1957v2",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Combining Expert Advice Efficiently",
        "authors": [
            "Wouter Koolen",
            "Steven de Rooij"
        ],
        "summary": "We show how models for prediction with expert advice can be defined concisely and clearly using hidden Markov models (HMMs); standard HMM algorithms can then be used to efficiently calculate, among other things, how the expert predictions should be weighted according to the model. We cast many existing models as HMMs and recover the best known running times in each case. We also describe two new models: the switch distribution, which was recently developed to improve Bayesian/Minimum Description Length model selection, and a new generalisation of the fixed share algorithm based on run-length coding. We give loss bounds for all models and shed new light on their relationships.",
        "published": "2008-02-14T14:54:57Z",
        "link": "http://arxiv.org/abs/0802.2015v2",
        "categories": [
            "cs.LG",
            "cs.DS",
            "cs.IT",
            "math.IT",
            "G.3"
        ]
    },
    {
        "title": "Domination in graphs with bounded propagation: algorithms, formulations   and hardness results",
        "authors": [
            "Ashkan Aazami"
        ],
        "summary": "We introduce a hierarchy of problems between the \\textsc{Dominating Set} problem and the \\textsc{Power Dominating Set} (PDS) problem called the $\\ell$-round power dominating set ($\\ell$-round PDS, for short) problem. For $\\ell=1$, this is the \\textsc{Dominating Set} problem, and for $\\ell\\geq n-1$, this is the PDS problem; here $n$ denotes the number of nodes in the input graph. In PDS the goal is to find a minimum size set of nodes $S$ that power dominates all the nodes, where a node $v$ is power dominated if (1) $v$ is in $S$ or it has a neighbor in $S$, or (2) $v$ has a neighbor $u$ such that $u$ and all of its neighbors except $v$ are power dominated. Note that rule (1) is the same as for the \\textsc{Dominating Set} problem, and that rule (2) is a type of propagation rule that applies iteratively. The $\\ell$-round PDS problem has the same set of rules as PDS, except we apply rule (2) in ``parallel'' in at most $\\ell-1$ rounds. We prove that $\\ell$-round PDS cannot be approximated better than $2^{\\log^{1-\\epsilon}{n}}$ even for $\\ell=4$ in general graphs. We provide a dynamic programming algorithm to solve $\\ell$-round PDS optimally in polynomial time on graphs of bounded tree-width. We present a PTAS (polynomial time approximation scheme) for $\\ell$-round PDS on planar graphs for $\\ell=O(\\tfrac{\\log{n}}{\\log{\\log{n}}})$. Finally, we give integer programming formulations for $\\ell$-round PDS.",
        "published": "2008-02-15T02:55:52Z",
        "link": "http://arxiv.org/abs/0802.2130v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Choice numbers of graphs",
        "authors": [
            "Shai Gutner"
        ],
        "summary": "A solution to a problem of Erd\\H{o}s, Rubin and Taylor is obtained by showing that if a graph $G$ is $(a:b)$-choosable, and $c/d > a/b$, then $G$ is not necessarily $(c:d)$-choosable. The simplest case of another problem, stated by the same authors, is settled, proving that every 2-choosable graph is also $(4:2)$-choosable. Applying probabilistic methods, an upper bound for the $k^{th}$ choice number of a graph is given. We also prove that a directed graph with maximum outdegree $d$ and no odd directed cycle is $(k(d+1):k)$-choosable for every $k \\geq 1$. Other results presented in this article are related to the strong choice number of graphs (a generalization of the strong chromatic number). We conclude with complexity analysis of some decision problems related to graph choosability.",
        "published": "2008-02-15T09:05:54Z",
        "link": "http://arxiv.org/abs/0802.2157v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Set Covering Problems with General Objective Functions",
        "authors": [
            "Jean Cardinal",
            "Christophe Dumeunier"
        ],
        "summary": "We introduce a parameterized version of set cover that generalizes several previously studied problems. Given a ground set V and a collection of subsets S_i of V, a feasible solution is a partition of V such that each subset of the partition is included in one of the S_i. The problem involves maximizing the mean subset size of the partition, where the mean is the generalized mean of parameter p, taken over the elements. For p=-1, the problem is equivalent to the classical minimum set cover problem. For p=0, it is equivalent to the minimum entropy set cover problem, introduced by Halperin and Karp. For p=1, the problem includes the maximum-edge clique partition problem as a special case. We prove that the greedy algorithm simultaneously approximates the problem within a factor of (p+1)^1/p for any p in R^+, and that this is the best possible unless P=NP. These results both generalize and simplify previous results for special cases. We also consider the corresponding graph coloring problem, and prove several tractability and inapproximability results. Finally, we consider a further generalization of the set cover problem in which we aim at minimizing the sum of some concave function of the part sizes. As an application, we derive an approximation ratio for a Rent-or-Buy set cover problem.",
        "published": "2008-02-15T11:56:28Z",
        "link": "http://arxiv.org/abs/0802.2184v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Digraph Decompositions and Monotonicity in Digraph Searching",
        "authors": [
            "Stephan Kreutzer",
            "Sebastian Ordyniak"
        ],
        "summary": "We consider monotonicity problems for graph searching games. Variants of these games - defined by the type of moves allowed for the players - have been found to be closely connected to graph decompositions and associated width measures such as path- or tree-width. Of particular interest is the question whether these games are monotone, i.e. whether the cops can catch a robber without ever allowing the robber to reach positions that have been cleared before. The monotonicity problem for graph searching games has intensely been studied in the literature, but for two types of games the problem was left unresolved. These are the games on digraphs where the robber is invisible and lazy or visible and fast. In this paper, we solve the problems by giving examples showing that both types of games are non-monotone. Graph searching games on digraphs are closely related to recent proposals for digraph decompositions generalising tree-width to directed graphs. These proposals have partly been motivated by attempts to develop a structure theory for digraphs similar to the graph minor theory developed by Robertson and Seymour for undirected graphs, and partly by the immense number of algorithmic results using tree-width of undirected graphs and the hope that part of this success might be reproducible on digraphs using a directed tree-width. Unfortunately the number of applications for the digraphs measures introduced so far is still small. We therefore explore the limits of the algorithmic applicability of digraph decompositions. In particular, we show that various natural candidates for problems that might benefit from digraphs having small directed tree-width remain NP-complete even on almost acyclic graphs.",
        "published": "2008-02-15T15:44:34Z",
        "link": "http://arxiv.org/abs/0802.2228v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Compressed Counting",
        "authors": [
            "Ping Li"
        ],
        "summary": "Counting is among the most fundamental operations in computing. For example, counting the pth frequency moment has been a very active area of research, in theoretical computer science, databases, and data mining. When p=1, the task (i.e., counting the sum) can be accomplished using a simple counter.   Compressed Counting (CC) is proposed for efficiently computing the pth frequency moment of a data stream signal A_t, where 0<p<=2. CC is applicable if the streaming data follow the Turnstile model, with the restriction that at the time t for the evaluation, A_t[i]>= 0, which includes the strict Turnstile model as a special case. For natural data streams encountered in practice, this restriction is minor.   The underly technique for CC is what we call skewed stable random projections, which captures the intuition that, when p=1 a simple counter suffices, and when p = 1+/\\Delta with small \\Delta, the sample complexity of a counter system should be low (continuously as a function of \\Delta). We show at small \\Delta the sample complexity (number of projections) k = O(1/\\epsilon) instead of O(1/\\epsilon^2).   Compressed Counting can serve a basic building block for other tasks in statistics and computing, for example, estimation entropies of data streams, parameter estimations using the method of moments and maximum likelihood.   Finally, another contribution is an algorithm for approximating the logarithmic norm, \\sum_{i=1}^D\\log A_t[i], and logarithmic distance. The logarithmic distance is useful in machine learning practice with heavy-tailed data.",
        "published": "2008-02-17T16:42:52Z",
        "link": "http://arxiv.org/abs/0802.2305v2",
        "categories": [
            "cs.IT",
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Min-Cost 2-Connected Subgraphs With k Terminals",
        "authors": [
            "Chandra Chekuri",
            "Nitish Korula"
        ],
        "summary": "In the k-2VC problem, we are given an undirected graph G with edge costs and an integer k; the goal is to find a minimum-cost 2-vertex-connected subgraph of G containing at least k vertices. A slightly more general version is obtained if the input also specifies a subset S \\subseteq V of terminals and the goal is to find a subgraph containing at least k terminals. Closely related to the k-2VC problem, and in fact a special case of it, is the k-2EC problem, in which the goal is to find a minimum-cost 2-edge-connected subgraph containing k vertices. The k-2EC problem was introduced by Lau et al., who also gave a poly-logarithmic approximation for it. No previous approximation algorithm was known for the more general k-2VC problem. We describe an O(\\log n \\log k) approximation for the k-2VC problem.",
        "published": "2008-02-18T18:34:28Z",
        "link": "http://arxiv.org/abs/0802.2528v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Improved Approximations for Multiprocessor Scheduling Under Uncertainty",
        "authors": [
            "Christopher Crutchfield",
            "Zoran Dzunic",
            "Jeremy T. Fineman",
            "David R. Karger",
            "Jacob Scott"
        ],
        "summary": "This paper presents improved approximation algorithms for the problem of multiprocessor scheduling under uncertainty, or SUU, in which the execution of each job may fail probabilistically. This problem is motivated by the increasing use of distributed computing to handle large, computationally intensive tasks. In the SUU problem we are given n unit-length jobs and m machines, a directed acyclic graph G of precedence constraints among jobs, and unrelated failure probabilities q_{ij} for each job j when executed on machine i for a single timestep. Our goal is to find a schedule that minimizes the expected makespan, which is the expected time at which all jobs complete.   Lin and Rajaraman gave the first approximations for this NP-hard problem for the special cases of independent jobs, precedence constraints forming disjoint chains, and precedence constraints forming trees. In this paper, we present asymptotically better approximation algorithms. In particular, we give an O(loglog min(m,n))-approximation for independent jobs (improving on the previously best O(log n)-approximation). We also give an O(log(n+m) loglog min(m,n))-approximation algorithm for precedence constraints that form disjoint chains (improving on the previously best O(log(n)log(m)log(n+m)/loglog(n+m))-approximation by a (log n/loglog n)^2 factor when n = poly(m). Our algorithm for precedence constraints forming chains can also be used as a component for precedence constraints forming trees, yielding a similar improvement over the previously best algorithms for trees.",
        "published": "2008-02-18T20:57:17Z",
        "link": "http://arxiv.org/abs/0802.2418v2",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "On Subgraph Isomorphism",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "Article explicitly expresses Subgraph Isomorphism by a polynomial size asymmetric linear system.",
        "published": "2008-02-19T09:06:40Z",
        "link": "http://arxiv.org/abs/0802.2612v2",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS",
            "math.CO",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "The complexity of planar graph choosability",
        "authors": [
            "Shai Gutner"
        ],
        "summary": "A graph $G$ is {\\em $k$-choosable} if for every assignment of a set $S(v)$ of $k$ colors to every vertex $v$ of $G$, there is a proper coloring of $G$ that assigns to each vertex $v$ a color from $S(v)$. We consider the complexity of deciding whether a given graph is $k$-choosable for some constant $k$. In particular, it is shown that deciding whether a given planar graph is 4-choosable is NP-hard, and so is the problem of deciding whether a given planar triangle-free graph is 3-choosable. We also obtain simple constructions of a planar graph which is not 4-choosable and a planar triangle-free graph which is not 3-choosable.",
        "published": "2008-02-19T15:26:19Z",
        "link": "http://arxiv.org/abs/0802.2668v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "The Isomorphism Problem for Planar 3-Connected Graphs is in Unambiguous   Logspace",
        "authors": [
            "Thomas Thierauf",
            "Fabian Wagner"
        ],
        "summary": "The isomorphism problem for planar graphs is known to be efficiently solvable. For planar 3-connected graphs, the isomorphism problem can be solved by efficient parallel algorithms, it is in the class $AC^1$. In this paper we improve the upper bound for planar 3-connected graphs to unambiguous logspace, in fact to $UL \\cap coUL$. As a consequence of our method we get that the isomorphism problem for oriented graphs is in $NL$. We also show that the problems are hard for $L$.",
        "published": "2008-02-20T14:03:55Z",
        "link": "http://arxiv.org/abs/0802.2825v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Efficient Minimization of DFAs with Partial Transition Functions",
        "authors": [
            "Antti Valmari",
            "Petri Lehtinen"
        ],
        "summary": "Let PT-DFA mean a deterministic finite automaton whose transition relation is a partial function. We present an algorithm for minimizing a PT-DFA in $O(m \\lg n)$ time and $O(m+n+\\alpha)$ memory, where $n$ is the number of states, $m$ is the number of defined transitions, and $\\alpha$ is the size of the alphabet. Time consumption does not depend on $\\alpha$, because the $\\alpha$ term arises from an array that is accessed at random and never initialized. It is not needed, if transitions are in a suitable order in the input. The algorithm uses two instances of an array-based data structure for maintaining a refinable partition. Its operations are all amortized constant time. One instance represents the classical blocks and the other a partition of transitions. Our measurements demonstrate the speed advantage of our algorithm on PT-DFAs over an $O(\\alpha n \\lg n)$ time, $O(\\alpha n)$ memory algorithm.",
        "published": "2008-02-20T14:04:34Z",
        "link": "http://arxiv.org/abs/0802.2826v1",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT"
        ]
    },
    {
        "title": "Design by Measure and Conquer, A Faster Exact Algorithm for Dominating   Set",
        "authors": [
            "Johan M. M. Van Rooij",
            "Hans L. Bodlaender"
        ],
        "summary": "The measure and conquer approach has proven to be a powerful tool to analyse exact algorithms for combinatorial problems, like Dominating Set and Independent Set. In this paper, we propose to use measure and conquer also as a tool in the design of algorithms. In an iterative process, we can obtain a series of branch and reduce algorithms. A mathematical analysis of an algorithm in the series with measure and conquer results in a quasiconvex programming problem. The solution by computer to this problem not only gives a bound on the running time, but also can give a new reduction rule, thus giving a new, possibly faster algorithm. This makes design by measure and conquer a form of computer aided algorithm design. When we apply the methodology to a Set Cover modelling of the Dominating Set problem, we obtain the currently fastest known exact algorithms for Dominating Set: an algorithm that uses $O(1.5134^n)$ time and polynomial space, and an algorithm that uses $O(1.5063^n)$ time.",
        "published": "2008-02-20T14:05:58Z",
        "link": "http://arxiv.org/abs/0802.2827v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Understanding maximal repetitions in strings",
        "authors": [
            "Maxime Crochemore",
            "Lucian Ilie"
        ],
        "summary": "The cornerstone of any algorithm computing all repetitions in a string of length n in O(n) time is the fact that the number of runs (or maximal repetitions) is O(n). We give a simple proof of this result. As a consequence of our approach, the stronger result concerning the linearity of the sum of exponents of all runs follows easily.",
        "published": "2008-02-20T14:10:15Z",
        "link": "http://arxiv.org/abs/0802.2829v1",
        "categories": [
            "cs.DS",
            "math.CO"
        ]
    },
    {
        "title": "Rent, Lease or Buy: Randomized Algorithms for Multislope Ski Rental",
        "authors": [
            "Zvi Lotker",
            "Boaz Patt-Shamir",
            "Dror Rawitz"
        ],
        "summary": "In the Multislope Ski Rental problem, the user needs a certain resource for some unknown period of time. To use the resource, the user must subscribe to one of several options, each of which consists of a one-time setup cost (``buying price''), and cost proportional to the duration of the usage (``rental rate''). The larger the price, the smaller the rent. The actual usage time is determined by an adversary, and the goal of an algorithm is to minimize the cost by choosing the best option at any point in time. Multislope Ski Rental is a natural generalization of the classical Ski Rental problem (where the only options are pure rent and pure buy), which is one of the fundamental problems of online computation. The Multislope Ski Rental problem is an abstraction of many problems where online decisions cannot be modeled by just two options, e.g., power management in systems which can be shut down in parts. In this paper we study randomized algorithms for Multislope Ski Rental. Our results include the best possible online randomized strategy for any additive instance, where the cost of switching from one option to another is the difference in their buying prices; and an algorithm that produces an $e$-competitive randomized strategy for any (non-additive) instance.",
        "published": "2008-02-20T14:13:19Z",
        "link": "http://arxiv.org/abs/0802.2832v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Trimmed Moebius Inversion and Graphs of Bounded Degree",
        "authors": [
            "Andreas Björklund",
            "Thore Husfeldt",
            "Petteri Kaski",
            "Mikko Koivisto"
        ],
        "summary": "We study ways to expedite Yates's algorithm for computing the zeta and Moebius transforms of a function defined on the subset lattice. We develop a trimmed variant of Moebius inversion that proceeds point by point, finishing the calculation at a subset before considering its supersets. For an $n$-element universe $U$ and a family $\\scr F$ of its subsets, trimmed Moebius inversion allows us to compute the number of packings, coverings, and partitions of $U$ with $k$ sets from $\\scr F$ in time within a polynomial factor (in $n$) of the number of supersets of the members of $\\scr F$. Relying on an intersection theorem of Chung et al. (1986) to bound the sizes of set families, we apply these ideas to well-studied combinatorial optimisation problems on graphs of maximum degree $\\Delta$. In particular, we show how to compute the Domatic Number in time within a polynomial factor of $(2^{\\Delta+1-2)^{n/(\\Delta+1)$ and the Chromatic Number in time within a polynomial factor of $(2^{\\Delta+1-\\Delta-1)^{n/(\\Delta+1)$. For any constant $\\Delta$, these bounds are $O\\bigl((2-\\epsilon)^n\\bigr)$ for $\\epsilon>0$ independent of the number of vertices $n$.",
        "published": "2008-02-20T14:15:00Z",
        "link": "http://arxiv.org/abs/0802.2834v1",
        "categories": [
            "cs.DS",
            "math.CO"
        ]
    },
    {
        "title": "Minimizing Flow Time in the Wireless Gathering Problem",
        "authors": [
            "Vincenzo Bonifaci",
            "Peter Korteweg",
            "Alberto Marchetti-Spaccamela",
            "Leen Stougie"
        ],
        "summary": "We address the problem of efficient data gathering in a wireless network through multi-hop communication. We focus on the objective of minimizing the maximum flow time of a data packet. We prove that no polynomial time algorithm for this problem can have approximation ratio less than $\\Omega(m^{1/3)$ when $m$ packets have to be transmitted, unless $P = NP$. We then use resource augmentation to assess the performance of a FIFO-like strategy. We prove that this strategy is 5-speed optimal, i.e., its cost remains within the optimal cost if we allow the algorithm to transmit data at a speed 5 times higher than that of the optimal solution we compare to.",
        "published": "2008-02-20T14:18:24Z",
        "link": "http://arxiv.org/abs/0802.2836v1",
        "categories": [
            "cs.DS",
            "cs.NI"
        ]
    },
    {
        "title": "Factoring Polynomials over Finite Fields using Balance Test",
        "authors": [
            "Chandan Saha"
        ],
        "summary": "We study the problem of factoring univariate polynomials over finite fields. Under the assumption of the Extended Riemann Hypothesis (ERH), (Gao, 2001) designed a polynomial time algorithm that fails to factor only if the input polynomial satisfies a strong symmetry property, namely square balance. In this paper, we propose an extension of Gao's algorithm that fails only under an even stronger symmetry property. We also show that our property can be used to improve the time complexity of best deterministic algorithms on most input polynomials. The property also yields a new randomized polynomial time algorithm.",
        "published": "2008-02-20T14:18:52Z",
        "link": "http://arxiv.org/abs/0802.2838v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Stackelberg Network Pricing Games",
        "authors": [
            "Patrick Briest",
            "Martin Hoefer",
            "Piotr Krysta"
        ],
        "summary": "We study a multi-player one-round game termed Stackelberg Network Pricing Game, in which a leader can set prices for a subset of $m$ priceable edges in a graph. The other edges have a fixed cost. Based on the leader's decision one or more followers optimize a polynomial-time solvable combinatorial minimization problem and choose a minimum cost solution satisfying their requirements based on the fixed costs and the leader's prices. The leader receives as revenue the total amount of prices paid by the followers for priceable edges in their solutions, and the problem is to find revenue maximizing prices. Our model extends several known pricing problems, including single-minded and unit-demand pricing, as well as Stackelberg pricing for certain follower problems like shortest path or minimum spanning tree. Our first main result is a tight analysis of a single-price algorithm for the single follower game, which provides a $(1+\\epsilon) \\log m$-approximation for any $\\epsilon >0$. This can be extended to provide a $(1+\\epsilon)(\\log k + \\log m)$-approximation for the general problem and $k$ followers. The latter result is essentially best possible, as the problem is shown to be hard to approximate within $\\mathcal{O(\\log^\\epsilon k + \\log^\\epsilon m)$. If followers have demands, the single-price algorithm provides a $(1+\\epsilon)m^2$-approximation, and the problem is hard to approximate within $\\mathcal{O(m^\\epsilon)$ for some $\\epsilon >0$. Our second main result is a polynomial time algorithm for revenue maximization in the special case of Stackelberg bipartite vertex cover, which is based on non-trivial max-flow and LP-duality techniques. Our results can be extended to provide constant-factor approximations for any constant number of followers.",
        "published": "2008-02-20T14:19:33Z",
        "link": "http://arxiv.org/abs/0802.2841v1",
        "categories": [
            "cs.DS",
            "cs.GT"
        ]
    },
    {
        "title": "Sublinear Communication Protocols for Multi-Party Pointer Jumping and a   Related Lower Bound",
        "authors": [
            "Joshua Brody",
            "Amit Chakrabarti"
        ],
        "summary": "We study the one-way number-on-the-forehead (NOF) communication complexity of the $k$-layer pointer jumping problem with $n$ vertices per layer. This classic problem, which has connections to many aspects of complexity theory, has seen a recent burst of research activity, seemingly preparing the ground for an $\\Omega(n)$ lower bound, for constant $k$. Our first result is a surprising sublinear -- i.e., $o(n)$ -- upper bound for the problem that holds for $k \\ge 3$, dashing hopes for such a lower bound. A closer look at the protocol achieving the upper bound shows that all but one of the players involved are collapsing, i.e., their messages depend only on the composition of the layers ahead of them. We consider protocols for the pointer jumping problem where all players are collapsing. Our second result shows that a strong $n - O(\\log n)$ lower bound does hold in this case. Our third result is another upper bound showing that nontrivial protocols for (a non-Boolean version of) pointer jumping are possible even when all players are collapsing. Our lower bound result uses a novel proof technique, different from those of earlier lower bounds that had an information-theoretic flavor. We hope this is useful in further study of the problem.",
        "published": "2008-02-20T14:20:14Z",
        "link": "http://arxiv.org/abs/0802.2843v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Shortest Vertex-Disjoint Two-Face Paths in Planar Graphs",
        "authors": [
            "Eric Colin De Verdière",
            "Alexander Schrijver"
        ],
        "summary": "Let $G$ be a directed planar graph of complexity $n$, each arc having a nonnegative length. Let $s$ and $t$ be two distinct faces of $G$; let $s_1,...,s_k$ be vertices incident with $s$; let $t_1,...,t_k$ be vertices incident with $t$. We give an algorithm to compute $k$ pairwise vertex-disjoint paths connecting the pairs $(s_i,t_i)$ in $G$, with minimal total length, in $O(kn\\log n)$ time.",
        "published": "2008-02-20T14:20:48Z",
        "link": "http://arxiv.org/abs/0802.2845v1",
        "categories": [
            "cs.DS",
            "math.CO"
        ]
    },
    {
        "title": "Geodesic Fréchet Distance Inside a Simple Polygon",
        "authors": [
            "Atlas F. Cook IV",
            "Carola Wenk"
        ],
        "summary": "We unveil an alluring alternative to parametric search that applies to both the non-geodesic and geodesic Fr\\'echet optimization problems. This randomized approach is based on a variant of red-blue intersections and is appealing due to its elegance and practical efficiency when compared to parametric search. We present the first algorithm for the geodesic Fr\\'echet distance between two polygonal curves $A$ and $B$ inside a simple bounding polygon $P$. The geodesic Fr\\'echet decision problem is solved almost as fast as its non-geodesic sibling and requires $O(N^{2\\log k)$ time and $O(k+N)$ space after $O(k)$ preprocessing, where $N$ is the larger of the complexities of $A$ and $B$ and $k$ is the complexity of $P$. The geodesic Fr\\'echet optimization problem is solved by a randomized approach in $O(k+N^{2\\log kN\\log N)$ expected time and $O(k+N^{2)$ space. This runtime is only a logarithmic factor larger than the standard non-geodesic Fr\\'echet algorithm (Alt and Godau 1995). Results are also presented for the geodesic Fr\\'echet distance in a polygonal domain with obstacles and the geodesic Hausdorff distance for sets of points or sets of line segments inside a simple polygon $P$.",
        "published": "2008-02-20T14:21:19Z",
        "link": "http://arxiv.org/abs/0802.2846v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "On Dynamic Breadth-First Search in External-Memory",
        "authors": [
            "Ulrich Meyer"
        ],
        "summary": "We provide the first non-trivial result on dynamic breadth-first search (BFS) in external-memory: For general sparse undirected graphs of initially $n$ nodes and O(n) edges and monotone update sequences of either $\\Theta(n)$ edge insertions or $\\Theta(n)$ edge deletions, we prove an amortized high-probability bound of $O(n/B^{2/3}+\\sort(n)\\cdot \\log B)$ I/Os per update. In contrast, the currently best approach for static BFS on sparse undirected graphs requires $\\Omega(n/B^{1/2}+\\sort(n))$ I/Os.",
        "published": "2008-02-20T14:21:21Z",
        "link": "http://arxiv.org/abs/0802.2847v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Deterministically Isolating a Perfect Matching in Bipartite Planar   Graphs",
        "authors": [
            "Samir Datta",
            "Raghav Kulkarni",
            "Sambuddha Roy"
        ],
        "summary": "We present a deterministic way of assigning small (log bit) weights to the edges of a bipartite planar graph so that the minimum weight perfect matching becomes unique. The isolation lemma as described in (Mulmuley et al. 1987) achieves the same for general graphs using a randomized weighting scheme, whereas we can do it deterministically when restricted to bipartite planar graphs. As a consequence, we reduce both decision and construction versions of the matching problem to testing whether a matrix is singular, under the promise that its determinant is 0 or 1, thus obtaining a highly parallel SPL algorithm for bipartite planar graphs. This improves the earlier known bounds of non-uniform SPL by (Allender et al. 1999) and $NC^2$ by (Miller and Naor 1995, Mahajan and Varadarajan 2000). It also rekindles the hope of obtaining a deterministic parallel algorithm for constructing a perfect matching in non-bipartite planar graphs, which has been open for a long time. Our techniques are elementary and simple.",
        "published": "2008-02-20T14:21:52Z",
        "link": "http://arxiv.org/abs/0802.2850v1",
        "categories": [
            "cs.DS",
            "math.CO"
        ]
    },
    {
        "title": "An Improved Randomized Truthful Mechanism for Scheduling Unrelated   Machines",
        "authors": [
            "Pinyan Lu",
            "Changyuan Yu"
        ],
        "summary": "We study the scheduling problem on unrelated machines in the mechanism design setting. This problem was proposed and studied in the seminal paper (Nisan and Ronen 1999), where they gave a 1.75-approximation randomized truthful mechanism for the case of two machines. We improve this result by a 1.6737-approximation randomized truthful mechanism. We also generalize our result to a $0.8368m$-approximation mechanism for task scheduling with $m$ machines, which improve the previous best upper bound of $0.875m(Mu'alem and Schapira 2007).",
        "published": "2008-02-20T14:22:30Z",
        "link": "http://arxiv.org/abs/0802.2851v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Tight Bounds for Blind Search on the Integers",
        "authors": [
            "Martin Dietzfelbinger",
            "Jonathan E. Rowe",
            "Ingo Wegener",
            "Philipp Woelfel"
        ],
        "summary": "We analyze a simple random process in which a token is moved in the interval $A=\\{0,...,n\\$: Fix a probability distribution $\\mu$ over $\\{1,...,n\\$. Initially, the token is placed in a random position in $A$. In round $t$, a random value $d$ is chosen according to $\\mu$. If the token is in position $a\\geq d$, then it is moved to position $a-d$. Otherwise it stays put. Let $T$ be the number of rounds until the token reaches position 0. We show tight bounds for the expectation of $T$ for the optimal distribution $\\mu$. More precisely, we show that $\\min_\\mu\\{E_\\mu(T)\\=\\Theta((\\log n)^2)$. For the proof, a novel potential function argument is introduced. The research is motivated by the problem of approximating the minimum of a continuous function over $[0,1]$ with a ``blind'' optimization strategy.",
        "published": "2008-02-20T14:22:33Z",
        "link": "http://arxiv.org/abs/0802.2852v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Trimming of Graphs, with Application to Point Labeling",
        "authors": [
            "Thomas Erlebach",
            "Torben Hagerup",
            "Klaus Jansen",
            "Moritz Minzlaff",
            "Alexander Wolff"
        ],
        "summary": "For $t,g>0$, a vertex-weighted graph of total weight $W$ is $(t,g)$-trimmable if it contains a vertex-induced subgraph of total weight at least $(1-1/t)W$ and with no simple path of more than $g$ edges. A family of graphs is trimmable if for each constant $t>0$, there is a constant $g=g(t)$ such that every vertex-weighted graph in the family is $(t,g)$-trimmable. We show that every family of graphs of bounded domino treewidth is trimmable. This implies that every family of graphs of bounded degree is trimmable if the graphs in the family have bounded treewidth or are planar. Based on this result, we derive a polynomial-time approximation scheme for the problem of labeling weighted points with nonoverlapping sliding labels of unit height and given lengths so as to maximize the total weight of the labeled points. This settles one of the last major open questions in the theory of map labeling.",
        "published": "2008-02-20T14:23:38Z",
        "link": "http://arxiv.org/abs/0802.2854v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "math.CO"
        ]
    },
    {
        "title": "Computing Minimum Spanning Trees with Uncertainty",
        "authors": [
            "Thomas Erlebach",
            "Michael Hoffmann",
            "Danny Krizanc",
            "Matús Mihal'ák",
            "Rajeev Raman"
        ],
        "summary": "We consider the minimum spanning tree problem in a setting where information about the edge weights of the given graph is uncertain. Initially, for each edge $e$ of the graph only a set $A_e$, called an uncertainty area, that contains the actual edge weight $w_e$ is known. The algorithm can `update' $e$ to obtain the edge weight $w_e \\in A_e$. The task is to output the edge set of a minimum spanning tree after a minimum number of updates. An algorithm is $k$-update competitive if it makes at most $k$ times as many updates as the optimum. We present a 2-update competitive algorithm if all areas $A_e$ are open or trivial, which is the best possible among deterministic algorithms. The condition on the areas $A_e$ is to exclude degenerate inputs for which no constant update competitive algorithm can exist. Next, we consider a setting where the vertices of the graph correspond to points in Euclidean space and the weight of an edge is equal to the distance of its endpoints. The location of each point is initially given as an uncertainty area, and an update reveals the exact location of the point. We give a general relation between the edge uncertainty and the vertex uncertainty versions of a problem and use it to derive a 4-update competitive algorithm for the minimum spanning tree problem in the vertex uncertainty model. Again, we show that this is best possible among deterministic algorithms.",
        "published": "2008-02-20T14:24:10Z",
        "link": "http://arxiv.org/abs/0802.2855v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Convergence Thresholds of Newton's Method for Monotone Polynomial   Equations",
        "authors": [
            "Javier Esparza",
            "Stefan Kiefer",
            "Michael Luttenberger"
        ],
        "summary": "Monotone systems of polynomial equations (MSPEs) are systems of fixed-point equations $X_1 = f_1(X_1, ..., X_n),$ $..., X_n = f_n(X_1, ..., X_n)$ where each $f_i$ is a polynomial with positive real coefficients. The question of computing the least non-negative solution of a given MSPE $\\vec X = \\vec f(\\vec X)$ arises naturally in the analysis of stochastic models such as stochastic context-free grammars, probabilistic pushdown automata, and back-button processes. Etessami and Yannakakis have recently adapted Newton's iterative method to MSPEs. In a previous paper we have proved the existence of a threshold $k_{\\vec f}$ for strongly connected MSPEs, such that after $k_{\\vec f}$ iterations of Newton's method each new iteration computes at least 1 new bit of the solution. However, the proof was purely existential. In this paper we give an upper bound for $k_{\\vec f}$ as a function of the minimal component of the least fixed-point $\\mu\\vec f$ of $\\vec f(\\vec X)$. Using this result we show that $k_{\\vec f}$ is at most single exponential resp. linear for strongly connected MSPEs derived from probabilistic pushdown automata resp. from back-button processes. Further, we prove the existence of a threshold for arbitrary MSPEs after which each new iteration computes at least $1/w2^h$ new bits of the solution, where $w$ and $h$ are the width and height of the DAG of strongly connected components.",
        "published": "2008-02-20T14:24:39Z",
        "link": "http://arxiv.org/abs/0802.2856v2",
        "categories": [
            "cs.DS",
            "cs.IT",
            "cs.NA",
            "math.IT"
        ]
    },
    {
        "title": "Lower bounds for adaptive linearity tests",
        "authors": [
            "Shachar Lovett"
        ],
        "summary": "Linearity tests are randomized algorithms which have oracle access to the truth table of some function f, and are supposed to distinguish between linear functions and functions which are far from linear. Linearity tests were first introduced by (Blum, Luby and Rubenfeld, 1993), and were later used in the PCP theorem, among other applications. The quality of a linearity test is described by its correctness c - the probability it accepts linear functions, its soundness s - the probability it accepts functions far from linear, and its query complexity q - the number of queries it makes. Linearity tests were studied in order to decrease the soundness of linearity tests, while keeping the query complexity small (for one reason, to improve PCP constructions). Samorodnitsky and Trevisan (Samorodnitsky and Trevisan 2000) constructed the Complete Graph Test, and prove that no Hyper Graph Test can perform better than the Complete Graph Test. Later in (Samorodnitsky and Trevisan 2006) they prove, among other results, that no non-adaptive linearity test can perform better than the Complete Graph Test. Their proof uses the algebraic machinery of the Gowers Norm. A result by (Ben-Sasson, Harsha and Raskhodnikova 2005) allows to generalize this lower bound also to adaptive linearity tests. We also prove the same optimal lower bound for adaptive linearity test, but our proof technique is arguably simpler and more direct than the one used in (Samorodnitsky and Trevisan 2006). We also study, like (Samorodnitsky and Trevisan 2006), the behavior of linearity tests on quadratic functions. However, instead of analyzing the Gowers Norm of certain functions, we provide a more direct combinatorial proof, studying the behavior of linearity tests on random quadratic functions...",
        "published": "2008-02-20T14:26:40Z",
        "link": "http://arxiv.org/abs/0802.2857v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "On Geometric Spanners of Euclidean and Unit Disk Graphs",
        "authors": [
            "Iyad A. Kanj",
            "Ljubomir Perkovic"
        ],
        "summary": "We consider the problem of constructing bounded-degree planar geometric spanners of Euclidean and unit-disk graphs. It is well known that the Delaunay subgraph is a planar geometric spanner with stretch factor $C_{del\\approx 2.42$; however, its degree may not be bounded. Our first result is a very simple linear time algorithm for constructing a subgraph of the Delaunay graph with stretch factor $\\rho =1+2\\pi(k\\cos{\\frac{\\pi{k)^{-1$ and degree bounded by $k$, for any integer parameter $k\\geq 14$. This result immediately implies an algorithm for constructing a planar geometric spanner of a Euclidean graph with stretch factor $\\rho \\cdot C_{del$ and degree bounded by $k$, for any integer parameter $k\\geq 14$. Moreover, the resulting spanner contains a Euclidean Minimum Spanning Tree (EMST) as a subgraph. Our second contribution lies in developing the structural results necessary to transfer our analysis and algorithm from Euclidean graphs to unit disk graphs, the usual model for wireless ad-hoc networks. We obtain a very simple distributed, {\\em strictly-localized algorithm that, given a unit disk graph embedded in the plane, constructs a geometric spanner with the above stretch factor and degree bound, and also containing an EMST as a subgraph. The obtained results dramatically improve the previous results in all aspects, as shown in the paper.",
        "published": "2008-02-20T14:36:52Z",
        "link": "http://arxiv.org/abs/0802.2864v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Fixed Parameter Polynomial Time Algorithms for Maximum Agreement and   Compatible Supertrees",
        "authors": [
            "Viet Tung Hoang",
            "Wing-Kin Sung"
        ],
        "summary": "Consider a set of labels $L$ and a set of trees ${\\mathcal T} = \\{{\\mathcal T}^{(1), {\\mathcal T}^{(2), ..., {\\mathcal T}^{(k) \\$ where each tree ${\\mathcal T}^{(i)$ is distinctly leaf-labeled by some subset of $L$. One fundamental problem is to find the biggest tree (denoted as supertree) to represent $\\mathcal T}$ which minimizes the disagreements with the trees in ${\\mathcal T}$ under certain criteria. This problem finds applications in phylogenetics, database, and data mining. In this paper, we focus on two particular supertree problems, namely, the maximum agreement supertree problem (MASP) and the maximum compatible supertree problem (MCSP). These two problems are known to be NP-hard for $k \\geq 3$. This paper gives the first polynomial time algorithms for both MASP and MCSP when both $k$ and the maximum degree $D$ of the trees are constant.",
        "published": "2008-02-20T14:38:47Z",
        "link": "http://arxiv.org/abs/0802.2867v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Sketch-Based Estimation of Subpopulation-Weight",
        "authors": [
            "Edith Cohen",
            "Haim Kaplan"
        ],
        "summary": "Summaries of massive data sets support approximate query processing over the original data. A basic aggregate over a set of records is the weight of subpopulations specified as a predicate over records' attributes. Bottom-k sketches are a powerful summarization format of weighted items that includes priority sampling and the classic weighted sampling without replacement. They can be computed efficiently for many representations of the data including distributed databases and data streams.   We derive novel unbiased estimators and efficient confidence bounds for subpopulation weight. Our estimators and bounds are tailored by distinguishing between applications (such as data streams) where the total weight of the sketched set can be computed by the summarization algorithm without a significant use of additional resources, and applications (such as sketches of network neighborhoods) where this is not the case.   Our rigorous derivations are based on clever applications of the Horvitz-Thompson estimator, and are complemented by efficient computational methods. We demonstrate their benefit on a wide range of Pareto distributions.",
        "published": "2008-02-23T15:25:04Z",
        "link": "http://arxiv.org/abs/0802.3448v1",
        "categories": [
            "cs.DB",
            "cs.DS",
            "cs.NI",
            "cs.PF",
            "H.3.3; H.2; F.2.2"
        ]
    },
    {
        "title": "Deriving Sorting Algorithms",
        "authors": [
            "José Bacelar Almeida",
            "Jorge Sousa Pinto"
        ],
        "summary": "This paper proposes new derivations of three well-known sorting algorithms, in their functional formulation. The approach we use is based on three main ingredients: first, the algorithms are derived from a simpler algorithm, i.e. the specification is already a solution to the problem (in this sense our derivations are program transformations). Secondly, a mixture of inductive and coinductive arguments are used in a uniform, algebraic style in our reasoning. Finally, the approach uses structural invariants so as to strengthen the equational reasoning with logical arguments that cannot be captured in the algebraic framework.",
        "published": "2008-02-26T19:47:57Z",
        "link": "http://arxiv.org/abs/0802.3881v1",
        "categories": [
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "Analysis of the Karmarkar-Karp Differencing Algorithm",
        "authors": [
            "Stefan Boettcher",
            "Stephan Mertens"
        ],
        "summary": "The Karmarkar-Karp differencing algorithm is the best known polynomial time heuristic for the number partitioning problem, fundamental in both theoretical computer science and statistical physics. We analyze the performance of the differencing algorithm on random instances by mapping it to a nonlinear rate equation. Our analysis reveals strong finite size effects that explain why the precise asymptotics of the differencing solution is hard to establish by simulations. The asymptotic series emerging from the rate equation satisfies all known bounds on the Karmarkar-Karp algorithm and projects a scaling $n^{-c\\ln n}$, where $c=1/(2\\ln2)=0.7213...$. Our calculations reveal subtle relations between the algorithm and Fibonacci-like sequences, and we establish an explicit identity to that effect.",
        "published": "2008-02-27T17:24:07Z",
        "link": "http://arxiv.org/abs/0802.4040v2",
        "categories": [
            "cs.NA",
            "cond-mat.dis-nn",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Call Admission Control Algorithm for pre-stored VBR video streams",
        "authors": [
            "Christos Tryfonas",
            "Dimitris Papamichail",
            "Andrew Mehler",
            "Steven Skiena"
        ],
        "summary": "We examine the problem of accepting a new request for a pre-stored VBR video stream that has been smoothed using any of the smoothing algorithms found in the literature. The output of these algorithms is a piecewise constant-rate schedule for a Variable Bit-Rate (VBR) stream. The schedule guarantees that the decoder buffer does not overflow or underflow. The problem addressed in this paper is the determination of the minimal time displacement of each new requested VBR stream so that it can be accomodated by the network and/or the video server without overbooking the committed traffic. We prove that this call-admission control problem for multiple requested VBR streams is NP-complete and inapproximable within a constant factor, by reducing it from the VERTEX COLOR problem. We also present a deterministic morphology-sensitive algorithm that calculates the minimal time displacement of a VBR stream request. The complexity of the proposed algorithm make it suitable for real-time determination of the time displacement parameter during the call admission phase.",
        "published": "2008-02-28T17:45:03Z",
        "link": "http://arxiv.org/abs/0802.4244v1",
        "categories": [
            "cs.NI",
            "cs.DS"
        ]
    },
    {
        "title": "A Simple Yao-Yao-Based Spanner of Bounded Degree",
        "authors": [
            "Mirela Damian"
        ],
        "summary": "It is a standing open question to decide whether the Yao-Yao structure for unit disk graphs (UDGs) is a length spanner of not. This question is highly relevant to the topology control problem for wireless ad hoc networks. In this paper we make progress towards resolving this question by showing that the Yao-Yao structure is a length spanner for UDGs of bounded aspect ratio. We also propose a new local algorithm, called Yao-Sparse-Sink, based on the Yao-Sink method introduced by Li, Wan, Wang and Frieder, that computes a (1+e)-spanner of bounded degree for a given UDG and for given e > 0. The Yao-Sparse-Sink method enables an efficient local computation of sparse sink trees. Finally, we show that all these structures for UDGs -- Yao, Yao-Yao, Yao-Sink and Yao-Sparse-Sink -- have arbitrarily large weight.",
        "published": "2008-02-29T14:39:59Z",
        "link": "http://arxiv.org/abs/0802.4325v2",
        "categories": [
            "cs.CG",
            "cs.DS",
            "C.2.1; F.2.2"
        ]
    },
    {
        "title": "Networks become navigable as nodes move and forget",
        "authors": [
            "Augustin Chaintreau",
            "Pierre Fraigniaud",
            "Emmanuelle Lebhar"
        ],
        "summary": "We propose a dynamical process for network evolution, aiming at explaining the emergence of the small world phenomenon, i.e., the statistical observation that any pair of individuals are linked by a short chain of acquaintances computable by a simple decentralized routing algorithm, known as greedy routing. Previously proposed dynamical processes enabled to demonstrate experimentally (by simulations) that the small world phenomenon can emerge from local dynamics. However, the analysis of greedy routing using the probability distributions arising from these dynamics is quite complex because of mutual dependencies. In contrast, our process enables complete formal analysis. It is based on the combination of two simple processes: a random walk process, and an harmonic forgetting process. Both processes reflect natural behaviors of the individuals, viewed as nodes in the network of inter-individual acquaintances. We prove that, in k-dimensional lattices, the combination of these two processes generates long-range links mutually independently distributed as a k-harmonic distribution. We analyze the performances of greedy routing at the stationary regime of our process, and prove that the expected number of steps for routing from any source to any target in any multidimensional lattice is a polylogarithmic function of the distance between the two nodes in the lattice. Up to our knowledge, these results are the first formal proof that navigability in small worlds can emerge from a dynamical process for network evolution. Our dynamical process can find practical applications to the design of spatial gossip and resource location protocols.",
        "published": "2008-03-03T14:44:08Z",
        "link": "http://arxiv.org/abs/0803.0248v1",
        "categories": [
            "cs.DS",
            "C.2.1; C.2.2; C.2.4"
        ]
    },
    {
        "title": "Stream sampling for variance-optimal estimation of subset sums",
        "authors": [
            "Edith Cohen",
            "Nick Duffield",
            "Haim Kaplan",
            "Carsten Lund",
            "Mikkel Thorup"
        ],
        "summary": "From a high volume stream of weighted items, we want to maintain a generic sample of a certain limited size $k$ that we can later use to estimate the total weight of arbitrary subsets. This is the classic context of on-line reservoir sampling, thinking of the generic sample as a reservoir. We present an efficient reservoir sampling scheme, $\\varoptk$, that dominates all previous schemes in terms of estimation quality.   $\\varoptk$ provides {\\em variance optimal unbiased estimation of subset sums}. More precisely, if we have seen $n$ items of the stream, then for {\\em any} subset size $m$, our scheme based on $k$ samples minimizes the average variance over all subsets of size $m$. In fact, the optimality is against any off-line scheme with $k$ samples tailored for the concrete set of items seen. In addition to optimal average variance, our scheme provides tighter worst-case bounds on the variance of {\\em particular} subsets than previously possible. It is efficient, handling each new item of the stream in $O(\\log k)$ time. Finally, it is particularly well suited for combination of samples from different streams in a distributed setting.",
        "published": "2008-03-04T15:12:24Z",
        "link": "http://arxiv.org/abs/0803.0473v2",
        "categories": [
            "cs.DS",
            "C.2.3; E.1; F.2; G.3; H.3"
        ]
    },
    {
        "title": "Fast unfolding of communities in large networks",
        "authors": [
            "Vincent D. Blondel",
            "Jean-Loup Guillaume",
            "Renaud Lambiotte",
            "Etienne Lefebvre"
        ],
        "summary": "We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad-hoc modular networks. .",
        "published": "2008-03-04T15:29:44Z",
        "link": "http://arxiv.org/abs/0803.0476v2",
        "categories": [
            "physics.soc-ph",
            "cond-mat.stat-mech",
            "cs.CY",
            "cs.DS"
        ]
    },
    {
        "title": "Spanning directed trees with many leaves",
        "authors": [
            "N Alon",
            "F. V. Fomin",
            "G. Gutin",
            "M. Krivelevich",
            "S. Saurabh"
        ],
        "summary": "The {\\sc Directed Maximum Leaf Out-Branching} problem is to find an out-branching (i.e. a rooted oriented spanning tree) in a given digraph with the maximum number of leaves. In this paper, we obtain two combinatorial results on the number of leaves in out-branchings. We show that   - every strongly connected $n$-vertex digraph $D$ with minimum in-degree at least 3 has an out-branching with at least $(n/4)^{1/3}-1$ leaves;   - if a strongly connected digraph $D$ does not contain an out-branching with $k$ leaves, then the pathwidth of its underlying graph UG($D$) is $O(k\\log k)$. Moreover, if the digraph is acyclic, the pathwidth is at most $4k$.   The last result implies that it can be decided in time $2^{O(k\\log^2 k)}\\cdot n^{O(1)}$ whether a strongly connected digraph on $n$ vertices has an out-branching with at least $k$ leaves. On acyclic digraphs the running time of our algorithm is $2^{O(k\\log k)}\\cdot n^{O(1)}$.",
        "published": "2008-03-05T16:38:34Z",
        "link": "http://arxiv.org/abs/0803.0701v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Complexity Analysis of Reed-Solomon Decoding over GF(2^m) Without Using   Syndromes",
        "authors": [
            "Ning Chen",
            "Zhiyuan Yan"
        ],
        "summary": "For the majority of the applications of Reed-Solomon (RS) codes, hard decision decoding is based on syndromes. Recently, there has been renewed interest in decoding RS codes without using syndromes. In this paper, we investigate the complexity of syndromeless decoding for RS codes, and compare it to that of syndrome-based decoding. Aiming to provide guidelines to practical applications, our complexity analysis differs in several aspects from existing asymptotic complexity analysis, which is typically based on multiplicative fast Fourier transform (FFT) techniques and is usually in big O notation. First, we focus on RS codes over characteristic-2 fields, over which some multiplicative FFT techniques are not applicable. Secondly, due to moderate block lengths of RS codes in practice, our analysis is complete since all terms in the complexities are accounted for. Finally, in addition to fast implementation using additive FFT techniques, we also consider direct implementation, which is still relevant for RS codes with moderate lengths. Comparing the complexities of both syndromeless and syndrome-based decoding algorithms based on direct and fast implementations, we show that syndromeless decoding algorithms have higher complexities than syndrome-based ones for high rate RS codes regardless of the implementation. Both errors-only and errors-and-erasures decoding are considered in this paper. We also derive tighter bounds on the complexities of fast polynomial multiplications based on Cantor's approach and the fast extended Euclidean algorithm.",
        "published": "2008-03-05T18:54:35Z",
        "link": "http://arxiv.org/abs/0803.0731v2",
        "categories": [
            "cs.IT",
            "cs.CC",
            "cs.DS",
            "math.IT"
        ]
    },
    {
        "title": "A quadratic algorithm for road coloring",
        "authors": [
            "Marie-Pierre Béal",
            "Dominique Perrin"
        ],
        "summary": "The Road Coloring Theorem states that every aperiodic directed graph with constant out-degree has a synchronized coloring. This theorem had been conjectured during many years as the Road Coloring Problem before being settled by A. Trahtman. Trahtman's proof leads to an algorithm that finds a synchronized labeling with a cubic worst-case time complexity. We show a variant of his construction with a worst-case complexity which is quadratic in time and linear in space. We also extend the Road Coloring Theorem to the periodic case.",
        "published": "2008-03-05T20:35:54Z",
        "link": "http://arxiv.org/abs/0803.0726v9",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Incremental Topological Ordering and Strong Component Maintenance",
        "authors": [
            "Bernhard Haeupler",
            "Siddhartha Sen",
            "Robert E. Tarjan"
        ],
        "summary": "We present an on-line algorithm for maintaining a topological order of a directed acyclic graph as arcs are added, and detecting a cycle when one is created. Our algorithm takes O(m^{1/2}) amortized time per arc, where m is the total number of arcs. For sparse graphs, this bound improves the best previous bound by a logarithmic factor and is tight to within a constant factor for a natural class of algorithms that includes all the existing ones. Our main insight is that the bidirectional search method of previous algorithms does not require an ordered search, but can be more general. This allows us to avoid the use of heaps (priority queues) entirely. Instead, the deterministic version of our algorithm uses (approximate) median-finding. The randomized version of our algorithm avoids this complication, making it very simple. We extend our topological ordering algorithm to give the first detailed algorithm for maintaining the strong components of a directed graph, and a topological order of these components, as arcs are added. This extension also has an amortized time bound of O(m^{1/2}) per arc.",
        "published": "2008-03-06T05:11:18Z",
        "link": "http://arxiv.org/abs/0803.0792v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Knapsack cryptosystems built on NP-hard instance",
        "authors": [
            "Laurent Evain"
        ],
        "summary": "We construct three public key knapsack cryptosystems. Standard knapsack cryptosystems hide easy instances of the knapsack problem and have been broken. The systems considered in the article face this problem: They hide a random (possibly hard) instance of the knapsack problem. We provide both complexity results (size of the key, time needed to encypher/decypher...) and experimental results. Security results are given for the second cryptosystem (the fastest one and the one with the shortest key). Probabilistic polynomial reductions show that finding the private key is as difficult as factorizing a product of two primes. We also consider heuristic attacks. First, the density of the cryptosystem can be chosen arbitrarily close to one, discarding low density attacks. Finally, we consider explicit heuristic attacks based on the LLL algorithm and we prove that with respect to these attacks, the public key is as secure as a random key.",
        "published": "2008-03-06T12:20:35Z",
        "link": "http://arxiv.org/abs/0803.0845v1",
        "categories": [
            "cs.CR",
            "cs.CC",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Graph Sparsification by Effective Resistances",
        "authors": [
            "Daniel A. Spielman",
            "Nikhil Srivastava"
        ],
        "summary": "We present a nearly-linear time algorithm that produces high-quality sparsifiers of weighted graphs. Given as input a weighted graph $G=(V,E,w)$ and a parameter $\\epsilon>0$, we produce a weighted subgraph $H=(V,\\tilde{E},\\tilde{w})$ of $G$ such that $|\\tilde{E}|=O(n\\log n/\\epsilon^2)$ and for all vectors $x\\in\\R^V$ $(1-\\epsilon)\\sum_{uv\\in E}(x(u)-x(v))^2w_{uv}\\le \\sum_{uv\\in\\tilde{E}}(x(u)-x(v))^2\\tilde{w}_{uv} \\le (1+\\epsilon)\\sum_{uv\\in E}(x(u)-x(v))^2w_{uv}. (*)$   This improves upon the sparsifiers constructed by Spielman and Teng, which had $O(n\\log^c n)$ edges for some large constant $c$, and upon those of Bencz\\'ur and Karger, which only satisfied (*) for $x\\in\\{0,1\\}^V$.   A key ingredient in our algorithm is a subroutine of independent interest: a nearly-linear time algorithm that builds a data structure from which we can query the approximate effective resistance between any two vertices in a graph in $O(\\log n)$ time.",
        "published": "2008-03-06T18:03:06Z",
        "link": "http://arxiv.org/abs/0803.0929v4",
        "categories": [
            "cs.DS",
            "G.2.2"
        ]
    },
    {
        "title": "Selective association rule generation",
        "authors": [
            "Michael Hahsler",
            "Christian Buchta",
            "Kurt Hornik"
        ],
        "summary": "Mining association rules is a popular and well researched method for discovering interesting relations between variables in large databases. A practical problem is that at medium to low support values often a large number of frequent itemsets and an even larger number of association rules are found in a database. A widely used approach is to gradually increase minimum support and minimum confidence or to filter the found rules using increasingly strict constraints on additional measures of interestingness until the set of rules found is reduced to a manageable size. In this paper we describe a different approach which is based on the idea to first define a set of ``interesting'' itemsets (e.g., by a mixture of mining and expert knowledge) and then, in a second step to selectively generate rules for only these itemsets. The main advantage of this approach over increasing thresholds or filtering rules is that the number of rules found is significantly reduced while at the same time it is not necessary to increase the support and confidence thresholds which might lead to missing important information in the database.",
        "published": "2008-03-06T19:43:35Z",
        "link": "http://arxiv.org/abs/0803.0954v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Faster Approximate Lossy Generalized Flow via Interior Point Algorithms",
        "authors": [
            "Samuel I. Daitch",
            "Daniel A. Spielman"
        ],
        "summary": "We present faster approximation algorithms for generalized network flow problems. A generalized flow is one in which the flow out of an edge differs from the flow into the edge by a constant factor. We limit ourselves to the lossy case, when these factors are at most 1.   Our algorithm uses a standard interior-point algorithm to solve a linear program formulation of the network flow problem. The system of linear equations that arises at each step of the interior-point algorithm takes the form of a symmetric M-matrix. We present an algorithm for solving such systems in nearly linear time. The algorithm relies on the Spielman-Teng nearly linear time algorithm for solving linear systems in diagonally-dominant matrices.   For a graph with m edges, our algorithm obtains an additive epsilon approximation of the maximum generalized flow and minimum cost generalized flow in time tildeO(m^(3/2) * log(1/epsilon)). In many parameter ranges, this improves over previous algorithms by a factor of approximately m^(1/2). We also obtain a similar improvement for exactly solving the standard min-cost flow problem.",
        "published": "2008-03-06T21:57:53Z",
        "link": "http://arxiv.org/abs/0803.0988v2",
        "categories": [
            "cs.DS",
            "cs.NA"
        ]
    },
    {
        "title": "The shortest game of Chinese Checkers and related problems",
        "authors": [
            "George I. Bell"
        ],
        "summary": "In 1979, David Fabian found a complete game of two-person Chinese Checkers in 30 moves (15 by each player) [Martin Gardner, Penrose Tiles to Trapdoor Ciphers, MAA, 1997]. This solution requires that the two players cooperate to generate a win as quickly as possible for one of them. We show, using computational search techniques, that no shorter game is possible. We also consider a solitaire version of Chinese Checkers where one player attempts to move her pieces across the board in as few moves as possible. In 1971, Octave Levenspiel found a solution in 27 moves [Ibid.]; we demonstrate that no shorter solution exists. To show optimality, we employ a variant of A* search, as well as bidirectional search.",
        "published": "2008-03-08T14:38:31Z",
        "link": "http://arxiv.org/abs/0803.1245v2",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.DS",
            "00A08, 97A20"
        ]
    },
    {
        "title": "Treewidth computation and extremal combinatorics",
        "authors": [
            "Fedor V. Fomin",
            "Yngve Villanger"
        ],
        "summary": "For a given graph G and integers b,f >= 0, let S be a subset of vertices of G of size b+1 such that the subgraph of G induced by S is connected and S can be separated from other vertices of G by removing f vertices. We prove that every graph on n vertices contains at most n\\binom{b+f}{b} such vertex subsets. This result from extremal combinatorics appears to be very useful in the design of several enumeration and exact algorithms. In particular, we use it to provide algorithms that for a given n-vertex graph G - compute the treewidth of G in time O(1.7549^n) by making use of exponential space and in time O(2.6151^n) and polynomial space; - decide in time O(({\\frac{2n+k+1}{3})^{k+1}\\cdot kn^6}) if the treewidth of G is at most k; - list all minimal separators of G in time O(1.6181^n) and all potential maximal cliques of G in time O(1.7549^n). This significantly improves previous algorithms for these problems.",
        "published": "2008-03-09T20:54:58Z",
        "link": "http://arxiv.org/abs/0803.1321v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Local Approximation Schemes for Topology Control",
        "authors": [
            "Mirela Damian",
            "Saurav Pandit",
            "Sriram Pemmaraju"
        ],
        "summary": "This paper presents a distributed algorithm on wireless ad-hoc networks that runs in polylogarithmic number of rounds in the size of the network and constructs a linear size, lightweight, (1+\\epsilon)-spanner for any given \\epsilon > 0. A wireless network is modeled by a d-dimensional \\alpha-quasi unit ball graph (\\alpha-UBG), which is a higher dimensional generalization of the standard unit disk graph (UDG) model. The d-dimensional \\alpha-UBG model goes beyond the unrealistic ``flat world'' assumption of UDGs and also takes into account transmission errors, fading signal strength, and physical obstructions. The main result in the paper is this: for any fixed \\epsilon > 0, 0 < \\alpha \\le 1, and d \\ge 2, there is a distributed algorithm running in O(\\log n \\log^* n) communication rounds on an n-node, d-dimensional \\alpha-UBG G that computes a (1+\\epsilon)-spanner G' of G with maximum degree \\Delta(G') = O(1) and total weight w(G') = O(w(MST(G)). This result is motivated by the topology control problem in wireless ad-hoc networks and improves on existing topology control algorithms along several dimensions. The technical contributions of the paper include a new, sequential, greedy algorithm with relaxed edge ordering and lazy updating, and clustering techniques for filtering out unnecessary edges.",
        "published": "2008-03-14T14:37:12Z",
        "link": "http://arxiv.org/abs/0803.2174v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "C.2.4; F.2.2"
        ]
    },
    {
        "title": "Rapport de recherche sur le problème du plus court chemin contraint",
        "authors": [
            "Olivier Laval",
            "Sophie Toulouse",
            "Anass Nagih"
        ],
        "summary": "This article provides an overview of the performance and the theoretical complexity of approximate and exact methods for various versions of the shortest path problem. The proposed study aims to improve the resolution of a more general covering problem within a column generation scheme in which the shortest path problem is the sub-problem.",
        "published": "2008-03-18T12:37:36Z",
        "link": "http://arxiv.org/abs/0803.2615v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Admission Control to Minimize Rejections and Online Set Cover with   Repetitions",
        "authors": [
            "Noga Alon",
            "Yossi Azar",
            "Shai Gutner"
        ],
        "summary": "We study the admission control problem in general networks. Communication requests arrive over time, and the online algorithm accepts or rejects each request while maintaining the capacity limitations of the network. The admission control problem has been usually analyzed as a benefit problem, where the goal is to devise an online algorithm that accepts the maximum number of requests possible. The problem with this objective function is that even algorithms with optimal competitive ratios may reject almost all of the requests, when it would have been possible to reject only a few. This could be inappropriate for settings in which rejections are intended to be rare events.   In this paper, we consider preemptive online algorithms whose goal is to minimize the number of rejected requests. Each request arrives together with the path it should be routed on. We show an $O(\\log^2 (mc))$-competitive randomized algorithm for the weighted case, where $m$ is the number of edges in the graph and $c$ is the maximum edge capacity. For the unweighted case, we give an $O(\\log m \\log c)$-competitive randomized algorithm. This settles an open question of Blum, Kalai and Kleinberg raised in \\cite{BlKaKl01}. We note that allowing preemption and handling requests with given paths are essential for avoiding trivial lower bounds.",
        "published": "2008-03-19T16:53:42Z",
        "link": "http://arxiv.org/abs/0803.2842v1",
        "categories": [
            "cs.DS",
            "C.2.2; F.2.2"
        ]
    },
    {
        "title": "A New Upper Bound for Max-2-Sat: A Graph-Theoretic Approach",
        "authors": [
            "Daniel Raible",
            "Henning Fernau"
        ],
        "summary": "In {\\sc MaxSat}, we ask for an assignment which satisfies the maximum number of clauses for a boolean formula in CNF. We present an algorithm yielding a run time upper bound of $O^*(2^{\\frac{1}{6.2158}})$ for {\\sc Max-2-Sat} (each clause contains at most 2 literals), where $K$ is the number of clauses. The run time has been achieved by using heuristic priorities on the choice of the variable on which we branch. The implementation of these heuristic priorities is rather simple, though they have a significant effect on the run time. The analysis is done using a tailored non-standard measure.",
        "published": "2008-03-25T11:32:22Z",
        "link": "http://arxiv.org/abs/0803.3531v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Void Traversal for Guaranteed Delivery in Geometric Routing",
        "authors": [
            "Mikhail Nesterenko",
            "Adnan Vora"
        ],
        "summary": "Geometric routing algorithms like GFG (GPSR) are lightweight, scalable algorithms that can be used to route in resource-constrained ad hoc wireless networks. However, such algorithms run on planar graphs only. To efficiently construct a planar graph, they require a unit-disk graph. To make the topology unit-disk, the maximum link length in the network has to be selected conservatively. In practical setting this leads to the designs where the node density is rather high. Moreover, the network diameter of a planar subgraph is greater than the original graph, which leads to longer routes. To remedy this problem, we propose a void traversal algorithm that works on arbitrary geometric graphs. We describe how to use this algorithm for geometric routing with guaranteed delivery and compare its performance with GFG.",
        "published": "2008-03-25T20:52:17Z",
        "link": "http://arxiv.org/abs/0803.3632v1",
        "categories": [
            "cs.OS",
            "cs.DC",
            "cs.DS",
            "C.2.2; C.2.1; F.2.2"
        ]
    },
    {
        "title": "Improved Lower Bounds for Constant GC-Content DNA Codes",
        "authors": [
            "Yeow Meng Chee",
            "San Ling"
        ],
        "summary": "The design of large libraries of oligonucleotides having constant GC-content and satisfying Hamming distance constraints between oligonucleotides and their Watson-Crick complements is important in reducing hybridization errors in DNA computing, DNA microarray technologies, and molecular bar coding. Various techniques have been studied for the construction of such oligonucleotide libraries, ranging from algorithmic constructions via stochastic local search to theoretical constructions via coding theory. We introduce a new stochastic local search method which yields improvements up to more than one third of the benchmark lower bounds of Gaborit and King (2005) for n-mer oligonucleotide libraries when n <= 14. We also found several optimal libraries by computing maximum cliques on certain graphs.",
        "published": "2008-03-26T02:26:36Z",
        "link": "http://arxiv.org/abs/0803.3657v1",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.CO",
            "math.IT",
            "q-bio.GN",
            "q-bio.QM"
        ]
    },
    {
        "title": "Succinct Data Structures for Retrieval and Approximate Membership",
        "authors": [
            "Martin Dietzfelbinger",
            "Rasmus Pagh"
        ],
        "summary": "The retrieval problem is the problem of associating data with keys in a set. Formally, the data structure must store a function f: U ->{0,1}^r that has specified values on the elements of a given set S, a subset of U, |S|=n, but may have any value on elements outside S. Minimal perfect hashing makes it possible to avoid storing the set S, but this induces a space overhead of Theta(n) bits in addition to the nr bits needed for function values. In this paper we show how to eliminate this overhead. Moreover, we show that for any k query time O(k) can be achieved using space that is within a factor 1+e^{-k} of optimal, asymptotically for large n. If we allow logarithmic evaluation time, the additive overhead can be reduced to O(log log n) bits whp. The time to construct the data structure is O(n), expected. A main technical ingredient is to utilize existing tight bounds on the probability of almost square random matrices with rows of low weight to have full row rank. In addition to direct constructions, we point out a close connection between retrieval structures and hash tables where keys are stored in an array and some kind of probing scheme is used. Further, we propose a general reduction that transfers the results on retrieval into analogous results on approximate membership, a problem traditionally addressed using Bloom filters. Again, we show how to eliminate the space overhead present in previously known methods, and get arbitrarily close to the lower bound. The evaluation procedures of our data structures are extremely simple (similar to a Bloom filter). For the results stated above we assume free access to fully random hash functions. However, we show how to justify this assumption using extra space o(n) to simulate full randomness on a RAM.",
        "published": "2008-03-26T10:53:49Z",
        "link": "http://arxiv.org/abs/0803.3693v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "Cluster Approach to the Domains Formation",
        "authors": [
            "Leonid B. Litinskii"
        ],
        "summary": "As a rule, a quadratic functional depending on a great number of binary variables has a lot of local minima. One of approaches allowing one to find in averaged deeper local minima is aggregation of binary variables into larger blocks/domains. To minimize the functional one has to change the states of aggregated variables (domains). In the present publication we discuss methods of domains formation. It is shown that the best results are obtained when domains are formed by variables that are strongly connected with each other.",
        "published": "2008-03-26T15:14:33Z",
        "link": "http://arxiv.org/abs/0803.3746v1",
        "categories": [
            "cs.NE",
            "cs.DS"
        ]
    },
    {
        "title": "On Two Dimensional Orthogonal Knapsack Problem",
        "authors": [
            "Xin Han",
            "Kazuo Iwama",
            "Guochuan Zhang"
        ],
        "summary": "In this paper, we study the following knapsack problem: Given a list of squares with profits, we are requested to pack a sublist of them into a rectangular bin (not a unit square bin) to make profits in the bin as large as possible. We first observe there is a Polynomial Time Approximation Scheme (PTAS) for the problem of packing weighted squares into rectangular bins with large resources, then apply the PTAS to the problem of packing squares with profits into a rectangular bin and get a $\\frac65+\\epsilon$ approximation algorithm.",
        "published": "2008-03-29T11:15:11Z",
        "link": "http://arxiv.org/abs/0803.4260v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Grammar-Based Random Walkers in Semantic Networks",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "Semantic networks qualify the meaning of an edge relating any two vertices. Determining which vertices are most \"central\" in a semantic network is difficult because one relationship type may be deemed subjectively more important than another. For this reason, research into semantic network metrics has focused primarily on context-based rankings (i.e. user prescribed contexts). Moreover, many of the current semantic network metrics rank semantic associations (i.e. directed paths between two vertices) and not the vertices themselves. This article presents a framework for calculating semantically meaningful primary eigenvector-based metrics such as eigenvector centrality and PageRank in semantic networks using a modified version of the random walker model of Markov chain analysis. Random walkers, in the context of this article, are constrained by a grammar, where the grammar is a user defined data structure that determines the meaning of the final vertex ranking. The ideas in this article are presented within the context of the Resource Description Framework (RDF) of the Semantic Web initiative.",
        "published": "2008-03-31T00:13:26Z",
        "link": "http://arxiv.org/abs/0803.4355v2",
        "categories": [
            "cs.AI",
            "cs.DS",
            "I.2.4; F.2.1; F.2.2"
        ]
    },
    {
        "title": "From Random Graph to Small World by Wandering",
        "authors": [
            "Bruno Gaume",
            "Fabien Mathieu"
        ],
        "summary": "Numerous studies show that most known real-world complex networks share similar properties in their connectivity and degree distribution. They are called small worlds. This article gives a method to turn random graphs into Small World graphs by the dint of random walks.",
        "published": "2008-04-01T11:59:43Z",
        "link": "http://arxiv.org/abs/0804.0149v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Mapping Semantic Networks to Undirected Networks",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "There exists an injective, information-preserving function that maps a semantic network (i.e a directed labeled network) to a directed network (i.e. a directed unlabeled network). The edge label in the semantic network is represented as a topological feature of the directed network. Also, there exists an injective function that maps a directed network to an undirected network (i.e. an undirected unlabeled network). The edge directionality in the directed network is represented as a topological feature of the undirected network. Through function composition, there exists an injective function that maps a semantic network to an undirected network. Thus, aside from space constraints, the semantic network construct does not have any modeling functionality that is not possible with either a directed or undirected network representation. Two proofs of this idea will be presented. The first is a proof of the aforementioned function composition concept. The second is a simpler proof involving an undirected binary encoding of a semantic network.",
        "published": "2008-04-02T01:19:55Z",
        "link": "http://arxiv.org/abs/0804.0277v1",
        "categories": [
            "cs.DS",
            "F.2.2; F.4.1; E.1"
        ]
    },
    {
        "title": "Exhaustive enumeration unveils clustering and freezing in random 3-SAT",
        "authors": [
            "John Ardelius",
            "Lenka Zdeborová"
        ],
        "summary": "We study geometrical properties of the complete set of solutions of the random 3-satisfiability problem. We show that even for moderate system sizes the number of clusters corresponds surprisingly well with the theoretic asymptotic prediction. We locate the freezing transition in the space of solutions which has been conjectured to be relevant in explaining the onset of computational hardness in random constraint satisfaction problems.",
        "published": "2008-04-02T14:32:44Z",
        "link": "http://arxiv.org/abs/0804.0362v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "A Parameterized Perspective on $P_2$-Packings",
        "authors": [
            "Jianer Chen",
            "Henning Fernau",
            "Dan Ning",
            "Daniel Raible",
            "Jianxin Wang"
        ],
        "summary": "}We study (vertex-disjoint) $P_2$-packings in graphs under a parameterized perspective. Starting from a maximal $P_2$-packing $\\p$ of size $j$ we use extremal arguments for determining how many vertices of $\\p$ appear in some $P_2$-packing of size $(j+1)$. We basically can 'reuse' $2.5j$ vertices. We also present a kernelization algorithm that gives a kernel of size bounded by $7k$. With these two results we build an algorithm which constructs a $P_2$-packing of size $k$ in time $\\Oh^*(2.482^{3k})$.",
        "published": "2008-04-03T14:36:19Z",
        "link": "http://arxiv.org/abs/0804.0570v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Decentralized Search with Random Costs",
        "authors": [
            "Oskar Sandberg"
        ],
        "summary": "A decentralized search algorithm is a method of routing on a random graph that uses only limited, local, information about the realization of the graph. In some random graph models it is possible to define such algorithms which produce short paths when routing from any vertex to any other, while for others it is not.   We consider random graphs with random costs assigned to the edges. In this situation, we use the methods of stochastic dynamic programming to create a decentralized search method which attempts to minimize the total cost, rather than the number of steps, of each path. We show that it succeeds in doing so among all decentralized search algorithms which monotonically approach the destination. Our algorithm depends on knowing the expected cost of routing from every vertex to any other, but we show that this may be calculated iteratively, and in practice can be easily estimated from the cost of previous routes and compressed into a small routing table. The methods applied here can also be applied directly in other situations, such as efficient searching in graphs with varying vertex degrees.",
        "published": "2008-04-03T15:32:29Z",
        "link": "http://arxiv.org/abs/0804.0577v1",
        "categories": [
            "math.PR",
            "cs.DS"
        ]
    },
    {
        "title": "A Memetic Algorithm for the Generalized Traveling Salesman Problem",
        "authors": [
            "Gregory Gutin",
            "Daniel Karapetyan"
        ],
        "summary": "The generalized traveling salesman problem (GTSP) is an extension of the well-known traveling salesman problem. In GTSP, we are given a partition of cities into groups and we are required to find a minimum length tour that includes exactly one city from each group. The recent studies on this subject consider different variations of a memetic algorithm approach to the GTSP. The aim of this paper is to present a new memetic algorithm for GTSP with a powerful local search procedure. The experiments show that the proposed algorithm clearly outperforms all of the known heuristics with respect to both solution quality and running time. While the other memetic algorithms were designed only for the symmetric GTSP, our algorithm can solve both symmetric and asymmetric instances.",
        "published": "2008-04-04T13:21:40Z",
        "link": "http://arxiv.org/abs/0804.0722v3",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Generalized Traveling Salesman Problem Reduction Algorithms",
        "authors": [
            "Gregory Gutin",
            "Daniel Karapetyan"
        ],
        "summary": "The generalized traveling salesman problem (GTSP) is an extension of the well-known traveling salesman problem. In GTSP, we are given a partition of cities into groups and we are required to find a minimum length tour that includes exactly one city from each group. The aim of this paper is to present a problem reduction algorithm that deletes redundant vertices and edges, preserving the optimal solution. The algorithm's running time is O(N^3) in the worst case, but it is significantly faster in practice. The algorithm has reduced the problem size by 15-20% on average in our experiments and this has decreased the solution time by 10-60% for each of the considered solvers.",
        "published": "2008-04-04T13:36:19Z",
        "link": "http://arxiv.org/abs/0804.0735v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Scalable Distributed Video-on-Demand: Theoretical Bounds and Practical   Algorithms",
        "authors": [
            "Laurent Viennot",
            "Yacine Boufkhad",
            "Fabien Mathieu",
            "Fabien De Montgolfier",
            "Diego Perino"
        ],
        "summary": "We analyze a distributed system where n nodes called boxes store a large set of videos and collaborate to serve simultaneously n videos or less. We explore under which conditions such a system can be scalable while serving any sequence of demands. We model this problem through a combination of two algorithms: a video allocation algorithm and a connection scheduling algorithm. The latter plays against an adversary that incrementally proposes video requests.",
        "published": "2008-04-04T14:08:49Z",
        "link": "http://arxiv.org/abs/0804.0743v2",
        "categories": [
            "cs.NI",
            "cs.DS"
        ]
    },
    {
        "title": "Cache-Oblivious Selection in Sorted X+Y Matrices",
        "authors": [
            "Mark de Berg",
            "Shripad Thite"
        ],
        "summary": "Let X[0..n-1] and Y[0..m-1] be two sorted arrays, and define the mxn matrix A by A[j][i]=X[i]+Y[j]. Frederickson and Johnson gave an efficient algorithm for selecting the k-th smallest element from A. We show how to make this algorithm IO-efficient. Our cache-oblivious algorithm performs O((m+n)/B) IOs, where B is the block size of memory transfers.",
        "published": "2008-04-06T22:31:04Z",
        "link": "http://arxiv.org/abs/0804.0936v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Optimum Binary Search Trees on the Hierarchical Memory Model",
        "authors": [
            "Shripad Thite"
        ],
        "summary": "The Hierarchical Memory Model (HMM) of computation is similar to the standard Random Access Machine (RAM) model except that the HMM has a non-uniform memory organized in a hierarchy of levels numbered 1 through h. The cost of accessing a memory location increases with the level number, and accesses to memory locations belonging to the same level cost the same. Formally, the cost of a single access to the memory location at address a is given by m(a), where m: N -> N is the memory cost function, and the h distinct values of m model the different levels of the memory hierarchy.   We study the problem of constructing and storing a binary search tree (BST) of minimum cost, over a set of keys, with probabilities for successful and unsuccessful searches, on the HMM with an arbitrary number of memory levels, and for the special case h=2.   While the problem of constructing optimum binary search trees has been well studied for the standard RAM model, the additional parameter m for the HMM increases the combinatorial complexity of the problem. We present two dynamic programming algorithms to construct optimum BSTs bottom-up. These algorithms run efficiently under some natural assumptions about the memory hierarchy. We also give an efficient algorithm to construct a BST that is close to optimum, by modifying a well-known linear-time approximation algorithm for the RAM model. We conjecture that the problem of constructing an optimum BST for the HMM with an arbitrary memory cost function m is NP-complete.",
        "published": "2008-04-07T00:06:08Z",
        "link": "http://arxiv.org/abs/0804.0940v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Adaptive Dynamics of Realistic Small-World Networks",
        "authors": [
            "Olof Mogren",
            "Oskar Sandberg",
            "Vilhelm Verendel",
            "Devdatt Dubhashi"
        ],
        "summary": "Continuing in the steps of Jon Kleinberg's and others celebrated work on decentralized search in small-world networks, we conduct an experimental analysis of a dynamic algorithm that produces small-world networks. We find that the algorithm adapts robustly to a wide variety of situations in realistic geographic networks with synthetic test data and with real world data, even when vertices are uneven and non-homogeneously distributed.   We investigate the same algorithm in the case where some vertices are more popular destinations for searches than others, for example obeying power-laws. We find that the algorithm adapts and adjusts the networks according to the distributions, leading to improved performance. The ability of the dynamic process to adapt and create small worlds in such diverse settings suggests a possible mechanism by which such networks appear in nature.",
        "published": "2008-04-07T19:39:59Z",
        "link": "http://arxiv.org/abs/0804.1115v1",
        "categories": [
            "cs.DS",
            "cs.DC"
        ]
    },
    {
        "title": "Approximating L1-distances between mixture distributions using random   projections",
        "authors": [
            "Satyaki Mahalanabis",
            "Daniel Stefankovic"
        ],
        "summary": "We consider the problem of computing L1-distances between every pair ofcprobability densities from a given family. We point out that the technique of Cauchy random projections (Indyk'06) in this context turns into stochastic integrals with respect to Cauchy motion.   For piecewise-linear densities these integrals can be sampled from if one can sample from the stochastic integral of the function x->(1,x). We give an explicit density function for this stochastic integral and present an efficient sampling algorithm. As a consequence we obtain an efficient algorithm to approximate the L1-distances with a small relative error.   For piecewise-polynomial densities we show how to approximately sample from the distributions resulting from the stochastic integrals. This also results in an efficient algorithm to approximate the L1-distances, although our inability to get exact samples worsens the dependence on the parameters.",
        "published": "2008-04-08T02:11:13Z",
        "link": "http://arxiv.org/abs/0804.1170v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Discovering More Accurate Frequent Web Usage Patterns",
        "authors": [
            "Murat Ali Bayir",
            "Ismail Hakki Toroslu",
            "Ahmet Cosar",
            "Guven Fidan"
        ],
        "summary": "Web usage mining is a type of web mining, which exploits data mining techniques to discover valuable information from navigation behavior of World Wide Web users. As in classical data mining, data preparation and pattern discovery are the main issues in web usage mining. The first phase of web usage mining is the data processing phase, which includes the session reconstruction operation from server logs. Session reconstruction success directly affects the quality of the frequent patterns discovered in the next phase. In reactive web usage mining techniques, the source data is web server logs and the topology of the web pages served by the web server domain. Other kinds of information collected during the interactive browsing of web site by user, such as cookies or web logs containing similar information, are not used. The next phase of web usage mining is discovering frequent user navigation patterns. In this phase, pattern discovery methods are applied on the reconstructed sessions obtained in the first phase in order to discover frequent user patterns. In this paper, we propose a frequent web usage pattern discovery method that can be applied after session reconstruction phase. In order to compare accuracy performance of session reconstruction phase and pattern discovery phase, we have used an agent simulator, which models behavior of web users and generates web user navigation as well as the log data kept by the web server.",
        "published": "2008-04-09T05:46:26Z",
        "link": "http://arxiv.org/abs/0804.1409v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Information Acquisition and Exploitation in Multichannel Wireless   Networks",
        "authors": [
            "Sudipto Guha",
            "Kamesh Munagala",
            "Saswati Sarkar"
        ],
        "summary": "A wireless system with multiple channels is considered, where each channel has several transmission states. A user learns about the instantaneous state of an available channel by transmitting a control packet in it. Since probing all channels consumes significant energy and time, a user needs to determine what and how much information it needs to acquire about the instantaneous states of the available channels so that it can maximize its transmission rate. This motivates the study of the trade-off between the cost of information acquisition and its value towards improving the transmission rate.   A simple model is presented for studying this information acquisition and exploitation trade-off when the channels are multi-state, with different distributions and information acquisition costs. The objective is to maximize a utility function which depends on both the cost and value of information. Solution techniques are presented for computing near-optimal policies with succinct representation in polynomial time. These policies provably achieve at least a fixed constant factor of the optimal utility on any problem instance, and in addition, have natural characterizations. The techniques are based on exploiting the structure of the optimal policy, and use of Lagrangean relaxations which simplify the space of approximately optimal solutions.",
        "published": "2008-04-10T14:53:30Z",
        "link": "http://arxiv.org/abs/0804.1724v1",
        "categories": [
            "cs.DS",
            "cs.NI",
            "F.2"
        ]
    },
    {
        "title": "An Optimal Bloom Filter Replacement Based on Matrix Solving",
        "authors": [
            "Ely Porat"
        ],
        "summary": "We suggest a method for holding a dictionary data structure, which maps keys to values, in the spirit of Bloom Filters. The space requirements of the dictionary we suggest are much smaller than those of a hashtable. We allow storing n keys, each mapped to value which is a string of k bits. Our suggested method requires nk + o(n) bits space to store the dictionary, and O(n) time to produce the data structure, and allows answering a membership query in O(1) memory probes. The dictionary size does not depend on the size of the keys. However, reducing the space requirements of the data structure comes at a certain cost. Our dictionary has a small probability of a one sided error. When attempting to obtain the value for a key that is stored in the dictionary we always get the correct answer. However, when testing for membership of an element that is not stored in the dictionary, we may get an incorrect answer, and when requesting the value of such an element we may get a certain random value. Our method is based on solving equations in GF(2^k) and using several hash functions. Another significant advantage of our suggested method is that we do not require using sophisticated hash functions. We only require pairwise independent hash functions. We also suggest a data structure that requires only nk bits space, has O(n2) preprocessing time, and has a O(log n) query time. However, this data structures requires a uniform hash functions. In order replace a Bloom Filter of n elements with an error proability of 2^{-k}, we require nk + o(n) memory bits, O(1) query time, O(n) preprocessing time, and only pairwise independent hash function. Even the most advanced previously known Bloom Filter would require nk+O(n) space, and a uniform hash functions, so our method is significantly less space consuming especially when k is small.",
        "published": "2008-04-11T11:24:04Z",
        "link": "http://arxiv.org/abs/0804.1845v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "Quantum circuits for strongly correlated quantum systems",
        "authors": [
            "Frank Verstraete",
            "J. Ignacio Cirac",
            "Jose I. Latorre"
        ],
        "summary": "In recent years, we have witnessed an explosion of experimental tools by which quantum systems can be manipulated in a controlled and coherent way. One of the most important goals now is to build quantum simulators, which would open up the possibility of exciting experiments probing various theories in regimes that are not achievable under normal lab circumstances. Here we present a novel approach to gain detailed control on the quantum simulation of strongly correlated quantum many-body systems by constructing the explicit quantum circuits that diagonalize their dynamics. We show that the exact quantum circuits underlying some of the most relevant many-body Hamiltonians only need a finite amount of local gates. As a particularly simple instance, the full dynamics of a one-dimensional Quantum Ising model in a transverse field with four spins is shown to be reproduced using a quantum circuit of only six local gates. This opens up the possibility of experimentally producing strongly correlated states, their time evolution at zero time and even thermal superpositions at zero temperature. Our method also allows to uncover the exact circuits corresponding to models that exhibit topological order and to stabilizer states.",
        "published": "2008-04-11T12:52:44Z",
        "link": "http://arxiv.org/abs/0804.1888v1",
        "categories": [
            "quant-ph",
            "cond-mat.str-el",
            "cs.DS",
            "hep-th"
        ]
    },
    {
        "title": "Tight Bounds and Faster Algorithms for Directed Max-Leaf Problems",
        "authors": [
            "Paul Bonsma",
            "Frederic Dorn"
        ],
        "summary": "An out-tree $T$ of a directed graph $D$ is a rooted tree subgraph with all arcs directed outwards from the root. An out-branching is a spanning out-tree. By $l(D)$ and $l_s(D)$ we denote the maximum number of leaves over all out-trees and out-branchings of $D$, respectively.   We give fixed parameter tractable algorithms for deciding whether $l_s(D)\\geq k$ and whether $l(D)\\geq k$ for a digraph $D$ on $n$ vertices, both with time complexity $2^{O(k\\log k)} \\cdot n^{O(1)}$. This improves on previous algorithms with complexity $2^{O(k^3\\log k)} \\cdot n^{O(1)}$ and $2^{O(k\\log^2 k)} \\cdot n^{O(1)}$, respectively.   To obtain the complexity bound in the case of out-branchings, we prove that when all arcs of $D$ are part of at least one out-branching, $l_s(D)\\geq l(D)/3$. The second bound we prove in this paper states that for strongly connected digraphs $D$ with minimum in-degree 3, $l_s(D)\\geq \\Theta(\\sqrt{n})$, where previously $l_s(D)\\geq \\Theta(\\sqrt[3]{n})$ was the best known bound. This bound is tight, and also holds for the larger class of digraphs with minimum in-degree 3 in which every arc is part of at least one out-branching.",
        "published": "2008-04-12T20:50:59Z",
        "link": "http://arxiv.org/abs/0804.2032v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Optimal Mechansim Design and Money Burning",
        "authors": [
            "Jason D. Hartline",
            "Tim Roughgarden"
        ],
        "summary": "Mechanism design is now a standard tool in computer science for aligning the incentives of self-interested agents with the objectives of a system designer. There is, however, a fundamental disconnect between the traditional application domains of mechanism design (such as auctions) and those arising in computer science (such as networks): while monetary transfers (i.e., payments) are essential for most of the known positive results in mechanism design, they are undesirable or even technologically infeasible in many computer systems. Classical impossibility results imply that the reach of mechanisms without transfers is severely limited.   Computer systems typically do have the ability to reduce service quality--routing systems can drop or delay traffic, scheduling protocols can delay the release of jobs, and computational payment schemes can require computational payments from users (e.g., in spam-fighting systems). Service degradation is tantamount to requiring that users burn money}, and such ``payments'' can be used to influence the preferences of the agents at a cost of degrading the social surplus.   We develop a framework for the design and analysis of money-burning mechanisms to maximize the residual surplus--the total value of the chosen outcome minus the payments required.",
        "published": "2008-04-14T04:32:45Z",
        "link": "http://arxiv.org/abs/0804.2097v1",
        "categories": [
            "cs.GT",
            "cs.DS"
        ]
    },
    {
        "title": "Truthful Unsplittable Flow for Large Capacity Networks",
        "authors": [
            "Yossi Azar",
            "Iftah Gamzu",
            "Shai Gutner"
        ],
        "summary": "In this paper, we focus our attention on the large capacities unsplittable flow problem in a game theoretic setting. In this setting, there are selfish agents, which control some of the requests characteristics, and may be dishonest about them. It is worth noting that in game theoretic settings many standard techniques, such as randomized rounding, violate certain monotonicity properties, which are imperative for truthfulness, and therefore cannot be employed. In light of this state of affairs, we design a monotone deterministic algorithm, which is based on a primal-dual machinery, which attains an approximation ratio of $\\frac{e}{e-1}$, up to a disparity of $\\epsilon$ away. This implies an improvement on the current best truthful mechanism, as well as an improvement on the current best combinatorial algorithm for the problem under consideration. Surprisingly, we demonstrate that any algorithm in the family of reasonable iterative path minimizing algorithms, cannot yield a better approximation ratio. Consequently, it follows that in order to achieve a monotone PTAS, if exists, one would have to exert different techniques. We also consider the large capacities \\textit{single-minded multi-unit combinatorial auction problem}. This problem is closely related to the unsplittable flow problem since one can formulate it as a special case of the integer linear program of the unsplittable flow problem. Accordingly, we obtain a comparable performance guarantee by refining the algorithm suggested for the unsplittable flow problem.",
        "published": "2008-04-14T08:03:30Z",
        "link": "http://arxiv.org/abs/0804.2112v1",
        "categories": [
            "cs.DS",
            "cs.GT",
            "F.2"
        ]
    },
    {
        "title": "Parimutuel Betting on Permutations",
        "authors": [
            "Shipra Agrawal",
            "Zizhuo Wang",
            "Yinyu Ye"
        ],
        "summary": "We focus on a permutation betting market under parimutuel call auction model where traders bet on the final ranking of n candidates. We present a Proportional Betting mechanism for this market. Our mechanism allows the traders to bet on any subset of the n x n 'candidate-rank' pairs, and rewards them proportionally to the number of pairs that appear in the final outcome. We show that market organizer's decision problem for this mechanism can be formulated as a convex program of polynomial size. More importantly, the formulation yields a set of n x n unique marginal prices that are sufficient to price the bets in this mechanism, and are computable in polynomial-time. The marginal prices reflect the traders' beliefs about the marginal distributions over outcomes. We also propose techniques to compute the joint distribution over n! permutations from these marginal distributions. We show that using a maximum entropy criterion, we can obtain a concise parametric form (with only n x n parameters) for the joint distribution which is defined over an exponentially large state space. We then present an approximation algorithm for computing the parameters of this distribution. In fact, the algorithm addresses the generic problem of finding the maximum entropy distribution over permutations that has a given mean, and may be of independent interest.",
        "published": "2008-04-15T00:20:17Z",
        "link": "http://arxiv.org/abs/0804.2288v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DS",
            "cs.MA",
            "I.2.1; F.2"
        ]
    },
    {
        "title": "A Critique of a Polynomial-time SAT Solver Devised by Sergey Gubin",
        "authors": [
            "Ian Christopher",
            "Dennis Huo",
            "Bryan Jacobs"
        ],
        "summary": "This paper refutes the validity of the polynomial-time algorithm for solving satisfiability proposed by Sergey Gubin. Gubin introduces the algorithm using 3-SAT and eventually expands it to accept a broad range of forms of the Boolean satisfiability problem. Because 3-SAT is NP-complete, the algorithm would have implied P = NP, had it been correct. Additionally, this paper refutes the correctness of his polynomial-time reduction of SAT to 2-SAT.",
        "published": "2008-04-16T23:00:51Z",
        "link": "http://arxiv.org/abs/0804.2699v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Parameterized Low-distortion Embeddings - Graph metrics into lines and   trees",
        "authors": [
            "Michael Fellows",
            "Fedor Fomin",
            "Daniel Lokshtanov",
            "Elena Losievskaja",
            "Frances A. Rosamond",
            "Saket Saurabh"
        ],
        "summary": "We revisit the issue of low-distortion embedding of metric spaces into the line, and more generally, into the shortest path metric of trees, from the parameterized complexity perspective.Let $M=M(G)$ be the shortest path metric of an edge weighted graph $G=(V,E)$ on $n$ vertices. We describe algorithms for the problem of finding a low distortion non-contracting embedding of $M$ into line and tree metrics.   We give an $O(nd^4(2d+1)^{2d})$ time algorithm that for an unweighted graph metric $M$ and integer $d$ either constructs an embedding of $M$ into the line with distortion at most $d$, or concludes that no such embedding exists. We find the result surprising, because the considered problem bears a strong resemblance to the notoriously hard Bandwidth Minimization problem which does not admit any FPT algorithm unless an unlikely collapse of parameterized complexity classes occurs.   We show that our algorithm can also be applied to construct small distortion embeddings of weighted graph metrics. The running time of our algorithm is $O(n(dW)^4(2d+1)^{2dW})$ where $W$ is the largest edge weight of the input graph. We also show that deciding whether a weighted graph metric $M(G)$ with maximum weight $W < |V(G)|$ can be embedded into the line with distortion at most $d$ is NP-Complete for every fixed rational $d \\geq 2$. This rules out any possibility of an algorithm with running time $O((nW)^{h(d)})$ where $h$ is a function of $d$ alone.   We generalize the result on embedding into the line by proving that for any tree $T$ with maximum degree $\\Delta$, embedding of $M$ into a shortest path metric of $T$ is FPT, parameterized by $(\\Delta,d)$.",
        "published": "2008-04-18T14:39:41Z",
        "link": "http://arxiv.org/abs/0804.3028v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Combinatorial invariants for graph isomorphism problem",
        "authors": [
            "Jarek Duda"
        ],
        "summary": "Presented approach in polynomial time calculates large number of invariants for each vertex, which won't change with graph isomorphism and should fully determine the graph. For example numbers of closed paths of length k for given starting vertex, what can be though as the diagonal terms of k-th power of the adjacency matrix. For k=2 we would get degree of verities invariant, higher describes local topology deeper. Now if two graphs are isomorphic, they have the same set of such vectors of invariants - we can sort theses vectors lexicographically and compare them. If they agree, permutations from sorting allow to reconstruct the isomorphism. I'm presenting arguments that these invariants should fully determine the graph, but unfortunately I can't prove it in this moment. This approach can give hope, that maybe P=NP - instead of checking all instances, we should make arithmetics on these large numbers.",
        "published": "2008-04-22T22:16:46Z",
        "link": "http://arxiv.org/abs/0804.3615v4",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "An $\\tilde{O}(n^{2.5})$-Time Algorithm for Online Topological Ordering",
        "authors": [
            "Hsiao-Fei Liu",
            "Kun-Mao Chao"
        ],
        "summary": "We present an $\\tilde{O}(n^{2.5})$-time algorithm for maintaining the topological order of a directed acyclic graph with $n$ vertices while inserting $m$ edges.",
        "published": "2008-04-24T09:40:20Z",
        "link": "http://arxiv.org/abs/0804.3860v2",
        "categories": [
            "cs.GT",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Minimum-energy broadcast in random-grid ad-hoc networks: approximation   and distributed algorithms",
        "authors": [
            "Tiziana Calamoneri",
            "Andrea E. F. Clementi",
            "Angelo Monti",
            "Gianluca Rossi",
            "Riccardo Silvestri"
        ],
        "summary": "The Min Energy broadcast problem consists in assigning transmission ranges to the nodes of an ad-hoc network in order to guarantee a directed spanning tree from a given source node and, at the same time, to minimize the energy consumption (i.e. the energy cost) yielded by the range assignment. Min energy broadcast is known to be NP-hard.   We consider random-grid networks where nodes are chosen independently at random from the $n$ points of a $\\sqrt n \\times \\sqrt n$ square grid in the plane. The probability of the existence of a node at a given point of the grid does depend on that point, that is, the probability distribution can be non-uniform.   By using information-theoretic arguments, we prove a lower bound $(1-\\epsilon) \\frac n{\\pi}$ on the energy cost of any feasible solution for this problem. Then, we provide an efficient solution of energy cost not larger than $1.1204 \\frac n{\\pi}$.   Finally, we present a fully-distributed protocol that constructs a broadcast range assignment of energy cost not larger than $8n$,thus still yielding constant approximation. The energy load is well balanced and, at the same time, the work complexity (i.e. the energy due to all message transmissions of the protocol) is asymptotically optimal. The completion time of the protocol is only an $O(\\log n)$ factor slower than the optimum. The approximation quality of our distributed solution is also experimentally evaluated.   All bounds hold with probability at least $1-1/n^{\\Theta(1)}$.",
        "published": "2008-04-24T11:17:57Z",
        "link": "http://arxiv.org/abs/0804.3902v1",
        "categories": [
            "cs.DS",
            "F.2"
        ]
    },
    {
        "title": "Time Dependent Contraction Hierarchies -- Basic Algorithmic Ideas",
        "authors": [
            "Peter Sanders"
        ],
        "summary": "Contraction hierarchies are a simple hierarchical routing technique that has proved extremely efficient for static road networks. We explain how to generalize them to networks with time-dependent edge weights. This is the first hierarchical speedup technique for time-dependent routing that allows bidirectional query algorithms.",
        "published": "2008-04-24T15:24:08Z",
        "link": "http://arxiv.org/abs/0804.3947v1",
        "categories": [
            "cs.DS",
            "G.2.2"
        ]
    },
    {
        "title": "Energy and Time Efficient Scheduling of Tasks with Dependencies on   Asymmetric Multiprocessors",
        "authors": [
            "Ioannis Chatzigiannakis",
            "Georgios Giannoulis",
            "Paul G. Spirakis"
        ],
        "summary": "In this work we study the problem of scheduling tasks with dependencies in multiprocessor architectures where processors have different speeds. We present the preemptive algorithm \"Save-Energy\" that given a schedule of tasks it post processes it to improve the energy efficiency without any deterioration of the makespan. In terms of time efficiency, we show that preemptive scheduling in an asymmetric system can achieve the same or better optimal makespan than in a symmetric system. Motivited by real multiprocessor systems, we investigate architectures that exhibit limited asymmetry: there are two essentially different speeds. Interestingly, this special case has not been studied in the field of parallel computing and scheduling theory; only the general case was studied where processors have $K$ essentially different speeds. We present the non-preemptive algorithm ``Remnants'' that achieves almost optimal makespan. We provide a refined analysis of a recent scheduling method. Based on this analysis, we specialize the scheduling policy and provide an algorithm of $(3 + o(1))$ expected approximation factor. Note that this improves the previous best factor (6 for two speeds). We believe that our work will convince researchers to revisit this well studied scheduling problem for these simple, yet realistic, asymmetric multiprocessor architectures.",
        "published": "2008-04-25T03:16:21Z",
        "link": "http://arxiv.org/abs/0804.4039v2",
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.PF",
            "C.1.4; D.1.4"
        ]
    },
    {
        "title": "Sketching and Streaming Entropy via Approximation Theory",
        "authors": [
            "Nicholas J. A. Harvey",
            "Jelani Nelson",
            "Krzysztof Onak"
        ],
        "summary": "We conclude a sequence of work by giving near-optimal sketching and streaming algorithms for estimating Shannon entropy in the most general streaming model, with arbitrary insertions and deletions. This improves on prior results that obtain suboptimal space bounds in the general model, and near-optimal bounds in the insertion-only model without sketching. Our high-level approach is simple: we give algorithms to estimate Renyi and Tsallis entropy, and use them to extrapolate an estimate of Shannon entropy. The accuracy of our estimates is proven using approximation theory arguments and extremal properties of Chebyshev polynomials, a technique which may be useful for other problems. Our work also yields the best-known and near-optimal additive approximations for entropy, and hence also for conditional entropy and mutual information.",
        "published": "2008-04-25T16:04:20Z",
        "link": "http://arxiv.org/abs/0804.4138v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Combining geometry and combinatorics: A unified approach to sparse   signal recovery",
        "authors": [
            "R. Berinde",
            "A. C. Gilbert",
            "P. Indyk",
            "H. Karloff",
            "M. J. Strauss"
        ],
        "summary": "There are two main algorithmic approaches to sparse signal recovery: geometric and combinatorial. The geometric approach starts with a geometric constraint on the measurement matrix and then uses linear programming to decode information about the signal from its measurements. The combinatorial approach constructs the measurement matrix and a combinatorial decoding algorithm to match. We present a unified approach to these two classes of sparse signal recovery algorithms.   The unifying elements are the adjacency matrices of high-quality unbalanced expanders. We generalize the notion of Restricted Isometry Property (RIP), crucial to compressed sensing results for signal recovery, from the Euclidean norm to the l_p norm for p about 1, and then show that unbalanced expanders are essentially equivalent to RIP-p matrices.   From known deterministic constructions for such matrices, we obtain new deterministic measurement matrix constructions and algorithms for signal recovery which, compared to previous deterministic algorithms, are superior in either the number of measurements or in noise tolerance.",
        "published": "2008-04-29T18:24:14Z",
        "link": "http://arxiv.org/abs/0804.4666v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "cs.NA",
            "F.2; G.1; G.2"
        ]
    },
    {
        "title": "Lattice Problems, Gauge Functions and Parameterized Algorithms",
        "authors": [
            "V. Arvind",
            "Pushkar S. Joglekar"
        ],
        "summary": "Given a k-dimensional subspace M\\subseteq \\R^n and a full rank integer lattice L\\subseteq \\R^n, the \\emph{subspace avoiding problem} SAP is to find a shortest vector in L\\setminus M. Treating k as a parameter, we obtain new parameterized approximation and exact algorithms for SAP based on the AKS sieving technique. More precisely, we give a randomized $(1+\\epsilon)$-approximation algorithm for parameterized SAP that runs in time 2^{O(n)}.(1/\\epsilon)^k, where the parameter k is the dimension of the subspace M. Thus, we obtain a 2^{O(n)} time algorithm for \\epsilon=2^{-O(n/k)}. We also give a 2^{O(n+k\\log k)} exact algorithm for the parameterized SAP for any \\ell_p norm.   Several of our algorithms work for all gauge functions as metric with some natural restrictions, in particular for all \\ell_p norms. We also prove an \\Omega(2^n) lower bound on the query complexity of AKS sieving based exact algorithms for SVP that accesses the gauge function as oracle.",
        "published": "2008-04-30T06:39:21Z",
        "link": "http://arxiv.org/abs/0804.4744v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "The Minimum Backlog Problem",
        "authors": [
            "Michael A. Bender",
            "Sándor P. Fekete",
            "Alexander Kröller",
            "Vincenzo Liberatore",
            "Joseph S. B. Mitchell",
            "Valentin Polishchuk",
            "Jukka Suomela"
        ],
        "summary": "We study the minimum backlog problem (MBP). This online problem arises, e.g., in the context of sensor networks. We focus on two main variants of MBP.   The discrete MBP is a 2-person game played on a graph $G=(V,E)$. The player is initially located at a vertex of the graph. In each time step, the adversary pours a total of one unit of water into cups that are located on the vertices of the graph, arbitrarily distributing the water among the cups. The player then moves from her current vertex to an adjacent vertex and empties the cup at that vertex. The player's objective is to minimize the backlog, i.e., the maximum amount of water in any cup at any time.   The geometric MBP is a continuous-time version of the MBP: the cups are points in the two-dimensional plane, the adversary pours water continuously at a constant rate, and the player moves in the plane with unit speed. Again, the player's objective is to minimize the backlog.   We show that the competitive ratio of any algorithm for the MBP has a lower bound of $\\Omega(D)$, where $D$ is the diameter of the graph (for the discrete MBP) or the diameter of the point set (for the geometric MBP). Therefore we focus on determining a strategy for the player that guarantees a uniform upper bound on the absolute value of the backlog.   For the absolute value of the backlog there is a trivial lower bound of $\\Omega(D)$, and the deamortization analysis of Dietz and Sleator gives an upper bound of $O(D\\log N)$ for $N$ cups. Our main result is a tight upper bound for the geometric MBP: we show that there is a strategy for the player that guarantees a backlog of $O(D)$, independently of the number of cups.",
        "published": "2008-04-30T13:13:12Z",
        "link": "http://arxiv.org/abs/0804.4819v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Search Space Contraction in Canonical Labeling of Graphs",
        "authors": [
            "Adolfo Piperno"
        ],
        "summary": "The individualization-refinement paradigm for computing a canonical labeling and the automorphism group of a graph is investigated. A new algorithmic design aimed at reducing the size of the associated search space is introduced, and a new tool, named \"Traces\", is presented, together with experimental results and comparisons with existing software, such as McKay's \"nauty\". It is shown that the approach presented here leads to a huge reduction in the search space, thereby making computation feasible for several classes of graphs which are hard for all the main canonical labeling tools in the literature.",
        "published": "2008-04-30T18:28:13Z",
        "link": "http://arxiv.org/abs/0804.4881v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "G.2.2"
        ]
    },
    {
        "title": "Algorithms for Probabilistically-Constrained Models of Risk-Averse   Stochastic Optimization with Black-Box Distributions",
        "authors": [
            "Chaitanya Swamy"
        ],
        "summary": "We consider various stochastic models that incorporate the notion of risk-averseness into the standard 2-stage recourse model, and develop novel techniques for solving the algorithmic problems arising in these models. A key notable feature of our work that distinguishes it from work in some other related models, such as the (standard) budget model and the (demand-) robust model, is that we obtain results in the black-box setting, that is, where one is given only sampling access to the underlying distribution. Our first model, which we call the risk-averse budget model, incorporates the notion of risk-averseness via a probabilistic constraint that restricts the probability (according to the underlying distribution) with which the second-stage cost may exceed a given budget B to at most a given input threshold \\rho. We also a consider a closely-related model that we call the risk-averse robust model, where we seek to minimize the first-stage cost and the (1-\\rho)-quantile of the second-stage cost.   We obtain approximation algorithms for a variety of combinatorial optimization problems including the set cover, vertex cover, multicut on trees, min cut, and facility location problems, in the risk-averse budget and robust models with black-box distributions. We obtain near-optimal solutions that preserve the budget approximately and incur a small blow-up of the probability threshold (both of which are unavoidable). To the best of our knowledge, these are the first approximation results for problems involving probabilistic constraints and black-box distributions. A major component of our results is a fully polynomial approximation scheme for solving the LP-relaxation of the risk-averse problem.",
        "published": "2008-05-04T03:57:52Z",
        "link": "http://arxiv.org/abs/0805.0389v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM",
            "F.2.2; G.1.6; G.3"
        ]
    },
    {
        "title": "Pruning Attribute Values From Data Cubes with Diamond Dicing",
        "authors": [
            "Hazel Webb",
            "Owen Kaser",
            "Daniel Lemire"
        ],
        "summary": "Data stored in a data warehouse are inherently multidimensional, but most data-pruning techniques (such as iceberg and top-k queries) are unidimensional. However, analysts need to issue multidimensional queries. For example, an analyst may need to select not just the most profitable stores or--separately--the most profitable products, but simultaneous sets of stores and products fulfilling some profitability constraints. To fill this need, we propose a new operator, the diamond dice. Because of the interaction between dimensions, the computation of diamonds is challenging. We present the first diamond-dicing experiments on large data sets. Experiments show that we can compute diamond cubes over fact tables containing 100 million facts in less than 35 minutes using a standard PC.",
        "published": "2008-05-06T15:45:15Z",
        "link": "http://arxiv.org/abs/0805.0747v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Bounds for self-stabilization in unidirectional networks",
        "authors": [
            "Samuel Bernard",
            "Stéphane Devismes",
            "Maria Gradinariu Potop-Butucaru",
            "Sébastien Tixeuil"
        ],
        "summary": "A distributed algorithm is self-stabilizing if after faults and attacks hit the system and place it in some arbitrary global state, the systems recovers from this catastrophic situation without external intervention in finite time. Unidirectional networks preclude many common techniques in self-stabilization from being used, such as preserving local predicates. In this paper, we investigate the intrinsic complexity of achieving self-stabilization in unidirectional networks, and focus on the classical vertex coloring problem. When deterministic solutions are considered, we prove a lower bound of $n$ states per process (where $n$ is the network size) and a recovery time of at least $n(n-1)/2$ actions in total. We present a deterministic algorithm with matching upper bounds that performs in arbitrary graphs. When probabilistic solutions are considered, we observe that at least $\\Delta + 1$ states per process and a recovery time of $\\Omega(n)$ actions in total are required (where $\\Delta$ denotes the maximal degree of the underlying simple undirected graph). We present a probabilistically self-stabilizing algorithm that uses $\\mathtt{k}$ states per process, where $\\mathtt{k}$ is a parameter of the algorithm. When $\\mathtt{k}=\\Delta+1$, the algorithm recovers in expected $O(\\Delta n)$ actions. When $\\mathtt{k}$ may grow arbitrarily, the algorithm recovers in expected O(n) actions in total. Thus, our algorithm can be made optimal with respect to space or time complexity.",
        "published": "2008-05-07T07:39:14Z",
        "link": "http://arxiv.org/abs/0805.0851v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Submodular approximation: sampling-based algorithms and lower bounds",
        "authors": [
            "Zoya Svitkina",
            "Lisa Fleischer"
        ],
        "summary": "We introduce several generalizations of classical computer science problems obtained by replacing simpler objective functions with general submodular functions. The new problems include submodular load balancing, which generalizes load balancing or minimum-makespan scheduling, submodular sparsest cut and submodular balanced cut, which generalize their respective graph cut problems, as well as submodular function minimization with a cardinality lower bound. We establish upper and lower bounds for the approximability of these problems with a polynomial number of queries to a function-value oracle. The approximation guarantees for most of our algorithms are of the order of sqrt(n/ln n). We show that this is the inherent difficulty of the problems by proving matching lower bounds. We also give an improved lower bound for the problem of approximately learning a monotone submodular function. In addition, we present an algorithm for approximately learning submodular functions with special structure, whose guarantee is close to the lower bound. Although quite restrictive, the class of functions with this structure includes the ones that are used for lower bounds both by us and in previous work. This demonstrates that if there are significantly stronger lower bounds for this problem, they rely on more general submodular functions.",
        "published": "2008-05-07T21:37:18Z",
        "link": "http://arxiv.org/abs/0805.1071v3",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Online Ad Slotting With Cancellations",
        "authors": [
            "Florin Constantin",
            "Jon Feldman",
            "S. Muthukrishnan",
            "Martin Pal"
        ],
        "summary": "Many advertisers buy advertisements (ads) on the Internet or on traditional media and seek simple, online mechanisms to reserve ad slots in advance. Media publishers represent a vast and varying inventory, and they too seek automatic, online mechanisms for pricing and allocating such reservations. In this paper, we present and study a simple model for auctioning such ad slots in advance. Bidders arrive sequentially and report which slots they are interested in. The seller must decide immediately whether or not to grant a reservation. Our model allows a seller to accept reservations, but possibly cancel the allocations later and pay the bidder a cancellation compensation (bump payment). Our main result is an online mechanism to derive prices and bump payments that is efficient to implement. This mechanism has many desirable properties. It is individually rational; winners have an incentive to be honest and bidding one's true value dominates any lower bid. Our mechanism's efficiency is within a constant fraction of the a posteriori optimally efficient solution. Its revenue is within a constant fraction of the a posteriori revenue of the Vickrey-Clarke-Groves mechanism. Our results make no assumptions about the order of arrival of bids or the value distribution of bidders and still hold if the items for sale are elements of a matroid, a more general setting than slot allocation.",
        "published": "2008-05-08T18:12:50Z",
        "link": "http://arxiv.org/abs/0805.1213v1",
        "categories": [
            "cs.GT",
            "cs.DS"
        ]
    },
    {
        "title": "Randomized Work-Competitive Scheduling for Cooperative Computing on   $k$-partite Task Graphs",
        "authors": [
            "Chadi Kari",
            "Alexander Russell",
            "Narasimha Shashidhar"
        ],
        "summary": "A fundamental problem in distributed computing is the task of cooperatively executing a given set of $t$ tasks by $p$ processors where the communication medium is dynamic and subject to failures. The dynamics of the communication medium lead to groups of processors being disconnected and possibly reconnected during the entire course of the computation furthermore tasks can have dependencies among them. In this paper, we present a randomized algorithm whose competitive ratio is dependent on the dynamics of the communication medium and also on the nature of the dependencies among the tasks.",
        "published": "2008-05-09T00:27:28Z",
        "link": "http://arxiv.org/abs/0805.1257v2",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Searching for Frequent Colors in Rectangles",
        "authors": [
            "Marek Karpinski",
            "Yakov Nekrich"
        ],
        "summary": "We study a new variant of colored orthogonal range searching problem: given a query rectangle $Q$ all colors $c$, such that at least a fraction $\\tau$ of all points in $Q$ are of color $c$, must be reported. We describe several data structures for that problem that use pseudo-linear space and answer queries in poly-logarithmic time.",
        "published": "2008-05-09T13:47:55Z",
        "link": "http://arxiv.org/abs/0805.1348v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "Approximation Algorithms for Shortest Descending Paths in Terrains",
        "authors": [
            "Mustaq Ahmed",
            "Sandip Das",
            "Sachin Lodha",
            "Anna Lubiw",
            "Anil Maheshwari",
            "Sasanka Roy"
        ],
        "summary": "A path from s to t on a polyhedral terrain is descending if the height of a point p never increases while we move p along the path from s to t. No efficient algorithm is known to find a shortest descending path (SDP) from s to t in a polyhedral terrain. We give two approximation algorithms (more precisely, FPTASs) that solve the SDP problem on general terrains. Both algorithms are simple, robust and easy to implement.",
        "published": "2008-05-09T19:39:19Z",
        "link": "http://arxiv.org/abs/0805.1401v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "A Time Efficient Indexing Scheme for Complex Spatiotemporal Retrieval",
        "authors": [
            "Lagogiannis George",
            "Lorentzos Nikos",
            "Sioutas Spyros",
            "Theodoridis Evaggelos"
        ],
        "summary": "The paper is concerned with the time efficient processing of spatiotemporal predicates, i.e. spatial predicates associated with an exact temporal constraint. A set of such predicates forms a buffer query or a Spatio-temporal Pattern (STP) Query with time. In the more general case of an STP query, the temporal dimension is introduced via the relative order of the spatial predicates (STP queries with order). Therefore, the efficient processing of a spatiotemporal predicate is crucial for the efficient implementation of more complex queries of practical interest. We propose an extension of a known approach, suitable for processing spatial predicates, which has been used for the efficient manipulation of STP queries with order. The extended method is supported by efficient indexing structures. We also provide experimental results that show the efficiency of the technique.",
        "published": "2008-05-10T17:18:32Z",
        "link": "http://arxiv.org/abs/0805.1487v1",
        "categories": [
            "cs.DB",
            "cs.DS",
            "H.2; E.1"
        ]
    },
    {
        "title": "A Simple In-Place Algorithm for In-Shuffle",
        "authors": [
            "Peiyush Jain"
        ],
        "summary": "The paper presents a simple, linear time, in-place algorithm for performing a 2-way in-shuffle which can be used with little modification for certain other k-way shuffles.",
        "published": "2008-05-12T09:28:18Z",
        "link": "http://arxiv.org/abs/0805.1598v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "NAPX: A Polynomial Time Approximation Scheme for the Noah's Ark Problem",
        "authors": [
            "G. Hickey",
            "P. Carmi",
            "A. Maheshwari",
            "N. Zeh"
        ],
        "summary": "The Noah's Ark Problem (NAP) is an NP-Hard optimization problem with relevance to ecological conservation management. It asks to maximize the phylogenetic diversity (PD) of a set of taxa given a fixed budget, where each taxon is associated with a cost of conservation and a probability of extinction. NAP has received renewed interest with the rise in availability of genetic sequence data, allowing PD to be used as a practical measure of biodiversity. However, only simplified instances of the problem, where one or more parameters are fixed as constants, have as of yet been addressed in the literature. We present NAPX, the first algorithm for the general version of NAP that returns a $1 - \\epsilon$ approximation of the optimal solution. It runs in $O(\\frac{n B^2 h^2 \\log^2n}{\\log^2(1 - \\epsilon)})$ time where $n$ is the number of species, and $B$ is the total budget and $h$ is the height of the input tree. We also provide improved bounds for its expected running time.",
        "published": "2008-05-12T15:04:26Z",
        "link": "http://arxiv.org/abs/0805.1661v2",
        "categories": [
            "cs.DS",
            "J.3"
        ]
    },
    {
        "title": "Small Approximate Pareto Sets for Bi-objective Shortest Paths and Other   Problems",
        "authors": [
            "Ilias Diakonikolas",
            "Mihalis Yannakakis"
        ],
        "summary": "We investigate the problem of computing a minimum set of solutions that approximates within a specified accuracy $\\epsilon$ the Pareto curve of a multiobjective optimization problem. We show that for a broad class of bi-objective problems (containing many important widely studied problems such as shortest paths, spanning tree, and many others), we can compute in polynomial time an $\\epsilon$-Pareto set that contains at most twice as many solutions as the minimum such set. Furthermore we show that the factor of 2 is tight for these problems, i.e., it is NP-hard to do better. We present upper and lower bounds for three or more objectives, as well as for the dual problem of computing a specified number $k$ of solutions which provide a good approximation to the Pareto curve.",
        "published": "2008-05-17T06:10:19Z",
        "link": "http://arxiv.org/abs/0805.2646v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Finger Indexed Sets: New Approaches",
        "authors": [
            "Spyros Sioutas"
        ],
        "summary": "In the particular case we have insertions/deletions at the tail of a given set S of $n$ one-dimensional elements, we present a simpler and more concrete algorithm than that presented in [Anderson, 2007] achieving the same (but also amortized) upper bound of $O(\\sqrt{logd/loglogd})$ for finger searching queries, where $d$ is the number of sorted keys between the finger element and the target element we are looking for. Furthermore, in general case we have insertions/deletions anywhere we present a new randomized algorithm achieving the same expected time bounds. Even the new solutions achieve the optimal bounds in amortized or expected case, the advantage of simplicity is of great importance due to practical merits we gain.",
        "published": "2008-05-17T14:05:12Z",
        "link": "http://arxiv.org/abs/0805.2671v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "E.1; E.5"
        ]
    },
    {
        "title": "Canonical polygon Queries on the plane: a New Approach",
        "authors": [
            "Spyros Sioutas",
            "Dimitrios Sofotassios",
            "Kostas Tsichlas",
            "Dimitrios Sotiropoulos",
            "Panayiotis Vlamos"
        ],
        "summary": "The polygon retrieval problem on points is the problem of preprocessing a set of $n$ points on the plane, so that given a polygon query, the subset of points lying inside it can be reported efficiently.   It is of great interest in areas such as Computer Graphics, CAD applications, Spatial Databases and GIS developing tasks. In this paper we study the problem of canonical $k$-vertex polygon queries on the plane. A canonical $k$-vertex polygon query always meets the following specific property: a point retrieval query can be transformed into a linear number (with respect to the number of vertices) of point retrievals for orthogonal objects such as rectangles and triangles (throughout this work we call a triangle orthogonal iff two of its edges are axis-parallel).   We present two new algorithms for this problem. The first one requires $O(n\\log^2{n})$ space and $O(k\\frac{log^3n}{loglogn}+A)$ query time. A simple modification scheme on first algorithm lead us to a second solution, which consumes $O(n^2)$ space and $O(k \\frac{logn}{loglogn}+A)$ query time, where $A$ denotes the size of the answer and $k$ is the number of vertices.   The best previous solution for the general polygon retrieval problem uses $O(n^2)$ space and answers a query in $O(k\\log{n}+A)$ time, where $k$ is the number of vertices. It is also very complicated and difficult to be implemented in a standard imperative programming language such as C or C++.",
        "published": "2008-05-17T16:00:09Z",
        "link": "http://arxiv.org/abs/0805.2681v2",
        "categories": [
            "cs.CG",
            "cs.DS",
            "E.1; E.5"
        ]
    },
    {
        "title": "Sequential Design of Experiments via Linear Programming",
        "authors": [
            "Sudipto Guha",
            "Kamesh Munagala"
        ],
        "summary": "The celebrated multi-armed bandit problem in decision theory models the basic trade-off between exploration, or learning about the state of a system, and exploitation, or utilizing the system. In this paper we study the variant of the multi-armed bandit problem where the exploration phase involves costly experiments and occurs before the exploitation phase; and where each play of an arm during the exploration phase updates a prior belief about the arm. The problem of finding an inexpensive exploration strategy to optimize a certain exploitation objective is NP-Hard even when a single play reveals all information about an arm, and all exploration steps cost the same.   We provide the first polynomial time constant-factor approximation algorithm for this class of problems. We show that this framework also generalizes several problems of interest studied in the context of data acquisition in sensor networks. Our analyses also extends to switching and setup costs, and to concave utility objectives.   Our solution approach is via a novel linear program rounding technique based on stochastic packing. In addition to yielding exploration policies whose performance is within a small constant factor of the adaptive optimal policy, a nice feature of this approach is that the resulting policies explore the arms sequentially without revisiting any arm. Sequentiality is a well-studied concept in decision theory, and is very desirable in domains where multiple explorations can be conducted in parallel, for instance, in the sensor network context.",
        "published": "2008-05-17T22:48:22Z",
        "link": "http://arxiv.org/abs/0805.2630v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Algorithmic problems in twisted groups of Lie type",
        "authors": [
            "Henrik Bäärnhielm"
        ],
        "summary": "This thesis contains a collection of algorithms for working with the twisted groups of Lie type known as Suzuki groups, and small and large Ree groups. The two main problems under consideration are constructive recognition and constructive membership testing. We also consider problems of generating and conjugating Sylow and maximal subgroups. The algorithms are motivated by, and form a part of, the Matrix Group Recognition Project. Obtaining both theoretically and practically efficient algorithms has been a central goal. The algorithms have been developed with, and implemented in, the computer algebra system MAGMA.",
        "published": "2008-05-24T04:21:29Z",
        "link": "http://arxiv.org/abs/0805.3742v2",
        "categories": [
            "math.GR",
            "cs.DS",
            "20-04; 20C40; 68W20; 68Q25"
        ]
    },
    {
        "title": "Properly Coloured Cycles and Paths: Results and Open Problems",
        "authors": [
            "Gregory Gutin",
            "Eun Jung Kim"
        ],
        "summary": "In this paper, we consider a number of results and seven conjectures on properly edge-coloured (PC) paths and cycles in edge-coloured multigraphs. We overview some known results and prove new ones. In particular, we consider a family of transformations of an edge-coloured multigraph $G$ into an ordinary graph that allow us to check the existence PC cycles and PC $(s,t)$-paths in $G$ and, if they exist, to find shortest ones among them. We raise a problem of finding the optimal transformation and consider a possible solution to the problem.",
        "published": "2008-05-26T09:17:21Z",
        "link": "http://arxiv.org/abs/0805.3901v3",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Succinct Geometric Indexes Supporting Point Location Queries",
        "authors": [
            "Prosenjit Bose",
            "Eric Y. Chen",
            "Meng He",
            "Anil Maheshwari",
            "Pat Morin"
        ],
        "summary": "We propose to design data structures called succinct geometric indexes of negligible space (more precisely, o(n) bits) that, by taking advantage of the n points in the data set permuted and stored elsewhere as a sequence, to support geometric queries in optimal time. Our first and main result is a succinct geometric index that can answer point location queries, a fundamental problem in computational geometry, on planar triangulations in O(lg n) time. We also design three variants of this index. The first supports point location using $\\lg n + 2\\sqrt{\\lg n} + O(\\lg^{1/4} n)$ point-line comparisons. The second supports point location in o(lg n) time when the coordinates are integers bounded by U. The last variant can answer point location in O(H+1) expected time, where H is the entropy of the query distribution. These results match the query efficiency of previous point location structures that use O(n) words or O(n lg n) bits, while saving drastic amounts of space.   We then generalize our succinct geometric index to planar subdivisions, and design indexes for other types of queries. Finally, we apply our techniques to design the first implicit data structures that support point location in $O(\\lg^2 n)$ time.",
        "published": "2008-05-27T15:15:05Z",
        "link": "http://arxiv.org/abs/0805.4147v1",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Balanced Families of Perfect Hash Functions and Their Applications",
        "authors": [
            "Noga Alon",
            "Shai Gutner"
        ],
        "summary": "The construction of perfect hash functions is a well-studied topic. In this paper, this concept is generalized with the following definition. We say that a family of functions from $[n]$ to $[k]$ is a $\\delta$-balanced $(n,k)$-family of perfect hash functions if for every $S \\subseteq [n]$, $|S|=k$, the number of functions that are 1-1 on $S$ is between $T/\\delta$ and $\\delta T$ for some constant $T>0$. The standard definition of a family of perfect hash functions requires that there will be at least one function that is 1-1 on $S$, for each $S$ of size $k$. In the new notion of balanced families, we require the number of 1-1 functions to be almost the same (taking $\\delta$ to be close to 1) for every such $S$. Our main result is that for any constant $\\delta > 1$, a $\\delta$-balanced $(n,k)$-family of perfect hash functions of size $2^{O(k \\log \\log k)} \\log n$ can be constructed in time $2^{O(k \\log \\log k)} n \\log n$. Using the technique of color-coding we can apply our explicit constructions to devise approximation algorithms for various counting problems in graphs. In particular, we exhibit a deterministic polynomial time algorithm for approximating both the number of simple paths of length $k$ and the number of simple cycles of size $k$ for any $k \\leq O(\\frac{\\log n}{\\log \\log \\log n})$ in a graph with $n$ vertices. The approximation is up to any fixed desirable relative error.",
        "published": "2008-05-28T09:49:18Z",
        "link": "http://arxiv.org/abs/0805.4300v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "A Dynamic Programming Framework for Combinatorial Optimization Problems   on Graphs with Bounded Pathwidth",
        "authors": [
            "Mugurel Ionut Andreica"
        ],
        "summary": "In this paper we present an algorithmic framework for solving a class of combinatorial optimization problems on graphs with bounded pathwidth. The problems are NP-hard in general, but solvable in linear time on this type of graphs. The problems are relevant for assessing network reliability and improving the network's performance and fault tolerance. The main technique considered in this paper is dynamic programming.",
        "published": "2008-06-04T19:18:45Z",
        "link": "http://arxiv.org/abs/0806.0840v2",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Drawing Binary Tanglegrams: An Experimental Evaluation",
        "authors": [
            "Martin Nöllenburg",
            "Danny Holten",
            "Markus Völker",
            "Alexander Wolff"
        ],
        "summary": "A binary tanglegram is a pair <S,T> of binary trees whose leaf sets are in one-to-one correspondence; matching leaves are connected by inter-tree edges. For applications, for example in phylogenetics or software engineering, it is required that the individual trees are drawn crossing-free. A natural optimization problem, denoted tanglegram layout problem, is thus to minimize the number of crossings between inter-tree edges.   The tanglegram layout problem is NP-hard and is currently considered both in application domains and theory. In this paper we present an experimental comparison of a recursive algorithm of Buchin et al., our variant of their algorithm, the algorithm hierarchy sort of Holten and van Wijk, and an integer quadratic program that yields optimal solutions.",
        "published": "2008-06-05T11:00:33Z",
        "link": "http://arxiv.org/abs/0806.0928v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "A Comparison of Performance Measures for Online Algorithms",
        "authors": [
            "Joan Boyar",
            "Sandy Irani",
            "Kim S. Larsen"
        ],
        "summary": "This paper provides a systematic study of several proposed measures for online algorithms in the context of a specific problem, namely, the two server problem on three colinear points. Even though the problem is simple, it encapsulates a core challenge in online algorithms which is to balance greediness and adaptability. We examine Competitive Analysis, the Max/Max Ratio, the Random Order Ratio, Bijective Analysis and Relative Worst Order Analysis, and determine how these measures compare the Greedy Algorithm, Double Coverage, and Lazy Double Coverage, commonly studied algorithms in the context of server problems. We find that by the Max/Max Ratio and Bijective Analysis, Greedy is the best of the three algorithms. Under the other measures, Double Coverage and Lazy Double Coverage are better, though Relative Worst Order Analysis indicates that Greedy is sometimes better. Only Bijective Analysis and Relative Worst Order Analysis indicate that Lazy Double Coverage is better than Double Coverage. Our results also provide the first proof of optimality of an algorithm under Relative Worst Order Analysis.",
        "published": "2008-06-05T14:50:08Z",
        "link": "http://arxiv.org/abs/0806.0983v2",
        "categories": [
            "cs.DS",
            "F.1.2"
        ]
    },
    {
        "title": "Fast Arithmetics Using Chinese Remaindering",
        "authors": [
            "George Davida",
            "Bruce Litow",
            "Guangwu Xu"
        ],
        "summary": "In this paper, some issues concerning the Chinese remaindering representation are discussed. Some new converting methods, including an efficient probabilistic algorithm based on a recent result of von zur Gathen and Shparlinski \\cite{Gathen-Shparlinski}, are described. An efficient refinement of the NC$^1$ division algorithm of Chiu, Davida and Litow \\cite{Chiu-Davida-Litow} is given, where the number of moduli is reduced by a factor of $\\log n$.",
        "published": "2008-06-10T18:21:09Z",
        "link": "http://arxiv.org/abs/0806.1722v1",
        "categories": [
            "cs.DS",
            "G.1.0"
        ]
    },
    {
        "title": "Tight Bounds for Hashing Block Sources",
        "authors": [
            "Kai-Min Chung",
            "Salil Vadhan"
        ],
        "summary": "It is known that if a 2-universal hash function $H$ is applied to elements of a {\\em block source} $(X_1,...,X_T)$, where each item $X_i$ has enough min-entropy conditioned on the previous items, then the output distribution $(H,H(X_1),...,H(X_T))$ will be ``close'' to the uniform distribution. We provide improved bounds on how much min-entropy per item is required for this to hold, both when we ask that the output be close to uniform in statistical distance and when we only ask that it be statistically close to a distribution with small collision probability. In both cases, we reduce the dependence of the min-entropy on the number $T$ of items from $2\\log T$ in previous work to $\\log T$, which we show to be optimal. This leads to corresponding improvements to the recent results of Mitzenmacher and Vadhan (SODA `08) on the analysis of hashing-based algorithms and data structures when the data items come from a block source.",
        "published": "2008-06-11T19:54:14Z",
        "link": "http://arxiv.org/abs/0806.1948v1",
        "categories": [
            "cs.DS",
            "E.2; G.3"
        ]
    },
    {
        "title": "A simple, polynomial-time algorithm for the matrix torsion problem",
        "authors": [
            "Francois Nicolas"
        ],
        "summary": "The Matrix Torsion Problem (MTP) is: given a square matrix M with rational entries, decide whether two distinct powers of M are equal. It has been shown by Cassaigne and the author that the MTP reduces to the Matrix Power Problem (MPP) in polynomial time: given two square matrices A and B with rational entries, the MTP is to decide whether B is a power of A. Since the MPP is decidable in polynomial time, it is also the case of the MTP. However, the algorithm for MPP is highly non-trivial. The aim of this note is to present a simple, direct, polynomial-time algorithm for the MTP.",
        "published": "2008-06-12T13:24:46Z",
        "link": "http://arxiv.org/abs/0806.2068v3",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Max Cut and the Smallest Eigenvalue",
        "authors": [
            "Luca Trevisan"
        ],
        "summary": "We describe a new approximation algorithm for Max Cut. Our algorithm runs in $\\tilde O(n^2)$ time, where $n$ is the number of vertices, and achieves an approximation ratio of $.531$. On instances in which an optimal solution cuts a $1-\\epsilon$ fraction of edges, our algorithm finds a solution that cuts a $1-4\\sqrt{\\epsilon} + 8\\epsilon-o(1)$ fraction of edges.   Our main result is a variant of spectral partitioning, which can be implemented in nearly linear time. Given a graph in which the Max Cut optimum is a $1-\\epsilon$ fraction of edges, our spectral partitioning algorithm finds a set $S$ of vertices and a bipartition $L,R=S-L$ of $S$ such that at least a $1-O(\\sqrt \\epsilon)$ fraction of the edges incident on $S$ have one endpoint in $L$ and one endpoint in $R$. (This can be seen as an analog of Cheeger's inequality for the smallest eigenvalue of the adjacency matrix of a graph.) Iterating this procedure yields the approximation results stated above.   A different, more complicated, variant of spectral partitioning leads to an $\\tilde O(n^3)$ time algorithm that cuts $1/2 + e^{-\\Omega(1/\\eps)}$ fraction of edges in graphs in which the optimum is $1/2 + \\epsilon$.",
        "published": "2008-06-12T17:51:02Z",
        "link": "http://arxiv.org/abs/0806.1978v5",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Exposing Multi-Relational Networks to Single-Relational Network Analysis   Algorithms",
        "authors": [
            "Marko A. Rodriguez",
            "Joshua Shinavier"
        ],
        "summary": "Many, if not most network analysis algorithms have been designed specifically for single-relational networks; that is, networks in which all edges are of the same type. For example, edges may either represent \"friendship,\" \"kinship,\" or \"collaboration,\" but not all of them together. In contrast, a multi-relational network is a network with a heterogeneous set of edge labels which can represent relationships of various types in a single data structure. While multi-relational networks are more expressive in terms of the variety of relationships they can capture, there is a need for a general framework for transferring the many single-relational network analysis algorithms to the multi-relational domain. It is not sufficient to execute a single-relational network analysis algorithm on a multi-relational network by simply ignoring edge labels. This article presents an algebra for mapping multi-relational networks to single-relational networks, thereby exposing them to single-relational network analysis algorithms.",
        "published": "2008-06-13T16:07:19Z",
        "link": "http://arxiv.org/abs/0806.2274v2",
        "categories": [
            "cs.DM",
            "cs.DS",
            "F.2.2; G.1.3"
        ]
    },
    {
        "title": "Approximately Counting Embeddings into Random Graphs",
        "authors": [
            "Martin Furer",
            "Shiva Prasad Kasiviswanathan"
        ],
        "summary": "Let H be a graph, and let C_H(G) be the number of (subgraph isomorphic) copies of H contained in a graph G. We investigate the fundamental problem of estimating C_H(G). Previous results cover only a few specific instances of this general problem, for example, the case when H has degree at most one (monomer-dimer problem). In this paper, we present the first general subcase of the subgraph isomorphism counting problem which is almost always efficiently approximable. The results rely on a new graph decomposition technique. Informally, the decomposition is a labeling of the vertices such that every edge is between vertices with different labels and for every vertex all neighbors with a higher label have identical labels. The labeling implicitly generates a sequence of bipartite graphs which permits us to break the problem of counting embeddings of large subgraphs into that of counting embeddings of small subgraphs. Using this method, we present a simple randomized algorithm for the counting problem. For all decomposable graphs H and all graphs G, the algorithm is an unbiased estimator. Furthermore, for all graphs H having a decomposition where each of the bipartite graphs generated is small and almost all graphs G, the algorithm is a fully polynomial randomized approximation scheme.   We show that the graph classes of H for which we obtain a fully polynomial randomized approximation scheme for almost all G includes graphs of degree at most two, bounded-degree forests, bounded-length grid graphs, subdivision of bounded-degree graphs, and major subclasses of outerplanar graphs, series-parallel graphs and planar graphs, whereas unbounded-length grid graphs are excluded.",
        "published": "2008-06-13T17:06:01Z",
        "link": "http://arxiv.org/abs/0806.2287v2",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Biased Range Trees",
        "authors": [
            "Vida Dujmovic",
            "John Howat",
            "Pat Morin"
        ],
        "summary": "A data structure, called a biased range tree, is presented that preprocesses a set S of n points in R^2 and a query distribution D for 2-sided orthogonal range counting queries. The expected query time for this data structure, when queries are drawn according to D, matches, to within a constant factor, that of the optimal decision tree for S and D. The memory and preprocessing requirements of the data structure are O(n log n).",
        "published": "2008-06-17T15:18:40Z",
        "link": "http://arxiv.org/abs/0806.2707v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "I.3.5"
        ]
    },
    {
        "title": "The Structure of Information Pathways in a Social Communication Network",
        "authors": [
            "Gueorgi Kossinets",
            "Jon Kleinberg",
            "Duncan Watts"
        ],
        "summary": "Social networks are of interest to researchers in part because they are thought to mediate the flow of information in communities and organizations. Here we study the temporal dynamics of communication using on-line data, including e-mail communication among the faculty and staff of a large university over a two-year period. We formulate a temporal notion of \"distance\" in the underlying social network by measuring the minimum time required for information to spread from one node to another -- a concept that draws on the notion of vector-clocks from the study of distributed computing systems. We find that such temporal measures provide structural insights that are not apparent from analyses of the pure social network topology. In particular, we define the network backbone to be the subgraph consisting of edges on which information has the potential to flow the quickest. We find that the backbone is a sparse graph with a concentration of both highly embedded edges and long-range bridges -- a finding that sheds new light on the relationship between tie strength and connectivity in social networks.",
        "published": "2008-06-19T14:22:25Z",
        "link": "http://arxiv.org/abs/0806.3201v1",
        "categories": [
            "physics.soc-ph",
            "cs.DS",
            "physics.data-an"
        ]
    },
    {
        "title": "Local Search Heuristics For The Multidimensional Assignment Problem",
        "authors": [
            "Gregory Gutin",
            "Daniel Karapetyan"
        ],
        "summary": "The Multidimensional Assignment Problem (MAP) (abbreviated s-AP in the case of s dimensions) is an extension of the well-known assignment problem. The most studied case of MAP is 3-AP, though the problems with larger values of s also have a large number of applications. We consider several known neighborhoods, generalize them and propose some new ones. The heuristics are evaluated both theoretically and experimentally and dominating algorithms are selected. We also demonstrate a combination of two neighborhoods may yield a heuristics which is superior to both of its components.",
        "published": "2008-06-19T18:31:51Z",
        "link": "http://arxiv.org/abs/0806.3258v6",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Fast computation of the median by successive binning",
        "authors": [
            "Ryan J. Tibshirani"
        ],
        "summary": "This paper describes a new median algorithm and a median approximation algorithm. The former has O(n) average running time and the latter has O(n) worst-case running time. These algorithms are highly competitive with the standard algorithm when computing the median of a single data set, but are significantly faster in updating the median when more data is added.",
        "published": "2008-06-20T00:44:53Z",
        "link": "http://arxiv.org/abs/0806.3301v2",
        "categories": [
            "stat.CO",
            "cs.DS",
            "stat.AP"
        ]
    },
    {
        "title": "Quantum and Randomized Lower Bounds for Local Search on   Vertex-Transitive Graphs",
        "authors": [
            "Hang Dinh",
            "Alexander Russell"
        ],
        "summary": "We study the problem of \\emph{local search} on a graph. Given a real-valued black-box function f on the graph's vertices, this is the problem of determining a local minimum of f--a vertex v for which f(v) is no more than f evaluated at any of v's neighbors. In 1983, Aldous gave the first strong lower bounds for the problem, showing that any randomized algorithm requires $\\Omega(2^{n/2 - o(1)})$ queries to determine a local minima on the n-dimensional hypercube. The next major step forward was not until 2004 when Aaronson, introducing a new method for query complexity bounds, both strengthened this lower bound to $\\Omega(2^{n/2}/n^2)$ and gave an analogous lower bound on the quantum query complexity. While these bounds are very strong, they are known only for narrow families of graphs (hypercubes and grids). We show how to generalize Aaronson's techniques in order to give randomized (and quantum) lower bounds on the query complexity of local search for the family of vertex-transitive graphs. In particular, we show that for any vertex-transitive graph G of N vertices and diameter d, the randomized and quantum query complexities for local search on G are $\\Omega(N^{1/2}/d\\log N)$ and $\\Omega(N^{1/4}/\\sqrt{d\\log N})$, respectively.",
        "published": "2008-06-20T18:46:50Z",
        "link": "http://arxiv.org/abs/0806.3437v1",
        "categories": [
            "quant-ph",
            "cs.DS"
        ]
    },
    {
        "title": "Stabilizing Tiny Interaction Protocols",
        "authors": [
            "Davide Canepa",
            "Maria Gradinariu Potop-Butucaru"
        ],
        "summary": "In this paper we present the self-stabilizing implementation of a class of token based algorithms. In the current work we only consider interactions between weak nodes. They are uniform, they do not have unique identifiers, are static and their interactions are restricted to a subset of nodes called neighbours. While interacting, a pair of neighbouring nodes may create mobile agents (that materialize in the current work the token abstraction) that perform traversals of the network and accelerate the system stabilization. In this work we only explore the power of oblivious stateless agents.   Our work shows that the agent paradigm is an elegant distributed tool for achieving self-stabilization in Tiny Interaction Protocols (TIP). Nevertheless, in order to reach the full power of classical self-stabilizing algorithms more complex classes of agents have to be considered (e.g. agents with memory, identifiers or communication skills). Interestingly, our work proposes for the first time a model that unifies the recent studies in mobile robots(agents) that evolve in a discrete space and the already established population protocols paradigm.",
        "published": "2008-06-20T21:01:52Z",
        "link": "http://arxiv.org/abs/0806.3471v2",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Approximating Multi-Criteria Max-TSP",
        "authors": [
            "Markus Bläser",
            "Bodo Manthey",
            "Oliver Putz"
        ],
        "summary": "We present randomized approximation algorithms for multi-criteria Max-TSP. For Max-STSP with k > 1 objective functions, we obtain an approximation ratio of $1/k - \\eps$ for arbitrarily small $\\eps > 0$. For Max-ATSP with k objective functions, we obtain an approximation ratio of $1/(k+1) - \\eps$.",
        "published": "2008-06-23T12:28:10Z",
        "link": "http://arxiv.org/abs/0806.3668v1",
        "categories": [
            "cs.DS",
            "F.2.2; G.2.1; G.2.1"
        ]
    },
    {
        "title": "Optimal Scheduling of File Transfers with Divisible Sizes on Multiple   Disjoint Paths",
        "authors": [
            "Mugurel Ionut Andreica"
        ],
        "summary": "In this paper I investigate several offline and online data transfer scheduling problems and propose efficient algorithms and techniques for addressing them. In the offline case, I present a novel, heuristic, algorithm for scheduling files with divisible sizes on multiple disjoint paths, in order to maximize the total profit (the problem is equivalent to the multiple knapsack problem with divisible item sizes). I then consider a cost optimization problem for transferring a sequence of identical files, subject to time constraints imposed by the data transfer providers. For the online case I propose an algorithmic framework based on the block partitioning method, which can speed up the process of resource allocation and reservation.",
        "published": "2008-06-24T07:16:26Z",
        "link": "http://arxiv.org/abs/0806.3827v2",
        "categories": [
            "cs.DS",
            "cs.NI"
        ]
    },
    {
        "title": "A comparison of two approaches for polynomial time algorithms computing   basic graph parameters",
        "authors": [
            "Frank Gurski"
        ],
        "summary": "In this paper we compare and illustrate the algorithmic use of graphs of bounded tree-width and graphs of bounded clique-width. For this purpose we give polynomial time algorithms for computing the four basic graph parameters independence number, clique number, chromatic number, and clique covering number on a given tree structure of graphs of bounded tree-width and graphs of bounded clique-width in polynomial time. We also present linear time algorithms for computing the latter four basic graph parameters on trees, i.e. graphs of tree-width 1, and on co-graphs, i.e. graphs of clique-width at most 2.",
        "published": "2008-06-25T11:26:47Z",
        "link": "http://arxiv.org/abs/0806.4073v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Space Efficient Multi-Dimensional Range Reporting",
        "authors": [
            "Marek Karpinski",
            "Yakov Nekrich"
        ],
        "summary": "We present a data structure that supports three-dimensional range reporting queries in $O(\\log \\log U + (\\log \\log n)^3+k)$ time and uses $O(n\\log^{1+\\eps} n)$ space, where $U$ is the size of the universe, $k$ is the number of points in the answer,and $\\eps$ is an arbitrary constant. This result improves over the data structure of Alstrup, Brodal, and Rauhe (FOCS 2000) that uses $O(n\\log^{1+\\eps} n)$ space and supports queries in $O(\\log n+k)$ time,the data structure of Nekrich (SoCG'07) that uses $O(n\\log^{3} n)$ space and supports queries in $O(\\log \\log U + (\\log \\log n)^2 + k)$ time, and the data structure of Afshani (ESA'08) that uses $O(n\\log^{3} n)$ space and also supports queries in $O(\\log \\log U + (\\log \\log n)^2 + k)$ time but relies on randomization during the preprocessing stage. Our result allows us to significantly reduce the space usage of the fastest previously known static and incremental $d$-dimensional data structures, $d\\geq 3$, at a cost of increasing the query time by a negligible $O(\\log \\log n)$ factor.",
        "published": "2008-06-26T16:32:57Z",
        "link": "http://arxiv.org/abs/0806.4361v2",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "The 1-fixed-endpoint Path Cover Problem is Polynomial on Interval Graph",
        "authors": [
            "Katerina Asdre",
            "Stavros D. Nikolopoulos"
        ],
        "summary": "We consider a variant of the path cover problem, namely, the $k$-fixed-endpoint path cover problem, or kPC for short, on interval graphs. Given a graph $G$ and a subset $\\mathcal{T}$ of $k$ vertices of $V(G)$, a $k$-fixed-endpoint path cover of $G$ with respect to $\\mathcal{T}$ is a set of vertex-disjoint paths $\\mathcal{P}$ that covers the vertices of $G$ such that the $k$ vertices of $\\mathcal{T}$ are all endpoints of the paths in $\\mathcal{P}$. The kPC problem is to find a $k$-fixed-endpoint path cover of $G$ of minimum cardinality; note that, if $\\mathcal{T}$ is empty the stated problem coincides with the classical path cover problem. In this paper, we study the 1-fixed-endpoint path cover problem on interval graphs, or 1PC for short, generalizing the 1HP problem which has been proved to be NP-complete even for small classes of graphs. Motivated by a work of Damaschke, where he left both 1HP and 2HP problems open for the class of interval graphs, we show that the 1PC problem can be solved in polynomial time on the class of interval graphs. The proposed algorithm is simple, runs in $O(n^2)$ time, requires linear space, and also enables us to solve the 1HP problem on interval graphs within the same time and space complexity.",
        "published": "2008-06-26T18:13:31Z",
        "link": "http://arxiv.org/abs/0806.4372v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "A Fixed-Parameter Algorithm for Random Instances of Weighted d-CNF   Satisfiability",
        "authors": [
            "Yong Gao"
        ],
        "summary": "We study random instances of the weighted $d$-CNF satisfiability problem (WEIGHTED $d$-SAT), a generic W[1]-complete problem. A random instance of the problem consists of a fixed parameter $k$ and a random $d$-CNF formula $\\weicnf{n}{p}{k, d}$ generated as follows: for each subset of $d$ variables and with probability $p$, a clause over the $d$ variables is selected uniformly at random from among the $2^d - 1$ clauses that contain at least one negated literals.   We show that random instances of WEIGHTED $d$-SAT can be solved in $O(k^2n + n^{O(1)})$-time with high probability, indicating that typical instances of WEIGHTED $d$-SAT under this instance distribution are fixed-parameter tractable. The result also hold for random instances from the model $\\weicnf{n}{p}{k,d}(d')$ where clauses containing less than $d' (1 < d' < d)$ negated literals are forbidden, and for random instances of the renormalized (miniaturized) version of WEIGHTED $d$-SAT in certain range of the random model's parameter $p(n)$. This, together with our previous results on the threshold behavior and the resolution complexity of unsatisfiable instances of $\\weicnf{n}{p}{k, d}$, provides an almost complete characterization of the typical-case behavior of random instances of WEIGHTED $d$-SAT.",
        "published": "2008-06-28T05:03:47Z",
        "link": "http://arxiv.org/abs/0806.4652v1",
        "categories": [
            "cs.DS",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Linear Time Algorithms for Finding a Dominating Set of Fixed Size in   Degenerated Graphs",
        "authors": [
            "Noga Alon",
            "Shai Gutner"
        ],
        "summary": "There is substantial literature dealing with fixed parameter algorithms for the dominating set problem on various families of graphs. In this paper, we give a $k^{O(dk)} n$ time algorithm for finding a dominating set of size at most $k$ in a $d$-degenerated graph with $n$ vertices. This proves that the dominating set problem is fixed-parameter tractable for degenerated graphs. For graphs that do not contain $K_h$ as a topological minor, we give an improved algorithm for the problem with running time $(O(h))^{hk} n$. For graphs which are $K_h$-minor-free, the running time is further reduced to $(O(\\log h))^{hk/2} n$. Fixed-parameter tractable algorithms that are linear in the number of vertices of the graph were previously known only for planar graphs.   For the families of graphs discussed above, the problem of finding an induced cycle of a given length is also addressed. For every fixed $H$ and $k$, we show that if an $H$-minor-free graph $G$ with $n$ vertices contains an induced cycle of size $k$, then such a cycle can be found in O(n) expected time as well as in $O(n \\log n)$ worst-case time. Some results are stated concerning the (im)possibility of establishing linear time algorithms for the more general family of degenerated graphs.",
        "published": "2008-06-29T06:09:57Z",
        "link": "http://arxiv.org/abs/0806.4735v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "AMS Without 4-Wise Independence on Product Domains",
        "authors": [
            "Vladimir Braverman",
            "Kai-Min Chung",
            "Zhenming Liu",
            "Michael Mitzenmacher",
            "Rafail Ostrovsky"
        ],
        "summary": "In their seminal work, Alon, Matias, and Szegedy introduced several sketching techniques, including showing that 4-wise independence is sufficient to obtain good approximations of the second frequency moment. In this work, we show that their sketching technique can be extended to product domains $[n]^k$ by using the product of 4-wise independent functions on $[n]$. Our work extends that of Indyk and McGregor, who showed the result for $k = 2$. Their primary motivation was the problem of identifying correlations in data streams. In their model, a stream of pairs $(i,j) \\in [n]^2$ arrive, giving a joint distribution $(X,Y)$, and they find approximation algorithms for how close the joint distribution is to the product of the marginal distributions under various metrics, which naturally corresponds to how close $X$ and $Y$ are to being independent. By using our technique, we obtain a new result for the problem of approximating the $\\ell_2$ distance between the joint distribution and the product of the marginal distributions for $k$-ary vectors, instead of just pairs, in a single pass. Our analysis gives a randomized algorithm that is a $(1 \\pm \\epsilon)$ approximation (with probability $1-\\delta$) that requires space logarithmic in $n$ and $m$ and proportional to $3^k$.",
        "published": "2008-06-29T21:34:28Z",
        "link": "http://arxiv.org/abs/0806.4790v5",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A Dynamic Programming Approach To Length-Limited Huffman Coding",
        "authors": [
            "Mordecai Golin",
            "Yan Zhang"
        ],
        "summary": "The ``state-of-the-art'' in Length Limited Huffman Coding algorithms is the $\\Theta(ND)$-time, $\\Theta(N)$-space one of Hirschberg and Larmore, where $D\\le N$ is the length restriction on the code. This is a very clever, very problem specific, technique. In this note we show that there is a simple Dynamic-Programming (DP) method that solves the problem with the same time and space bounds. The fact that there was an $\\Theta(ND)$ time DP algorithm was previously known; it is a straightforward DP with the Monge property (which permits an order of magnitude speedup). It was not interesting, though, because it also required $\\Theta(ND)$ space. The main result of this paper is the technique developed for reducing the space. It is quite simple and applicable to many other problems modeled by DPs with the Monge property. We illustrate this with examples from web-proxy design and wireless mobile paging.",
        "published": "2008-06-30T14:13:18Z",
        "link": "http://arxiv.org/abs/0806.4899v1",
        "categories": [
            "cs.DS",
            "cs.IT",
            "math.IT",
            "E.1; E.4; H.1.1"
        ]
    },
    {
        "title": "A Novel Mathematical Model for the Unique Shortest Path Routing Problem",
        "authors": [
            "Changyong Zhang"
        ],
        "summary": "Link weights are the principal parameters of shortest path routing protocols, the most commonly used protocols for IP networks. The problem of optimally setting link weights for unique shortest path routing is addressed. Due to the complexity of the constraints involved, there exist challenges to formulate the problem properly, so that a solution algorithm may be developed which could prove to be more efficient than those already in existence. In this paper, a novel complete formulation with a polynomial number of constraints is first introduced and then mathematically proved to be correct. It is further illustrated that the formulation has advantages over a prior one in terms of both constraint structure and model size for a proposed decomposition method to solve the problem.",
        "published": "2008-06-30T23:22:47Z",
        "link": "http://arxiv.org/abs/0807.0038v2",
        "categories": [
            "math.OC",
            "cs.DS",
            "90B10; 90B18"
        ]
    },
    {
        "title": "Range Medians",
        "authors": [
            "Sariel Har-Peled",
            "S. Muthukrishnan"
        ],
        "summary": "We study a generalization of the classical median finding problem to batched query case: given an array of unsorted $n$ items and $k$ (not necessarily disjoint) intervals in the array, the goal is to determine the median in {\\em each} of the intervals in the array. We give an algorithm that uses $O(n\\log n + k\\log k \\log n)$ comparisons and show a lower bound of $\\Omega(n\\log k)$ comparisons for this problem. This is optimal for $k=O(n/\\log n)$.",
        "published": "2008-07-01T20:05:29Z",
        "link": "http://arxiv.org/abs/0807.0222v1",
        "categories": [
            "cs.DS",
            "cs.OH"
        ]
    },
    {
        "title": "Shortest Paths Avoiding Forbidden Subpaths",
        "authors": [
            "Mustaq Ahmed",
            "Anna Lubiw"
        ],
        "summary": "In this paper we study a variant of the shortest path problem in graphs: given a weighted graph G and vertices s and t, and given a set X of forbidden paths in G, find a shortest s-t path P such that no path in X is a subpath of P. Path P is allowed to repeat vertices and edges. We call each path in X an exception, and our desired path a shortest exception-avoiding path. We formulate a new version of the problem where the algorithm has no a priori knowledge of X, and finds out about an exception x in X only when a path containing x fails. This situation arises in computing shortest paths in optical networks. We give an algorithm that finds a shortest exception avoiding path in time polynomial in |G| and |X|. The main idea is to run Dijkstra's algorithm incrementally after replicating vertices when an exception is discovered.",
        "published": "2008-07-04T19:30:30Z",
        "link": "http://arxiv.org/abs/0807.0807v2",
        "categories": [
            "cs.DM",
            "cs.DS",
            "G.2.2; F.2.2"
        ]
    },
    {
        "title": "Greedy D-Approximation Algorithm for Covering with Arbitrary Constraints   and Submodular Cost",
        "authors": [
            "Christos Koufogiannakis",
            "Neal E. Young"
        ],
        "summary": "This paper describes a simple greedy D-approximation algorithm for any covering problem whose objective function is submodular and non-decreasing, and whose feasible region can be expressed as the intersection of arbitrary (closed upwards) covering constraints, each of which constrains at most D variables of the problem. (A simple example is Vertex Cover, with D = 2.) The algorithm generalizes previous approximation algorithms for fundamental covering problems and online paging and caching problems.",
        "published": "2008-07-04T23:31:29Z",
        "link": "http://arxiv.org/abs/0807.0644v4",
        "categories": [
            "cs.DS",
            "cs.DC",
            "68W25",
            "G.1.6"
        ]
    },
    {
        "title": "Bloomier Filters: A second look",
        "authors": [
            "Denis Charles",
            "Kumar Chellapilla"
        ],
        "summary": "A Bloom filter is a space efficient structure for storing static sets, where the space efficiency is gained at the expense of a small probability of false-positives. A Bloomier filter generalizes a Bloom filter to compactly store a function with a static support. In this article we give a simple construction of a Bloomier filter. The construction is linear in space and requires constant time to evaluate. The creation of our Bloomier filter takes linear time which is faster than the existing construction. We show how one can improve the space utilization further at the cost of increasing the time for creating the data structure.",
        "published": "2008-07-06T19:45:37Z",
        "link": "http://arxiv.org/abs/0807.0928v1",
        "categories": [
            "cs.DS",
            "E.1; E.2"
        ]
    },
    {
        "title": "Algorithms for Secretary Problems on Graphs and Hypergraphs",
        "authors": [
            "Nitish Korula",
            "Martin Pal"
        ],
        "summary": "We examine several online matching problems, with applications to Internet advertising reservation systems. Consider an edge-weighted bipartite graph G, with partite sets L, R. We develop an 8-competitive algorithm for the following secretary problem: Initially given R, and the size of L, the algorithm receives the vertices of L sequentially, in a random order. When a vertex l \\in L is seen, all edges incident to l are revealed, together with their weights. The algorithm must immediately either match l to an available vertex of R, or decide that l will remain unmatched.   Dimitrov and Plaxton show a 16-competitive algorithm for the transversal matroid secretary problem, which is the special case with weights on vertices, not edges. (Equivalently, one may assume that for each l \\in L, the weights on all edges incident to l are identical.) We use a similar algorithm, but simplify and improve the analysis to obtain a better competitive ratio for the more general problem. Perhaps of more interest is the fact that our analysis is easily extended to obtain competitive algorithms for similar problems, such as to find disjoint sets of edges in hypergraphs where edges arrive online. We also introduce secretary problems with adversarially chosen groups. Finally, we give a 2e-competitive algorithm for the secretary problem on graphic matroids, where, with edges appearing online, the goal is to find a maximum-weight acyclic subgraph of a given graph.",
        "published": "2008-07-07T22:51:46Z",
        "link": "http://arxiv.org/abs/0807.1139v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Randomized greedy algorithms for independent sets and matchings in   regular graphs: Exact results and finite girth corrections",
        "authors": [
            "David Gamarnik",
            "David Goldberg"
        ],
        "summary": "We derive new results for the performance of a simple greedy algorithm for finding large independent sets and matchings in constant degree regular graphs. We show that for $r$-regular graphs with $n$ nodes and girth at least $g$, the algorithm finds an independent set of expected cardinality $f(r)n - O\\big(\\frac{(r-1)^{\\frac{g}{2}}}{\\frac{g}{2}!} n\\big)$, where $f(r)$ is a function which we explicitly compute. A similar result is established for matchings. Our results imply improved bounds for the size of the largest independent set in these graphs, and provide the first results of this type for matchings. As an implication we show that the greedy algorithm returns a nearly perfect matching when both the degree $r$ and girth $g$ are large. Furthermore, we show that the cardinality of independent sets and matchings produced by the greedy algorithm in \\emph{arbitrary} bounded degree graphs is concentrated around the mean. Finally, we analyze the performance of the greedy algorithm for the case of random i.i.d. weighted independent sets and matchings, and obtain a remarkably simple expression for the limiting expected values produced by the algorithm. In fact, all the other results are obtained as straightforward corollaries from the results for the weighted case.",
        "published": "2008-07-08T15:29:42Z",
        "link": "http://arxiv.org/abs/0807.1277v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "F.2.2; G.1.6; G.2.1; G.2.2; G.3"
        ]
    },
    {
        "title": "Online Scheduling to Minimize the Maximum Delay Factor",
        "authors": [
            "Chandra Chekuri",
            "Benjamin Moseley"
        ],
        "summary": "In this paper two scheduling models are addressed. First is the standard model (unicast) where requests (or jobs) are independent. The other is the broadcast model where broadcasting a page can satisfy multiple outstanding requests for that page. We consider online scheduling of requests when they have deadlines. Unlike previous models, which mainly consider the objective of maximizing throughput while respecting deadlines, here we focus on scheduling all the given requests with the goal of minimizing the maximum {\\em delay factor}.We prove strong lower bounds on the achievable competitive ratios for delay factor scheduling even with unit-time requests.For the unicast model we give algorithms that are $(1 + \\eps)$-speed $O({1 \\over \\eps})$-competitive in both the single machine and multiple machine settings. In the broadcast model we give an algorithm for similar-sized pages that is $(2+ \\eps)$-speed $O({1 \\over \\eps^2})$-competitive. For arbitrary page sizes we give an algorithm that is $(4+\\eps)$-speed $O({1 \\over \\eps^2})$-competitive.",
        "published": "2008-07-11T17:00:07Z",
        "link": "http://arxiv.org/abs/0807.1891v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Derandomizing the Lovasz Local Lemma more effectively",
        "authors": [
            "Robin A. Moser"
        ],
        "summary": "The famous Lovasz Local Lemma [EL75] is a powerful tool to non-constructively prove the existence of combinatorial objects meeting a prescribed collection of criteria. Kratochvil et al. applied this technique to prove that a k-CNF in which each variable appears at most 2^k/(ek) times is always satisfiable [KST93]. In a breakthrough paper, Beck found that if we lower the occurrences to O(2^(k/48)/k), then a deterministic polynomial-time algorithm can find a satisfying assignment to such an instance [Bec91]. Alon randomized the algorithm and required O(2^(k/8)/k) occurrences [Alo91]. In [Mos06], we exhibited a refinement of his method which copes with O(2^(k/6)/k) of them. The hitherto best known randomized algorithm is due to Srinivasan and is capable of solving O(2^(k/4)/k) occurrence instances [Sri08]. Answering two questions asked by Srinivasan, we shall now present an approach that tolerates O(2^(k/2)/k) occurrences per variable and which can most easily be derandomized. The new algorithm bases on an alternative type of witness tree structure and drops a number of limiting aspects common to all previous methods.",
        "published": "2008-07-14T09:55:09Z",
        "link": "http://arxiv.org/abs/0807.2120v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2; G.2"
        ]
    },
    {
        "title": "Ranking Unit Squares with Few Visibilities",
        "authors": [
            "Bernd Gärtner"
        ],
        "summary": "Given a set of n unit squares in the plane, the goal is to rank them in space in such a way that only few squares see each other vertically. We prove that ranking the squares according to the lexicographic order of their centers results in at most 3n-7 pairwise visibilities for n at least 4. We also show that this bound is best possible, by exhibiting a set of n squares with at least 3n-7 pairwise visibilities under any ranking.",
        "published": "2008-07-14T15:23:52Z",
        "link": "http://arxiv.org/abs/0807.2178v1",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Optimal Acyclic Hamiltonian Path Completion for Outerplanar Triangulated   st-Digraphs (with Application to Upward Topological Book Embeddings)",
        "authors": [
            "Tamara Mchedlidze",
            "Antonios Symvonis"
        ],
        "summary": "Given an embedded planar acyclic digraph G, we define the problem of \"acyclic hamiltonian path completion with crossing minimization (Acyclic-HPCCM)\" to be the problem of determining an hamiltonian path completion set of edges such that, when these edges are embedded on G, they create the smallest possible number of edge crossings and turn G to a hamiltonian digraph. Our results include:   --We provide a characterization under which a triangulated st-digraph G is hamiltonian.   --For an outerplanar triangulated st-digraph G, we define the st-polygon decomposition of G and, based on its properties, we develop a linear-time algorithm that solves the Acyclic-HPCCM problem with at most one crossing per edge of G.   --For the class of st-planar digraphs, we establish an equivalence between the Acyclic-HPCCM problem and the problem of determining an upward 2-page topological book embedding with minimum number of spine crossings. We infer (based on this equivalence) for the class of outerplanar triangulated st-digraphs an upward topological 2-page book embedding with minimum number of spine crossings and at most one spine crossing per edge.   To the best of our knowledge, it is the first time that edge-crossing minimization is studied in conjunction with the acyclic hamiltonian completion problem and the first time that an optimal algorithm with respect to spine crossing minimization is presented for upward topological book embeddings.",
        "published": "2008-07-15T10:12:03Z",
        "link": "http://arxiv.org/abs/0807.2330v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "G.2.2"
        ]
    },
    {
        "title": "An Efficient Algorithm for a Sharp Approximation of Universally   Quantified Inequalities",
        "authors": [
            "Alexandre Goldsztejn",
            "Claude Michel",
            "Michel Rueher"
        ],
        "summary": "This paper introduces a new algorithm for solving a sub-class of quantified constraint satisfaction problems (QCSP) where existential quantifiers precede universally quantified inequalities on continuous domains. This class of QCSPs has numerous applications in engineering and design. We propose here a new generic branch and prune algorithm for solving such continuous QCSPs. Standard pruning operators and solution identification operators are specialized for universally quantified inequalities. Special rules are also proposed for handling the parameters of the constraints. First experimentation show that our algorithm outperforms the state of the art methods.",
        "published": "2008-07-15T14:45:38Z",
        "link": "http://arxiv.org/abs/0807.2269v1",
        "categories": [
            "cs.NA",
            "cs.DS",
            "G.1"
        ]
    },
    {
        "title": "Hybrid Keyword Search Auctions",
        "authors": [
            "Ashish Goel",
            "Kamesh Munagala"
        ],
        "summary": "Search auctions have become a dominant source of revenue generation on the Internet. Such auctions have typically used per-click bidding and pricing. We propose the use of hybrid auctions where an advertiser can make a per-impression as well as a per-click bid, and the auctioneer then chooses one of the two as the pricing mechanism. We assume that the advertiser and the auctioneer both have separate beliefs (called priors) on the click-probability of an advertisement. We first prove that the hybrid auction is truthful, assuming that the advertisers are risk-neutral. We then show that this auction is superior to the existing per-click auction in multiple ways: 1) It takes into account the risk characteristics of the advertisers. 2) For obscure keywords, the auctioneer is unlikely to have a very sharp prior on the click-probabilities. In such situations, the hybrid auction can result in significantly higher revenue. 3) An advertiser who believes that its click-probability is much higher than the auctioneer's estimate can use per-impression bids to correct the auctioneer's prior without incurring any extra cost. 4) The hybrid auction can allow the advertiser and auctioneer to implement complex dynamic programming strategies. As Internet commerce matures, we need more sophisticated pricing models to exploit all the information held by each of the participants. We believe that hybrid auctions could be an important step in this direction.",
        "published": "2008-07-16T04:04:32Z",
        "link": "http://arxiv.org/abs/0807.2496v2",
        "categories": [
            "cs.GT",
            "cs.DS",
            "cs.IR"
        ]
    },
    {
        "title": "Algorithms for Scheduling Weighted Packets with Deadlines in a Bounded   Queue",
        "authors": [
            "Fei Li"
        ],
        "summary": "Motivated by the Quality-of-Service (QoS) buffer management problem, we consider online scheduling of packets with hard deadlines in a finite capacity queue. At any time, a queue can store at most $b \\in \\mathbb Z^+$ packets. Packets arrive over time. Each packet is associated with a non-negative value and an integer deadline. In each time step, only one packet is allowed to be sent. Our objective is to maximize the total value gained by the packets sent by their deadlines in an online manner. Due to the Internet traffic's chaotic characteristics, no stochastic assumptions are made on the packet input sequences. This model is called a {\\em finite-queue model}.   We use competitive analysis to measure an online algorithm's performance versus an unrealizable optimal offline algorithm who constructs the worst possible input based on the knowledge of the online algorithm. For the finite-queue model, we first present a deterministic 3-competitive memoryless online algorithm. Then, we give a randomized ($\\phi^2 = ((1 + \\sqrt{5}) / 2)^2 \\approx 2.618$)-competitive memoryless online algorithm.   The algorithmic framework and its theoretical analysis include several interesting features. First, our algorithms use (possibly) modified characteristics of packets; these characteristics may not be same as those specified in the input sequence. Second, our analysis method is different from the classical potential function approach.",
        "published": "2008-07-17T04:58:30Z",
        "link": "http://arxiv.org/abs/0807.2694v4",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "The rank convergence of HITS can be slow",
        "authors": [
            "Enoch Peserico",
            "Luca Pretto"
        ],
        "summary": "We prove that HITS, to \"get right\" h of the top k ranked nodes of an N>=2k node graph, can require h^(Omega(N h/k)) iterations (i.e. a substantial Omega(N h log(h)/k) matrix multiplications even with a \"squaring trick\"). Our proof requires no algebraic tools and is entirely self-contained.",
        "published": "2008-07-18T16:42:57Z",
        "link": "http://arxiv.org/abs/0807.3006v1",
        "categories": [
            "cs.DS",
            "cs.IR",
            "F.2.2; H.3.3"
        ]
    },
    {
        "title": "Finding paths of length k in O*(2^k) time",
        "authors": [
            "Ryan Williams"
        ],
        "summary": "We give a randomized algorithm that determines if a given graph has a simple path of length at least k in O(2^k poly(n,k)) time.",
        "published": "2008-07-18T18:25:15Z",
        "link": "http://arxiv.org/abs/0807.3026v3",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Phylogenetic estimation with partial likelihood tensors",
        "authors": [
            "J. G. Sumner",
            "M. A. Charleston"
        ],
        "summary": "We present an alternative method for calculating likelihoods in molecular phylogenetics. Our method is based on partial likelihood tensors, which are generalizations of partial likelihood vectors, as used in Felsenstein's approach. Exploiting a lexicographic sorting and partial likelihood tensors, it is possible to obtain significant computational savings. We show this on a range of simulated data by enumerating all numerical calculations that are required by our method and the standard approach.",
        "published": "2008-07-22T04:24:11Z",
        "link": "http://arxiv.org/abs/0807.3387v1",
        "categories": [
            "q-bio.QM",
            "cs.DS",
            "q-bio.PE"
        ]
    },
    {
        "title": "Linear Coloring and Linear Graphs",
        "authors": [
            "Kyriaki Ioannidou",
            "Stavros D. Nikolopoulos"
        ],
        "summary": "Motivated by the definition of linear coloring on simplicial complexes, recently introduced in the context of algebraic topology \\cite{Civan}, and the framework through which it was studied, we introduce the linear coloring on graphs. We provide an upper bound for the chromatic number $\\chi(G)$, for any graph $G$, and show that $G$ can be linearly colored in polynomial time by proposing a simple linear coloring algorithm. Based on these results, we define a new class of perfect graphs, which we call co-linear graphs, and study their complement graphs, namely linear graphs. The linear coloring of a graph $G$ is a vertex coloring such that two vertices can be assigned the same color, if their corresponding clique sets are associated by the set inclusion relation (a clique set of a vertex $u$ is the set of all maximal cliques containing $u$); the linear chromatic number $\\mathcal{\\lambda}(G)$ of $G$ is the least integer $k$ for which $G$ admits a linear coloring with $k$ colors. We show that linear graphs are those graphs $G$ for which the linear chromatic number achieves its theoretical lower bound in every induced subgraph of $G$. We prove inclusion relations between these two classes of graphs and other subclasses of chordal and co-chordal graphs, and also study the structure of the forbidden induced subgraphs of the class of linear graphs.",
        "published": "2008-07-26T12:04:27Z",
        "link": "http://arxiv.org/abs/0807.4234v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "G.2.2; F.2.2"
        ]
    },
    {
        "title": "Improved Algorithms for Approximate String Matching (Extended Abstract)",
        "authors": [
            "Dimitris Papamichail",
            "Georgios Papamichail"
        ],
        "summary": "The problem of approximate string matching is important in many different areas such as computational biology, text processing and pattern recognition. A great effort has been made to design efficient algorithms addressing several variants of the problem, including comparison of two strings, approximate pattern identification in a string or calculation of the longest common subsequence that two strings share.   We designed an output sensitive algorithm solving the edit distance problem between two strings of lengths n and m respectively in time O((s-|n-m|)min(m,n,s)+m+n) and linear space, where s is the edit distance between the two strings. This worst-case time bound sets the quadratic factor of the algorithm independent of the longest string length and improves existing theoretical bounds for this problem. The implementation of our algorithm excels also in practice, especially in cases where the two strings compared differ significantly in length. Source code of our algorithm is available at http://www.cs.miami.edu/\\~dimitris/edit_distance",
        "published": "2008-07-28T07:17:57Z",
        "link": "http://arxiv.org/abs/0807.4368v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "FiEstAS sampling -- a Monte Carlo algorithm for multidimensional   numerical integration",
        "authors": [
            "Yago Ascasibar"
        ],
        "summary": "This paper describes a new algorithm for Monte Carlo integration, based on the Field Estimator for Arbitrary Spaces (FiEstAS). The algorithm is discussed in detail, and its performance is evaluated in the context of Bayesian analysis, with emphasis on multimodal distributions with strong parameter degeneracies. Source code is available upon request.",
        "published": "2008-07-28T15:47:20Z",
        "link": "http://arxiv.org/abs/0807.4479v1",
        "categories": [
            "astro-ph",
            "cs.DS"
        ]
    },
    {
        "title": "Lower Bounds for Embedding into Distributions over Excluded Minor Graph   Families",
        "authors": [
            "Douglas E. Carroll",
            "Ashish Goel"
        ],
        "summary": "It was shown recently by Fakcharoenphol et al that arbitrary finite metrics can be embedded into distributions over tree metrics with distortion O(log n). It is also known that this bound is tight since there are expander graphs which cannot be embedded into distributions over trees with better than Omega(log n) distortion.   We show that this same lower bound holds for embeddings into distributions over any minor excluded family. Given a family of graphs F which excludes minor M where |M|=k, we explicitly construct a family of graphs with treewidth-(k+1) which cannot be embedded into a distribution over F with better than Omega(log n) distortion. Thus, while these minor excluded families of graphs are more expressive than trees, they do not provide asymptotically better approximations in general. An important corollary of this is that graphs of treewidth-k cannot be embedded into distributions over graphs of treewidth-(k-3) with distortion less than Omega(log n).   We also extend a result of Alon et al by showing that for any k, planar graphs cannot be embedded into distributions over treewidth-k graphs with better than Omega(log n) distortion.",
        "published": "2008-07-29T07:52:57Z",
        "link": "http://arxiv.org/abs/0807.4582v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Approximate kernel clustering",
        "authors": [
            "Subhash Khot",
            "Assaf Naor"
        ],
        "summary": "In the kernel clustering problem we are given a large $n\\times n$ positive semi-definite matrix $A=(a_{ij})$ with $\\sum_{i,j=1}^na_{ij}=0$ and a small $k\\times k$ positive semi-definite matrix $B=(b_{ij})$. The goal is to find a partition $S_1,...,S_k$ of $\\{1,... n\\}$ which maximizes the quantity $$ \\sum_{i,j=1}^k (\\sum_{(i,j)\\in S_i\\times S_j}a_{ij})b_{ij}. $$ We study the computational complexity of this generic clustering problem which originates in the theory of machine learning. We design a constant factor polynomial time approximation algorithm for this problem, answering a question posed by Song, Smola, Gretton and Borgwardt. In some cases we manage to compute the sharp approximation threshold for this problem assuming the Unique Games Conjecture (UGC). In particular, when $B$ is the $3\\times 3$ identity matrix the UGC hardness threshold of this problem is exactly $\\frac{16\\pi}{27}$. We present and study a geometric conjecture of independent interest which we show would imply that the UGC threshold when $B$ is the $k\\times k$ identity matrix is $\\frac{8\\pi}{9}(1-\\frac{1}{k})$ for every $k\\ge 3$.",
        "published": "2008-07-29T10:40:55Z",
        "link": "http://arxiv.org/abs/0807.4626v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "math.FA"
        ]
    },
    {
        "title": "Finding Dense Subgraphs in G(n,1/2)",
        "authors": [
            "Atish Das Sarma",
            "Amit Deshpande",
            "Ravi Kannan"
        ],
        "summary": "Finding the largest clique is a notoriously hard problem, even on random graphs. It is known that the clique number of a random graph G(n,1/2) is almost surely either k or k+1, where k = 2log n - 2log(log n) - 1. However, a simple greedy algorithm finds a clique of size only (1+o(1))log n, with high probability, and finding larger cliques -- that of size even (1+ epsilon)log n -- in randomized polynomial time has been a long-standing open problem. In this paper, we study the following generalization: given a random graph G(n,1/2), find the largest subgraph with edge density at least (1-delta). We show that a simple modification of the greedy algorithm finds a subset of 2log n vertices whose induced subgraph has edge density at least 0.951, with high probability. To complement this, we show that almost surely there is no subset of 2.784log n vertices whose induced subgraph has edge density 0.951 or more.",
        "published": "2008-07-31T17:12:25Z",
        "link": "http://arxiv.org/abs/0807.5111v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "On the hitting times of quantum versus random walks",
        "authors": [
            "Frederic Magniez",
            "Ashwin Nayak",
            "Peter C. Richter",
            "Miklos Santha"
        ],
        "summary": "In this paper we define new Monte Carlo type classical and quantum hitting times, and we prove several relationships among these and the already existing Las Vegas type definitions. In particular, we show that for some marked state the two types of hitting time are of the same order in both the classical and the quantum case.   Further, we prove that for any reversible ergodic Markov chain $P$, the quantum hitting time of the quantum analogue of $P$ has the same order as the square root of the classical hitting time of $P$. We also investigate the (im)possibility of achieving a gap greater than quadratic using an alternative quantum walk.   Finally, we present new quantum algorithms for the detection and finding problems. The complexities of both algorithms are related to the new, potentially smaller, quantum hitting times. The detection algorithm is based on phase estimation and is particularly simple. The finding algorithm combines a similar phase estimation based procedure with ideas of Tulsi from his recent theorem for the 2D grid. Extending his result, we show that for any state-transitive Markov chain with unique marked state, the quantum hitting time is of the same order for both the detection and finding problems.",
        "published": "2008-08-01T14:45:51Z",
        "link": "http://arxiv.org/abs/0808.0084v1",
        "categories": [
            "quant-ph",
            "cs.DS"
        ]
    },
    {
        "title": "Eigenvalue bounds, spectral partitioning, and metrical deformations via   flows",
        "authors": [
            "Punyashloka Biswal",
            "James R. Lee",
            "Satish Rao"
        ],
        "summary": "We present a new method for upper bounding the second eigenvalue of the Laplacian of graphs. Our approach uses multi-commodity flows to deform the geometry of the graph; we embed the resulting metric into Euclidean space to recover a bound on the Rayleigh quotient. Using this, we show that every $n$-vertex graph of genus $g$ and maximum degree $d$ satisfies $\\lambda_2(G) = O((g+1)^3 d/n)$. This recovers the $O(d/n)$ bound of Spielman and Teng for planar graphs, and compares to Kelner's bound of $O((g+1) poly(d)/n)$, but our proof does not make use of conformal mappings or circle packings. We are thus able to extend this to resolve positively a conjecture of Spielman and Teng, by proving that $\\lambda_2(G) = O(d h^6 \\log h/n)$ whenever $G$ is $K_h$-minor free. This shows, in particular, that spectral partitioning can be used to recover $O(\\sqrt{n})$-sized separators in bounded degree graphs that exclude a fixed minor. We extend this further by obtaining nearly optimal bounds on $\\lambda_2$ for graphs which exclude small-depth minors in the sense of Plotkin, Rao, and Smith. Consequently, we show that spectral algorithms find small separators in a general class of geometric graphs.   Moreover, while the standard \"sweep\" algorithm applied to the second eigenvector may fail to find good quotient cuts in graphs of unbounded degree, our approach produces a vector that works for arbitrary graphs. This yields an alternate proof of the result of Alon, Seymour, and Thomas that every excluded-minor family of graphs has $O(\\sqrt{n})$-node balanced separators.",
        "published": "2008-08-01T16:45:16Z",
        "link": "http://arxiv.org/abs/0808.0148v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "math.MG",
            "math.SP"
        ]
    },
    {
        "title": "Front Propagation with Rejuvenation in Flipping Processes",
        "authors": [
            "T. Antal",
            "D. ben-Avraham",
            "E. Ben-Naim",
            "P. L. Krapivsky"
        ],
        "summary": "We study a directed flipping process that underlies the performance of the random edge simplex algorithm. In this stochastic process, which takes place on a one-dimensional lattice whose sites may be either occupied or vacant, occupied sites become vacant at a constant rate and simultaneously cause all sites to the right to change their state. This random process exhibits rich phenomenology. First, there is a front, defined by the position of the left-most occupied site, that propagates at a nontrivial velocity. Second, the front involves a depletion zone with an excess of vacant sites. The total excess D_k increases logarithmically, D_k ~ ln k, with the distance k from the front. Third, the front exhibits rejuvenation -- young fronts are vigorous but old fronts are sluggish. We investigate these phenomena using a quasi-static approximation, direct solutions of small systems, and numerical simulations.",
        "published": "2008-08-01T17:44:13Z",
        "link": "http://arxiv.org/abs/0808.0159v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.DS",
            "math.PR"
        ]
    },
    {
        "title": "Twice-Ramanujan Sparsifiers",
        "authors": [
            "Joshua Batson",
            "Daniel A. Spielman",
            "Nikhil Srivastava"
        ],
        "summary": "We prove that every graph has a spectral sparsifier with a number of edges linear in its number of vertices. As linear-sized spectral sparsifiers of complete graphs are expanders, our sparsifiers of arbitrary graphs can be viewed as generalizations of expander graphs. In particular, we prove that for every $d>1$ and every undirected, weighted graph $G=(V,E,w)$ on $n$ vertices, there exists a weighted graph $H=(V,F,\\tilde{w})$ with at most $\\lceil d(n-1) \\rceil$ edges such that for every $x \\in \\mathbb{R}^{V}$, \\[ x^{T}L_{G}x \\leq x^{T}L_{H}x \\leq (\\frac{d+1+2\\sqrt{d}}{d+1-2\\sqrt{d}})\\cdot x^{T}L_{G}x \\] where $L_{G}$ and $L_{H}$ are the Laplacian matrices of $G$ and $H$, respectively. Thus, $H$ approximates $G$ spectrally at least as well as a Ramanujan expander with $dn/2$ edges approximates the complete graph. We give an elementary deterministic polynomial time algorithm for constructing $H$.",
        "published": "2008-08-01T18:19:17Z",
        "link": "http://arxiv.org/abs/0808.0163v3",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Computing the nucleolus of weighted voting games",
        "authors": [
            "Edith Elkind",
            "Dmitrii V. Pasechnik"
        ],
        "summary": "Weighted voting games (WVG) are coalitional games in which an agent's contribution to a coalition is given by his it weight, and a coalition wins if its total weight meets or exceeds a given quota. These games model decision-making in political bodies as well as collaboration and surplus division in multiagent domains. The computational complexity of various solution concepts for weighted voting games received a lot of attention in recent years. In particular, Elkind et al.(2007) studied the complexity of stability-related solution concepts in WVGs, namely, of the core, the least core, and the nucleolus. While they have completely characterized the algorithmic complexity of the core and the least core, for the nucleolus they have only provided an NP-hardness result. In this paper, we solve an open problem posed by Elkind et al. by showing that the nucleolus of WVGs, and, more generally, k-vector weighted voting games with fixed k, can be computed in pseudopolynomial time, i.e., there exists an algorithm that correctly computes the nucleolus and runs in time polynomial in the number of players and the maximum weight. In doing so, we propose a general framework for computing the nucleolus, which may be applicable to a wider of class of games.",
        "published": "2008-08-03T05:22:47Z",
        "link": "http://arxiv.org/abs/0808.0298v1",
        "categories": [
            "cs.GT",
            "cs.DS",
            "G.1.6; I.2.8"
        ]
    },
    {
        "title": "Executable Set Theory and Arithmetic Encodings in Prolog",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "The paper is organized as a self-contained literate Prolog program that implements elements of an executable finite set theory with focus on combinatorial generation and arithmetic encodings. The complete Prolog code is available at http://logic.csci.unt.edu/tarau/research/2008/pHFS.zip . First, ranking and unranking functions for some \"mathematically elegant\" data types in the universe of Hereditarily Finite Sets with Urelements are provided, resulting in arithmetic encodings for powersets, hypergraphs, ordinals and choice functions. After implementing a digraph representation of Hereditarily Finite Sets we define {\\em decoration functions} that can recover well-founded sets from encodings of their associated acyclic digraphs. We conclude with an encoding of arbitrary digraphs and discuss a concept of duality induced by the set membership relation. In the process, we uncover the surprising possibility of internally sharing isomorphic objects, independently of their language level types and meanings.",
        "published": "2008-08-05T04:59:56Z",
        "link": "http://arxiv.org/abs/0808.0540v1",
        "categories": [
            "cs.LO",
            "cs.DM",
            "cs.DS",
            "cs.MS",
            "cs.SC"
        ]
    },
    {
        "title": "Ranking Catamorphisms and Unranking Anamorphisms on Hereditarily Finite   Datatypes",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "Using specializations of unfold and fold on a generic tree data type we derive unranking and ranking functions providing natural number encodings for various Hereditarily Finite datatypes.   In this context, we interpret unranking operations as instances of a generic anamorphism and ranking operations as instances of the corresponding catamorphism.   Starting with Ackerman's Encoding from Hereditarily Finite Sets to Natural Numbers we define pairings and tuple encodings that provide building blocks for a theory of Hereditarily Finite Functions.   The more difficult problem of ranking and unranking Hereditarily Finite Permutations is then tackled using Lehmer codes and factoradics.   The self-contained source code of the paper, as generated from a literate Haskell program, is available at \\url{http://logic.csci.unt.edu/tarau/research/2008/fFUN.zip}.   Keywords: ranking/unranking, pairing/tupling functions, Ackermann encoding, hereditarily finite sets, hereditarily finite functions, permutations and factoradics, computational mathematics, Haskell data representations",
        "published": "2008-08-06T00:54:05Z",
        "link": "http://arxiv.org/abs/0808.0753v1",
        "categories": [
            "cs.SC",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "A Functional Hitchhiker's Guide to Hereditarily Finite Sets, Ackermann   Encodings and Pairing Functions",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "The paper is organized as a self-contained literate Haskell program that implements elements of an executable finite set theory with focus on combinatorial generation and arithmetic encodings. The code, tested under GHC 6.6.1, is available at http://logic.csci.unt.edu/tarau/research/2008/fSET.zip .   We introduce ranking and unranking functions generalizing Ackermann's encoding to the universe of Hereditarily Finite Sets with Urelements. Then we build a lazy enumerator for Hereditarily Finite Sets with Urelements that matches the unranking function provided by the inverse of Ackermann's encoding and we describe functors between them resulting in arithmetic encodings for powersets, hypergraphs, ordinals and choice functions. After implementing a digraph representation of Hereditarily Finite Sets we define {\\em decoration functions} that can recover well-founded sets from encodings of their associated acyclic digraphs. We conclude with an encoding of arbitrary digraphs and discuss a concept of duality induced by the set membership relation.   Keywords: hereditarily finite sets, ranking and unranking functions, executable set theory, arithmetic encodings, Haskell data representations, functional programming and computational mathematics",
        "published": "2008-08-06T01:05:09Z",
        "link": "http://arxiv.org/abs/0808.0754v1",
        "categories": [
            "cs.MS",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Declarative Combinatorics: Boolean Functions, Circuit Synthesis and BDDs   in Haskell",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "We describe Haskell implementations of interesting combinatorial generation algorithms with focus on boolean functions and logic circuit representations.   First, a complete exact combinational logic circuit synthesizer is described as a combination of catamorphisms and anamorphisms.   Using pairing and unpairing functions on natural number representations of truth tables, we derive an encoding for Binary Decision Diagrams (BDDs) with the unique property that its boolean evaluation faithfully mimics its structural conversion to a a natural number through recursive application of a matching pairing function.   We then use this result to derive ranking and unranking functions for BDDs and reduced BDDs.   Finally, a generalization of the encoding techniques to Multi-Terminal BDDs is provided.   The paper is organized as a self-contained literate Haskell program, available at http://logic.csci.unt.edu/tarau/research/2008/fBDD.zip .   Keywords: exact combinational logic synthesis, binary decision diagrams, encodings of boolean functions, pairing/unpairing functions, ranking/unranking functions for BDDs and MTBDDs, declarative combinatorics in Haskell",
        "published": "2008-08-06T01:41:51Z",
        "link": "http://arxiv.org/abs/0808.0760v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "On Complexity of Minimum Leaf Out-branching Problem",
        "authors": [
            "Peter Dankelmann",
            "Gregory Gutin",
            "Eun Jung Kim"
        ],
        "summary": "Given a digraph $D$, the Minimum Leaf Out-Branching problem (MinLOB) is the problem of finding in $D$ an out-branching with the minimum possible number of leaves, i.e., vertices of out-degree 0. Gutin, Razgon and Kim (2008) proved that MinLOB is polynomial time solvable for acyclic digraphs which are exactly the digraphs of directed path-width (DAG-width, directed tree-width, respectively) 0. We investigate how much one can extend this polynomiality result. We prove that already for digraphs of directed path-width (directed tree-width, DAG-width, respectively) 1, MinLOB is NP-hard. On the other hand, we show that for digraphs of restricted directed tree-width (directed path-width, DAG-width, respectively) and a fixed integer $k$, the problem of checking whether there is an out-branching with at most $k$ leaves is polynomial time solvable.",
        "published": "2008-08-07T08:41:48Z",
        "link": "http://arxiv.org/abs/0808.0980v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Dynamic Connectivity: Connecting to Networks and Geometry",
        "authors": [
            "Timothy M. Chan",
            "Mihai Patrascu",
            "Liam Roditty"
        ],
        "summary": "Dynamic connectivity is a well-studied problem, but so far the most compelling progress has been confined to the edge-update model: maintain an understanding of connectivity in an undirected graph, subject to edge insertions and deletions. In this paper, we study two more challenging, yet equally fundamental problems.   Subgraph connectivity asks to maintain an understanding of connectivity under vertex updates: updates can turn vertices on and off, and queries refer to the subgraph induced by \"on\" vertices. (For instance, this is closer to applications in networks of routers, where node faults may occur.)   We describe a data structure supporting vertex updates in O (m^{2/3}) amortized time, where m denotes the number of edges in the graph. This greatly improves over the previous result [Chan, STOC'02], which required fast matrix multiplication and had an update time of O(m^0.94). The new data structure is also simpler.   Geometric connectivity asks to maintain a dynamic set of n geometric objects, and query connectivity in their intersection graph. (For instance, the intersection graph of balls describes connectivity in a network of sensors with bounded transmission radius.)   Previously, nontrivial fully dynamic results were known only for special cases like axis-parallel line segments and rectangles. We provide similarly improved update times, O (n^{2/3}), for these special cases. Moreover, we show how to obtain sublinear update bounds for virtually all families of geometric objects which allow sublinear-time range queries, such as arbitrary 2D line segments, d-dimensional simplices, and d-dimensional balls.",
        "published": "2008-08-07T22:16:15Z",
        "link": "http://arxiv.org/abs/0808.1128v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "Cache oblivious storage and access heuristics for blocked matrix-matrix   multiplication",
        "authors": [
            "Nicolas Bock",
            "Emanuel H. Rubensson",
            "Paweł Sałek",
            "Anders M. N. Niklasson",
            "Matt Challacombe"
        ],
        "summary": "We investigate effects of ordering in blocked matrix--matrix multiplication. We find that submatrices do not have to be stored contiguously in memory to achieve near optimal performance. Instead it is the choice of execution order of the submatrix multiplications that leads to a speedup of up to four times for small block sizes. This is in contrast to results for single matrix elements showing that contiguous memory allocation quickly becomes irrelevant as the blocksize increases.",
        "published": "2008-08-07T22:49:29Z",
        "link": "http://arxiv.org/abs/0808.1108v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Minimum Dissatisfaction Personnel Scheduling",
        "authors": [
            "Mugurel Ionut Andreica",
            "Romulus Andreica",
            "Angela Andreica"
        ],
        "summary": "In this paper we consider two problems regarding the scheduling of available personnel in order to perform a given quantity of work, which can be arbitrarily decomposed into a sequence of activities. We are interested in schedules which minimize the overall dissatisfaction, where each employee's dissatisfaction is modeled as a time-dependent linear function. For the two situations considered we provide a detailed mathematical analysis, as well as efficient algorithms for determining optimal schedules.",
        "published": "2008-08-08T17:15:34Z",
        "link": "http://arxiv.org/abs/0808.1246v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Algorithmic Pricing via Virtual Valuations",
        "authors": [
            "Shuchi Chawla",
            "Jason Hartline",
            "Robert Kleinberg"
        ],
        "summary": "Algorithmic pricing is the computational problem that sellers (e.g., in supermarkets) face when trying to set prices for their items to maximize their profit in the presence of a known demand. Guruswami et al. (2005) propose this problem and give logarithmic approximations (in the number of consumers) when each consumer's values for bundles are known precisely. Subsequently several versions of the problem have been shown to have poly-logarithmic inapproximability. This problem has direct ties to the important open question of better understanding the Bayesian optimal mechanism in multi-parameter settings; however, logarithmic approximations are inadequate for this purpose. It is therefore of vital interest to consider special cases where constant approximations are possible. We consider the unit-demand variant of this problem. Here a consumer has a valuation for each different item and their value for a set of items is simply the maximum value they have for any item in the set. We assume that the preferences of the consumers are drawn from a distribution, the standard assumption in economics; furthermore, the setting of a specific set of customers with known preferences, which is employed in all prior work in algorithmic pricing, is a special case of this general problem, where there is a discrete Bayesian distribution for preferences specified by picking one consumer uniformly from the given set of consumers. Our work complements these existing works by considering the case where the consumer's valuations for the different items are independent random variables. Our main result is a constant approximation that makes use of an interesting connection between this problem and the concept of virtual valuations from the single-parameter Bayesian optimal mechanism design literature.",
        "published": "2008-08-12T18:08:21Z",
        "link": "http://arxiv.org/abs/0808.1671v1",
        "categories": [
            "cs.GT",
            "cs.DS"
        ]
    },
    {
        "title": "The Optimal Quantile Estimator for Compressed Counting",
        "authors": [
            "Ping Li"
        ],
        "summary": "Compressed Counting (CC) was recently proposed for very efficiently computing the (approximate) $\\alpha$th frequency moments of data streams, where $0<\\alpha <= 2$. Several estimators were reported including the geometric mean estimator, the harmonic mean estimator, the optimal power estimator, etc. The geometric mean estimator is particularly interesting for theoretical purposes. For example, when $\\alpha -> 1$, the complexity of CC (using the geometric mean estimator) is $O(1/\\epsilon)$, breaking the well-known large-deviation bound $O(1/\\epsilon^2)$. The case $\\alpha\\approx 1$ has important applications, for example, computing entropy of data streams.   For practical purposes, this study proposes the optimal quantile estimator. Compared with previous estimators, this estimator is computationally more efficient and is also more accurate when $\\alpha> 1$.",
        "published": "2008-08-13T01:38:45Z",
        "link": "http://arxiv.org/abs/0808.1766v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A Very Efficient Scheme for Estimating Entropy of Data Streams Using   Compressed Counting",
        "authors": [
            "Ping Li"
        ],
        "summary": "Compressed Counting (CC)} was recently proposed for approximating the $\\alpha$th frequency moments of data streams, for $0<\\alpha \\leq 2$. Under the relaxed strict-Turnstile model, CC dramatically improves the standard algorithm based on symmetric stable random projections}, especially as $\\alpha\\to 1$. A direct application of CC is to estimate the entropy, which is an important summary statistic in Web/network measurement and often serves a crucial \"feature\" for data mining. The R\\'enyi entropy and the Tsallis entropy are functions of the $\\alpha$th frequency moments; and both approach the Shannon entropy as $\\alpha\\to 1$. A recent theoretical work suggested using the $\\alpha$th frequency moment to approximate the Shannon entropy with $\\alpha=1+\\delta$ and very small $|\\delta|$ (e.g., $<10^{-4}$).   In this study, we experiment using CC to estimate frequency moments, R\\'enyi entropy, Tsallis entropy, and Shannon entropy, on real Web crawl data. We demonstrate the variance-bias trade-off in estimating Shannon entropy and provide practical recommendations. In particular, our experiments enable us to draw some important conclusions:   (1) As $\\alpha\\to 1$, CC dramatically improves {\\em symmetric stable random projections} in estimating frequency moments, R\\'enyi entropy, Tsallis entropy, and Shannon entropy. The improvements appear to approach \"infinity.\"   (2) Using {\\em symmetric stable random projections} and $\\alpha = 1+\\delta$ with very small $|\\delta|$ does not provide a practical algorithm because the required sample size is enormous.",
        "published": "2008-08-13T03:05:33Z",
        "link": "http://arxiv.org/abs/0808.1771v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Transitive-Closure Spanners",
        "authors": [
            "Arnab Bhattacharyya",
            "Elena Grigorescu",
            "Kyomin Jung",
            "Sofya Raskhodnikova",
            "David P. Woodruff"
        ],
        "summary": "Given a directed graph G = (V,E) and an integer k>=1, a k-transitive-closure-spanner (k-TC-spanner) of G is a directed graph H = (V, E_H) that has (1) the same transitive-closure as G and (2) diameter at most k. These spanners were implicitly studied in access control, data structures, and property testing, and properties of these spanners have been rediscovered over the span of 20 years. The main goal in each of these applications is to obtain the sparsest k-TC-spanners. We bring these diverse areas under the unifying framework of TC-spanners.   We initiate the study of approximability of the size of the sparsest k-TC-spanner for a given directed graph. We completely resolve the approximability of 2-TC-spanners, showing that it is Theta(log n) unless P = NP. For k>2, we present a polynomial-time algorithm that finds a k-TC-spanner with size within O((n log n)^{1-1/k}) of the optimum. Our algorithmic techniques also yield algorithms with the best-known approximation ratio for well-studied problems on directed spanners when k>3: DIRECTED k-SPANNER, CLIENT/SERVER DIRECTED k-SPANNER, and k-DIAMETER SPANNING SUBGRAPH. For constant k>=3, we show that the size of the sparsest k-TC-spanner is hard to approximate with 2^{log^{1-eps} n} ratio unless NP \\subseteq DTIME(n^{polylog n}}). Finally, we study the size of the sparsest k-TC-spanners for H-minor-free graph families. Combining our constructions with our insight that 2-TC-spanners can be used for designing property testers, we obtain a monotonicity tester with O(log^2 n /eps) queries for any poset whose transitive reduction is an H-minor free digraph, improving the Theta(sqrt(n) log n/eps)-queries required of the tester due to Fischer et al (2002).",
        "published": "2008-08-13T06:44:10Z",
        "link": "http://arxiv.org/abs/0808.1787v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2; G.2.2; G.2.3"
        ]
    },
    {
        "title": "Nearly Tight Low Stretch Spanning Trees",
        "authors": [
            "Ittai Abraham",
            "Yair Bartal",
            "Ofer Neiman"
        ],
        "summary": "We prove that any graph $G$ with $n$ points has a distribution $\\mathcal{T}$ over spanning trees such that for any edge $(u,v)$ the expected stretch $E_{T \\sim \\mathcal{T}}[d_T(u,v)/d_G(u,v)]$ is bounded by $\\tilde{O}(\\log n)$. Our result is obtained via a new approach of building ``highways'' between portals and a new strong diameter probabilistic decomposition theorem.",
        "published": "2008-08-14T15:58:55Z",
        "link": "http://arxiv.org/abs/0808.2017v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Better Bounds for Frequency Moments in Random-Order Streams",
        "authors": [
            "Alexandr Andoni",
            "Andrew McGregor",
            "Krzysztof Onak",
            "Rina Panigrahy"
        ],
        "summary": "Estimating frequency moments of data streams is a very well studied problem and tight bounds are known on the amount of space that is necessary and sufficient when the stream is adversarially ordered. Recently, motivated by various practical considerations and applications in learning and statistics, there has been growing interest into studying streams that are randomly ordered. In the paper we improve the previous lower bounds on the space required to estimate the frequency moments of a randomly ordered streams.",
        "published": "2008-08-16T00:43:14Z",
        "link": "http://arxiv.org/abs/0808.2222v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Declarative Combinatorics: Isomorphisms, Hylomorphisms and Hereditarily   Finite Data Types in Haskell",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "This paper is an exploration in a functional programming framework of {\\em isomorphisms} between elementary data types (natural numbers, sets, multisets, finite functions, permutations binary decision diagrams, graphs, hypergraphs, parenthesis languages, dyadic rationals, primes, DNA sequences etc.) and their extension to hereditarily finite universes through {\\em hylomorphisms} derived from {\\em ranking/unranking} and {\\em pairing/unpairing} operations.   An embedded higher order {\\em combinator language} provides any-to-any encodings automatically.   Besides applications to experimental mathematics, a few examples of ``free algorithms'' obtained by transferring operations between data types are shown. Other applications range from stream iterators on combinatorial objects to self-delimiting codes, succinct data representations and generation of random instances.   The paper covers 59 data types and, through the use of the embedded combinator language, provides 3540 distinct bijective transformations between them.   The self-contained source code of the paper, as generated from a literate Haskell program, is available at \\url{http://logic.csci.unt.edu/tarau/research/2008/fISO.zip}.   {\\bf Keywords}: Haskell data representations, data type isomorphisms, declarative combinatorics, computational mathematics, Ackermann encoding, G\\\"{o}del numberings, arithmetization, ranking/unranking, hereditarily finite sets, functions and permutations, encodings of binary decision diagrams, dyadic rationals, DNA encodings",
        "published": "2008-08-21T16:47:38Z",
        "link": "http://arxiv.org/abs/0808.2953v4",
        "categories": [
            "cs.PL",
            "cs.DS"
        ]
    },
    {
        "title": "On the Monotonicity of Work Function in k-Server Conjecture",
        "authors": [
            "Ming-Zhe Chen"
        ],
        "summary": "This paper presents a mistake in work function algorithm of k-server conjecture. That is, the monotonicity of the work function is not always true.",
        "published": "2008-08-24T15:37:35Z",
        "link": "http://arxiv.org/abs/0808.3197v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Efficient algorithms for the basis of finite Abelian groups",
        "authors": [
            "Gregory Karagiorgos",
            "Dimitrios Poulakis"
        ],
        "summary": "Let $G$ be a finite abelian group $G$ with $N$ elements. In this paper we give a O(N) time algorithm for computing a basis of $G$. Furthermore, we obtain an algorithm for computing a basis from a generating system of $G$ with $M$ elements having time complexity $O(M\\sum_{p|N} e(p)\\lceil p^{1/2}\\rceil^{\\mu(p)})$, where $p$ runs over all the prime divisors of $N$, and $p^{e(p)}$, $\\mu(p)$ are the exponent and the number of cyclic groups which are direct factors of the $p$-primary component of $G$, respectively. In case where $G$ is a cyclic group having a generating system with $M$ elements, a $O(MN^{\\epsilon})$ time algorithm for the computation of a basis of $G$ is obtained.",
        "published": "2008-08-25T11:01:15Z",
        "link": "http://arxiv.org/abs/0808.3331v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Counting Hexagonal Patches and Independent Sets in Circle Graphs",
        "authors": [
            "Paul Bonsma",
            "Felix Breuer"
        ],
        "summary": "A hexagonal patch is a plane graph in which inner faces have length 6, inner vertices have degree 3, and boundary vertices have degree 2 or 3. We consider the following counting problem: given a sequence of twos and threes, how many hexagonal patches exist with this degree sequence along the outer face? This problem is motivated by the study of benzenoid hydrocarbons and fullerenes in computational chemistry. We give the first polynomial time algorithm for this problem. We show that it can be reduced to counting maximum independent sets in circle graphs, and give a simple and fast algorithm for this problem.",
        "published": "2008-08-28T10:39:10Z",
        "link": "http://arxiv.org/abs/0808.3881v2",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Spectral Sparsification of Graphs",
        "authors": [
            "Daniel A. Spielman",
            "Shang-Hua Teng"
        ],
        "summary": "We introduce a new notion of graph sparsificaiton based on spectral similarity of graph Laplacians: spectral sparsification requires that the Laplacian quadratic form of the sparsifier approximate that of the original. This is equivalent to saying that the Laplacian of the sparsifier is a good preconditioner for the Laplacian of the original. We prove that every graph has a spectral sparsifier of nearly linear size. Moreover, we present an algorithm that produces spectral sparsifiers in time $\\softO{m}$, where $m$ is the number of edges in the original graph. This construction is a key component of a nearly-linear time algorithm for solving linear equations in diagonally-dominant matrcies. Our sparsification algorithm makes use of a nearly-linear time algorithm for graph partitioning that satisfies a strong guarantee: if the partition it outputs is very unbalanced, then the larger part is contained in a subgraph of high conductance.",
        "published": "2008-08-29T17:17:29Z",
        "link": "http://arxiv.org/abs/0808.4134v3",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Approximating Transitivity in Directed Networks",
        "authors": [
            "Piotr Berman",
            "Bhaskar DasGupta",
            "Marek Karpinski"
        ],
        "summary": "We study the problem of computing a minimum equivalent digraph (also known as the problem of computing a strong transitive reduction) and its maximum objective function variant, with two types of extensions. First, we allow to declare a set $D\\subset E$ and require that a valid solution $A$ satisfies $D\\subset A$ (it is sometimes called transitive reduction problem). In the second extension (called $p$-ary transitive reduction), we have integer edge labeling and we view two paths as equivalent if they have the same beginning, ending and the sum of labels modulo $p$. A solution $A\\subseteq E$ is valid if it gives an equivalent path for every original path. For all problems we establish the following: polynomial time minimization of $|A|$ within ratio 1.5, maximization of $|E-A|$ within ratio 2, MAX-SNP hardness even of the length of simple cycles is limited to 5. Furthermore, we believe that the combinatorial technique behind the approximation algorithm for the minimization version might be of interest to other graph connectivity problems as well.",
        "published": "2008-09-01T08:58:32Z",
        "link": "http://arxiv.org/abs/0809.0188v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Canonical Coin Systems for Change-Making Problems",
        "authors": [
            "Xuan Cai"
        ],
        "summary": "The Change-Making Problem is to represent a given value with the fewest coins under a given coin system. As a variation of the knapsack problem, it is known to be NP-hard. Nevertheless, in most real money systems, the greedy algorithm yields optimal solutions. In this paper, we study what type of coin systems that guarantee the optimality of the greedy algorithm. We provide new proofs for a sufficient and necessary condition for the so-called \\emph{canonical} coin systems with four or five types of coins, and a sufficient condition for non-canonical coin systems, respectively. Moreover, we present an $O(m^2)$ algorithm that decides whether a tight coin system is canonical.",
        "published": "2008-09-02T11:04:19Z",
        "link": "http://arxiv.org/abs/0809.0400v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "G.2.1"
        ]
    },
    {
        "title": "Stochastic Combinatorial Optimization under Probabilistic Constraints",
        "authors": [
            "Shipra Agrawal",
            "Amin Saberi",
            "Yinyu Ye"
        ],
        "summary": "In this paper, we present approximation algorithms for combinatorial optimization problems under probabilistic constraints. Specifically, we focus on stochastic variants of two important combinatorial optimization problems: the k-center problem and the set cover problem, with uncertainty characterized by a probability distribution over set of points or elements to be covered. We consider these problems under adaptive and non-adaptive settings, and present efficient approximation algorithms for the case when underlying distribution is a product distribution. In contrast to the expected cost model prevalent in stochastic optimization literature, our problem definitions support restrictions on the probability distributions of the total costs, via incorporating constraints that bound the probability with which the incurred costs may exceed a given threshold.",
        "published": "2008-09-02T16:07:42Z",
        "link": "http://arxiv.org/abs/0809.0460v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Efficient Implementation of the Generalized Tunstall Code Generation   Algorithm",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "A method is presented for constructing a Tunstall code that is linear time in the number of output items. This is an improvement on the state of the art for non-Bernoulli sources, including Markov sources, which require a (suboptimal) generalization of Tunstall's algorithm proposed by Savari and analytically examined by Tabus and Rissanen. In general, if n is the total number of output leaves across all Tunstall trees, s is the number of trees (states), and D is the number of leaves of each internal node, then this method takes O((1+(log s)/D) n) time and O(n) space.",
        "published": "2008-09-05T05:14:10Z",
        "link": "http://arxiv.org/abs/0809.0949v3",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "Minkowski Sum Selection and Finding",
        "authors": [
            "Cheng-Wei Luo",
            "Hsiao-Fei Liu",
            "Peng-An Chen",
            "Kun-Mao Chao"
        ],
        "summary": "For the \\textsc{Minkowski Sum Selection} problem with linear objective functions, we obtain the following results: (1) optimal $O(n\\log n)$ time algorithms for $\\lambda=1$; (2) $O(n\\log^2 n)$ time deterministic algorithms and expected $O(n\\log n)$ time randomized algorithms for any fixed $\\lambda>1$. For the \\textsc{Minkowski Sum Finding} problem with linear objective functions or objective functions of the form   $f(x,y)=\\frac{by}{ax}$, we construct optimal $O(n\\log n)$ time algorithms for any fixed $\\lambda\\geq 1$.",
        "published": "2008-09-06T14:31:49Z",
        "link": "http://arxiv.org/abs/0809.1171v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "Multirate Anypath Routing in Wireless Mesh Networks",
        "authors": [
            "Rafael Laufer",
            "Leonard Kleinrock"
        ],
        "summary": "In this paper, we present a new routing paradigm that generalizes opportunistic routing in wireless mesh networks. In multirate anypath routing, each node uses both a set of next hops and a selected transmission rate to reach a destination. Using this rate, a packet is broadcast to the nodes in the set and one of them forwards the packet on to the destination. To date, there is no theory capable of jointly optimizing both the set of next hops and the transmission rate used by each node. We bridge this gap by introducing a polynomial-time algorithm to this problem and provide the proof of its optimality. The proposed algorithm runs in the same running time as regular shortest-path algorithms and is therefore suitable for deployment in link-state routing protocols. We conducted experiments in a 802.11b testbed network, and our results show that multirate anypath routing performs on average 80% and up to 6.4 times better than anypath routing with a fixed rate of 11 Mbps. If the rate is fixed at 1 Mbps instead, performance improves by up to one order of magnitude.",
        "published": "2008-09-09T21:49:04Z",
        "link": "http://arxiv.org/abs/0809.1681v1",
        "categories": [
            "cs.NI",
            "cs.DS"
        ]
    },
    {
        "title": "Improved Smoothed Analysis of the k-Means Method",
        "authors": [
            "Bodo Manthey",
            "Heiko Röglin"
        ],
        "summary": "The k-means method is a widely used clustering algorithm. One of its distinguished features is its speed in practice. Its worst-case running-time, however, is exponential, leaving a gap between practical and theoretical performance. Arthur and Vassilvitskii (FOCS 2006) aimed at closing this gap, and they proved a bound of $\\poly(n^k, \\sigma^{-1})$ on the smoothed running-time of the k-means method, where n is the number of data points and $\\sigma$ is the standard deviation of the Gaussian perturbation. This bound, though better than the worst-case bound, is still much larger than the running-time observed in practice.   We improve the smoothed analysis of the k-means method by showing two upper bounds on the expected running-time of k-means. First, we prove that the expected running-time is bounded by a polynomial in $n^{\\sqrt k}$ and $\\sigma^{-1}$. Second, we prove an upper bound of $k^{kd} \\cdot \\poly(n, \\sigma^{-1})$, where d is the dimension of the data space. The polynomial is independent of k and d, and we obtain a polynomial bound for the expected running-time for $k, d \\in O(\\sqrt{\\log n/\\log \\log n})$.   Finally, we show that k-means runs in smoothed polynomial time for one-dimensional instances.",
        "published": "2008-09-10T07:00:38Z",
        "link": "http://arxiv.org/abs/0809.1715v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Characterization of the errors of the FMM in particle simulations",
        "authors": [
            "Felipe A. Cruz",
            "L. A. Barba"
        ],
        "summary": "The Fast Multipole Method (FMM) offers an acceleration for pairwise interaction calculation, known as $N$-body problems, from $\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$ with $N$ particles. This has brought dramatic increase in the capability of particle simulations in many application areas, such as electrostatics, particle formulations of fluid mechanics, and others. Although the literature on the subject provides theoretical error bounds for the FMM approximation, there are not many reports of the measured errors in a suite of computational experiments. We have performed such an experimental investigation, and summarized the results of about 1000 calculations using the FMM algorithm, to characterize the accuracy of the method in relation with the different parameters available to the user. In addition to the more standard diagnostic of the maximum error, we supply illustrations of the spatial distribution of the errors, which offers visual evidence of all the contributing factors to the overall approximation accuracy: multipole expansion, local expansion, hierarchical spatial decomposition (interaction lists, local domain, far domain). This presentation is a contribution to any researcher wishing to incorporate the FMM acceleration to their application code, as it aids in understanding where accuracy is gained or compromised.",
        "published": "2008-09-10T15:06:08Z",
        "link": "http://arxiv.org/abs/0809.1810v1",
        "categories": [
            "cs.DS",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Thinking Twice about Second-Price Ad Auctions",
        "authors": [
            "Yossi Azar",
            "Benjamin Birnbaum",
            "Anna R. Karlin",
            "C. Thach Nguyen"
        ],
        "summary": "Recent work has addressed the algorithmic problem of allocating advertisement space for keywords in sponsored search auctions so as to maximize revenue, most of which assume that pricing is done via a first-price auction. This does not realistically model the Generalized Second Price (GSP) auction used in practice, in which bidders pay the next-highest bid for keywords that they are allocated. Towards the goal of more realistically modeling these auctions, we introduce the Second-Price Ad Auctions problem, in which bidders' payments are determined by the GSP mechanism. We show that the complexity of the Second-Price Ad Auctions problem is quite different than that of the more studied First-Price Ad Auctions problem. First, unlike the first-price variant, for which small constant-factor approximations are known, it is NP-hard to approximate the Second-Price Ad Auctions problem to any non-trivial factor, even when the bids are small compared to the budgets. Second, this discrepancy extends even to the 0-1 special case that we call the Second-Price Matching problem (2PM). Offline 2PM is APX-hard, and for online 2PM there is no deterministic algorithm achieving a non-trivial competitive ratio and no randomized algorithm achieving a competitive ratio better than 2. This contrasts with the results for the analogous special case in the first-price model, the standard bipartite matching problem, which is solvable in polynomial time and which has deterministic and randomized online algorithms achieving better competitive ratios. On the positive side, we provide a 2-approximation for offline 2PM and a 5.083-competitive randomized algorithm for online 2PM. The latter result makes use of a new generalization of a result on the performance of the \"Ranking\" algorithm for online bipartite matching.",
        "published": "2008-09-10T23:33:47Z",
        "link": "http://arxiv.org/abs/0809.1895v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Fast C-K-R Partitions of Sparse Graphs",
        "authors": [
            "Manor Mendel",
            "Chaya Schwob"
        ],
        "summary": "We present fast algorithms for constructing probabilistic embeddings and approximate distance oracles in sparse graphs. The main ingredient is a fast algorithm for sampling the probabilistic partitions of Calinescu, Karloff, and Rabani in sparse graphs.",
        "published": "2008-09-11T02:10:29Z",
        "link": "http://arxiv.org/abs/0809.1902v2",
        "categories": [
            "cs.DS",
            "F.2; E.1"
        ]
    },
    {
        "title": "Betweenness Centrality : Algorithms and Lower Bounds",
        "authors": [
            "Shiva Kintali"
        ],
        "summary": "One of the most fundamental problems in large scale network analysis is to determine the importance of a particular node in a network. Betweenness centrality is the most widely used metric to measure the importance of a node in a network. In this paper, we present a randomized parallel algorithm and an algebraic method for computing betweenness centrality of all nodes in a network. We prove that any path-comparison based algorithm cannot compute betweenness in less than O(nm) time.",
        "published": "2008-09-11T02:49:07Z",
        "link": "http://arxiv.org/abs/0809.1906v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Low congestion online routing and an improved mistake bound for online   prediction of graph labeling",
        "authors": [
            "Jittat Fakcharoenphol",
            "Boonserm Kijsirikul"
        ],
        "summary": "In this paper, we show a connection between a certain online low-congestion routing problem and an online prediction of graph labeling. More specifically, we prove that if there exists a routing scheme that guarantees a congestion of $\\alpha$ on any edge, there exists an online prediction algorithm with mistake bound $\\alpha$ times the cut size, which is the size of the cut induced by the label partitioning of graph vertices. With previous known bound of $O(\\log n)$ for $\\alpha$ for the routing problem on trees with $n$ vertices, we obtain an improved prediction algorithm for graphs with high effective resistance.   In contrast to previous approaches that move the graph problem into problems in vector space using graph Laplacian and rely on the analysis of the perceptron algorithm, our proof are purely combinatorial. Further more, our approach directly generalizes to the case where labels are not binary.",
        "published": "2008-09-11T19:32:49Z",
        "link": "http://arxiv.org/abs/0809.2075v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "cs.LG",
            "F.2.2"
        ]
    },
    {
        "title": "Algorithms for Locating Constrained Optimal Intervals",
        "authors": [
            "Hsiao-Fei Liu",
            "Peng-An Chen",
            "Kun-Mao Chao"
        ],
        "summary": "In this work, we obtain the following new results.   1. Given a sequence $D=((h_1,s_1), (h_2,s_2) ..., (h_n,s_n))$ of number pairs, where $s_i>0$ for all $i$, and a number $L_h$, we propose an O(n)-time algorithm for finding an index interval $[i,j]$ that maximizes $\\frac{\\sum_{k=i}^{j} h_k}{\\sum_{k=i}^{j} s_k}$ subject to $\\sum_{k=i}^{j} h_k \\geq L_h$.   2. Given a sequence $D=((h_1,s_1), (h_2,s_2) ..., (h_n,s_n))$ of number pairs, where $s_i=1$ for all $i$, and an integer $L_s$ with $1\\leq L_s\\leq n$, we propose an $O(n\\frac{T(L_s^{1/2})}{L_s^{1/2}})$-time algorithm for finding an index interval $[i,j]$ that maximizes $\\frac{\\sum_{k=i}^{j} h_k}{\\sqrt{\\sum_{k=i}^{j} s_k}}$ subject to $\\sum_{k=i}^{j} s_k \\geq L_s$, where $T(n')$ is the time required to solve the all-pairs shortest paths problem on a graph of $n'$ nodes. By the latest result of Chan \\cite{Chan}, $T(n')=O(n'^3 \\frac{(\\log\\log n')^3}{(\\log n')^2})$, so our algorithm runs in subquadratic time $O(nL_s\\frac{(\\log\\log L_s)^3}{(\\log L_s)^2})$.",
        "published": "2008-09-11T20:12:08Z",
        "link": "http://arxiv.org/abs/0809.2097v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "The fast intersection transform with applications to counting paths",
        "authors": [
            "Andreas Björklund",
            "Thore Husfeldt",
            "Petteri Kaski",
            "Mikko Koivisto"
        ],
        "summary": "We present an algorithm for evaluating a linear ``intersection transform'' of a function defined on the lattice of subsets of an $n$-element set. In particular, the algorithm constructs an arithmetic circuit for evaluating the transform in ``down-closure time'' relative to the support of the function and the evaluation domain. As an application, we develop an algorithm that, given as input a digraph with $n$ vertices and bounded integer weights at the edges, counts paths by weight and given length $0\\leq\\ell\\leq n-1$ in time $O^*(\\exp(n\\cdot H(\\ell/(2n))))$, where $H(p)=-p\\log p-(1-p)\\log(1-p)$, and the notation $O^*(\\cdot)$ suppresses a factor polynomial in $n$.",
        "published": "2008-09-15T11:15:05Z",
        "link": "http://arxiv.org/abs/0809.2489v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Simpler Analyses of Local Search Algorithms for Facility Location",
        "authors": [
            "Anupam Gupta",
            "Kanat Tangwongsan"
        ],
        "summary": "We study local search algorithms for metric instances of facility location problems: the uncapacitated facility location problem (UFL), as well as uncapacitated versions of the $k$-median, $k$-center and $k$-means problems. All these problems admit natural local search heuristics: for example, in the UFL problem the natural moves are to open a new facility, close an existing facility, and to swap a closed facility for an open one; in $k$-medians, we are allowed only swap moves. The local-search algorithm for $k$-median was analyzed by Arya et al. (SIAM J. Comput. 33(3):544-562, 2004), who used a clever ``coupling'' argument to show that local optima had cost at most constant times the global optimum. They also used this argument to show that the local search algorithm for UFL was 3-approximation; their techniques have since been applied to other facility location problems.   In this paper, we give a proof of the $k$-median result which avoids this coupling argument. These arguments can be used in other settings where the Arya et al. arguments have been used. We also show that for the problem of opening $k$ facilities $F$ to minimize the objective function $\\Phi_p(F) = \\big(\\sum_{j \\in V} d(j, F)^p\\big)^{1/p}$, the natural swap-based local-search algorithm is a $\\Theta(p)$-approximation. This implies constant-factor approximations for $k$-medians (when $p=1$), and $k$-means (when $p = 2$), and an $O(\\log n)$-approximation algorithm for the $k$-center problem (which is essentially $p = \\log n$).",
        "published": "2008-09-15T15:42:47Z",
        "link": "http://arxiv.org/abs/0809.2554v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Polynomial kernels for 3-leaf power graph modification problems",
        "authors": [
            "Stephane Bessy",
            "Christophe Paul",
            "Anthony Perez"
        ],
        "summary": "A graph G=(V,E) is a 3-leaf power iff there exists a tree T whose leaves are V and such that (u,v) is an edge iff u and v are at distance at most 3 in T. The 3-leaf power graph edge modification problems, i.e. edition (also known as the closest 3-leaf power), completion and edge-deletion, are FTP when parameterized by the size of the edge set modification. However polynomial kernel was known for none of these three problems. For each of them, we provide cubic kernels that can be computed in linear time for each of these problems. We thereby answer an open problem first mentioned by Dom, Guo, Huffner and Niedermeier (2005).",
        "published": "2008-09-17T06:16:37Z",
        "link": "http://arxiv.org/abs/0809.2858v2",
        "categories": [
            "cs.DM",
            "cs.DS",
            "F.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Sorting by Placement and Shift",
        "authors": [
            "Sergi Elizalde",
            "Peter Winkler"
        ],
        "summary": "In sorting situations where the final destination of each item is known, it is natural to repeatedly choose items and place them where they belong, allowing the intervening items to shift by one to make room. (In fact, a special case of this algorithm is commonly used to hand-sort files.) However, it is not obvious that this algorithm necessarily terminates.   We show that in fact the algorithm terminates after at most $2^{n-1}-1$ steps in the worst case (confirming a conjecture of L. Larson), and that there are super-exponentially many permutations for which this exact bound can be achieved. The proof involves a curious symmetrical binary representation.",
        "published": "2008-09-17T16:29:38Z",
        "link": "http://arxiv.org/abs/0809.2957v1",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.DS",
            "68W40 (Primary); 68R05, 05A05 (Secondary)"
        ]
    },
    {
        "title": "Single source shortest paths in $H$-minor free graphs",
        "authors": [
            "Raphael Yuster"
        ],
        "summary": "We present an algorithm for the Single Source Shortest Paths (SSSP) problem in \\emph{$H$-minor free} graphs. For every fixed $H$, if $G$ is a graph with $n$ vertices having integer edge lengths and $s$ is a designated source vertex of $G$, the algorithm runs in $\\tilde{O}(n^{\\sqrt{11.5}-2} \\log L) \\le O(n^{1.392} \\log L)$ time, where $L$ is the absolute value of the smallest edge length. The algorithm computes shortest paths and the distances from $s$ to all vertices of the graph, or else provides a certificate that $G$ is not $H$-minor free. Our result improves an earlier $O(n^{1.5} \\log L)$ time algorithm for this problem, which follows from a general SSSP algorithm of Goldberg.",
        "published": "2008-09-17T17:51:09Z",
        "link": "http://arxiv.org/abs/0809.2970v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A Local Clustering Algorithm for Massive Graphs and its Application to   Nearly-Linear Time Graph Partitioning",
        "authors": [
            "Daniel A. Spielman",
            "Shang-Hua Teng"
        ],
        "summary": "We study the design of local algorithms for massive graphs. A local algorithm is one that finds a solution containing or near a given vertex without looking at the whole graph. We present a local clustering algorithm. Our algorithm finds a good cluster--a subset of vertices whose internal connections are significantly richer than its external connections--near a given vertex. The running time of our algorithm, when it finds a non-empty local cluster, is nearly linear in the size of the cluster it outputs.   Our clustering algorithm could be a useful primitive for handling massive graphs, such as social networks and web-graphs. As an application of this clustering algorithm, we present a partitioning algorithm that finds an approximate sparsest cut with nearly optimal balance. Our algorithm takes time nearly linear in the number edges of the graph.   Using the partitioning algorithm of this paper, we have designed a nearly-linear time algorithm for constructing spectral sparsifiers of graphs, which we in turn use in a nearly-linear time algorithm for solving linear systems in symmetric, diagonally-dominant matrices. The linear system solver also leads to a nearly linear-time algorithm for approximating the second-smallest eigenvalue and corresponding eigenvector of the Laplacian matrix of a graph. These other results are presented in two companion papers.",
        "published": "2008-09-18T19:20:07Z",
        "link": "http://arxiv.org/abs/0809.3232v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Inferring Company Structure from Limited Available Information",
        "authors": [
            "Mugurel Ionut Andreica",
            "Angela Andreica",
            "Romulus Andreica"
        ],
        "summary": "In this paper we present several algorithmic techniques for inferring the structure of a company when only a limited amount of information is available. We consider problems with two types of inputs: the number of pairs of employees with a given property and restricted information about the hierarchical structure of the company. We provide dynamic programming and greedy algorithms for these problems.",
        "published": "2008-09-20T20:05:21Z",
        "link": "http://arxiv.org/abs/0809.3527v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Locating Restricted Facilities on Binary Maps",
        "authors": [
            "Mugurel Ionut Andreica",
            "Cristina Teodora Andreica",
            "Madalina Ecaterina Andreica"
        ],
        "summary": "In this paper we consider several facility location problems with applications to cost and social welfare optimization, when the area map is encoded as a binary (0,1) mxn matrix. We present algorithmic solutions for all the problems. Some cases are too particular to be used in practical situations, but they are at least a starting point for more generic solutions.",
        "published": "2008-09-20T20:06:34Z",
        "link": "http://arxiv.org/abs/0809.3528v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Dynamic tree algorithms",
        "authors": [
            "Hanène Mohamed",
            "Philippe Robert"
        ],
        "summary": "In this paper, a general tree algorithm processing a random flow of arrivals is analyzed. Capetanakis--Tsybakov--Mikhailov's protocol in the context of communication networks with random access is an example of such an algorithm. In computer science, this corresponds to a trie structure with a dynamic input. Mathematically, it is related to a stopped branching process with exogeneous arrivals (immigration). Under quite general assumptions on the distribution of the number of arrivals and on the branching procedure, it is shown that there exists a positive constant $\\lambda_c$ so that if the arrival rate is smaller than $\\lambda_c$, then the algorithm is stable under the flow of requests, that is, that the total size of an associated tree is integrable. At the same time, a gap in the earlier proofs of stability in the literature is fixed. When the arrivals are Poisson, an explicit characterization of $\\lambda_c$ is given. Under the stability condition, the asymptotic behavior of the average size of a tree starting with a large number of individuals is analyzed. The results are obtained with the help of a probabilistic rewriting of the functional equations describing the dynamics of the system. The proofs use extensively this stochastic background throughout the paper. In this analysis, two basic limit theorems play a key role: the renewal theorem and the convergence to equilibrium of an auto-regressive process with a moving average.",
        "published": "2008-09-21T11:46:05Z",
        "link": "http://arxiv.org/abs/0809.3577v2",
        "categories": [
            "math.PR",
            "cs.DS",
            "68W40, 60K20 (Primary), 90B15 (Secondary)"
        ]
    },
    {
        "title": "Approximating acyclicity parameters of sparse hypergraphs",
        "authors": [
            "Fedor V. Fomin",
            "Petr A. Golovach",
            "Dimitrios M. Thilikos"
        ],
        "summary": "The notions of hypertree width and generalized hypertree width were introduced by Gottlob, Leone, and Scarcello in order to extend the concept of hypergraph acyclicity. These notions were further generalized by Grohe and Marx, who introduced the fractional hypertree width of a hypergraph. All these width parameters on hypergraphs are useful for extending tractability of many problems in database theory and artificial intelligence. In this paper, we study the approximability of (generalized, fractional) hyper treewidth of sparse hypergraphs where the criterion of sparsity reflects the sparsity of their incidence graphs. Our first step is to prove that the (generalized, fractional) hypertree width of a hypergraph H is constant-factor sandwiched by the treewidth of its incidence graph, when the incidence graph belongs to some apex-minor-free graph class. This determines the combinatorial borderline above which the notion of (generalized, fractional) hypertree width becomes essentially more general than treewidth, justifying that way its functionality as a hypergraph acyclicity measure. While for more general sparse families of hypergraphs treewidth of incidence graphs and all hypertree width parameters may differ arbitrarily, there are sparse families where a constant factor approximation algorithm is possible. In particular, we give a constant factor approximation polynomial time algorithm for (generalized, fractional) hypertree width on hypergraphs whose incidence graphs belong to some H-minor-free graph class.",
        "published": "2008-09-22T08:17:22Z",
        "link": "http://arxiv.org/abs/0809.3646v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Multistep greedy algorithm identifies community structure in real-world   and computer-generated networks",
        "authors": [
            "Philipp Schuetz",
            "Amedeo Caflisch"
        ],
        "summary": "We have recently introduced a multistep extension of the greedy algorithm for modularity optimization. The extension is based on the idea that merging l pairs of communities (l>1) at each iteration prevents premature condensation into few large communities. Here, an empirical formula is presented for the choice of the step width l that generates partitions with (close to) optimal modularity for 17 real-world and 1100 computer-generated networks. Furthermore, an in-depth analysis of the communities of two real-world networks (the metabolic network of the bacterium E. coli and the graph of coappearing words in the titles of papers coauthored by Martin Karplus) provides evidence that the partition obtained by the multistep greedy algorithm is superior to the one generated by the original greedy algorithm not only with respect to modularity but also according to objective criteria. In other words, the multistep extension of the greedy algorithm reduces the danger of getting trapped in local optima of modularity and generates more reasonable partitions.",
        "published": "2008-09-25T13:46:30Z",
        "link": "http://arxiv.org/abs/0809.4398v1",
        "categories": [
            "cs.DS",
            "cond-mat.dis-nn",
            "physics.soc-ph",
            "q-bio.MN",
            "q-bio.QM"
        ]
    },
    {
        "title": "A Generic Top-Down Dynamic-Programming Approach to Prefix-Free Coding",
        "authors": [
            "Mordecai Golin",
            "Xiaoming Xu",
            "Jiajin Yu"
        ],
        "summary": "Given a probability distribution over a set of n words to be transmitted, the Huffman Coding problem is to find a minimal-cost prefix free code for transmitting those words. The basic Huffman coding problem can be solved in O(n log n) time but variations are more difficult. One of the standard techniques for solving these variations utilizes a top-down dynamic programming approach. In this paper we show that this approach is amenable to dynamic programming speedup techniques, permitting a speedup of an order of magnitude for many algorithms in the literature for such variations as mixed radix, reserved length and one-ended coding. These speedups are immediate implications of a general structural property that permits batching together the calculation of many DP entries.",
        "published": "2008-09-26T09:42:04Z",
        "link": "http://arxiv.org/abs/0809.4577v1",
        "categories": [
            "cs.DS",
            "cs.IT",
            "math.IT",
            "E.1; E.4; H.1.1"
        ]
    },
    {
        "title": "The Imaginary Sliding Window As a New Data Structure for Adaptive   Algorithms",
        "authors": [
            "Boris Ryabko"
        ],
        "summary": "The scheme of the sliding window is known in Information Theory, Computer Science, the problem of predicting and in stastistics. Let a source with unknown statistics generate some word $... x_{-1}x_{0}x_{1}x_{2}...$ in some alphabet $A$. For every moment $t, t=... $ $-1, 0, 1, ...$, one stores the word (\"window\") $ x_{t-w} x_{t-w+1}... x_{t-1}$ where $w$,$w \\geq 1$, is called \"window length\". In the theory of universal coding, the code of the $x_{t}$ depends on source ststistics estimated by the window, in the problem of predicting, each letter $x_{t}$ is predicted using information of the window, etc. After that the letter $x_{t}$ is included in the window on the right, while $x_{t-w}$ is removed from the window. It is the sliding window scheme. This scheme has two merits: it allows one i) to estimate the source statistics quite precisely and ii) to adapt the code in case of a change in the source' statistics. However this scheme has a defect, namely, the necessity to store the window (i.e. the word $x_{t-w}... x_{t-1})$ which needs a large memory size for large $w$. A new scheme named \"the Imaginary Sliding Window (ISW)\" is constructed. The gist of this scheme is that not the last element $x_{t-w}$ but rather a random one is removed from the window. This allows one to retain both merits of the sliding window as well as the possibility of not storing the window and thus significantly decreasing the memory size.",
        "published": "2008-09-27T07:42:44Z",
        "link": "http://arxiv.org/abs/0809.4743v1",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT"
        ]
    },
    {
        "title": "Efficient, Differentially Private Point Estimators",
        "authors": [
            "Adam Smith"
        ],
        "summary": "Differential privacy is a recent notion of privacy for statistical databases that provides rigorous, meaningful confidentiality guarantees, even in the presence of an attacker with access to arbitrary side information.   We show that for a large class of parametric probability models, one can construct a differentially private estimator whose distribution converges to that of the maximum likelihood estimator. In particular, it is efficient and asymptotically unbiased. This result provides (further) compelling evidence that rigorous notions of privacy in statistical databases can be consistent with statistically valid inference.",
        "published": "2008-09-27T19:57:34Z",
        "link": "http://arxiv.org/abs/0809.4794v1",
        "categories": [
            "cs.CR",
            "cs.DS"
        ]
    },
    {
        "title": "Multi-Armed Bandits in Metric Spaces",
        "authors": [
            "Robert Kleinberg",
            "Aleksandrs Slivkins",
            "Eli Upfal"
        ],
        "summary": "In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of trials so as to maximize the total payoff of the chosen strategies. While the performance of bandit algorithms with a small finite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. The goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efficient solutions. In this work we study a very general setting for the multi-armed bandit problem in which the strategies form a metric space, and the payoff function satisfies a Lipschitz condition with respect to the metric. We refer to this problem as the \"Lipschitz MAB problem\". We present a complete solution for the multi-armed problem in this setting. That is, for every metric space (L,X) we define an isometry invariant which bounds from below the performance of Lipschitz MAB algorithms for X, and we present an algorithm which comes arbitrarily close to meeting this bound. Furthermore, our technique gives even better results for benign payoff functions.",
        "published": "2008-09-29T01:58:13Z",
        "link": "http://arxiv.org/abs/0809.4882v1",
        "categories": [
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Planar Visibility Counting",
        "authors": [
            "Matthias Fischer",
            "Matthias Hilbig",
            "Claudius Jähn",
            "Friedhelm Meyer auf der Heide",
            "Martin Ziegler"
        ],
        "summary": "For a fixed virtual scene (=collection of simplices) S and given observer position p, how many elements of S are weakly visible (i.e. not fully occluded by others) from p? The present work explores the trade-off between query time and preprocessing space for these quantities in 2D: exactly, in the approximate deterministic, and in the probabilistic sense. We deduce the EXISTENCE of an O(m^2/n^2) space data structure for S that, given p and time O(log n), allows to approximate the ratio of occluded segments up to arbitrary constant absolute error; here m denotes the size of the Visibility Graph--which may be quadratic, but typically is just linear in the size n of the scene S. On the other hand, we present a data structure CONSTRUCTIBLE in O(n*log(n)+m^2*polylog(n)/k) preprocessing time and space with similar approximation properties and query time O(k*polylog n), where k<n is an arbitrary parameter. We describe an implementation of this approach and demonstrate the practical benefit of the parameter k to trade memory for query time in an empirical evaluation on three classes of benchmark scenes.",
        "published": "2008-10-01T01:53:52Z",
        "link": "http://arxiv.org/abs/0810.0052v3",
        "categories": [
            "cs.CG",
            "cs.DS",
            "I.3.5; F.2.2"
        ]
    },
    {
        "title": "Acerca del Algoritmo de Dijkstra",
        "authors": [
            "Alvaro Salas"
        ],
        "summary": "In this paper we prove the correctness of Dijkstra's algorithm. We also discuss it and at the end we show an application.",
        "published": "2008-10-01T04:55:11Z",
        "link": "http://arxiv.org/abs/0810.0075v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A Fast Generic Sequence Matching Algorithm",
        "authors": [
            "David R. Musser",
            "Gor V. Nishanov"
        ],
        "summary": "A string matching -- and more generally, sequence matching -- algorithm is presented that has a linear worst-case computing time bound, a low worst-case bound on the number of comparisons (2n), and sublinear average-case behavior that is better than that of the fastest versions of the Boyer-Moore algorithm. The algorithm retains its efficiency advantages in a wide variety of sequence matching problems of practical interest, including traditional string matching; large-alphabet problems (as in Unicode strings); and small-alphabet, long-pattern problems (as in DNA searches). Since it is expressed as a generic algorithm for searching in sequences over an arbitrary type T, it is well suited for use in generic software libraries such as the C++ Standard Template Library. The algorithm was obtained by adding to the Knuth-Morris-Pratt algorithm one of the pattern-shifting techniques from the Boyer-Moore algorithm, with provision for use of hashing in this technique. In situations in which a hash function or random access to the sequences is not available, the algorithm falls back to an optimized version of the Knuth-Morris-Pratt algorithm.",
        "published": "2008-10-01T19:54:51Z",
        "link": "http://arxiv.org/abs/0810.0264v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "The Ratio Index for Budgeted Learning, with Applications",
        "authors": [
            "Ashish Goel",
            "Sanjeev Khanna",
            "Brad Null"
        ],
        "summary": "In the budgeted learning problem, we are allowed to experiment on a set of alternatives (given a fixed experimentation budget) with the goal of picking a single alternative with the largest possible expected payoff. Approximation algorithms for this problem were developed by Guha and Munagala by rounding a linear program that couples the various alternatives together. In this paper we present an index for this problem, which we call the ratio index, which also guarantees a constant factor approximation. Index-based policies have the advantage that a single number (i.e. the index) can be computed for each alternative irrespective of all other alternatives, and the alternative with the highest index is experimented upon. This is analogous to the famous Gittins index for the discounted multi-armed bandit problem.   The ratio index has several interesting structural properties. First, we show that it can be computed in strongly polynomial time. Second, we show that with the appropriate discount factor, the Gittins index and our ratio index are constant factor approximations of each other, and hence the Gittins index also gives a constant factor approximation to the budgeted learning problem. Finally, we show that the ratio index can be used to create an index-based policy that achieves an O(1)-approximation for the finite horizon version of the multi-armed bandit problem. Moreover, the policy does not require any knowledge of the horizon (whereas we compare its performance against an optimal strategy that is aware of the horizon). This yields the following surprising result: there is an index-based policy that achieves an O(1)-approximation for the multi-armed bandit problem, oblivious to the underlying discount factor.",
        "published": "2008-10-03T01:37:45Z",
        "link": "http://arxiv.org/abs/0810.0558v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Packing multiway cuts in capacitated graphs",
        "authors": [
            "Siddharth Barman",
            "Shuchi Chawla"
        ],
        "summary": "We consider the following \"multiway cut packing\" problem in undirected graphs: we are given a graph G=(V,E) and k commodities, each corresponding to a set of terminals located at different vertices in the graph; our goal is to produce a collection of cuts {E_1,...,E_k} such that E_i is a multiway cut for commodity i and the maximum load on any edge is minimized. The load on an edge is defined to be the number of cuts in the solution crossing the edge. In the capacitated version of the problem the goal is to minimize the maximum relative load on any edge--the ratio of the edge's load to its capacity. Multiway cut packing arises in the context of graph labeling problems where we are given a partial labeling of a set of items and a neighborhood structure over them, and, informally, the goal is to complete the labeling in the most consistent way. This problem was introduced by Rabani, Schulman, and Swamy (SODA'08), who developed an O(log n/log log n) approximation for it in general graphs, as well as an improved O(log^2 k) approximation in trees. Here n is the number of nodes in the graph. We present the first constant factor approximation for this problem in arbitrary undirected graphs. Our approach is based on the observation that every instance of the problem admits a near-optimal laminar solution (that is, one in which no pair of cuts cross each other).",
        "published": "2008-10-03T16:13:00Z",
        "link": "http://arxiv.org/abs/0810.0674v1",
        "categories": [
            "cs.DS",
            "G.2.2"
        ]
    },
    {
        "title": "A linear time algorithm for L(2,1)-labeling of trees",
        "authors": [
            "Toru Hasunuma",
            "Toshimasa Ishii",
            "Hirotaka Ono",
            "Yushi Uno"
        ],
        "summary": "An L(2,1)-labeling of a graph $G$ is an assignment $f$ from the vertex set $V(G)$ to the set of nonnegative integers such that $|f(x)-f(y)|\\ge 2$ if $x$ and $y$ are adjacent and $|f(x)-f(y)|\\ge 1$ if $x$ and $y$ are at distance 2, for all $x$ and $y$ in $V(G)$. A $k$-L(2,1)-labeling is an assignment $f:V(G)\\to\\{0,..., k\\}$, and the L(2,1)-labeling problem asks the minimum $k$, which we denote by $\\lambda(G)$, among all possible assignments. It is known that this problem is NP-hard even for graphs of treewidth 2, and tree is one of a very few classes for which the problem is polynomially solvable. The running time of the best known algorithm for trees had been $\\mO(\\Delta^{4.5} n)$ for more than a decade, however, an $\\mO(n^{1.75})$-time algorithm has been proposed recently, which substantially improved the previous one, where $\\Delta$ is the maximum degree of $T$ and $n=|V(T)|$. In this paper, we finally establish a linear time algorithm for L(2,1)-labeling of trees.",
        "published": "2008-10-06T08:21:17Z",
        "link": "http://arxiv.org/abs/0810.0906v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Community Structure in Large Networks: Natural Cluster Sizes and the   Absence of Large Well-Defined Clusters",
        "authors": [
            "Jure Leskovec",
            "Kevin J. Lang",
            "Anirban Dasgupta",
            "Michael W. Mahoney"
        ],
        "summary": "A large body of work has been devoted to defining and identifying clusters or communities in social and information networks. We explore from a novel perspective several questions related to identifying meaningful communities in large social and information networks, and we come to several striking conclusions. We employ approximation algorithms for the graph partitioning problem to characterize as a function of size the statistical and structural properties of partitions of graphs that could plausibly be interpreted as communities. In particular, we define the network community profile plot, which characterizes the \"best\" possible community--according to the conductance measure--over a wide range of size scales. We study over 100 large real-world social and information networks. Our results suggest a significantly more refined picture of community structure in large networks than has been appreciated previously. In particular, we observe tight communities that are barely connected to the rest of the network at very small size scales; and communities of larger size scales gradually \"blend into\" the expander-like core of the network and thus become less \"community-like.\" This behavior is not explained, even at a qualitative level, by any of the commonly-used network generation models. Moreover, it is exactly the opposite of what one would expect based on intuition from expander graphs, low-dimensional or manifold-like graphs, and from small social networks that have served as testbeds of community detection algorithms. We have found that a generative graph model, in which new edges are added via an iterative \"forest fire\" burning process, is able to produce graphs exhibiting a network community profile plot similar to what we observe in our network datasets.",
        "published": "2008-10-08T05:42:43Z",
        "link": "http://arxiv.org/abs/0810.1355v1",
        "categories": [
            "cs.DS",
            "physics.data-an",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Constraint satisfaction problems with isolated solutions are hard",
        "authors": [
            "Lenka Zdeborová",
            "Marc Mézard"
        ],
        "summary": "We study the phase diagram and the algorithmic hardness of the random `locked' constraint satisfaction problems, and compare them to the commonly studied 'non-locked' problems like satisfiability of boolean formulas or graph coloring. The special property of the locked problems is that clusters of solutions are isolated points. This simplifies significantly the determination of the phase diagram, which makes the locked problems particularly appealing from the mathematical point of view. On the other hand we show empirically that the clustered phase of these problems is extremely hard from the algorithmic point of view: the best known algorithms all fail to find solutions. Our results suggest that the easy/hard transition (for currently known algorithms) in the locked problems coincides with the clustering transition. These should thus be regarded as new benchmarks of really hard constraint satisfaction problems.",
        "published": "2008-10-08T18:28:28Z",
        "link": "http://arxiv.org/abs/0810.1499v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Identifying almost sorted permutations from TCP buffer dynamics",
        "authors": [
            "Gabriel Istrate"
        ],
        "summary": "Associate to each sequence $A$ of integers (intending to represent packet IDs) a sequence of positive integers of the same length ${\\mathcal M}(A)$. The $i$'th entry of ${\\mathcal M}(A)$ is the size (at time $i$) of the smallest buffer needed to hold out-of-order packets, where space is accounted for unreceived packets as well. Call two sequences $A$, $B$ {\\em equivalent} (written $A\\equiv_{FB} B$) if ${\\mathcal M}(A)={\\mathcal M}(B)$.   We prove the following result: any two permutations $A,B$ of the same length with $SUS(A)$, $SUS(B)\\leq 3$ (where SUS is the {\\em shuffled-up-sequences} reordering measure), and such that $A\\equiv_{FB} B$ are identical.   The result (which is no longer valid if we replace the upper bound 3 by 4) was motivated by RESTORED, a receiver-oriented model of network traffic we have previously introduced.",
        "published": "2008-10-09T12:27:32Z",
        "link": "http://arxiv.org/abs/0810.1639v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "math.CO"
        ]
    },
    {
        "title": "Near-Optimal Radio Use For Wireless Network Synchronization",
        "authors": [
            "Milan Bradonjic",
            "Eddie Kohler",
            "Rafail Ostrovsky"
        ],
        "summary": "We consider the model of communication where wireless devices can either switch their radios off to save energy, or switch their radios on and engage in communication. We distill a clean theoretical formulation of this problem of minimizing radio use and present near-optimal solutions. Our base model ignores issues of communication interference, although we also extend the model to handle this requirement. We assume that nodes intend to communicate periodically, or according to some time-based schedule. Clearly, perfectly synchronized devices could switch their radios on for exactly the minimum periods required by their joint schedules. The main challenge in the deployment of wireless networks is to synchronize the devices' schedules, given that their initial schedules may be offset relative to one another (even if their clocks run at the same speed). We significantly improve previous results, and show optimal use of the radio for two processors and near-optimal use of the radio for synchronization of an arbitrary number of processors. In particular, for two processors we prove deterministically matching $\\Theta(\\sqrt{n})$ upper and lower bounds on the number of times the radio has to be on, where $n$ is the discretized uncertainty period of the clock shift between the two processors. (In contrast, all previous results for two processors are randomized.) For $m=n^\\beta$ processors (for any $\\beta < 1$) we prove $\\Omega(n^{(1-\\beta)/2})$ is the lower bound on the number of times the radio has to be switched on (per processor), and show a nearly matching (in terms of the radio use) $\\~{O}(n^{(1-\\beta)/2})$ randomized upper bound per processor, with failure probability exponentially close to 0. For $\\beta \\geq 1$ our algorithm runs with at most $poly-log(n)$ radio invocations per processor. Our bounds also hold in a radio-broadcast model where interference must be taken into account.",
        "published": "2008-10-09T20:41:23Z",
        "link": "http://arxiv.org/abs/0810.1756v2",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Split decomposition and graph-labelled trees: characterizations and   fully-dynamic algorithms for totally decomposable graphs",
        "authors": [
            "Emeric Gioan",
            "Christophe Paul"
        ],
        "summary": "In this paper, we revisit the split decomposition of graphs and give new combinatorial and algorithmic results for the class of totally decomposable graphs, also known as the distance hereditary graphs, and for two non-trivial subclasses, namely the cographs and the 3-leaf power graphs. Precisely, we give strutural and incremental characterizations, leading to optimal fully-dynamic recognition algorithms for vertex and edge modifications, for each of these classes. These results rely on a new framework to represent the split decomposition, namely the graph-labelled trees, which also captures the modular decomposition of graphs and thereby unify these two decompositions techniques. The point of the paper is to use bijections between these graph classes and trees whose nodes are labelled by cliques and stars. Doing so, we are also able to derive an intersection model for distance hereditary graphs, which answers an open problem.",
        "published": "2008-10-10T07:49:30Z",
        "link": "http://arxiv.org/abs/0810.1823v2",
        "categories": [
            "cs.DM",
            "cs.DS",
            "G.2.1; G.2.2"
        ]
    },
    {
        "title": "1.25 Approximation Algorithm for the Steiner Tree Problem with Distances   One and Two",
        "authors": [
            "Piotr Berman",
            "Marek Karpinski",
            "Alex Zelikovsky"
        ],
        "summary": "We give a 1.25 approximation algorithm for the Steiner Tree Problem with distances one and two, improving on the best known bound for that problem.",
        "published": "2008-10-10T11:25:09Z",
        "link": "http://arxiv.org/abs/0810.1851v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "A Model for Communication in Clusters of Multi-core Machines",
        "authors": [
            "Christine Task",
            "Arun Chauhan"
        ],
        "summary": "A common paradigm for scientific computing is distributed message-passing systems, and a common approach to these systems is to implement them across clusters of high-performance workstations. As multi-core architectures become increasingly mainstream, these clusters are very likely to include multi-core machines. However, the theoretical models which are currently used to develop communication algorithms across these systems do not take into account the unique properties of processes running on shared-memory architectures, including shared external network connections and communication via shared memory locations. Because of this, existing algorithms are far from optimal for modern clusters. Additionally, recent attempts to adapt these algorithms to multicore systems have proceeded without the introduction of a more accurate formal model and have generally neglected to capitalize on the full power these systems offer. We propose a new model which simply and effectively captures the strengths of multi-core machines in collective communications patterns and suggest how it could be used to properly optimize these patterns.",
        "published": "2008-10-13T04:04:42Z",
        "link": "http://arxiv.org/abs/0810.2150v2",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Efficient Pattern Matching on Binary Strings",
        "authors": [
            "Simone Faro",
            "Thierry Lecroq"
        ],
        "summary": "The binary string matching problem consists in finding all the occurrences of a pattern in a text where both strings are built on a binary alphabet. This is an interesting problem in computer science, since binary data are omnipresent in telecom and computer network applications. Moreover the problem finds applications also in the field of image processing and in pattern matching on compressed texts. Recently it has been shown that adaptations of classical exact string matching algorithms are not very efficient on binary data. In this paper we present two efficient algorithms for the problem adapted to completely avoid any reference to bits allowing to process pattern and text byte by byte. Experimental results show that the new algorithms outperform existing solutions in most cases.",
        "published": "2008-10-14T08:44:27Z",
        "link": "http://arxiv.org/abs/0810.2390v2",
        "categories": [
            "cs.DS",
            "cs.IR",
            "F.2.2; H.3.3; E.4"
        ]
    },
    {
        "title": "Watermarking Digital Images Based on a Content Based Image Retrieval   Technique",
        "authors": [
            "Dimitrios K. Tsolis",
            "Spyros Sioutas",
            "Theodore S. Papatheodorou"
        ],
        "summary": "The current work is focusing on the implementation of a robust watermarking algorithm for digital images, which is based on an innovative spread spectrum analysis algorithm for watermark embedding and on a content-based image retrieval technique for watermark detection. The highly robust watermark algorithms are applying \"detectable watermarks\" for which a detection mechanism checks if the watermark exists or no (a Boolean decision) based on a watermarking key. The problem is that the detection of a watermark in a digital image library containing thousands of images means that the watermark detection algorithm is necessary to apply all the keys to the digital images. This application is non-efficient for very large image databases. On the other hand \"readable\" watermarks may prove weaker but easier to detect as only the detection mechanism is required. The proposed watermarking algorithm combine's the advantages of both \"detectable\" and \"readable\" watermarks. The result is a fast and robust watermarking algorithm.",
        "published": "2008-10-17T04:22:01Z",
        "link": "http://arxiv.org/abs/0810.3058v1",
        "categories": [
            "cs.DS",
            "cs.CR",
            "C.3; F.2.2"
        ]
    },
    {
        "title": "A cache-friendly truncated FFT",
        "authors": [
            "David Harvey"
        ],
        "summary": "We describe a cache-friendly version of van der Hoeven's truncated FFT and inverse truncated FFT, focusing on the case of `large' coefficients, such as those arising in the Schonhage--Strassen algorithm for multiplication in Z[x]. We describe two implementations and examine their performance.",
        "published": "2008-10-17T17:36:27Z",
        "link": "http://arxiv.org/abs/0810.3203v1",
        "categories": [
            "cs.SC",
            "cs.DS"
        ]
    },
    {
        "title": "Dynamic Approaches to In-Network Aggregation",
        "authors": [
            "Oliver Kennedy",
            "Christoph Koch",
            "Al Demers"
        ],
        "summary": "Collaboration between small-scale wireless devices hinges on their ability to infer properties shared across multiple nearby nodes. Wireless-enabled mobile devices in particular create a highly dynamic environment not conducive to distributed reasoning about such global properties. This paper addresses a specific instance of this problem: distributed aggregation. We present extensions to existing unstructured aggregation protocols that enable estimation of count, sum, and average aggregates in highly dynamic environments. With the modified protocols, devices with only limited connectivity can maintain estimates of the aggregate, despite \\textit{unexpected} peer departures and arrivals. Our analysis of these aggregate maintenance extensions demonstrates their effectiveness in unstructured environments despite high levels of node mobility.",
        "published": "2008-10-17T19:48:38Z",
        "link": "http://arxiv.org/abs/0810.3227v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Efficient Algorithms and Routing Protocols for Handling Transient Single   Node Failures",
        "authors": [
            "Amit M Bhosle",
            "Teofilo F Gonzalez"
        ],
        "summary": "Single node failures represent more than 85% of all node failures in the today's large communication networks such as the Internet. Also, these node failures are usually transient. Consequently, having the routing paths globally recomputed does not pay off since the failed nodes recover fairly quickly, and the recomputed routing paths need to be discarded. Instead, we develop algorithms and protocols for dealing with such transient single node failures by suppressing the failure (instead of advertising it across the network), and routing messages to the destination via alternate paths that do not use the failed node. We compare our solution to that of Ref. [11] wherein the authors have presented a \"Failure Insensitive Routing\" protocol as a proactive recovery scheme for handling transient node failures. We show that our algorithms are faster by an order of magnitude while our paths are equally good. We show via simulation results that our paths are usually within 15% of the optimal for randomly generated graph with 100-1000 nodes.",
        "published": "2008-10-19T22:57:53Z",
        "link": "http://arxiv.org/abs/0810.3438v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A new distance for high level RNA secondary structure comparison",
        "authors": [
            "Julien Allali",
            "Marie-France Sagot"
        ],
        "summary": "We describe an algorithm for comparing two RNA secondary structures coded in the form of trees that introduces two new operations, called node fusion and edge fusion, besides the tree edit operations of deletion, insertion, and relabeling classically used in the literature. This allows us to address some serious limitations of the more traditional tree edit operations when the trees represent RNAs and what is searched for is a common structural core of two RNAs. Although the algorithm complexity has an exponential term, this term depends only on the number of successive fusions that may be applied to a same node, not on the total number of fusions. The algorithm remains therefore efficient in practice and is used for illustrative purposes on ribosomal as well as on other types of RNAs.",
        "published": "2008-10-22T08:24:53Z",
        "link": "http://arxiv.org/abs/0810.4002v1",
        "categories": [
            "cs.DS",
            "q-bio.QM"
        ]
    },
    {
        "title": "Locally computable approximations for spectral clustering and absorption   times of random walks",
        "authors": [
            "Pekka Orponen",
            "Satu Elisa Schaeffer",
            "Vanesa Avalos Gaytán"
        ],
        "summary": "We address the problem of determining a natural local neighbourhood or \"cluster\" associated to a given seed vertex in an undirected graph. We formulate the task in terms of absorption times of random walks from other vertices to the vertex of interest, and observe that these times are well approximated by the components of the principal eigenvector of the corresponding fundamental matrix of the graph's adjacency matrix. We further present a locally computable gradient-descent method to estimate this Dirichlet-Fiedler vector, based on minimising the respective Rayleigh quotient. Experimental evaluation shows that the approximations behave well and yield well-defined local clusters.",
        "published": "2008-10-22T17:48:12Z",
        "link": "http://arxiv.org/abs/0810.4061v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "G.2.2; G.3; H.3.3"
        ]
    },
    {
        "title": "Efficient Algorithmic Techniques for Several Multidimensional Geometric   Data Management and Analysis Problems",
        "authors": [
            "Mugurel Ionut Andreica"
        ],
        "summary": "In this paper I present several novel, efficient, algorithmic techniques for solving some multidimensional geometric data management and analysis problems. The techniques are based on several data structures from computational geometry (e.g. segment tree and range tree) and on the well-known sweep-line method.",
        "published": "2008-10-24T09:44:03Z",
        "link": "http://arxiv.org/abs/0810.4423v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Kernel(s) for Problems With no Kernel: On Out-Trees With Many Leaves",
        "authors": [
            "Henning Fernau",
            "Fedor V. Fomin",
            "Daniel Lokshtanov",
            "Daniel Raible",
            "Saket Saurabh",
            "Yngve Villanger"
        ],
        "summary": "The {\\sc $k$-Leaf Out-Branching} problem is to find an out-branching (i.e. a rooted oriented spanning tree) with at least $k$ leaves in a given digraph. The problem has recently received much attention from the viewpoint of parameterized algorithms {alonLNCS4596,AlonFGKS07fsttcs,BoDo2,KnLaRo}. In this paper we step aside and take a kernelization based approach to the {\\sc $k$-Leaf-Out-Branching} problem. We give the first polynomial kernel for {\\sc Rooted $k$-Leaf-Out-Branching}, a variant of {\\sc $k$-Leaf-Out-Branching} where the root of the tree searched for is also a part of the input. Our kernel has cubic size and is obtained using extremal combinatorics.   For the {\\sc $k$-Leaf-Out-Branching} problem we show that no polynomial kernel is possible unless polynomial hierarchy collapses to third level %$PH=\\Sigma_p^3$ by applying a recent breakthrough result by Bodlaender et al. {BDFH08} in a non-trivial fashion. However our positive results for {\\sc Rooted $k$-Leaf-Out-Branching} immediately imply that the seemingly intractable the {\\sc $k$-Leaf-Out-Branching} problem admits a data reduction to $n$ independent $O(k^3)$ kernels. These two results, tractability and intractability side by side, are the first separating {\\it many-to-one kernelization} from {\\it Turing kernelization}. This answers affirmatively an open problem regarding \"cheat kernelization\" raised in {IWPECOPEN08}.",
        "published": "2008-10-27T11:53:47Z",
        "link": "http://arxiv.org/abs/0810.4796v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "A constructive proof of the Lovasz Local Lemma",
        "authors": [
            "Robin A. Moser"
        ],
        "summary": "The Lovasz Local Lemma [EL75] is a powerful tool to prove the existence of combinatorial objects meeting a prescribed collection of criteria. The technique can directly be applied to the satisfiability problem, yielding that a k-CNF formula in which each clause has common variables with at most 2^(k-2) other clauses is always satisfiable. All hitherto known proofs of the Local Lemma are non-constructive and do thus not provide a recipe as to how a satisfying assignment to such a formula can be efficiently found. In his breakthrough paper [Bec91], Beck demonstrated that if the neighbourhood of each clause be restricted to O(2^(k/48)), a polynomial time algorithm for the search problem exists. Alon simplified and randomized his procedure and improved the bound to O(2^(k/8)) [Alo91]. Srinivasan presented in [Sri08] a variant that achieves a bound of essentially O(2^(k/4)). In [Mos08], we improved this to O(2^(k/2)). In the present paper, we give a randomized algorithm that finds a satisfying assignment to every k-CNF formula in which each clause has a neighbourhood of at most the asymptotic optimum of 2^(k-5)-1 other clauses and that runs in expected time polynomial in the size of the formula, irrespective of k. If k is considered a constant, we can also give a deterministic variant. In contrast to all previous approaches, our analysis does not anymore invoke the standard non-constructive versions of the Local Lemma and can therefore be considered an alternative, constructive proof of it.",
        "published": "2008-10-27T14:02:48Z",
        "link": "http://arxiv.org/abs/0810.4812v2",
        "categories": [
            "cs.DS",
            "F.2; G.2"
        ]
    },
    {
        "title": "Exponential-Time Approximation of Hard Problems",
        "authors": [
            "Marek Cygan",
            "Lukasz Kowalik",
            "Marcin Pilipczuk",
            "Mateusz Wykurz"
        ],
        "summary": "We study optimization problems that are neither approximable in polynomial time (at least with a constant factor) nor fixed parameter tractable, under widely believed complexity assumptions. Specifically, we focus on Maximum Independent Set, Vertex Coloring, Set Cover, and Bandwidth.   In recent years, many researchers design exact exponential-time algorithms for these and other hard problems. The goal is getting the time complexity still of order $O(c^n)$, but with the constant $c$ as small as possible. In this work we extend this line of research and we investigate whether the constant $c$ can be made even smaller when one allows constant factor approximation. In fact, we describe a kind of approximation schemes -- trade-offs between approximation factor and the time complexity.   We study two natural approaches. The first approach consists of designing a backtracking algorithm with a small search tree. We present one result of that kind: a $(4r-1)$-approximation of Bandwidth in time $O^*(2^{n/r})$, for any positive integer $r$.   The second approach uses general transformations from exponential-time exact algorithms to approximations that are faster but still exponential-time. For example, we show that for any reduction rate $r$, one can transform any $O^*(c^n)$-time algorithm for Set Cover into a $(1+\\ln r)$-approximation algorithm running in time $O^*(c^{n/r})$. We believe that results of that kind extend the applicability of exact algorithms for NP-hard problems.",
        "published": "2008-10-27T20:18:00Z",
        "link": "http://arxiv.org/abs/0810.4934v1",
        "categories": [
            "cs.DS",
            "F.2.2; F.2.3"
        ]
    },
    {
        "title": "FPT Algorithms and Kernels for the Directed $k$-Leaf Problem",
        "authors": [
            "Jean Daligault",
            "Gregory Gutin",
            "Eun Jung Kim",
            "Anders Yeo"
        ],
        "summary": "A subgraph $T$ of a digraph $D$ is an {\\em out-branching} if $T$ is an oriented spanning tree with only one vertex of in-degree zero (called the {\\em root}). The vertices of $T$ of out-degree zero are {\\em leaves}. In the {\\sc Directed $k$-Leaf} Problem, we are given a digraph $D$ and an integral parameter $k$, and we are to decide whether $D$ has an out-branching with at least $k$ leaves. Recently, Kneis et al. (2008) obtained an algorithm for the problem of running time $4^{k}\\cdot n^{O(1)}$. We describe a new algorithm for the problem of running time $3.72^{k}\\cdot n^{O(1)}$. In {\\sc Rooted Directed $k$-Leaf} Problem, apart from $D$ and $k$, we are given a vertex $r$ of $D$ and we are to decide whether $D$ has an out-branching rooted at $r$ with at least $k$ leaves. Very recently, Fernau et al. (2008) found an $O(k^3)$-size kernel for {\\sc Rooted Directed $k$-Leaf}. In this paper, we obtain an $O(k)$ kernel for {\\sc Rooted Directed $k$-Leaf} restricted to acyclic digraphs.",
        "published": "2008-10-27T21:44:42Z",
        "link": "http://arxiv.org/abs/0810.4946v3",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "A New Algorithm for Building Alphabetic Minimax Trees",
        "authors": [
            "Travis Gagie"
        ],
        "summary": "We show how to build an alphabetic minimax tree for a sequence (W = w_1, >..., w_n) of real weights in (O (n d \\log \\log n)) time, where $d$ is the number of distinct integers (\\lceil w_i \\rceil). We apply this algorithm to building an alphabetic prefix code given a sample.",
        "published": "2008-10-28T15:59:55Z",
        "link": "http://arxiv.org/abs/0810.5064v1",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT"
        ]
    },
    {
        "title": "Lower bounds for distributed markov chain problems",
        "authors": [
            "Rahul Sami",
            "Andy Twigg"
        ],
        "summary": "We study the worst-case communication complexity of distributed algorithms computing a path problem based on stationary distributions of random walks in a network $G$ with the caveat that $G$ is also the communication network. The problem is a natural generalization of shortest path lengths to expected path lengths, and represents a model used in many practical applications such as pagerank and eigentrust as well as other problems involving Markov chains defined by networks.   For the problem of computing a single stationary probability, we prove an $\\Omega(n^2 \\log n)$ bits lower bound; the trivial centralized algorithm costs $O(n^3)$ bits and no known algorithm beats this. We also prove lower bounds for the related problems of approximately computing the stationary probabilities, computing only the ranking of the nodes, and computing the node with maximal rank. As a corollary, we obtain lower bounds for labelling schemes for the hitting time between two nodes.",
        "published": "2008-10-29T12:52:59Z",
        "link": "http://arxiv.org/abs/0810.5263v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Relating Web pages to enable information-gathering tasks",
        "authors": [
            "Amitabha Bagchi",
            "Garima Lahoti"
        ],
        "summary": "We argue that relationships between Web pages are functions of the user's intent. We identify a class of Web tasks - information-gathering - that can be facilitated by a search engine that provides links to pages which are related to the page the user is currently viewing. We define three kinds of intentional relationships that correspond to whether the user is a) seeking sources of information, b) reading pages which provide information, or c) surfing through pages as part of an extended information-gathering process. We show that these three relationships can be productively mined using a combination of textual and link information and provide three scoring mechanisms that correspond to them: {\\em SeekRel}, {\\em FactRel} and {\\em SurfRel}. These scoring mechanisms incorporate both textual and link information. We build a set of capacitated subnetworks - each corresponding to a particular keyword - that mirror the interconnection structure of the World Wide Web. The scores are computed by computing flows on these subnetworks. The capacities of the links are derived from the {\\em hub} and {\\em authority} values of the nodes they connect, following the work of Kleinberg (1998) on assigning authority to pages in hyperlinked environments. We evaluated our scoring mechanism by running experiments on four data sets taken from the Web. We present user evaluations of the relevance of the top results returned by our scoring mechanisms and compare those to the top results returned by Google's Similar Pages feature, and the {\\em Companion} algorithm proposed by Dean and Henzinger (1999).",
        "published": "2008-10-30T07:17:49Z",
        "link": "http://arxiv.org/abs/0810.5428v2",
        "categories": [
            "cs.IR",
            "cs.DS",
            "H.3.3"
        ]
    },
    {
        "title": "Worst-case time decremental connectivity and k-edge witness",
        "authors": [
            "Andrew Twigg"
        ],
        "summary": "We give a simple algorithm for decremental graph connectivity that handles edge deletions in worst-case time $O(k \\log n)$ and connectivity queries in $O(\\log k)$, where $k$ is the number of edges deleted so far, and uses worst-case space $O(m^2)$. We use this to give an algorithm for $k$-edge witness (``does the removal of a given set of $k$ edges disconnect two vertices $u,v$?'') with worst-case time $O(k^2 \\log n)$ and space $O(k^2 n^2)$. For $k = o(\\sqrt{n})$ these improve the worst-case $O(\\sqrt{n})$ bound for deletion due to Eppstein et al. We also give a decremental connectivity algorithm using $O(n^2 \\log n / \\log \\log n)$ space, whose time complexity depends on the toughness and independence number of the input graph. Finally, we show how to construct a distributed data structure for \\kvw by giving a labeling scheme. This is the first data structure for \\kvw that can efficiently distributed without just giving each vertex a copy of the whole structure. Its complexity depends on being able to construct a linear layout with good properties.",
        "published": "2008-10-30T12:15:33Z",
        "link": "http://arxiv.org/abs/0810.5477v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A branch-and-bound feature selection algorithm for U-shaped cost   functions",
        "authors": [
            "Marcelo Ris",
            "Junior Barrera",
            "David C. Martins Jr"
        ],
        "summary": "This paper presents the formulation of a combinatorial optimization problem with the following characteristics: i.the search space is the power set of a finite set structured as a Boolean lattice; ii.the cost function forms a U-shaped curve when applied to any lattice chain. This formulation applies for feature selection in the context of pattern recognition. The known approaches for this problem are branch-and-bound algorithms and heuristics, that explore partially the search space. Branch-and-bound algorithms are equivalent to the full search, while heuristics are not. This paper presents a branch-and-bound algorithm that differs from the others known by exploring the lattice structure and the U-shaped chain curves of the search space. The main contribution of this paper is the architecture of this algorithm that is based on the representation and exploration of the search space by new lattice properties proven here. Several experiments, with well known public data, indicate the superiority of the proposed method to SFFS, which is a popular heuristic that gives good results in very short computational time. In all experiments, the proposed method got better or equal results in similar or even smaller computational time.",
        "published": "2008-10-30T20:24:28Z",
        "link": "http://arxiv.org/abs/0810.5573v1",
        "categories": [
            "cs.CV",
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Anonymizing Graphs",
        "authors": [
            "Tomas Feder",
            "Shubha U. Nabar",
            "Evimaria Terzi"
        ],
        "summary": "Motivated by recently discovered privacy attacks on social networks, we study the problem of anonymizing the underlying graph of interactions in a social network. We call a graph (k,l)-anonymous if for every node in the graph there exist at least k other nodes that share at least l of its neighbors. We consider two combinatorial problems arising from this notion of anonymity in graphs. More specifically, given an input graph we ask for the minimum number of edges to be added so that the graph becomes (k,l)-anonymous. We define two variants of this minimization problem and study their properties. We show that for certain values of k and l the problems are polynomial-time solvable, while for others they become NP-hard. Approximation algorithms for the latter cases are also given.",
        "published": "2008-10-30T21:12:25Z",
        "link": "http://arxiv.org/abs/0810.5578v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Interpolation of Shifted-Lacunary Polynomials",
        "authors": [
            "Mark Giesbrecht",
            "Daniel S. Roche"
        ],
        "summary": "Given a \"black box\" function to evaluate an unknown rational polynomial f in Q[x] at points modulo a prime p, we exhibit algorithms to compute the representation of the polynomial in the sparsest shifted power basis. That is, we determine the sparsity t, the shift s (a rational), the exponents 0 <= e1 < e2 < ... < et, and the coefficients c1,...,ct in Q\\{0} such that f(x) = c1(x-s)^e1+c2(x-s)^e2+...+ct(x-s)^et. The computed sparsity t is absolutely minimal over any shifted power basis. The novelty of our algorithm is that the complexity is polynomial in the (sparse) representation size, and in particular is logarithmic in deg(f). Our method combines previous celebrated results on sparse interpolation and computing sparsest shifts, and provides a way to handle polynomials with extremely high degree which are, in some sense, sparse in information.",
        "published": "2008-10-31T13:35:08Z",
        "link": "http://arxiv.org/abs/0810.5685v6",
        "categories": [
            "cs.SC",
            "cs.DS",
            "cs.MS",
            "68W30 (Primary), 12Y05 (Secondary)"
        ]
    },
    {
        "title": "Anonymizing Unstructured Data",
        "authors": [
            "Rajeev Motwani",
            "Shubha U. Nabar"
        ],
        "summary": "In this paper we consider the problem of anonymizing datasets in which each individual is associated with a set of items that constitute private information about the individual. Illustrative datasets include market-basket datasets and search engine query logs. We formalize the notion of k-anonymity for set-valued data as a variant of the k-anonymity model for traditional relational datasets. We define an optimization problem that arises from this definition of anonymity and provide O(klogk) and O(1)-approximation algorithms for the same. We demonstrate applicability of our algorithms to the America Online query log dataset.",
        "published": "2008-10-31T19:25:02Z",
        "link": "http://arxiv.org/abs/0810.5582v2",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Characterizing Graphs of Zonohedra",
        "authors": [
            "Muhammad Abdullah Adnan",
            "Masud Hasan"
        ],
        "summary": "A classic theorem by Steinitz states that a graph G is realizable by a convex polyhedron if and only if G is 3-connected planar. Zonohedra are an important subclass of convex polyhedra having the property that the faces of a zonohedron are parallelograms and are in parallel pairs. In this paper we give characterization of graphs of zonohedra. We also give a linear time algorithm to recognize such a graph. In our quest for finding the algorithm, we prove that in a zonohedron P both the number of zones and the number of faces in each zone is O(square root{n}), where n is the number of vertices of P.",
        "published": "2008-11-03T10:19:10Z",
        "link": "http://arxiv.org/abs/0811.0254v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "When are two algorithms the same?",
        "authors": [
            "Andreas Blass",
            "Nachum Dershowitz",
            "Yuri Gurevich"
        ],
        "summary": "People usually regard algorithms as more abstract than the programs that implement them. The natural way to formalize this idea is that algorithms are equivalence classes of programs with respect to a suitable equivalence relation. We argue that no such equivalence relation exists.",
        "published": "2008-11-05T20:38:22Z",
        "link": "http://arxiv.org/abs/0811.0811v1",
        "categories": [
            "cs.GL",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "A role-free approach to indexing large RDF data sets in secondary memory   for efficient SPARQL evaluation",
        "authors": [
            "George H. L. Fletcher",
            "Peter W. Beck"
        ],
        "summary": "Massive RDF data sets are becoming commonplace. RDF data is typically generated in social semantic domains (such as personal information management) wherein a fixed schema is often not available a priori. We propose a simple Three-way Triple Tree (TripleT) secondary-memory indexing technique to facilitate efficient SPARQL query evaluation on such data sets. The novelty of TripleT is that (1) the index is built over the atoms occurring in the data set, rather than at a coarser granularity, such as whole triples occurring in the data set; and (2) the atoms are indexed regardless of the roles (i.e., subjects, predicates, or objects) they play in the triples of the data set. We show through extensive empirical evaluation that TripleT exhibits multiple orders of magnitude improvement over the state of the art on RDF indexing, in terms of both storage and query processing costs.",
        "published": "2008-11-07T05:08:41Z",
        "link": "http://arxiv.org/abs/0811.1083v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "NB-FEB: An Easy-to-Use and Scalable Universal Synchronization Primitive   for Parallel Programming",
        "authors": [
            "Phuong Hoai Ha",
            "Philippas Tsigas",
            "Otto J. Anshus"
        ],
        "summary": "This paper addresses the problem of universal synchronization primitives that can support scalable thread synchronization for large-scale many-core architectures. The universal synchronization primitives that have been deployed widely in conventional architectures like CAS and LL/SC are expected to reach their scalability limits in the evolution to many-core architectures with thousands of cores. We introduce a non-blocking full/empty bit primitive, or NB-FEB for short, as a promising synchronization primitive for parallel programming on may-core architectures. We show that the NB-FEB primitive is universal, scalable, feasible and convenient to use. NB-FEB, together with registers, can solve the consensus problem for an arbitrary number of processes (universality). NB-FEB is combinable, namely its memory requests to the same memory location can be combined into only one memory request, which consequently mitigates performance degradation due to synchronization \"hot spots\" (scalability). Since NB-FEB is a variant of the original full/empty bit that always returns a value instead of waiting for a conditional flag, it is as feasible as the original full/empty bit, which has been implemented in many computer systems (feasibility). The original full/empty bit is well-known as a special-purpose primitive for fast producer-consumer synchronization and has been used extensively in the specific domain of applications. In this paper, we show that NB-FEB can be deployed easily as a general-purpose primitive. Using NB-FEB, we construct a non-blocking software transactional memory system called NBFEB-STM, which can be used to handle concurrent threads conveniently. NBFEB-STM is space efficient: the space complexity of each object updated by $N$ concurrent threads/transactions is $\\Theta(N)$, the optimal.",
        "published": "2008-11-09T00:41:07Z",
        "link": "http://arxiv.org/abs/0811.1304v1",
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.DS"
        ]
    },
    {
        "title": "Applying Practice to Theory",
        "authors": [
            "Ryan Williams"
        ],
        "summary": "How can complexity theory and algorithms benefit from practical advances in computing? We give a short overview of some prior work using practical computing to attack problems in computational complexity and algorithms, informally describe how linear program solvers may be used to help prove new lower bounds for satisfiability, and suggest a research program for developing new understanding in circuit complexity.",
        "published": "2008-11-09T00:49:41Z",
        "link": "http://arxiv.org/abs/0811.1305v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Distributed Algorithms for Computing Alternate Paths Avoiding Failed   Nodes and Links",
        "authors": [
            "Amit M. Bhosle",
            "Teofilo F. Gonzalez"
        ],
        "summary": "A recent study characterizing failures in computer networks shows that transient single element (node/link) failures are the dominant failures in large communication networks like the Internet. Thus, having the routing paths globally recomputed on a failure does not pay off since the failed element recovers fairly quickly, and the recomputed routing paths need to be discarded. In this paper, we present the first distributed algorithm that computes the alternate paths required by some \"proactive recovery schemes\" for handling transient failures. Our algorithm computes paths that avoid a failed node, and provides an alternate path to a particular destination from an upstream neighbor of the failed node. With minor modifications, we can have the algorithm compute alternate paths that avoid a failed link as well. To the best of our knowledge all previous algorithms proposed for computing alternate paths are centralized, and need complete information of the network graph as input to the algorithm.",
        "published": "2008-11-09T03:34:39Z",
        "link": "http://arxiv.org/abs/0811.1301v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.NI"
        ]
    },
    {
        "title": "Algorithmic Techniques for Several Optimization Problems Regarding   Distributed Systems with Tree Topologies",
        "authors": [
            "Mugurel Ionut Andreica"
        ],
        "summary": "As the development of distributed systems progresses, more and more challenges arise and the need for developing optimized systems and for optimizing existing systems from multiple perspectives becomes more stringent. In this paper I present novel algorithmic techniques for solving several optimization problems regarding distributed systems with tree topologies. I address topics like: reliability improvement, partitioning, coloring, content delivery, optimal matchings, as well as some tree counting aspects. Some of the presented techniques are only of theoretical interest, while others can be used in practical settings.",
        "published": "2008-11-09T12:59:45Z",
        "link": "http://arxiv.org/abs/0811.1335v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "cs.NI",
            "G.2.2; G.2.1"
        ]
    },
    {
        "title": "Exact Exponential Time Algorithms for Max Internal Spanning Tree",
        "authors": [
            "Henning Fernau",
            "Serge Gaspers",
            "Daniel Raible"
        ],
        "summary": "We consider the NP-hard problem of finding a spanning tree with a maximum number of internal vertices. This problem is a generalization of the famous   Hamiltonian Path problem. Our dynamic-programming algorithms for general and degree-bounded graphs have running times of the form O*(c^n) (c <= 3). The main result, however, is a branching algorithm for graphs with maximum degree three. It only needs polynomial space and has a running time of O*(1.8669^n) when analyzed with respect to the number of vertices. We also show that its running time is 2.1364^k n^O(1) when the goal is to find a spanning tree with at least k internal vertices. Both running time bounds are obtained via a Measure & Conquer analysis, the latter one being a novel use of this kind of analyses for parameterized algorithms.",
        "published": "2008-11-12T12:09:08Z",
        "link": "http://arxiv.org/abs/0811.1875v3",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Perfect Matchings via Uniform Sampling in Regular Bipartite Graphs",
        "authors": [
            "Ashish Goel",
            "Michael Kapralov",
            "Sanjeev Khanna"
        ],
        "summary": "In this paper we further investigate the well-studied problem of finding a perfect matching in a regular bipartite graph. The first non-trivial algorithm, with running time $O(mn)$, dates back to K\\\"{o}nig's work in 1916 (here $m=nd$ is the number of edges in the graph, $2n$ is the number of vertices, and $d$ is the degree of each node). The currently most efficient algorithm takes time $O(m)$, and is due to Cole, Ost, and Schirra. We improve this running time to $O(\\min\\{m, \\frac{n^{2.5}\\ln n}{d}\\})$; this minimum can never be larger than $O(n^{1.75}\\sqrt{\\ln n})$. We obtain this improvement by proving a uniform sampling theorem: if we sample each edge in a $d$-regular bipartite graph independently with a probability $p = O(\\frac{n\\ln n}{d^2})$ then the resulting graph has a perfect matching with high probability. The proof involves a decomposition of the graph into pieces which are guaranteed to have many perfect matchings but do not have any small cuts. We then establish a correspondence between potential witnesses to non-existence of a matching (after sampling) in any piece and cuts of comparable size in that same piece. Karger's sampling theorem for preserving cuts in a graph can now be adapted to prove our uniform sampling theorem for preserving perfect matchings. Using the $O(m\\sqrt{n})$ algorithm (due to Hopcroft and Karp) for finding maximum matchings in bipartite graphs on the sampled graph then yields the stated running time. We also provide an infinite family of instances to show that our uniform sampling result is tight up to poly-logarithmic factors (in fact, up to $\\ln^2 n$).",
        "published": "2008-11-15T05:49:17Z",
        "link": "http://arxiv.org/abs/0811.2457v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Computing voting power in easy weighted voting games",
        "authors": [
            "Haris Aziz",
            "Mike Paterson"
        ],
        "summary": "Weighted voting games are ubiquitous mathematical models which are used in economics, political science, neuroscience, threshold logic, reliability theory and distributed systems. They model situations where agents with variable voting weight vote in favour of or against a decision. A coalition of agents is winning if and only if the sum of weights of the coalition exceeds or equals a specified quota. The Banzhaf index is a measure of voting power of an agent in a weighted voting game. It depends on the number of coalitions in which the agent is the difference in the coalition winning or losing. It is well known that computing Banzhaf indices in a weighted voting game is NP-hard. We give a comprehensive classification of weighted voting games which can be solved in polynomial time. Among other results, we provide a polynomial ($O(k{(\\frac{n}{k})}^k)$) algorithm to compute the Banzhaf indices in weighted voting games in which the number of weight values is bounded by $k$.",
        "published": "2008-11-15T14:55:51Z",
        "link": "http://arxiv.org/abs/0811.2497v2",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Phase transition for Local Search on planted SAT",
        "authors": [
            "Andrei A. Bulatov",
            "Evgeny S. Skvortsov"
        ],
        "summary": "The Local Search algorithm (or Hill Climbing, or Iterative Improvement) is one of the simplest heuristics to solve the Satisfiability and Max-Satisfiability problems. It is a part of many satisfiability and max-satisfiability solvers, where it is used to find a good starting point for a more sophisticated heuristics, and to improve a candidate solution. In this paper we give an analysis of Local Search on random planted 3-CNF formulas. We show that if there is k<7/6 such that the clause-to-variable ratio is less than k ln(n) (n is the number of variables in a CNF) then Local Search whp does not find a satisfying assignment, and if there is k>7/6 such that the clause-to-variable ratio is greater than k ln(n)$ then the local search whp finds a satisfying assignment. As a byproduct we also show that for any constant r there is g such that Local Search applied to a random (not necessarily planted) 3-CNF with clause-to-variable ratio r produces an assignment that satisfies at least gn clauses less than the maximal number of satisfiable clauses.",
        "published": "2008-11-16T01:41:15Z",
        "link": "http://arxiv.org/abs/0811.2546v1",
        "categories": [
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "An Efficient Algorithm for Partial Order Production",
        "authors": [
            "Jean Cardinal",
            "Samuel Fiorini",
            "Gwenaël Joret",
            "Raphaël M. Jungers",
            "J. Ian Munro"
        ],
        "summary": "We consider the problem of partial order production: arrange the elements of an unknown totally ordered set T into a target partially ordered set S, by comparing a minimum number of pairs in T. Special cases include sorting by comparisons, selection, multiple selection, and heap construction.   We give an algorithm performing ITLB + o(ITLB) + O(n) comparisons in the worst case. Here, n denotes the size of the ground sets, and ITLB denotes a natural information-theoretic lower bound on the number of comparisons needed to produce the target partial order.   Our approach is to replace the target partial order by a weak order (that is, a partial order with a layered structure) extending it, without increasing the information theoretic lower bound too much. We then solve the problem by applying an efficient multiple selection algorithm. The overall complexity of our algorithm is polynomial. This answers a question of Yao (SIAM J. Comput. 18, 1989).   We base our analysis on the entropy of the target partial order, a quantity that can be efficiently computed and provides a good estimate of the information-theoretic lower bound.",
        "published": "2008-11-17T16:23:45Z",
        "link": "http://arxiv.org/abs/0811.2572v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Generating Random Networks Without Short Cycles",
        "authors": [
            "Mohsen Bayati",
            "Andrea Montanari",
            "Amin Saberi"
        ],
        "summary": "Random graph generation is an important tool for studying large complex networks. Despite abundance of random graph models, constructing models with application-driven constraints is poorly understood. In order to advance state-of-the-art in this area, we focus on random graphs without short cycles as a stylized family of graphs, and propose the RandGraph algorithm for randomly generating them. For any constant k, when m=O(n^{1+1/[2k(k+3)]}), RandGraph generates an asymptotically uniform random graph with n vertices, m edges, and no cycle of length at most k using O(n^2m) operations. We also characterize the approximation error for finite values of n. To the best of our knowledge, this is the first polynomial-time algorithm for the problem. RandGraph works by sequentially adding $m$ edges to an empty graph with n vertices. Recently, such sequential algorithms have been successful for random sampling problems. Our main contributions to this line of research includes introducing a new approach for sequentially approximating edge-specific probabilities at each step of the algorithm, and providing a new method for analyzing such algorithms.",
        "published": "2008-11-18T08:05:26Z",
        "link": "http://arxiv.org/abs/0811.2853v2",
        "categories": [
            "cs.DS",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Secondary Indexing in One Dimension: Beyond B-trees and Bitmap Indexes",
        "authors": [
            "Rasmus Pagh",
            "S. Srinivasa Rao"
        ],
        "summary": "Let S be a finite, ordered alphabet, and let x = x_1 x_2 ... x_n be a string over S. A \"secondary index\" for x answers alphabet range queries of the form: Given a range [a_l,a_r] over S, return the set I_{[a_l;a_r]} = {i |x_i \\in [a_l; a_r]}. Secondary indexes are heavily used in relational databases and scientific data analysis. It is well-known that the obvious solution, storing a dictionary for the position set associated with each character, does not always give optimal query time. In this paper we give the first theoretically optimal data structure for the secondary indexing problem. In the I/O model, the amount of data read when answering a query is within a constant factor of the minimum space needed to represent I_{[a_l;a_r]}, assuming that the size of internal memory is (|S| log n)^{delta} blocks, for some constant delta > 0. The space usage of the data structure is O(n log |S|) bits in the worst case, and we further show how to bound the size of the data structure in terms of the 0-th order entropy of x. We show how to support updates achieving various time-space trade-offs.   We also consider an approximate version of the basic secondary indexing problem where a query reports a superset of I_{[a_l;a_r]} containing each element not in I_{[a_l;a_r]} with probability at most epsilon, where epsilon > 0 is the false positive probability. For this problem the amount of data that needs to be read by the query algorithm is reduced to O(|I_{[a_l;a_r]}| log(1/epsilon)) bits.",
        "published": "2008-11-18T13:31:05Z",
        "link": "http://arxiv.org/abs/0811.2904v1",
        "categories": [
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Exact phase transition of backtrack-free search with implications on the   power of greedy algorithms",
        "authors": [
            "Liang Li",
            "Tian Liu",
            "Ke Xu"
        ],
        "summary": "Backtracking is a basic strategy to solve constraint satisfaction problems (CSPs). A satisfiable CSP instance is backtrack-free if a solution can be found without encountering any dead-end during a backtracking search, implying that the instance is easy to solve. We prove an exact phase transition of backtrack-free search in some random CSPs, namely in Model RB and in Model RD. This is the first time an exact phase transition of backtrack-free search can be identified on some random CSPs. Our technical results also have interesting implications on the power of greedy algorithms, on the width of random hypergraphs and on the exact satisfiability threshold of random CSPs.",
        "published": "2008-11-19T06:33:39Z",
        "link": "http://arxiv.org/abs/0811.3055v1",
        "categories": [
            "cs.AI",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Dynamic External Hashing: The Limit of Buffering",
        "authors": [
            "Zhewei Wei",
            "Ke Yi",
            "Qin Zhang"
        ],
        "summary": "Hash tables are one of the most fundamental data structures in computer science, in both theory and practice. They are especially useful in external memory, where their query performance approaches the ideal cost of just one disk access. Knuth gave an elegant analysis showing that with some simple collision resolution strategies such as linear probing or chaining, the expected average number of disk I/Os of a lookup is merely $1+1/2^{\\Omega(b)}$, where each I/O can read a disk block containing $b$ items. Inserting a new item into the hash table also costs $1+1/2^{\\Omega(b)}$ I/Os, which is again almost the best one can do if the hash table is entirely stored on disk. However, this assumption is unrealistic since any algorithm operating on an external hash table must have some internal memory (at least $\\Omega(1)$ blocks) to work with. The availability of a small internal memory buffer can dramatically reduce the amortized insertion cost to $o(1)$ I/Os for many external memory data structures. In this paper we study the inherent query-insertion tradeoff of external hash tables in the presence of a memory buffer. In particular, we show that for any constant $c>1$, if the query cost is targeted at $1+O(1/b^{c})$ I/Os, then it is not possible to support insertions in less than $1-O(1/b^{\\frac{c-1}{4}})$ I/Os amortized, which means that the memory buffer is essentially useless. While if the query cost is relaxed to $1+O(1/b^{c})$ I/Os for any constant $c<1$, there is a simple dynamic hash table with $o(1)$ insertion cost. These results also answer the open question recently posed by Jensen and Pagh.",
        "published": "2008-11-19T08:11:14Z",
        "link": "http://arxiv.org/abs/0811.3062v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "An experimental analysis of Lemke-Howson algorithm",
        "authors": [
            "Bruno Codenotti",
            "Stefano De Rossi",
            "Marino Pagan"
        ],
        "summary": "We present an experimental investigation of the performance of the Lemke-Howson algorithm, which is the most widely used algorithm for the computation of a Nash equilibrium for bimatrix games. Lemke-Howson algorithm is based upon a simple pivoting strategy, which corresponds to following a path whose endpoint is a Nash equilibrium. We analyze both the basic Lemke-Howson algorithm and a heuristic modification of it, which we designed to cope with the effects of a 'bad' initial choice of the pivot. Our experimental findings show that, on uniformly random games, the heuristics achieves a linear running time, while the basic Lemke-Howson algorithm runs in time roughly proportional to a polynomial of degree seven. To conduct the experiments, we have developed our own implementation of Lemke-Howson algorithm, which turns out to be significantly faster than state-of-the-art software. This allowed us to run the algorithm on a much larger set of data, and on instances of much larger size, compared with previous work.",
        "published": "2008-11-20T00:32:16Z",
        "link": "http://arxiv.org/abs/0811.3247v1",
        "categories": [
            "cs.DS",
            "cs.NA"
        ]
    },
    {
        "title": "Linear Time Approximation Schemes for the Gale-Berlekamp Game and   Related Minimization Problems",
        "authors": [
            "Marek Karpinski",
            "Warren Schudy"
        ],
        "summary": "We design a linear time approximation scheme for the Gale-Berlekamp Switching Game and generalize it to a wider class of dense fragile minimization problems including the Nearest Codeword Problem (NCP) and Unique Games Problem. Further applications include, among other things, finding a constrained form of matrix rigidity and maximum likelihood decoding of an error correcting code. As another application of our method we give the first linear time approximation schemes for correlation clustering with a fixed number of clusters and its hierarchical generalization. Our results depend on a new technique for dealing with small objective function values of optimization problems and could be of independent interest.",
        "published": "2008-11-20T01:07:49Z",
        "link": "http://arxiv.org/abs/0811.3244v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.1; G.2.2; G.3"
        ]
    },
    {
        "title": "Binar Sort: A Linear Generalized Sorting Algorithm",
        "authors": [
            "William F. Gilreath"
        ],
        "summary": "Sorting is a common and ubiquitous activity for computers. It is not surprising that there exist a plethora of sorting algorithms. For all the sorting algorithms, it is an accepted performance limit that sorting algorithms are linearithmic or O(N lg N). The linearithmic lower bound in performance stems from the fact that the sorting algorithms use the ordering property of the data. The sorting algorithm uses comparison by the ordering property to arrange the data elements from an initial permutation into a sorted permutation.   Linear O(N) sorting algorithms exist, but use a priori knowledge of the data to use a specific property of the data and thus have greater performance. In contrast, the linearithmic sorting algorithms are generalized by using a universal property of data-comparison, but have a linearithmic performance lower bound. The trade-off in sorting algorithms is generality for performance by the chosen property used to sort the data elements.   A general-purpose, linear sorting algorithm in the context of the trade-off of performance for generality at first consideration seems implausible. But, there is an implicit assumption that only the ordering property is universal. But, as will be discussed and examined, it is not the only universal property for data elements. The binar sort is a general-purpose sorting algorithm that uses this other universal property to sort linearly.",
        "published": "2008-11-21T01:38:09Z",
        "link": "http://arxiv.org/abs/0811.3448v2",
        "categories": [
            "cs.DS",
            "B.2.4; F.2.2"
        ]
    },
    {
        "title": "Binar Shuffle Algorithm: Shuffling Bit by Bit",
        "authors": [
            "William F. Gilreath"
        ],
        "summary": "Frequently, randomly organized data is needed to avoid an anomalous operation of other algorithms and computational processes. An analogy is that a deck of cards is ordered within the pack, but before a game of poker or solitaire the deck is shuffled to create a random permutation. Shuffling is used to assure that an aggregate of data elements for a sequence S is randomly arranged, but avoids an ordered or partially ordered permutation.   Shuffling is the process of arranging data elements into a random permutation. The sequence S as an aggregation of N data elements, there are N! possible permutations. For the large number of possible permutations, two of the possible permutations are for a sorted or ordered placement of data elements--both an ascending and descending sorted permutation. Shuffling must avoid inadvertently creating either an ascending or descending permutation.   Shuffling is frequently coupled to another algorithmic function -- pseudo-random number generation. The efficiency and quality of the shuffle is directly dependent upon the random number generation algorithm utilized. A more effective and efficient method of shuffling is to use parameterization to configure the shuffle, and to shuffle into sub-arrays by utilizing the encoding of the data elements. The binar shuffle algorithm uses the encoding of the data elements and parameterization to avoid any direct coupling to a random number generation algorithm, but still remain a linear O(N) shuffle algorithm.",
        "published": "2008-11-21T01:45:50Z",
        "link": "http://arxiv.org/abs/0811.3449v1",
        "categories": [
            "cs.DS",
            "B.2.4; F.2.2"
        ]
    },
    {
        "title": "Faster Approximate String Matching for Short Patterns",
        "authors": [
            "Philip Bille"
        ],
        "summary": "We study the classical approximate string matching problem, that is, given strings $P$ and $Q$ and an error threshold $k$, find all ending positions of substrings of $Q$ whose edit distance to $P$ is at most $k$. Let $P$ and $Q$ have lengths $m$ and $n$, respectively. On a standard unit-cost word RAM with word size $w \\geq \\log n$ we present an algorithm using time $$ O(nk \\cdot \\min(\\frac{\\log^2 m}{\\log n},\\frac{\\log^2 m\\log w}{w}) + n) $$ When $P$ is short, namely, $m = 2^{o(\\sqrt{\\log n})}$ or $m = 2^{o(\\sqrt{w/\\log w})}$ this improves the previously best known time bounds for the problem. The result is achieved using a novel implementation of the Landau-Vishkin algorithm based on tabulation and word-level parallelism.",
        "published": "2008-11-21T08:52:59Z",
        "link": "http://arxiv.org/abs/0811.3490v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Low-Memory Adaptive Prefix Coding",
        "authors": [
            "Travis Gagie",
            "Marek Karpinski",
            "Yakov Nekrich"
        ],
        "summary": "In this paper we study the adaptive prefix coding problem in cases where the size of the input alphabet is large. We present an online prefix coding algorithm that uses $O(\\sigma^{1 / \\lambda + \\epsilon}) $ bits of space for any constants $\\eps>0$, $\\lambda>1$, and encodes the string of symbols in $O(\\log \\log \\sigma)$ time per symbol \\emph{in the worst case}, where $\\sigma$ is the size of the alphabet. The upper bound on the encoding length is $\\lambda n H (s) +(\\lambda \\ln 2 + 2 + \\epsilon) n + O (\\sigma^{1 / \\lambda} \\log^2 \\sigma)$ bits.",
        "published": "2008-11-21T18:23:00Z",
        "link": "http://arxiv.org/abs/0811.3602v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Revisiting Norm Estimation in Data Streams",
        "authors": [
            "Daniel M. Kane",
            "Jelani Nelson",
            "David P. Woodruff"
        ],
        "summary": "The problem of estimating the pth moment F_p (p nonnegative and real) in data streams is as follows. There is a vector x which starts at 0, and many updates of the form x_i <-- x_i + v come sequentially in a stream. The algorithm also receives an error parameter 0 < eps < 1. The goal is then to output an approximation with relative error at most eps to F_p = ||x||_p^p.   Previously, it was known that polylogarithmic space (in the vector length n) was achievable if and only if p <= 2. We make several new contributions in this regime, including:   (*) An optimal space algorithm for 0 < p < 2, which, unlike previous algorithms which had optimal dependence on 1/eps but sub-optimal dependence on n, does not rely on a generic pseudorandom generator.   (*) A near-optimal space algorithm for p = 0 with optimal update and query time.   (*) A near-optimal space algorithm for the \"distinct elements\" problem (p = 0 and all updates have v = 1) with optimal update and query time.   (*) Improved L_2 --> L_2 dimensionality reduction in a stream.   (*) New 1-pass lower bounds to show optimality and near-optimality of our algorithms, as well as of some previous algorithms (the \"AMS sketch\" for p = 2, and the L_1-difference algorithm of Feigenbaum et al.).   As corollaries of our work, we also obtain a few separations in the complexity of moment estimation problems: F_0 in 1 pass vs. 2 passes, p = 0 vs. p > 0, and F_0 with strictly positive updates vs. arbitrary updates.",
        "published": "2008-11-21T22:55:07Z",
        "link": "http://arxiv.org/abs/0811.3648v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Tight Approximation Ratio of a General Greedy Splitting Algorithm for   the Minimum k-Way Cut Problem",
        "authors": [
            "Mingyu Xiao",
            "Leizhen Cai",
            "Andrew C. Yao"
        ],
        "summary": "For an edge-weighted connected undirected graph, the minimum $k$-way cut problem is to find a subset of edges of minimum total weight whose removal separates the graph into $k$ connected components. The problem is NP-hard when $k$ is part of the input and W[1]-hard when $k$ is taken as a parameter.   A simple algorithm for approximating a minimum $k$-way cut is to iteratively increase the number of components of the graph by $h-1$, where $2 \\le h \\le k$, until the graph has $k$ components. The approximation ratio of this algorithm is known for $h \\le 3$ but is open for $h \\ge 4$.   In this paper, we consider a general algorithm that iteratively increases the number of components of the graph by $h_i-1$, where $h_1 \\le h_2 \\le ... \\le h_q$ and $\\sum_{i=1}^q (h_i-1) = k-1$. We prove that the approximation ratio of this general algorithm is $2 - (\\sum_{i=1}^q {h_i \\choose 2})/{k \\choose 2}$, which is tight. Our result implies that the approximation ratio of the simple algorithm is $2-h/k + O(h^2/k^2)$ in general and $2-h/k$ if $k-1$ is a multiple of $h-1$.",
        "published": "2008-11-23T03:47:50Z",
        "link": "http://arxiv.org/abs/0811.3723v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "G.1.2"
        ]
    },
    {
        "title": "Communication Efficiency in Self-stabilizing Silent Protocols",
        "authors": [
            "Stéphane Devismes",
            "Toshimitsu Masuzawa",
            "Sébastien Tixeuil"
        ],
        "summary": "Self-stabilization is a general paradigm to provide forward recovery capabilities to distributed systems and networks. Intuitively, a protocol is self-stabilizing if it is able to recover without external intervention from any catastrophic transient failure. In this paper, our focus is to lower the communication complexity of self-stabilizing protocols \\emph{below} the need of checking every neighbor forever. In more details, the contribution of the paper is threefold: (i) We provide new complexity measures for communication efficiency of self-stabilizing protocols, especially in the stabilized phase or when there are no faults, (ii) On the negative side, we show that for non-trivial problems such as coloring, maximal matching, and maximal independent set, it is impossible to get (deterministic or probabilistic) self-stabilizing solutions where every participant communicates with less than every neighbor in the stabilized phase, and (iii) On the positive side, we present protocols for coloring, maximal matching, and maximal independent set such that a fraction of the participants communicates with exactly one neighbor in the stabilized phase.",
        "published": "2008-11-23T17:29:25Z",
        "link": "http://arxiv.org/abs/0811.3760v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Finding Sparse Cuts Locally Using Evolving Sets",
        "authors": [
            "Reid Andersen",
            "Yuval Peres"
        ],
        "summary": "A {\\em local graph partitioning algorithm} finds a set of vertices with small conductance (i.e. a sparse cut) by adaptively exploring part of a large graph $G$, starting from a specified vertex. For the algorithm to be local, its complexity must be bounded in terms of the size of the set that it outputs, with at most a weak dependence on the number $n$ of vertices in $G$. Previous local partitioning algorithms find sparse cuts using random walks and personalized PageRank. In this paper, we introduce a randomized local partitioning algorithm that finds a sparse cut by simulating the {\\em volume-biased evolving set process}, which is a Markov chain on sets of vertices. We prove that for any set of vertices $A$ that has conductance at most $\\phi$, for at least half of the starting vertices in $A$ our algorithm will output (with probability at least half), a set of conductance $O(\\phi^{1/2} \\log^{1/2} n)$. We prove that for a given run of the algorithm, the expected ratio between its computational complexity and the volume of the set that it outputs is $O(\\phi^{-1/2} polylog(n))$. In comparison, the best previous local partitioning algorithm, due to Andersen, Chung, and Lang, has the same approximation guarantee, but a larger ratio of $O(\\phi^{-1} polylog(n))$ between the complexity and output volume. Using our local partitioning algorithm as a subroutine, we construct a fast algorithm for finding balanced cuts. Given a fixed value of $\\phi$, the resulting algorithm has complexity $O((m+n\\phi^{-1/2}) polylog(n))$ and returns a cut with conductance $O(\\phi^{1/2} \\log^{1/2} n)$ and volume at least $v_{\\phi}/2$, where $v_{\\phi}$ is the largest volume of any set with conductance at most $\\phi$.",
        "published": "2008-11-23T22:39:38Z",
        "link": "http://arxiv.org/abs/0811.3779v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "The Simultaneous Membership Problem for Chordal, Comparability and   Permutation graphs",
        "authors": [
            "Krishnam Raju Jampani",
            "Anna Lubiw"
        ],
        "summary": "In this paper we introduce the 'simultaneous membership problem', defined for any graph class C characterized in terms of representations, e.g. any class of intersection graphs. Two graphs G_1 and G_2, sharing some vertices X (and the corresponding induced edges), are said to be 'simultaneous members' of graph class C, if there exist representations R_1 and R_2 of G_1 and G_2 that are \"consistent\" on X. Equivalently (for the classes C that we consider) there exist edges E' between G_1-X and G_2-X such that G_1 \\cup G_2 \\cup E' belongs to class C.   Simultaneous membership problems have application in any situation where it is desirable to consistently represent two related graphs, for example: interval graphs capturing overlaps of DNA fragments of two similar organisms; or graphs connected in time, where one is an updated version of the other. Simultaneous membership problems are related to simultaneous planar embeddings, graph sandwich problems and probe graph recognition problems.   In this paper we give efficient algorithms for the simultaneous membership problem on chordal, comparability and permutation graphs. These results imply that graph sandwich problems for the above classes are tractable for an interesting special case: when the set of optional edges form a complete bipartite graph. Our results complement the recent polynomial time recognition algorithms for probe chordal, comparability, and permutation graphs, where the set of optional edges form a clique.",
        "published": "2008-11-25T02:54:32Z",
        "link": "http://arxiv.org/abs/0811.4007v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Search Result Clustering via Randomized Partitioning of Query-Induced   Subgraphs",
        "authors": [
            "Aleksandar Bradic"
        ],
        "summary": "In this paper, we present an approach to search result clustering, using partitioning of underlying link graph. We define the notion of \"query-induced subgraph\" and formulate the problem of search result clustering as a problem of efficient partitioning of given subgraph into topic-related clusters. Also, we propose a novel algorithm for approximative partitioning of such graph, which results in cluster quality comparable to the one obtained by deterministic algorithms, while operating in more efficient computation time, suitable for practical implementations. Finally, we present a practical clustering search engine developed as a part of this research and use it to get results about real-world performance of proposed concepts.",
        "published": "2008-11-25T23:11:55Z",
        "link": "http://arxiv.org/abs/0811.4186v1",
        "categories": [
            "cs.IR",
            "cs.DS",
            "H.3.3; I.1.2; E.1; G.3"
        ]
    },
    {
        "title": "Dynamic Indexability: The Query-Update Tradeoff for One-Dimensional   Range Queries",
        "authors": [
            "Ke Yi"
        ],
        "summary": "The B-tree is a fundamental secondary index structure that is widely used for answering one-dimensional range reporting queries. Given a set of $N$ keys, a range query can be answered in $O(\\log_B \\nm + \\frac{K}{B})$ I/Os, where $B$ is the disk block size, $K$ the output size, and $M$ the size of the main memory buffer. When keys are inserted or deleted, the B-tree is updated in $O(\\log_B N)$ I/Os, if we require the resulting changes to be committed to disk right away. Otherwise, the memory buffer can be used to buffer the recent updates, and changes can be written to disk in batches, which significantly lowers the amortized update cost. A systematic way of batching up updates is to use the logarithmic method, combined with fractional cascading, resulting in a dynamic B-tree that supports insertions in $O(\\frac{1}{B}\\log\\nm)$ I/Os and queries in $O(\\log\\nm + \\frac{K}{B})$ I/Os. Such bounds have also been matched by several known dynamic B-tree variants in the database literature.   In this paper, we prove that for any dynamic one-dimensional range query index structure with query cost $O(q+\\frac{K}{B})$ and amortized insertion cost $O(u/B)$, the tradeoff $q\\cdot \\log(u/q) = \\Omega(\\log B)$ must hold if $q=O(\\log B)$. For most reasonable values of the parameters, we have $\\nm = B^{O(1)}$, in which case our query-insertion tradeoff implies that the bounds mentioned above are already optimal. Our lower bounds hold in a dynamic version of the {\\em indexability model}, which is of independent interests.",
        "published": "2008-11-26T15:36:14Z",
        "link": "http://arxiv.org/abs/0811.4346v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "How robust is quicksort average complexity?",
        "authors": [
            "Suman Kumar Sourabh",
            "Soubhik Chakraborty"
        ],
        "summary": "The paper questions the robustness of average case time complexity of the fast and popular quicksort algorithm. Among the six standard probability distributions examined in the paper, only continuous uniform, exponential and standard normal are supporting it whereas the others are supporting the worst case complexity measure. To the question -why are we getting the worst case complexity measure each time the average case measure is discredited? -- one logical answer is average case complexity under the universal distribution equals worst case complexity. This answer, which is hard to challenge, however gives no idea as to which of the standard probability distributions come under the umbrella of universality. The morale is that average case complexity measures, in cases where they are different from those in worst case, should be deemed as robust provided only they get the support from at least the standard probability distributions, both discrete and continuous. Regretfully, this is not the case with quicksort.",
        "published": "2008-11-26T17:23:22Z",
        "link": "http://arxiv.org/abs/0811.4376v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Compact Labelings For Efficient First-Order Model-Checking",
        "authors": [
            "Bruno Courcelle",
            "Cyril Gavoille",
            "Mamadou Moustapha Kanté"
        ],
        "summary": "We consider graph properties that can be checked from labels, i.e., bit sequences, of logarithmic length attached to vertices. We prove that there exists such a labeling for checking a first-order formula with free set variables in the graphs of every class that is \\emph{nicely locally cwd-decomposable}. This notion generalizes that of a \\emph{nicely locally tree-decomposable} class. The graphs of such classes can be covered by graphs of bounded \\emph{clique-width} with limited overlaps. We also consider such labelings for \\emph{bounded} first-order formulas on graph classes of \\emph{bounded expansion}. Some of these results are extended to counting queries.",
        "published": "2008-11-28T13:29:15Z",
        "link": "http://arxiv.org/abs/0811.4713v2",
        "categories": [
            "cs.DS",
            "cs.LO",
            "68R05, 68R10, 05C78, 05C85",
            "F.0; G.2.2"
        ]
    },
    {
        "title": "Fast and Quality-Guaranteed Data Streaming in Resource-Constrained   Sensor Networks",
        "authors": [
            "Emad Soroush",
            "Kui Wu",
            "Jian Pei"
        ],
        "summary": "In many emerging applications, data streams are monitored in a network environment. Due to limited communication bandwidth and other resource constraints, a critical and practical demand is to online compress data streams continuously with quality guarantee. Although many data compression and digital signal processing methods have been developed to reduce data volume, their super-linear time and more-than-constant space complexity prevents them from being applied directly on data streams, particularly over resource-constrained sensor networks. In this paper, we tackle the problem of online quality guaranteed compression of data streams using fast linear approximation (i.e., using line segments to approximate a time series). Technically, we address two versions of the problem which explore quality guarantees in different forms. We develop online algorithms with linear time complexity and constant cost in space. Our algorithms are optimal in the sense they generate the minimum number of segments that approximate a time series with the required quality guarantee. To meet the resource constraints in sensor networks, we also develop a fast algorithm which creates connecting segments with very simple computation. The low cost nature of our methods leads to a unique edge on the applications of massive and fast streaming environment, low bandwidth networks, and heavily constrained nodes in computational power. We implement and evaluate our methods in the application of an acoustic wireless sensor network.",
        "published": "2008-11-28T20:59:55Z",
        "link": "http://arxiv.org/abs/0811.4672v1",
        "categories": [
            "cs.DS",
            "cs.MM",
            "C.3; G.1.2"
        ]
    },
    {
        "title": "Lower Bounds on Performance of Metric Tree Indexing Schemes for Exact   Similarity Search in High Dimensions",
        "authors": [
            "Vladimir Pestov"
        ],
        "summary": "Within a mathematically rigorous model, we analyse the curse of dimensionality for deterministic exact similarity search in the context of popular indexing schemes: metric trees. The datasets $X$ are sampled randomly from a domain $\\Omega$, equipped with a distance, $\\rho$, and an underlying probability distribution, $\\mu$. While performing an asymptotic analysis, we send the intrinsic dimension $d$ of $\\Omega$ to infinity, and assume that the size of a dataset, $n$, grows superpolynomially yet subexponentially in $d$. Exact similarity search refers to finding the nearest neighbour in the dataset $X$ to a query point $\\omega\\in\\Omega$, where the query points are subject to the same probability distribution $\\mu$ as datapoints. Let $\\mathscr F$ denote a class of all 1-Lipschitz functions on $\\Omega$ that can be used as decision functions in constructing a hierarchical metric tree indexing scheme. Suppose the VC dimension of the class of all sets $\\{\\omega\\colon f(\\omega)\\geq a\\}$, $a\\in\\R$ is $o(n^{1/4}/\\log^2n)$. (In view of a 1995 result of Goldberg and Jerrum, even a stronger complexity assumption $d^{O(1)}$ is reasonable.) We deduce the $\\Omega(n^{1/4})$ lower bound on the expected average case performance of hierarchical metric-tree based indexing schemes for exact similarity search in $(\\Omega,X)$. In paricular, this bound is superpolynomial in $d$.",
        "published": "2008-11-30T15:17:22Z",
        "link": "http://arxiv.org/abs/0812.0146v4",
        "categories": [
            "cs.DS",
            "68P10",
            "H.3.3"
        ]
    },
    {
        "title": "Optimal Tracking of Distributed Heavy Hitters and Quantiles",
        "authors": [
            "Ke Yi",
            "Qin Zhang"
        ],
        "summary": "We consider the the problem of tracking heavy hitters and quantiles in the distributed streaming model. The heavy hitters and quantiles are two important statistics for characterizing a data distribution. Let $A$ be a multiset of elements, drawn from the universe $U=\\{1,...,u\\}$. For a given $0 \\le \\phi \\le 1$, the $\\phi$-heavy hitters are those elements of $A$ whose frequency in $A$ is at least $\\phi |A|$; the $\\phi$-quantile of $A$ is an element $x$ of $U$ such that at most $\\phi|A|$ elements of $A$ are smaller than $A$ and at most $(1-\\phi)|A|$ elements of $A$ are greater than $x$. Suppose the elements of $A$ are received at $k$ remote {\\em sites} over time, and each of the sites has a two-way communication channel to a designated {\\em coordinator}, whose goal is to track the set of $\\phi$-heavy hitters and the $\\phi$-quantile of $A$ approximately at all times with minimum communication. We give tracking algorithms with worst-case communication cost $O(k/\\eps \\cdot \\log n)$ for both problems, where $n$ is the total number of items in $A$, and $\\eps$ is the approximation error. This substantially improves upon the previous known algorithms. We also give matching lower bounds on the communication costs for both problems, showing that our algorithms are optimal. We also consider a more general version of the problem where we simultaneously track the $\\phi$-quantiles for all $0 \\le \\phi \\le 1$.",
        "published": "2008-12-01T03:51:12Z",
        "link": "http://arxiv.org/abs/0812.0209v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Stackelberg Network Pricing is Hard to Approximate",
        "authors": [
            "Gwenaël Joret"
        ],
        "summary": "In the Stackelberg Network Pricing problem, one has to assign tariffs to a certain subset of the arcs of a given transportation network. The aim is to maximize the amount paid by the user of the network, knowing that the user will take a shortest st-path once the tariffs are fixed. Roch, Savard, and Marcotte (Networks, Vol. 46(1), 57-67, 2005) proved that this problem is NP-hard, and gave an O(log m)-approximation algorithm, where m denote the number of arcs to be priced. In this note, we show that the problem is also APX-hard.",
        "published": "2008-12-01T16:15:58Z",
        "link": "http://arxiv.org/abs/0812.0320v1",
        "categories": [
            "cs.DS",
            "cs.GT"
        ]
    },
    {
        "title": "k-means requires exponentially many iterations even in the plane",
        "authors": [
            "Andrea Vattani"
        ],
        "summary": "The k-means algorithm is a well-known method for partitioning n points that lie in the d-dimensional space into k clusters. Its main features are simplicity and speed in practice. Theoretically, however, the best known upper bound on its running time (i.e. O(n^{kd})) can be exponential in the number of points. Recently, Arthur and Vassilvitskii [3] showed a super-polynomial worst-case analysis, improving the best known lower bound from \\Omega(n) to 2^{\\Omega(\\sqrt{n})} with a construction in d=\\Omega(\\sqrt{n}) dimensions. In [3] they also conjectured the existence of superpolynomial lower bounds for any d >= 2.   Our contribution is twofold: we prove this conjecture and we improve the lower bound, by presenting a simple construction in the plane that leads to the exponential lower bound 2^{\\Omega(n)}.",
        "published": "2008-12-01T22:55:39Z",
        "link": "http://arxiv.org/abs/0812.0382v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Delaunay Triangulations in Linear Time? (Part I)",
        "authors": [
            "Kevin Buchin"
        ],
        "summary": "We present a new and simple randomized algorithm for constructing the Delaunay triangulation using nearest neighbor graphs for point location. Under suitable assumptions, it runs in linear expected time for points in the plane with polynomially bounded spread, i.e., if the ratio between the largest and smallest pointwise distance is polynomially bounded. This also holds for point sets with bounded spread in higher dimensions as long as the expected complexity of the Delaunay triangulation of a sample of the points is linear in the sample size.",
        "published": "2008-12-01T23:09:13Z",
        "link": "http://arxiv.org/abs/0812.0387v3",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Approximation Algorithms for Bregman Co-clustering and Tensor Clustering",
        "authors": [
            "Stefanie Jegelka",
            "Suvrit Sra",
            "Arindam Banerjee"
        ],
        "summary": "In the past few years powerful generalizations to the Euclidean k-means problem have been made, such as Bregman clustering [7], co-clustering (i.e., simultaneous clustering of rows and columns of an input matrix) [9,18], and tensor clustering [8,34]. Like k-means, these more general problems also suffer from the NP-hardness of the associated optimization. Researchers have developed approximation algorithms of varying degrees of sophistication for k-means, k-medians, and more recently also for Bregman clustering [2]. However, there seem to be no approximation algorithms for Bregman co- and tensor clustering. In this paper we derive the first (to our knowledge) guaranteed methods for these increasingly important clustering settings. Going beyond Bregman divergences, we also prove an approximation factor for tensor clustering with arbitrary separable metrics. Through extensive experiments we evaluate the characteristics of our method, and show that it also has practical impact.",
        "published": "2008-12-01T23:17:35Z",
        "link": "http://arxiv.org/abs/0812.0389v4",
        "categories": [
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Preference Games and Personalized Equilibria, with Applications to   Fractional BGP",
        "authors": [
            "Laura J. Poplawski",
            "Rajmohan Rajaraman",
            "Ravi Sundaram",
            "Shang-Hua Teng"
        ],
        "summary": "We study the complexity of computing equilibria in two classes of network games based on flows - fractional BGP (Border Gateway Protocol) games and fractional BBC (Bounded Budget Connection) games. BGP is the glue that holds the Internet together and hence its stability, i.e. the equilibria of fractional BGP games (Haxell, Wilfong), is a matter of practical importance. BBC games (Laoutaris et al) follow in the tradition of the large body of work on network formation games and capture a variety of applications ranging from social networks and overlay networks to peer-to-peer networks.   The central result of this paper is that there are no fully polynomial-time approximation schemes (unless PPAD is in FP) for computing equilibria in both fractional BGP games and fractional BBC games. We obtain this result by proving the hardness for a new and surprisingly simple game, the fractional preference game, which is reducible to both fractional BGP and BBC games.   We define a new flow-based notion of equilibrium for matrix games -- personalized equilibria -- generalizing both fractional BBC and fractional BGP games. We prove not just the existence, but the existence of rational personalized equilibria for all matrix games, which implies the existence of rational equilibria for fractional BGP and BBC games. In particular, this provides an alternative proof and strengthening of the main result in [Haxell, Wilfong]. For k-player matrix games, where k = 2, we provide a combinatorial characterization leading to a polynomial-time algorithm for computing all personalized equilibria. For k >= 5, we prove that personalized equilibria are PPAD-hard to approximate in fully polynomial time. We believe that the concept of personalized equilibria has potential for real-world significance.",
        "published": "2008-12-02T21:12:03Z",
        "link": "http://arxiv.org/abs/0812.0598v2",
        "categories": [
            "cs.GT",
            "cs.DS",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Linear-Time Algorithms for Geometric Graphs with Sublinearly Many Edge   Crossings",
        "authors": [
            "David Eppstein",
            "Michael T. Goodrich",
            "Darren Strash"
        ],
        "summary": "We provide linear-time algorithms for geometric graphs with sublinearly many crossings. That is, we provide algorithms running in O(n) time on connected geometric graphs having n vertices and k crossings, where k is smaller than n by an iterated logarithmic factor. Specific problems we study include Voronoi diagrams and single-source shortest paths. Our algorithms all run in linear time in the standard comparison-based computational model; hence, we make no assumptions about the distribution or bit complexities of edge weights, nor do we utilize unusual bit-level operations on memory words. Instead, our algorithms are based on a planarization method that \"zeroes in\" on edge crossings, together with methods for extending planar separator decompositions to geometric graphs with sublinearly many crossings. Incidentally, our planarization algorithm also solves an open computational geometry problem of Chazelle for triangulating a self-intersecting polygonal chain having n segments and k crossings in linear time, for the case when k is sublinear in n by an iterated logarithmic factor.",
        "published": "2008-12-04T10:29:00Z",
        "link": "http://arxiv.org/abs/0812.0893v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "cs.DS",
            "cs.GR",
            "F.2.2; G.2.2; G.3"
        ]
    },
    {
        "title": "Adaptive Uncertainty Resolution in Bayesian Combinatorial Optimization   Problems",
        "authors": [
            "Sudipto Guha",
            "Kamesh Munagala"
        ],
        "summary": "In several applications such as databases, planning, and sensor networks, parameters such as selectivity, load, or sensed values are known only with some associated uncertainty. The performance of such a system (as captured by some objective function over the parameters) is significantly improved if some of these parameters can be probed or observed. In a resource constrained situation, deciding which parameters to observe in order to optimize system performance itself becomes an interesting and important optimization problem. This general problem is the focus of this paper.   One of the most important considerations in this framework is whether adaptivity is required for the observations. Adaptive observations introduce blocking or sequential operations in the system whereas non-adaptive observations can be performed in parallel. One of the important questions in this regard is to characterize the benefit of adaptivity for probes and observation.   We present general techniques for designing constant factor approximations to the optimal observation schemes for several widely used scheduling and metric objective functions. We show a unifying technique that relates this optimization problem to the outlier version of the corresponding deterministic optimization. By making this connection, our technique shows constant factor upper bounds for the benefit of adaptivity of the observation schemes. We show that while probing yields significant improvement in the objective function, being adaptive about the probing is not beneficial beyond constant factors.",
        "published": "2008-12-04T19:48:16Z",
        "link": "http://arxiv.org/abs/0812.1012v3",
        "categories": [
            "cs.DS",
            "F.2"
        ]
    },
    {
        "title": "Improved Approximation for the Number of Hamiltonian Cycles in Dense   Digraphs",
        "authors": [
            "Jinshan Zhang"
        ],
        "summary": "We propose an improved algorithm for counting the number of Hamiltonian cycles in a directed graph. The basic idea of the method is sequential acceptance/rejection, which is successfully used in approximating the number of perfect matchings in dense bipartite graphs. As a consequence, a new bound on the number of Hamiltonian cycles in a directed graph is proved, by using the ratio of the number of 1-factors. Based on this bound, we prove that our algorithm runs in expected time of $O(n^{8.5})$ for dense problems. This improves the Markov chain method, the most powerful existing method, a factor of at least $n^{4.5}(\\log n)^{4}$ in running time. This class of dense problems is shown to be nontrivial in counting, in the sense that it is $#$P-Complete.",
        "published": "2008-12-05T12:28:57Z",
        "link": "http://arxiv.org/abs/0812.1123v4",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.0"
        ]
    },
    {
        "title": "Emerge-Sort: Converging to Ordered Sequences by Simple Local Operators",
        "authors": [
            "Dimitris Kalles",
            "Alexis Kaporis"
        ],
        "summary": "In this paper we examine sorting on the assumption that we do not know in advance which way to sort a sequence of numbers and we set at work simple local comparison and swap operators whose repeating application ends up in sorted sequences. These are the basic elements of Emerge-Sort, our approach to self-organizing sorting, which we then validate experimentally across a range of samples. Observing an O(n2) run-time behaviour, we note that the n/logn delay coefficient that differentiates Emerge-Sort from the classical comparison based algorithms is an instantiation of the price of anarchy we pay for not imposing a sorting order and for letting that order emerge through the local interactions.",
        "published": "2008-12-05T12:57:00Z",
        "link": "http://arxiv.org/abs/0812.1126v2",
        "categories": [
            "cs.AI",
            "cs.DS"
        ]
    },
    {
        "title": "Oscillations with TCP-like Flow Control in Networks of Queues",
        "authors": [
            "Matthew Andrews",
            "Aleksandrs Slivkins"
        ],
        "summary": "We consider a set of flows passing through a set of servers. The injection rate into each flow is governed by a flow control that increases the injection rate when all the servers on the flow's path are empty and decreases the injection rate when some server is congested. We show that if each server's congestion is governed by the arriving traffic at the server then the system can *oscillate*. This is in contrast to previous work on flow control where congestion was modeled as a function of the flow injection rates and the system was shown to converge to a steady state that maximizes an overall network utility.",
        "published": "2008-12-06T23:57:44Z",
        "link": "http://arxiv.org/abs/0812.1321v1",
        "categories": [
            "cs.NI",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "An Extension of the Permutation Group Enumeration Technique (Collapse of   the Polynomial Hierarchy: $\\mathbf{NP = P}$)",
        "authors": [
            "Javaid Aslam"
        ],
        "summary": "The distinguishing result of this paper is a $\\mathbf{P}$-time enumerable partition of all the potential perfect matchings in a bipartite graph. This partition is a set of equivalence classes induced by the missing edges in the potential perfect matchings.   We capture the behavior of these missing edges in a polynomially bounded representation of the exponentially many perfect matchings by a graph theoretic structure, called MinSet Sequence, where MinSet is a P-time enumerable structure derived from a graph theoretic counterpart of a generating set of the symmetric group. This leads to a polynomially bounded generating set of all the classes, enabling the enumeration of perfect matchings in polynomial time. The sequential time complexity of this $\\mathbf{\\#P}$-complete problem is shown to be $O(n^{45}\\log n)$.   And thus we prove a result even more surprising than $\\mathbf{NP = P}$, that is, $\\mathbf{\\#P}=\\mathbf{FP}$, where $\\mathbf{FP}$ is the class of functions, $f: \\{0, 1\\}^* \\rightarrow \\mathbb{N} $, computable in polynomial time on a deterministic model of computation.",
        "published": "2008-12-07T19:47:28Z",
        "link": "http://arxiv.org/abs/0812.1385v26",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.2.0"
        ]
    },
    {
        "title": "Fast phylogeny reconstruction through learning of ancestral sequences",
        "authors": [
            "Radu Mihaescu",
            "Cameron Hill",
            "Satish Rao"
        ],
        "summary": "Given natural limitations on the length DNA sequences, designing phylogenetic reconstruction methods which are reliable under limited information is a crucial endeavor. There have been two approaches to this problem: reconstructing partial but reliable information about the tree (\\cite{Mo07, DMR08,DHJ06,GMS08}), and reaching \"deeper\" in the tree through reconstruction of ancestral sequences. In the latter category, \\cite{DMR06} settled an important conjecture of M.Steel, showing that, under the CFN model of evolution, all trees on $n$ leaves with edge lengths bounded by the Ising model phase transition can be recovered with high probability from genomes of length $O(\\log n)$ with a polynomial time algorithm. Their methods had a running time of $O(n^{10})$.   Here we enhance our methods from \\cite{DHJ06} with the learning of ancestral sequences and provide an algorithm for reconstructing a sub-forest of the tree which is reliable given available data, without requiring a-priori known bounds on the edge lengths of the tree. Our methods are based on an intuitive minimum spanning tree approach and run in $O(n^3)$ time. For the case of full reconstruction of trees with edges under the phase transition, we maintain the same sequence length requirements as \\cite{DMR06}, despite the considerably faster running time.",
        "published": "2008-12-08T22:51:02Z",
        "link": "http://arxiv.org/abs/0812.1587v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "A quasi-polynomial time approximation scheme for Euclidean capacitated   vehicle routing",
        "authors": [
            "Aparna Das",
            "Claire Mathieu"
        ],
        "summary": "In the capacitated vehicle routing problem, introduced by Dantzig and Ramser in 1959, we are given the locations of n customers and a depot, along with a vehicle of capacity k, and wish to find a minimum length collection of tours, each starting from the depot and visiting at most k customers, whose union covers all the customers. We give a quasi-polynomial time approximation scheme for the setting where the customers and the depot are on the plane, and distances are given by the Euclidean metric.",
        "published": "2008-12-08T23:58:17Z",
        "link": "http://arxiv.org/abs/0812.1595v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Two Dimensional Connectivity for Vehicular Ad-Hoc Networks",
        "authors": [
            "Masoud Farivar",
            "Behzad Mehrdad",
            "Farid Ashtiani"
        ],
        "summary": "In this paper, we focus on two-dimensional connectivity in sparse vehicular ad hoc networks (VANETs). In this respect, we find thresholds for the arrival rates of vehicles at entrances of a block of streets such that the connectivity is guaranteed for any desired probability. To this end, we exploit a mobility model recently proposed for sparse VANETs, based on BCMP open queuing networks and solve the related traffic equations to find the traffic characteristics of each street and use the results to compute the exact probability of connectivity along these streets. Then, we use the results from percolation theory and the proposed fast algorithms for evaluation of bond percolation problem in a random graph corresponding to the block of the streets. We then find sufficiently accurate two dimensional connectivity-related parameters, such as the average number of intersections connected to each other and the size of the largest set of inter-connected intersections. We have also proposed lower bounds for the case of heterogeneous network with two transmission ranges. In the last part of the paper, we apply our method to several numerical examples and confirm our results by simulations.",
        "published": "2008-12-09T07:16:10Z",
        "link": "http://arxiv.org/abs/0812.1628v1",
        "categories": [
            "cs.NI",
            "cs.DS"
        ]
    },
    {
        "title": "Dynamic Complexity of Formal Languages",
        "authors": [
            "Wouter Gelade",
            "Marcel Marquardt",
            "Thomas Schwentick"
        ],
        "summary": "The paper investigates the power of the dynamic complexity classes DynFO, DynQF and DynPROP over string languages. The latter two classes contain problems that can be maintained using quantifier-free first-order updates, with and without auxiliary functions, respectively. It is shown that the languages maintainable in DynPROP exactly are the regular languages, even when allowing arbitrary precomputation. This enables lower bounds for DynPROP and separates DynPROP from DynQF and DynFO. Further, it is shown that any context-free language can be maintained in DynFO and a number of specific context-free languages, for example all Dyck-languages, are maintainable in DynQF. Furthermore, the dynamic complexity of regular tree languages is investigated and some results concerning arbitrary structures are obtained: there exist first-order definable properties which are not maintainable in DynPROP. On the other hand any existential first-order property can be maintained in DynQF when allowing precomputation.",
        "published": "2008-12-10T14:13:57Z",
        "link": "http://arxiv.org/abs/0812.1915v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "The convex hull of a regular set of integer vectors is polyhedral and   effectively computable",
        "authors": [
            "Alain Finkel",
            "Jérôme Leroux"
        ],
        "summary": "Number Decision Diagrams (NDD) provide a natural finite symbolic representation for regular set of integer vectors encoded as strings of digit vectors (least or most significant digit first). The convex hull of the set of vectors represented by a NDD is proved to be an effectively computable convex polyhedron.",
        "published": "2008-12-10T16:26:36Z",
        "link": "http://arxiv.org/abs/0812.1951v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "Accelerated Data-Flow Analysis",
        "authors": [
            "Jérôme Leroux",
            "Gregoire Sutre"
        ],
        "summary": "Acceleration in symbolic verification consists in computing the exact effect of some control-flow loops in order to speed up the iterative fix-point computation of reachable states. Even if no termination guarantee is provided in theory, successful results were obtained in practice by different tools implementing this framework. In this paper, the acceleration framework is extended to data-flow analysis. Compared to a classical widening/narrowing-based abstract interpretation, the loss of precision is controlled here by the choice of the abstract domain and does not depend on the way the abstract value is computed. Our approach is geared towards precision, but we don't loose efficiency on the way. Indeed, we provide a cubic-time acceleration-based algorithm for solving interval constraints with full multiplication.",
        "published": "2008-12-10T20:08:08Z",
        "link": "http://arxiv.org/abs/0812.2011v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Convex Hull of Arithmetic Automata",
        "authors": [
            "Jérôme Leroux"
        ],
        "summary": "Arithmetic automata recognize infinite words of digits denoting decompositions of real and integer vectors. These automata are known expressive and efficient enough to represent the whole set of solutions of complex linear constraints combining both integral and real variables. In this paper, the closed convex hull of arithmetic automata is proved rational polyhedral. Moreover an algorithm computing the linear constraints defining these convex set is provided. Such an algorithm is useful for effectively extracting geometrical properties of the whole set of solutions of complex constraints symbolically represented by arithmetic automata.",
        "published": "2008-12-10T20:33:27Z",
        "link": "http://arxiv.org/abs/0812.2014v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A Factor 3/2 Approximation for Generalized Steiner Tree Problem with   Distances One and Two",
        "authors": [
            "Piotr Berman",
            "Marek Karpinski",
            "Alex Zelikovsky"
        ],
        "summary": "We design a 3/2 approximation algorithm for the Generalized Steiner Tree problem (GST) in metrics with distances 1 and 2. This is the first polynomial time approximation algorithm for a wide class of non-geometric metric GST instances with approximation factor below 2.",
        "published": "2008-12-11T12:50:54Z",
        "link": "http://arxiv.org/abs/0812.2137v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Performance of a greedy algorithm for edge covering by cliques in   interval graphs",
        "authors": [
            "Gabrio Caimi",
            "Holger Flier",
            "Martin Fuchsberger",
            "Marc Nunkesser"
        ],
        "summary": "In this paper a greedy algorithm to detect conflict cliques in interval graphs and circular-arc graphs is analyzed. In a graph, a stable set requires that at most one vertex is chosen for each edge. It is equivalent to requiring that at most one vertex for each maximal clique is chosen. We show that this algorithm finds all maximal cliques for interval graphs, i.e. it can compute the convex hull of the stable set polytope. In case of circular-arc graphs, the algorithm is not able to detect all maximal cliques, yet remaining correct. This problem occurs in the context of railway scheduling. A train requests the allocation of a railway infrastructure resource for a specific time interval. As one is looking for conflict-free train schedules, the used resource allocation intervals in a schedule must not overlap. The conflict-free choices of used intervals for each resource correspond to stable sets in the interval graph associated to the allocation time intervals.",
        "published": "2008-12-11T15:35:45Z",
        "link": "http://arxiv.org/abs/0812.2115v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Characterizing Truthful Multi-Armed Bandit Mechanisms",
        "authors": [
            "Moshe Babaioff",
            "Yogeshwer Sharma",
            "Aleksandrs Slivkins"
        ],
        "summary": "We consider a multi-round auction setting motivated by pay-per-click auctions for Internet advertising. In each round the auctioneer selects an advertiser and shows her ad, which is then either clicked or not. An advertiser derives value from clicks; the value of a click is her private information. Initially, neither the auctioneer nor the advertisers have any information about the likelihood of clicks on the advertisements. The auctioneer's goal is to design a (dominant strategies) truthful mechanism that (approximately) maximizes the social welfare.   If the advertisers bid their true private values, our problem is equivalent to the \"multi-armed bandit problem\", and thus can be viewed as a strategic version of the latter. In particular, for both problems the quality of an algorithm can be characterized by \"regret\", the difference in social welfare between the algorithm and the benchmark which always selects the same \"best\" advertisement. We investigate how the design of multi-armed bandit algorithms is affected by the restriction that the resulting mechanism must be truthful. We find that truthful mechanisms have certain strong structural properties -- essentially, they must separate exploration from exploitation -- and they incur much higher regret than the optimal multi-armed bandit algorithms. Moreover, we provide a truthful mechanism which (essentially) matches our lower bound on regret.",
        "published": "2008-12-12T04:13:01Z",
        "link": "http://arxiv.org/abs/0812.2291v7",
        "categories": [
            "cs.DS",
            "cs.GT",
            "cs.LG",
            "F.2.2; K.4.4; F.1.2; J.4"
        ]
    },
    {
        "title": "Efficient Isomorphism Testing for a Class of Group Extensions",
        "authors": [
            "Francois Le Gall"
        ],
        "summary": "The group isomorphism problem asks whether two given groups are isomorphic or not. Whereas the case where both groups are abelian is well understood and can be solved efficiently, very little is known about the complexity of isomorphism testing for nonabelian groups. In this paper we study this problem for a class of groups corresponding to one of the simplest ways of constructing nonabelian groups from abelian groups: the groups that are extensions of an abelian group A by a cyclic group of order m. We present an efficient algorithm solving the group isomorphism problem for all the groups of this class such that the order of A is coprime with m. More precisely, our algorithm runs in time almost linear in the orders of the input groups and works in the general setting where the groups are given as black-boxes.",
        "published": "2008-12-12T09:39:02Z",
        "link": "http://arxiv.org/abs/0812.2298v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "math.GR",
            "quant-ph"
        ]
    },
    {
        "title": "Approximating the least hypervolume contributor: NP-hard in general, but   fast in practice",
        "authors": [
            "Karl Bringmann",
            "Tobias Friedrich"
        ],
        "summary": "The hypervolume indicator is an increasingly popular set measure to compare the quality of two Pareto sets. The basic ingredient of most hypervolume indicator based optimization algorithms is the calculation of the hypervolume contribution of single solutions regarding a Pareto set. We show that exact calculation of the hypervolume contribution is #P-hard while its approximation is NP-hard. The same holds for the calculation of the minimal contribution. We also prove that it is NP-hard to decide whether a solution has the least hypervolume contribution. Even deciding whether the contribution of a solution is at most $(1+\\eps)$ times the minimal contribution is NP-hard. This implies that it is neither possible to efficiently find the least contributing solution (unless $P = NP$) nor to approximate it (unless $NP = BPP$).   Nevertheless, in the second part of the paper we present a fast approximation algorithm for this problem. We prove that for arbitrarily given $\\eps,\\delta>0$ it calculates a solution with contribution at most $(1+\\eps)$ times the minimal contribution with probability at least $(1-\\delta)$. Though it cannot run in polynomial time for all instances, it performs extremely fast on various benchmark datasets. The algorithm solves very large problem instances which are intractable for exact algorithms (e.g., 10000 solutions in 100 dimensions) within a few seconds.",
        "published": "2008-12-14T13:57:10Z",
        "link": "http://arxiv.org/abs/0812.2636v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Learning Low Rank Matrices from O(n) Entries",
        "authors": [
            "Raghunandan H. Keshavan",
            "Andrea Montanari",
            "Sewoong Oh"
        ],
        "summary": "How many random entries of an n by m, rank r matrix are necessary to reconstruct the matrix within an accuracy d? We address this question in the case of a random matrix with bounded rank, whereby the observed entries are chosen uniformly at random. We prove that, for any d>0, C(r,d)n observations are sufficient. Finally we discuss the question of reconstructing the matrix efficiently, and demonstrate through extensive simulations that this task can be accomplished in nPoly(log n) operations, for small rank.",
        "published": "2008-12-14T18:30:44Z",
        "link": "http://arxiv.org/abs/0812.2599v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Optimal Succinctness for Range Minimum Queries",
        "authors": [
            "Johannes Fischer"
        ],
        "summary": "For a static array A of n ordered objects, a range minimum query asks for the position of the minimum between two specified array indices. We show how to preprocess A into a scheme of size 2n+o(n) bits that allows to answer range minimum queries on A in constant time. This space is asymptotically optimal in the important setting where access to A is not permitted after the preprocessing step. Our scheme can be computed in linear time, using only n + o(n) additional bits at construction time. In interesting by-product is that we also improve on LCA-computation in BPS- or DFUDS-encoded trees.",
        "published": "2008-12-15T12:03:31Z",
        "link": "http://arxiv.org/abs/0812.2775v3",
        "categories": [
            "cs.DS",
            "E.1"
        ]
    },
    {
        "title": "The Violation Heap: A Relaxed Fibonacci-Like Heap",
        "authors": [
            "Amr Elmasry"
        ],
        "summary": "We give a priority queue that achieves the same amortized bounds as Fibonacci heaps. Namely, find-min requires O(1) worst-case time, insert, meld and decrease-key require O(1) amortized time, and delete-min requires $O(\\log n)$ amortized time. Our structure is simple and promises an efficient practical behavior when compared to other known Fibonacci-like heaps. The main idea behind our construction is to propagate rank updates instead of performing cascaded cuts following a decrease-key operation, allowing for a relaxed structure.",
        "published": "2008-12-15T16:16:58Z",
        "link": "http://arxiv.org/abs/0812.2851v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Minimax Trees in Linear Time",
        "authors": [
            "Pawel Gawrychowski",
            "Travis Gagie"
        ],
        "summary": "A minimax tree is similar to a Huffman tree except that, instead of minimizing the weighted average of the leaves' depths, it minimizes the maximum of any leaf's weight plus its depth. Golumbic (1976) introduced minimax trees and gave a Huffman-like, $\\Oh{n \\log n}$-time algorithm for building them. Drmota and Szpankowski (2002) gave another $\\Oh{n \\log n}$-time algorithm, which checks the Kraft Inequality in each step of a binary search. In this paper we show how Drmota and Szpankowski's algorithm can be made to run in linear time on a word RAM with (\\Omega (\\log n))-bit words. We also discuss how our solution applies to problems in data compression, group testing and circuit design.",
        "published": "2008-12-15T17:15:51Z",
        "link": "http://arxiv.org/abs/0812.2868v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Compressive sensing: a paradigm shift in signal processing",
        "authors": [
            "Olga Holtz"
        ],
        "summary": "We survey a new paradigm in signal processing known as \"compressive sensing\". Contrary to old practices of data acquisition and reconstruction based on the Shannon-Nyquist sampling principle, the new theory shows that it is possible to reconstruct images or signals of scientific interest accurately and even exactly from a number of samples which is far smaller than the desired resolution of the image/signal, e.g., the number of pixels in the image. This new technique draws from results in several fields of mathematics, including algebra, optimization, probability theory, and harmonic analysis. We will discuss some of the key mathematical ideas behind compressive sensing, as well as its implications to other fields: numerical analysis, information theory, theoretical computer science, and engineering.",
        "published": "2008-12-16T19:53:30Z",
        "link": "http://arxiv.org/abs/0812.3137v1",
        "categories": [
            "math.HO",
            "cs.DS",
            "cs.NA",
            "math.NA",
            "math.OC",
            "90C05, 90C25, 65F50, 94A08, 94A20, 68P30, 65Y20"
        ]
    },
    {
        "title": "Algorithmic and Statistical Challenges in Modern Large-Scale Data   Analysis are the Focus of MMDS 2008",
        "authors": [
            "Michael W. Mahoney",
            "Lek-Heng Lim",
            "Gunnar E. Carlsson"
        ],
        "summary": "The 2008 Workshop on Algorithms for Modern Massive Data Sets (MMDS 2008), sponsored by the NSF, DARPA, LinkedIn, and Yahoo!, was held at Stanford University, June 25--28. The goals of MMDS 2008 were (1) to explore novel techniques for modeling and analyzing massive, high-dimensional, and nonlinearly-structured scientific and internet data sets; and (2) to bring together computer scientists, statisticians, mathematicians, and data analysis practitioners to promote cross-fertilization of ideas.",
        "published": "2008-12-19T03:53:03Z",
        "link": "http://arxiv.org/abs/0812.3702v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Pancake Flipping with Two Spatulas",
        "authors": [
            "Masud Hasan",
            "Atif Rahman",
            "M. Sohel Rahman",
            "Mahfuza Sharmin",
            "Rukhsana Yeasmin"
        ],
        "summary": "In this paper we study several variations of the \\emph{pancake flipping problem}, which is also well known as the problem of \\emph{sorting by prefix reversals}. We consider the variations in the sorting process by adding with prefix reversals other similar operations such as prefix transpositions and prefix transreversals. These type of sorting problems have applications in interconnection networks and computational biology. We first study the problem of sorting unsigned permutations by prefix reversals and prefix transpositions and present a 3-approximation algorithm for this problem. Then we give a 2-approximation algorithm for sorting by prefix reversals and prefix transreversals. We also provide a 3-approximation algorithm for sorting by prefix reversals and prefix transpositions where the operations are always applied at the unsorted suffix of the permutation. We further analyze the problem in more practical way and show quantitatively how approximation ratios of our algorithms improve with the increase of number of prefix reversals applied by optimal algorithms. Finally, we present experimental results to support our analysis.",
        "published": "2008-12-20T03:10:11Z",
        "link": "http://arxiv.org/abs/0812.3933v2",
        "categories": [
            "cs.DS",
            "cs.OH"
        ]
    },
    {
        "title": "Comparing RNA structures using a full set of biologically relevant edit   operations is intractable",
        "authors": [
            "Guillaume Blin",
            "Sylvie Hamel",
            "Stéphane Vialette"
        ],
        "summary": "Arc-annotated sequences are useful for representing structural information of RNAs and have been extensively used for comparing RNA structures in both terms of sequence and structural similarities. Among the many paradigms referring to arc-annotated sequences and RNA structures comparison (see \\cite{IGMA_BliDenDul08} for more details), the most important one is the general edit distance. The problem of computing an edit distance between two non-crossing arc-annotated sequences was introduced in \\cite{Evans99}. The introduced model uses edit operations that involve either single letters or pairs of letters (never considered separately) and is solvable in polynomial-time \\cite{ZhangShasha:1989}. To account for other possible RNA structural evolutionary events, new edit operations, allowing to consider either silmutaneously or separately letters of a pair were introduced in \\cite{jiangli}; unfortunately at the cost of computational tractability. It has been proved that comparing two RNA secondary structures using a full set of biologically relevant edit operations is {\\sf\\bf NP}-complete. Nevertheless, in \\cite{DBLP:conf/spire/GuignonCH05}, the authors have used a strong combinatorial restriction in order to compare two RNA stem-loops with a full set of biologically relevant edit operations; which have allowed them to design a polynomial-time and space algorithm for comparing general secondary RNA structures. In this paper we will prove theoretically that comparing two RNA structures using a full set of biologically relevant edit operations cannot be done without strong combinatorial restrictions.",
        "published": "2008-12-20T08:04:25Z",
        "link": "http://arxiv.org/abs/0812.3946v1",
        "categories": [
            "cs.DS",
            "q-bio.QM"
        ]
    },
    {
        "title": "Multi-level algorithms for modularity clustering",
        "authors": [
            "Andreas Noack",
            "Randolf Rotta"
        ],
        "summary": "Modularity is one of the most widely used quality measures for graph clusterings. Maximizing modularity is NP-hard, and the runtime of exact algorithms is prohibitive for large graphs. A simple and effective class of heuristics coarsens the graph by iteratively merging clusters (starting from singletons), and optionally refines the resulting clustering by iteratively moving individual vertices between clusters. Several heuristics of this type have been proposed in the literature, but little is known about their relative performance.   This paper experimentally compares existing and new coarsening- and refinement-based heuristics with respect to their effectiveness (achieved modularity) and efficiency (runtime). Concerning coarsening, it turns out that the most widely used criterion for merging clusters (modularity increase) is outperformed by other simple criteria, and that a recent algorithm by Schuetz and Caflisch is no improvement over simple greedy coarsening for these criteria. Concerning refinement, a new multi-level algorithm is shown to produce significantly better clusterings than conventional single-level algorithms. A comparison with published benchmark results and algorithm implementations shows that combinations of coarsening and multi-level refinement are competitive with the best algorithms in the literature.",
        "published": "2008-12-22T15:32:10Z",
        "link": "http://arxiv.org/abs/0812.4073v2",
        "categories": [
            "cs.DS",
            "cond-mat.stat-mech",
            "cs.DM",
            "physics.soc-ph",
            "G.2.2; G.2.3; I.5.3"
        ]
    },
    {
        "title": "An Improved Approximation Algorithm for the Column Subset Selection   Problem",
        "authors": [
            "Christos Boutsidis",
            "Michael W. Mahoney",
            "Petros Drineas"
        ],
        "summary": "We consider the problem of selecting the best subset of exactly $k$ columns from an $m \\times n$ matrix $A$. We present and analyze a novel two-stage algorithm that runs in $O(\\min\\{mn^2,m^2n\\})$ time and returns as output an $m \\times k$ matrix $C$ consisting of exactly $k$ columns of $A$. In the first (randomized) stage, the algorithm randomly selects $\\Theta(k \\log k)$ columns according to a judiciously-chosen probability distribution that depends on information in the top-$k$ right singular subspace of $A$. In the second (deterministic) stage, the algorithm applies a deterministic column-selection procedure to select and return exactly $k$ columns from the set of columns selected in the first stage. Let $C$ be the $m \\times k$ matrix containing those $k$ columns, let $P_C$ denote the projection matrix onto the span of those columns, and let $A_k$ denote the best rank-$k$ approximation to the matrix $A$. Then, we prove that, with probability at least 0.8, $$ \\FNorm{A - P_CA} \\leq \\Theta(k \\log^{1/2} k) \\FNorm{A-A_k}. $$ This Frobenius norm bound is only a factor of $\\sqrt{k \\log k}$ worse than the best previously existing existential result and is roughly $O(\\sqrt{k!})$ better than the best previous algorithmic result for the Frobenius norm version of this Column Subset Selection Problem (CSSP). We also prove that, with probability at least 0.8, $$ \\TNorm{A - P_CA} \\leq \\Theta(k \\log^{1/2} k)\\TNorm{A-A_k} + \\Theta(k^{3/4}\\log^{1/4}k)\\FNorm{A-A_k}. $$ This spectral norm bound is not directly comparable to the best previously existing bounds for the spectral norm version of this CSSP. Our bound depends on $\\FNorm{A-A_k}$, whereas previous results depend on $\\sqrt{n-k}\\TNorm{A-A_k}$; if these two quantities are comparable, then our bound is asymptotically worse by a $(k \\log k)^{1/4}$ factor.",
        "published": "2008-12-22T21:16:55Z",
        "link": "http://arxiv.org/abs/0812.4293v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "An $O(k^{3} log n)$-Approximation Algorithm for Vertex-Connectivity   Survivable Network Design",
        "authors": [
            "Julia Chuzhoy",
            "Sanjeev Khanna"
        ],
        "summary": "In the Survivable Network Design problem (SNDP), we are given an undirected graph $G(V,E)$ with costs on edges, along with a connectivity requirement $r(u,v)$ for each pair $u,v$ of vertices. The goal is to find a minimum-cost subset $E^*$ of edges, that satisfies the given set of pairwise connectivity requirements. In the edge-connectivity version we need to ensure that there are $r(u,v)$ edge-disjoint paths for every pair $u, v$ of vertices, while in the vertex-connectivity version the paths are required to be vertex-disjoint. The edge-connectivity version of SNDP is known to have a 2-approximation. However, no non-trivial approximation algorithm has been known so far for the vertex version of SNDP, except for special cases of the problem. We present an extremely simple algorithm to achieve an $O(k^3 \\log n)$-approximation for this problem, where $k$ denotes the maximum connectivity requirement, and $n$ denotes the number of vertices. We also give a simple proof of the recently discovered $O(k^2 \\log n)$-approximation result for the single-source version of vertex-connectivity SNDP. We note that in both cases, our analysis in fact yields slightly better guarantees in that the $\\log n$ term in the approximation guarantee can be replaced with a $\\log \\tau$ term where $\\tau$ denotes the number of distinct vertices that participate in one or more pairs with a positive connectivity requirement.",
        "published": "2008-12-23T19:04:25Z",
        "link": "http://arxiv.org/abs/0812.4442v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Random Projections for the Nonnegative Least-Squares Problem",
        "authors": [
            "Christos Boutsidis",
            "Petros Drineas"
        ],
        "summary": "Constrained least-squares regression problems, such as the Nonnegative Least Squares (NNLS) problem, where the variables are restricted to take only nonnegative values, often arise in applications. Motivated by the recent development of the fast Johnson-Lindestrauss transform, we present a fast random projection type approximation algorithm for the NNLS problem. Our algorithm employs a randomized Hadamard transform to construct a much smaller NNLS problem and solves this smaller problem using a standard NNLS solver. We prove that our approach finds a nonnegative solution vector that, with high probability, is close to the optimum nonnegative solution in a relative error approximation sense. We experimentally evaluate our approach on a large collection of term-document data and verify that it does offer considerable speedups without a significant loss in accuracy. Our analysis is based on a novel random projection type result that might be of independent interest. In particular, given a tall and thin matrix $\\Phi \\in \\mathbb{R}^{n \\times d}$ ($n \\gg d$) and a vector $y \\in \\mathbb{R}^d$, we prove that the Euclidean length of $\\Phi y$ can be estimated very accurately by the Euclidean length of $\\tilde{\\Phi}y$, where $\\tilde{\\Phi}$ consists of a small subset of (appropriately rescaled) rows of $\\Phi$.",
        "published": "2008-12-24T16:43:22Z",
        "link": "http://arxiv.org/abs/0812.4547v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Almost stable matchings in constant time",
        "authors": [
            "Patrik Floréen",
            "Petteri Kaski",
            "Valentin Polishchuk",
            "Jukka Suomela"
        ],
        "summary": "We show that the ratio of matched individuals to blocking pairs grows linearly with the number of propose--accept rounds executed by the Gale--Shapley algorithm for the stable marriage problem. Consequently, the participants can arrive at an almost stable matching even without full information about the problem instance; for each participant, knowing only its local neighbourhood is enough. In distributed-systems parlance, this means that if each person has only a constant number of acceptable partners, an almost stable matching emerges after a constant number of synchronous communication rounds. This holds even if ties are present in the preference lists.   We apply our results to give a distributed $(2+\\epsilon)$-approximation algorithm for maximum-weight matching in bicoloured graphs and a centralised randomised constant-time approximation scheme for estimating the size of a stable matching.",
        "published": "2008-12-29T11:04:46Z",
        "link": "http://arxiv.org/abs/0812.4893v1",
        "categories": [
            "cs.DS",
            "cs.DC"
        ]
    },
    {
        "title": "Kronecker Graphs: An Approach to Modeling Networks",
        "authors": [
            "Jure Leskovec",
            "Deepayan Chakrabarti",
            "Jon Kleinberg",
            "Christos Faloutsos",
            "Zoubin Ghahramani"
        ],
        "summary": "How can we model networks with a mathematically tractable model that allows for rigorous analysis of network properties? Networks exhibit a long list of surprising properties: heavy tails for the degree distribution; small diameters; and densification and shrinking diameters over time. Most present network models either fail to match several of the above properties, are complicated to analyze mathematically, or both. In this paper we propose a generative model for networks that is both mathematically tractable and can generate networks that have the above mentioned properties. Our main idea is to use the Kronecker product to generate graphs that we refer to as \"Kronecker graphs\".   First, we prove that Kronecker graphs naturally obey common network properties. We also provide empirical evidence showing that Kronecker graphs can effectively model the structure of real networks.   We then present KronFit, a fast and scalable algorithm for fitting the Kronecker graph generation model to large real networks. A naive approach to fitting would take super- exponential time. In contrast, KronFit takes linear time, by exploiting the structure of Kronecker matrix multiplication and by using statistical simulation techniques.   Experiments on large real and synthetic networks show that KronFit finds accurate parameters that indeed very well mimic the properties of target networks. Once fitted, the model parameters can be used to gain insights about the network structure, and the resulting synthetic graphs can be used for null- models, anonymization, extrapolations, and graph summarization.",
        "published": "2008-12-29T13:22:23Z",
        "link": "http://arxiv.org/abs/0812.4905v2",
        "categories": [
            "stat.ML",
            "cs.DS",
            "physics.data-an",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Obtaining a Planar Graph by Vertex Deletion",
        "authors": [
            "Dániel Marx",
            "Ildikó Schlotter"
        ],
        "summary": "In the k-Apex problem the task is to find at most k vertices whose deletion makes the given graph planar. The graphs for which there exists a solution form a minor closed class of graphs, hence by the deep results of Robertson and Seymour, there is an O(n^3) time algorithm for every fixed value of k. However, the proof is extremely complicated and the constants hidden by the big-O notation are huge. Here we give a much simpler algorithm for this problem with quadratic running time, by iteratively reducing the input graph and then applying techniques for graphs of bounded treewidth.",
        "published": "2008-12-29T14:57:14Z",
        "link": "http://arxiv.org/abs/0812.4919v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A 7/9 - Approximation Algorithm for the Maximum Traveling Salesman   Problem",
        "authors": [
            "Katarzyna Paluch",
            "Marcin Mucha",
            "Aleksander Madry"
        ],
        "summary": "We give a 7/9 - Approximation Algorithm for the Maximum Traveling Salesman Problem.",
        "published": "2008-12-30T19:11:48Z",
        "link": "http://arxiv.org/abs/0812.5101v1",
        "categories": [
            "cs.GT",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Spam: It's Not Just for Inboxes and Search Engines! Making Hirsch   h-index Robust to Scientospam",
        "authors": [
            "Dimitrios Katsaros",
            "Leonidas Akritidis",
            "Panayiotis Bozanis"
        ],
        "summary": "What is the 'level of excellence' of a scientist and the real impact of his/her work upon the scientific thinking and practising? How can we design a fair, an unbiased metric -- and most importantly -- a metric robust to manipulation?",
        "published": "2008-01-02T13:06:37Z",
        "link": "http://arxiv.org/abs/0801.0386v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "On the relationship between the structural and socioacademic communities   of a coauthorship network",
        "authors": [
            "Marko A. Rodriguez",
            "Alberto Pepe"
        ],
        "summary": "This article presents a study that compares detected structural communities in a coauthorship network to the socioacademic characteristics of the scholars that compose the network. The coauthorship network was created from the bibliographic record of a multi-institution, interdisciplinary research group focused on the study of sensor networks and wireless communication. Four different community detection algorithms were employed to assign a structural community to each scholar in the network: leading eigenvector, walktrap, edge betweenness and spinglass. Socioacademic characteristics were gathered from the scholars and include such information as their academic department, academic affiliation, country of origin, and academic position. A Pearson's $\\chi^2$ test, with a simulated Monte Carlo, revealed that structural communities best represent groupings of individuals working in the same academic department and at the same institution. A generalization of this result suggests that, even in interdisciplinary, multi-institutional research groups, coauthorship is primarily driven by departmental and institutional affiliation.",
        "published": "2008-01-15T17:26:20Z",
        "link": "http://arxiv.org/abs/0801.2345v3",
        "categories": [
            "cs.DL",
            "physics.soc-ph",
            "H.3.7; G.2.2"
        ]
    },
    {
        "title": "Online-concordance \"Perekhresni stezhky\" (\"The Cross-Paths\"), a novel by   Ivan Franko",
        "authors": [
            "Solomiya Buk",
            "Andrij Rovenchak"
        ],
        "summary": "In the article, theoretical principles and practical realization for the compilation of the concordance to \"Perekhresni stezhky\" (\"The Cross-Paths\"), a novel by Ivan Franko, are described. Two forms for the context presentation are proposed. The electronic version of this lexicographic work is available online.",
        "published": "2008-01-21T17:41:57Z",
        "link": "http://arxiv.org/abs/0801.3239v1",
        "categories": [
            "cs.CL",
            "cs.DL"
        ]
    },
    {
        "title": "Knowledge management by wikis",
        "authors": [
            "Sander Spek"
        ],
        "summary": "Wikis provide a new way of collaboration and knowledge sharing. Wikis are software that allows users to work collectively on a web-based knowledge base. Wikis are characterised by a sense of anarchism, collaboration, connectivity, organic development and self-healing, and they rely on trust. We list several concerns about applying wikis in professional organisation. After these concerns are met, wikis can provide a progessive, new knowledge sharing and collaboration tool.",
        "published": "2008-02-06T08:01:48Z",
        "link": "http://arxiv.org/abs/0802.0745v1",
        "categories": [
            "cs.DL",
            "J.0"
        ]
    },
    {
        "title": "NCore: Architecture and Implementation of a Flexible, Collaborative   Digital Library",
        "authors": [
            "Dean B. Krafft",
            "Aaron Birkland",
            "Ellen J. Cramer"
        ],
        "summary": "NCore is an open source architecture and software platform for creating flexible, collaborative digital libraries. NCore was developed by the National Science Digital Library (NSDL) project, and it serves as the central technical infrastructure for NSDL. NCore consists of a central Fedora-based digital repository, a specific data model, an API, and a set of backend services and frontend tools that create a new model for collaborative, contributory digital libraries. This paper describes NCore, presents and analyzes its architecture, tools and services; and reports on the experience of NSDL in building and operating a major digital library on it over the past year and the experience of the Digital Library for Earth Systems Education in porting their existing digital library and tools to the NCore platform.",
        "published": "2008-03-10T21:06:31Z",
        "link": "http://arxiv.org/abs/0803.1500v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "The aDORe Federation Architecture",
        "authors": [
            "Herbert Van de Sompel",
            "Ryan Chute",
            "Patrick Hochstenbach"
        ],
        "summary": "The need to federate repositories emerges in two distinctive scenarios. In one scenario, scalability-related problems in the operation of a repository reach a point beyond which continued service requires parallelization and hence federation of the repository infrastructure. In the other scenario, multiple distributed repositories manage collections of interest to certain communities or applications, and federation is an approach to present a unified perspective across these repositories. The high-level, 3-Tier aDORe federation architecture can be used as a guideline to federate repositories in both cases. This paper describes the architecture, consisting of core interfaces for federated repositories in Tier-1, two shared infrastructure components in Tier-2, and a single-point of access to the federation in Tier-3. The paper also illustrates two large-scale deployments of the aDORe federation architecture: the aDORe Archive repository (over 100,000,000 digital objects) at the Los Alamos National Laboratory and the Ghent University Image Repository federation (multiple terabytes of image files).",
        "published": "2008-03-31T18:02:57Z",
        "link": "http://arxiv.org/abs/0803.4511v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Object Re-Use & Exchange: A Resource-Centric Approach",
        "authors": [
            "Carl Lagoze",
            "Herbert Van de Sompel",
            "Michael L. Nelson",
            "Simeon Warner",
            "Robert Sanderson",
            "Pete Johnston"
        ],
        "summary": "The OAI Object Reuse and Exchange (OAI-ORE) framework recasts the repository-centric notion of digital object to a bounded aggregation of Web resources. In this manner, digital library content is more integrated with the Web architecture, and thereby more accessible to Web applications and clients. This generalized notion of an aggregation that is independent of repository containment conforms more closely with notions in eScience and eScholarship, where content is distributed across multiple services and databases. We provide a motivation for the OAI-ORE project, review previous interoperability efforts, describe draft ORE specifications and report on promising results from early experimentation that illustrate improved interoperability and reuse of digital objects.",
        "published": "2008-04-14T21:02:00Z",
        "link": "http://arxiv.org/abs/0804.2273v1",
        "categories": [
            "cs.DL",
            "cs.NI",
            "C.2.3"
        ]
    },
    {
        "title": "Information Resources in High-Energy Physics: Surveying the Present   Landscape and Charting the Future Course",
        "authors": [
            "Anne Gentil-Beccot",
            "Salvatore Mele",
            "Annette Holtkamp",
            "Heath B. O'Connell",
            "Travis C. Brooks"
        ],
        "summary": "Access to previous results is of paramount importance in the scientific process. Recent progress in information management focuses on building e-infrastructures for the optimization of the research workflow, through both policy-driven and user-pulled dynamics. For decades, High-Energy Physics (HEP) has pioneered innovative solutions in the field of information management and dissemination. In light of a transforming information environment, it is important to assess the current usage of information resources by researchers and HEP provides a unique test-bed for this assessment. A survey of about 10% of practitioners in the field reveals usage trends and information needs. Community-based services, such as the pioneering arXiv and SPIRES systems, largely answer the need of the scientists, with a limited but increasing fraction of younger users relying on Google. Commercial services offered by publishers or database vendors are essentially unused in the field. The survey offers an insight into the most important features that users require to optimize their research workflow. These results inform the future evolution of information management in HEP and, as these researchers are traditionally ``early adopters'' of innovation in scholarly communication, can inspire developments of disciplinary repositories serving other communities.",
        "published": "2008-04-16T23:01:51Z",
        "link": "http://arxiv.org/abs/0804.2701v2",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Towards Usage-based Impact Metrics: - First Results from the MESUR   Project",
        "authors": [
            "Johan Bollen",
            "Herbert Van de Sompel",
            "Marko A. Rodriguez"
        ],
        "summary": "Scholarly usage data holds the potential to be used as a tool to study the dynamics of scholarship in real time, and to form the basis for the definition of novel metrics of scholarly impact. However, the formal groundwork to reliably and validly exploit usage data is lacking, and the exact nature, meaning and applicability of usage-based metrics is poorly understood. The MESUR project funded by the Andrew W. Mellon Foundation constitutes a systematic effort to define, validate and cross-validate a range of usage-based metrics of scholarly impact. MESUR has collected nearly 1 billion usage events as well as all associated bibliographic and citation data from significant publishers, aggregators and institutional consortia to construct a large-scale usage data reference set. This paper describes some major challenges related to aggregating and processing usage data, and discusses preliminary results obtained from analyzing the MESUR reference data set. The results confirm the intrinsic value of scholarly usage data, and support the feasibility of reliable and valid usage-based metrics of scholarly impact.",
        "published": "2008-04-23T19:23:44Z",
        "link": "http://arxiv.org/abs/0804.3791v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Specification of an extensible and portable file format for electronic   structure and crystallographic data",
        "authors": [
            "X. Gonze",
            "C. -O. Almbladh",
            "A. Cucca",
            "D. Caliste",
            "C. Freysoldt",
            "M. A. L. Marques",
            "V. Olevano",
            "Y. Pouillon",
            "M. J. Verstraete"
        ],
        "summary": "In order to allow different software applications, in constant evolution, to interact and exchange data, flexible file formats are needed. A file format specification for different types of content has been elaborated to allow communication of data for the software developed within the European Network of Excellence \"NANOQUANTA\", focusing on first-principles calculations of materials and nanosystems. It might be used by other software as well, and is described here in detail. The format relies on the NetCDF binary input/output library, already used in many different scientific communities, that provides flexibility as well as portability accross languages and platforms. Thanks to NetCDF, the content can be accessed by keywords, ensuring the file format is extensible and backward compatible.",
        "published": "2008-05-02T08:48:26Z",
        "link": "http://arxiv.org/abs/0805.0192v1",
        "categories": [
            "cs.DL",
            "cond-mat.mtrl-sci",
            "cs.DB"
        ]
    },
    {
        "title": "Disentangling Visibility and Self-Promotion Bias in the arXiv:astro-ph   Positional Citation Effect",
        "authors": [
            "J. P. Dietrich"
        ],
        "summary": "We established in an earlier study that articles listed at or near the top of the daily arXiv:astro-ph mailings receive on average significantly more citations than articles further down the list. In our earlier work we were not able to decide whether this positional citation effect was due to author self-promotion of intrinsically more citable papers or whether papers are cited more often simply because they are at the top of the astro-ph listing. Using new data we can now disentangle both effects. Based on their submission times we separate articles into a self-promoted sample and a sample of articles that achieved a high rank on astro-ph by chance and compare their citation distributions with those of articles on lower astro-ph positions. We find that the positional citation effect is a superposition of self-promotion and visibility bias.",
        "published": "2008-05-02T20:00:06Z",
        "link": "http://arxiv.org/abs/0805.0307v2",
        "categories": [
            "astro-ph",
            "cs.DL"
        ]
    },
    {
        "title": "Clustering of scientific citations in Wikipedia",
        "authors": [
            "Finn Aarup Nielsen"
        ],
        "summary": "The instances of templates in Wikipedia form an interesting data set of structured information. Here I focus on the cite journal template that is primarily used for citation to articles in scientific journals. These citations can be extracted and analyzed: Non-negative matrix factorization is performed on a (article x journal) matrix resulting in a soft clustering of Wikipedia articles and scientific journals, each cluster more or less representing a scientific topic.",
        "published": "2008-05-08T12:29:36Z",
        "link": "http://arxiv.org/abs/0805.1154v2",
        "categories": [
            "cs.DL",
            "cs.NE",
            "G.1.10; G.2.3; H.2.8"
        ]
    },
    {
        "title": "Semantic Analysis of Tag Similarity Measures in Collaborative Tagging   Systems",
        "authors": [
            "Ciro Cattuto",
            "Dominik Benz",
            "Andreas Hotho",
            "Gerd Stumme"
        ],
        "summary": "Social bookmarking systems allow users to organise collections of resources on the Web in a collaborative fashion. The increasing popularity of these systems as well as first insights into their emergent semantics have made them relevant to disciplines like knowledge extraction and ontology learning. The problem of devising methods to measure the semantic relatedness between tags and characterizing it semantically is still largely open. Here we analyze three measures of tag relatedness: tag co-occurrence, cosine similarity of co-occurrence distributions, and FolkRank, an adaptation of the PageRank algorithm to folksonomies. Each measure is computed on tags from a large-scale dataset crawled from the social bookmarking system del.icio.us. To provide a semantic grounding of our findings, a connection to WordNet (a semantic lexicon for the English language) is established by mapping tags into synonym sets of WordNet, and applying there well-known metrics of semantic similarity. Our results clearly expose different characteristics of the selected measures of relatedness, making them applicable to different subtasks of knowledge extraction such as synonym detection or discovery of concept hierarchies.",
        "published": "2008-05-14T14:10:02Z",
        "link": "http://arxiv.org/abs/0805.2045v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.5; G.2.2; H.1.2; H.1.m; H.5.3"
        ]
    },
    {
        "title": "Innovation in Scholarly Communication: Vision and Projects from   High-Energy Physics",
        "authors": [
            "Rolf-Dieter Heuer",
            "Annette Holtkamp",
            "Salvatore Mele"
        ],
        "summary": "Having always been at the forefront of information management and open access, High-Energy Physics (HEP) proves to be an ideal test-bed for innovations in scholarly communication including new information and communication technologies. Three selected topics of scholarly communication in High-Energy Physics are presented here: A new open access business model, SCOAP3, a world-wide sponsoring consortium for peer-reviewed HEP literature; the design, development and deployment of an e-infrastructure for information management; and the emerging debate on long-term preservation, re-use and (open) access to HEP data.",
        "published": "2008-05-18T17:38:14Z",
        "link": "http://arxiv.org/abs/0805.2739v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "LCSH, SKOS and Linked Data",
        "authors": [
            "Ed Summers",
            "Antoine Isaac",
            "Clay Redding",
            "Dan Krech"
        ],
        "summary": "A technique for converting Library of Congress Subject Headings MARCXML to Simple Knowledge Organization System (SKOS) RDF is described. Strengths of the SKOS vocabulary are highlighted, as well as possible points for extension, and the integration of other semantic web vocabularies such as Dublin Core. An application for making the vocabulary available as linked-data on the Web is also described.",
        "published": "2008-05-19T13:11:41Z",
        "link": "http://arxiv.org/abs/0805.2855v3",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.7"
        ]
    },
    {
        "title": "Universality of citation distributions: towards an objective measure of   scientific impact",
        "authors": [
            "Filippo Radicchi",
            "Santo Fortunato",
            "Claudio Castellano"
        ],
        "summary": "We study the distributions of citations received by a single publication within several disciplines, spanning broad areas of science. We show that the probability that an article is cited $c$ times has large variations between different disciplines, but all distributions are rescaled on a universal curve when the relative indicator $c_f=c/c_0$ is considered, where $c_0$ is the average number of citations per article for the discipline. In addition we show that the same universal behavior occurs when citation distributions of articles published in the same field, but in different years, are compared. These findings provide a strong validation of $c_f$ as an unbiased indicator for citation performance across disciplines and years. Based on this indicator, we introduce a generalization of the h-index suitable for comparing scientists working in different fields.",
        "published": "2008-06-05T13:55:51Z",
        "link": "http://arxiv.org/abs/0806.0974v2",
        "categories": [
            "physics.soc-ph",
            "cond-mat.stat-mech",
            "cs.DL",
            "physics.data-an"
        ]
    },
    {
        "title": "ICT, Community Memory and Technological Appropriation",
        "authors": [
            "Rodrigo Torrens",
            "Luis A. Nunez",
            "Raisa Urribarri"
        ],
        "summary": "The core mission of universities and higher education institutions is to make public the results of their work and to preserve the collective memory of the institution. This includes the effective use of information and communication technologies (ICT) to systematically compile academic and research achievements as well as disseminate effectively this accumulated knowledge to society at large. Current efforts in Latin America and Venezuela in particular, are limited but provide some valuable insights to pave the road to this important goal. The institutional repository of Universidad de Los Andes (ULA) in Venezuela (www.saber.ula.ve) is such an example of ICT usage to store, manage and disseminate digital material produced by our University. In this paper we elaborate on the overall process of promoting a culture of content creation, publishing and preservation within ULA.",
        "published": "2008-06-06T22:37:43Z",
        "link": "http://arxiv.org/abs/0806.1246v1",
        "categories": [
            "cs.DL",
            "cs.CY"
        ]
    },
    {
        "title": "Cross-concordances: terminology mapping and its effectiveness for   information retrieval",
        "authors": [
            "Philipp Mayr",
            "Vivien Petras"
        ],
        "summary": "The German Federal Ministry for Education and Research funded a major terminology mapping initiative, which found its conclusion in 2007. The task of this terminology mapping initiative was to organize, create and manage 'cross-concordances' between controlled vocabularies (thesauri, classification systems, subject heading lists) centred around the social sciences but quickly extending to other subject areas. 64 crosswalks with more than 500,000 relations were established. In the final phase of the project, a major evaluation effort to test and measure the effectiveness of the vocabulary mappings in an information system environment was conducted. The paper reports on the cross-concordance work and evaluation results.",
        "published": "2008-06-23T20:37:10Z",
        "link": "http://arxiv.org/abs/0806.3765v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Automatic Metadata Generation using Associative Networks",
        "authors": [
            "Marko A. Rodriguez",
            "Johan Bollen",
            "Herbert Van de Sompel"
        ],
        "summary": "In spite of its tremendous value, metadata is generally sparse and incomplete, thereby hampering the effectiveness of digital information services. Many of the existing mechanisms for the automated creation of metadata rely primarily on content analysis which can be costly and inefficient. The automatic metadata generation system proposed in this article leverages resource relationships generated from existing metadata as a medium for propagation from metadata-rich to metadata-poor resources. Because of its independence from content analysis, it can be applied to a wide variety of resource media types and is shown to be computationally inexpensive. The proposed method operates through two distinct phases. Occurrence and co-occurrence algorithms first generate an associative network of repository resources leveraging existing repository metadata. Second, using the associative network as a substrate, metadata associated with metadata-rich resources is propagated to metadata-poor resources by means of a discrete-form spreading activation algorithm. This article discusses the general framework for building associative networks, an algorithm for disseminating metadata through such networks, and the results of an experiment and validation of the proposed method using a standard bibliographic dataset.",
        "published": "2008-06-30T21:23:28Z",
        "link": "http://arxiv.org/abs/0807.0023v2",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.1; H.3.7"
        ]
    },
    {
        "title": "Astrophysics in S.Co.P.E",
        "authors": [
            "M. Brescia",
            "S. Cavuoti",
            "G. D'Angelo",
            "R. D'Abrusco",
            "C. Donalek",
            "N. Deniskina",
            "O. Laurino",
            "G. Longo"
        ],
        "summary": "S.Co.P.E. is one of the four projects funded by the Italian Government in order to provide Southern Italy with a distributed computing infrastructure for fundamental science. Beside being aimed at building the infrastructure, S.Co.P.E. is also actively pursuing research in several areas among which astrophysics and observational cosmology. We shortly summarize the most significant results obtained in the first two years of the project and related to the development of middleware and Data Mining tools for the Virtual Observatory.",
        "published": "2008-07-07T08:38:00Z",
        "link": "http://arxiv.org/abs/0807.0967v1",
        "categories": [
            "astro-ph",
            "cs.DL"
        ]
    },
    {
        "title": "Random drift versus selection in academic vocabulary: an evolutionary   analysis of published keywords",
        "authors": [
            "R. Alexander Bentley"
        ],
        "summary": "The evolution of vocabulary in academic publishing is characterized via keyword frequencies recorded the ISI Web of Science citations database. In four distinct case-studies, evolutionary analysis of keyword frequency change through time is compared to a model of random copying used as the null hypothesis, such that selection may be identified against it. The case studies from the physical sciences indicate greater selection in keyword choice than in the social sciences. Similar evolutionary analyses can be applied to a wide range of phenomena; wherever the popularity of multiple items through time has been recorded, as with web searches, or sales of popular music and books, for example.",
        "published": "2008-07-08T07:05:01Z",
        "link": "http://arxiv.org/abs/0807.1182v1",
        "categories": [
            "physics.soc-ph",
            "cs.DL"
        ]
    },
    {
        "title": "Eigenfactor : Does the Principle of Repeated Improvement Result in   Better Journal Impact Estimates than Raw Citation Counts?",
        "authors": [
            "Philip M. Davis"
        ],
        "summary": "Eigenfactor.org, a journal evaluation tool which uses an iterative algorithm to weight citations (similar to the PageRank algorithm used for Google) has been proposed as a more valid method for calculating the impact of journals. The purpose of this brief communication is to investigate whether the principle of repeated improvement provides different rankings of journals than does a simple unweighted citation count (the method used by ISI).",
        "published": "2008-07-17T01:01:59Z",
        "link": "http://arxiv.org/abs/0807.2678v3",
        "categories": [
            "cs.DL",
            "cs.DB"
        ]
    },
    {
        "title": "Approximating Document Frequency with Term Count Values",
        "authors": [
            "Martin Klein",
            "Michael L. Nelson"
        ],
        "summary": "For bounded datasets such as the TREC Web Track (WT10g) the computation of term frequency (TF) and inverse document frequency (IDF) is not difficult. However, when the corpus is the entire web, direct IDF calculation is impossible and values must instead be estimated. Most available datasets provide values for term count (TC) meaning the number of times a certain term occurs in the entire corpus. Intuitively this value is different from document frequency (DF), the number of documents (e.g., web pages) a certain term occurs in. We conduct a comparison study between TC and DF values within the Web as Corpus (WaC). We found a very strong correlation with Spearman's rho >0.8 (p<0.005) which makes us confident in claiming that for such recently created corpora the TC and DF values can be used interchangeably to compute IDF values. These results are useful for the generation of accurate lexical signatures based on the TF-IDF scheme.",
        "published": "2008-07-23T21:44:46Z",
        "link": "http://arxiv.org/abs/0807.3755v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.0"
        ]
    },
    {
        "title": "A Distributed Process Infrastructure for a Distributed Data Structure",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "The Resource Description Framework (RDF) is continuing to grow outside the bounds of its initial function as a metadata framework and into the domain of general-purpose data modeling. This expansion has been facilitated by the continued increase in the capacity and speed of RDF database repositories known as triple-stores. High-end RDF triple-stores can hold and process on the order of 10 billion triples. In an effort to provide a seamless integration of the data contained in RDF repositories, the Linked Data community is providing specifications for linking RDF data sets into a universal distributed graph that can be traversed by both man and machine. While the seamless integration of RDF data sets is important, at the scale of the data sets that currently exist and will ultimately grow to become, the \"download and index\" philosophy of the World Wide Web will not so easily map over to the Semantic Web. This essay discusses the importance of adding a distributed RDF process infrastructure to the current distributed RDF data structure.",
        "published": "2008-07-24T15:16:16Z",
        "link": "http://arxiv.org/abs/0807.3908v1",
        "categories": [
            "cs.AI",
            "cs.DL",
            "I.2.4; C.1.4"
        ]
    },
    {
        "title": "Use of Astronomical Literature - A Report on Usage Patterns",
        "authors": [
            "Edwin A. Henneken",
            "Michael J. Kurtz",
            "Alberto Accomazzi",
            "Carolyn S. Grant",
            "Donna Thompson",
            "Elizabeth Bohlen",
            "Stephen S. Murray"
        ],
        "summary": "In this paper we present a number of metrics for usage of the SAO/NASA Astrophysics Data System (ADS). Since the ADS is used by the entire astronomical community, these are indicative of how the astronomical literature is used. We will show how the use of the ADS has changed both quantitatively and qualitatively. We will also show that different types of users access the system in different ways. Finally, we show how use of the ADS has evolved over the years in various regions of the world.   The ADS is funded by NASA Grant NNG06GG68G.",
        "published": "2008-08-01T17:55:14Z",
        "link": "http://arxiv.org/abs/0808.0103v2",
        "categories": [
            "cs.DL",
            "astro-ph"
        ]
    },
    {
        "title": "Building a terminology network for search: the KoMoHe project",
        "authors": [
            "Philipp Mayr",
            "Vivien Petras"
        ],
        "summary": "The paper reports about results on the GESIS-IZ project \"Competence Center Modeling and Treatment of Semantic Heterogeneity\" (KoMoHe). KoMoHe supervised a terminology mapping effort, in which 'cross-concordances' between major controlled vocabularies were organized, created and managed. In this paper we describe the establishment and implementation of cross-concordances for search in a digital library (DL).",
        "published": "2008-08-04T22:11:45Z",
        "link": "http://arxiv.org/abs/0808.0518v1",
        "categories": [
            "cs.DL",
            "cs.DB"
        ]
    },
    {
        "title": "Comparing human and automatic thesaurus mapping approaches in the   agricultural domain",
        "authors": [
            "Boris Lauser",
            "Gudrun Johannsen",
            "Caterina Caracciolo",
            "Johannes Keizer",
            "Willem Robert van Hage",
            "Philipp Mayr"
        ],
        "summary": "Knowledge organization systems (KOS), like thesauri and other controlled vocabularies, are used to provide subject access to information systems across the web. Due to the heterogeneity of these systems, mapping between vocabularies becomes crucial for retrieving relevant information. However, mapping thesauri is a laborious task, and thus big efforts are being made to automate the mapping process. This paper examines two mapping approaches involving the agricultural thesaurus AGROVOC, one machine-created and one human created. We are addressing the basic question \"What are the pros and cons of human and automatic mapping and how can they complement each other?\" By pointing out the difficulties in specific cases or groups of cases and grouping the sample into simple and difficult types of mappings, we show the limitations of current automatic methods and come up with some basic recommendations on what approach to use when.",
        "published": "2008-08-16T10:10:21Z",
        "link": "http://arxiv.org/abs/0808.2246v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Author-choice open access publishing in the biological and medical   literature: a citation analysis",
        "authors": [
            "Philip M. Davis"
        ],
        "summary": "In this article, we analyze the citations to articles published in 11 biological and medical journals from 2003 to 2007 that employ author-choice open access models. Controlling for known explanatory predictors of citations, only 2 of the 11 journals show positive and significant open access effects. Analyzing all journals together, we report a small but significant increase in article citations of 17%. In addition, there is strong evidence to suggest that the open access advantage is declining by about 7% per year, from 32% in 2004 to 11% in 2007.",
        "published": "2008-08-18T17:02:59Z",
        "link": "http://arxiv.org/abs/0808.2428v3",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Confirmation Bias and the Open Access Advantage: Some Methodological   Suggestions for the Davis Citation Study",
        "authors": [
            "Stevan Harnad"
        ],
        "summary": "Davis (2008) analyzes citations from 2004-2007 in 11 biomedical journals. 15% of authors paid to make them Open Access (OA). The outcome is a significant OA citation Advantage, but a small one (21%). The author infers that the OA advantage has been shrinking yearly, but the data suggest the opposite. Further analyses are necessary:   (1) Not just author-choice (paid) OA but Free OA self-archiving needs to be taken into account rather than being counted as non-OA.   (2) proportion of OA articles per journal per year needs to be reported and taken into account.   (3) The Journal Impact Factor and the relation between the size of the OA Advantage article 'citation-bracket' need to be taken into account.   (4) The sample-size for the highest-impact, largest-sample journal analyzed, PNAS, is restricted and excluded from some of the analyses. The full PNAS dataset is needed.   (5) The interaction between OA and time, 2004-2007, is based on retrospective data from a June 2008 total cumulative citation count. The dates of both the cited articles and the citing articles need to be taken into account.   The author proposes that author self-selection bias for is the primary cause of the observed OA Advantage, but this study does not test this or of any of the other potential causal factors. The author suggests that paid OA is not worth the cost, per extra citation. But with OA self-archiving both the OA and the extra citations are free.",
        "published": "2008-08-25T03:36:14Z",
        "link": "http://arxiv.org/abs/0808.3296v2",
        "categories": [
            "cs.DL",
            "cs.DB"
        ]
    },
    {
        "title": "The first-mover advantage in scientific publication",
        "authors": [
            "M. E. J. Newman"
        ],
        "summary": "Mathematical models of the scientific citation process predict a strong \"first-mover\" effect under which the first papers in a field will, essentially regardless of content, receive citations at a rate enormously higher than papers published later. Moreover papers are expected to retain this advantage in perpetuity -- they should receive more citations indefinitely, no matter how many other papers are published after them. We test this conjecture against data from a selection of fields and in several cases find a first-mover effect of a magnitude similar to that predicted by the theory. Were we wearing our cynical hat today, we might say that the scientist who wants to become famous is better off -- by a wide margin -- writing a modest paper in next year's hottest field than an outstanding paper in this year's. On the other hand, there are some papers, albeit only a small fraction, that buck the trend and attract significantly more citations than theory predicts despite having relatively late publication dates. We suggest that papers of this kind, though they often receive comparatively few citations overall, are probably worthy of our attention.",
        "published": "2008-09-02T21:33:02Z",
        "link": "http://arxiv.org/abs/0809.0522v1",
        "categories": [
            "physics.soc-ph",
            "cs.DL",
            "cs.SI"
        ]
    },
    {
        "title": "How long should an astronomical paper be to increase its Impact?",
        "authors": [
            "Krzysztof Zbigniew Stanek"
        ],
        "summary": "Naively, one would expect longer papers to have larger impact (i.e., to be cited more). I tested this expectation by selecting all (~30,000) refereed papers from A&A, AJ, ApJ and MNRAS published between 2000 and 2004. These particular years were chosen so papers analyzed would not be too \"fresh\", but at the same time length of each article could be obtained via ADS. I find that indeed longer papers published in these four major astronomy journals are on average cited more, with a median number of citations increasing from 6 for articles 2-3 pages long to about 50 for articles ~50 pages long. I do however observe a significant \"Letters effect\", i.e. ApJ and A&A articles 4 pages long are cited more than articles 5-10 pages long. Also, the very few longest (>80 pages) papers are actually cited less than somewhat shorter papers. For individual journals, median citations per paper increase from 11 for ~9,300 A&A papers to 14 for ~5,300 MNRAS papers, 16 for ~2,550 AJ papers, and 20 for ~12,850 ApJ papers (including ApJ Letters and Supplement). I conclude with some semi-humorous career advice, directed especially at first-year graduate students.",
        "published": "2008-09-03T20:00:03Z",
        "link": "http://arxiv.org/abs/0809.0692v1",
        "categories": [
            "astro-ph",
            "cs.DL",
            "physics.soc-ph"
        ]
    },
    {
        "title": "A Simple Framework to Typify Social Bibliographic Communities",
        "authors": [
            "Christoph Schommer"
        ],
        "summary": "Social Communities in bibliographic databases exist since many years, researchers share common research interests, and work and publish together. A social community may vary in type and size, being fully connected between participating members or even more expressed by a consortium of small and individual members who play individual roles in it. In this work, we focus on social communities inside the bibliographic database DBLP and characterize communities through a simple typifying description model. Generally, we understand a publication as a transaction between the associated authors. The idea therefore is to concern with directed associative relationships among them, to decompose each pattern to its fundamental structure, and to describe the communities by expressive attributes. Finally, we argue that the decomposition supports the management of discovered structures towards the use of adaptive-incremental mind-maps.",
        "published": "2008-09-16T21:25:41Z",
        "link": "http://arxiv.org/abs/0809.2818v1",
        "categories": [
            "cs.DL",
            "cs.CG",
            "H.3.1; H.3.6"
        ]
    },
    {
        "title": "Correlation of Expert and Search Engine Rankings",
        "authors": [
            "Michael L. Nelson",
            "Martin Klein",
            "Manoranjan Magudamudi"
        ],
        "summary": "In previous research it has been shown that link-based web page metrics can be used to predict experts' assessment of quality. We are interested in a related question: do expert rankings of real-world entities correlate with search engine rankings of corresponding web resources? For example, each year US News & World Report publishes a list of (among others) top 50 graduate business schools. Does their expert ranking correlate with the search engine ranking of the URLs of those business schools? To answer this question we conducted 9 experiments using 8 expert rankings on a range of academic, athletic, financial and popular culture topics. We compared the expert rankings with the rankings in Google, Live Search (formerly MSN) and Yahoo (with list lengths of 10, 25, and 50). In 57 search engine vs. expert comparisons, only 1 strong and 4 moderate correlations were statistically significant. In 42 inter-search engine comparisons, only 2 strong and 4 moderate correlations were statistically significant. The correlations appeared to decrease with the size of the lists: the 3 strong correlations were for lists of 10, the 8 moderate correlations were for lists of 25, and no correlations were found for lists of 50.",
        "published": "2008-09-17T04:27:14Z",
        "link": "http://arxiv.org/abs/0809.2851v2",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "The decline in the concentration of citations, 1900-2007",
        "authors": [
            "Vincent Lariviere",
            "Yves Gingras",
            "Eric Archambault"
        ],
        "summary": "This paper challenges recent research (Evans, 2008) reporting that the concentration of cited scientific literature increases with the online availability of articles and journals. Using Thomson Reuters' Web of Science, the present paper analyses changes in the concentration of citations received (two- and five-year citation windows) by papers published between 1900 and 2005. Three measures of concentration are used: the percentage of papers that received at least one citation (cited papers); the percentage of papers needed to account for 20, 50 and 80 percent of the citations; and, the Herfindahl-Hirschman index. These measures are used for four broad disciplines: natural sciences and engineering, medical fields, social sciences, and the humanities. All these measures converge and show that, contrary to what was reported by Evans, the dispersion of citations is actually increasing.",
        "published": "2008-09-30T16:27:42Z",
        "link": "http://arxiv.org/abs/0809.5250v1",
        "categories": [
            "physics.soc-ph",
            "cs.DL"
        ]
    },
    {
        "title": "Peer-review in the Internet age",
        "authors": [
            "Pawel Sobkowicz"
        ],
        "summary": "The importance of peer-review in the scientific process can not be overestimated. Yet, due to increasing pressures of research and exponentially growing number of publications the task faced by the referees becomes ever more difficult. We discuss here a few possible improvements that would enable more efficient review of the scientific literature, using the growing Internet connectivity. In particular, a practical automated model for providing the referees with references to papers that might have strong relationship with the work under review, based on general network properties of citations is proposed.",
        "published": "2008-10-02T17:02:18Z",
        "link": "http://arxiv.org/abs/0810.0486v1",
        "categories": [
            "physics.soc-ph",
            "cs.DL"
        ]
    },
    {
        "title": "Assembling Actor-based Mind-Maps from Text Stream",
        "authors": [
            "Claudine Brucks",
            "Christoph Schommer"
        ],
        "summary": "For human beings, the processing of text streams of unknown size leads generally to problems because e.g. noise must be selected out, information be tested for its relevance or redundancy, and linguistic phenomenon like ambiguity or the resolution of pronouns be advanced. Putting this into simulation by using an artificial mind-map is a challenge, which offers the gate for a wide field of applications like automatic text summarization or punctual retrieval. In this work we present a framework that is a first step towards an automatic intellect. It aims at assembling a mind-map based on incoming text streams and on a subject-verb-object strategy, having the verb as an interconnection between the adjacent nouns. The mind-map's performance is enriched by a pronoun resolution engine that bases on the work of D. Klein, and C. D. Manning.",
        "published": "2008-10-25T16:00:08Z",
        "link": "http://arxiv.org/abs/0810.4616v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "I.2.4; H.3.1"
        ]
    },
    {
        "title": "On Granular Knowledge Structures",
        "authors": [
            "Yi Zeng",
            "Ning Zhong"
        ],
        "summary": "Knowledge plays a central role in human and artificial intelligence. One of the key characteristics of knowledge is its structured organization. Knowledge can be and should be presented in multiple levels and multiple views to meet people's needs in different levels of granularities and from different perspectives. In this paper, we stand on the view point of granular computing and provide our understanding on multi-level and multi-view of knowledge through granular knowledge structures (GKS). Representation of granular knowledge structures, operations for building granular knowledge structures and how to use them are investigated. As an illustration, we provide some examples through results from an analysis of proceeding papers. Results show that granular knowledge structures could help users get better understanding of the knowledge source from set theoretical, logical and visual point of views. One may consider using them to meet specific needs or solve certain kinds of problems.",
        "published": "2008-10-26T07:17:42Z",
        "link": "http://arxiv.org/abs/0810.4668v1",
        "categories": [
            "cs.AI",
            "cs.DL"
        ]
    },
    {
        "title": "Combining Advanced Visualization and Automatized Reasoning for   Webometrics: A Test Study",
        "authors": [
            "Claire François",
            "Jean-Charles Lamirel",
            "Shadi Al Shehabi"
        ],
        "summary": "This paper presents a first attempt at performing a precise and automatic identification of the linking behaviour in a scientific domain through the analysis of the communication of the related academic institutions on the web. The proposed approach is based on the paradigm of multiple viewpoint data analysis (MVDA) than can be fruitfully exploited to highlight relationships between data, like websites, carrying several kinds of description. It uses the MultiSOM clustering and mapping method. The domain that has been chosen for this study is the domain of Computer Science in Germany. The analysis is conduced on a set of 438 websites of this domain using all together, thematic, geographic and linking information. It highlights interesting results concerning both global and local linking behaviour.",
        "published": "2008-10-28T15:43:45Z",
        "link": "http://arxiv.org/abs/0810.5057v2",
        "categories": [
            "cs.IR",
            "cs.DL"
        ]
    },
    {
        "title": "A Web-Based Resource Model for eScience: Object Reuse & Exchange",
        "authors": [
            "Carl Lagoze",
            "Herbert Van de Sompel",
            "Michael Nelson",
            "Simeon Warner",
            "Robert Sanderson",
            "Pete Johnston"
        ],
        "summary": "Work in the Open Archives Initiative - Object Reuse and Exchange (OAI-ORE) focuses on an important aspect of infrastructure for eScience: the specification of the data model and a suite of implementation standards to identify and describe compound objects. These are objects that aggregate multiple sources of content including text, images, data, visualization tools, and the like. These aggregations are an essential product of eScience, and will become increasingly common in the age of data-driven scholarship. The OAI-ORE specifications conform to the core concepts of the Web architecture and the semantic Web, ensuring that applications that use them will integrate well into the general Web environment.",
        "published": "2008-11-04T19:07:36Z",
        "link": "http://arxiv.org/abs/0811.0573v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Visualization of association graphs for assisting the interpretation of   classifications",
        "authors": [
            "Eric San Juan",
            "Ivana Roche"
        ],
        "summary": "Given a query on the PASCAL database maintained by the INIST, we design user interfaces to visualize and browse two types of graphs extracted from abstracts: 1) the graph of all associations between authors (co-author graph), 2) the graph of strong associations between authors and terms automatically extracted from abstracts and grouped using linguistic variations. We adapt for this purpose the TermWatch system that comprises a term extractor, a relation identifier which yields the terminological network and a clustering module. The results are output on two interfaces: a graphic one mapping the clusters in a 2D space and a terminological hypertext network allowing the user to interactively explore results and return to source texts.",
        "published": "2008-11-05T12:58:38Z",
        "link": "http://arxiv.org/abs/0811.0717v1",
        "categories": [
            "stat.AP",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Origins of Modern Data Analysis Linked to the Beginnings and Early   Development of Computer Science and Information Engineering",
        "authors": [
            "Fionn Murtagh"
        ],
        "summary": "The history of data analysis that is addressed here is underpinned by two themes, -- those of tabular data analysis, and the analysis of collected heterogeneous data. \"Exploratory data analysis\" is taken as the heuristic approach that begins with data and information and seeks underlying explanation for what is observed or measured. I also cover some of the evolving context of research and applications, including scholarly publishing, technology transfer and the economic relationship of the university to society.",
        "published": "2008-11-15T18:41:34Z",
        "link": "http://arxiv.org/abs/0811.2519v1",
        "categories": [
            "cs.CY",
            "cs.DL"
        ]
    },
    {
        "title": "Collecting and Preserving Videogames and Their Related Materials: A   Review of Current Practice, Game-Related Archives and Research Projects",
        "authors": [
            "Megan A. Winget",
            "Caitlin Murray"
        ],
        "summary": "This paper reviews the major methods and theories regarding the preservation of new media artifacts such as videogames, and argues for the importance of collecting and coming to a better understanding of videogame artifacts of creation, which will help build a more detailed understanding of the essential qualities of these culturally significant artifacts. We will also review the major videogame collections in the United States, Europe and Japan to give an idea of the current state of videogame archives, and argue for a fuller, more comprehensive coverage of these materials in institutional repositories.",
        "published": "2008-11-19T15:36:13Z",
        "link": "http://arxiv.org/abs/0811.3137v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Anti Plagiarism Application with Algorithm Karp-Rabin at Thesis in   Gunadarma University",
        "authors": [
            "A. B. Mutiara",
            "S. Agustina"
        ],
        "summary": "Plagiarism that is plagiarizing or composition retrieval, opinion, etcetera from other people and makes it is likely composition and opinion him-self. Plagiarism can be considered to be crime because stealing others copyrights. Like action a student copying some part of writings without valid permission from the original writer. In education world, plagiarism perpetrator can get the devil to pay from school/university. Plagiarism perpetrator conceived of plagiator. This thing is possible unable to be paid attention by the side of campus because of limitation from some interconnected factors for example student amounts Gunadarma University reaching thousands and incommensurate to tester amounts or lecturer the side of campus in charge directs problem thesis. In this paper, an application have been developed in order to check and look for 5 type percentage similarity from a thesis with other one at certain part or chapters. Percentage got that is 0%, under 15%, between 15-50%, up to 50% and 100%. So it should be expected that the results could be used by thesis advisor and also thesis examiner from the Student at Gunadarma University.",
        "published": "2008-11-26T15:55:22Z",
        "link": "http://arxiv.org/abs/0811.4349v1",
        "categories": [
            "cs.IT",
            "cs.DL",
            "math.IT"
        ]
    },
    {
        "title": "Frozen Footprints",
        "authors": [
            "Massimo Franceschet"
        ],
        "summary": "Bibliometrics has the ambitious goal of measuring science. To this end, it exploits the way science is disseminated trough scientific publications and the resulting citation network of scientific papers. We survey the main historical contributions to the field, the most interesting bibliometric indicators, and the most popular bibliometric data sources. Moreover, we discuss distributions commonly used to model bibliometric phenomena and give an overview of methods to build bibliometric maps of science.",
        "published": "2008-11-27T18:12:28Z",
        "link": "http://arxiv.org/abs/0811.4603v2",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "An evaluation of Bradfordizing effects",
        "authors": [
            "Philipp Mayr"
        ],
        "summary": "The purpose of this paper is to apply and evaluate the bibliometric method Bradfordizing for information retrieval (IR) experiments. Bradfordizing is used for generating core document sets for subject-specific questions and to reorder result sets from distributed searches. The method will be applied and tested in a controlled scenario of scientific literature databases from social and political sciences, economics, psychology and medical science (SOLIS, SoLit, USB Koeln Opac, CSA Sociological Abstracts, World Affairs Online, Psyndex and Medline) and 164 standardized topics. An evaluation of the method and its effects is carried out in two laboratory-based information retrieval experiments (CLEF and KoMoHe) using a controlled document corpus and human relevance assessments. The results show that Bradfordizing is a very robust method for re-ranking the main document types (journal articles and monographs) in today's digital libraries (DL). The IR tests show that relevance distributions after re-ranking improve at a significant level if articles in the core are compared with articles in the succeeding zones. The items in the core are significantly more often assessed as relevant, than items in zone 2 (z2) or zone 3 (z3). The improvements between the zones are statistically significant based on the Wilcoxon signed-rank test and the paired T-Test.",
        "published": "2008-12-01T10:44:25Z",
        "link": "http://arxiv.org/abs/0812.0262v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Questions & Answers for TEI Newcomers",
        "authors": [
            "Laurent Romary"
        ],
        "summary": "This paper provides an introduction to the Text Encoding Initia-tive (TEI), focused at bringing in newcomers who have to deal with a digital document project and are looking at the capacity that the TEI environment may have to fulfil his needs. To this end, we avoid a strictly technical presentation of the TEI and concentrate on the actual issues that such projects face, with parallel made on the situation within two institutions. While a quick walkthrough the TEI technical framework is provided, the papers ends up by showing the essential role of the community in the actual technical contributions that are being brought to the TEI.",
        "published": "2008-12-18T15:51:17Z",
        "link": "http://arxiv.org/abs/0812.3563v2",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Tsallis $q$-exponential describes the distribution of scientific   citations - A new characterization of the impact",
        "authors": [
            "A. D. Anastasiadis",
            "Marcelo P. de Albuquerque",
            "Marcio P. de Albuquerque",
            "Diogo B. Mussi"
        ],
        "summary": "In this work we have studied the research activity for countries of Europe, Latin America and Africa for all sciences between 1945 and November 2008. All the data are captured from the Web of Science database during this period. The analysis of the experimental data shows that, within a nonextensive thermostatistical formalism, the Tsallis \\emph{q}-exponential distribution $N(c)$ satisfactorily describes Institute of Scientific Information citations. The data which are examined in the present survey can be fitted successfully as a first approach by applying a {\\it single} curve (namely, $N(c) \\propto 1/[1+(q-1) c/T]^{\\frac{1}{q-1}}$ with $q\\simeq 4/3$ for {\\it all} the available citations $c$, $T$ being an \"effective temperature\". The present analysis ultimately suggests that the phenomenon might essentially be {\\it one and the same} along the {\\it entire} range of the citation number. Finally, this manuscript provides a new ranking index, via the \"effective temperature\" $T$, for the impact level of the research activity in these countries, taking into account the number of the publications and their citations.",
        "published": "2008-12-22T21:34:10Z",
        "link": "http://arxiv.org/abs/0812.4296v1",
        "categories": [
            "cs.DL",
            "physics.data-an"
        ]
    },
    {
        "title": "Content-based and Algorithmic Classifications of Journals: Perspectives   on the Dynamics of Scientific Communication and Indexer Effects",
        "authors": [
            "Ismael Rafols",
            "Loet Leydesdorff"
        ],
        "summary": "The aggregated journal-journal citation matrix -based on the Journal Citation Reports (JCR) of the Science Citation Index- can be decomposed by indexers and/or algorithmically. In this study, we test the results of two recently available algorithms for the decomposition of large matrices against two content-based classifications of journals: the ISI Subject Categories and the field/subfield classification of Glaenzel & Schubert (2003). The content-based schemes allow for the attribution of more than a single category to a journal, whereas the algorithms maximize the ratio of within-category citations over between-category citations in the aggregated category-category citation matrix. By adding categories, indexers generate between-category citations, which may enrich the database, for example, in the case of inter-disciplinary developments. The consequent indexer effects are significant in sparse areas of the matrix more than in denser ones. Algorithmic decompositions, on the other hand, are more heavily skewed towards a relatively small number of categories, while this is deliberately counter-acted upon in the case of content-based classifications. Because of the indexer effects, science policy studies and the sociology of science should be careful when using content-based classifications, which are made for bibliographic disclosure, and not for the purpose of analyzing latent structures in scientific communications. Despite the large differences among them, the four classification schemes enable us to generate surprisingly similar maps of science at the global level. Erroneous classifications are cancelled as noise at the aggregate level, but may disturb the evaluation locally.",
        "published": "2008-12-23T03:45:51Z",
        "link": "http://arxiv.org/abs/0812.4332v1",
        "categories": [
            "physics.data-an",
            "cs.DL",
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Applying the CobiT Control Framework to Spreadsheet Developments",
        "authors": [
            "Raymond J. Butler"
        ],
        "summary": "One of the problems reported by researchers and auditors in the field of spreadsheet risks is that of getting and keeping managements attention to the problem. Since 1996, the Information Systems Audit & Control Foundation and the IT Governance Institute have published CobiT which brings mainstream IT control issues into the corporate governance arena. This paper illustrates how spreadsheet risk and control issues can be mapped onto the CobiT framework and thus brought to managers attention in a familiar format.",
        "published": "2008-01-03T22:09:15Z",
        "link": "http://arxiv.org/abs/0801.0609v1",
        "categories": [
            "cs.SE",
            "cs.CY",
            "J.1; H.4.1; K.6.4; D.2.9"
        ]
    },
    {
        "title": "Survey of Technologies for Web Application Development",
        "authors": [
            "Barry Doyle",
            "Cristina Videira Lopes"
        ],
        "summary": "Web-based application developers face a dizzying array of platforms, languages, frameworks and technical artifacts to choose from. We survey, classify, and compare technologies supporting Web application development. The classification is based on (1) foundational technologies; (2)integration with other information sources; and (3) dynamic content generation. We further survey and classify software engineering techniques and tools that have been adopted from traditional programming into Web programming. We conclude that, although the infrastructure problems of the Web have largely been solved, the cacophony of technologies for Web-based applications reflects the lack of a solid model tailored for this domain.",
        "published": "2008-01-17T05:06:44Z",
        "link": "http://arxiv.org/abs/0801.2618v1",
        "categories": [
            "cs.SE",
            "cs.IR",
            "cs.NI",
            "A.1; D.1.0; D.1.1; D.2.11; H.3.5; H.5.4"
        ]
    },
    {
        "title": "Comparison of Spreadsheets with other Development Tools (limitations,   solutions, workarounds and alternatives)",
        "authors": [
            "Simon Murphy"
        ],
        "summary": "The spreadsheet paradigm has some unique risks and challenges that are not present in more traditional development technologies. Many of the recent advances in other branches of software development have bypassed spreadsheets and spreadsheet developers. This paper compares spreadsheets and spreadsheet development to more traditional platforms such as databases and procedural languages. It also considers the fundamental danger introduced in the transition from paper spreadsheets to electronic. Suggestions are made to manage the risks and work around the limitations.",
        "published": "2008-01-24T21:40:17Z",
        "link": "http://arxiv.org/abs/0801.3853v1",
        "categories": [
            "cs.SE",
            "cs.CY",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "Increased security through open source",
        "authors": [
            "Jaap-Henk Hoepman",
            "Bart Jacobs"
        ],
        "summary": "In this paper we discuss the impact of open source on both the security and transparency of a software system. We focus on the more technical aspects of this issue, combining and extending arguments developed over the years. We stress that our discussion of the problem only applies to software for general purpose computing systems. For embedded systems, where the software usually cannot easily be patched or upgraded, different considerations may apply.",
        "published": "2008-01-25T12:06:48Z",
        "link": "http://arxiv.org/abs/0801.3924v1",
        "categories": [
            "cs.CR",
            "cs.CY",
            "cs.SE"
        ]
    },
    {
        "title": "Computational Models of Spreadsheet Development: Basis for Educational   Approaches",
        "authors": [
            "Karin Hodnigg",
            "Markus Clermont",
            "Roland T. Mittermeir"
        ],
        "summary": "Among the multiple causes of high error rates in spreadsheets, lack of proper training and of deep understanding of the computational model upon which spreadsheet computations rest might not be the least issue. The paper addresses this problem by presenting a didactical model focussing on cell interaction, thus exceeding the atomicity of cell computations. The approach is motivated by an investigation how different spreadsheet systems handle certain computational issues implied from moving cells, copy-paste operations, or recursion.",
        "published": "2008-01-28T13:55:55Z",
        "link": "http://arxiv.org/abs/0801.4274v1",
        "categories": [
            "cs.HC",
            "cs.SE"
        ]
    },
    {
        "title": "Spreadsheet Debugging",
        "authors": [
            "Yirsaw Ayalew",
            "Roland Mittermeir"
        ],
        "summary": "Spreadsheet programs, artifacts developed by non-programmers, are used for a variety of important tasks and decisions. Yet a significant proportion of them have severe quality problems. To address this issue, our previous work presented an interval-based testing methodology for spreadsheets. Interval-based testing rests on the observation that spreadsheets are mainly used for numerical computations. It also incorporates ideas from symbolic testing and interval analysis. This paper addresses the issue of efficiently debugging spreadsheets. Based on the interval-based testing methodology, this paper presents a technique for tracing faults in spreadsheet programs. The fault tracing technique proposed uses the dataflow information and cell marks to identify the most influential faulty cell(s) for a given formula cell containing a propagated fault.",
        "published": "2008-01-28T14:07:58Z",
        "link": "http://arxiv.org/abs/0801.4280v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "Source Code Protection for Applications Written in Microsoft Excel and   Google Spreadsheet",
        "authors": [
            "Thomas A. Grossman"
        ],
        "summary": "Spreadsheets are used to develop application software that is distributed to users. Unfortunately, the users often have the ability to change the programming statements (\"source code\") of the spreadsheet application. This causes a host of problems. By critically examining the suitability of spreadsheet computer programming languages for application development, six \"application development features\" are identified, with source code protection being the most important. We investigate the status of these features and discuss how they might be implemented in the dominant Microsoft Excel spreadsheet and in the new Google Spreadsheet. Although Google Spreadsheet currently provides no source code control, its web-centric delivery model offers technical advantages for future provision of a rich set of features. Excel has a number of tools that can be combined to provide \"pretty good protection\" of source code, but weak passwords reduce its robustness. User access to Excel source code must be considered a programmer choice rather than an attribute of the spreadsheet.",
        "published": "2008-01-30T21:35:17Z",
        "link": "http://arxiv.org/abs/0801.4774v1",
        "categories": [
            "cs.SE",
            "D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Spreadsheet Assurance by \"Control Around\" is a Viable Alternative to the   Traditional Approach",
        "authors": [
            "Harmen Ettema",
            "Paul Janssen",
            "Jacques de Swart"
        ],
        "summary": "The traditional approach to spreadsheet auditing generally consists of auditing every distinct formula within a spreadsheet. Although tools are developed to support auditors during this process, the approach is still very time consuming and therefore relatively expensive. As an alternative to the traditional \"control through\" approach, this paper discusses a \"control around\" approach. Within the proposed approach not all distinct formulas are audited separately, but the relationship between input data and output data of a spreadsheet is audited through comparison with a shadow model developed in a modelling language. Differences between the two models then imply possible errors in the spreadsheet. This paper describes relevant issues regarding the \"control around\" approach and the circumstances in which this approach is preferred above a traditional spreadsheet audit approach.",
        "published": "2008-01-30T21:53:43Z",
        "link": "http://arxiv.org/abs/0801.4775v1",
        "categories": [
            "cs.SE",
            "J.1; H.4.1; K.6.4; D.2.9"
        ]
    },
    {
        "title": "Investigating the Potential of Test-Driven Development for Spreadsheet   Engineering",
        "authors": [
            "Alan Rust",
            "Brian Bishop",
            "Kevin McDaid"
        ],
        "summary": "It is widely documented that the absence of a structured approach to spreadsheet engineering is a key factor in the high level of spreadsheet errors. In this paper we propose and investigate the application of Test-Driven Development to the creation of spreadsheets. Test-Driven Development is an emerging development technique in software engineering that has been shown to result in better quality software code. It has also been shown that this code requires less testing and is easier to maintain. Through a pair of case studies we demonstrate that Test-Driven Development can be applied to the development of spreadsheets. We present the detail of these studies preceded by a clear explanation of the technique and its application to spreadsheet engineering. A supporting tool under development by the authors is also documented along with proposed research to determine the effectiveness of the methodology and the associated tool.",
        "published": "2008-01-31T00:39:38Z",
        "link": "http://arxiv.org/abs/0801.4802v1",
        "categories": [
            "cs.SE",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "A topological formal treatment for scenario-based software specification   of concurrent real-time systems",
        "authors": [
            "Miriam C. B. Alves",
            "Christine C. Dantas",
            "Nanci N. Arai",
            "Rovedy B. da Silva"
        ],
        "summary": "Real-time systems are computing systems in which the meeting of their requirements is vital for their correctness. Consequently, if the real-time requirements of these systems are poorly understood and verified, the results can be disastrous and lead to irremediable project failures at the early phases of development. The present work addresses the problem of detecting deadlock situations early in the requirements specification phase of a concurrent real time system, proposing a simple proof-of-concepts prototype that joins scenario-based requirements specifications and techniques based on topology. The efforts are concentrated in the integration of the formal representation of Message Sequence Chart scenarios into the deadlock detection algorithm of Fajstrup et al., based on geometric and algebraic topology.",
        "published": "2008-02-01T22:12:47Z",
        "link": "http://arxiv.org/abs/0802.0212v1",
        "categories": [
            "cs.SE",
            "cs.LO"
        ]
    },
    {
        "title": "Algorithmically independent sequences",
        "authors": [
            "Cristian Calude",
            "Marius Zimand"
        ],
        "summary": "Two objects are independent if they do not affect each other. Independence is well-understood in classical information theory, but less in algorithmic information theory. Working in the framework of algorithmic information theory, the paper proposes two types of independence for arbitrary infinite binary sequences and studies their properties. Our two proposed notions of independence have some of the intuitive properties that one naturally expects. For example, for every sequence $x$, the set of sequences that are independent (in the weaker of the two senses) with $x$ has measure one. For both notions of independence we investigate to what extent pairs of independent sequences, can be effectively constructed via Turing reductions (from one or more input sequences). In this respect, we prove several impossibility results. For example, it is shown that there is no effective way of producing from an arbitrary sequence with positive constructive Hausdorff dimension two sequences that are independent (even in the weaker type of independence) and have super-logarithmic complexity. Finally, a few conjectures and open questions are discussed.",
        "published": "2008-02-04T20:32:07Z",
        "link": "http://arxiv.org/abs/0802.0487v1",
        "categories": [
            "cs.IT",
            "cs.SE",
            "math.AG",
            "math.IT"
        ]
    },
    {
        "title": "An Empirical Study of Cache-Oblivious Priority Queues and their   Application to the Shortest Path Problem",
        "authors": [
            "Benjamin Sach",
            "Raphaël Clifford"
        ],
        "summary": "In recent years the Cache-Oblivious model of external memory computation has provided an attractive theoretical basis for the analysis of algorithms on massive datasets. Much progress has been made in discovering algorithms that are asymptotically optimal or near optimal. However, to date there are still relatively few successful experimental studies. In this paper we compare two different Cache-Oblivious priority queues based on the Funnel and Bucket Heap and apply them to the single source shortest path problem on graphs with positive edge weights. Our results show that when RAM is limited and data is swapping to external storage, the Cache-Oblivious priority queues achieve orders of magnitude speedups over standard internal memory techniques. However, for the single source shortest path problem both on simulated and real world graph data, these speedups are markedly lower due to the time required to access the graph adjacency list itself.",
        "published": "2008-02-07T18:02:11Z",
        "link": "http://arxiv.org/abs/0802.1026v1",
        "categories": [
            "cs.DS",
            "cs.SE"
        ]
    },
    {
        "title": "Program Promises",
        "authors": [
            "Demissies Aredo",
            "Mark Burgess",
            "Simen Hagen"
        ],
        "summary": "The framework of promise theory offers an alternative way of understanding programming models, especially in distributed systems. We show that promise theory can express some familiar constructs and resolve some problems in program interface design, using fewer and simpler concepts than the Unified Modelling Language (UML).",
        "published": "2008-02-12T08:40:51Z",
        "link": "http://arxiv.org/abs/0802.1586v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Using Alloy to model-check visual design notations",
        "authors": [
            "Anthony J. H. Simons",
            "Carlos Alberto Fernandez-y-Fernandez"
        ],
        "summary": "This paper explores the process of validation for the abstract syntax of a graphical notation. We define an unified specification for five of the UML diagrams used by the Discovery Method and, in this document, we illustrate how diagrams can be represented in Alloy and checked against our specification in order to know if these are valid under the Discovery notation.",
        "published": "2008-02-15T18:25:50Z",
        "link": "http://arxiv.org/abs/0802.2258v1",
        "categories": [
            "cs.SE",
            "cs.SC",
            "I.6.4; D.3.1; I.3.5"
        ]
    },
    {
        "title": "Software graphs and programmer awareness",
        "authors": [
            "G. J. Baxter",
            "M. R. Frean"
        ],
        "summary": "Dependencies between types in object-oriented software can be viewed as directed graphs, with types as nodes and dependencies as edges. The in-degree and out-degree distributions of such graphs have quite different forms, with the former resembling a power-law distribution and the latter an exponential distribution. This effect appears to be independent of application or type relationship. A simple generative model is proposed to explore the proposition that the difference arises because the programmer is aware of the out-degree of a type but not of its in-degree. The model reproduces the two distributions, and compares reasonably well to those observed in 14 different type relationships across 12 different Java applications.",
        "published": "2008-02-16T03:38:03Z",
        "link": "http://arxiv.org/abs/0802.2306v1",
        "categories": [
            "cs.SE",
            "cs.PL"
        ]
    },
    {
        "title": "A New Approach to Spreadsheet Analytics Management in Financial Markets",
        "authors": [
            "Brian Sentence"
        ],
        "summary": "Spreadsheets in financial markets are frequently used as database, calculator and reporting application combined. This paper describes an alternative approach in which spreadsheet design and database technology have been brought together in order to alleviate management and regulatory concerns over the operational risks of spreadsheet usage. In particular, the paper focuses on the rapid creation and centralised deployment of statistical analytics within a software system now in use by major investment banks, and presents a novel technique for the manipulation in spreadsheets of high volumes of intraday market data.",
        "published": "2008-02-20T20:17:52Z",
        "link": "http://arxiv.org/abs/0802.2932v1",
        "categories": [
            "cs.SE",
            "cs.CY",
            "J.1; H.4.1; K.6.4; D.2.9"
        ]
    },
    {
        "title": "Spreadsheet Errors: What We Know. What We Think We Can Do",
        "authors": [
            "Raymond R. Panko"
        ],
        "summary": "Fifteen years of research studies have concluded unanimously that spreadsheet errors are both common and non-trivial. Now we must seek ways to reduce spreadsheet errors. Several approaches have been suggested, some of which are promising and others, while appealing because they are easy to do, are not likely to be effective. To date, only one technique, cell-by-cell code inspection, has been demonstrated to be effective. We need to conduct further research to determine the degree to which other techniques can reduce spreadsheet errors.",
        "published": "2008-02-23T19:48:15Z",
        "link": "http://arxiv.org/abs/0802.3457v1",
        "categories": [
            "cs.SE",
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "Spreadsheet Development Methodologies using Resolver: Moving   spreadsheets into the 21st Century",
        "authors": [
            "Patrick Kemmis",
            "Giles Thomas"
        ],
        "summary": "We intend to demonstrate the innate problems with existing spreadsheet products and to show how to tackle these issues using a new type of spreadsheet program called Resolver. It addresses the issues head-on and thereby moves the 1980's \"VisiCalc paradigm\" on to match the advances in computer languages and user requirements. Continuous display of the spreadsheet grid and the equivalent computer program, together with the ability to interact and add code through either interface, provides a number of new methodologies for spreadsheet development.",
        "published": "2008-02-24T01:16:52Z",
        "link": "http://arxiv.org/abs/0802.3475v1",
        "categories": [
            "cs.SE",
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.9"
        ]
    },
    {
        "title": "Fun Boy Three Were Wrong: it is what you do, not the way that you do it",
        "authors": [
            "Jocelyn Paine"
        ],
        "summary": "I revisit some classic publications on modularity, to show what problems its pioneers wanted to solve. These problems occur with spreadsheets too: to recognise them may help us avoid them.",
        "published": "2008-02-24T01:34:36Z",
        "link": "http://arxiv.org/abs/0802.3476v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "It Ain't What You View, But The Way That You View It: documenting   spreadsheets with Excelsior, semantic wikis, and literate programming",
        "authors": [
            "Jocelyn Paine"
        ],
        "summary": "I describe preliminary experiments in documenting Excelsior versions of spreadsheets using semantic wikis and literate programming. The objective is to create well-structured and comprehensive documentation, easy to use by those unfamiliar with the spreadsheets documented. I discuss why so much documentation is hard to use, and briefly explain semantic wikis and literate programming; although parts of the paper are Excelsior-specific, these sections may be of more general interest.",
        "published": "2008-02-24T01:57:01Z",
        "link": "http://arxiv.org/abs/0802.3478v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "J.1; H.4.1; K.6.4; D.2.9"
        ]
    },
    {
        "title": "Dynamic data models: an application of MOP-based persistence in Common   Lisp",
        "authors": [
            "Pierre Thierry",
            "Simon E. B. Thierry"
        ],
        "summary": "The data model of an application, the nature and format of data stored across executions, is typically a very rigid part of its early specification, even when prototyping, and changing it after code that relies on it was written can prove quite expensive and error-prone.   Code and data in a running Lisp image can be dynamically modified. A MOP-based persistence library can bring this dynamicity to the data model. This enables to extend the easy prototyping way of development to the storage of data and helps avoiding interruptions of service. This article presents the conditions to do this portably and transparently.",
        "published": "2008-02-25T14:20:18Z",
        "link": "http://arxiv.org/abs/0802.3628v1",
        "categories": [
            "cs.SE",
            "D.2.2; D.2.11; D.3.3; D.1.5; D.4.5; H.2.4"
        ]
    },
    {
        "title": "Knowledge Technologies",
        "authors": [
            "Nick Milton"
        ],
        "summary": "Several technologies are emerging that provide new ways to capture, store, present and use knowledge. This book is the first to provide a comprehensive introduction to five of the most important of these technologies: Knowledge Engineering, Knowledge Based Engineering, Knowledge Webs, Ontologies and Semantic Webs. For each of these, answers are given to a number of key questions (What is it? How does it operate? How is a system developed? What can it be used for? What tools are available? What are the main issues?). The book is aimed at students, researchers and practitioners interested in Knowledge Management, Artificial Intelligence, Design Engineering and Web Technologies.   During the 1990s, Nick worked at the University of Nottingham on the application of AI techniques to knowledge management and on various knowledge acquisition projects to develop expert systems for military applications. In 1999, he joined Epistemics where he worked on numerous knowledge projects and helped establish knowledge management programmes at large organisations in the engineering, technology and legal sectors. He is author of the book \"Knowledge Acquisition in Practice\", which describes a step-by-step procedure for acquiring and implementing expertise. He maintains strong links with leading research organisations working on knowledge technologies, such as knowledge-based engineering, ontologies and semantic technologies.",
        "published": "2008-02-26T11:26:09Z",
        "link": "http://arxiv.org/abs/0802.3789v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.LG",
            "cs.SE"
        ]
    },
    {
        "title": "Pattern-Oriented Analysis and Design (POAD) Theory",
        "authors": [
            "Jerry Overton"
        ],
        "summary": "Pattern-Oriented Analysis and Design (POAD) is the practice of building complex software by applying proven designs to specific problem domains. Although a great deal of research and practice has been devoted to formalizing existing design patterns and discovering new ones, there has been relatively little research into methods for combining these patterns into software applications. This is partly because the creation of complex software applications is so expensive. This paper proposes a mathematical model of POAD that may allow future research in pattern-oriented techniques to be performed using less expensive formal techniques rather than expensive, complex software development.",
        "published": "2008-02-26T13:51:39Z",
        "link": "http://arxiv.org/abs/0802.3784v1",
        "categories": [
            "cs.SE",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Spreadsheet Structure Discovery with Logic Programming",
        "authors": [
            "Jocelyn Paine"
        ],
        "summary": "Our term \"structure discovery\" denotes the recovery of structure, such as the grouping of cells, that was intended by a spreadsheet's author but is not explicit in the spreadsheet. We are implementing structure discovery tools in the logic-programming language Prolog for our spreadsheet analysis program Model Master, by writing grammars for spreadsheet structures. The objective is an \"intelligent structure monitor\" to run beside Excel, allowing users to reconfigure spreadsheets to the representational needs of the task at hand. This could revolutionise spreadsheet \"best practice\". We also describe a formulation of spreadsheet reverse-engineering based on \"arrows\".",
        "published": "2008-02-27T00:25:47Z",
        "link": "http://arxiv.org/abs/0802.3940v1",
        "categories": [
            "cs.SE",
            "D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Complexity Metrics for Spreadsheet Models",
        "authors": [
            "Andrej Bregar"
        ],
        "summary": "Several complexity metrics are described which are related to logic structure, data structure and size of spreadsheet models. They primarily concentrate on the dispersion of cell references and cell paths. Most metrics are newly defined, while some are adapted from traditional software engineering. Their purpose is the identification of cells which are liable to errors. In addition, they can be used to estimate the values of dependent process metrics, such as the development duration and effort, and especially to adjust the cell error rate in accordance with the contents of each individual cell, in order to accurately asses the reliability of a model. Finally, two conceptual constructs - the reference branching condition cell and the condition block - are discussed, aiming at improving the reliability, modifiability, auditability and comprehensibility of logical tests.",
        "published": "2008-02-27T00:37:56Z",
        "link": "http://arxiv.org/abs/0802.3895v1",
        "categories": [
            "cs.SE",
            "D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "EuSpRIG 2006 Commercial Spreadsheet Review",
        "authors": [
            "Simon Murphy"
        ],
        "summary": "This management summary provides an outline of a commercial spreadsheet review process. The aim of this process is to ensure remedial or enhancement work can safely be undertaken on a spreadsheet with a commercially acceptable level of risk of introducing new errors.",
        "published": "2008-02-29T21:49:18Z",
        "link": "http://arxiv.org/abs/0803.0015v1",
        "categories": [
            "cs.SE",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "A Software Development Methodology for Research and Prototyping in   Financial Markets",
        "authors": [
            "Andrew Kumiega",
            "Ben Van Vliet"
        ],
        "summary": "The objective of this paper is to develop a standardized methodology for software development in the very unique industry and culture of financial markets. The prototyping process we present allows the development team to deliver for review and comment intermediate-level models based upon clearly defined customer requirements. This spreadsheet development methodology is presented within a larger business context, that of trading system development, the subject of an upcoming book by the authors of this paper.",
        "published": "2008-03-03T01:05:40Z",
        "link": "http://arxiv.org/abs/0803.0162v1",
        "categories": [
            "cs.SE",
            "D.1.7; D.2.1; D.2.11; D.3.2; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Rapid Spreadsheet Reshaping with Excelsior: multiple drastic changes to   content and layout are easy when you represent enough structure",
        "authors": [
            "Jocelyn Paine",
            "Emre Tek",
            "Duncan Williamson"
        ],
        "summary": "Spreadsheets often need changing in ways made tedious and risky by Excel. For example: simultaneously altering many tables' size, orientation, and position; inserting cross-tabulations; moving data between sheets; splitting and merging sheets. A safer, faster restructuring tool is, we claim, Excelsior. The result of a research project into reducing spreadsheet risk, Excelsior is the first ever tool for modularising spreadsheets; i.e. for building them from components which can be independently created, tested, debugged, and updated. It represents spreadsheets in a way that makes these components explicit, separates them from layout, and allows both components and layout to be changed without breaking dependent formulae. Here, we report experiments to test that this does indeed make such changes easier. In one, we automatically generated a cross-tabulation and added it to a spreadsheet. In the other, we generated new versions of a 10,000-cell housing-finance spreadsheet containing many interconnected 20*40 tables. We varied table sizes from 5*10 to 200*2,000; moved tables between sheets; and flipped table orientations. Each change generated a spreadsheet with different structure but identical outputs; each change took just a few minutes.",
        "published": "2008-03-03T01:11:39Z",
        "link": "http://arxiv.org/abs/0803.0163v1",
        "categories": [
            "cs.SE",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "Considering Functional Spreadsheet Operator Usage Suggests the Value of   Example Driven Modelling for Decision Support Systems",
        "authors": [
            "Simon Thorne",
            "David Ball"
        ],
        "summary": "Most spreadsheet surveys both for reporting use and error focus on the practical application of the spreadsheet in a particular industry. Typically these studies will illustrate that a particular percentage of spreadsheets are used for optimisation and a further percentage are used for 'What if' analysis. Much less common is examining the classes of function, as defined by the vendor, used by modellers to build their spreadsheet models. This alternative analysis allows further insight into the programming nature of spreadsheets and may assist researchers in targeting particular structures in spreadsheet software for further investigation. Further, understanding the functional make-up of spreadsheets allows effective evaluation of novel approaches from a programming point of view. It allows greater insight into studies that report what spreadsheets are used for since it is explicit which functional structures are in use in spreadsheets. We conclude that a deeper understanding of the use of operators and the operator's relationship to error would provide fresh insight into the spreadsheet error problem. Considering functional spreadsheet operator usage suggests the value of Example Driven Modelling for Decision Support Systems",
        "published": "2008-03-03T01:25:41Z",
        "link": "http://arxiv.org/abs/0803.0164v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "An approach to control collaborative processes in PLM systems",
        "authors": [
            "Soumaya El Kadiri",
            "Philippe Pernelle",
            "Miguel Delattre",
            "Abdelaziz Bouras"
        ],
        "summary": "Companies that collaborate within the product development processes need to implement an effective management of their collaborative activities. Despite the implementation of a PLM system, the collaborative activities are not efficient as it might be expected. This paper presents an analysis of the problems related to the collaborative work using a PLM system. From this analysis, we propose an approach for improving collaborative processes within a PLM system, based on monitoring indicators. This approach leads to identify and therefore to mitigate the brakes of the collaborative work.",
        "published": "2008-03-05T14:04:19Z",
        "link": "http://arxiv.org/abs/0803.0666v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "A Computational Framework for the Near Elimination of Spreadsheet Risk",
        "authors": [
            "Yusuf Jafry",
            "Fredrika Sidoroff",
            "Roger Chi"
        ],
        "summary": "We present Risk Integrated's Enterprise Spreadsheet Platform (ESP), a technical approach to the near-elimination of spreadsheet risk in the enterprise computing environment, whilst maintaining the full flexibility of spreadsheets for modelling complex financial structures and processes. In its Basic Mode of use, the system comprises a secure and robust centralised spreadsheet management framework. In Advanced Mode, the system can be viewed as a robust computational framework whereby users can \"submit jobs\" to the spreadsheet, and retrieve the results from the computations, but with no direct access to the underlying spreadsheet. An example application, Monte Carlo simulation, is presented to highlight the benefits of this approach with regard to mitigating spreadsheet risk in complex, mission-critical, financial calculations.",
        "published": "2008-03-12T11:22:12Z",
        "link": "http://arxiv.org/abs/0803.1748v1",
        "categories": [
            "cs.SE",
            "cs.CY",
            "D.1.7; D.2.1; D.2.11; D.3.2; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "TellTable Spreadsheet Audit: from Technical Possibility to Operating   Prototype",
        "authors": [
            "John Nash",
            "Andy Adler",
            "Neil Smith"
        ],
        "summary": "At the 2003 EuSpRIG meeting, we presented a framework and software infrastructure to generate and analyse an audit trail for a spreadsheet file. This report describes the results of a pilot implementation of this software (now called TellTable; see www.telltable.com), along with developments in the server infrastructure and availability, extensions to other \"Office Suite\" files, integration of the audit tool into the server interface, and related developments, licensing and reports. We continue to seek collaborators and partners in what is primarily an open-source project with some shared-source components.",
        "published": "2008-03-12T11:36:41Z",
        "link": "http://arxiv.org/abs/0803.1751v1",
        "categories": [
            "cs.SE",
            "D.1.7; D.2.1; D.2.11; D.3.2; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Excelsior: Bringing the Benefits of Modularisation to Excel",
        "authors": [
            "Jocelyn Paine"
        ],
        "summary": "Excel lacks features for modular design. Had it such features, as do most programming languages, they would save time, avoid unneeded programming, make mistakes less likely, make code-control easier, help organisations adopt a uniform house style, and open business opportunities in buying and selling spreadsheet modules. I present Excelsior, a system for bringing these benefits to Excel.",
        "published": "2008-03-13T18:41:21Z",
        "link": "http://arxiv.org/abs/0803.2027v1",
        "categories": [
            "cs.SE",
            "D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Lissom, a Source Level Proof Carrying Code Platform",
        "authors": [
            "Joao Gomes",
            "Daniel Martins",
            "Simao Melo de Sousa",
            "Jorge Sousa Pinto"
        ],
        "summary": "This paper introduces a proposal for a Proof Carrying Code (PCC) architecture called Lissom. Started as a challenge for final year Computing students, Lissom was thought as a mean to prove to a sceptic community, and in particular to students, that formal verification tools can be put to practice in a realistic environment, and be used to solve complex and concrete problems. The attractiveness of the problems that PCC addresses has already brought students to show interest in this project.",
        "published": "2008-03-15T18:53:06Z",
        "link": "http://arxiv.org/abs/0803.2317v1",
        "categories": [
            "cs.LO",
            "cs.SE"
        ]
    },
    {
        "title": "Archiving: The Overlooked Spreadsheet Risk",
        "authors": [
            "Victoria Lemieux"
        ],
        "summary": "This paper maintains that archiving has been overlooked as a key spreadsheet internal control. The case of failed Jamaican commercial banks demonstrates how poor archiving can lead to weaknesses in spreadsheet control that contribute to operational risk. In addition, the Sarbanes-0xley Act contains a number of provisions that require tighter control over the archiving of spreadsheets. To mitigate operational risks and achieve compliance with the records-related provisions of Sarbanes-Oxley, the author argues that organisations should introduce records management programmes that provide control over the archiving of spreadsheets. At a minimum, spreadsheet archiving controls should identify and ensure compliance with retention requirements, support document production in the event of regulatory inquiries or litigation, and prevent unauthorised destruction of records.",
        "published": "2008-03-21T21:35:40Z",
        "link": "http://arxiv.org/abs/0803.3231v1",
        "categories": [
            "cs.SE",
            "cs.CY",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Structure and Interpretation of Computer Programs",
        "authors": [
            "Ganesh M. Narayan",
            "K. Gopinath",
            "V. Sridhar"
        ],
        "summary": "Call graphs depict the static, caller-callee relation between \"functions\" in a program. With most source/target languages supporting functions as the primitive unit of composition, call graphs naturally form the fundamental control flow representation available to understand/develop software. They are also the substrate on which various interprocedural analyses are performed and are integral part of program comprehension/testing. Given their universality and usefulness, it is imperative to ask if call graphs exhibit any intrinsic graph theoretic features -- across versions, program domains and source languages. This work is an attempt to answer these questions: we present and investigate a set of meaningful graph measures that help us understand call graphs better; we establish how these measures correlate, if any, across different languages and program domains; we also assess the overall, language independent software quality by suitably interpreting these measures.",
        "published": "2008-03-27T22:58:43Z",
        "link": "http://arxiv.org/abs/0803.4025v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.8; D.2.3; D.2.5; D.2.10"
        ]
    },
    {
        "title": "Merging Object and Process Diagrams for Business Information Modeling",
        "authors": [
            "Patrick Chénais"
        ],
        "summary": "While developing an information system for the University of Bern, we were faced with two major issues: managing software changes and adapting Business Information Models. Software techniques well-suited to software development teams exist, yet the models obtained are often too complex for the business user. We will first highlight the conceptual problems encountered while designing the Business Information Model. We will then propose merging class diagrams and business process modeling to achieve a necessary transparency. We will finally present a modeling tool we developed which, using pilot case studies, helps to show some of the advantages of a dual model approach.",
        "published": "2008-04-02T14:50:29Z",
        "link": "http://arxiv.org/abs/0804.0366v1",
        "categories": [
            "cs.SE",
            "D.2; D.2.1; D.2.2; K.6.3"
        ]
    },
    {
        "title": "Sarbanes-Oxley: What About all the Spreadsheets?",
        "authors": [
            "Raymond R. Panko",
            "Nicholas Ordway"
        ],
        "summary": "The Sarbanes-Oxley Act of 2002 has finally forced corporations to examine the validity of their spreadsheets. They are beginning to understand the spreadsheet error literature, including what it tells them about the need for comprehensive spreadsheet testing. However, controlling for fraud will require a completely new set of capabilities, and a great deal of new research will be needed to develop fraud control capabilities. This paper discusses the riskiness of spreadsheets, which can now be quantified to a considerable degree. It then discusses how to use control frameworks to reduce the dangers created by spreadsheets. It focuses especially on testing, which appears to be the most crucial element in spreadsheet controls.",
        "published": "2008-04-06T23:42:29Z",
        "link": "http://arxiv.org/abs/0804.0797v1",
        "categories": [
            "cs.SE",
            "cs.CY",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1; K.3; K.6.3"
        ]
    },
    {
        "title": "Reducing Overconfidence in Spreadsheet Development",
        "authors": [
            "Raymond R. Panko"
        ],
        "summary": "Despite strong evidence of widespread errors, spreadsheet developers rarely subject their spreadsheets to post-development testing to reduce errors. This may be because spreadsheet developers are overconfident in the accuracy of their spreadsheets. This conjecture is plausible because overconfidence is present in a wide variety of human cognitive domains, even among experts. This paper describes two experiments in overconfidence in spreadsheet development. The first is a pilot study to determine the existence of overconfidence. The second tests a manipulation to reduce overconfidence and errors. The manipulation is modestly successful, indicating that overconfidence reduction is a promising avenue to pursue.",
        "published": "2008-04-07T00:20:24Z",
        "link": "http://arxiv.org/abs/0804.0941v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "The Wall and The Ball: A Study of Domain Referent Spreadsheet Errors",
        "authors": [
            "Richard J. Irons"
        ],
        "summary": "The Cell Error Rate in simple spreadsheets averages about 2% to 5%. This CER has been measured in domain free environments. This paper compares the CERs occurring in domain free and applied domain tasks. The applied domain task requires the application of simple linear algebra to a costing problem. The results show that domain referent knowledge influences participants' approaches to spreadsheet creation and spreadsheet usage. The conclusion is that spreadsheet error making is influenced by domain knowledge and domain perception. Qualitative findings also suggest that spreadsheet error making is a part of overall human behaviour, and ought to be analyzed against this wider canvas.",
        "published": "2008-04-07T00:46:41Z",
        "link": "http://arxiv.org/abs/0804.0943v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "A classification of invasive patterns in AOP",
        "authors": [
            "Freddy Munoz",
            "Benoit Baudry",
            "Olivier Barais"
        ],
        "summary": "Aspect-Oriented Programming (AOP) improves modularity by encapsulating crosscutting concerns into aspects. Some mechanisms to compose aspects allow invasiveness as a mean to integrate concerns. Invasiveness means that AOP languages have unrestricted access to program properties. Such kind of languages are interesting because they allow performing complex operations and better introduce functionalities. In this report we present a classification of invasive patterns in AOP. This classification characterizes the aspects invasive behavior and allows developers to abstract about the aspect incidence over the program they crosscut.",
        "published": "2008-04-10T13:21:32Z",
        "link": "http://arxiv.org/abs/0804.1696v2",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Philosophical Smoke Signals: Theory and Practice in Information Systems   Design",
        "authors": [
            "David King",
            "Chris Kimble"
        ],
        "summary": "Although the gulf between the theory and practice in Information Systems is much lamented, few researchers have offered a way forward except through a number of (failed) attempts to develop a single systematic theory for Information Systems. In this paper, we encourage researchers to re-examine the practical consequences of their theoretical arguments. By examining these arguments we may be able to form a number of more rigorous theories of Information Systems, allowing us to draw theory and practice together without undertaking yet another attempt at the holy grail of a single unified systematic theory of Information Systems.",
        "published": "2008-04-17T16:46:55Z",
        "link": "http://arxiv.org/abs/0804.2852v1",
        "categories": [
            "cs.SE",
            "cs.GL",
            "K.6.3; I.0"
        ]
    },
    {
        "title": "Design and Implementation of a Tracer Driver: Easy and Efficient Dynamic   Analyses of Constraint Logic Programs",
        "authors": [
            "Ludovic Langevine",
            "Mireille Ducasse"
        ],
        "summary": "Tracers provide users with useful information about program executions. In this article, we propose a ``tracer driver''. From a single tracer, it provides a powerful front-end enabling multiple dynamic analysis tools to be easily implemented, while limiting the overhead of the trace generation. The relevant execution events are specified by flexible event patterns and a large variety of trace data can be given either systematically or ``on demand''. The proposed tracer driver has been designed in the context of constraint logic programming; experiments have been made within GNU-Prolog. Execution views provided by existing tools have been easily emulated with a negligible overhead. Experimental measures show that the flexibility and power of the described architecture lead to good performance. The tracer driver overhead is inversely proportional to the average time between two traced events. Whereas the principles of the tracer driver are independent of the traced programming language, it is best suited for high-level languages, such as constraint logic programming, where each traced execution event encompasses numerous low-level execution steps. Furthermore, constraint logic programming is especially hard to debug. The current environments do not provide all the useful dynamic analysis tools. They can significantly benefit from our tracer driver which enables dynamic analyses to be integrated at a very low cost.",
        "published": "2008-04-25T14:05:36Z",
        "link": "http://arxiv.org/abs/0804.4116v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5; D.3.2"
        ]
    },
    {
        "title": "The Complexity of Coverage",
        "authors": [
            "Krishnendu Chatterjee",
            "Luca de Alfaro",
            "Rupak Majumdar"
        ],
        "summary": "We study the problem of generating a test sequence that achieves maximal coverage for a reactive system under test. We formulate the problem as a repeated game between the tester and the system, where the system state space is partitioned according to some coverage criterion and the objective of the tester is to maximize the set of partitions (or coverage goals) visited during the game. We show the complexity of the maximal coverage problem for non-deterministic systems is PSPACE-complete, but is NP-complete for deterministic systems. For the special case of non-deterministic systems with a re-initializing ``reset'' action, which represent running a new test input on a re-initialized system, we show that the complexity is again co-NP-complete. Our proof technique for reset games uses randomized testing strategies that circumvent the exponentially large memory requirement in the deterministic case.",
        "published": "2008-04-29T04:26:08Z",
        "link": "http://arxiv.org/abs/0804.4525v1",
        "categories": [
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Plat_Forms -- a contest: The web development platform comparison",
        "authors": [
            "Lutz Prechelt"
        ],
        "summary": "\"Plat_Forms\" is a competition in which top-class teams of three programmers compete to implement the same requirements for a web-based system within 30 hours, each team using a different technology platform (Java EE, .NET, PHP, Perl, Python, or Ruby on Rails). The results will provide new insights into the real (rather than purported) pros, cons, and emergent properties of each platform. The evaluation will analyze many aspects of each solution, both external (usability, functionality, reliability, performance, etc.) and internal (structure, understandability, flexibility, etc.).",
        "published": "2008-05-06T06:53:01Z",
        "link": "http://arxiv.org/abs/0805.0650v1",
        "categories": [
            "cs.SE",
            "D.2"
        ]
    },
    {
        "title": "Modeling and verifying a broad array of network properties",
        "authors": [
            "Vladimir Filkov",
            "Zachary M. Saul",
            "Soumen Roy",
            "Raissa M. D'Souza",
            "Premkumar T. Devanbu"
        ],
        "summary": "Motivated by widely observed examples in nature, society and software, where groups of already related nodes arrive together and attach to an existing network, we consider network growth via sequential attachment of linked node groups, or graphlets. We analyze the simplest case, attachment of the three node V-graphlet, where, with probability alpha, we attach a peripheral node of the graphlet, and with probability (1-alpha), we attach the central node. Our analytical results and simulations show that tuning alpha produces a wide range in degree distribution and degree assortativity, achieving assortativity values that capture a diverse set of many real-world systems. We introduce a fifteen-dimensional attribute vector derived from seven well-known network properties, which enables comprehensive comparison between any two networks. Principal Component Analysis (PCA) of this attribute vector space shows a significantly larger coverage potential of real-world network properties by a simple extension of the above model when compared against a classic model of network growth.",
        "published": "2008-05-12T10:09:37Z",
        "link": "http://arxiv.org/abs/0805.1489v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.SE",
            "q-bio.QM",
            "stat.AP"
        ]
    },
    {
        "title": "Detecting Errors in Spreadsheets",
        "authors": [
            "Yirsaw Ayalew",
            "Markus Clermont",
            "Roland T. Mittermeir"
        ],
        "summary": "The paper presents two complementary strategies for identifying errors in spreadsheet programs. The strategies presented are grounded on the assumption that spreadsheets are software, albeit of a different nature than conventional procedural software. Correspondingly, strategies for identifying errors have to take into account the inherent properties of spreadsheets as much as they have to recognize that the conceptual models of 'spreadsheet programmers' differ from the conceptual models of conventional programmers. Nevertheless, nobody can and will write a spreadsheet, without having such a conceptual model in mind, be it of numeric nature or be it of geometrical nature focused on some layout.",
        "published": "2008-05-12T20:31:41Z",
        "link": "http://arxiv.org/abs/0805.1740v1",
        "categories": [
            "cs.SE",
            "D.1.7, D.2.1, D.2.11, D.3.2, D.3.3, H.4.1, K.6.4, K.8.1"
        ]
    },
    {
        "title": "State and history in operating systems",
        "authors": [
            "Victor Yodaiken"
        ],
        "summary": "A method of using recursive functions to describe state change is applied to process switching in UNIX-like operating systems.",
        "published": "2008-05-18T19:44:21Z",
        "link": "http://arxiv.org/abs/0805.2749v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Managing Critical Spreadsheets in a Compliant Environment",
        "authors": [
            "Soheil Saadat"
        ],
        "summary": "The use of uncontrolled financial spreadsheets can expose organizations to unacceptable business and compliance risks, including errors in the financial reporting process, spreadsheet misuse and fraud, or even significant operational errors. These risks have been well documented and thoroughly researched. With the advent of regulatory mandates such as SOX 404 and FDICIA in the U.S., and MiFID, Basel II and Combined Code in the UK and Europe, leading tax and audit firms are now recommending that organizations automate their internal controls over critical spreadsheets and other end-user computing applications, including Microsoft Access databases. At a minimum, auditors mandate version control, change control and access control for operational spreadsheets, with more advanced controls for critical financial spreadsheets. This paper summarises the key issues regarding the establishment and maintenance of control of Business Critical spreadsheets.",
        "published": "2008-05-27T20:28:10Z",
        "link": "http://arxiv.org/abs/0805.4211v1",
        "categories": [
            "cs.SE",
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "A Structured Methodology for Spreadsheet Modelling",
        "authors": [
            "Brian Knight",
            "David Chadwick",
            "Kamalesen Rajalingham"
        ],
        "summary": "In this paper, we discuss the problem of the software engineering of a class of business spreadsheet models. A methodology for structured software development is proposed, which is based on structured analysis of data, represented as Jackson diagrams. It is shown that this analysis allows a straightforward modularisation, and that individual modules may be represented with indentation in the block-structured form of structured programs. The benefits of structured format are discussed, in terms of comprehensibility, ease of maintenance, and reduction in errors. The capability of the methodology to provide a modular overview in the model is described, and examples are given. The potential for a reverse-engineering tool, to transform existing spreadsheet models is discussed.",
        "published": "2008-05-27T20:56:35Z",
        "link": "http://arxiv.org/abs/0805.4218v1",
        "categories": [
            "cs.SE",
            "D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Building Financial Accuracy into Spreadsheets",
        "authors": [
            "Andrew Hawker"
        ],
        "summary": "Students learning how to apply spreadsheets to accounting problems are not always well served by the built-in financial functions. Problems can arise because of differences between UK and US practice, through anomalies in the functions themselves, and because the promptings of Wizards' engender an attitude of filling in the blanks on the screen, and hoping for the best. Some examples of these problems are described, and suggestions are presented for ways of improving the situation. Principally, it is suggested that spreadsheet prompts and 'Help' screens should offer integrated guidance, covering some aspects of financial practice, as well as matters of spreadsheet technique.",
        "published": "2008-05-27T21:11:48Z",
        "link": "http://arxiv.org/abs/0805.4219v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Classification of Spreadsheet Errors",
        "authors": [
            "Kamalasen Rajalingham",
            "David R. Chadwick",
            "Brian Knight"
        ],
        "summary": "This paper describes a framework for a systematic classification of spreadsheet errors. This classification or taxonomy of errors is aimed at facilitating analysis and comprehension of the different types of spreadsheet errors. The taxonomy is an outcome of an investigation of the widespread problem of spreadsheet errors and an analysis of specific types of these errors. This paper contains a description of the various elements and categories of the classification and is supported by appropriate examples.",
        "published": "2008-05-27T21:27:42Z",
        "link": "http://arxiv.org/abs/0805.4224v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Risk Assessment For Spreadsheet Developments: Choosing Which Models to   Audit",
        "authors": [
            "Raymond J. Butler"
        ],
        "summary": "Errors in spreadsheet applications and models are alarmingly common (some authorities, with justification cite spreadsheets containing errors as the norm rather than the exception). Faced with this body of evidence, the auditor can be faced with a huge task - the temptation may be to launch code inspections for every spreadsheet in an organisation. This can be very expensive and time-consuming. This paper describes risk assessment based on the \"SpACE\" audit methodology used by H M Customs & Excise's tax inspectors. This allows the auditor to target resources on the spreadsheets posing the highest risk of error, and justify the deployment of those resources to managers and clients. Since the opposite of audit risk is audit assurance the paper also offers an overview of some elements of good practice in the use of spreadsheets in business.",
        "published": "2008-05-27T23:26:32Z",
        "link": "http://arxiv.org/abs/0805.4236v1",
        "categories": [
            "cs.SE",
            "cs.CY",
            "D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Pattern-based Model-to-Model Transformation: Long Version",
        "authors": [
            "Juan de Lara",
            "Esther Guerra"
        ],
        "summary": "We present a new, high-level approach for the specification of model-to-model transformations based on declarative patterns. These are (atomic or composite) constraints on triple graphs declaring the allowed or forbidden relationships between source and target models. In this way, a transformation is defined by specifying a set of triple graph constraints that should be satisfied by the result of the transformation.   The description of the transformation is then compiled into lower-level operational mechanisms to perform forward or backward transformations, as well as to establish mappings between two existent models. In this paper we study one of such mechanisms based on the generation of operational triple graph grammar rules. Moreover, we exploit deduction techniques at the specification level to generate more specialized constraints (preserving the specification semantics) reflecting pattern dependencies, from which additional rules can be derived.   This is an extended version of the paper submitted to ICGT'08, with additional definitions and proofs.",
        "published": "2008-05-30T12:48:16Z",
        "link": "http://arxiv.org/abs/0805.4745v1",
        "categories": [
            "cs.SE",
            "cs.DM",
            "cs.LO"
        ]
    },
    {
        "title": "GuiLiner: A Configurable and Extensible Graphical User Interface for   Scientific Analysis and Simulation Software",
        "authors": [
            "N. C. Manoukis",
            "E. C. Anderson"
        ],
        "summary": "The computer programs most users interact with daily are driven by a graphical user interface (GUI). However, many scientific applications are used with a command line interface (CLI) for the ease of development and increased flexibility this mode provides. Scientific application developers would benefit from being able to provide a GUI easily for their CLI programs, thus retaining the advantages of both modes of interaction. GuiLiner is a generic, extensible and flexible front-end designed to ``host'' a wide variety of data analysis or simulation programs. Scientific application developers who produce a correctly formatted XML file describing their program's options and some of its documentation can immediately use GuiLiner to produce a carefully implemented GUI for their analysis or simulation programs.",
        "published": "2008-06-02T15:57:55Z",
        "link": "http://arxiv.org/abs/0806.0314v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "D.2.2; H.1.2; I.3.6"
        ]
    },
    {
        "title": "Design Patterns for Complex Event Processing",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "Currently engineering efficient and successful event-driven applications based on the emerging Complex Event Processing (CEP) technology, is a laborious trial and error process. The proposed CEP design pattern approach should support CEP engineers in their design decisions to build robust and efficient CEP solutions with well understood tradeoffs and should enable an interdisciplinary and efficient communication process about successful CEP solutions in different application domains.",
        "published": "2008-06-06T07:39:49Z",
        "link": "http://arxiv.org/abs/0806.1100v1",
        "categories": [
            "cs.SE",
            "D.2; D.2.11; D.2.13; C.2.4"
        ]
    },
    {
        "title": "VPOET: Using a Distributed Collaborative Platform for Semantic Web   Applications",
        "authors": [
            "Mariano Rico",
            "David Camacho",
            "Oscar Corcho"
        ],
        "summary": "This paper describes a distributed collaborative wiki-based platform that has been designed to facilitate the development of Semantic Web applications. The applications designed using this platform are able to build semantic data through the cooperation of different developers and to exploit that semantic data. The paper shows a practical case study on the application VPOET, and how an application based on Google Gadgets has been designed to test VPOET and let human users exploit the semantic data created. This practical example can be used to show how different Semantic Web technologies can be integrated into a particular Web application, and how the knowledge can be cooperatively improved.",
        "published": "2008-06-09T00:39:08Z",
        "link": "http://arxiv.org/abs/0806.1361v1",
        "categories": [
            "cs.SE",
            "cs.NI"
        ]
    },
    {
        "title": "Cardinality heterogeneities in Web service composition: Issues and   solutions",
        "authors": [
            "M. Mrissa",
            "Ph. Thiran",
            "J-M. Jacquet",
            "D. Benslimane",
            "Z. Maamar"
        ],
        "summary": "Data exchanges between Web services engaged in a composition raise several heterogeneities. In this paper, we address the problem of data cardinality heterogeneity in a composition. Firstly, we build a theoretical framework to describe different aspects of Web services that relate to data cardinality, and secondly, we solve this problem by developing a solution for cardinality mediation based on constraint logic programming.",
        "published": "2008-06-11T09:05:21Z",
        "link": "http://arxiv.org/abs/0806.1816v1",
        "categories": [
            "cs.SE",
            "cs.DB"
        ]
    },
    {
        "title": "A Process Algebra Software Engineering Environment",
        "authors": [
            "B. Diertens"
        ],
        "summary": "In previous work we described how the process algebra based language PSF can be used in software engineering, using the ToolBus, a coordination architecture also based on process algebra, as implementation model. In this article we summarize that work and describe the software development process more formally by presenting the tools we use in this process in a CASE setting, leading to the PSF-ToolBus software engineering environment. We generalize the refine step in this environment towards a process algebra based software engineering workbench of which several instances can be combined to form an environment.",
        "published": "2008-06-17T09:20:54Z",
        "link": "http://arxiv.org/abs/0806.2730v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "In Pursuit of Spreadsheet Excellence",
        "authors": [
            "Grenville J. Croll"
        ],
        "summary": "The first fully-documented study into the quantitative impact of errors in operational spreadsheets identified an interesting anomaly. One of the five participating organisations involved in the study contributed a set of five spreadsheets of such quality that they set the organisation apart in a statistical sense. This virtuoso performance gave rise to a simple sampling test - The Clean Sheet Test - which can be used to objectively evaluate if an organisation is in control of the spreadsheets it is using in important processes such as financial reporting.",
        "published": "2008-06-22T00:14:09Z",
        "link": "http://arxiv.org/abs/0806.3536v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Interpolation in local theory extensions",
        "authors": [
            "Viorica Sofronie-Stokkermans"
        ],
        "summary": "In this paper we study interpolation in local extensions of a base theory. We identify situations in which it is possible to obtain interpolants in a hierarchical manner, by using a prover and a procedure for generating interpolants in the base theory as black-boxes. We present several examples of theory extensions in which interpolants can be computed this way, and discuss applications in verification, knowledge representation, and modular reasoning in combinations of local theories.",
        "published": "2008-06-27T15:51:02Z",
        "link": "http://arxiv.org/abs/0806.4553v2",
        "categories": [
            "cs.LO",
            "cs.SE",
            "F.4.1; F.3.1"
        ]
    },
    {
        "title": "Quantitative Paradigm of Software Reliability as Content Relevance",
        "authors": [
            "Yuri Arkhipkin"
        ],
        "summary": "This paper presents a quantitative approach to software reliability and content relevance definitions validated by the systems' potential reliability law.Thus it is argued for the unified math nature or quantitative paradigm of software reliability and content relevance.",
        "published": "2008-07-01T05:29:07Z",
        "link": "http://arxiv.org/abs/0807.0070v1",
        "categories": [
            "cs.SE",
            "cs.IR",
            "D.2.5; D.2.8; D.2.9; H.3.1; D.1.m"
        ]
    },
    {
        "title": "Increase of Software Safety",
        "authors": [
            "Arkadiy Khandjian"
        ],
        "summary": "New model of software safety is offered. Distribution of mistakes in program on stages of life cycle is researched. Study of ways of increase of reliability of software at help simulation program is leaded.",
        "published": "2008-07-01T13:53:44Z",
        "link": "http://arxiv.org/abs/0807.0161v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "A Counterexample Guided Abstraction-Refinement Framework for Markov   Decision Processes",
        "authors": [
            "Rohit Chadha",
            "Mahesh Viswanthan"
        ],
        "summary": "The main challenge in using abstractions effectively, is to construct a suitable abstraction for the system being verified. One approach that tries to address this problem is that of {\\it counterexample guided abstraction-refinement (CEGAR)}, wherein one starts with a coarse abstraction of the system, and progressively refines it, based on invalid counterexamples seen in prior model checking runs, until either an abstraction proves the correctness of the system or a valid counterexample is generated. While CEGAR has been successfully used in verifying non-probabilistic systems automatically, CEGAR has not been applied in the context of probabilistic systems. The main issues that need to be tackled in order to extend the approach to probabilistic systems is a suitable notion of ``counterexample'', algorithms to generate counterexamples, check their validity, and then automatically refine an abstraction based on an invalid counterexample. In this paper, we address these issues, and present a CEGAR framework for Markov Decision Processes.",
        "published": "2008-07-08T19:47:11Z",
        "link": "http://arxiv.org/abs/0807.1173v1",
        "categories": [
            "cs.SE",
            "cs.LO",
            "D.2.4"
        ]
    },
    {
        "title": "CPBVP: A Constraint-Programming Framework for Bounded Program   Verification",
        "authors": [
            "Hélène Collavizza",
            "Michel Rueher",
            "Pascal Van Hentenryck"
        ],
        "summary": "This paper studies how to verify the conformity of a program with its specification and proposes a novel constraint-programming framework for bounded program verification (CPBPV). The CPBPV framework uses constraint stores to represent the specification and the program and explores execution paths nondeterministically. The input program is partially correct if each constraint store so produced implies the post-condition. CPBPV does not explore spurious execution paths as it incrementally prunes execution paths early by detecting that the constraint store is not consistent. CPBPV uses the rich language of constraint programming to express the constraint store. Finally, CPBPV is parametrized with a list of solvers which are tried in sequence, starting with the least expensive and less general. Experimental results often produce orders of magnitude improvements over earlier approaches, running times being often independent of the variable domains. Moreover, CPBPV was able to detect subtle errors in some programs while other frameworks based on model checking have failed.",
        "published": "2008-07-15T14:18:43Z",
        "link": "http://arxiv.org/abs/0807.2383v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Reducing Spreadsheet Risk with FormulaDataSleuth",
        "authors": [
            "Bill Bekenn",
            "Ray Hooper"
        ],
        "summary": "A new MS Excel application has been developed which seeks to reduce the risks associated with the development, operation and auditing of Excel spreadsheets. FormulaDataSleuth provides a means of checking spreadsheet formulas and data as they are developed or used, enabling the users to identify actual or potential errors quickly and thereby halt their propagation. In this paper, we will describe, with examples, how the application works and how it can be applied to reduce the risks associated with Excel spreadsheets.",
        "published": "2008-07-18T16:00:34Z",
        "link": "http://arxiv.org/abs/0807.2997v1",
        "categories": [
            "cs.HC",
            "cs.SE"
        ]
    },
    {
        "title": "Audit and Change Analysis of Spreadsheets",
        "authors": [
            "John C. Nash",
            "Neil Smith",
            "Andy Adler"
        ],
        "summary": "Because spreadsheets have a large and growing importance in real-world work, their contents need to be controlled and validated. Generally spreadsheets have been difficult to verify, since data and executable information are stored together. Spreadsheet applications with multiple authors are especially difficult to verify, since controls over access are difficult to enforce. Facing similar problems, traditional software engineering has developed numerous tools and methodologies to control, verify and audit large applications with multiple developers. We present some tools we have developed to enable 1) the audit of selected, filtered, or all changes in a spreadsheet, that is, when a cell was changed, its original and new contents and who made the change, and 2) control of access to the spreadsheet file(s) so that auditing is trustworthy. Our tools apply to OpenOffice.org calc spreadsheets, which can generally be exchanged with Microsoft Excel.",
        "published": "2008-07-20T17:28:25Z",
        "link": "http://arxiv.org/abs/0807.3168v1",
        "categories": [
            "cs.HC",
            "cs.SE"
        ]
    },
    {
        "title": "Accuracy in Spreadsheet Modelling Systems",
        "authors": [
            "Thomas A. Grossman"
        ],
        "summary": "Accuracy in spreadsheet modelling systems can be reduced due to difficulties with the inputs, the model itself, or the spreadsheet implementation of the model. When the \"true\" outputs from the system are unknowable, accuracy is evaluated subjectively. Less than perfect accuracy can be acceptable depending on the purpose of the model, problems with inputs, or resource constraints. Users build modelling systems iteratively, and choose to allocate limited resources to the inputs, the model, the spreadsheet implementation, and to employing the system for business analysis. When making these choices, users can suffer from expectation bias and diagnosis bias. Existing research results tend to focus on errors in the spreadsheet implementation. Because industry has tolerance for system inaccuracy, errors in spreadsheet implementations may not be a serious concern. Spreadsheet productivity may be of more interest.",
        "published": "2008-07-20T20:29:38Z",
        "link": "http://arxiv.org/abs/0807.3183v1",
        "categories": [
            "cs.HC",
            "cs.SE"
        ]
    },
    {
        "title": "When, why and how to test spreadsheets",
        "authors": [
            "Louise Pryor"
        ],
        "summary": "Testing is a vital part of software development, and spreadsheets are like any other software in this respect. This paper discusses the testing of spreadsheets in the light of one practitioner's experience. It considers the concept of software testing and how it differs from reviewing, and describes when it might take place. Different types of testing are described, and some techniques for performing them presented. Some of the commonly encountered problems are discussed.",
        "published": "2008-07-20T21:10:34Z",
        "link": "http://arxiv.org/abs/0807.3187v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Interface Matching and Combining Techniques for Services Integration",
        "authors": [
            "Frédéric Le Mouël",
            "Noha Ibrahim",
            "Stéphane Frénot"
        ],
        "summary": "The development of many highly dynamic environments, like pervasive environments, introduces the possibility to use geographically close-related services. Dynamically integrating and unintegrating these services in running applications is a key challenge for this use. In this article, we classify service integration issues according to interfaces exported by services and internal combining techniques. We also propose a contextual integration service, IntegServ, and an interface, Integrable, for developing services.",
        "published": "2008-07-24T17:51:07Z",
        "link": "http://arxiv.org/abs/0807.3933v1",
        "categories": [
            "cs.OS",
            "cs.SE"
        ]
    },
    {
        "title": "Encapsulation theory fundamentals",
        "authors": [
            "Edmund Kirwan"
        ],
        "summary": "This paper proposes a theory of encapsulation, establishing a relationship between encapsulation and information hiding through the concept of potential structural complexity (P.S.C.), the maximum possible number of source code dependencies that can exist between program units in a software system. The P.S.C. of various, simple systems is examined in an attempt to demonstrate how P.S.C. changes as program units are encapsulated among different configurations of subsystems.",
        "published": "2008-07-26T08:55:36Z",
        "link": "http://arxiv.org/abs/0807.4224v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Research Challenges in Management and Compliance of Policies on the Web",
        "authors": [
            "Holger M. Kienle",
            "Hausi A. Müller"
        ],
        "summary": "In this paper we argue that policies are an increasing concern for organizations that are operating a web site. Examples of policies that are relevant in the domain of the web address issues such as privacy of personal data, accessibility for the disabled, user conduct, e-commerce, and intellectual property. Web site policies--and the overarching concept of web site governance--are cross-cutting concerns that have to be addressed and implemented at different levels (e.g., policy documents, legal statements, business processes, contracts, auditing, and software systems). For web sites, policies are also reflected in the legal statements that the web site posts, and in the behavior and features that the web site offers to its users. Both policies and software tend to evolve independently, but at the same time they both have to be kept in sync. This is a practical challenge for operators of web sites that is poorly addressed right now and is, we believe, a promising avenue for future research. In this paper, we discuss various challenges that policy poses for web sites with an emphasis on privacy and data protection and identify open issues for future research.",
        "published": "2008-07-30T18:40:28Z",
        "link": "http://arxiv.org/abs/0807.4912v1",
        "categories": [
            "cs.CY",
            "cs.SE"
        ]
    },
    {
        "title": "Integrating OPC Data into GSN Infrastructures",
        "authors": [
            "Olivier Passalacqua",
            "Eric Benoit",
            "Marc-Philippe Huget",
            "Patrice Moreaux"
        ],
        "summary": "This paper presents the design and the implementation of an interface software component between OLE for Process Control (OPC) formatted data and the Global Sensor Network (GSN) framework for management of data from sensors. This interface, named wrapper in the GSN context, communicates in Data Access mode with an OPC server and converts the received data to the internal GSN format, according to several temporal modes. This work is realized in the context of a Ph.D. Thesis about the control of distributed information fusion systems. The developed component allows the injection of OPC data, like measurements or industrial processes states information, into a distributed information fusion system deployed in a GSN framework. The component behaves as a client of the OPC server. Developed in Java and based on the Opensaca Utgard, it can be deployed on any computation node supporting a Java virtual machine. The experiments show the component conformity according to the Data Access 2.05a specification of the OPC standard and to the temporal modes.",
        "published": "2008-08-01T04:38:22Z",
        "link": "http://arxiv.org/abs/0808.0055v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Towards a Process for Developing Maintenance Tools in Academia",
        "authors": [
            "Holger M. Kienle",
            "Hausi A. Müller"
        ],
        "summary": "Building of tools--from simple prototypes to industrial-strength applications--is a pervasive activity in academic research. When proposing a new technique for software maintenance, effective tool support is typically required to demonstrate the feasibility and effectiveness of the approach. However, even though tool building is both pervasive and requiring significant time and effort, it is still pursued in an ad hoc manner. In this paper, we address these issues by proposing a dedicated development process for tool building that takes the unique characteristics of an academic research environment into account. We first identify process requirements based on a review of the literature and our extensive tool building experience in the domain of maintenance tools. We then outline a process framework based on work products that accommodates the requirements while providing needed flexibility for tailoring the process to account for specific tool building approaches and project constraints. The work products are concrete milestones of the process, tracking progress, rationalizing (design) decisions, and documenting the current state of the tool building project. Thus, the work products provide important input for strategic project decisions and rapid initiation of new team members. Leveraging a dedicated tool building process promises tools that are designed, build, and maintained in a more disciplined, predictable and efficient manner.",
        "published": "2008-08-03T20:14:30Z",
        "link": "http://arxiv.org/abs/0808.0347v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Comparison between CPBPV, ESC/Java, CBMC, Blast, EUREKA and Why for   Bounded Program Verification",
        "authors": [
            "Hélène Collavizza",
            "Michel Rueher",
            "Pascal Van Hentenryck"
        ],
        "summary": "This report describes experimental results for a set of benchmarks on program verification. It compares the capabilities of CPBVP \"Constraint Programming framework for Bounded Program Verification\" [4] with the following frameworks: ESC/Java, CBMC, Blast, EUREKA and Why.",
        "published": "2008-08-11T12:55:19Z",
        "link": "http://arxiv.org/abs/0808.1508v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Initial Results on the F-logic to OWL Bi-directional Translation on a   Tabled Prolog Engine",
        "authors": [
            "Paul Fodor"
        ],
        "summary": "In this paper, we show our results on the bi-directional data exchange between the F-logic language supported by the Flora2 system and the OWL language. Most of the TBox and ABox axioms are translated preserving the semantics between the two representations, such as: proper inclusion, individual definition, functional properties, while some axioms and restrictions require a change in the semantics, such as: numbered and qualified cardinality restrictions. For the second case, we translate the OWL definite style inference rules into F-logic style constraints. We also describe a set of reasoning examples using the above translation, including the reasoning in Flora2 of a variety of ABox queries.",
        "published": "2008-08-12T19:58:59Z",
        "link": "http://arxiv.org/abs/0808.1721v1",
        "categories": [
            "cs.AI",
            "cs.SE"
        ]
    },
    {
        "title": "Correctness is not enough",
        "authors": [
            "Louise Pryor"
        ],
        "summary": "The usual aim of spreadsheet audit is to verify correctness. There are two problems with this: first, it is often difficult to tell whether the spreadsheets in question are correct, and second, even if they are, they may still give the wrong results. These problems are explained in this paper, which presents the key criteria for judging a spreadsheet and discusses how those criteria can be achieved",
        "published": "2008-08-14T19:03:10Z",
        "link": "http://arxiv.org/abs/0808.2045v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Network Motifs in Object-Oriented Software Systems",
        "authors": [
            "Yutao Ma",
            "Keqing He",
            "Jing Liu"
        ],
        "summary": "Nowadays, software has become a complex piece of work that may be beyond our control. Understanding how software evolves over time plays an important role in controlling software development processes. Recently, a few researchers found the quantitative evidence of structural duplication in software systems or web applications, which is similar to the evolutionary trend found in biological systems. To investigate the principles or rules of software evolution, we introduce the relevant theories and methods of complex networks into structural evolution and change of software systems. According to the results of our experiment on network motifs, we find that the stability of a motif shows positive correlation with its abundance and a motif with high Z score tends to have stable structure. These findings imply that the evolution of software systems is based on functional cloning as well as structural duplication and tends to be structurally stable. So, the work presented in this paper will be useful for the analysis of structural changes of software systems in reverse engineering.",
        "published": "2008-08-25T02:53:47Z",
        "link": "http://arxiv.org/abs/0808.3292v1",
        "categories": [
            "cs.SE",
            "D.2.8; K.6.3"
        ]
    },
    {
        "title": "Scientific Workflow Systems for 21st Century e-Science, New Bottle or   New Wine?",
        "authors": [
            "Yong Zhao",
            "Ioan Raicu",
            "Ian Foster"
        ],
        "summary": "With the advances in e-Sciences and the growing complexity of scientific analyses, more and more scientists and researchers are relying on workflow systems for process coordination, derivation automation, provenance tracking, and bookkeeping. While workflow systems have been in use for decades, it is unclear whether scientific workflows can or even should build on existing workflow technologies, or they require fundamentally new approaches. In this paper, we analyze the status and challenges of scientific workflows, investigate both existing technologies and emerging languages, platforms and systems, and identify the key challenges that must be addressed by workflow systems for e-science in the 21st century.",
        "published": "2008-08-26T16:46:49Z",
        "link": "http://arxiv.org/abs/0808.3545v1",
        "categories": [
            "cs.SE",
            "cs.DC",
            "D.1.3; D.4.7"
        ]
    },
    {
        "title": "The Prolog Interface to the Unstructured Information Management   Architecture",
        "authors": [
            "Paul Fodor",
            "Adam Lally",
            "David Ferrucci"
        ],
        "summary": "In this paper we describe the design and implementation of the Prolog interface to the Unstructured Information Management Architecture (UIMA) and some of its applications in natural language processing. The UIMA Prolog interface translates unstructured data and the UIMA Common Analysis Structure (CAS) into a Prolog knowledge base, over which, the developers write rules and use resolution theorem proving to search and generate new annotations over the unstructured data. These rules can explore all the previous UIMA annotations (such as, the syntactic structure, parsing statistics) and external Prolog knowledge bases (such as, Prolog WordNet and Extended WordNet) to implement a variety of tasks for the natural language analysis. We also describe applications of this logic programming interface in question analysis (such as, focus detection, answer-type and other constraints detection), shallow parsing (such as, relations in the syntactic structure), and answer selection.",
        "published": "2008-09-03T17:38:32Z",
        "link": "http://arxiv.org/abs/0809.0680v1",
        "categories": [
            "cs.SE",
            "cs.IR"
        ]
    },
    {
        "title": "Domain Specific Software Architecture for Design Center Automation",
        "authors": [
            "Anshuman Sinha",
            "Haritha Nandela",
            "Vijaya Balakrishna"
        ],
        "summary": "Domain specific software architecture aims at software reuse through construction of domain architecture reference model. The constructed reference model presents a set of individual components and their interaction points. When starting on a new large software project, the design engineer starts with pre-constructed model, which can be easily browsed and picks up opportunities of use in the new solution design. This report discusses application of domain reference design methods by deriving domain specific reference architecture for a product ordering system in a design center. The product in this case is instock and special order blinds from different manufacturers in a large supply store. The development of mature domain specific reference software architecture for this domain is not the objective of this report. However, this report would like to capture the method used in one such process and that is the primary concern of this report. This report lists subjective details of such a process applied to the domain of ordering custom and instock blinds from a large home construction and goods supply store. This report also describes the detailed process of derivation of knowledge models, unified knowledge models and the reference architecture for this domain. However, this domain model is only partially complete which may not be used for any real applications. This report is a result of a course project undertaken while studying this methodology.",
        "published": "2008-09-08T19:07:17Z",
        "link": "http://arxiv.org/abs/0809.1409v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Metrics-Based Spreadsheet Visualization: Support for Focused Maintenance",
        "authors": [
            "Karin Hodnigg",
            "Roland T. Mittermeir"
        ],
        "summary": "Legacy spreadsheets are both, an asset, and an enduring problem concerning spreadsheets in business. To make spreadsheets stay alive and remain correct, comprehension of a given spreadsheet is highly important. Visualization techniques should ease the complex and mindblowing challenges of finding structures in a huge set of spreadsheet cells for building an adequate mental model of spreadsheet programs. Since spreadsheet programs are as diverse as the purpose they are serving and as inhomogeneous as their programmers, to find an appropriate representation or visualization technique for every spreadsheet program seems futile. We thus propose different visualization and representation methods that may ease spreadsheet comprehension but should not be applied with all kind of spreadsheet programs. Therefore, this paper proposes to use (complexity) measures as indicators for proper visualization.",
        "published": "2008-09-17T20:58:29Z",
        "link": "http://arxiv.org/abs/0809.3009v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Automating Spreadsheet Discovery & Risk Assessment",
        "authors": [
            "Eric Perry"
        ],
        "summary": "There have been many articles and mishaps published about the risks of uncontrolled spreadsheets in today's business environment, including non-compliance, operational risk, errors, and fraud all leading to significant loss events. Spreadsheets fall into the realm of end user developed applications and are often absent the proper safeguards and controls an IT organization would enforce for enterprise applications. There is also an overall lack of software programming discipline enforced in how spreadsheets are developed. However, before an organization can apply proper controls and discipline to critical spreadsheets, an accurate and living inventory of spreadsheets across the enterprise must be created, and all critical spreadsheets must be identified. As such, this paper proposes an automated approach to the initial stages of the spreadsheet management lifecycle - discovery, inventory and risk assessment. Without the use of technology, these phases are often treated as a one-off project. By leveraging technology, they become a sustainable business process.",
        "published": "2008-09-17T21:16:02Z",
        "link": "http://arxiv.org/abs/0809.3016v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Spreadsheet modelling for solving combinatorial problems: The vendor   selection problem",
        "authors": [
            "Pandelis G. Ipsilandis"
        ],
        "summary": "Spreadsheets have grown up and became very powerful and easy to use tools in applying analytical techniques for solving business problems. Operations managers, production managers, planners and schedulers can work with them in developing solid and practical Do-It-Yourself Decision Support Systems. Small and Medium size organizations, can apply OR methodologies without the presence of specialized software and trained personnel, which in many cases cannot afford anyway. This paper examines an efficient approach in solving combinatorial programming problems with the use of spreadsheets. A practical application, which demonstrates the approach, concerns the development of a spreadsheet-based DSS for the Multi Item Procurement Problem with Fixed Vendor Cost. The DSS has been build using exclusively standard spreadsheet feature and can solve real problems of substantial size. The benefits and limitations of the approach are also discussed.",
        "published": "2008-09-21T11:21:40Z",
        "link": "http://arxiv.org/abs/0809.3574v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Spreadsheet Components For All",
        "authors": [
            "Jocelyn Paine"
        ],
        "summary": "We have prototyped a \"spreadsheet component repository\" Web site, from which users can copy \"components\" into their own Excel or Google spreadsheets. Components are collections of cells containing formulae: in real life, they would do useful calculations that many practitioners find hard to program, and would be rigorously tested and documented. Crucially, the user can tell the repository which cells in their spreadsheet to use for a componen's inputs and outputs. The repository will then reshape the component to fit. A single component can therefore be used in many different sizes and shapes of spreadsheet. We hope to set up a spreadsheet equivalent of the high-quality numerical subroutine libraries that revolutionised scientific computing, but where instead of subroutines, the library contains such components.",
        "published": "2008-09-21T13:31:35Z",
        "link": "http://arxiv.org/abs/0809.3584v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "A Primer on Spreadsheet Analytics",
        "authors": [
            "Thomas A. Grossman"
        ],
        "summary": "This paper provides guidance to an analyst who wants to extract insight from a spreadsheet model. It discusses the terminology of spreadsheet analytics, how to prepare a spreadsheet model for analysis, and a hierarchy of analytical techniques. These techniques include sensitivity analysis, tornado charts,and backsolving (or goal-seeking). This paper presents native-Excel approaches for automating these techniques, and discusses add-ins that are even more efficient. Spreadsheet optimization and spreadsheet Monte Carlo simulation are briefly discussed. The paper concludes by calling for empirical research, and describing desired features spreadsheet sensitivity analysis and spreadsheet optimization add-ins.",
        "published": "2008-09-21T13:49:23Z",
        "link": "http://arxiv.org/abs/0809.3586v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Spreadsheet End-User Behaviour Analysis",
        "authors": [
            "Brian Bishop",
            "Kevin McDaid"
        ],
        "summary": "To aid the development of spreadsheet debugging tools, a knowledge of end-users natural behaviour within the Excel environment would be advantageous. This paper details the design and application of a novel data acquisition tool, which can be used for the unobtrusive recording of end-users mouse, keyboard and Excel specific actions during the debugging of Excel spreadsheets. A debugging experiment was conducted using this data acquisition tool, and based on analysis of end-users performance and behaviour data, the authors developed a \"spreadsheet cell coverage feedback\" debugging tool. Results from the debugging experiment are presented in terms of enduser debugging performance and behaviour, and the outcomes of an evaluation experiment with the debugging tool are detailed.",
        "published": "2008-09-21T13:59:03Z",
        "link": "http://arxiv.org/abs/0809.3587v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Information and Data Quality in Spreadsheets",
        "authors": [
            "Patrick O'Beirne"
        ],
        "summary": "The quality of the data in spreadsheets is less discussed than the structural integrity of the formulas. Yet it is an area of great interest to the owners and users of the spreadsheet. This paper provides an overview of Information Quality (IQ) and Data Quality (DQ) with specific reference to how data is sourced, structured, and presented in spreadsheets.",
        "published": "2008-09-21T19:37:28Z",
        "link": "http://arxiv.org/abs/0809.3609v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Revisiting the Panko-Halverson Taxonomy of Spreadsheet Errors",
        "authors": [
            "Raymond R. Panko"
        ],
        "summary": "The purpose of this paper is to revisit the Panko-Halverson taxonomy of spreadsheet errors and suggest revisions. There are several reasons for doing so: First, the taxonomy has been widely used. Therefore, it should have scrutiny; Second, the taxonomy has not been widely available in its original form and most users refer to secondary sources. Consequently, they often equate the taxonomy with the simplified extracts used in particular experiments or field studies; Third, perhaps as a consequence, most users use only a fraction of the taxonomy. In particular, they tend not to use the taxonomy's life-cycle dimension; Fourth, the taxonomy has been tested against spreadsheets in experiments and spreadsheets in operational use. It is time to review how it has fared in these tests; Fifth, the taxonomy was based on the types of spreadsheet errors that were known to the authors in the mid-1990s. Subsequent experience has shown that the taxonomy needs to be extended for situations beyond those original experiences; Sixth, the omission category in the taxonomy has proven to be too narrow. Although this paper will focus on the Panko-Halverson taxonomy, this does not mean that that it is the only possible error taxonomy or even the best error taxonomy.",
        "published": "2008-09-21T20:29:53Z",
        "link": "http://arxiv.org/abs/0809.3613v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "The ADAPT Tool: From AADL Architectural Models to Stochastic Petri Nets   through Model Transformation",
        "authors": [
            "Ana E. Rugina",
            "Karama Kanoun",
            "Mohamed Kaaniche"
        ],
        "summary": "ADAPT is a tool that aims at easing the task of evaluating dependability measures in the context of modern model driven engineering processes based on AADL (Architecture Analysis and Design Language). Hence, its input is an AADL architectural model annotated with dependability-related information. Its output is a dependability evaluation model in the form of a Generalized Stochastic Petri Net (GSPN). The latter can be processed by existing dependability evaluation tools, to compute quantitative measures such as reliability, availability, etc.. ADAPT interfaces OSATE (the Open Source AADL Tool Environment) on the AADL side and SURF-2, on the dependability evaluation side. In addition, ADAPT provides the GSPN in XML/XMI format, which represents a gateway to other dependability evaluation tools, as the processing techniques for XML files allow it to be easily converted to a tool-specific GSPN.",
        "published": "2008-09-24T07:26:30Z",
        "link": "http://arxiv.org/abs/0809.4108v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Software dependability modeling using an industry-standard architecture   description language",
        "authors": [
            "Ana-Elena Rugina",
            "Peter H. Feiler",
            "Karama Kanoun",
            "Mohamed Kaaniche"
        ],
        "summary": "Performing dependability evaluation along with other analyses at architectural level allows both making architectural tradeoffs and predicting the effects of architectural decisions on the dependability of an application. This paper gives guidelines for building architectural dependability models for software systems using the AADL (Architecture Analysis and Design Language). It presents reusable modeling patterns for fault-tolerant applications and shows how the presented patterns can be used in the context of a subsystem of a real-life application.",
        "published": "2008-09-24T07:26:34Z",
        "link": "http://arxiv.org/abs/0809.4109v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Control software analysis, Part I Open-loop properties",
        "authors": [
            "Eric Feron",
            "Fernando Alegre"
        ],
        "summary": "As the digital world enters further into everyday life, questions are raised about the increasing challenges brought by the interaction of real-time software with physical devices. Many accidents and incidents encountered in areas as diverse as medical systems, transportation systems or weapon systems are ultimately attributed to \"software failures\". Since real-time software that interacts with physical systems might as well be called control software, the long litany of accidents due to real-time software failures might be taken as an equally long list of opportunities for control systems engineering. In this paper, we are interested only in run-time errors in those pieces of software that are a direct implementation of control system specifications: For well-defined and well-understood control architectures such as those present in standard textbooks on digital control systems, the current state of theoretical computer science is well-equipped enough to address and analyze control algorithms. It appears that a central element to these analyses is Lyapunov stability theory, which translate into invariant theory in computer implementations.",
        "published": "2008-09-28T01:44:28Z",
        "link": "http://arxiv.org/abs/0809.4812v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Software Engineering & Systems Design Nature",
        "authors": [
            "Kirill A Sorudeykin"
        ],
        "summary": "The main problems of Software Engineering appear as a result of incompatibilities. For example, the quality of organization of the production process depends on correspondence with existent resources and on a common understanding of project goals by all team members. Software design is another example. Its successfulness rides on the architecture's conformity with a project's concepts. This is a point of great nicety. All elements should create a single space of interaction. And if the laws of such a space are imperfect, missequencing comes and the concept of a software system fails. We must do our best for this not to happen. To that end, having a subtle perception of systems structures is essential. Such knowledge can be based only on a fresh approach to the logical law.",
        "published": "2008-10-06T04:50:01Z",
        "link": "http://arxiv.org/abs/0810.0874v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "A static theory of promises",
        "authors": [
            "Jan A. Bergstra",
            "Mark Burgess"
        ],
        "summary": "We discuss for the concept of promises within a framework that can be applied to either humans or technology. We compare promises to the more established notion of obligations and find promises to be both simpler and more effective at reducing uncertainty in behavioural outcomes.",
        "published": "2008-10-18T07:41:26Z",
        "link": "http://arxiv.org/abs/0810.3294v5",
        "categories": [
            "cs.MA",
            "cs.SE"
        ]
    },
    {
        "title": "A Call-Graph Profiler for GNU Octave",
        "authors": [
            "Muthiah Annamalai",
            "Leela Velusamy"
        ],
        "summary": "We report the design and implementation of a call-graph profiler for GNU Octave, a numerical computing platform. GNU Octave simplifies matrix computation for use in modeling or simulation. Our work provides a call-graph profiler, which is an improvement on the flat profiler. We elaborate design constraints of building a profiler for numerical computation, and benchmark the profiler by comparing it to the rudimentary timer start-stop (tic-toc) measurements, for a similar set of programs. The profiler code provides clean interfaces to internals of GNU Octave, for other (newer) profiling tools on GNU Octave.",
        "published": "2008-10-20T08:29:21Z",
        "link": "http://arxiv.org/abs/0810.3468v1",
        "categories": [
            "cs.PF",
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Pilotage des processus collaboratifs dans les systèmes PLM. Quels   indicateurs pour quelle évaluation des performances ?",
        "authors": [
            "Soumaya Elkadiri",
            "Philippe Pernelle",
            "Miguel Delattre",
            "Abdelaziz Bouras"
        ],
        "summary": "Les entreprises qui collaborent dans un processus de d\\'eveloppement de produit ont besoin de mettre en oeuvre une gestion efficace des activit\\'es collaborative. Malgr\\'e la mise en place d'un PLM, les activit\\'es collaborative sont loin d'\\^etre aussi efficace que l'on pourrait s'y attendre. Cet article propose une analyse des probl\\'ematiques de la collaboration avec un syst\\`eme PLM. A partir de ces analyses, nous proposons la mise en place d'indicateurs et d'actions sur les processus visant \\`a identifier puis att\\'enuer les freins dans le travail collaboratif. ----- Companies that collaborate within the product development processes need to implement an effective management of their collaborative activities. Despite the implementation of a PLM system, the collaborative activities are not efficient as it might be expected. This paper presents an analysis of the problems related to the collaborative work using a PLM system, identified through a survey. From this analysis, we propose an approach for improving collaborative processes within a PLM system, based on monitoring indicators. This approach leads to identify and therefore to mitigate the brakes of the collaborative work.",
        "published": "2008-11-12T20:08:51Z",
        "link": "http://arxiv.org/abs/0811.1947v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Collaborative process control: Observation of tracks generated by PLM   system",
        "authors": [
            "Soumaya Elkadiri",
            "Philippe Pernelle",
            "Miguel Delattre",
            "Abdelaziz Bouras"
        ],
        "summary": "This paper aims at analyzing the problems related to collaborative work using a PLM system. This research is mainly focused on the organisational aspects of SMEs involved in networks composed of large companies, subcontractors and other industrial partners. From this analysis, we propose the deployment of an approach based on an observation process of tracks generated by PLM system. The specific contributions are of two fold. First is to identify the brake points of collaborative work. The second, thanks to the exploitation of generated tracks, it allows reducing risks by reacting in real time to the incidents or dysfunctions that may occur. The overall system architecture based on services technology and supporting the proposed approach is described, as well as associated prototype developed using an industrial PLM system.",
        "published": "2008-11-12T20:09:41Z",
        "link": "http://arxiv.org/abs/0811.1950v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "A Transformation--Based Approach for the Design of Parallel/Distributed   Scientific Software: the FFT",
        "authors": [
            "Harry B. Hunt",
            "Lenore R. Mullin",
            "Daniel J. Rosenkrantz",
            "James E. Raynolds"
        ],
        "summary": "We describe a methodology for designing efficient parallel and distributed scientific software. This methodology utilizes sequences of mechanizable algebra--based optimizing transformations. In this study, we apply our methodology to the FFT, starting from a high--level algebraic algorithm description. Abstract multiprocessor plans are developed and refined to specify which computations are to be done by each processor. Templates are then created that specify the locations of computations and data on the processors, as well as data flow among processors. Templates are developed in both the MPI and OpenMP programming styles.   Preliminary experiments comparing code constructed using our methodology with code from several standard scientific libraries show that our code is often competitive and sometimes performs better. Interestingly, our code handled a larger range of problem sizes on one target architecture.",
        "published": "2008-11-15T22:32:59Z",
        "link": "http://arxiv.org/abs/0811.2535v1",
        "categories": [
            "cs.SE",
            "cs.PL"
        ]
    },
    {
        "title": "Encapsulation theory: the configuration efficiency limit",
        "authors": [
            "Edmund Kirwan"
        ],
        "summary": "This paper shows how maximum possible configuration efficiency of an indefinitely large software system is constrained by chosing a fixed upper limit to the number of program units per subsystem. It is then shown how the configuration efficiency of an indefinitely large software system depends on the ratio of the total number of informaiton hiding violational software units divided by the total number of program units.",
        "published": "2008-11-16T14:47:16Z",
        "link": "http://arxiv.org/abs/0811.2578v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "chi2TeX Semi-automatic translation from chiwriter to LaTeX",
        "authors": [
            "Justislav Bogevolnov"
        ],
        "summary": "Semi-automatic translation of math-filled book from obsolete ChiWriter format to LaTeX. Is it possible? Idea of criterion whether to use automatic or hand mode for translation. Illustrations.",
        "published": "2008-11-20T12:49:12Z",
        "link": "http://arxiv.org/abs/0811.3328v1",
        "categories": [
            "cs.SE",
            "cs.CV"
        ]
    },
    {
        "title": "Dynamic System Adaptation by Constraint Orchestration",
        "authors": [
            "L. P. J. Groenewegen",
            "E. P. de Vink"
        ],
        "summary": "For Paradigm models, evolution is just-in-time specified coordination conducted by a special reusable component McPal. Evolution can be treated consistently and on-the-fly through Paradigm's constraint orchestration, also for originally unforeseen evolution. UML-like diagrams visually supplement such migration, as is illustrated for the case of a critical section solution evolving into a pipeline architecture.",
        "published": "2008-11-21T09:47:26Z",
        "link": "http://arxiv.org/abs/0811.3492v1",
        "categories": [
            "cs.SE",
            "D.2.11; F.3.1"
        ]
    },
    {
        "title": "Solving package dependencies: from EDOS to Mancoosi",
        "authors": [
            "Ralf Treinen",
            "Stefano Zacchiroli"
        ],
        "summary": "Mancoosi (Managing the Complexity of the Open Source Infrastructure) is an ongoing research project funded by the European Union for addressing some of the challenges related to the \"upgrade problem\" of interdependent software components of which Debian packages are prototypical examples. Mancoosi is the natural continuation of the EDOS project which has already contributed tools for distribution-wide quality assurance in Debian and other GNU/Linux distributions. The consortium behind the project consists of several European public and private research institutions as well as some commercial GNU/Linux distributions from Europe and South America. Debian is represented by a small group of Debian Developers who are working in the ranks of the involved universities to drive and integrate back achievements into Debian. This paper presents relevant results from EDOS in dependency management and gives an overview of the Mancoosi project and its objectives, with a particular focus on the prospective benefits for Debian.",
        "published": "2008-11-21T19:45:50Z",
        "link": "http://arxiv.org/abs/0811.3620v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Description of the CUDF Format",
        "authors": [
            "Ralf Treinen",
            "Stefano Zacchiroli"
        ],
        "summary": "This document contains several related specifications, together they describe the document formats related to the solver competition which will be organized by Mancoosi. In particular, this document describes: - DUDF (Distribution Upgradeability Description Format), the document format to be used to submit upgrade problem instances from user machines to a (distribution-specific) database of upgrade problems; - CUDF (Common Upgradeability Description Format), the document format used to encode upgrade problems, abstracting over distribution-specific details. Solvers taking part in the competition will be fed with input in CUDF format.",
        "published": "2008-11-21T19:46:46Z",
        "link": "http://arxiv.org/abs/0811.3621v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Benchmarking the solar dynamo with Maxima",
        "authors": [
            "Valery V. Pipin"
        ],
        "summary": "Recently, Jouve et al(A&A, 2008) published the paper that presents the numerical benchmark for the solar dynamo models. Here, I would like to show a way how to get it with help of computer algebra system Maxima. This way was used in our paper (Pipin & Seehafer, A&A 2008, in print) to test some new ideas in the large-scale stellar dynamos. In the present paper I complement the dynamo benchmark with the standard test that address the problem of the free-decay modes in the sphere which is submerged in vacuum.",
        "published": "2008-11-25T11:35:35Z",
        "link": "http://arxiv.org/abs/0811.4061v2",
        "categories": [
            "cs.SE",
            "cs.SC"
        ]
    },
    {
        "title": "Ensuring Query Compatibility with Evolving XML Schemas",
        "authors": [
            "Pierre Genevès",
            "Nabil Layaïda",
            "Vincent Quint"
        ],
        "summary": "During the life cycle of an XML application, both schemas and queries may change from one version to another. Schema evolutions may affect query results and potentially the validity of produced data. Nowadays, a challenge is to assess and accommodate the impact of theses changes in rapidly evolving XML applications.   This article proposes a logical framework and tool for verifying forward/backward compatibility issues involving schemas and queries. First, it allows analyzing relations between schemas. Second, it allows XML designers to identify queries that must be reformulated in order to produce the expected results across successive schema versions. Third, it allows examining more precisely the impact of schema changes over queries, therefore facilitating their reformulation.",
        "published": "2008-11-26T14:37:01Z",
        "link": "http://arxiv.org/abs/0811.4324v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.0; D.3.1; D.3.4; E.1; F.3.1; F.3.2; F.4.1; F.4.3; H.2.3; I.2.4;\n  I.7.2"
        ]
    },
    {
        "title": "Revisiting the Core Ontology and Problem in Requirements Engineering",
        "authors": [
            "Ivan Jureta",
            "John Mylopoulos",
            "Stephane Faulkner"
        ],
        "summary": "In their seminal paper in the ACM Transactions on Software Engineering and Methodology, Zave and Jackson established a core ontology for Requirements Engineering (RE) and used it to formulate the \"requirements problem\", thereby defining what it means to successfully complete RE. Given that stakeholders of the system-to-be communicate the information needed to perform RE, we show that Zave and Jackson's ontology is incomplete. It does not cover all types of basic concerns that the stakeholders communicate. These include beliefs, desires, intentions, and attitudes. In response, we propose a core ontology that covers these concerns and is grounded in sound conceptual foundations resting on a foundational ontology. The new core ontology for RE leads to a new formulation of the requirements problem that extends Zave and Jackson's formulation. We thereby establish new standards for what minimum information should be represented in RE languages and new criteria for determining whether RE has been successfully completed.",
        "published": "2008-11-26T16:36:29Z",
        "link": "http://arxiv.org/abs/0811.4364v1",
        "categories": [
            "cs.SE",
            "D.2.1"
        ]
    },
    {
        "title": "Control software analysis, part II: Closed-loop analysis",
        "authors": [
            "Eric Feron",
            "Fernando Alegre"
        ],
        "summary": "The analysis and proper documentation of the properties of closed-loop control software presents many distinct aspects from the analysis of the same software running open-loop. Issues of physical system representations arise, and it is desired that such representations remain independent from the representations of the control program. For that purpose, a concurrent program representation of the plant and the control processes is proposed, although the closed-loop system is sufficiently serialized to enable a sequential analysis. While dealing with closed-loop system properties, it is also shown by means of examples how special treatment of nonlinearities extends from the analysis of control specifications to code analysis.",
        "published": "2008-12-10T17:57:14Z",
        "link": "http://arxiv.org/abs/0812.1986v1",
        "categories": [
            "cs.SE",
            "cs.PL"
        ]
    },
    {
        "title": "XML Static Analyzer User Manual",
        "authors": [
            "Pierre Geneves",
            "Nabil Layaida"
        ],
        "summary": "This document describes how to use the XML static analyzer in practice. It provides informal documentation for using the XML reasoning solver implementation. The solver allows automated verification of properties that are expressed as logical formulas over trees. A logical formula may for instance express structural constraints or navigation properties (like e.g. path existence and node selection) in finite trees. Logical formulas can be expressed using the syntax of XPath expressions, DTD, XML Schemas, and Relax NG definitions.",
        "published": "2008-12-18T15:22:46Z",
        "link": "http://arxiv.org/abs/0812.3550v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "cs.LO",
            "cs.SE",
            "D.3.0; D.3.1; D.3.4; E.1; F.3.1; F.3.2; F.4.1; F.4.3; H.2.3; I.2.4;\n  I.7.2"
        ]
    },
    {
        "title": "Context-aware adaptation for group communication support applications   with dynamic architecture",
        "authors": [
            "Ismael Bouassida Rodriguez",
            "Khalil DRIRA",
            "Christophe Chassot",
            "Mohamed Jmaiel"
        ],
        "summary": "In this paper, we propose a refinement-based adaptation approach for the architecture of distributed group communication support applications. Unlike most of previous works, our approach reaches implementable, context-aware and dynamically adaptable architectures. To model the context, we manage simultaneously four parameters that influence Qos provided by the application. These parameters are: the available bandwidth, the exchanged data communication priority, the energy level and the available memory for processing. These parameters make it possible to refine the choice between the various architectural configurations when passing from a given abstraction level to the lower level which implements it. Our approach allows the importance degree associated with each parameter to be adapted dynamically. To implement adaptation, we switch between the various configurations of the same level, and we modify the state of the entities of a given configuration when necessary. We adopt the direct and mediated Producer- Consumer architectural styles and graphs for architecture modelling. In order to validate our approach we elaborate a simulation model.",
        "published": "2008-12-19T07:53:12Z",
        "link": "http://arxiv.org/abs/0812.3716v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Architecture Logicielles pour des Applications hétérogènes,   distribuées et reconfigurables",
        "authors": [
            "Christine Louberry",
            "Marc Dalmau",
            "Philippe Roose"
        ],
        "summary": "The recent apparition of mobile wireless sensor aware to their physical environment and able to process information must allow proposing applications able to take into account their physical context and to react according to the changes of the environment. It suppose to design applications integrating both software and hardware components able to communicate. Applications must use context information from components to measure the quality of the proposed services in order to adapt them in real time. This work is interested in the integration of sensors in distributed applications. It present a service oriented software architecture allowing to manage and to reconfigure applications in heterogeneous environment where entities of different nature collaborate: software components and wireless sensors.",
        "published": "2008-12-19T08:03:21Z",
        "link": "http://arxiv.org/abs/0812.3719v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Bootstrapping Inductive and Coinductive Types in HasCASL",
        "authors": [
            "Lutz Schröder"
        ],
        "summary": "We discuss the treatment of initial datatypes and final process types in the wide-spectrum language HasCASL. In particular, we present specifications that illustrate how datatypes and process types arise as bootstrapped concepts using HasCASL's type class mechanism, and we describe constructions of types of finite and infinite trees that establish the conservativity of datatype and process type declarations adhering to certain reasonable formats. The latter amounts to modifying known constructions from HOL to avoid unique choice; in categorical terminology, this means that we establish that quasitoposes with an internal natural numbers object support initial algebras and final coalgebras for a range of polynomial functors, thereby partially generalising corresponding results from topos theory. Moreover, we present similar constructions in categories of internal complete partial orders in quasitoposes.",
        "published": "2008-12-19T16:22:15Z",
        "link": "http://arxiv.org/abs/0812.3836v2",
        "categories": [
            "cs.LO",
            "cs.SE",
            "D.2.1; E.1; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "XML Rewriting Attacks: Existing Solutions and their Limitations",
        "authors": [
            "Azzedine Benameur",
            "Faisal Abdul Kadir",
            "Serge Fenet"
        ],
        "summary": "Web Services are web-based applications made available for web users or remote Web-based programs. In order to promote interoperability, they publish their interfaces in the so-called WSDL file and allow remote call over the network. Although Web Services can be used in different ways, the industry standard is the Service Oriented Architecture Web Services that doesn't rely on the implementation details. In this architecture, communication is performed through XML-based messages called SOAP messages. However, those messages are prone to attacks that can lead to code injection, unauthorized accesses, identity theft, etc. This type of attacks, called XML Rewriting Attacks, are all based on unauthorized, yet possible, modifications of SOAP messages. We present in this paper an explanation of this kind of attack, review the existing solutions, and show their limitations. We also propose some ideas to secure SOAP messages, as well as implementation ideas.",
        "published": "2008-12-22T13:40:54Z",
        "link": "http://arxiv.org/abs/0812.4181v1",
        "categories": [
            "cs.CR",
            "cs.SE"
        ]
    },
    {
        "title": "Formalizing common sense for scalable inconsistency-robust information   integration using Direct Logic(TM) reasoning and the Actor Model",
        "authors": [
            "Carl Hewitt"
        ],
        "summary": "Because contemporary large software systems are pervasively inconsistent, it is not safe to reason about them using classical logic. The goal of Direct Logic is to be a minimal fix to classical mathematical logic that meets the requirements of large-scale Internet applications (including sense making for natural language) by addressing the following issues: inconsistency robustness, contrapositive inference bug, and direct argumentation.   Direct Logic makes the following contributions over previous work:   * Direct Inference (no contrapositive bug for inference)   * Direct Argumentation (inference directly expressed)   * Inconsistency-robust deduction without artifices such as indices (labels) on propositions or restrictions on reiteration   * Intuitive inferences hold including the following:   * Boolean Equivalences   * Reasoning by splitting for disjunctive cases   * Soundness   * Inconsistency-robust Proof by Contradiction   Since the global state model of computation (first formalized by Turing) is inadequate to the needs of modern large-scale Internet applications the Actor Model was developed to meet this need. Using, the Actor Model, this paper proves that Logic Programming is not computationally universal in that there are computations that cannot be implemented using logical inference. Consequently the Logic Programming paradigm is strictly less general than the Procedural Embedding of Knowledge paradigm.",
        "published": "2008-12-28T21:37:23Z",
        "link": "http://arxiv.org/abs/0812.4852v103",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Removing the Stiffness of Elastic Force from the Immersed Boundary   Method for the 2D Stokes Equations",
        "authors": [
            "Thomas Y. Hou",
            "Zuoqiang Shi"
        ],
        "summary": "The Immersed Boundary method has evolved into one of the most useful computational methods in studying fluid structure interaction. On the other hand, the Immersed Boundary method is also known to suffer from a severe timestep stability restriction when using an explicit time discretization. In this paper, we propose several efficient semi-implicit schemes to remove this stiffness from the Immersed Boundary method for the two-dimensional Stokes flow. First, we obtain a novel unconditionally stable semi-implicit discretization for the immersed boundary problem. Using this unconditionally stable discretization as a building block, we derive several efficient semi-implicit schemes for the immersed boundary problem by applying the Small Scale Decomposition to this unconditionally stable discretization. Our stability analysis and extensive numerical experiments show that our semi-implicit schemes offer much better stability property than the explicit scheme. Unlike other implicit or semi-implicit schemes proposed in the literature, our semi-implicit schemes can be solved explicitly in the spectral space. Thus the computational cost of our semi-implicit schemes is comparable to that of an explicit scheme, but with a much better stability property.",
        "published": "2008-01-15T22:22:25Z",
        "link": "http://arxiv.org/abs/0801.2398v2",
        "categories": [
            "cs.CE",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "A model for reactive porous transport during re-wetting of hardened   concrete",
        "authors": [
            "Michael Chapwanya",
            "Wentao Liu",
            "John M. Stockie"
        ],
        "summary": "A mathematical model is developed that captures the transport of liquid water in hardened concrete, as well as the chemical reactions that occur between the imbibed water and the residual calcium silicate compounds residing in the porous concrete matrix. The main hypothesis in this model is that the reaction product -- calcium silicate hydrate gel -- clogs the pores within the concrete thereby hindering water transport. Numerical simulations are employed to determine the sensitivity of the model solution to changes in various physical parameters, and compare to experimental results available in the literature.",
        "published": "2008-01-19T18:54:01Z",
        "link": "http://arxiv.org/abs/0801.3046v3",
        "categories": [
            "cs.CE",
            "physics.flu-dyn"
        ]
    },
    {
        "title": "A Pyramidal Evolutionary Algorithm with Different Inter-Agent Partnering   Strategies for Scheduling Problems",
        "authors": [
            "Uwe Aickelin"
        ],
        "summary": "This paper combines the idea of a hierarchical distributed genetic algorithm with different inter-agent partnering strategies. Cascading clusters of sub-populations are built from bottom up, with higher-level sub-populations optimising larger parts of the problem. Hence higher-level sub-populations search a larger search space with a lower resolution whilst lower-level sub-populations search a smaller search space with a higher resolution. The effects of different partner selection schemes amongst the agents on solution quality are examined for two multiple-choice optimisation problems. It is shown that partnering strategies that exploit problem-specific knowledge are superior and can counter inappropriate (sub-) fitness measurements.",
        "published": "2008-01-21T15:55:22Z",
        "link": "http://arxiv.org/abs/0801.3209v2",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Towards a Real-Time Data Driven Wildland Fire Model",
        "authors": [
            "Jan Mandel",
            "Jonathan D. Beezley",
            "Soham Chakraborty",
            "Janice L. Coen",
            "Craig C. Douglas",
            "Anthony Vodacek",
            "Zhen Wang"
        ],
        "summary": "A wildland fire model based on semi-empirical relations for the spread rate of a surface fire and post-frontal heat release is coupled with the Weather Research and Forecasting atmospheric model (WRF). The propagation of the fire front is implemented by a level set method. Data is assimilated by a morphing ensemble Kalman filter, which provides amplitude as well as position corrections. Thermal images of a fire will provide the observations and will be compared to a synthetic image from the model state.",
        "published": "2008-01-25T04:41:01Z",
        "link": "http://arxiv.org/abs/0801.3875v2",
        "categories": [
            "physics.ao-ph",
            "cs.CE"
        ]
    },
    {
        "title": "A Bayesian Optimisation Algorithm for the Nurse Scheduling Problem",
        "authors": [
            "Jingpeng Li",
            "Uwe Aickelin"
        ],
        "summary": "A Bayesian optimization algorithm for the nurse scheduling problem is presented, which involves choosing a suitable scheduling rule from a set for each nurses assignment. Unlike our previous work that used Gas to implement implicit learning, the learning in the proposed algorithm is explicit, ie. Eventually, we will be able to identify and mix building blocks directly. The Bayesian optimization algorithm is applied to implement such explicit learning by building a Bayesian network of the joint distribution of solutions. The conditional probability of each variable in the network is computed according to an initial set of promising solutions. Subsequently, each new instance for each variable is generated, ie in our case, a new rule string has been obtained. Another set of rule strings will be generated in this way, some of which will replace previous strings based on fitness selection. If stopping conditions are not met, the conditional probabilities for all nodes in the Bayesian network are updated again using the current set of promising rule strings. Computational results from 52 real data instances demonstrate the success of this approach. It is also suggested that the learning mechanism in the proposed approach might be suitable for other scheduling problems.",
        "published": "2008-01-25T16:07:25Z",
        "link": "http://arxiv.org/abs/0801.3971v3",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Phylogenies without Branch Bounds: Contracting the Short, Pruning the   Deep",
        "authors": [
            "Constantinos Daskalakis",
            "Elchanan Mossel",
            "Sebastien Roch"
        ],
        "summary": "We introduce a new phylogenetic reconstruction algorithm which, unlike most previous rigorous inference techniques, does not rely on assumptions regarding the branch lengths or the depth of the tree. The algorithm returns a forest which is guaranteed to contain all edges that are: 1) sufficiently long and 2) sufficiently close to the leaves. How much of the true tree is recovered depends on the sequence length provided. The algorithm is distance-based and runs in polynomial time.",
        "published": "2008-01-28T05:10:22Z",
        "link": "http://arxiv.org/abs/0801.4190v2",
        "categories": [
            "q-bio.PE",
            "cs.CE",
            "cs.DS",
            "math.PR",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Risk-Seeking versus Risk-Avoiding Investments in Noisy Periodic   Environments",
        "authors": [
            "J. Emeterio Navarro Barrientos",
            "Frank E. Walter",
            "Frank Schweitzer"
        ],
        "summary": "We study the performance of various agent strategies in an artificial investment scenario. Agents are equipped with a budget, $x(t)$, and at each time step invest a particular fraction, $q(t)$, of their budget. The return on investment (RoI), $r(t)$, is characterized by a periodic function with different types and levels of noise. Risk-avoiding agents choose their fraction $q(t)$ proportional to the expected positive RoI, while risk-seeking agents always choose a maximum value $q_{max}$ if they predict the RoI to be positive (\"everything on red\"). In addition to these different strategies, agents have different capabilities to predict the future $r(t)$, dependent on their internal complexity. Here, we compare 'zero-intelligent' agents using technical analysis (such as moving least squares) with agents using reinforcement learning or genetic algorithms to predict $r(t)$. The performance of agents is measured by their average budget growth after a certain number of time steps. We present results of extensive computer simulations, which show that, for our given artificial environment, (i) the risk-seeking strategy outperforms the risk-avoiding one, and (ii) the genetic algorithm was able to find this optimal strategy itself, and thus outperforms other prediction approaches considered.",
        "published": "2008-01-28T15:09:58Z",
        "link": "http://arxiv.org/abs/0801.4305v2",
        "categories": [
            "q-fin.PM",
            "cs.CE",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Investigating Artificial Immune Systems For Job Shop Rescheduling In   Changing Environments",
        "authors": [
            "Uwe Aickelin",
            "Edmund Burke",
            "Aniza Din"
        ],
        "summary": "Artificial immune system can be used to generate schedules in changing environments and it has been proven to be more robust than schedules developed using a genetic algorithm. Good schedules can be produced especially when the number of the antigens is increased. However, an increase in the range of the antigens had somehow affected the fitness of the immune system. In this research, we are trying to improve the result of the system by rescheduling the same problem using the same method while at the same time maintaining the robustness of the schedules.",
        "published": "2008-01-28T15:26:59Z",
        "link": "http://arxiv.org/abs/0801.4312v3",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Shrinkage Effect in Ancestral Maximum Likelihood",
        "authors": [
            "Elchanan Mossel",
            "Sebastien Roch",
            "Mike Steel"
        ],
        "summary": "Ancestral maximum likelihood (AML) is a method that simultaneously reconstructs a phylogenetic tree and ancestral sequences from extant data (sequences at the leaves). The tree and ancestral sequences maximize the probability of observing the given data under a Markov model of sequence evolution, in which branch lengths are also optimized but constrained to take the same value on any edge across all sequence sites. AML differs from the more usual form of maximum likelihood (ML) in phylogenetics because ML averages over all possible ancestral sequences. ML has long been known to be statistically consistent -- that is, it converges on the correct tree with probability approaching 1 as the sequence length grows. However, the statistical consistency of AML has not been formally determined, despite informal remarks in a literature that dates back 20 years. In this short note we prove a general result that implies that AML is statistically inconsistent. In particular we show that AML can `shrink' short edges in a tree, resulting in a tree that has no internal resolution as the sequence length grows. Our results apply to any number of taxa.",
        "published": "2008-02-07T06:52:44Z",
        "link": "http://arxiv.org/abs/0802.0914v1",
        "categories": [
            "q-bio.PE",
            "cs.CE",
            "math.PR",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Exploiting problem structure in a genetic algorithm approach to a nurse   rostering problem",
        "authors": [
            "Uwe Aickelin",
            "Kathryn Dowsland"
        ],
        "summary": "There is considerable interest in the use of genetic algorithms to solve problems arising in the areas of scheduling and timetabling. However, the classical genetic algorithm paradigm is not well equipped to handle the conflict between objectives and constraints that typically occurs in such problems. In order to overcome this, successful implementations frequently make use of problem specific knowledge. This paper is concerned with the development of a GA for a nurse rostering problem at a major UK hospital. The structure of the constraints is used as the basis for a co-evolutionary strategy using co-operating sub-populations. Problem specific knowledge is also used to define a system of incentives and disincentives, and a complementary mutation operator. Empirical results based on 52 weeks of live data show how these features are able to improve an unsuccessful canonical GA to the point where it is able to provide a practical solution to the problem",
        "published": "2008-02-14T11:25:37Z",
        "link": "http://arxiv.org/abs/0802.2001v3",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Hospital Case Cost Estimates Modelling - Algorithm Comparison",
        "authors": [
            "Peter Andru",
            "Alexei Botchkarev"
        ],
        "summary": "Ontario (Canada) Health System stakeholders support the idea and necessity of the integrated source of data that would include both clinical (e.g. diagnosis, intervention, length of stay, case mix group) and financial (e.g. cost per weighted case, cost per diem) characteristics of the Ontario healthcare system activities at the patient-specific level. At present, the actual patient-level case costs in the explicit form are not available in the financial databases for all hospitals. The goal of this research effort is to develop financial models that will assign each clinical case in the patient-specific data warehouse a dollar value, representing the cost incurred by the Ontario health care facility which treated the patient. Five mathematical models have been developed and verified using real dataset. All models can be classified into two groups based on their underlying method: 1. Models based on using relative intensity weights of the cases, and 2. Models based on using cost per diem.",
        "published": "2008-02-28T04:56:48Z",
        "link": "http://arxiv.org/abs/0802.4126v1",
        "categories": [
            "cs.CE",
            "cs.DB"
        ]
    },
    {
        "title": "Simulation Optimization of the Crossdock Door Assignment Problem",
        "authors": [
            "Uwe Aickelin",
            "Adrian Adewunmi"
        ],
        "summary": "The purpose of this report is to present the Crossdock Door Assignment Problem, which involves assigning destinations to outbound dock doors of Crossdock centres such that travel distance by material handling equipment is minimized. We propose a two fold solution; simulation and optimization of the simulation model simulation optimization. The novel aspect of our solution approach is that we intend to use simulation to derive a more realistic objective function and use Memetic algorithms to find an optimal solution. The main advantage of using Memetic algorithms is that it combines a local search with Genetic Algorithms. The Crossdock Door Assignment Problem is a new domain application to Memetic Algorithms and it is yet unknown how it will perform.",
        "published": "2008-03-11T12:56:51Z",
        "link": "http://arxiv.org/abs/0803.1576v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Using Intelligent Agents to Understand Management Practices and Retail   Productivity",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Helen Celia",
            "Christopher Clegg"
        ],
        "summary": "Intelligent agents offer a new and exciting way of understanding the world of work. In this paper we apply agent-based modeling and simulation to investigate a set of problems in a retail context. Specifically, we are working to understand the relationship between human resource management practices and retail productivity. Despite the fact we are working within a relatively novel and complex domain, it is clear that intelligent agents could offer potential for fostering sustainable organizational capabilities in the future. The project is still at an early stage. So far we have conducted a case study in a UK department store to collect data and capture impressions about operations and actors within departments. Furthermore, based on our case study we have built and tested our first version of a retail branch simulator which we will present in this paper.",
        "published": "2008-03-11T14:55:58Z",
        "link": "http://arxiv.org/abs/0803.1604v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "cs.MA"
        ]
    },
    {
        "title": "An Agent-Based Simulation of In-Store Customer Experiences",
        "authors": [
            "Peer-Olaf Siebers",
            "Uwe Aickelin",
            "Helen Celia",
            "Christopher Clegg"
        ],
        "summary": "Agent-based modelling and simulation offers a new and exciting way of understanding the world of work. In this paper we describe the development of an agent-based simulation model, designed to help to understand the relationship between human resource management practices and retail productivity. We report on the current development of our simulation model which includes new features concerning the evolution of customers over time. To test some of these features we have conducted a series of experiments dealing with customer pool sizes, standard and noise reduction modes, and the spread of the word of mouth. Our multi-disciplinary research team draws upon expertise from work psychologists and computer scientists. Despite the fact we are working within a relatively novel and complex domain, it is clear that intelligent agents offer potential for fostering sustainable organisational capabilities in the future.",
        "published": "2008-03-11T16:11:34Z",
        "link": "http://arxiv.org/abs/0803.1621v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "cs.MA"
        ]
    },
    {
        "title": "Investigating a Hybrid Metaheuristic For Job Shop Rescheduling",
        "authors": [
            "Salwani Abdullah",
            "Uwe Aickelin",
            "Edmund Burke",
            "Aniza Din",
            "Rong Qu"
        ],
        "summary": "Previous research has shown that artificial immune systems can be used to produce robust schedules in a manufacturing environment. The main goal is to develop building blocks (antibodies) of partial schedules that can be used to construct backup solutions (antigens) when disturbances occur during production. The building blocks are created based upon underpinning ideas from artificial immune systems and evolved using a genetic algorithm (Phase I). Each partial schedule (antibody) is assigned a fitness value and the best partial schedules are selected to be converted into complete schedules (antigens). We further investigate whether simulated annealing and the great deluge algorithm can improve the results when hybridised with our artificial immune system (Phase II). We use ten fixed solutions as our target and measure how well we cover these specific scenarios.",
        "published": "2008-03-12T09:26:47Z",
        "link": "http://arxiv.org/abs/0803.1728v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "An Investigation of the Sequential Sampling Method for Crossdocking   Simulation Output Variance Reduction",
        "authors": [
            "Adrian Adewunmi",
            "Uwe Aickelin",
            "Mike Byrne"
        ],
        "summary": "This paper investigates the reduction of variance associated with a simulation output performance measure, using the Sequential Sampling method while applying minimum simulation replications, for a class of JIT (Just in Time) warehousing system called crossdocking. We initially used the Sequential Sampling method to attain a desired 95% confidence interval half width of plus/minus 0.5 for our chosen performance measure (Total usage cost, given the mean maximum level of 157,000 pounds and a mean minimum level of 149,000 pounds). From our results, we achieved a 95% confidence interval half width of plus/minus 2.8 for our chosen performance measure (Total usage cost, with an average mean value of 115,000 pounds). However, the Sequential Sampling method requires a huge number of simulation replications to reduce variance for our simulation output value to the target level. Arena (version 11) simulation software was used to conduct this study.",
        "published": "2008-03-13T15:02:48Z",
        "link": "http://arxiv.org/abs/0803.1985v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Improved Squeaky Wheel Optimisation for Driver Scheduling",
        "authors": [
            "Uwe Aickelin",
            "Edmund Burke",
            "Jingpeng Li"
        ],
        "summary": "This paper presents a technique called Improved Squeaky Wheel Optimisation for driver scheduling problems. It improves the original Squeaky Wheel Optimisations effectiveness and execution speed by incorporating two additional steps of Selection and Mutation which implement evolution within a single solution. In the ISWO, a cycle of Analysis-Selection-Mutation-Prioritization-Construction continues until stopping conditions are reached. The Analysis step first computes the fitness of a current solution to identify troublesome components. The Selection step then discards these troublesome components probabilistically by using the fitness measure, and the Mutation step follows to further discard a small number of components at random. After the above steps, an input solution becomes partial and thus the resulting partial solution needs to be repaired. The repair is carried out by using the Prioritization step to first produce priorities that determine an order by which the following Construction step then schedules the remaining components. Therefore, the optimisation in the ISWO is achieved by solution disruption, iterative improvement and an iterative constructive repair process performed. Encouraging experimental results are reported.",
        "published": "2008-03-13T15:28:06Z",
        "link": "http://arxiv.org/abs/0803.1993v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "The Application of Bayesian Optimization and Classifier Systems in Nurse   Scheduling",
        "authors": [
            "Jingpeng Li",
            "Uwe Aickelin"
        ],
        "summary": "Two ideas taken from Bayesian optimization and classifier systems are presented for personnel scheduling based on choosing a suitable scheduling rule from a set for each persons assignment. Unlike our previous work of using genetic algorithms whose learning is implicit, the learning in both approaches is explicit, i.e. we are able to identify building blocks directly. To achieve this target, the Bayesian optimization algorithm builds a Bayesian network of the joint probability distribution of the rules used to construct solutions, while the adapted classifier system assigns each rule a strength value that is constantly updated according to its usefulness in the current situation. Computational results from 52 real data instances of nurse scheduling demonstrate the success of both approaches. It is also suggested that the learning mechanism in the proposed approaches might be suitable for other scheduling problems.",
        "published": "2008-03-13T15:43:34Z",
        "link": "http://arxiv.org/abs/0803.1994v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "A Distance Metric for Tree-Sibling Time Consistent Phylogenetic Networks",
        "authors": [
            "Gabriel Cardona",
            "Merce Llabres",
            "Francesc Rossello",
            "Gabriel Valiente"
        ],
        "summary": "The presence of reticulate evolutionary events in phylogenies turn phylogenetic trees into phylogenetic networks. These events imply in particular that there may exist multiple evolutionary paths from a non-extant species to an extant one, and this multiplicity makes the comparison of phylogenetic networks much more difficult than the comparison of phylogenetic trees. In fact, all attempts to define a sound distance measure on the class of all phylogenetic networks have failed so far. Thus, the only practical solutions have been either the use of rough estimates of similarity (based on comparison of the trees embedded in the networks), or narrowing the class of phylogenetic networks to a certain class where such a distance is known and can be efficiently computed. The first approach has the problem that one may identify two networks as equivalent, when they are not; the second one has the drawback that there may not exist algorithms to reconstruct such networks from biological sequences.   We present in this paper a distance measure on the class of tree-sibling time consistent phylogenetic networks, which generalize tree-child time consistent phylogenetic networks, and thus also galled-trees. The practical interest of this distance measure is twofold: it can be computed in polynomial time by means of simple algorithms, and there also exist polynomial-time algorithms for reconstructing networks of this class from DNA sequence data.   The Perl package Bio::PhyloNetwork, included in the BioPerl bundle, implements many algorithms on phylogenetic networks, including the computation of the distance presented in this paper.",
        "published": "2008-03-19T22:24:11Z",
        "link": "http://arxiv.org/abs/0803.2904v1",
        "categories": [
            "q-bio.PE",
            "cs.CE",
            "cs.DM"
        ]
    },
    {
        "title": "Enhanced Direct and Indirect Genetic Algorithm Approaches for a Mall   Layout and Tenant Selection Problem",
        "authors": [
            "Uwe Aickelin",
            "Kathryn Dowsland"
        ],
        "summary": "During our earlier research, it was recognised that in order to be successful with an indirect genetic algorithm approach using a decoder, the decoder has to strike a balance between being an optimiser in its own right and finding feasible solutions. Previously this balance was achieved manually. Here we extend this by presenting an automated approach where the genetic algorithm itself, simultaneously to solving the problem, sets weights to balance the components out. Subsequently we were able to solve a complex and non-linear scheduling problem better than with a standard direct genetic algorithm implementation.",
        "published": "2008-03-20T10:19:01Z",
        "link": "http://arxiv.org/abs/0803.2957v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Building Better Nurse Scheduling Algorithms",
        "authors": [
            "Uwe Aickelin",
            "Paul White"
        ],
        "summary": "The aim of this research is twofold: Firstly, to model and solve a complex nurse scheduling problem with an integer programming formulation and evolutionary algorithms. Secondly, to detail a novel statistical method of comparing and hence building better scheduling algorithms by identifying successful algorithm modifications. The comparison method captures the results of algorithms in a single figure that can then be compared using traditional statistical techniques. Thus, the proposed method of comparing algorithms is an objective procedure designed to assist in the process of improving an algorithm. This is achieved even when some results are non-numeric or missing due to infeasibility. The final algorithm outperforms all previous evolutionary algorithms, which relied on human expertise for modification.",
        "published": "2008-03-20T11:15:37Z",
        "link": "http://arxiv.org/abs/0803.2967v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "An Indirect Genetic Algorithm for a Nurse Scheduling Problem",
        "authors": [
            "Uwe Aickelin",
            "Kathryn Dowsland"
        ],
        "summary": "This paper describes a Genetic Algorithms approach to a manpower-scheduling problem arising at a major UK hospital. Although Genetic Algorithms have been successfully used for similar problems in the past, they always had to overcome the limitations of the classical Genetic Algorithms paradigm in handling the conflict between objectives and constraints. The approach taken here is to use an indirect coding based on permutations of the nurses, and a heuristic decoder that builds schedules from these permutations. Computational experiments based on 52 weeks of live data are used to evaluate three different decoders with varying levels of intelligence, and four well-known crossover operators. Results are further enhanced by introducing a hybrid crossover operator and by making use of simple bounds to reduce the size of the solution space. The results reveal that the proposed algorithm is able to find high quality solutions and is both faster and more flexible than a recently published Tabu Search approach.",
        "published": "2008-03-20T11:21:19Z",
        "link": "http://arxiv.org/abs/0803.2969v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "An Estimation of Distribution Algorithm for Nurse Scheduling",
        "authors": [
            "Uwe Aickelin",
            "Jingpeng Li"
        ],
        "summary": "Schedules can be built in a similar way to a human scheduler by using a set of rules that involve domain knowledge. This paper presents an Estimation of Distribution Algorithm (eda) for the nurse scheduling problem, which involves choosing a suitable scheduling rule from a set for the assignment of each nurse. Unlike previous work that used Genetic Algorithms (ga) to implement implicit learning, the learning in the proposed algorithm is explicit, i.e. we identify and mix building blocks directly. The eda is applied to implement such explicit learning by building a Bayesian network of the joint distribution of solutions. The conditional probability of each variable in the network is computed according to an initial set of promising solutions. Subsequently, each new instance for each variable is generated by using the corresponding conditional probabilities, until all variables have been generated, i.e. in our case, a new rule string has been obtained. Another set of rule strings will be generated in this way, some of which will replace previous strings based on fitness selection. If stopping conditions are not met, the conditional probabilities for all nodes in the Bayesian network are updated again using the current set of promising rule strings. Computational results from 52 real data instances demonstrate the success of this approach. It is also suggested that the learning mechanism in the proposed approach might be suitable for other scheduling problems.",
        "published": "2008-03-20T12:07:26Z",
        "link": "http://arxiv.org/abs/0803.2975v2",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "A Component Based Heuristic Search method with Adaptive Perturbations   for Hospital Personnel Scheduling",
        "authors": [
            "Jingpeng Li",
            "Uwe Aickelin",
            "Edmund Burke"
        ],
        "summary": "Nurse rostering is a complex scheduling problem that affects hospital personnel on a daily basis all over the world. This paper presents a new component-based approach with adaptive perturbations, for a nurse scheduling problem arising at a major UK hospital. The main idea behind this technique is to decompose a schedule into its components (i.e. the allocated shift pattern of each nurse), and then mimic a natural evolutionary process on these components to iteratively deliver better schedules. The worthiness of all components in the schedule has to be continuously demonstrated in order for them to remain there. This demonstration employs a dynamic evaluation function which evaluates how well each component contributes towards the final objective. Two perturbation steps are then applied: the first perturbation eliminates a number of components that are deemed not worthy to stay in the current schedule; the second perturbation may also throw out, with a low level of probability, some worthy components. The eliminated components are replenished with new ones using a set of constructive heuristics using local optimality criteria. Computational results using 52 data instances demonstrate the applicability of the proposed approach in solving real-world problems.",
        "published": "2008-03-27T12:15:43Z",
        "link": "http://arxiv.org/abs/0803.3900v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Bayesian Optimisation Algorithm for Nurse Scheduling",
        "authors": [
            "Jingpeng Li",
            "Uwe Aickelin"
        ],
        "summary": "Our research has shown that schedules can be built mimicking a human scheduler by using a set of rules that involve domain knowledge. This chapter presents a Bayesian Optimization Algorithm (BOA) for the nurse scheduling problem that chooses such suitable scheduling rules from a set for each nurses assignment. Based on the idea of using probabilistic models, the BOA builds a Bayesian network for the set of promising solutions and samples these networks to generate new candidate solutions. Computational results from 52 real data instances demonstrate the success of this approach. It is also suggested that the learning mechanism in the proposed algorithm may be suitable for other scheduling problems.",
        "published": "2008-04-03T11:14:11Z",
        "link": "http://arxiv.org/abs/0804.0524v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Méthode de calcul du rayonnement acoustique de structures complexes",
        "authors": [
            "Marianne Viallet",
            "Gérald Poumérol",
            "Olivier Dessombz",
            "Louis Jezequel"
        ],
        "summary": "In the automotive industry, predicting noise during design cycle is a necessary step. Well-known methods exist to answer this issue in low frequency domain. Among these, Finite Element Methods, adapted to closed domains, are quite easy to implement whereas Boundary Element Methods are more adapted to infinite domains, but may induce singularity problems. In this article, the described method, the SDM, allows to use both methods in their best application domain. A new method is also presented to solve the SDM exterior problem. Instead of using Boundary Element Methods, an original use of Finite Elements is made. Efficiency of this new version of the Substructure Deletion Method is discussed.",
        "published": "2008-04-08T06:24:49Z",
        "link": "http://arxiv.org/abs/0804.1187v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "A Logic Programming Framework for Combinational Circuit Synthesis",
        "authors": [
            "Paul Tarau",
            "Brenda Luderman"
        ],
        "summary": "Logic Programming languages and combinational circuit synthesis tools share a common \"combinatorial search over logic formulae\" background. This paper attempts to reconnect the two fields with a fresh look at Prolog encodings for the combinatorial objects involved in circuit synthesis. While benefiting from Prolog's fast unification algorithm and built-in backtracking mechanism, efficiency of our search algorithm is ensured by using parallel bitstring operations together with logic variable equality propagation, as a mapping mechanism from primary inputs to the leaves of candidate Leaf-DAGs implementing a combinational circuit specification. After an exhaustive expressiveness comparison of various minimal libraries, a surprising first-runner, Strict Boolean Inequality \"<\" together with constant function \"1\" also turns out to have small transistor-count implementations, competitive to NAND-only or NOR-only libraries. As a practical outcome, a more realistic circuit synthesizer is implemented that combines rewriting-based simplification of (<,1) circuits with exhaustive Leaf-DAG circuit search.   Keywords: logic programming and circuit design, combinatorial object generation, exact combinational circuit synthesis, universal boolean logic libraries, symbolic rewriting, minimal transistor-count circuit synthesis",
        "published": "2008-04-14T02:40:31Z",
        "link": "http://arxiv.org/abs/0804.2095v1",
        "categories": [
            "cs.LO",
            "cs.CE",
            "cs.DM",
            "cs.PL"
        ]
    },
    {
        "title": "Prediction and Mitigation of Crush Conditions in Emergency Evacuations",
        "authors": [
            "Peter J. Harding",
            "Martyn Amos",
            "Steve Gwynne"
        ],
        "summary": "Several simulation environments exist for the simulation of large-scale evacuations of buildings, ships, or other enclosed spaces. These offer sophisticated tools for the study of human behaviour, the recreation of environmental factors such as fire or smoke, and the inclusion of architectural or structural features, such as elevators, pillars and exits. Although such simulation environments can provide insights into crowd behaviour, they lack the ability to examine potentially dangerous forces building up within a crowd. These are commonly referred to as crush conditions, and are a common cause of death in emergency evacuations.   In this paper, we describe a methodology for the prediction and mitigation of crush conditions. The paper is organised as follows. We first establish the need for such a model, defining the main factors that lead to crush conditions, and describing several exemplar case studies. We then examine current methods for studying crush, and describe their limitations. From this, we develop a three-stage hybrid approach, using a combination of techniques. We conclude with a brief discussion of the potential benefits of our approach.",
        "published": "2008-05-03T13:00:42Z",
        "link": "http://arxiv.org/abs/0805.0360v1",
        "categories": [
            "cs.CE",
            "cs.MA"
        ]
    },
    {
        "title": "Tuplix Calculus Specifications of Financial Transfer Networks",
        "authors": [
            "J. A. Bergstra",
            "S. Nolst Trenite",
            "M. B. van der Zwaag"
        ],
        "summary": "We study the application of Tuplix Calculus in modular financial budget design. We formalize organizational structure using financial transfer networks. We consider the notion of flux of money over a network, and a way to enforce the matching of influx and outflux for parts of a network. We exploit so-called signed attribute notation to make internal streams visible through encapsulations. Finally, we propose a Tuplix Calculus construct for the definition of data functions.",
        "published": "2008-05-13T09:05:41Z",
        "link": "http://arxiv.org/abs/0805.1806v1",
        "categories": [
            "cs.CE",
            "cs.LO"
        ]
    },
    {
        "title": "Parallel Pricing Algorithms for Multi--Dimensional Bermudan/American   Options using Monte Carlo methods",
        "authors": [
            "Mireille Bossy",
            "Françoise Baude",
            "Viet Dung Doan",
            "Abhijeet Gaikwad",
            "Ian Stokes-Rees"
        ],
        "summary": "In this paper we present two parallel Monte Carlo based algorithms for pricing multi--dimensional Bermudan/American options. First approach relies on computation of the optimal exercise boundary while the second relies on classification of continuation and exercise values. We also evaluate the performance of both the algorithms in a desktop grid environment. We show the effectiveness of the proposed approaches in a heterogeneous computing environment, and identify scalability constraints due to the algorithmic structure.",
        "published": "2008-05-13T12:34:04Z",
        "link": "http://arxiv.org/abs/0805.1827v1",
        "categories": [
            "cs.DC",
            "cs.CE"
        ]
    },
    {
        "title": "Performability Aspects of the Atlas Vo; Using Lmbench Suite",
        "authors": [
            "Fotis Georgatos",
            "John Kouvakis",
            "John Kouretis"
        ],
        "summary": "The ATLAS Virtual Organization is grid's largest Virtual Organization which is currently in full production stage. Hereby a case is being made that a user working within that VO is going to face a wide spectrum of different systems, whose heterogeneity is enough to count as \"orders of magnitude\" according to a number of metrics; including integer/float operations, memory throughput (STREAM) and communication latencies. Furthermore, the spread of performance does not appear to follow any known distribution pattern, which is demonstrated in graphs produced during May 2007 measurements. It is implied that the current practice where either \"all-WNs-are-equal\" or, the alternative of SPEC-based rating used by LCG/EGEE is an oversimplification which is inappropriate and expensive from an operational point of view, therefore new techniques are needed for optimal grid resources allocation.",
        "published": "2008-05-19T19:45:31Z",
        "link": "http://arxiv.org/abs/0805.2949v1",
        "categories": [
            "cs.PF",
            "cs.CE",
            "cs.DC"
        ]
    },
    {
        "title": "The VO-Neural project: recent developments and some applications",
        "authors": [
            "M. Brescia",
            "S. Cavuoti",
            "G. d'Angelo",
            "R. D'Abrusco",
            "N. Deniskina",
            "M. Garofalo",
            "O. Laurino",
            "G. Longo",
            "A. Nocella",
            "B. Skordovski"
        ],
        "summary": "VO-Neural is the natural evolution of the Astroneural project which was started in 1994 with the aim to implement a suite of neural tools for data mining in astronomical massive data sets. At a difference with its ancestor, which was implemented under Matlab, VO-Neural is written in C++, object oriented, and it is specifically tailored to work in distributed computing architectures. We discuss the current status of implementation of VO-Neural, present an application to the classification of Active Galactic Nuclei, and outline the ongoing work to improve the functionalities of the package.",
        "published": "2008-06-05T16:44:23Z",
        "link": "http://arxiv.org/abs/0806.1006v1",
        "categories": [
            "astro-ph",
            "cs.CE"
        ]
    },
    {
        "title": "GRID-Launcher v.1.0",
        "authors": [
            "N. Deniskina",
            "M. Brescia",
            "S. Cavuoti",
            "G. d'Angelo",
            "O. Laurino",
            "G. Longo"
        ],
        "summary": "GRID-launcher-1.0 was built within the VO-Tech framework, as a software interface between the UK-ASTROGRID and a generic GRID infrastructures in order to allow any ASTROGRID user to launch on the GRID computing intensive tasks from the ASTROGRID Workbench or Desktop. Even though of general application, so far the Grid-Launcher has been tested on a few selected softwares (VONeural-MLP, VONeural-SVM, Sextractor and SWARP) and on the SCOPE-GRID.",
        "published": "2008-06-06T12:35:17Z",
        "link": "http://arxiv.org/abs/0806.1144v1",
        "categories": [
            "astro-ph",
            "cs.CE"
        ]
    },
    {
        "title": "Nodal distances for rooted phylogenetic trees",
        "authors": [
            "Gabriel Cardona",
            "Merce Llabres",
            "Francesc Rossello",
            "Gabriel Valiente"
        ],
        "summary": "Dissimilarity measures for (possibly weighted) phylogenetic trees based on the comparison of their vectors of path lengths between pairs of taxa, have been present in the systematics literature since the early seventies. But, as far as rooted phylogenetic trees goes, these vectors can only separate non-weighted binary trees, and therefore these dissimilarity measures are metrics only on this class. In this paper we overcome this problem, by splitting in a suitable way each path length between two taxa into two lengths. We prove that the resulting splitted path lengths matrices single out arbitrary rooted phylogenetic trees with nested taxa and arcs weighted in the set of positive real numbers. This allows the definition of metrics on this general class by comparing these matrices by means of metrics in spaces of real-valued $n\\times n$ matrices. We conclude this paper by establishing some basic facts about the metrics for non-weighted phylogenetic trees defined in this way using $L^p$ metrics on these spaces of matrices.",
        "published": "2008-06-12T10:17:02Z",
        "link": "http://arxiv.org/abs/0806.2035v1",
        "categories": [
            "q-bio.PE",
            "cs.CE",
            "cs.DM"
        ]
    },
    {
        "title": "Continuing Progress on a Lattice QCD Software Infrastructure",
        "authors": [
            "Balint Joo"
        ],
        "summary": "We report on the progress of the software effort in the QCD Application Area of SciDAC. In particular, we discuss how the software developed under SciDAC enabled the aggressive exploitation of leadership computers, and we report on progress in the area of QCD software for multi-core architectures.",
        "published": "2008-06-13T19:22:00Z",
        "link": "http://arxiv.org/abs/0806.2312v1",
        "categories": [
            "hep-lat",
            "cs.CE"
        ]
    },
    {
        "title": "Path lengths in tree-child time consistent hybridization networks",
        "authors": [
            "Gabriel Cardona",
            "Merce Llabres",
            "Francesc Rossello",
            "Gabriel Valiente"
        ],
        "summary": "Hybridization networks are representations of evolutionary histories that allow for the inclusion of reticulate events like recombinations, hybridizations, or lateral gene transfers. The recent growth in the number of hybridization network reconstruction algorithms has led to an increasing interest in the definition of metrics for their comparison that can be used to assess the accuracy or robustness of these methods. In this paper we establish some basic results that make it possible the generalization to tree-child time consistent (TCTC) hybridization networks of some of the oldest known metrics for phylogenetic trees: those based on the comparison of the vectors of path lengths between leaves. More specifically, we associate to each hybridization network a suitably defined vector of `splitted' path lengths between its leaves, and we prove that if two TCTC hybridization networks have the same such vectors, then they must be isomorphic. Thus, comparing these vectors by means of a metric for real-valued vectors defines a metric for TCTC hybridization networks. We also consider the case of fully resolved hybridization networks, where we prove that simpler, `non-splitted' vectors can be used.",
        "published": "2008-07-01T09:13:32Z",
        "link": "http://arxiv.org/abs/0807.0087v1",
        "categories": [
            "q-bio.PE",
            "cs.CE",
            "cs.DM",
            "q-bio.QM"
        ]
    },
    {
        "title": "Simulations of Large-scale WiFi-based Wireless Networks:   Interdisciplinary Challenges and Applications",
        "authors": [
            "Maziar Nekovee"
        ],
        "summary": "Wireless Fidelity (WiFi) is the fastest growing wireless technology to date. In addition to providing wire-free connectivity to the Internet WiFi technology also enables mobile devices to connect directly to each other and form highly dynamic wireless adhoc networks. Such distributed networks can be used to perform cooperative communication tasks such ad data routing and information dissemination in the absence of a fixed infrastructure. Furthermore, adhoc grids composed of wirelessly networked portable devices are emerging as a new paradigm in grid computing. In this paper we review computational and algorithmic challenges of high-fidelity simulations of such WiFi-based wireless communication and computing networks, including scalable topology maintenance, mobility modelling, parallelisation and synchronisation. We explore similarities and differences between the simulations of these networks and simulations of interacting many-particle systems, such as molecular dynamics (MD) simulations. We show how the cell linked-list algorithm which we have adapted from our MD simulations can be used to greatly improve the computational performance of wireless network simulators in the presence of mobility, and illustrate with an example from our simulation studies of worm attacks on mobile wireless adhoc networks.",
        "published": "2008-07-09T16:04:37Z",
        "link": "http://arxiv.org/abs/0807.1475v1",
        "categories": [
            "cs.CE",
            "cs.DC"
        ]
    },
    {
        "title": "On dual Schur domain decomposition method for linear first-order   transient problems",
        "authors": [
            "K. B. Nakshatrala",
            "A. Prakash",
            "K. D. Hjelmstad"
        ],
        "summary": "This paper addresses some numerical and theoretical aspects of dual Schur domain decomposition methods for linear first-order transient partial differential equations. In this work, we consider the trapezoidal family of schemes for integrating the ordinary differential equations (ODEs) for each subdomain and present four different coupling methods, corresponding to different algebraic constraints, for enforcing kinematic continuity on the interface between the subdomains.   Method 1 (d-continuity) is based on the conventional approach using continuity of the primary variable and we show that this method is unstable for a lot of commonly used time integrators including the mid-point rule. To alleviate this difficulty, we propose a new Method 2 (Modified d-continuity) and prove its stability for coupling all time integrators in the trapezoidal family (except the forward Euler). Method 3 (v-continuity) is based on enforcing the continuity of the time derivative of the primary variable. However, this constraint introduces a drift in the primary variable on the interface. We present Method 4 (Baumgarte stabilized) which uses Baumgarte stabilization to limit this drift and we derive bounds for the stabilization parameter to ensure stability.   Our stability analysis is based on the ``energy'' method, and one of the main contributions of this paper is the extension of the energy method (which was previously introduced in the context of numerical methods for ODEs) to assess the stability of numerical formulations for index-2 differential-algebraic equations (DAEs).",
        "published": "2008-07-14T08:10:33Z",
        "link": "http://arxiv.org/abs/0807.2108v2",
        "categories": [
            "cs.NA",
            "cs.CE"
        ]
    },
    {
        "title": "Hardware/Software Co-Design for Spike Based Recognition",
        "authors": [
            "Arfan Ghani",
            "Martin McGinnity",
            "Liam Maguire",
            "Jim Harkin"
        ],
        "summary": "The practical applications based on recurrent spiking neurons are limited due to their non-trivial learning algorithms. The temporal nature of spiking neurons is more favorable for hardware implementation where signals can be represented in binary form and communication can be done through the use of spikes. This work investigates the potential of recurrent spiking neurons implementations on reconfigurable platforms and their applicability in temporal based applications. A theoretical framework of reservoir computing is investigated for hardware/software implementation. In this framework, only readout neurons are trained which overcomes the burden of training at the network level. These recurrent neural networks are termed as microcircuits which are viewed as basic computational units in cortical computation. This paper investigates the potential of recurrent neural reservoirs and presents a novel hardware/software strategy for their implementation on FPGAs. The design is implemented and the functionality is tested in the context of speech recognition application.",
        "published": "2008-07-14T23:44:47Z",
        "link": "http://arxiv.org/abs/0807.2282v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CE",
            "C.1.3"
        ]
    },
    {
        "title": "Accelerated Option Pricing in Multiple Scenarios",
        "authors": [
            "Stefan Dirnstorfer",
            "Andreas J. Grau"
        ],
        "summary": "This paper covers a massive acceleration of Monte-Carlo based pricing method for financial products and financial derivatives. The method is applicable in risk management settings, where a financial product has to be priced under a number of potential future scenarios. Instead of starting a separate nested Monte Carlo simulation for each scenario under consideration, the new method covers the utilization of very few representative nested simulations and estimating the product prices at each scenario by a smoothing method based on the state-space. This smoothing technique can be e.g. non-parametric regression or kernel smoothing.",
        "published": "2008-07-31T17:40:55Z",
        "link": "http://arxiv.org/abs/0807.5120v2",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "HEP data analysis using jHepWork and Java",
        "authors": [
            "S. Chekanov"
        ],
        "summary": "A role of Java in high-energy physics and recent progress in development of a platform-independent data-analysis framework, jHepWork, is discussed. The framework produces professional graphics and has many libraries for data manipulation.",
        "published": "2008-09-04T15:33:23Z",
        "link": "http://arxiv.org/abs/0809.0840v2",
        "categories": [
            "cs.CE",
            "hep-ex",
            "hep-ph"
        ]
    },
    {
        "title": "Electricity Demand and Energy Consumption Management System",
        "authors": [
            "Juan Ojeda Sarmiento"
        ],
        "summary": "This project describes the electricity demand and energy consumption management system and its application to Southern Peru smelter. It is composed of an hourly demand-forecasting module and of a simulation component for a plant electrical system. The first module was done using dynamic neural networks with backpropagation training algorithm; it is used to predict the electric power demanded every hour, with an error percentage below of 1%. This information allows efficient management of energy peak demands before this happen, distributing the raise of electric load to other hours or improving those equipments that increase the demand. The simulation module is based in advanced estimation techniques, such as: parametric estimation, neural network modeling, statistic regression and previously developed models, which simulates the electric behavior of the smelter plant. These modules facilitate electricity demand and consumption proper planning, because they allow knowing the behavior of the hourly demand and the consumption patterns of the plant, including the bill components, but also energy deficiencies and opportunities for improvement, based on analysis of information about equipments, processes and production plans, as well as maintenance programs. Finally the results of its application in Southern Peru smelter are presented.",
        "published": "2008-09-14T22:26:49Z",
        "link": "http://arxiv.org/abs/0809.2421v5",
        "categories": [
            "cs.AI",
            "cs.CE"
        ]
    },
    {
        "title": "Mathematical Tool of Discrete Dynamic Modeling of Complex Systems in   Control Loop",
        "authors": [
            "Armen Bagdasaryan"
        ],
        "summary": "In this paper we present a method of discrete modeling and analysis of multi-level dynamics of complex large-scale hierarchical dynamic systems subject to external dynamic control mechanism. In a model each state describes parallel dynamics and simultaneous trends of changes in system parameters. The essence of the approach is in analysis of system state dynamics while it is in the control loop.",
        "published": "2008-09-16T11:14:20Z",
        "link": "http://arxiv.org/abs/0809.2680v1",
        "categories": [
            "cs.MA",
            "cs.CE"
        ]
    },
    {
        "title": "A Control Variate Approach for Improving Efficiency of Ensemble Monte   Carlo",
        "authors": [
            "T. Borogovac",
            "F. J. Alexander",
            "P. Vakili"
        ],
        "summary": "In this paper we present a new approach to control variates for improving computational efficiency of Ensemble Monte Carlo. We present the approach using simulation of paths of a time-dependent nonlinear stochastic equation. The core idea is to extract information at one or more nominal model parameters and use this information to gain estimation efficiency at neighboring parameters. This idea is the basis of a general strategy, called DataBase Monte Carlo (DBMC), for improving efficiency of Monte Carlo. In this paper we describe how this strategy can be implemented using the variance reduction technique of Control Variates (CV). We show that, once an initial setup cost for extracting information is incurred, this approach can lead to significant gains in computational efficiency. The initial setup cost is justified in projects that require a large number of estimations or in those that are to be performed under real-time constraints.",
        "published": "2008-09-18T15:29:40Z",
        "link": "http://arxiv.org/abs/0809.3187v1",
        "categories": [
            "cs.CE",
            "cond-mat.stat-mech",
            "stat.CO",
            "G.3; J.2; I.6.8"
        ]
    },
    {
        "title": "Mathematical and computer tools of discrete dynamic modeling and   analysis of complex systems in control loop",
        "authors": [
            "Armen Bagdasaryan"
        ],
        "summary": "We present a method of discrete modeling and analysis of multilevel dynamics of complex large-scale hierarchical dynamic systems subject to external dynamic control mechanism. Architectural model of information system supporting simulation and analysis of dynamic processes and development scenarios (strategies) of complex large-scale hierarchical systems is also proposed.",
        "published": "2008-09-22T12:03:52Z",
        "link": "http://arxiv.org/abs/0809.3688v1",
        "categories": [
            "cs.CE",
            "cs.MA"
        ]
    },
    {
        "title": "New avenue to the Parton Distribution Functions: Self-Organizing Maps",
        "authors": [
            "J. Carnahan",
            "H. Honkanen",
            "S. Liuti",
            "Y. Loitiere",
            "P. R. Reynolds"
        ],
        "summary": "Neural network algorithms have been recently applied to construct Parton Distribution Function (PDF) parametrizations which provide an alternative to standard global fitting procedures. We propose a technique based on an interactive neural network algorithm using Self-Organizing Maps (SOMs). SOMs are a class of clustering algorithms based on competitive learning among spatially-ordered neurons. Our SOMs are trained on selections of stochastically generated PDF samples. The selection criterion for every optimization iteration is based on the features of the clustered PDFs. Our main goal is to provide a fitting procedure that, at variance with the standard neural network approaches, allows for an increased control of the systematic bias by enabling user interaction in the various stages of the process.",
        "published": "2008-10-15T05:41:51Z",
        "link": "http://arxiv.org/abs/0810.2598v2",
        "categories": [
            "hep-ph",
            "cs.CE"
        ]
    },
    {
        "title": "The Design of Compressive Sensing Filter",
        "authors": [
            "Lianlin Li",
            "Wenji Zhang",
            "Yin Xiang",
            "Fang Li"
        ],
        "summary": "In this paper, the design of universal compressive sensing filter based on normal filters including the lowpass, highpass, bandpass, and bandstop filters with different cutoff frequencies (or bandwidth) has been developed to enable signal acquisition with sub-Nyquist sampling. Moreover, to control flexibly the size and the coherence of the compressive sensing filter, as an example, the microstrip filter based on defected ground structure (DGS) has been employed to realize the compressive sensing filter. Of course, the compressive sensing filter also can be constructed along the identical idea by many other structures, for example, the man-made electromagnetic materials, the plasma with different electron density, and so on. By the proposed architecture, the n-dimensional signals of S-sparse in arbitrary orthogonal frame can be exactly reconstructed with measurements on the order of Slog(n) with overwhelming probability, which is consistent with the bonds estimated by theoretical analysis.",
        "published": "2008-11-17T08:33:49Z",
        "link": "http://arxiv.org/abs/0811.2637v1",
        "categories": [
            "cs.CE",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "System Theoretic Viewpoint on Modeling of Complex Systems: Design,   Synthesis, Simulation, and Control",
        "authors": [
            "Armen Bagdasaryan"
        ],
        "summary": "We consider the basic features of complex dynamic and control systems, including systems having hierarchical structure. Special attention is paid to the problems of design and synthesis of complex systems and control models, and to the development of simulation techniques and systems. A model of complex system is proposed and briefly analyzed.",
        "published": "2008-12-24T12:07:48Z",
        "link": "http://arxiv.org/abs/0812.4523v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Certifying floating-point implementations using Gappa",
        "authors": [
            "Florent De Dinechin",
            "Christoph Quirin Lauter",
            "Guillaume Melquiond"
        ],
        "summary": "High confidence in floating-point programs requires proving numerical properties of final and intermediate values. One may need to guarantee that a value stays within some range, or that the error relative to some ideal value is well bounded. Such work may require several lines of proof for each line of code, and will usually be broken by the smallest change to the code (e.g. for maintenance or optimization purpose). Certifying these programs by hand is therefore very tedious and error-prone. This article discusses the use of the Gappa proof assistant in this context. Gappa has two main advantages over previous approaches: Its input format is very close to the actual C code to validate, and it automates error evaluation and propagation using interval arithmetic. Besides, it can be used to incrementally prove complex mathematical properties pertaining to the C code. Yet it does not require any specific knowledge about automatic theorem proving, and thus is accessible to a wide community. Moreover, Gappa may generate a formal proof of the results that can be checked independently by a lower-level proof assistant like Coq, hence providing an even higher confidence in the certification of the numerical code. The article demonstrates the use of this tool on a real-size example, an elementary function with correctly rounded output.",
        "published": "2008-01-03T13:34:03Z",
        "link": "http://arxiv.org/abs/0801.0523v1",
        "categories": [
            "cs.NA",
            "cs.MS"
        ]
    },
    {
        "title": "Generic and Typical Ranks of Three-Way Arrays",
        "authors": [
            "P. Comon",
            "J. ten Berge"
        ],
        "summary": "The concept of tensor rank, introduced in the twenties, has been popularized at the beginning of the seventies. This has allowed to carry out Factor Analysis on arrays with more than two indices. The generic rank may be seen as an upper bound to the number of factors that can be extracted from a given tensor. We explain in this short paper how to obtain numerically the generic rank of tensors of arbitrary dimensions, and compare it with the rare algebraic results already known at order three. In particular, we examine the cases of symmetric tensors, tensors with symmetric matrix slices, or tensors with free entries.",
        "published": "2008-02-17T09:48:07Z",
        "link": "http://arxiv.org/abs/0802.2371v1",
        "categories": [
            "cs.OH",
            "cs.MS",
            "F.1.3; G.0; G.3; I.1; J.2"
        ]
    },
    {
        "title": "Optimizing polynomials for floating-point implementation",
        "authors": [
            "Florent De Dinechin",
            "Christoph Quirin Lauter"
        ],
        "summary": "The floating-point implementation of a function on an interval often reduces to polynomial approximation, the polynomial being typically provided by Remez algorithm. However, the floating-point evaluation of a Remez polynomial sometimes leads to catastrophic cancellations. This happens when some of the polynomial coefficients are very small in magnitude with respects to others. In this case, it is better to force these coefficients to zero, which also reduces the operation count. This technique, classically used for odd or even functions, may be generalized to a much larger class of functions. An algorithm is presented that forces to zero the smaller coefficients of the initial polynomial thanks to a modified Remez algorithm targeting an incomplete monomial basis. One advantage of this technique is that it is purely numerical, the function being used as a numerical black box. This algorithm is implemented within a larger polynomial implementation tool that is demonstrated on a range of examples, resulting in polynomials with less coefficients than those obtained the usual way.",
        "published": "2008-03-04T13:49:44Z",
        "link": "http://arxiv.org/abs/0803.0439v1",
        "categories": [
            "cs.NA",
            "cs.MS"
        ]
    },
    {
        "title": "A Method for Solving Cyclic Block Penta-diagonal Systems of Linear   Equations",
        "authors": [
            "Milan Batista"
        ],
        "summary": "A method for solving cyclic block three-diagonal systems of equations is generalized for solving a block cyclic penta-diagonal system of equations. Introducing a special form of two new variables the original system is split into three block pentagonal systems, which can be solved by the known methods. As such method belongs to class of direct methods without pivoting. Implementation of the algorithm is discussed in some details and the numerical examples are present.",
        "published": "2008-03-06T18:45:39Z",
        "link": "http://arxiv.org/abs/0803.0874v3",
        "categories": [
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "GraphStream: A Tool for bridging the gap between Complex Systems and   Dynamic Graphs",
        "authors": [
            "Yoann Pigné",
            "Antoine Dutot",
            "Frédéric Guinand",
            "Damien Olivier"
        ],
        "summary": "The notion of complex systems is common to many domains, from Biology to Economy, Computer Science, Physics, etc. Often, these systems are made of sets of entities moving in an evolving environment. One of their major characteristics is the emergence of some global properties stemmed from local interactions between the entities themselves and between the entities and the environment. The structure of these systems as sets of interacting entities leads researchers to model them as graphs. However, their understanding requires most often to consider the dynamics of their evolution. It is indeed not relevant to study some properties out of any temporal consideration. Thus, dynamic graphs seem to be a very suitable model for investigating the emergence and the conservation of some properties. GraphStream is a Java-based library whose main purpose is to help researchers and developers in their daily tasks of dynamic problem modeling and of classical graph management tasks: creation, processing, display, etc. It may also be used, and is indeed already used, for teaching purpose. GraphStream relies on an event-based engine allowing several event sources. Events may be included in the core of the application, read from a file or received from an event handler.",
        "published": "2008-03-14T07:09:13Z",
        "link": "http://arxiv.org/abs/0803.2093v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Conformal Computing: Algebraically connecting the hardware/software   boundary using a uniform approach to high-performance computation for   software and hardware applications",
        "authors": [
            "Lenore R. Mullin",
            "James E. Raynolds"
        ],
        "summary": "We present a systematic, algebraically based, design methodology for efficient implementation of computer programs optimized over multiple levels of the processor/memory and network hierarchy. Using a common formalism to describe the problem and the partitioning of data over processors and memory levels allows one to mathematically prove the efficiency and correctness of a given algorithm as measured in terms of a set of metrics (such as processor/network speeds, etc.). The approach allows the average programmer to achieve high-level optimizations similar to those used by compiler writers (e.g. the notion of \"tiling\").   The approach presented in this monograph makes use of A Mathematics of Arrays (MoA, Mullin 1988) and an indexing calculus (i.e. the psi-calculus) to enable the programmer to develop algorithms using high-level compiler-like optimizations through the ability to algebraically compose and reduce sequences of array operations. Extensive discussion and benchmark results are presented for the Fast Fourier Transform and other important algorithms.",
        "published": "2008-03-17T02:38:49Z",
        "link": "http://arxiv.org/abs/0803.2386v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "The QWalk Simulator of Quantum Walks",
        "authors": [
            "F. L. Marquezino",
            "R. Portugal"
        ],
        "summary": "Several research groups are giving special attention to quantum walks recently, because this research area have been used with success in the development of new efficient quantum algorithms. A general simulator of quantum walks is very important for the development of this area, since it allows the researchers to focus on the mathematical and physical aspects of the research instead of deviating the efforts to the implementation of specific numerical simulations. In this paper we present QWalk, a quantum walk simulator for one- and two-dimensional lattices. Finite two-dimensional lattices with generic topologies can be used. Decoherence can be simulated by performing measurements or by breaking links of the lattice. We use examples to explain the usage of the software and to show some recent results of the literature that are easily reproduced by the simulator.",
        "published": "2008-03-24T20:25:49Z",
        "link": "http://arxiv.org/abs/0803.3459v1",
        "categories": [
            "quant-ph",
            "cs.MS"
        ]
    },
    {
        "title": "Differentiation of Kaltofen's division-free determinant algorithm",
        "authors": [
            "Gilles Villard"
        ],
        "summary": "Kaltofen has proposed a new approach in [Kaltofen 1992] for computing matrix determinants. The algorithm is based on a baby steps/giant steps construction of Krylov subspaces, and computes the determinant as the constant term of a characteristic polynomial. For matrices over an abstract field and by the results of Baur and Strassen 1983, the determinant algorithm, actually a straight-line program, leads to an algorithm with the same complexity for computing the adjoint of a matrix [Kaltofen 1992]. However, the latter is obtained by the reverse mode of automatic differentiation and somehow is not ``explicit''. We study this adjoint algorithm, show how it can be implemented (without resorting to an automatic transformation), and demonstrate its use on polynomial matrices.",
        "published": "2008-04-07T12:37:43Z",
        "link": "http://arxiv.org/abs/0804.1021v1",
        "categories": [
            "cs.SC",
            "cs.MS"
        ]
    },
    {
        "title": "Certified Exact Transcendental Real Number Computation in Coq",
        "authors": [
            "Russell O'Connor"
        ],
        "summary": "Reasoning about real number expressions in a proof assistant is challenging. Several problems in theorem proving can be solved by using exact real number computation. I have implemented a library for reasoning and computing with complete metric spaces in the Coq proof assistant and used this library to build a constructive real number implementation including elementary real number functions and proofs of correctness. Using this library, I have created a tactic that automatically proves strict inequalities over closed elementary real number expressions by computation.",
        "published": "2008-05-16T18:02:24Z",
        "link": "http://arxiv.org/abs/0805.2438v1",
        "categories": [
            "cs.LO",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "Algorithmic Based Fault Tolerance Applied to High Performance Computing",
        "authors": [
            "George Bosilca",
            "Remi Delmas",
            "Jack Dongarra",
            "Julien Langou"
        ],
        "summary": "We present a new approach to fault tolerance for High Performance Computing system. Our approach is based on a careful adaptation of the Algorithmic Based Fault Tolerance technique (Huang and Abraham, 1984) to the need of parallel distributed computation. We obtain a strongly scalable mechanism for fault tolerance. We can also detect and correct errors (bit-flip) on the fly of a computation. To assess the viability of our approach, we have developed a fault tolerant matrix-matrix multiplication subroutine and we propose some models to predict its running time. Our parallel fault-tolerant matrix-matrix multiplication scores 1.4 TFLOPS on 484 processors (cluster jacquard.nersc.gov) and returns a correct result while one process failure has happened. This represents 65% of the machine peak efficiency and less than 12% overhead with respect to the fastest failure-free implementation. We predict (and have observed) that, as we increase the processor count, the overhead of the fault tolerance drops significantly.",
        "published": "2008-06-19T02:06:57Z",
        "link": "http://arxiv.org/abs/0806.3121v1",
        "categories": [
            "cs.DC",
            "cs.MS"
        ]
    },
    {
        "title": "Revisiting the upper bounding process in a safe Branch and Bound   algorithm",
        "authors": [
            "Alexandre Goldsztejn",
            "Yahia Lebbah",
            "Claude Michel",
            "Michel Rueher"
        ],
        "summary": "Finding feasible points for which the proof succeeds is a critical issue in safe Branch and Bound algorithms which handle continuous problems. In this paper, we introduce a new strategy to compute very accurate approximations of feasible points. This strategy takes advantage of the Newton method for under-constrained systems of equations and inequalities. More precisely, it exploits the optimal solution of a linear relaxation of the problem to compute efficiently a promising upper bound. First experiments on the Coconuts benchmarks demonstrate that this approach is very effective.",
        "published": "2008-07-15T14:18:23Z",
        "link": "http://arxiv.org/abs/0807.2382v1",
        "categories": [
            "cs.NA",
            "cs.MS",
            "math.OC"
        ]
    },
    {
        "title": "Executable Set Theory and Arithmetic Encodings in Prolog",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "The paper is organized as a self-contained literate Prolog program that implements elements of an executable finite set theory with focus on combinatorial generation and arithmetic encodings. The complete Prolog code is available at http://logic.csci.unt.edu/tarau/research/2008/pHFS.zip . First, ranking and unranking functions for some \"mathematically elegant\" data types in the universe of Hereditarily Finite Sets with Urelements are provided, resulting in arithmetic encodings for powersets, hypergraphs, ordinals and choice functions. After implementing a digraph representation of Hereditarily Finite Sets we define {\\em decoration functions} that can recover well-founded sets from encodings of their associated acyclic digraphs. We conclude with an encoding of arbitrary digraphs and discuss a concept of duality induced by the set membership relation. In the process, we uncover the surprising possibility of internally sharing isomorphic objects, independently of their language level types and meanings.",
        "published": "2008-08-05T04:59:56Z",
        "link": "http://arxiv.org/abs/0808.0540v1",
        "categories": [
            "cs.LO",
            "cs.DM",
            "cs.DS",
            "cs.MS",
            "cs.SC"
        ]
    },
    {
        "title": "Ranking and Unranking of Hereditarily Finite Functions and Permutations",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "Prolog's ability to return multiple answers on backtracking provides an elegant mechanism to derive reversible encodings of combinatorial objects as Natural Numbers i.e. {\\em ranking} and {\\em unranking} functions. Starting from a generalization of Ackerman's encoding of Hereditarily Finite Sets with Urelements and a novel tupling/untupling operation, we derive encodings for Finite Functions and use them as building blocks for an executable theory of {\\em Hereditarily Finite Functions}. The more difficult problem of {\\em ranking} and {\\em unranking} {\\em Hereditarily Finite Permutations} is then tackled using Lehmer codes and factoradics.   The paper is organized as a self-contained literate Prolog program available at \\url{http://logic.csci.unt.edu/tarau/research/2008/pHFF.zip}",
        "published": "2008-08-05T05:20:51Z",
        "link": "http://arxiv.org/abs/0808.0554v1",
        "categories": [
            "cs.LO",
            "cs.MS"
        ]
    },
    {
        "title": "A Functional Hitchhiker's Guide to Hereditarily Finite Sets, Ackermann   Encodings and Pairing Functions",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "The paper is organized as a self-contained literate Haskell program that implements elements of an executable finite set theory with focus on combinatorial generation and arithmetic encodings. The code, tested under GHC 6.6.1, is available at http://logic.csci.unt.edu/tarau/research/2008/fSET.zip .   We introduce ranking and unranking functions generalizing Ackermann's encoding to the universe of Hereditarily Finite Sets with Urelements. Then we build a lazy enumerator for Hereditarily Finite Sets with Urelements that matches the unranking function provided by the inverse of Ackermann's encoding and we describe functors between them resulting in arithmetic encodings for powersets, hypergraphs, ordinals and choice functions. After implementing a digraph representation of Hereditarily Finite Sets we define {\\em decoration functions} that can recover well-founded sets from encodings of their associated acyclic digraphs. We conclude with an encoding of arbitrary digraphs and discuss a concept of duality induced by the set membership relation.   Keywords: hereditarily finite sets, ranking and unranking functions, executable set theory, arithmetic encodings, Haskell data representations, functional programming and computational mathematics",
        "published": "2008-08-06T01:05:09Z",
        "link": "http://arxiv.org/abs/0808.0754v1",
        "categories": [
            "cs.MS",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Accelerating Scientific Computations with Mixed Precision Algorithms",
        "authors": [
            "Marc Baboulin",
            "Alfredo Buttari",
            "Jack Dongarra",
            "Jakub Kurzak",
            "Julie Langou",
            "Julien Langou",
            "Piotr Luszczek",
            "Stanimire Tomov"
        ],
        "summary": "On modern architectures, the performance of 32-bit operations is often at least twice as fast as the performance of 64-bit operations. By using a combination of 32-bit and 64-bit floating point arithmetic, the performance of many dense and sparse linear algebra algorithms can be significantly enhanced while maintaining the 64-bit accuracy of the resulting solution. The approach presented here can apply not only to conventional processors but also to other technologies such as Field Programmable Gate Arrays (FPGA), Graphical Processing Units (GPU), and the STI Cell BE processor. Results on modern processor architectures and the STI Cell BE are presented.",
        "published": "2008-08-20T17:50:36Z",
        "link": "http://arxiv.org/abs/0808.2794v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Interpolation of Shifted-Lacunary Polynomials",
        "authors": [
            "Mark Giesbrecht",
            "Daniel S. Roche"
        ],
        "summary": "Given a \"black box\" function to evaluate an unknown rational polynomial f in Q[x] at points modulo a prime p, we exhibit algorithms to compute the representation of the polynomial in the sparsest shifted power basis. That is, we determine the sparsity t, the shift s (a rational), the exponents 0 <= e1 < e2 < ... < et, and the coefficients c1,...,ct in Q\\{0} such that f(x) = c1(x-s)^e1+c2(x-s)^e2+...+ct(x-s)^et. The computed sparsity t is absolutely minimal over any shifted power basis. The novelty of our algorithm is that the complexity is polynomial in the (sparse) representation size, and in particular is logarithmic in deg(f). Our method combines previous celebrated results on sparse interpolation and computing sparsest shifts, and provides a way to handle polynomials with extremely high degree which are, in some sense, sparse in information.",
        "published": "2008-10-31T13:35:08Z",
        "link": "http://arxiv.org/abs/0810.5685v6",
        "categories": [
            "cs.SC",
            "cs.DS",
            "cs.MS",
            "68W30 (Primary), 12Y05 (Secondary)"
        ]
    },
    {
        "title": "Parallel GPU Implementation of Iterative PCA Algorithms",
        "authors": [
            "M. Andrecut"
        ],
        "summary": "Principal component analysis (PCA) is a key statistical technique for multivariate data analysis. For large data sets the common approach to PCA computation is based on the standard NIPALS-PCA algorithm, which unfortunately suffers from loss of orthogonality, and therefore its applicability is usually limited to the estimation of the first few components. Here we present an algorithm based on Gram-Schmidt orthogonalization (called GS-PCA), which eliminates this shortcoming of NIPALS-PCA. Also, we discuss the GPU (Graphics Processing Unit) parallel implementation of both NIPALS-PCA and GS-PCA algorithms. The numerical results show that the GPU parallel optimized versions, based on CUBLAS (NVIDIA) are substantially faster (up to 12 times) than the CPU optimized versions based on CBLAS (GNU Scientific Library).",
        "published": "2008-11-07T04:34:01Z",
        "link": "http://arxiv.org/abs/0811.1081v1",
        "categories": [
            "q-bio.QM",
            "cs.MS",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Efficient Multiplication of Dense Matrices over GF(2)",
        "authors": [
            "Martin Albrecht",
            "Gregory Bard",
            "William Hart"
        ],
        "summary": "We describe an efficient implementation of a hierarchy of algorithms for multiplication of dense matrices over the field with two elements (GF(2)). In particular we present our implementation -- in the M4RI library -- of Strassen-Winograd matrix multiplication and the \"Method of the Four Russians\" multiplication (M4RM) and compare it against other available implementations. Good performance is demonstrated on on AMD's Opteron and particulary good performance on Intel's Core 2 Duo. The open-source M4RI library is available stand-alone as well as part of the Sage mathematics software.   In machine terms, addition in GF(2) is logical-XOR, and multiplication is logical-AND, thus a machine word of 64-bits allows one to operate on 64 elements of GF(2) in parallel: at most one CPU cycle for 64 parallel additions or multiplications. As such, element-wise operations over GF(2) are relatively cheap. In fact, in this paper, we conclude that the actual bottlenecks are memory reads and writes and issues of data locality. We present our empirical findings in relation to minimizing these and give an analysis thereof.",
        "published": "2008-11-11T14:23:49Z",
        "link": "http://arxiv.org/abs/0811.1714v1",
        "categories": [
            "cs.MS",
            "G.4"
        ]
    },
    {
        "title": "Geometric scaling: a simple preconditioner for certain linear systems   with discontinuous coefficients",
        "authors": [
            "Dan Gordon",
            "Rachel Gordon"
        ],
        "summary": "Linear systems with large differences between coefficients (\"discontinuous coefficients\") arise in many cases in which partial differential equations(PDEs) model physical phenomena involving heterogeneous media. The standard approach to solving such problems is to use domain decomposition techniques, with domain boundaries conforming to the boundaries between the different media. This approach can be difficult to implement when the geometry of the domain boundaries is complicated or the grid is unstructured. This work examines the simple preconditioning technique of scaling the equations by dividing each equation by the Lp-norm of its coefficients. This preconditioning is called geometric scaling (GS). It has long been known that diagonal scaling can be useful in improving convergence, but there is no study on the general usefulness of this approach for discontinuous coefficients. GS was tested on several nonsymmetric linear systems with discontinuous coefficients derived from convection-diffusion elliptic PDEs with small to moderate convection terms. It is shown that GS improved the convergence properties of restarted GMRES and Bi-CGSTAB, with and without the ILUT preconditioner. GS was also shown to improve the distribution of the eigenvalues by reducing their concentration around the origin very significantly.",
        "published": "2008-12-15T11:35:17Z",
        "link": "http://arxiv.org/abs/0812.2769v2",
        "categories": [
            "cs.MS",
            "cs.NA",
            "G.1.3; G.1.8; G.4"
        ]
    },
    {
        "title": "On sign conditions over real multivariate polynomials",
        "authors": [
            "Gabriela Jeronimo",
            "Daniel Perrucci",
            "Juan Sabia"
        ],
        "summary": "We present a new probabilistic algorithm to find a finite set of points intersecting the closure of each connected component of the realization of every sign condition over a family of real polynomials defining regular hypersurfaces that intersect transversally. This enables us to show a probabilistic procedure to list all feasible sign conditions over the polynomials. In addition, we extend these results to the case of closed sign conditions over an arbitrary family of real multivariate polynomials. The complexity bounds for these procedures improve the known ones.",
        "published": "2008-01-03T20:03:05Z",
        "link": "http://arxiv.org/abs/0801.0586v2",
        "categories": [
            "math.AG",
            "cs.CG",
            "cs.SC",
            "14P10; 14Q20; 68W30"
        ]
    },
    {
        "title": "Factorization in categories of systems of linear partial differential   equations",
        "authors": [
            "S. P. Tsarev"
        ],
        "summary": "We start with elementary algebraic theory of factorization of linear ordinary differential equations developed in the period 1880-1930. After exposing these classical results we sketch more sophisticated algorithmic approaches developed in the last 20 years.   The main part of this paper is devoted to modern generalizations of the notion of factorization to the case of systems of linear partial differential equations and their relation with explicit solvability of nonlinear partial differential equations based on some constructions from the theory of abelian categories.",
        "published": "2008-01-09T00:50:20Z",
        "link": "http://arxiv.org/abs/0801.1341v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Fast Integer Multiplication using Modular Arithmetic",
        "authors": [
            "Anindya De",
            "Piyush P Kurur",
            "Chandan Saha",
            "Ramprasad Saptharishi"
        ],
        "summary": "We give an $O(N\\cdot \\log N\\cdot 2^{O(\\log^*N)})$ algorithm for multiplying two $N$-bit integers that improves the $O(N\\cdot \\log N\\cdot \\log\\log N)$ algorithm by Sch\\\"{o}nhage-Strassen. Both these algorithms use modular arithmetic. Recently, F\\\"{u}rer gave an $O(N\\cdot \\log N\\cdot 2^{O(\\log^*N)})$ algorithm which however uses arithmetic over complex numbers as opposed to modular arithmetic. In this paper, we use multivariate polynomial multiplication along with ideas from F\\\"{u}rer's algorithm to achieve this improvement in the modular setting. Our algorithm can also be viewed as a $p$-adic version of F\\\"{u}rer's algorithm. Thus, we show that the two seemingly different approaches to integer multiplication, modular and complex arithmetic, are similar.",
        "published": "2008-01-09T12:44:55Z",
        "link": "http://arxiv.org/abs/0801.1416v3",
        "categories": [
            "cs.SC",
            "cs.DS"
        ]
    },
    {
        "title": "Analyzing the Topology Types arising in a Family of Algebraic Curves   Depending On Two Parameters",
        "authors": [
            "Juan Gerardo Alcazar"
        ],
        "summary": "Given the implicit equation $F(x,y,t,s)$ of a family of algebraic plane curves depending on the parameters $t,s$, we provide an algorithm for studying the topology types arising in the family. For this purpose, the algorithm computes a finite partition of the parameter space so that the topology type of the family stays invariant over each element of the partition. The ideas contained in the paper can be seen as a generalization of the ideas in \\cite{JGRS}, where the problem is solved for families of algebraic curves depending on one parameter, to the two-parameters case.",
        "published": "2008-01-10T21:08:22Z",
        "link": "http://arxiv.org/abs/0801.1676v2",
        "categories": [
            "cs.SC",
            "I.1.2"
        ]
    },
    {
        "title": "Hopf Algebras in General and in Combinatorial Physics: a practical   introduction",
        "authors": [
            "G. H. E. Duchamp",
            "P. Blasiak",
            "A. Horzela",
            "K. A. Penson",
            "A. I. Solomon"
        ],
        "summary": "This tutorial is intended to give an accessible introduction to Hopf algebras. The mathematical context is that of representation theory, and we also illustrate the structures with examples taken from combinatorics and quantum physics, showing that in this latter case the axioms of Hopf algebra arise naturally. The text contains many exercises, some taken from physics, aimed at expanding and exemplifying the concepts introduced.",
        "published": "2008-02-02T15:06:41Z",
        "link": "http://arxiv.org/abs/0802.0249v1",
        "categories": [
            "quant-ph",
            "cs.SC",
            "math.CO"
        ]
    },
    {
        "title": "Approximate substitutions and the normal ordering problem",
        "authors": [
            "H. Cheballah",
            "G. H. E. Duchamp",
            "K. A. Penson"
        ],
        "summary": "In this paper, we show that the infinite generalised Stirling matrices associated with boson strings with one annihilation operator are projective limits of approximate substitutions, the latter being characterised by a finite set of algebraic equations.",
        "published": "2008-02-08T14:52:52Z",
        "link": "http://arxiv.org/abs/0802.1162v1",
        "categories": [
            "quant-ph",
            "cs.SC",
            "math.CO"
        ]
    },
    {
        "title": "The Invar tensor package: Differential invariants of Riemann",
        "authors": [
            "Jose M. Martin-Garcia",
            "David Yllanes",
            "Renato Portugal"
        ],
        "summary": "The long standing problem of the relations among the scalar invariants of the Riemann tensor is computationally solved for all 6x10^23 objects with up to 12 derivatives of the metric. This covers cases ranging from products of up to 6 undifferentiated Riemann tensors to cases with up to 10 covariant derivatives of a single Riemann. We extend our computer algebra system Invar to produce within seconds a canonical form for any of those objects in terms of a basis. The process is as follows: (1) an invariant is converted in real time into a canonical form with respect to the permutation symmetries of the Riemann tensor; (2) Invar reads a database of more than 6x10^5 relations and applies those coming from the cyclic symmetry of the Riemann tensor; (3) then applies the relations coming from the Bianchi identity, (4) the relations coming from commutations of covariant derivatives, (5) the dimensionally-dependent identities for dimension 4, and finally (6) simplifies invariants that can be expressed as product of dual invariants. Invar runs on top of the tensor computer algebra systems xTensor (for Mathematica) and Canon (for Maple).",
        "published": "2008-02-11T16:38:05Z",
        "link": "http://arxiv.org/abs/0802.1274v1",
        "categories": [
            "cs.SC",
            "gr-qc",
            "hep-th"
        ]
    },
    {
        "title": "Kolmogorov Complexity Theory over the Reals",
        "authors": [
            "Martin Ziegler",
            "Wouter M. Koolen"
        ],
        "summary": "Kolmogorov Complexity constitutes an integral part of computability theory, information theory, and computational complexity theory -- in the discrete setting of bits and Turing machines. Over real numbers, on the other hand, the BSS-machine (aka real-RAM) has been established as a major model of computation. This real realm has turned out to exhibit natural counterparts to many notions and results in classical complexity and recursion theory; although usually with considerably different proofs. The present work investigates similarities and differences between discrete and real Kolmogorov Complexity as introduced by Montana and Pardo (1998).",
        "published": "2008-02-14T18:30:55Z",
        "link": "http://arxiv.org/abs/0802.2027v2",
        "categories": [
            "cs.CC",
            "cs.SC",
            "F.4.1; F.1.1; E.4; I.1.2; I.1.3"
        ]
    },
    {
        "title": "Reconstruction of eye movements during blinks",
        "authors": [
            "M. S. Baptista",
            "C. Bohn",
            "R. Kliegl",
            "R. Engbert",
            "J. Kurths"
        ],
        "summary": "In eye movement research in reading, the amount of data plays a crucial role for the validation of results. A methodological problem for the analysis of the eye movement in reading are blinks, when readers close their eyes. Blinking rate increases with increasing reading time, resulting in high data losses, especially for older adults or reading impaired subjects. We present a method, based on the symbolic sequence dynamics of the eye movements, that reconstructs the horizontal position of the eyes while the reader blinks. The method makes use of an observed fact that the movements of the eyes before closing or after opening contain information about the eyes movements during blinks. Test results indicate that our reconstruction method is superior to methods that use simpler interpolation approaches. In addition, analyses of the reconstructed data show no significant deviation from the usual behavior observed in readers.",
        "published": "2008-02-15T13:37:27Z",
        "link": "http://arxiv.org/abs/0802.2201v2",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Using Alloy to model-check visual design notations",
        "authors": [
            "Anthony J. H. Simons",
            "Carlos Alberto Fernandez-y-Fernandez"
        ],
        "summary": "This paper explores the process of validation for the abstract syntax of a graphical notation. We define an unified specification for five of the UML diagrams used by the Discovery Method and, in this document, we illustrate how diagrams can be represented in Alloy and checked against our specification in order to know if these are valid under the Discovery notation.",
        "published": "2008-02-15T18:25:50Z",
        "link": "http://arxiv.org/abs/0802.2258v1",
        "categories": [
            "cs.SE",
            "cs.SC",
            "I.6.4; D.3.1; I.3.5"
        ]
    },
    {
        "title": "xPerm: fast index canonicalization for tensor computer algebra",
        "authors": [
            "Jose M. Martin-Garcia"
        ],
        "summary": "We present a very fast implementation of the Butler-Portugal algorithm for index canonicalization with respect to permutation symmetries. It is called xPerm, and has been written as a combination of a Mathematica package and a C subroutine. The latter performs the most demanding parts of the computations and can be linked from any other program or computer algebra system. We demonstrate with tests and timings the effectively polynomial performance of the Butler-Portugal algorithm with respect to the number of indices, though we also show a case in which it is exponential. Our implementation handles generic tensorial expressions with several dozen indices in hundredths of a second, or one hundred indices in a few seconds, clearly outperforming all other current canonicalizers. The code has been already under intensive testing for several years and has been essential in recent investigations in large-scale tensor computer algebra.",
        "published": "2008-03-06T13:26:32Z",
        "link": "http://arxiv.org/abs/0803.0862v1",
        "categories": [
            "cs.SC",
            "gr-qc",
            "hep-th"
        ]
    },
    {
        "title": "On the Computation of the Topology of a Non-Reduced Implicit Space Curve",
        "authors": [
            "Daouda Niang Diatta",
            "Bernard Mourrain",
            "Olivier Ruatta"
        ],
        "summary": "An algorithm is presented for the computation of the topology of a non-reduced space curve defined as the intersection of two implicit algebraic surfaces. It computes a Piecewise Linear Structure (PLS) isotopic to the original space curve. The algorithm is designed to provide the exact result for all inputs. It's a symbolic-numeric algorithm based on subresultant computation. Simple algebraic criteria are given to certify the output of the algorithm. The algorithm uses only one projection of the non-reduced space curve augmented with adjacency information around some \"particular points\" of the space curve. The algorithm is implemented with the Mathemagix Computer Algebra System (CAS) using the SYNAPS library as a backend.",
        "published": "2008-03-07T15:28:52Z",
        "link": "http://arxiv.org/abs/0803.1110v1",
        "categories": [
            "math.AC",
            "cs.CG",
            "cs.SC"
        ]
    },
    {
        "title": "Compressed Modular Matrix Multiplication",
        "authors": [
            "Jean-Guillaume Dumas",
            "Laurent Fousse",
            "Bruno Salvy"
        ],
        "summary": "We propose to store several integers modulo a small prime into a single machine word. Modular addition is performed by addition and possibly subtraction of a word containing several times the modulo. Modular Multiplication is not directly accessible but modular dot product can be performed by an integer multiplication by the reverse integer. Modular multiplication by a word containing a single residue is a also possible. Therefore matrix multiplication can be performed on such a compressed storage. We here give bounds on the sizes of primes and matrices for which such a compression is possible. We also explicit the details of the required compressed arithmetic routines.",
        "published": "2008-03-13T19:15:42Z",
        "link": "http://arxiv.org/abs/0803.1975v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Two Algorithms for Solving A General Backward Pentadiagonal Linear   Systems",
        "authors": [
            "A. A. Karawia"
        ],
        "summary": "In this paper we present an efficient computational and symbolic algorithms for solving a backward pentadiagonal linear systems. The implementation of the algorithms using Computer Algebra Systems (CAS) such as MAPLE, MACSYMA, MATHEMATICA, and MATLAB are straightforward. An examples are given in order to illustrate the algorithms. The symbolic algorithm is competitive the other methods for solving a backward pentadiagonal linear systems.",
        "published": "2008-03-15T20:04:18Z",
        "link": "http://arxiv.org/abs/0803.2319v1",
        "categories": [
            "cs.SC",
            "cs.NA",
            "G.1.3; I.1.2"
        ]
    },
    {
        "title": "Towards a Symbolic-Numeric Method to Compute Puiseux Series: The Modular   Part",
        "authors": [
            "Adrien Poteaux",
            "Marc Rybowicz"
        ],
        "summary": "We have designed a new symbolic-numeric strategy to compute efficiently and accurately floating point Puiseux series defined by a bivariate polynomial over an algebraic number field. In essence, computations modulo a well chosen prime $p$ are used to obtain the exact information required to guide floating point computations. In this paper, we detail the symbolic part of our algorithm: First of all, we study modular reduction of Puiseux series and give a good reduction criterion to ensure that the information required by the numerical part is preserved. To establish our results, we introduce a simple modification of classical Newton polygons, that we call \"generic Newton polygons\", which happen to be very convenient. Then, we estimate the arithmetic complexity of computing Puiseux series over finite fields and improve known bounds. Finally, we give bit-complexity bounds for deterministic and randomized versions of the symbolic part. The details of the numerical part will be described in a forthcoming paper.",
        "published": "2008-03-20T16:23:40Z",
        "link": "http://arxiv.org/abs/0803.3027v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Decomposing replicable functions",
        "authors": [
            "John McKay",
            "David Sevilla"
        ],
        "summary": "We describe an algorithm to decompose rational functions from which we determine the poset of groups fixing these functions.",
        "published": "2008-03-24T16:02:26Z",
        "link": "http://arxiv.org/abs/0803.3419v2",
        "categories": [
            "math.NT",
            "cs.SC",
            "math.RT",
            "11F11, 30F35 (Primary); 68W30 (Secondary)"
        ]
    },
    {
        "title": "Twenty-Five Moves Suffice for Rubik's Cube",
        "authors": [
            "Tomas Rokicki"
        ],
        "summary": "How many moves does it take to solve Rubik's Cube? Positions are known that require 20 moves, and it has already been shown that there are no positions that require 27 or more moves; this is a surprisingly large gap. This paper describes a program that is able to find solutions of length 20 or less at a rate of more than 16 million positions a second. We use this program, along with some new ideas and incremental improvements in other techniques, to show that there is no position that requires 26 moves.",
        "published": "2008-03-24T19:37:09Z",
        "link": "http://arxiv.org/abs/0803.3435v1",
        "categories": [
            "cs.SC",
            "cs.DM"
        ]
    },
    {
        "title": "Preferred extensions as stable models",
        "authors": [
            "Juan Carlos Nieves",
            "Mauricio Osorio",
            "Ulises Cortés"
        ],
        "summary": "Given an argumentation framework AF, we introduce a mapping function that constructs a disjunctive logic program P, such that the preferred extensions of AF correspond to the stable models of P, after intersecting each stable model with the relevant atoms. The given mapping function is of polynomial size w.r.t. AF. In particular, we identify that there is a direct relationship between the minimal models of a propositional formula and the preferred extensions of an argumentation framework by working on representing the defeated arguments. Then we show how to infer the preferred extensions of an argumentation framework by using UNSAT algorithms and disjunctive stable model solvers. The relevance of this result is that we define a direct relationship between one of the most satisfactory argumentation semantics and one of the most successful approach of non-monotonic reasoning i.e., logic programming with the stable model semantics.",
        "published": "2008-03-26T20:08:31Z",
        "link": "http://arxiv.org/abs/0803.3812v1",
        "categories": [
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "On Ritt's decomposition Theorem in the case of finite fields",
        "authors": [
            "Jaime Gutierrez",
            "David Sevilla"
        ],
        "summary": "A classical theorem by Ritt states that all the complete decomposition chains of a univariate polynomial satisfying a certain tameness condition have the same length. In this paper we present our conclusions about the generalization of these theorem in the case of finite coefficient fields when the tameness condition is dropped.",
        "published": "2008-03-27T16:39:03Z",
        "link": "http://arxiv.org/abs/0803.3976v2",
        "categories": [
            "cs.SC",
            "math.AC",
            "I.1.1; I.1.2"
        ]
    },
    {
        "title": "Differentiation of Kaltofen's division-free determinant algorithm",
        "authors": [
            "Gilles Villard"
        ],
        "summary": "Kaltofen has proposed a new approach in [Kaltofen 1992] for computing matrix determinants. The algorithm is based on a baby steps/giant steps construction of Krylov subspaces, and computes the determinant as the constant term of a characteristic polynomial. For matrices over an abstract field and by the results of Baur and Strassen 1983, the determinant algorithm, actually a straight-line program, leads to an algorithm with the same complexity for computing the adjoint of a matrix [Kaltofen 1992]. However, the latter is obtained by the reverse mode of automatic differentiation and somehow is not ``explicit''. We study this adjoint algorithm, show how it can be implemented (without resorting to an automatic transformation), and demonstrate its use on polynomial matrices.",
        "published": "2008-04-07T12:37:43Z",
        "link": "http://arxiv.org/abs/0804.1021v1",
        "categories": [
            "cs.SC",
            "cs.MS"
        ]
    },
    {
        "title": "On decomposition of tame polynomials and rational functions",
        "authors": [
            "Jaime Gutierrez",
            "David Sevilla"
        ],
        "summary": "In this paper we present algorithmic considerations and theoretical results about the relation between the orders of certain groups associated to the components of a polynomial and the order of the group that corresponds to the polynomial, proving it for arbitrary tame polynomials, and considering the case of rational functions.",
        "published": "2008-04-10T09:31:04Z",
        "link": "http://arxiv.org/abs/0804.1649v1",
        "categories": [
            "cs.SC",
            "math.AC",
            "I.1.1; I.1.2"
        ]
    },
    {
        "title": "Computation of unirational fields",
        "authors": [
            "Jaime Gutierrez",
            "David Sevilla"
        ],
        "summary": "One of the main contributions which Volker Weispfenning made to mathematics is related to Groebner bases theory. In this paper we present an algorithm for computing all algebraic intermediate subfields in a separably generated unirational field extension (which in particular includes the zero characteristic case). One of the main tools is Groebner bases theory. Our algorithm also requires computing primitive elements and factoring over algebraic extensions. Moreover, the method can be extended to finitely generated K-algebras.",
        "published": "2008-04-10T11:59:15Z",
        "link": "http://arxiv.org/abs/0804.1679v1",
        "categories": [
            "cs.SC",
            "math.AC",
            "I.1.1; I.1.2"
        ]
    },
    {
        "title": "Building counterexamples to generalizations for rational functions of   Ritt's decomposition theorem",
        "authors": [
            "Jaime Gutierrez",
            "David Sevilla"
        ],
        "summary": "The classical Ritt's Theorems state several properties of univariate polynomial decomposition. In this paper we present new counterexamples to Ritt's first theorem, which states the equality of length of decomposition chains of a polynomial, in the case of rational functions. Namely, we provide an explicit example of a rational function with coefficients in Q and two decompositions of different length.   Another aspect is the use of some techniques that could allow for other counterexamples, namely, relating groups and decompositions and using the fact that the alternating group A_4 has two subgroup chains of different lengths; and we provide more information about the generalizations of another property of polynomial decomposition: the stability of the base field. We also present an algorithm for computing the fixing group of a rational function providing the complexity over Q.",
        "published": "2008-04-10T12:42:16Z",
        "link": "http://arxiv.org/abs/0804.1687v1",
        "categories": [
            "math.AC",
            "cs.SC",
            "12Y05; 13P99; 68W30"
        ]
    },
    {
        "title": "Computation of unirational fields (extended abstract)",
        "authors": [
            "Jaime Gutierrez",
            "David Sevilla"
        ],
        "summary": "In this paper we present an algorithm for computing all algebraic intermediate subfields in a separably generated unirational field extension (which in particular includes the zero characteristic case). One of the main tools is Groebner bases theory. Our algorithm also requires computing computing primitive elements and factoring over algebraic extensions. Moreover, the method can be extended to finitely generated K-algebras.",
        "published": "2008-04-10T14:00:10Z",
        "link": "http://arxiv.org/abs/0804.1707v1",
        "categories": [
            "cs.SC",
            "math.AC",
            "I.1.1; I.1.2"
        ]
    },
    {
        "title": "Schemes for Deterministic Polynomial Factoring",
        "authors": [
            "Gábor Ivanyos",
            "Marek Karpinski",
            "Nitin Saxena"
        ],
        "summary": "In this work we relate the deterministic complexity of factoring polynomials (over finite fields) to certain combinatorial objects we call m-schemes. We extend the known conditional deterministic subexponential time polynomial factoring algorithm for finite fields to get an underlying m-scheme. We demonstrate how the properties of m-schemes relate to improvements in the deterministic complexity of factoring polynomials over finite fields assuming the generalized Riemann Hypothesis (GRH). In particular, we give the first deterministic polynomial time algorithm (assuming GRH) to find a nontrivial factor of a polynomial of prime degree n where (n-1) is a smooth number.",
        "published": "2008-04-11T23:04:17Z",
        "link": "http://arxiv.org/abs/0804.1974v1",
        "categories": [
            "cs.CC",
            "cs.SC"
        ]
    },
    {
        "title": "Products of Ordinary Differential Operators by Evaluation and   Interpolation",
        "authors": [
            "Alin Bostan",
            "Frédéric Chyzak",
            "Nicolas Le Roux"
        ],
        "summary": "It is known that multiplication of linear differential operators over ground fields of characteristic zero can be reduced to a constant number of matrix products. We give a new algorithm by evaluation and interpolation which is faster than the previously-known one by a constant factor, and prove that in characteristic zero, multiplication of differential operators and of matrices are computationally equivalent problems. In positive characteristic, we show that differential operators can be multiplied in nearly optimal time. Theoretical results are validated by intensive experiments.",
        "published": "2008-04-14T13:37:14Z",
        "link": "http://arxiv.org/abs/0804.2181v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Power Series Composition and Change of Basis",
        "authors": [
            "Alin Bostan",
            "Bruno Salvy",
            "Éric Schost"
        ],
        "summary": "Efficient algorithms are known for many operations on truncated power series (multiplication, powering, exponential, ...). Composition is a more complex task. We isolate a large class of power series for which composition can be performed efficiently. We deduce fast algorithms for converting polynomials between various bases, including Euler, Bernoulli, Fibonacci, and the orthogonal Laguerre, Hermite, Jacobi, Krawtchouk, Meixner and Meixner-Pollaczek.",
        "published": "2008-04-15T09:43:27Z",
        "link": "http://arxiv.org/abs/0804.2337v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Fast Conversion Algorithms for Orthogonal Polynomials",
        "authors": [
            "Alin Bostan",
            "Bruno Salvy",
            "Éric Schost"
        ],
        "summary": "We discuss efficient conversion algorithms for orthogonal polynomials. We describe a known conversion algorithm from an arbitrary orthogonal basis to the monomial basis, and deduce a new algorithm of the same complexity for the converse operation.",
        "published": "2008-04-15T12:47:14Z",
        "link": "http://arxiv.org/abs/0804.2373v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Experiments in Model-Checking Optimistic Replication Algorithms",
        "authors": [
            "Hanifa Boucheneb",
            "Abdessamad Imine"
        ],
        "summary": "This paper describes a series of model-checking experiments to verify optimistic replication algorithms based on Operational Transformation (OT) approach used for supporting collaborative edition. We formally define, using tool UPPAAL, the behavior and the main consistency requirement (i.e. convergence property) of the collaborative editing systems, as well as the abstract behavior of the environment where these systems are supposed to operate. Due to data replication and the unpredictable nature of user interactions, such systems have infinitely many states. So, we show how to exploit some features of the UPPAAL specification language to attenuate the severe state explosion problem. Two models are proposed. The first one, called concrete model, is very close to the system implementation but runs up against a severe explosion of states. The second model, called symbolic model, aims to overcome the limitation of the concrete model by delaying the effective selection and execution of editing operations until the construction of symbolic execution traces of all sites is completed. Experimental results have shown that the symbolic model allows a significant gain in both space and time. Using the symbolic model, we have been able to show that if the number of sites exceeds 2 then the convergence property is not satisfied for all OT algorithms considered here. A counterexample is provided for every algorithm.",
        "published": "2008-04-18T14:04:38Z",
        "link": "http://arxiv.org/abs/0804.3023v2",
        "categories": [
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "\"E pluribus unum\" or How to Derive Single-equation Descriptions for   Output-quantities in Nonlinear Circuits using Differential Algebra",
        "authors": [
            "Eberhard H. -A. Gerbracht"
        ],
        "summary": "In this paper we describe by a number of examples how to deduce one single characterizing higher order differential equation for output quantities of an analog circuit.   In the linear case, we apply basic \"symbolic\" methods from linear algebra to the system of differential equations which is used to model the analog circuit. For nonlinear circuits and their corresponding nonlinear differential equations, we show how to employ computer algebra tools implemented in Maple, which are based on differential algebra.",
        "published": "2008-04-18T19:55:58Z",
        "link": "http://arxiv.org/abs/0804.2992v1",
        "categories": [
            "cs.SC",
            "math.CA"
        ]
    },
    {
        "title": "Symbolic computations in differential geometry",
        "authors": [
            "Diego Conti"
        ],
        "summary": "We introduce the C++ library Wedge, based on GiNaC, for symbolic computations in differential geometry. We show how Wedge makes it possible to use the language C++ to perform such computations, and illustrate some advantages of this approach with explicit examples. In particular, we describe a short program to determine whether a given linear exterior differential system is involutive.",
        "published": "2008-04-20T14:55:56Z",
        "link": "http://arxiv.org/abs/0804.3193v1",
        "categories": [
            "math.DG",
            "cs.SC",
            "53-04 (Primary); 53C05, 68W30, 58A15 (Secondary)"
        ]
    },
    {
        "title": "Hochschild Homology and Cohomology of Klein Surfaces",
        "authors": [
            "Frédéric Butin"
        ],
        "summary": "Within the framework of deformation quantization, a first step towards the study of star-products is the calculation of Hochschild cohomology. The aim of this article is precisely to determine the Hochschild homology and cohomology in two cases of algebraic varieties. On the one hand, we consider singular curves of the plane; here we recover, in a different way, a result proved by Fronsdal and make it more precise. On the other hand, we are interested in Klein surfaces. The use of a complex suggested by Kontsevich and the help of Groebner bases allow us to solve the problem.",
        "published": "2008-04-28T06:06:07Z",
        "link": "http://arxiv.org/abs/0804.4324v2",
        "categories": [
            "math-ph",
            "cs.SC",
            "math.AC",
            "math.MP",
            "math.QA",
            "math.RA"
        ]
    },
    {
        "title": "A Simple Dynamic Mind-map Framework To Discover Associative   Relationships in Transactional Data Streams",
        "authors": [
            "Christoph Schommer"
        ],
        "summary": "In this paper, we informally introduce dynamic mind-maps that represent a new approach on the basis of a dynamic construction of connectionist structures during the processing of a data stream. This allows the representation and processing of recursively defined structures and avoids the problem of a more traditional, fixed-size architecture with the processing of input structures of unknown size. For a data stream analysis with association discovery, the incremental analysis of data leads to results on demand. Here, we describe a framework that uses symbolic cells to calculate associations based on transactional data streams as it exists in e.g. bibliographic databases. We follow a natural paradigm of applying simple operations on cells yielding on a mind-map structure that adapts over time.",
        "published": "2008-05-09T08:10:09Z",
        "link": "http://arxiv.org/abs/0805.1296v1",
        "categories": [
            "cs.NE",
            "cs.SC",
            "I.2.6; H.2.8"
        ]
    },
    {
        "title": "Grammatical Evolution with Restarts for Fast Fractal Generation",
        "authors": [
            "Manuel Cebrian",
            "Manuel Alfonseca",
            "Alfonso Ortega"
        ],
        "summary": "In a previous work, the authors proposed a Grammatical Evolution algorithm to automatically generate Lindenmayer Systems which represent fractal curves with a pre-determined fractal dimension. This paper gives strong statistical evidence that the probability distributions of the execution time of that algorithm exhibits a heavy tail with an hyperbolic probability decay for long executions, which explains the erratic performance of different executions of the algorithm. Three different restart strategies have been incorporated in the algorithm to mitigate the problems associated to heavy tail distributions: the first assumes full knowledge of the execution time probability distribution, the second and third assume no knowledge. These strategies exploit the fact that the probability of finding a solution in short executions is non-negligible and yield a severe reduction, both in the expected execution time (up to one order of magnitude) and in its variance, which is reduced from an infinite to a finite value.",
        "published": "2008-05-12T17:55:59Z",
        "link": "http://arxiv.org/abs/0805.1696v2",
        "categories": [
            "cs.NE",
            "cs.SC"
        ]
    },
    {
        "title": "Aplicacion de la descomposicion racional univariada a monstrous   moonshine (in Spanish)",
        "authors": [
            "John McKay",
            "David Sevilla"
        ],
        "summary": "This paper shows how to use Computational Algebra techniques, namely the decomposition of rational functions in one variable, to explore a certain set of modular functions, called replicable functions, that arise in Monstrous Moonshine. In particular, we have computed all the rational relations with coefficients in Z between pairs of replicable functions.   -----   En este articulo mostramos como usar tecnicas de Algebra Computacional, concretamente la descomposcion de funciones racionales univariadas, para estudiar un cierto conjunto de funciones modulares, llamadas funciones replicables, que aparecen en Monstrous Moonshine. En concreto, hemos calculado todas las relaciones racionales con coeficientes en Z entre pares de funciones replicables.",
        "published": "2008-05-15T14:02:45Z",
        "link": "http://arxiv.org/abs/0805.2311v1",
        "categories": [
            "math.NT",
            "cs.SC",
            "11F11, 30F35 (Primary), 68W30 (Secondary)"
        ]
    },
    {
        "title": "Computing the fixing group of a rational function",
        "authors": [
            "Jaime Gutierrez",
            "Rosario Rubio",
            "David Sevilla"
        ],
        "summary": "Let G=Aut_K (K(x)) be the Galois group of the transcendental degree one pure field extension K(x)/K. In this paper we describe polynomial time algorithms for computing the field Fix(H) fixed by a subgroup H < G and for computing the fixing group G_f of a rational function f in K(x).",
        "published": "2008-05-15T14:58:18Z",
        "link": "http://arxiv.org/abs/0805.2331v1",
        "categories": [
            "cs.SC",
            "math.AC",
            "I.1.1; I.1.2"
        ]
    },
    {
        "title": "Unirational fields of transcendence degree one and functional   decomposition",
        "authors": [
            "Jaime Gutierrez",
            "Rosario Rubio",
            "David Sevilla"
        ],
        "summary": "In this paper we present an algorithm to compute all unirational fields of transcendence degree one containing a given finite set of multivariate rational functions. In particular, we provide an algorithm to decompose a multivariate rational function f of the form f=g(h), where g is a univariate rational function and h a multivariate one.",
        "published": "2008-05-15T15:19:44Z",
        "link": "http://arxiv.org/abs/0805.2338v1",
        "categories": [
            "cs.SC",
            "math.AC",
            "I.1.1; I.1.2"
        ]
    },
    {
        "title": "Determination of the basis of the space of all root functionals of a   system of polynomial equations and of the basis of its ideal by the operation   of the extension of bounded root functionals",
        "authors": [
            "Timur R. Seifullin"
        ],
        "summary": "It is proposed the algorithm that find a basis of the ideal and a basis of the space of all root functionals by using the extension operation for bounded root functionals, when the number of polynomials is equal to the number of variables, if it is known that the ideal of polynomials is 0-dimensional. The asyptotic complexity of this algorithm is d^{O(n)} operations, where n is the number of polynomials and the number of variables, d is the maximal degree of polynomials. The extension operation has connection with the multivariate Bezoutian construction.",
        "published": "2008-05-29T18:15:22Z",
        "link": "http://arxiv.org/abs/0805.4543v2",
        "categories": [
            "math.AG",
            "cs.SC",
            "math.AC",
            "13P99"
        ]
    },
    {
        "title": "Checking the Quality of Clinical Guidelines using Automated Reasoning   Tools",
        "authors": [
            "Arjen Hommersom",
            "Peter J. F. Lucas",
            "Patrick van Bommel"
        ],
        "summary": "Requirements about the quality of clinical guidelines can be represented by schemata borrowed from the theory of abductive diagnosis, using temporal logic to model the time-oriented aspects expressed in a guideline. Previously, we have shown that these requirements can be verified using interactive theorem proving techniques. In this paper, we investigate how this approach can be mapped to the facilities of a resolution-based theorem prover, Otter, and a complementary program that searches for finite models of first-order statements, Mace. It is shown that the reasoning required for checking the quality of a guideline can be mapped to such fully automated theorem-proving facilities. The medical quality of an actual guideline concerning diabetes mellitus 2 is investigated in this way.",
        "published": "2008-06-02T11:02:40Z",
        "link": "http://arxiv.org/abs/0806.0250v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Subresultants in Recursive Polynomial Remainder Sequence",
        "authors": [
            "Akira Terui"
        ],
        "summary": "We introduce concepts of \"recursive polynomial remainder sequence (PRS)\" and \"recursive subresultant,\" and investigate their properties. In calculating PRS, if there exists the GCD (greatest common divisor) of initial polynomials, we calculate \"recursively\" with new PRS for the GCD and its derivative, until a constant is derived. We call such a PRS a recursive PRS. We define recursive subresultants to be determinants representing the coefficients in recursive PRS by coefficients of initial polynomials. Finally, we discuss usage of recursive subresultants in approximate algebraic computation, which motivates the present work.",
        "published": "2008-06-03T10:09:36Z",
        "link": "http://arxiv.org/abs/0806.0478v1",
        "categories": [
            "math.AC",
            "cs.SC",
            "13P99; 68W30"
        ]
    },
    {
        "title": "Recursive Polynomial Remainder Sequence and the Nested Subresultants",
        "authors": [
            "Akira Terui"
        ],
        "summary": "We give two new expressions of subresultants, nested subresultant and reduced nested subresultant, for the recursive polynomial remainder sequence (PRS) which has been introduced by the author. The reduced nested subresultant reduces the size of the subresultant matrix drastically compared with the recursive subresultant proposed by the authors before, hence it is much more useful for investigation of the recursive PRS. Finally, we discuss usage of the reduced nested subresultant in approximate algebraic computation, which motivates the present work.",
        "published": "2008-06-03T10:24:55Z",
        "link": "http://arxiv.org/abs/0806.0488v1",
        "categories": [
            "math.AC",
            "cs.SC",
            "13P99; 68W30"
        ]
    },
    {
        "title": "Recursive Polynomial Remainder Sequence and its Subresultants",
        "authors": [
            "Akira Terui"
        ],
        "summary": "We introduce concepts of \"recursive polynomial remainder sequence (PRS)\" and \"recursive subresultant,\" along with investigation of their properties. A recursive PRS is defined as, if there exists the GCD (greatest common divisor) of initial polynomials, a sequence of PRSs calculated \"recursively\" for the GCD and its derivative until a constant is derived, and recursive subresultants are defined by determinants representing the coefficients in recursive PRS as functions of coefficients of initial polynomials. We give three different constructions of subresultant matrices for recursive subresultants; while the first one is built-up just with previously defined matrices thus the size of the matrix increases fast as the recursion deepens, the last one reduces the size of the matrix drastically by the Gaussian elimination on the second one which has a \"nested\" expression, i.e. a Sylvester matrix whose elements are themselves determinants.",
        "published": "2008-06-03T10:52:48Z",
        "link": "http://arxiv.org/abs/0806.0495v1",
        "categories": [
            "math.AC",
            "cs.SC",
            "13P99; 68W30"
        ]
    },
    {
        "title": "Consistency and Completeness of Rewriting in the Calculus of   Constructions",
        "authors": [
            "Daria Walukiewicz-Chrzaszcz",
            "Jacek Chrzaszcz"
        ],
        "summary": "Adding rewriting to a proof assistant based on the Curry-Howard isomorphism, such as Coq, may greatly improve usability of the tool. Unfortunately adding an arbitrary set of rewrite rules may render the underlying formal system undecidable and inconsistent. While ways to ensure termination and confluence, and hence decidability of type-checking, have already been studied to some extent, logical consistency has got little attention so far. In this paper we show that consistency is a consequence of canonicity, which in turn follows from the assumption that all functions defined by rewrite rules are complete. We provide a sound and terminating, but necessarily incomplete algorithm to verify this property. The algorithm accepts all definitions that follow dependent pattern matching schemes presented by Coquand and studied by McBride in his PhD thesis. It also accepts many definitions by rewriting, containing rules which depart from standard pattern matching.",
        "published": "2008-06-10T20:27:28Z",
        "link": "http://arxiv.org/abs/0806.1749v3",
        "categories": [
            "cs.LO",
            "cs.SC",
            "F.4.1; F.4.2"
        ]
    },
    {
        "title": "Round Trip Time Prediction Using the Symbolic Function Network Approach",
        "authors": [
            "George S. Eskander",
            "Amir Atiya",
            "Kil To Chong",
            "Hyongsuk Kim",
            "Sung Goo Yoo"
        ],
        "summary": "In this paper, we develop a novel approach to model the Internet round trip time using a recently proposed symbolic type neural network model called symbolic function network. The developed predictor is shown to have good generalization performance and simple representation compared to the multilayer perceptron based predictors.",
        "published": "2008-06-23T10:04:14Z",
        "link": "http://arxiv.org/abs/0806.3646v2",
        "categories": [
            "cs.NE",
            "cs.SC"
        ]
    },
    {
        "title": "The implicit equation of a canal surface",
        "authors": [
            "Marc Dohm",
            "Severinas Zube"
        ],
        "summary": "A canal surface is an envelope of a one parameter family of spheres. In this paper we present an efficient algorithm for computing the implicit equation of a canal surface generated by a rational family of spheres. By using Laguerre and Lie geometries, we relate the equation of the canal surface to the equation of a dual variety of a certain curve in 5-dimensional projective space. We define the \\mu-basis for arbitrary dimension and give a simple algorithm for its computation. This is then applied to the dual variety, which allows us to deduce the implicit equations of the the dual variety, the canal surface and any offset to the canal surface.",
        "published": "2008-06-25T15:22:16Z",
        "link": "http://arxiv.org/abs/0806.4127v1",
        "categories": [
            "math.AG",
            "cs.SC",
            "math.AC"
        ]
    },
    {
        "title": "Executable Set Theory and Arithmetic Encodings in Prolog",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "The paper is organized as a self-contained literate Prolog program that implements elements of an executable finite set theory with focus on combinatorial generation and arithmetic encodings. The complete Prolog code is available at http://logic.csci.unt.edu/tarau/research/2008/pHFS.zip . First, ranking and unranking functions for some \"mathematically elegant\" data types in the universe of Hereditarily Finite Sets with Urelements are provided, resulting in arithmetic encodings for powersets, hypergraphs, ordinals and choice functions. After implementing a digraph representation of Hereditarily Finite Sets we define {\\em decoration functions} that can recover well-founded sets from encodings of their associated acyclic digraphs. We conclude with an encoding of arbitrary digraphs and discuss a concept of duality induced by the set membership relation. In the process, we uncover the surprising possibility of internally sharing isomorphic objects, independently of their language level types and meanings.",
        "published": "2008-08-05T04:59:56Z",
        "link": "http://arxiv.org/abs/0808.0540v1",
        "categories": [
            "cs.LO",
            "cs.DM",
            "cs.DS",
            "cs.MS",
            "cs.SC"
        ]
    },
    {
        "title": "Pairing Functions, Boolean Evaluation and Binary Decision Diagrams in   Prolog",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "A \"pairing function\" J associates a unique natural number z to any two natural numbers x,y such that for two \"unpairing functions\" K and L, the equalities K(J(x,y))=x, L(J(x,y))=y and J(K(z),L(z))=z hold. Using pairing functions on natural number representations of truth tables, we derive an encoding for Binary Decision Diagrams with the unique property that its boolean evaluation faithfully mimics its structural conversion to a a natural number through recursive application of a matching pairing function. We then use this result to derive {\\em ranking} and {\\em unranking} functions for BDDs and reduced BDDs. The paper is organized as a self-contained literate Prolog program, available at http://logic.csci.unt.edu/tarau/research/2008/pBDD.zip   Keywords: logic programming and computational mathematics, pairing/unpairing functions, encodings of boolean functions, binary decision diagrams, natural number representations of truth tables",
        "published": "2008-08-05T05:33:09Z",
        "link": "http://arxiv.org/abs/0808.0555v2",
        "categories": [
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Ranking Catamorphisms and Unranking Anamorphisms on Hereditarily Finite   Datatypes",
        "authors": [
            "Paul Tarau"
        ],
        "summary": "Using specializations of unfold and fold on a generic tree data type we derive unranking and ranking functions providing natural number encodings for various Hereditarily Finite datatypes.   In this context, we interpret unranking operations as instances of a generic anamorphism and ranking operations as instances of the corresponding catamorphism.   Starting with Ackerman's Encoding from Hereditarily Finite Sets to Natural Numbers we define pairings and tuple encodings that provide building blocks for a theory of Hereditarily Finite Functions.   The more difficult problem of ranking and unranking Hereditarily Finite Permutations is then tackled using Lehmer codes and factoradics.   The self-contained source code of the paper, as generated from a literate Haskell program, is available at \\url{http://logic.csci.unt.edu/tarau/research/2008/fFUN.zip}.   Keywords: ranking/unranking, pairing/tupling functions, Ackermann encoding, hereditarily finite sets, hereditarily finite functions, permutations and factoradics, computational mathematics, Haskell data representations",
        "published": "2008-08-06T00:54:05Z",
        "link": "http://arxiv.org/abs/0808.0753v1",
        "categories": [
            "cs.SC",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "A Novel Symbolic Type Neural Network Model- Application to River Flow   Forecasting",
        "authors": [
            "George S. Eskander",
            "Amir F. Atiya"
        ],
        "summary": "In this paper we introduce a new symbolic type neural tree network called symbolic function network (SFN) that is based on using elementary functions to model systems in a symbolic form. The proposed formulation permits feature selection, functional selection, and flexible structure. We applied this model on the River Flow forecasting problem. The results found to be superior in both fitness and sparsity.",
        "published": "2008-08-09T22:05:48Z",
        "link": "http://arxiv.org/abs/0808.1378v1",
        "categories": [
            "cs.NE",
            "cs.SC"
        ]
    },
    {
        "title": "A Refined Difference Field Theory for Symbolic Summation",
        "authors": [
            "Carsten Schneider"
        ],
        "summary": "In this article we present a refined summation theory based on Karr's difference field approach. The resulting algorithms find sum representations with optimal nested depth. For instance, the algorithms have been applied successively to evaluate Feynman integrals from Perturbative Quantum Field Theory.",
        "published": "2008-08-19T07:46:04Z",
        "link": "http://arxiv.org/abs/0808.2543v1",
        "categories": [
            "cs.SC",
            "math-ph",
            "math.CO",
            "math.MP"
        ]
    },
    {
        "title": "Parameterized Telescoping Proves Algebraic Independence of Sums",
        "authors": [
            "Carsten Schneider"
        ],
        "summary": "Usually creative telescoping is used to derive recurrences for sums. In this article we show that the non-existence of a creative telescoping solution, and more generally, of a parameterized telescoping solution, proves algebraic independence of certain types of sums. Combining this fact with summation-theory shows transcendence of whole classes of sums. Moreover, this result throws new light on the question why, e.g., Zeilberger's algorithm fails to find a recurrence with minimal order.",
        "published": "2008-08-19T13:58:01Z",
        "link": "http://arxiv.org/abs/0808.2596v1",
        "categories": [
            "cs.SC",
            "math.CO",
            "math.NT"
        ]
    },
    {
        "title": "Tschirnhaus-Weierstrass curves",
        "authors": [
            "Josef Schicho",
            "David Sevilla"
        ],
        "summary": "We define the concept of Tschirnhaus-Weierstrass curve, named after the Weierstrass form of an elliptic curve and Tschirnhaus transformations. Every pointed curve has a Tschirnhaus-Weierstrass form, and this representation is unique up to a scaling of variables. This is useful for computing isomorphisms between curves.",
        "published": "2008-08-22T08:00:36Z",
        "link": "http://arxiv.org/abs/0808.3038v2",
        "categories": [
            "math.AG",
            "cs.SC",
            "14H99 (Primary) 14Q05, 68W30 (Secondary)"
        ]
    },
    {
        "title": "Simultaneous Modular Reduction and Kronecker Substitution for Small   Finite Fields",
        "authors": [
            "Jean-Guillaume Dumas",
            "Laurent Fousse",
            "Bruno Salvy"
        ],
        "summary": "We present algorithms to perform modular polynomial multiplication or modular dot product efficiently in a single machine word. We pack polynomials into integers and perform several modular operations with machine integer or floating point arithmetic. The modular polynomials are converted into integers using Kronecker substitution (evaluation at a sufficiently large integer). With some control on the sizes and degrees, arithmetic operations on the polynomials can be performed directly with machine integers or floating point numbers and the number of conversions can be reduced. We also present efficient ways to recover the modular values of the coefficients. This leads to practical gains of quite large constant factors for polynomial multiplication, prime field linear algebra and small extension field arithmetic.",
        "published": "2008-08-30T14:28:23Z",
        "link": "http://arxiv.org/abs/0809.0063v1",
        "categories": [
            "cs.SC",
            "math.NT"
        ]
    },
    {
        "title": "Obtaining Exact Interpolation Multivariate Polynomial by Approximation",
        "authors": [
            "Yong Feng",
            "Jingzhong Zhang",
            "Xiaolin Qin",
            "Xun Yuan"
        ],
        "summary": "In some fields such as Mathematics Mechanization, automated reasoning and Trustworthy Computing etc., exact results are needed. Symbolic computations are used to obtain the exact results. Symbolic computations are of high complexity. In order to improve the situation, exactly interpolating methods are often proposed for the exact results and approximate interpolating methods for the approximate ones. In this paper, we study how to obtain exact interpolation polynomial with rational coefficients by approximate interpolating methods.",
        "published": "2008-09-09T02:33:30Z",
        "link": "http://arxiv.org/abs/0809.1476v1",
        "categories": [
            "cs.SC",
            "cs.CG"
        ]
    },
    {
        "title": "How to Integrate a Polynomial over a Simplex",
        "authors": [
            "Velleda Baldoni",
            "Nicole Berline",
            "Jesus De Loera",
            "Matthias Köppe",
            "Michèle Vergne"
        ],
        "summary": "This paper settles the computational complexity of the problem of integrating a polynomial function f over a rational simplex. We prove that the problem is NP-hard for arbitrary polynomials via a generalization of a theorem of Motzkin and Straus. On the other hand, if the polynomial depends only on a fixed number of variables, while its degree and the dimension of the simplex are allowed to vary, we prove that integration can be done in polynomial time. As a consequence, for polynomials of fixed total degree, there is a polynomial time algorithm as well. We conclude the article with extensions to other polytopes, discussion of other available methods and experimental results.",
        "published": "2008-09-11T19:00:12Z",
        "link": "http://arxiv.org/abs/0809.2083v3",
        "categories": [
            "math.MG",
            "cs.CC",
            "cs.SC"
        ]
    },
    {
        "title": "An Unified Definition of Data Mining",
        "authors": [
            "Christoph Schommer"
        ],
        "summary": "Since many years, theoretical concepts of Data Mining have been developed and improved. Data Mining has become applied to many academic and industrial situations, and recently, soundings of public opinion about privacy have been carried out. However, a consistent and standardized definition is still missing, and the initial explanation given by Frawley et al. has pragmatically often changed over the years. Furthermore, alternative terms like Knowledge Discovery have been conjured and forged, and a necessity of a Data Warehouse has been endeavoured to persuade the users. In this work, we pick up current definitions and introduce an unified definition that covers existing attempted explanations. For this, we appeal to the natural original of chemical states of aggregation.",
        "published": "2008-09-16T13:13:17Z",
        "link": "http://arxiv.org/abs/0809.2696v1",
        "categories": [
            "cs.SC",
            "cs.CY",
            "H.2.8; K.6"
        ]
    },
    {
        "title": "A local construction of the Smith normal form of a matrix polynomial",
        "authors": [
            "Jon Wilkening",
            "Jia Yu"
        ],
        "summary": "We present an algorithm for computing a Smith form with multipliers of a regular matrix polynomial over a field. This algorithm differs from previous ones in that it computes a local Smith form for each irreducible factor in the determinant separately and then combines them into a global Smith form, whereas other algorithms apply a sequence of unimodular row and column operations to the original matrix. The performance of the algorithm in exact arithmetic is reported for several test cases.",
        "published": "2008-09-17T18:58:42Z",
        "link": "http://arxiv.org/abs/0809.2978v2",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Poisson Homology in Degree 0 for some Rings of Symplectic Invariants",
        "authors": [
            "Frédéric Butin"
        ],
        "summary": "Let $\\go{g}$ be a finite-dimensional semi-simple Lie algebra, $\\go{h}$ a Cartan subalgebra of $\\go{g}$, and $W$ its Weyl group. The group $W$ acts diagonally on $V:=\\go{h}\\oplus\\go{h}^*$, as well as on $\\mathbb{C}[V]$. The purpose of this article is to study the Poisson homology of the algebra of invariants $\\mathbb{C}[V]^W$ endowed with the standard symplectic bracket. To begin with, we give general results about the Poisson homology space in degree 0, denoted by $HP_0(\\mathbb{C}[V]^W)$, in the case where $\\go{g}$ is of type $B_n-C_n$ or $D_n$, results which support Alev's conjecture. Then we are focusing the interest on the particular cases of ranks 2 and 3, by computing the Poisson homology space in degree 0 in the cases where $\\go{g}$ is of type $B_2$ ($\\go{so}_5$), $D_2$ ($\\go{so}_4$), then $B_3$ ($\\go{so}_7$), and $D_3=A_3$ ($\\go{so}_6\\simeq\\go{sl}_4$). In order to do this, we make use of a functional equation introduced by Y. Berest, P. Etingof and V. Ginzburg. We recover, by a different method, the result established by J. Alev and L. Foissy, according to which the dimension of $HP_0(\\mathbb{C}[V]^W)$ equals 2 for $B_2$. Then we calculate the dimension of this space and we show that it is equal to 1 for $D_2$. We also calculate it for the rank 3 cases, we show that it is equal to 3 for $B_3-C_3$ and 1 for $D_3=A_3$.",
        "published": "2008-09-29T15:16:33Z",
        "link": "http://arxiv.org/abs/0809.4983v1",
        "categories": [
            "math-ph",
            "cs.SC",
            "math.MP",
            "math.RA"
        ]
    },
    {
        "title": "An Evidential Path Logic for Multi-Relational Networks",
        "authors": [
            "Marko A. Rodriguez",
            "Joe Geldart"
        ],
        "summary": "Multi-relational networks are used extensively to structure knowledge. Perhaps the most popular instance, due to the widespread adoption of the Semantic Web, is the Resource Description Framework (RDF). One of the primary purposes of a knowledge network is to reason; that is, to alter the topology of the network according to an algorithm that uses the existing topological structure as its input. There exist many such reasoning algorithms. With respect to the Semantic Web, the bivalent, monotonic reasoners of the RDF Schema (RDFS) and the Web Ontology Language (OWL) are the most prevalent. However, nothing prevents other forms of reasoning from existing in the Semantic Web. This article presents a non-bivalent, non-monotonic, evidential logic and reasoner that is an algebraic ring over a multi-relational network equipped with two binary operations that can be composed to execute various forms of inference. Given its multi-relational grounding, it is possible to use the presented evidential framework as another method for structuring knowledge and reasoning in the Semantic Web. The benefits of this framework are that it works with arbitrary, partial, and contradictory knowledge while, at the same time, it supports a tractable approximate reasoning process.",
        "published": "2008-10-08T17:49:15Z",
        "link": "http://arxiv.org/abs/0810.1481v2",
        "categories": [
            "cs.LO",
            "cs.SC",
            "I.2.4"
        ]
    },
    {
        "title": "Liouvillian Solutions of Difference-Differential Equations",
        "authors": [
            "Ruyong Feng",
            "Michael F. Singer",
            "Min Wu"
        ],
        "summary": "For a field k$with an automorphism \\sigma and a derivation \\delta, we introduce the notion of liouvillian solutions of linear difference-differential systems {\\sigma(Y) = AY, \\delta(Y) = BY} over k and characterize the existence of liouvillian solutions in terms of the Galois group of the systems. We will give an algorithm to decide whether such a system has liouvillian solutions when k = C(x,t), \\sigma(x) = x+1, \\delta = d/dt$ and the size of the system is a prime.",
        "published": "2008-10-09T05:16:48Z",
        "link": "http://arxiv.org/abs/0810.1574v1",
        "categories": [
            "cs.SC",
            "math.CA"
        ]
    },
    {
        "title": "Characterizing 1-Dof Henneberg-I graphs with efficient configuration   spaces",
        "authors": [
            "Heping Gao",
            "Meera Sitharam"
        ],
        "summary": "We define and study exact, efficient representations of realization spaces of a natural class of underconstrained 2D Euclidean Distance Constraint Systems(EDCS) or Frameworks based on 1-dof Henneberg-I graphs. Each representation corresponds to a choice of parameters and yields a different parametrized configuration space. Our notion of efficiency is based on the algebraic complexities of sampling the configuration space and of obtaining a realization from the sample (parametrized) configuration. Significantly, we give purely combinatorial characterizations that capture (i) the class of graphs that have efficient configuration spaces and (ii) the possible choices of representation parameters that yield efficient configuration spaces for a given graph. Our results automatically yield an efficient algorithm for sampling realizations, without missing extreme or boundary realizations. In addition, our results formally show that our definition of efficient configuration space is robust and that our characterizations are tight. We choose the class of 1-dof Henneberg-I graphs in order to take the next step in a systematic and graded program of combinatorial characterizations of efficient configuration spaces. In particular, the results presented here are the first characterizations that go beyond graphs that have connected and convex configuration spaces.",
        "published": "2008-10-12T20:17:21Z",
        "link": "http://arxiv.org/abs/0810.1997v2",
        "categories": [
            "cs.CG",
            "cs.RO",
            "cs.SC"
        ]
    },
    {
        "title": "A cache-friendly truncated FFT",
        "authors": [
            "David Harvey"
        ],
        "summary": "We describe a cache-friendly version of van der Hoeven's truncated FFT and inverse truncated FFT, focusing on the case of `large' coefficients, such as those arising in the Schonhage--Strassen algorithm for multiplication in Z[x]. We describe two implementations and examine their performance.",
        "published": "2008-10-17T17:36:27Z",
        "link": "http://arxiv.org/abs/0810.3203v1",
        "categories": [
            "cs.SC",
            "cs.DS"
        ]
    },
    {
        "title": "Rational Hadamard products via Quantum Diagonal Operators",
        "authors": [
            "Gérard Henry Edmond Duchamp",
            "Silvia Goodenough",
            "Karol A. Penson"
        ],
        "summary": "We use the remark that, through Bargmann-Fock representation, diagonal operators of the Heisenberg-Weyl algebra are scalars for the Hadamard product to give some properties (like the stability of periodic fonctions) of the Hadamard product by a rational fraction. In particular, we provide through this way explicit formulas for the multiplication table of the Hadamard product in the algebra of rational functions in $\\C[[z]]$.",
        "published": "2008-10-20T19:16:29Z",
        "link": "http://arxiv.org/abs/0810.3641v1",
        "categories": [
            "cs.SC",
            "math-ph",
            "math.CO",
            "math.MP"
        ]
    },
    {
        "title": "Kaltofen's division-free determinant algorithm differentiated for matrix   adjoint computation",
        "authors": [
            "Gilles Villard"
        ],
        "summary": "Kaltofen has proposed a new approach in 1992 for computing matrix determinants without divisions. The algorithm is based on a baby steps/giant steps construction of Krylov subspaces, and computes the determinant as the constant term of a characteristic polynomial. For matrices over an abstract ring, by the results of Baur and Strassen, the determinant algorithm, actually a straight-line program, leads to an algorithm with the same complexity for computing the adjoint of a matrix. However, the latter adjoint algorithm is obtained by the reverse mode of automatic differentiation, hence somehow is not \"explicit\". We present an alternative (still closely related) algorithm for the adjoint thatcan be implemented directly, we mean without resorting to an automatic transformation. The algorithm is deduced by applying program differentiation techniques \"by hand\" to Kaltofen's method, and is completely decribed. As subproblem, we study the differentiation of programs that compute minimum polynomials of lineraly generated sequences, and we use a lazy polynomial evaluation mechanism for reducing the cost of Strassen's avoidance of divisions in our case.",
        "published": "2008-10-31T09:43:48Z",
        "link": "http://arxiv.org/abs/0810.5647v1",
        "categories": [
            "cs.SC",
            "cs.CC"
        ]
    },
    {
        "title": "Interpolation of Shifted-Lacunary Polynomials",
        "authors": [
            "Mark Giesbrecht",
            "Daniel S. Roche"
        ],
        "summary": "Given a \"black box\" function to evaluate an unknown rational polynomial f in Q[x] at points modulo a prime p, we exhibit algorithms to compute the representation of the polynomial in the sparsest shifted power basis. That is, we determine the sparsity t, the shift s (a rational), the exponents 0 <= e1 < e2 < ... < et, and the coefficients c1,...,ct in Q\\{0} such that f(x) = c1(x-s)^e1+c2(x-s)^e2+...+ct(x-s)^et. The computed sparsity t is absolutely minimal over any shifted power basis. The novelty of our algorithm is that the complexity is polynomial in the (sparse) representation size, and in particular is logarithmic in deg(f). Our method combines previous celebrated results on sparse interpolation and computing sparsest shifts, and provides a way to handle polynomials with extremely high degree which are, in some sense, sparse in information.",
        "published": "2008-10-31T13:35:08Z",
        "link": "http://arxiv.org/abs/0810.5685v6",
        "categories": [
            "cs.SC",
            "cs.DS",
            "cs.MS",
            "68W30 (Primary), 12Y05 (Secondary)"
        ]
    },
    {
        "title": "How to turn a scripting language into a domain specific language for   computer algebra",
        "authors": [
            "Raphael Jolly",
            "Heinz Kredel"
        ],
        "summary": "We have developed two computer algebra systems, meditor [Jolly:2007] and JAS [Kredel:2006]. These CAS systems are available as Java libraries. For the use-case of interactively entering and manipulating mathematical expressions, there is a need of a scripting front-end for our libraries. Most other CAS invent and implement their own scripting interface for this purpose. We, however, do not want to reinvent the wheel and propose to use a contemporary scripting language with access to Java code. In this paper we discuss the requirements for a scripting language in computer algebra and check whether the languages Python, Ruby, Groovy and Scala meet these requirements. We conclude that, with minor problems, any of these languages is suitable for our purpose.",
        "published": "2008-11-06T23:07:36Z",
        "link": "http://arxiv.org/abs/0811.1061v2",
        "categories": [
            "cs.SC",
            "G.4; I.1; D.2.11"
        ]
    },
    {
        "title": "Trading GRH for algebra: algorithms for factoring polynomials and   related structures",
        "authors": [
            "Gábor Ivanyos",
            "Marek Karpinski",
            "Lajos Rónyai",
            "Nitin Saxena"
        ],
        "summary": "In this paper we develop techniques that eliminate the need of the Generalized Riemann Hypothesis (GRH) from various (almost all) known results about deterministic polynomial factoring over finite fields. Our main result shows that given a polynomial f(x) of degree n over a finite field k, we can find in deterministic poly(n^{\\log n},\\log |k|) time \"either\" a nontrivial factor of f(x) \"or\" a nontrivial automorphism of k[x]/(f(x)) of order n. This main tool leads to various new GRH-free results, most striking of which are:   (1) Given a noncommutative algebra over a finite field, we can find a zero divisor in deterministic subexponential time.   (2) Given a positive integer r such that either 8|r or r has at least two distinct odd prime factors. There is a deterministic polynomial time algorithm to find a nontrivial factor of the r-th cyclotomic polynomial over a finite field.   In this paper, following the seminal work of Lenstra (1991) on constructing isomorphisms between finite fields, we further generalize classical Galois theory constructs like cyclotomic extensions, Kummer extensions, Teichmuller subgroups, to the case of commutative semisimple algebras with automorphisms. These generalized constructs help eliminate the dependence on GRH.",
        "published": "2008-11-19T17:57:25Z",
        "link": "http://arxiv.org/abs/0811.3165v2",
        "categories": [
            "cs.CC",
            "cs.SC"
        ]
    },
    {
        "title": "Craig Interpolation for Quantifier-Free Presburger Arithmetic",
        "authors": [
            "Angelo Brillout",
            "Daniel Kroening",
            "Thomas Wahl"
        ],
        "summary": "Craig interpolation has become a versatile algorithmic tool for improving software verification. Interpolants can, for instance, accelerate the convergence of fixpoint computations for infinite-state systems. They also help improve the refinement of iteratively computed lazy abstractions. Efficient interpolation procedures have been presented only for a few theories. In this paper, we introduce a complete interpolation method for the full range of quantifier-free Presburger arithmetic formulas. We propose a novel convex variable projection for integer inequalities and a technique to combine them with equalities. The derivation of the interpolant has complexity low-degree polynomial in the size of the refutation proof and is typically fast in practice.",
        "published": "2008-11-21T11:44:22Z",
        "link": "http://arxiv.org/abs/0811.3521v1",
        "categories": [
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Benchmarking the solar dynamo with Maxima",
        "authors": [
            "Valery V. Pipin"
        ],
        "summary": "Recently, Jouve et al(A&A, 2008) published the paper that presents the numerical benchmark for the solar dynamo models. Here, I would like to show a way how to get it with help of computer algebra system Maxima. This way was used in our paper (Pipin & Seehafer, A&A 2008, in print) to test some new ideas in the large-scale stellar dynamos. In the present paper I complement the dynamo benchmark with the standard test that address the problem of the free-decay modes in the sphere which is submerged in vacuum.",
        "published": "2008-11-25T11:35:35Z",
        "link": "http://arxiv.org/abs/0811.4061v2",
        "categories": [
            "cs.SE",
            "cs.SC"
        ]
    },
    {
        "title": "Automated Induction for Complex Data Structures",
        "authors": [
            "Adel Bouhoula",
            "Florent Jacquemard"
        ],
        "summary": "We propose a procedure for automated implicit inductive theorem proving for equational specifications made of rewrite rules with conditions and constraints. The constraints are interpreted over constructor terms (representing data values), and may express syntactic equality, disequality, ordering and also membership in a fixed tree language. Constrained equational axioms between constructor terms are supported and can be used in order to specify complex data structures like sets, sorted lists, trees, powerlists...   Our procedure is based on tree grammars with constraints, a formalism which can describe exactly the initial model of the given specification (when it is sufficiently complete and terminating). They are used in the inductive proofs first as an induction scheme for the generation of subgoals at induction steps, second for checking validity and redundancy criteria by reduction to an emptiness problem, and third for defining and solving membership constraints.   We show that the procedure is sound and refutationally complete. It generalizes former test set induction techniques and yields natural proofs for several non-trivial examples presented in the paper, these examples are difficult to specify and carry on automatically with related induction procedures.",
        "published": "2008-11-28T13:58:46Z",
        "link": "http://arxiv.org/abs/0811.4720v1",
        "categories": [
            "cs.LO",
            "cs.SC",
            "F.4.1; F.4.2; F.4.3"
        ]
    },
    {
        "title": "Stable normal forms for polynomial system solving",
        "authors": [
            "Bernard Mourrain",
            "Philippe Trébuchet"
        ],
        "summary": "This paper describes and analyzes a method for computing border bases of a zero-dimensional ideal $I$. The criterion used in the computation involves specific commutation polynomials and leads to an algorithm and an implementation extending the one provided in [MT'05]. This general border basis algorithm weakens the monomial ordering requirement for \\grob bases computations. It is up to date the most general setting for representing quotient algebras, embedding into a single formalism Gr\\\"obner bases, Macaulay bases and new representation that do not fit into the previous categories. With this formalism we show how the syzygies of the border basis are generated by commutation relations. We also show that our construction of normal form is stable under small perturbations of the ideal, if the number of solutions remains constant. This new feature for a symbolic algorithm has a huge impact on the practical efficiency as it is illustrated by the experiments on classical benchmark polynomial systems, at the end of the paper.",
        "published": "2008-11-29T11:26:50Z",
        "link": "http://arxiv.org/abs/0812.0067v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Moment matrices, trace matrices and the radical of ideals",
        "authors": [
            "Itnuit Janovitz-Freireich",
            "Agnes Szanto",
            "Bernard Mourrain",
            "Lajos Ronyai"
        ],
        "summary": "Let $f_1,...,f_s \\in \\mathbb{K}[x_1,...,x_m]$ be a system of polynomials generating a zero-dimensional ideal $\\I$, where $\\mathbb{K}$ is an arbitrary algebraically closed field. Assume that the factor algebra $\\A=\\mathbb{K}[x_1,...,x_m]/\\I$ is Gorenstein and that we have a bound $\\delta>0$ such that a basis for $\\A$ can be computed from multiples of $f_1,...,f_s$ of degrees at most $\\delta$. We propose a method using Sylvester or Macaulay type resultant matrices of $f_1,...,f_s$ and $J$, where $J$ is a polynomial of degree $\\delta$ generalizing the Jacobian, to compute moment matrices, and in particular matrices of traces for $\\A$. These matrices of traces in turn allow us to compute a system of multiplication matrices $\\{M_{x_i}|i=1,...,m\\}$ of the radical $\\sqrt{\\I}$, following the approach in the previous work by Janovitz-Freireich, R\\'{o}nyai and Sz\\'ant\\'o. Additionally, we give bounds for $\\delta$ for the case when $\\I$ has finitely many projective roots in $\\mathbb{P}^m_\\CC$.",
        "published": "2008-11-29T16:14:07Z",
        "link": "http://arxiv.org/abs/0812.0088v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Polynomial relations among principal minors of a 4x4-matrix",
        "authors": [
            "Shaowei Lin",
            "Bernd Sturmfels"
        ],
        "summary": "The image of the principal minor map for n x n-matrices is shown to be closed. In the 19th century, Nansen and Muir studied the implicitization problem of finding all relations among principal minors when n=4. We complete their partial results by constructing explicit polynomials of degree 12 that scheme-theoretically define this affine variety and also its projective closure in $\\PP^{15}$. The latter is the main component in the singular locus of the 2 x 2 x 2 x 2-hyperdeterminant.",
        "published": "2008-12-02T21:34:51Z",
        "link": "http://arxiv.org/abs/0812.0601v3",
        "categories": [
            "math.AG",
            "cs.SC",
            "math.AC",
            "14M12, 15A72, 15A29, 15A15, 13P10, 14L30"
        ]
    },
    {
        "title": "A Sparse Flat Extension Theorem for Moment Matrices",
        "authors": [
            "Monique Laurent",
            "Bernard Mourrain"
        ],
        "summary": "In this note we prove a generalization of the flat extension theorem of Curto and Fialkow for truncated moment matrices. It applies to moment matrices indexed by an arbitrary set of monomials and its border, assuming that this set is connected to 1. When formulated in a basis-free setting, this gives an equivalent result for truncated Hankel operators.",
        "published": "2008-12-15T07:49:22Z",
        "link": "http://arxiv.org/abs/0812.2563v1",
        "categories": [
            "cs.SC",
            "math.AC"
        ]
    },
    {
        "title": "On the total order of reducibility of a pencil of algebraic plane curves",
        "authors": [
            "Laurent Busé",
            "Guillaume Chèze"
        ],
        "summary": "In this paper, the problem of bounding the number of reducible curves in a pencil of algebraic plane curves is addressed. Unlike most of the previous related works, each reducible curve of the pencil is here counted with its appropriate multiplicity. It is proved that this number of reducible curves, counted with multiplicity, is bounded by d^2-1 where d is the degree of the pencil. Then, a sharper bound is given by taking into account the Newton's polygon of the pencil.",
        "published": "2008-12-26T20:56:46Z",
        "link": "http://arxiv.org/abs/0812.4706v2",
        "categories": [
            "math.AC",
            "cs.SC",
            "math.AG"
        ]
    },
    {
        "title": "Using a computer algebra system to simplify expressions for   Titchmarsh-Weyl m-functions associated with the Hydrogen Atom on the half   line",
        "authors": [
            "Cecilia Knoll",
            "Charles Fulton"
        ],
        "summary": "In this paper we give simplified formulas for certain polynomials which arise in some new Titchmarsh-Weyl m-functions for the radial part of the separated Hydrogen atom on the half line and two independent programs for generating them using the symbolic manipulator Mathematica.",
        "published": "2008-12-29T21:10:17Z",
        "link": "http://arxiv.org/abs/0812.4974v1",
        "categories": [
            "math.SP",
            "cs.SC",
            "math.CO",
            "34B20"
        ]
    },
    {
        "title": "Adversarial Models and Resilient Schemes for Network Coding",
        "authors": [
            "Leah Nutman",
            "Michael Langberg"
        ],
        "summary": "In a recent paper, Jaggi et al. (INFOCOM 2007), presented a distributed polynomial-time rate-optimal network-coding scheme that works in the presence of Byzantine faults. We revisit their adversarial models and augment them with three, arguably realistic, models. In each of the models, we present a distributed scheme that demonstrates the usefulness of the model. In particular, all of the schemes obtain optimal rate $C-z$, where $C$ is the network capacity and $z$ is a bound on the number of links controlled by the adversary.",
        "published": "2008-01-04T16:17:49Z",
        "link": "http://arxiv.org/abs/0801.0701v2",
        "categories": [
            "cs.IT",
            "cs.DC",
            "cs.NI",
            "math.IT"
        ]
    },
    {
        "title": "Increasing GP Computing Power via Volunteer Computing",
        "authors": [
            "Daniel Lombrana Gonzalez",
            "Francisco Fernandez de Vega",
            "L. Trujillo",
            "G. Olague",
            "F. Chavez de la O",
            "M. Cardenas",
            "L. Araujo",
            "P. Castillo",
            "K. Sharman"
        ],
        "summary": "This paper describes how it is possible to increase GP Computing Power via Volunteer Computing (VC) using the BOINC framework. Two experiments using well-known GP tools -Lil-gp & ECJ- are performed in order to demonstrate the benefit of using VC in terms of computing power and speed up. Finally we present an extension of the model where any GP tool or framework can be used inside BOINC regardless of its programming language, complexity or required operating system.",
        "published": "2008-01-08T11:36:35Z",
        "link": "http://arxiv.org/abs/0801.1210v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Core Persistence in Peer-to-Peer Systems: Relating Size to Lifetime",
        "authors": [
            "Vincent Gramoli",
            "Anne-Marie Kermarrec",
            "Achour Mostefaoui",
            "Michel Raynal",
            "Bruno Sericola"
        ],
        "summary": "Distributed systems are now both very large and highly dynamic. Peer to peer overlay networks have been proved efficient to cope with this new deal that traditional approaches can no longer accommodate. While the challenge of organizing peers in an overlay network has generated a lot of interest leading to a large number of solutions, maintaining critical data in such a network remains an open issue. In this paper, we are interested in defining the portion of nodes and frequency one has to probe, given the churn observed in the system, in order to achieve a given probability of maintaining the persistence of some critical data. More specifically, we provide a clear result relating the size and the frequency of the probing set along with its proof as well as an analysis of the way of leveraging such an information in a large scale dynamic distributed system.",
        "published": "2008-01-09T12:41:15Z",
        "link": "http://arxiv.org/abs/0801.1419v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Bi-criteria Pipeline Mappings for Parallel Image Processing",
        "authors": [
            "Anne Benoit",
            "Harald Kosch",
            "Veronika Rehn-Sonigo",
            "Yves Robert"
        ],
        "summary": "Mapping workflow applications onto parallel platforms is a challenging problem, even for simple application patterns such as pipeline graphs. Several antagonistic criteria should be optimized, such as throughput and latency (or a combination). Typical applications include digital image processing, where images are processed in steady-state mode. In this paper, we study the mapping of a particular image processing application, the JPEG encoding. Mapping pipelined JPEG encoding onto parallel platforms is useful for instance for encoding Motion JPEG images. As the bi-criteria mapping problem is NP-complete, we concentrate on the evaluation and performance of polynomial heuristics.",
        "published": "2008-01-11T14:48:43Z",
        "link": "http://arxiv.org/abs/0801.1772v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Picking up the Pieces: Self-Healing in Reconfigurable Networks",
        "authors": [
            "Jared Saia",
            "Amitabh Trehan"
        ],
        "summary": "We consider the problem of self-healing in networks that are reconfigurable in the sense that they can change their topology during an attack. Our goal is to maintain connectivity in these networks, even in the presence of repeated adversarial node deletion, by carefully adding edges after each attack. We present a new algorithm, DASH, that provably ensures that: 1) the network stays connected even if an adversary deletes up to all nodes in the network; and 2) no node ever increases its degree by more than 2 log n, where n is the number of nodes initially in the network. DASH is fully distributed; adds new edges only among neighbors of deleted nodes; and has average latency and bandwidth costs that are at most logarithmic in n. DASH has these properties irrespective of the topology of the initial network, and is thus orthogonal and complementary to traditional topology-based approaches to defending against attack.   We also prove lower-bounds showing that DASH is asymptotically optimal in terms of minimizing maximum degree increase over multiple attacks. Finally, we present empirical results on power-law graphs that show that DASH performs well in practice, and that it significantly outperforms naive algorithms in reducing maximum degree increase. We also present empirical results on performance of our algorithms and a new heuristic with regard to stretch (increase in shortest path lengths).",
        "published": "2008-01-24T07:46:50Z",
        "link": "http://arxiv.org/abs/0801.3710v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "cs.NI",
            "C.2.1; C.2.3; C.2.4; C.4; H.3.4"
        ]
    },
    {
        "title": "On Reliability of Dynamic Addressing Routing Protocols in Mobile Ad Hoc   Networks",
        "authors": [
            "Marcello Caleffi",
            "Giancarlo Ferraiuolo",
            "Luigi Paura"
        ],
        "summary": "In this paper, a reliability analysis is carried out to state a performance comparison between two recently proposed proactive routing algorithms. These protocols are able to scale in ad hoc and sensor networks by resorting to dynamic addressing, to face with the topology variability, which is typical of ad hoc, and sensor networks. Numerical simulations are also carried out to corroborate the results of the analysis.",
        "published": "2008-01-26T15:15:21Z",
        "link": "http://arxiv.org/abs/0801.4082v1",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "e-Science perspectives in Venezuela",
        "authors": [
            "G. Diaz",
            "J. Florez-Lopez",
            "V. Hamar",
            "H. Hoeger",
            "C. Mendoza",
            "Z. Mendez",
            "L. A. Nunez",
            "N. Ruiz",
            "R. Torrens",
            "M. Uzcategui"
        ],
        "summary": "We describe the e-Science strategy in Venezuela, in particular initiatives by the Centro Nacional de Calculo Cientifico Universidad de Los Andes (CECALCULA), Merida, the Universidad de Los Andes (ULA), Merida, and the Instituto Venezolano de Investigaciones Cientificas (IVIC), Caracas. We present the plans for the Venezuelan Academic Grid and the current status of Grid ULA supported by Internet2. We show different web-based scientific applications that are being developed in quantum chemistry, atomic physics, structural damage analysis, biomedicine and bioclimate within the framework of the E-Infrastructure shared between Europe and Latin America (EELA)",
        "published": "2008-01-27T20:22:29Z",
        "link": "http://arxiv.org/abs/0801.4150v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Improved lower bound for deterministic broadcasting in radio networks",
        "authors": [
            "Carlos Brito",
            "Shailesh Vaya"
        ],
        "summary": "We consider the problem of deterministic broadcasting in radio networks when the nodes have limited knowledge about the topology of the network. We show that for every deterministic broadcasting protocol there exists a network, of radius 2, for which the protocol takes at least $\\Omega(\\sqrt{n}) rounds for completing the broadcast. Our argument can be extended to prove a lower bound of Omega(\\sqrt{nD}) rounds for broadcasting in radio networks of radius D. This resolves one of the open problems posed in [29], where in the authors proved a lower bound of $\\Omega(n^{1/4}) rounds for broadcasting in constant diameter networks.   We prove the new lower $\\Omega(\\sqrt{n})$ bound for a special family of radius 2 networks. Each network of this family consists of O(\\sqrt{n}) components which are connected to each other via only the source node. At the heart of the proof is a novel simulation argument, which essentially says that any arbitrarily complicated strategy of the source node can be simulated by the nodes of the networks, if the source node just transmits partial topological knowledge about some component instead of arbitrary complicated messages. To the best of our knowledge this type of simulation argument is novel and may be useful in further improving the lower bound or may find use in other applications.   Keywords: radio networks, deterministic broadcast, lower bound, advice string, simulation, selective families, limited topological knowledge.",
        "published": "2008-01-31T10:27:10Z",
        "link": "http://arxiv.org/abs/0801.4845v1",
        "categories": [
            "cs.DM",
            "cs.DC"
        ]
    },
    {
        "title": "Energy Aware Self-Organizing Density Management in Wireless Sensor   Networks",
        "authors": [
            "Erwan Le Merrer",
            "Vincent Gramoli",
            "Anne-Marie Kermarrec",
            "Aline Viana",
            "Marin Bertier"
        ],
        "summary": "Energy consumption is the most important factor that determines sensor node lifetime. The optimization of wireless sensor network lifetime targets not only the reduction of energy consumption of a single sensor node but also the extension of the entire network lifetime. We propose a simple and adaptive energy-conserving topology management scheme, called SAND (Self-Organizing Active Node Density). SAND is fully decentralized and relies on a distributed probing approach and on the redundancy resolution of sensors for energy optimizations, while preserving the data forwarding and sensing capabilities of the network. We present the SAND's algorithm, its analysis of convergence, and simulation results. Simulation results show that, though slightly increasing path lengths from sensor to sink nodes, the proposed scheme improves significantly the network lifetime for different neighborhood densities degrees, while preserving both sensing and routing fidelity.",
        "published": "2008-02-05T07:03:28Z",
        "link": "http://arxiv.org/abs/0802.0550v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Timed Quorum System for Large-Scale and Dynamic Environments",
        "authors": [
            "Vincent Gramoli",
            "Michel Raynal"
        ],
        "summary": "This paper presents Timed Quorum System (TQS), a new quorum system especially suited for large-scale and dynamic systems. TQS requires that two quorums intersect with high probability if they are used in the same small period of time. It proposed an algorithm that implements TQS and that verifies probabilistic atomicity: a consistency criterion that requires each operation to respect atomicity with high probability. This TQS implementation has quorum of size O(\\sqrt{nD}) and expected access time of O(log \\sqrt{nD}) message delays, where n measures the size of the system and D is a required parameter to handle dynamism.",
        "published": "2008-02-05T07:09:08Z",
        "link": "http://arxiv.org/abs/0802.0552v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Snap-Stabilization in Message-Passing Systems",
        "authors": [
            "Sylvie Delaët",
            "Stéphane Devismes",
            "Mikhail Nesterenko",
            "Sébastien Tixeuil"
        ],
        "summary": "In this paper, we tackle the open problem of snap-stabilization in message-passing systems. Snap-stabilization is a nice approach to design protocols that withstand transient faults. Compared to the well-known self-stabilizing approach, snap-stabilization guarantees that the effect of faults is contained immediately after faults cease to occur. Our contribution is twofold: we show that (1) snap-stabilization is impossible for a wide class of problems if we consider networks with finite yet unbounded channel capacity; (2) snap-stabilization becomes possible in the same setting if we assume bounded-capacity channels. We propose three snap-stabilizing protocols working in fully-connected networks. Our work opens exciting new research perspectives, as it enables the snap-stabilizing paradigm to be implemented in actual networks.",
        "published": "2008-02-08T10:51:24Z",
        "link": "http://arxiv.org/abs/0802.1123v2",
        "categories": [
            "cs.DC",
            "cs.NI",
            "cs.PF"
        ]
    },
    {
        "title": "Convergence of some leader election algorithms",
        "authors": [
            "Svante Janson",
            "Christian Lavault",
            "Guy Louchard"
        ],
        "summary": "We start with a set of n players. With some probability P(n,k), we kill n-k players; the other ones stay alive, and we repeat with them. What is the distribution of the number X_n of phases (or rounds) before getting only one player? We present a probabilistic analysis of this algorithm under some conditions on the probability distributions P(n,k), including stochastic monotonicity and the assumption that roughly a fixed proportion alpha of the players survive in each round.   We prove a kind of convergence in distribution for X_n-log_a n, where the basis a=1/alpha; as in many other similar problems there are oscillations and no true limit distribution, but suitable subsequences converge, and there is an absolutely continuous random variable Z such that the distribution of X_n can be approximated by Z+log_a n rounded to the nearest larger integer.   Applications of the general result include the leader election algorithm where players are eliminated by independent coin tosses and a variation of the leader election algorithm proposed by W.R. Franklin. We study the latter algorithm further, including numerical results.",
        "published": "2008-02-11T11:01:11Z",
        "link": "http://arxiv.org/abs/0802.1389v1",
        "categories": [
            "cs.DC",
            "math.PR"
        ]
    },
    {
        "title": "Improved Approximations for Multiprocessor Scheduling Under Uncertainty",
        "authors": [
            "Christopher Crutchfield",
            "Zoran Dzunic",
            "Jeremy T. Fineman",
            "David R. Karger",
            "Jacob Scott"
        ],
        "summary": "This paper presents improved approximation algorithms for the problem of multiprocessor scheduling under uncertainty, or SUU, in which the execution of each job may fail probabilistically. This problem is motivated by the increasing use of distributed computing to handle large, computationally intensive tasks. In the SUU problem we are given n unit-length jobs and m machines, a directed acyclic graph G of precedence constraints among jobs, and unrelated failure probabilities q_{ij} for each job j when executed on machine i for a single timestep. Our goal is to find a schedule that minimizes the expected makespan, which is the expected time at which all jobs complete.   Lin and Rajaraman gave the first approximations for this NP-hard problem for the special cases of independent jobs, precedence constraints forming disjoint chains, and precedence constraints forming trees. In this paper, we present asymptotically better approximation algorithms. In particular, we give an O(loglog min(m,n))-approximation for independent jobs (improving on the previously best O(log n)-approximation). We also give an O(log(n+m) loglog min(m,n))-approximation algorithm for precedence constraints that form disjoint chains (improving on the previously best O(log(n)log(m)log(n+m)/loglog(n+m))-approximation by a (log n/loglog n)^2 factor when n = poly(m). Our algorithm for precedence constraints forming chains can also be used as a component for precedence constraints forming trees, yielding a similar improvement over the previously best algorithms for trees.",
        "published": "2008-02-18T20:57:17Z",
        "link": "http://arxiv.org/abs/0802.2418v2",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "The Forgiving Tree: A Self-Healing Distributed Data Structure",
        "authors": [
            "Tom Hayes",
            "Navin Rustagi",
            "Jared Saia",
            "Amitabh Trehan"
        ],
        "summary": "We consider the problem of self-healing in peer-to-peer networks that are under repeated attack by an omniscient adversary. We assume that the following process continues for up to n rounds where n is the total number of nodes initially in the network: the adversary deletes an arbitrary node from the network, then the network responds by quickly adding a small number of new edges.   We present a distributed data structure that ensures two key properties. First, the diameter of the network is never more than $O(\\log \\Delta)$ times its original diameter, where $\\Delta$ is the maximum degree of the network initially. We note that for many peer-to-peer systems, $\\Delta$ is polylogarithmic, so the diameter increase would be a O(log log n) multiplicative factor. Second, the degree of any node never increases by more than 3 over its original degree. Our data structure is fully distributed, has O(1) latency per round and requires each node to send and receive O(1) messages per round. The data structure requires an initial setup phase that has latency equal to the diameter of the original network, and requires, with high probability, each node v to send O(log n) messages along every edge incident to v. Our approach is orthogonal and complementary to traditional topology-based approaches to defending against attack.",
        "published": "2008-02-22T08:22:33Z",
        "link": "http://arxiv.org/abs/0802.3267v1",
        "categories": [
            "cs.DC",
            "cs.NI",
            "C.2.1; C.2.3; C.2.4; C.4; H.3.4"
        ]
    },
    {
        "title": "PVM-Distributed Implementation of the Radiance Code",
        "authors": [
            "Francisco R. Villatoro",
            "Antonio J. Nebro",
            "Jose E. Fernández"
        ],
        "summary": "The Parallel Virtual Machine (PVM) tool has been used for a distributed implementation of Greg Ward's Radiance code. In order to generate exactly the same primary rays with both the sequential and the parallel codes, the quincunx sampling technique used in Radiance for the reduction of the number of primary rays by interpolation, must be left untouched in the parallel implementation. The octree of local ambient values used in Radiance for the indirect illumination has been shared among all the processors. Both static and dynamic image partitioning techniques which replicate the octree of the complete scene in all the processors and have load-balancing, have been developed for one frame rendering. Speedups larger than 7.5 have been achieved in a network of 8 workstations. For animation sequences, a new dynamic partitioning distribution technique with superlinear speedups has also been developed.",
        "published": "2008-02-22T17:32:17Z",
        "link": "http://arxiv.org/abs/0802.3355v1",
        "categories": [
            "cs.DC",
            "cs.GR"
        ]
    },
    {
        "title": "Algebraic Pattern Matching in Join Calculus",
        "authors": [
            "Qin Ma",
            "Luc Maranget"
        ],
        "summary": "We propose an extension of the join calculus with pattern matching on algebraic data types. Our initial motivation is twofold: to provide an intuitive semantics of the interaction between concurrency and pattern matching; to define a practical compilation scheme from extended join definitions into ordinary ones plus ML pattern matching. To assess the correctness of our compilation scheme, we develop a theory of the applied join calculus, a calculus with value passing and value matching. We implement this calculus as an extension of the current JoCaml system.",
        "published": "2008-02-27T13:21:51Z",
        "link": "http://arxiv.org/abs/0802.4018v2",
        "categories": [
            "cs.PL",
            "cs.DC",
            "D.1.3; D.3.3; F.3.2"
        ]
    },
    {
        "title": "New bounds on classical and quantum one-way communication complexity",
        "authors": [
            "Rahul Jain",
            "Shengyu Zhang"
        ],
        "summary": "In this paper we provide new bounds on classical and quantum distributional communication complexity in the two-party, one-way model of communication. In the classical model, our bound extends the well known upper bound of Kremer, Nisan and Ron to include non-product distributions. We show that for a boolean function f:X x Y -> {0,1} and a non-product distribution mu on X x Y and epsilon in (0,1/2) constant: D_{epsilon}^{1, mu}(f)= O((I(X:Y)+1) vc(f)), where D_{epsilon}^{1, mu}(f) represents the one-way distributional communication complexity of f with error at most epsilon under mu; vc(f) represents the Vapnik-Chervonenkis dimension of f and I(X:Y) represents the mutual information, under mu, between the random inputs of the two parties. For a non-boolean function f:X x Y ->[k], we show a similar upper bound on D_{epsilon}^{1, mu}(f) in terms of k, I(X:Y) and the pseudo-dimension of f' = f/k. In the quantum one-way model we provide a lower bound on the distributional communication complexity, under product distributions, of a function f, in terms the well studied complexity measure of f referred to as the rectangle bound or the corruption bound of f . We show for a non-boolean total function f : X x Y -> Z and a product distribution mu on XxY, Q_{epsilon^3/8}^{1, mu}(f) = Omega(rec_ epsilon^{1, mu}(f)), where Q_{epsilon^3/8}^{1, mu}(f) represents the quantum one-way distributional communication complexity of f with error at most epsilon^3/8 under mu and rec_ epsilon^{1, mu}(f) represents the one-way rectangle bound of f with error at most epsilon under mu . Similarly for a non-boolean partial function f:XxY -> Z U {*} and a product distribution mu on X x Y, we show, Q_{epsilon^6/(2 x 15^4)}^{1, mu}(f) = Omega(rec_ epsilon^{1, mu}(f)).",
        "published": "2008-02-27T22:28:52Z",
        "link": "http://arxiv.org/abs/0802.4101v1",
        "categories": [
            "cs.IT",
            "cs.DC",
            "math.IT",
            "H.1.1"
        ]
    },
    {
        "title": "A Bit-Compatible Shared Memory Parallelization for ILU(k)   Preconditioning and a Bit-Compatible Generalization to Distributed Memory",
        "authors": [
            "Xin Dong",
            "Gene Cooperman"
        ],
        "summary": "ILU(k) is a commonly used preconditioner for iterative linear solvers for sparse, non-symmetric systems. It is often preferred for the sake of its stability. We present TPILU(k), the first efficiently parallelized ILU(k) preconditioner that maintains this important stability property. Even better, TPILU(k) preconditioning produces an answer that is bit-compatible with the sequential ILU(k) preconditioning. In terms of performance, the TPILU(k) preconditioning is shown to run faster whenever more cores are made available to it --- while continuing to be as stable as sequential ILU(k). This is in contrast to some competing methods that may become unstable if the degree of thread parallelism is raised too far. Where Block Jacobi ILU(k) fails in an application, it can be replaced by TPILU(k) in order to maintain good performance, while also achieving full stability. As a further optimization, TPILU(k) offers an optional level-based incomplete inverse method as a fast approximation for the original ILU(k) preconditioned matrix. Although this enhancement is not bit-compatible with classical ILU(k), it is bit-compatible with the output from the single-threaded version of the same algorithm. In experiments on a 16-core computer, the enhanced TPILU(k)-based iterative linear solver performed up to 9 times faster. As we approach an era of many-core computing, the ability to efficiently take advantage of many cores will become ever more important. TPILU(k) also demonstrates good performance on cluster or Grid. For example, the new algorithm achieves 50 times speedup with 80 nodes for general sparse matrices of dimension 160,000 that are diagonally dominant.",
        "published": "2008-03-01T07:21:27Z",
        "link": "http://arxiv.org/abs/0803.0048v4",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Mobile Agents for Content-Based WWW Distributed Image Retrieval",
        "authors": [
            "Sabu M. Thampi",
            "K. Chandra Sekaran"
        ],
        "summary": "At present, the de-facto standard for providing contents in the Internet is the World Wide Web. A technology, which is now emerging on the Web, is Content-Based Image Retrieval (CBIR). CBIR applies methods and algorithms from computer science to analyse and index images based on their visual content. Mobile agents push the flexibility of distributed systems to their limits since not only computations are dynamically distributed but also the code that performs them. The current commercial applet-based methodologies for accessing image database systems offer limited flexibility, scalability and robustness. In this paper the author proposes a new framework for content-based WWW distributed image retrieval based on Java-based mobile agents. The implementation of the framework shows that its performance is comparable to, and in some cases outperforms, the current approach.",
        "published": "2008-03-01T08:27:59Z",
        "link": "http://arxiv.org/abs/0803.0053v1",
        "categories": [
            "cs.DC",
            "cs.IR"
        ]
    },
    {
        "title": "Quiescence of Self-stabilizing Gossiping among Mobile Agents in Graphs",
        "authors": [
            "Toshimitsu Masuzawa",
            "Sébastien Tixeuil"
        ],
        "summary": "This paper considers gossiping among mobile agents in graphs: agents move on the graph and have to disseminate their initial information to every other agent. We focus on self-stabilizing solutions for the gossip problem, where agents may start from arbitrary locations in arbitrary states. Self-stabilization requires (some of the) participating agents to keep moving forever, hinting at maximizing the number of agents that could be allowed to stop moving eventually. This paper formalizes the self-stabilizing agent gossip problem, introduces the quiescence number (i.e., the maximum number of eventually stopping agents) of self-stabilizing solutions and investigates the quiescence number with respect to several assumptions related to agent anonymity, synchrony, link duplex capacity, and whiteboard capacity.",
        "published": "2008-03-03T09:14:21Z",
        "link": "http://arxiv.org/abs/0803.0189v2",
        "categories": [
            "cs.DC",
            "cs.PF",
            "cs.RO"
        ]
    },
    {
        "title": "Self-Stabilizing Pulse Synchronization Inspired by Biological Pacemaker   Networks",
        "authors": [
            "Ariel Daliot",
            "Danny Dolev",
            "Hanna Parnas"
        ],
        "summary": "We define the ``Pulse Synchronization'' problem that requires nodes to achieve tight synchronization of regular pulse events, in the settings of distributed computing systems. Pulse-coupled synchronization is a phenomenon displayed by a large variety of biological systems, typically overcoming a high level of noise. Inspired by such biological models, a robust and self-stabilizing Byzantine pulse synchronization algorithm for distributed computer systems is presented. The algorithm attains near optimal synchronization tightness while tolerating up to a third of the nodes exhibiting Byzantine behavior concurrently. Pulse synchronization has been previously shown to be a powerful building block for designing algorithms in this severe fault model. We have previously shown how to stabilize general Byzantine algorithms, using pulse synchronization. To the best of our knowledge there is no other scheme to do this without the use of synchronized pulses.",
        "published": "2008-03-03T13:46:45Z",
        "link": "http://arxiv.org/abs/0803.0241v2",
        "categories": [
            "cs.DC",
            "C.1.4; C.2.4; D.4.5"
        ]
    },
    {
        "title": "Integrity-Enhancing Replica Coordination for Byzantine Fault Tolerant   Systems",
        "authors": [
            "Wenbing Zhao"
        ],
        "summary": "Strong replica consistency is often achieved by writing deterministic applications, or by using a variety of mechanisms to render replicas deterministic. There exists a large body of work on how to render replicas deterministic under the benign fault model. However, when replicas can be subject to malicious faults, most of the previous work is no longer effective. Furthermore, the determinism of the replicas is often considered harmful from the security perspective and for many applications, their integrity strongly depends on the randomness of some of their internal operations. This calls for new approaches towards achieving replica consistency while preserving the replica randomness. In this paper, we present two such approaches. One is based on Byzantine agreement and the other on threshold coin-tossing. Each approach has its strength and weaknesses. We compare the performance of the two approaches and outline their respective best use scenarios.",
        "published": "2008-03-11T04:20:06Z",
        "link": "http://arxiv.org/abs/0803.1520v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Proactive Service Migration for Long-Running Byzantine Fault Tolerant   Systems",
        "authors": [
            "Wenbing Zhao"
        ],
        "summary": "In this paper, we describe a novel proactive recovery scheme based on service migration for long-running Byzantine fault tolerant systems. Proactive recovery is an essential method for ensuring long term reliability of fault tolerant systems that are under continuous threats from malicious adversaries. The primary benefit of our proactive recovery scheme is a reduced vulnerability window. This is achieved by removing the time-consuming reboot step from the critical path of proactive recovery. Our migration-based proactive recovery is coordinated among the replicas, therefore, it can automatically adjust to different system loads and avoid the problem of excessive concurrent proactive recoveries that may occur in previous work with fixed watchdog timeouts. Moreover, the fast proactive recovery also significantly improves the system availability in the presence of faults.",
        "published": "2008-03-11T04:34:04Z",
        "link": "http://arxiv.org/abs/0803.1521v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Lighweight Target Tracking Using Passive Traces in Sensor Networks",
        "authors": [
            "Andrei Marculescu",
            "Sotiris Nikoletseas",
            "Olivier Powell",
            "Jose Rolim"
        ],
        "summary": "We study the important problem of tracking moving targets in wireless sensor networks. We try to overcome the limitations of standard state of the art tracking methods based on continuous location tracking, i.e. the high energy dissipation and communication overhead imposed by the active participation of sensors in the tracking process and the low scalability, especially in sparse networks. Instead, our approach uses sensors in a passive way: they just record and judiciously spread information about observed target presence in their vicinity; this information is then used by the (powerful) tracking agent to locate the target by just following the traces left at sensors. Our protocol is greedy, local, distributed, energy efficient and very successful, in the sense that (as shown by extensive simulations) the tracking agent manages to quickly locate and follow the target; also, we achieve good trade-offs between the energy dissipation and latency.",
        "published": "2008-03-14T18:01:17Z",
        "link": "http://arxiv.org/abs/0803.2219v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "SAFIUS - A secure and accountable filesystem over untrusted storage",
        "authors": [
            "V Sriram",
            "Ganesh Narayan",
            "K Gopinath"
        ],
        "summary": "We describe SAFIUS, a secure accountable file system that resides over an untrusted storage. SAFIUS provides strong security guarantees like confidentiality, integrity, prevention from rollback attacks, and accountability. SAFIUS also enables read/write sharing of data and provides the standard UNIX-like interface for applications. To achieve accountability with good performance, it uses asynchronous signatures; to reduce the space required for storing these signatures, a novel signature pruning mechanism is used. SAFIUS has been implemented on a GNU/Linux based system modifying OpenGFS. Preliminary performance studies show that SAFIUS has a tolerable overhead for providing secure storage: while it has an overhead of about 50% of OpenGFS in data intensive workloads (due to the overhead of performing encryption/decryption in software), it is comparable (or better in some cases) to OpenGFS in metadata intensive workloads.",
        "published": "2008-03-16T18:24:13Z",
        "link": "http://arxiv.org/abs/0803.2365v1",
        "categories": [
            "cs.OS",
            "cs.CR",
            "cs.DC",
            "cs.NI",
            "cs.PF",
            "D.4.6; D.4.2; C.2.4"
        ]
    },
    {
        "title": "Performance Evaluation of Multiple TCP connections in iSCSI",
        "authors": [
            "Bhargava Kumar K",
            "Ganesh M. Narayan",
            "K. Gopinath"
        ],
        "summary": "Scaling data storage is a significant concern in enterprise systems and Storage Area Networks (SANs) are deployed as a means to scale enterprise storage. SANs based on Fibre Channel have been used extensively in the last decade while iSCSI is fast becoming a serious contender due to its reduced costs and unified infrastructure. This work examines the performance of iSCSI with multiple TCP connections. Multiple TCP connections are often used to realize higher bandwidth but there may be no fairness in how bandwidth is distributed. We propose a mechanism to share congestion information across multiple flows in ``Fair-TCP'' for improved performance. Our results show that Fair-TCP significantly improves the performance for I/O intensive workloads.",
        "published": "2008-03-23T19:10:00Z",
        "link": "http://arxiv.org/abs/0803.3338v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.OS",
            "cs.PF",
            "B.3.2; D.4.2; H.3.4; C.2.2"
        ]
    },
    {
        "title": "Void Traversal for Guaranteed Delivery in Geometric Routing",
        "authors": [
            "Mikhail Nesterenko",
            "Adnan Vora"
        ],
        "summary": "Geometric routing algorithms like GFG (GPSR) are lightweight, scalable algorithms that can be used to route in resource-constrained ad hoc wireless networks. However, such algorithms run on planar graphs only. To efficiently construct a planar graph, they require a unit-disk graph. To make the topology unit-disk, the maximum link length in the network has to be selected conservatively. In practical setting this leads to the designs where the node density is rather high. Moreover, the network diameter of a planar subgraph is greater than the original graph, which leads to longer routes. To remedy this problem, we propose a void traversal algorithm that works on arbitrary geometric graphs. We describe how to use this algorithm for geometric routing with guaranteed delivery and compare its performance with GFG.",
        "published": "2008-03-25T20:52:17Z",
        "link": "http://arxiv.org/abs/0803.3632v1",
        "categories": [
            "cs.OS",
            "cs.DC",
            "cs.DS",
            "C.2.2; C.2.1; F.2.2"
        ]
    },
    {
        "title": "Distributed Averaging in the presence of a Sparse Cut",
        "authors": [
            "Hariharan Narayanan"
        ],
        "summary": "We consider the question of averaging on a graph that has one sparse cut separating two subgraphs that are internally well connected.   While there has been a large body of work devoted to algorithms for distributed averaging, nearly all algorithms involve only {\\it convex} updates. In this paper, we suggest that {\\it non-convex} updates can lead to significant improvements. We do so by exhibiting a decentralized algorithm for graphs with one sparse cut that uses non-convex averages and has an averaging time that can be significantly smaller than the averaging time of known distributed algorithms, such as those of \\cite{tsitsiklis, Boyd}. We use stochastic dominance to prove this result in a way that may be of independent interest.",
        "published": "2008-03-25T22:04:50Z",
        "link": "http://arxiv.org/abs/0803.3642v3",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Stateless and Delivery Guaranteed Geometric Routing on Virtual   Coordinate System",
        "authors": [
            "Ke Liu",
            "Nael Abu-Ghazaleh"
        ],
        "summary": "Stateless geographic routing provides relatively good performance at a fixed overhead, which is typically much lower than conventional routing protocols such as AODV. However, the performance of geographic routing is impacted by physical voids, and localization errors. Accordingly, virtual coordinate systems (VCS) were proposed as an alternative approach that is resilient to localization errors and that naturally routes around physical voids. However, VCS also faces virtual anomalies, causing their performance to trail geographic routing. In existing VCS routing protocols, there is a lack of an effective stateless and delivery guaranteed complementary routing algorithm that can be used to traverse voids. Most proposed solutions use variants of flooding or blind searching when a void is encountered. In this paper, we propose a spanning-path virtual coordinate system which can be used as a complete routing algorithm or as the complementary algorithm to greedy forwarding that is invoked when voids are encountered. With this approach, and for the first time, we demonstrate a stateless and delivery guaranteed geometric routing algorithm on VCS. When used in conjunction with our previously proposed aligned virtual coordinate system (AVCS), it out-performs not only all geometric routing protocols on VCS, but also geographic routing with accurate location information.",
        "published": "2008-03-28T05:58:51Z",
        "link": "http://arxiv.org/abs/0803.4049v2",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "Unified storage systems for distributed Tier-2 centres",
        "authors": [
            "Greig A. Cowan",
            "Graeme A. Stewart",
            "Andrew Elwell"
        ],
        "summary": "The start of data taking at the Large Hadron Collider will herald a new era in data volumes and distributed processing in particle physics. Data volumes of hundreds of Terabytes will be shipped to Tier-2 centres for analysis by the LHC experiments using the Worldwide LHC Computing Grid (WLCG).   In many countries Tier-2 centres are distributed between a number of institutes, e.g., the geographically spread Tier-2s of GridPP in the UK. This presents a number of challenges for experiments to utilise these centres efficaciously, as CPU and storage resources may be sub-divided and exposed in smaller units than the experiment would ideally want to work with. In addition, unhelpful mismatches between storage and CPU at the individual centres may be seen, which make efficient exploitation of a Tier-2's resources difficult.   One method of addressing this is to unify the storage across a distributed Tier-2, presenting the centres' aggregated storage as a single system. This greatly simplifies data management for the VO, which then can access a greater amount of data across the Tier-2. However, such an approach will lead to scenarios where analysis jobs on one site's batch system must access data hosted on another site.   We investigate this situation using the Glasgow and Edinburgh clusters, which are part of the ScotGrid distributed Tier-2. In particular we look at how to mitigate the problems associated with ``distant'' data access and discuss the security implications of having LAN access protocols traverse the WAN between centres.",
        "published": "2008-03-28T23:26:31Z",
        "link": "http://arxiv.org/abs/0803.4223v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "GRID Architecture through a Public Cluster",
        "authors": [
            "Z. Akbar",
            "L. T. Handoko"
        ],
        "summary": "An architecture to enable some blocks consisting of several nodes in a public cluster connected to different grid collaborations is introduced. It is realized by inserting a web-service in addition to the standard Globus Toolkit. The new web-service performs two main tasks : authenticate the digital certificate contained in an incoming requests and forward it to the designated block. The appropriate block is mapped with the username of the block's owner contained in the digital certificate. It is argued that this algorithm opens an opportunity for any blocks in a public cluster to join various global grids.",
        "published": "2008-03-31T05:05:04Z",
        "link": "http://arxiv.org/abs/0803.4370v3",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Distributed Consensus over Wireless Sensor Networks Affected by   Multipath Fading",
        "authors": [
            "Gesualdo Scutari",
            "Sergio Barbarossa"
        ],
        "summary": "The design of sensor networks capable of reaching a consensus on a globally optimal decision test, without the need for a fusion center, is a problem that has received considerable attention in the last years. Many consensus algorithms have been proposed, with convergence conditions depending on the graph describing the interaction among the nodes. In most works, the graph is undirected and there are no propagation delays. Only recently, the analysis has been extended to consensus algorithms incorporating propagation delays. In this work, we propose a consensus algorithm able to converge to a globally optimal decision statistic, using a wideband wireless network, governed by a fairly simple MAC mechanism, where each link is a multipath, frequency-selective, channel. The main contribution of the paper is to derive necessary and sufficient conditions on the network topology and sufficient conditions on the channel transfer functions guaranteeing the exponential convergence of the consensus algorithm to a globally optimal decision value, for any bounded delay condition.",
        "published": "2008-04-03T09:22:41Z",
        "link": "http://arxiv.org/abs/0804.0506v1",
        "categories": [
            "cs.DC",
            "cs.MA"
        ]
    },
    {
        "title": "On ad hoc routing with guaranteed delivery",
        "authors": [
            "Mark Braverman"
        ],
        "summary": "We point out a simple poly-time log-space routing algorithm in ad hoc networks with guaranteed delivery using universal exploration sequences.",
        "published": "2008-04-05T16:16:38Z",
        "link": "http://arxiv.org/abs/0804.0862v1",
        "categories": [
            "cs.DC",
            "cs.CC",
            "C.2.2; F.2.2"
        ]
    },
    {
        "title": "Adaptive Dynamics of Realistic Small-World Networks",
        "authors": [
            "Olof Mogren",
            "Oskar Sandberg",
            "Vilhelm Verendel",
            "Devdatt Dubhashi"
        ],
        "summary": "Continuing in the steps of Jon Kleinberg's and others celebrated work on decentralized search in small-world networks, we conduct an experimental analysis of a dynamic algorithm that produces small-world networks. We find that the algorithm adapts robustly to a wide variety of situations in realistic geographic networks with synthetic test data and with real world data, even when vertices are uneven and non-homogeneously distributed.   We investigate the same algorithm in the case where some vertices are more popular destinations for searches than others, for example obeying power-laws. We find that the algorithm adapts and adjusts the networks according to the distributions, leading to improved performance. The ability of the dynamic process to adapt and create small worlds in such diverse settings suggests a possible mechanism by which such networks appear in nature.",
        "published": "2008-04-07T19:39:59Z",
        "link": "http://arxiv.org/abs/0804.1115v1",
        "categories": [
            "cs.DS",
            "cs.DC"
        ]
    },
    {
        "title": "Fast k Nearest Neighbor Search using GPU",
        "authors": [
            "Vincent Garcia",
            "Eric Debreuve",
            "Michel Barlaud"
        ],
        "summary": "The recent improvements of graphics processing units (GPU) offer to the computer vision community a powerful processing platform. Indeed, a lot of highly-parallelizable computer vision problems can be significantly accelerated using GPU architecture. Among these algorithms, the k nearest neighbor search (KNN) is a well-known problem linked with many applications such as classification, estimation of statistical properties, etc. The main drawback of this task lies in its computation burden, as it grows polynomially with the data size. In this paper, we show that the use of the NVIDIA CUDA API accelerates the search for the KNN up to a factor of 120.",
        "published": "2008-04-09T10:06:15Z",
        "link": "http://arxiv.org/abs/0804.1448v1",
        "categories": [
            "cs.CV",
            "cs.DC"
        ]
    },
    {
        "title": "Distributed and Recursive Parameter Estimation in Parametrized Linear   State-Space Models",
        "authors": [
            "S. Sundhar Ram",
            "V. V. Veeravalli",
            "A. Nedic"
        ],
        "summary": "We consider a network of sensors deployed to sense a spatio-temporal field and estimate a parameter of interest. We are interested in the case where the temporal process sensed by each sensor can be modeled as a state-space process that is perturbed by random noise and parametrized by an unknown parameter. To estimate the unknown parameter from the measurements that the sensors sequentially collect, we propose a distributed and recursive estimation algorithm, which we refer to as the incremental recursive prediction error algorithm. This algorithm has the distributed property of incremental gradient algorithms and the on-line property of recursive prediction error algorithms. We study the convergence behavior of the algorithm and provide sufficient conditions for its convergence. Our convergence result is rather general and contains as special cases the known convergence results for the incremental versions of the least-mean square algorithm. Finally, we use the algorithm developed in this paper to identify the source of a gas-leak (diffusing source) in a closed warehouse and also report numerical simulations to verify convergence.",
        "published": "2008-04-10T03:47:05Z",
        "link": "http://arxiv.org/abs/0804.1607v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Push & Pull: autonomous deployment of mobile sensors for a complete   coverage",
        "authors": [
            "N. Bartolini",
            "T. Calamoneri",
            "E. G. Fusco",
            "A. Massini",
            "S. Silvestri"
        ],
        "summary": "Mobile sensor networks are important for several strategic applications devoted to monitoring critical areas. In such hostile scenarios, sensors cannot be deployed manually and are either sent from a safe location or dropped from an aircraft. Mobile devices permit a dynamic deployment reconfiguration that improves the coverage in terms of completeness and uniformity.   In this paper we propose a distributed algorithm for the autonomous deployment of mobile sensors called Push&Pull. According to our proposal, movement decisions are made by each sensor on the basis of locally available information and do not require any prior knowledge of the operating conditions or any manual tuning of key parameters.   We formally prove that, when a sufficient number of sensors are available, our approach guarantees a complete and uniform coverage. Furthermore, we demonstrate that the algorithm execution always terminates preventing movement oscillations.   Numerous simulations show that our algorithm reaches a complete coverage within reasonable time with moderate energy consumption, even when the target area has irregular shapes. Performance comparisons between Push&Pull and one of the most acknowledged algorithms show how the former one can efficiently reach a more uniform and complete coverage under a wide range of working scenarios.",
        "published": "2008-04-14T14:38:52Z",
        "link": "http://arxiv.org/abs/0804.2191v4",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "On the Expressive Power of Multiple Heads in CHR",
        "authors": [
            "Cinzia Di Giusto",
            "Maurizio Gabbrielli",
            "Maria Chiara Meo"
        ],
        "summary": "Constraint Handling Rules (CHR) is a committed-choice declarative language which has been originally designed for writing constraint solvers and which is nowadays a general purpose language. CHR programs consist of multi-headed guarded rules which allow to rewrite constraints into simpler ones until a solved form is reached. Many empirical evidences suggest that multiple heads augment the expressive power of the language, however no formal result in this direction has been proved, so far.   In the first part of this paper we analyze the Turing completeness of CHR with respect to the underneath constraint theory. We prove that if the constraint theory is powerful enough then restricting to single head rules does not affect the Turing completeness of the language. On the other hand, differently from the case of the multi-headed language, the single head CHR language is not Turing powerful when the underlying signature (for the constraint theory) does not contain function symbols.   In the second part we prove that, no matter which constraint theory is considered, under some reasonable assumptions it is not possible to encode the CHR language (with multi-headed rules) into a single headed language while preserving the semantics of the programs. We also show that, under some stronger assumptions, considering an increasing number of atoms in the head of a rule augments the expressive power of the language.   These results provide a formal proof for the claim that multiple heads augment the expressive power of the CHR language.",
        "published": "2008-04-21T16:21:43Z",
        "link": "http://arxiv.org/abs/0804.3351v6",
        "categories": [
            "cs.LO",
            "cs.DC",
            "D.3.2; D.3.3; F.1.1; F.1.2; F.3.3"
        ]
    },
    {
        "title": "Robust Machine Learning Applied to Terascale Astronomical Datasets",
        "authors": [
            "Nicholas M. Ball",
            "Robert J. Brunner",
            "Adam D. Myers"
        ],
        "summary": "We present recent results from the LCDM (Laboratory for Cosmological Data Mining; http://lcdm.astro.uiuc.edu) collaboration between UIUC Astronomy and NCSA to deploy supercomputing cluster resources and machine learning algorithms for the mining of terascale astronomical datasets. This is a novel application in the field of astronomy, because we are using such resources for data mining, and not just performing simulations. Via a modified implementation of the NCSA cyberenvironment Data-to-Knowledge, we are able to provide improved classifications for over 100 million stars and galaxies in the Sloan Digital Sky Survey, improved distance measures, and a full exploitation of the simple but powerful k-nearest neighbor algorithm. A driving principle of this work is that our methods should be extensible from current terascale datasets to upcoming petascale datasets and beyond. We discuss issues encountered to-date, and further issues for the transition to petascale. In particular, disk I/O will become a major limiting factor unless the necessary infrastructure is implemented.",
        "published": "2008-04-21T21:58:18Z",
        "link": "http://arxiv.org/abs/0804.3417v1",
        "categories": [
            "astro-ph",
            "cs.DC"
        ]
    },
    {
        "title": "Energy and Time Efficient Scheduling of Tasks with Dependencies on   Asymmetric Multiprocessors",
        "authors": [
            "Ioannis Chatzigiannakis",
            "Georgios Giannoulis",
            "Paul G. Spirakis"
        ],
        "summary": "In this work we study the problem of scheduling tasks with dependencies in multiprocessor architectures where processors have different speeds. We present the preemptive algorithm \"Save-Energy\" that given a schedule of tasks it post processes it to improve the energy efficiency without any deterioration of the makespan. In terms of time efficiency, we show that preemptive scheduling in an asymmetric system can achieve the same or better optimal makespan than in a symmetric system. Motivited by real multiprocessor systems, we investigate architectures that exhibit limited asymmetry: there are two essentially different speeds. Interestingly, this special case has not been studied in the field of parallel computing and scheduling theory; only the general case was studied where processors have $K$ essentially different speeds. We present the non-preemptive algorithm ``Remnants'' that achieves almost optimal makespan. We provide a refined analysis of a recent scheduling method. Based on this analysis, we specialize the scheduling policy and provide an algorithm of $(3 + o(1))$ expected approximation factor. Note that this improves the previous best factor (6 for two speeds). We believe that our work will convince researchers to revisit this well studied scheduling problem for these simple, yet realistic, asymmetric multiprocessor architectures.",
        "published": "2008-04-25T03:16:21Z",
        "link": "http://arxiv.org/abs/0804.4039v2",
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.PF",
            "C.1.4; D.1.4"
        ]
    },
    {
        "title": "Étude de performance des systèmes de découverte de ressources",
        "authors": [
            "Heithem Abbes",
            "Christophe Cérin",
            "Jean-Christophe Dubacq",
            "Mohamed Jemni"
        ],
        "summary": "The Desktop Grid offers solutions to overcome several challenges and to answer increasingly needs of scientific computing. This technology consists mainly in exploiting PC resources, geographically dispersed, to treat time consuming applications and/or important storage capacity requiring applications. However, as resources number increases, the need for scalability, self-organisation, dynamic reconfiguration, decentralization and performance becomes more and more essential. In this context, this paper evaluates the scalability and performance of P2P tools for registering and discovering services (Publish/Subscribe systems). Three protocols are used in this purpose: Bonjour, Avahi and Pastry. We have studied the behaviour of these protocols related to two criteria: the elapsed time for registrations services and the needed time to discover new services.",
        "published": "2008-04-29T11:51:19Z",
        "link": "http://arxiv.org/abs/0804.4590v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Tight local approximation results for max-min linear programs",
        "authors": [
            "Patrik Floréen",
            "Marja Hassinen",
            "Petteri Kaski",
            "Jukka Suomela"
        ],
        "summary": "In a bipartite max-min LP, we are given a bipartite graph $\\myG = (V \\cup I \\cup K, E)$, where each agent $v \\in V$ is adjacent to exactly one constraint $i \\in I$ and exactly one objective $k \\in K$. Each agent $v$ controls a variable $x_v$. For each $i \\in I$ we have a nonnegative linear constraint on the variables of adjacent agents. For each $k \\in K$ we have a nonnegative linear objective function of the variables of adjacent agents. The task is to maximise the minimum of the objective functions. We study local algorithms where each agent $v$ must choose $x_v$ based on input within its constant-radius neighbourhood in $\\myG$. We show that for every $\\epsilon>0$ there exists a local algorithm achieving the approximation ratio ${\\Delta_I (1 - 1/\\Delta_K)} + \\epsilon$. We also show that this result is the best possible -- no local algorithm can achieve the approximation ratio ${\\Delta_I (1 - 1/\\Delta_K)}$. Here $\\Delta_I$ is the maximum degree of a vertex $i \\in I$, and $\\Delta_K$ is the maximum degree of a vertex $k \\in K$. As a methodological contribution, we introduce the technique of graph unfolding for the design of local approximation algorithms.",
        "published": "2008-04-30T12:54:34Z",
        "link": "http://arxiv.org/abs/0804.4815v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Universe Detectors for Sybil Defense in Ad Hoc Wireless Networks",
        "authors": [
            "Adnan Vora",
            "Mikhail Nesterenko",
            "Sébastien Tixeuil",
            "Sylvie Delaët"
        ],
        "summary": "The Sybil attack in unknown port networks such as wireless is not considered tractable. A wireless node is not capable of independently differentiating the universe of real nodes from the universe of arbitrary non-existent fictitious nodes created by the attacker. Similar to failure detectors, we propose to use universe detectors to help nodes determine which universe is real. In this paper, we (i) define several variants of the neighborhood discovery problem under Sybil attack (ii) propose a set of matching universe detectors (iii) demonstrate the necessity of additional topological constraints for the problems to be solvable: node density and communication range; (iv) present SAND -- an algorithm that solves these problems with the help of appropriate universe detectors, this solution demonstrates that the proposed universe detectors are the weakest detectors possible for each problem.",
        "published": "2008-05-01T11:45:54Z",
        "link": "http://arxiv.org/abs/0805.0087v2",
        "categories": [
            "cs.DC",
            "cs.CR",
            "cs.NI"
        ]
    },
    {
        "title": "Two-enqueuer queue in Common2",
        "authors": [
            "David Eisenstat"
        ],
        "summary": "The question of whether all shared objects with consensus number 2 belong to Common2, the set of objects that can be implemented in a wait-free manner by any type of consensus number 2, was first posed by Herlihy. In the absence of general results, several researchers have obtained implementations for restricted-concurrency versions of FIFO queues. We present the first Common2 algorithm for a queue with two enqueuers and any number of dequeuers.",
        "published": "2008-05-04T21:08:50Z",
        "link": "http://arxiv.org/abs/0805.0444v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Bounds for self-stabilization in unidirectional networks",
        "authors": [
            "Samuel Bernard",
            "Stéphane Devismes",
            "Maria Gradinariu Potop-Butucaru",
            "Sébastien Tixeuil"
        ],
        "summary": "A distributed algorithm is self-stabilizing if after faults and attacks hit the system and place it in some arbitrary global state, the systems recovers from this catastrophic situation without external intervention in finite time. Unidirectional networks preclude many common techniques in self-stabilization from being used, such as preserving local predicates. In this paper, we investigate the intrinsic complexity of achieving self-stabilization in unidirectional networks, and focus on the classical vertex coloring problem. When deterministic solutions are considered, we prove a lower bound of $n$ states per process (where $n$ is the network size) and a recovery time of at least $n(n-1)/2$ actions in total. We present a deterministic algorithm with matching upper bounds that performs in arbitrary graphs. When probabilistic solutions are considered, we observe that at least $\\Delta + 1$ states per process and a recovery time of $\\Omega(n)$ actions in total are required (where $\\Delta$ denotes the maximal degree of the underlying simple undirected graph). We present a probabilistically self-stabilizing algorithm that uses $\\mathtt{k}$ states per process, where $\\mathtt{k}$ is a parameter of the algorithm. When $\\mathtt{k}=\\Delta+1$, the algorithm recovers in expected $O(\\Delta n)$ actions. When $\\mathtt{k}$ may grow arbitrarily, the algorithm recovers in expected O(n) actions in total. Thus, our algorithm can be made optimal with respect to space or time complexity.",
        "published": "2008-05-07T07:39:14Z",
        "link": "http://arxiv.org/abs/0805.0851v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Service Oriented Architecture in Network Security - a novel Organisation   in Security Systems",
        "authors": [
            "Michael Hilker",
            "Christoph Schommer"
        ],
        "summary": "Current network security systems are a collection of various security components, which are directly installed in the operating system. These check the whole node for suspicious behaviour. Armouring intrusions e.g. have the ability to hide themselves from being checked. We present in this paper an alternative organisation of security systems. The node is completely virtualized with current virtualization systems so that the operating system with applications and the security system is distinguished. The security system then checks the node from outside and the right security components are provided through a service oriented architecture. Due to the running in a virtual machine, the infected nodes can be halted, duplicated, and moved to other nodes for further analysis and legal aspects. This organisation is in this article analysed and a preliminary implementation showing promising results are discussed.",
        "published": "2008-05-07T14:13:32Z",
        "link": "http://arxiv.org/abs/0805.0850v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "C.2.0"
        ]
    },
    {
        "title": "Randomized Work-Competitive Scheduling for Cooperative Computing on   $k$-partite Task Graphs",
        "authors": [
            "Chadi Kari",
            "Alexander Russell",
            "Narasimha Shashidhar"
        ],
        "summary": "A fundamental problem in distributed computing is the task of cooperatively executing a given set of $t$ tasks by $p$ processors where the communication medium is dynamic and subject to failures. The dynamics of the communication medium lead to groups of processors being disconnected and possibly reconnected during the entire course of the computation furthermore tasks can have dependencies among them. In this paper, we present a randomized algorithm whose competitive ratio is dependent on the dynamics of the communication medium and also on the nature of the dependencies among the tasks.",
        "published": "2008-05-09T00:27:28Z",
        "link": "http://arxiv.org/abs/0805.1257v2",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Parallel Pricing Algorithms for Multi--Dimensional Bermudan/American   Options using Monte Carlo methods",
        "authors": [
            "Mireille Bossy",
            "Françoise Baude",
            "Viet Dung Doan",
            "Abhijeet Gaikwad",
            "Ian Stokes-Rees"
        ],
        "summary": "In this paper we present two parallel Monte Carlo based algorithms for pricing multi--dimensional Bermudan/American options. First approach relies on computation of the optimal exercise boundary while the second relies on classification of continuation and exercise values. We also evaluate the performance of both the algorithms in a desktop grid environment. We show the effectiveness of the proposed approaches in a heterogeneous computing environment, and identify scalability constraints due to the algorithmic structure.",
        "published": "2008-05-13T12:34:04Z",
        "link": "http://arxiv.org/abs/0805.1827v1",
        "categories": [
            "cs.DC",
            "cs.CE"
        ]
    },
    {
        "title": "P&P protocol: local coordination of mobile sensors for self-deployment",
        "authors": [
            "N. Bartolini",
            "A. Massini",
            "S. Silvestri"
        ],
        "summary": "The use of mobile sensors is of great relevance for a number of strategic applications devoted to monitoring critical areas where sensors can not be deployed manually. In these networks, each sensor adapts its position on the basis of a local evaluation of the coverage efficiency, thus permitting an autonomous deployment.   Several algorithms have been proposed to deploy mobile sensors over the area of interest. The applicability of these approaches largely depends on a proper formalization of rigorous rules to coordinate sensor movements, solve local conflicts and manage possible failures of communications and devices.   In this paper we introduce P&P, a communication protocol that permits a correct and efficient coordination of sensor movements in agreement with the PUSH&PULL algorithm. We deeply investigate and solve the problems that may occur when coordinating asynchronous local decisions in the presence of an unreliable transmission medium and possibly faulty devices such as in the typical working scenario of mobile sensor networks.   Simulation results show the performance of our protocol under a range of operative settings, including conflict situations, irregularly shaped target areas, and node failures.",
        "published": "2008-05-14T06:50:58Z",
        "link": "http://arxiv.org/abs/0805.1981v2",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "Fork Sequential Consistency is Blocking",
        "authors": [
            "Christian Cachin",
            "Idit Keidar",
            "Alexander Shraer"
        ],
        "summary": "We consider an untrusted server storing shared data on behalf of clients. We show that no storage access protocol can on the one hand preserve sequential consistency and wait-freedom when the server is correct, and on the other hand always preserve fork sequential consistency.",
        "published": "2008-05-14T14:23:53Z",
        "link": "http://arxiv.org/abs/0805.2068v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "State and history in operating systems",
        "authors": [
            "Victor Yodaiken"
        ],
        "summary": "A method of using recursive functions to describe state change is applied to process switching in UNIX-like operating systems.",
        "published": "2008-05-18T19:44:21Z",
        "link": "http://arxiv.org/abs/0805.2749v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Network QoS Management in Cyber-Physical Systems",
        "authors": [
            "Feng Xia",
            "Longhua Ma",
            "Jinxiang Dong",
            "Youxian Sun"
        ],
        "summary": "Technical advances in ubiquitous sensing, embedded computing, and wireless communication are leading to a new generation of engineered systems called cyber-physical systems (CPS). CPS promises to transform the way we interact with the physical world just as the Internet transformed how we interact with one another. Before this vision becomes a reality, however, a large number of challenges have to be addressed. Network quality of service (QoS) management in this new realm is among those issues that deserve extensive research efforts. It is envisioned that wireless sensor/actuator networks (WSANs) will play an essential role in CPS. This paper examines the main characteristics of WSANs and the requirements of QoS provisioning in the context of cyber-physical computing. Several research topics and challenges are identified. As a sample solution, a feedback scheduling framework is proposed to tackle some of the identified challenges. A simple example is also presented that illustrates the effectiveness of the proposed solution.",
        "published": "2008-05-19T12:49:11Z",
        "link": "http://arxiv.org/abs/0805.2854v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "C.2.1; J.7"
        ]
    },
    {
        "title": "Performability Aspects of the Atlas Vo; Using Lmbench Suite",
        "authors": [
            "Fotis Georgatos",
            "John Kouvakis",
            "John Kouretis"
        ],
        "summary": "The ATLAS Virtual Organization is grid's largest Virtual Organization which is currently in full production stage. Hereby a case is being made that a user working within that VO is going to face a wide spectrum of different systems, whose heterogeneity is enough to count as \"orders of magnitude\" according to a number of metrics; including integer/float operations, memory throughput (STREAM) and communication latencies. Furthermore, the spread of performance does not appear to follow any known distribution pattern, which is demonstrated in graphs produced during May 2007 measurements. It is implied that the current practice where either \"all-WNs-are-equal\" or, the alternative of SPEC-based rating used by LCG/EGEE is an oversimplification which is inappropriate and expensive from an operational point of view, therefore new techniques are needed for optimal grid resources allocation.",
        "published": "2008-05-19T19:45:31Z",
        "link": "http://arxiv.org/abs/0805.2949v1",
        "categories": [
            "cs.PF",
            "cs.CE",
            "cs.DC"
        ]
    },
    {
        "title": "Coupling Component Systems towards Systems of Systems",
        "authors": [
            "Frédéric Autran",
            "Jean-Philippe Auzelle",
            "Denise Cattan",
            "Jean-Luc Garnier",
            "Dominique Luzeaux",
            "Frédérique Mayer",
            "Marc Peyrichon",
            "Jean-René Ruault"
        ],
        "summary": "Systems of systems (SoS) are a hot topic in our \"fully connected global world\". Our aim is not to provide another definition of what SoS are, but rather to focus on the adequacy of reusing standard system architecting techniques within this approach in order to improve performance, fault detection and safety issues in large-scale coupled systems that definitely qualify as SoS, whatever the definition is. A key issue will be to secure the availability of the services provided by the SoS despite the evolution of the various systems composing the SoS. We will also tackle contracting issues and responsibility transfers, as they should be addressed to ensure the expected behavior of the SoS whilst the various independently contracted systems evolve asynchronously.",
        "published": "2008-05-21T05:02:22Z",
        "link": "http://arxiv.org/abs/0805.3196v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Compressing Binary Decision Diagrams",
        "authors": [
            "Esben Rune Hansen",
            "S. Srinivasa Rao",
            "Peter Tiedemann"
        ],
        "summary": "The paper introduces a new technique for compressing Binary Decision Diagrams in those cases where random access is not required. Using this technique, compression and decompression can be done in linear time in the size of the BDD and compression will in many cases reduce the size of the BDD to 1-2 bits per node. Empirical results for our compression technique are presented, including comparisons with previously introduced techniques, showing that the new technique dominate on all tested instances.",
        "published": "2008-05-21T12:44:23Z",
        "link": "http://arxiv.org/abs/0805.3267v1",
        "categories": [
            "cs.AI",
            "cs.DC"
        ]
    },
    {
        "title": "Event Synchronization by Lightweight Message Passing",
        "authors": [
            "Avik Chaudhuri"
        ],
        "summary": "Concurrent ML's events and event combinators facilitate modular concurrent programming with first-class synchronization abstractions. A standard implementation of these abstractions relies on fairly complex manipulations of first-class continuations in the underlying language. In this paper, we present a lightweight implementation of these abstractions in Concurrent Haskell, a language that already provides first-order message passing. At the heart of our implementation is a new distributed synchronization protocol. In contrast with several previous translations of event abstractions in concurrent languages, we remain faithful to the standard semantics for events and event combinators; for example, we retain the symmetry of $\\mathtt{choose}$ for expressing selective communication.",
        "published": "2008-05-27T01:31:05Z",
        "link": "http://arxiv.org/abs/0805.4029v1",
        "categories": [
            "cs.PL",
            "cs.DC",
            "D.3.3; D.1.3; F.3.3"
        ]
    },
    {
        "title": "Design and Implementation Aspects of a novel Java P2P Simulator with GUI",
        "authors": [
            "V. Chrissikopoulos",
            "G. Papaloukopoulos",
            "E. Sakkopoulos",
            "S. Sioutas"
        ],
        "summary": "Peer-to-peer networks consist of thousands or millions of nodes that might join and leave arbitrarily. The evaluation of new protocols in real environments is many times practically impossible, especially at design and testing stages. The purpose of this paper is to describe the implementation aspects of a new Java based P2P simulator that has been developed to support scalability in the evaluation of such P2P dynamic environments. Evolving the functionality presented by previous solutions, we provide a friendly graphical user interface through which the high-level theoretic researcher/designer of a P2P system can easily construct an overlay with the desirable number of nodes and evaluate its operations using a number of key distributions. Furthermore, the simulator has built-in ability to produce statistics about the distributed structure. Emphasis was given to the parametrical configuration of the simulator. As a result the developed tool can be utilized in the simulation and evaluation procedures of a variety of different protocols, with only few changes in the Java code.",
        "published": "2008-05-27T14:36:20Z",
        "link": "http://arxiv.org/abs/0805.4134v1",
        "categories": [
            "cs.NI",
            "cs.DB",
            "cs.DC",
            "D.2; H.2; E.1"
        ]
    },
    {
        "title": "On Secure Distributed Implementations of Dynamic Access Control",
        "authors": [
            "Avik Chaudhuri"
        ],
        "summary": "Distributed implementations of access control abound in distributed storage protocols. While such implementations are often accompanied by informal justifications of their correctness, our formal analysis reveals that their correctness can be tricky. In particular, we discover several subtleties in a standard protocol based on capabilities, that can break security under a simple specification of access control. At the same time, we show a sensible refinement of the specification for which a secure implementation of access control is possible. Our models and proofs are formalized in the applied pi calculus, following some new techniques that may be of independent interest. Finally, we indicate how our principles can be applied to securely distribute other state machines.",
        "published": "2008-05-30T03:50:12Z",
        "link": "http://arxiv.org/abs/0805.4665v1",
        "categories": [
            "cs.CR",
            "cs.DC"
        ]
    },
    {
        "title": "Telex: Principled System Support for Write-Sharing in Collaborative   Applications",
        "authors": [
            "Lamia Benmouffok",
            "Jean-Michel Busca",
            "Joan Manuel Marquès",
            "Marc Shapiro",
            "Pierre Sutra",
            "Georgios Tsoukalas"
        ],
        "summary": "The Telex system is designed for sharing mutable data in a distributed environment, particularly for collaborative applications. Users operate on their local, persistent replica of shared documents; they can work disconnected and suffer no network latency. The Telex approach to detect and correct conflicts is application independent, based on an action-constraint graph (ACG) that summarises the concurrency semantics of applications. The ACG is stored efficiently in a multilog structure that eliminates contention and is optimised for locality. Telex supports multiple applications and multi-document updates. The Telex system clearly separates system logic (which includes replication, views, undo, security, consistency, conflicts, and commitment) from application logic. An example application is a shared calendar for managing multi-user meetings; the system detects meeting conflicts and resolves them consistently.",
        "published": "2008-05-30T07:01:51Z",
        "link": "http://arxiv.org/abs/0805.4680v3",
        "categories": [
            "cs.OS",
            "cs.DC"
        ]
    },
    {
        "title": "Local approximation algorithms for a class of 0/1 max-min linear   programs",
        "authors": [
            "Patrik Floréen",
            "Marja Hassinen",
            "Petteri Kaski",
            "Jukka Suomela"
        ],
        "summary": "We study the applicability of distributed, local algorithms to 0/1 max-min LPs where the objective is to maximise ${\\min_k \\sum_v c_{kv} x_v}$ subject to ${\\sum_v a_{iv} x_v \\le 1}$ for each $i$ and ${x_v \\ge 0}$ for each $v$. Here $c_{kv} \\in \\{0,1\\}$, $a_{iv} \\in \\{0,1\\}$, and the support sets ${V_i = \\{v : a_{iv} > 0 \\}}$ and ${V_k = \\{v : c_{kv}>0 \\}}$ have bounded size; in particular, we study the case $|V_k| \\le 2$. Each agent $v$ is responsible for choosing the value of $x_v$ based on information within its constant-size neighbourhood; the communication network is the hypergraph where the sets $V_k$ and $V_i$ constitute the hyperedges. We present a local approximation algorithm which achieves an approximation ratio arbitrarily close to the theoretical lower bound presented in prior work.",
        "published": "2008-06-02T14:27:46Z",
        "link": "http://arxiv.org/abs/0806.0282v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "The Lotus-Eater Attack",
        "authors": [
            "Ian A. Kash",
            "Eric J. Friedman",
            "Joseph Y. Halpern"
        ],
        "summary": "Many protocols for distributed and peer-to-peer systems have the feature that nodes will stop providing service for others once they have received a certain amount of service. Examples include BitTorent's unchoking policy, BAR Gossip's balanced exchanges, and threshold strategies in scrip systems. An attacker can exploit this by providing service in a targeted way to prevent chosen nodes from providing service. While such attacks cannot be prevented, we discuss techniques that can be used to limit the damage they do. These techniques presume that a certain number of processes will follow the recommended protocol, even if they could do better by ``gaming'' the system.",
        "published": "2008-06-10T17:31:42Z",
        "link": "http://arxiv.org/abs/0806.1711v1",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "Local Read-Write Operations in Sensor Networks",
        "authors": [
            "Ted Herman",
            "Morten Mjelde"
        ],
        "summary": "Designing protocols and formulating convenient programming units of abstraction for sensor networks is challenging due to communication errors and platform constraints. This paper investigates properties and implementation reliability for a \\emph{local read-write} abstraction. Local read-write is inspired by the class of read-modify-write operations defined for shared-memory multiprocessor architectures. The class of read-modify-write operations is important in solving consensus and related synchronization problems for concurrency control. Local read-write is shown to be an atomic abstraction for synchronizing neighborhood states in sensor networks. The paper compares local read-write to similar lightweight operations in wireless sensor networks, such as read-all, write-all, and a transaction-based abstraction: for some optimistic scenarios, local read-write is a more efficient neighborhood operation. A partial implementation is described, which shows that three outcomes characterize operation response: success, failure, and cancel. A failure response indicates possible inconsistency for the operation result, which is the result of a timeout event at the operation's initiator. The paper presents experimental results on operation performance with different timeout values and situations of no contention, with some tests also on various neighborhood sizes.",
        "published": "2008-06-11T00:03:00Z",
        "link": "http://arxiv.org/abs/0806.1768v1",
        "categories": [
            "cs.OS",
            "cs.DC",
            "C.2.4; D.1.3; C.3"
        ]
    },
    {
        "title": "Beyond Nash Equilibrium: Solution Concepts for the 21st Century",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "Nash equilibrium is the most commonly-used notion of equilibrium in game theory. However, it suffers from numerous problems. Some are well known in the game theory community; for example, the Nash equilibrium of repeated prisoner's dilemma is neither normatively nor descriptively reasonable. However, new problems arise when considering Nash equilibrium from a computer science perspective: for example, Nash equilibrium is not robust (it does not tolerate ``faulty'' or ``unexpected'' behavior), it does not deal with coalitions, it does not take computation cost into account, and it does not deal with cases where players are not aware of all aspects of the game. Solution concepts that try to address these shortcomings of Nash equilibrium are discussed.",
        "published": "2008-06-12T19:23:07Z",
        "link": "http://arxiv.org/abs/0806.2139v1",
        "categories": [
            "cs.GT",
            "cs.AI",
            "cs.CR",
            "cs.DC",
            "C.2.4; F.0; I.2.11; J.4"
        ]
    },
    {
        "title": "Ad-hoc Limited Scale-Free Models for Unstructured Peer-to-Peer Networks",
        "authors": [
            "Hasan Guclu",
            "Durgesh Kumari",
            "Murat Yuksel"
        ],
        "summary": "Several protocol efficiency metrics (e.g., scalability, search success rate, routing reachability and stability) depend on the capability of preserving structure even over the churn caused by the ad-hoc nodes joining or leaving the network. Preserving the structure becomes more prohibitive due to the distributed and potentially uncooperative nature of such networks, as in the peer-to-peer (P2P) networks. Thus, most practical solutions involve unstructured approaches while attempting to maintain the structure at various levels of protocol stack. The primary focus of this paper is to investigate construction and maintenance of scale-free topologies in a distributed manner without requiring global topology information at the time when nodes join or leave. We consider the uncooperative behavior of peers by limiting the number of neighbors to a pre-defined hard cutoff value (i.e., no peer is a major hub), and the ad-hoc behavior of peers by rewiring the neighbors of nodes leaving the network. We also investigate the effect of these hard cutoffs and rewiring of ad-hoc nodes on the P2P search efficiency.",
        "published": "2008-06-14T19:01:50Z",
        "link": "http://arxiv.org/abs/0806.2395v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "MultiKulti Algorithm: Migrating the Most Different Genotypes in an   Island Model",
        "authors": [
            "Lourdes Araujo",
            "Juan J. Merelo Guervos",
            "Carlos Cotta",
            "Francisco Fernandez de Vega"
        ],
        "summary": "Migration policies in distributed evolutionary algorithms has not been an active research area until recently. However, in the same way as operators have an impact on performance, the choice of migrants is due to have an impact too. In this paper we propose a new policy (named multikulti) for choosing the individuals that are going to be sent to other nodes, based on multiculturality: the individual sent should be as different as possible to the receiving population. We have checked this policy on different discrete optimization problems, and found that, in average or in median, this policy outperforms classical ones like sending the best or a random individual.",
        "published": "2008-06-17T17:19:13Z",
        "link": "http://arxiv.org/abs/0806.2843v2",
        "categories": [
            "cs.NE",
            "cs.DC"
        ]
    },
    {
        "title": "A Random Search Framework for Convergence Analysis of Distributed   Beamforming with Feedback",
        "authors": [
            "C. Lin",
            "V. V. Veeravalli",
            "S. Meyn"
        ],
        "summary": "The focus of this work is on the analysis of transmit beamforming schemes with a low-rate feedback link in wireless sensor/relay networks, where nodes in the network need to implement beamforming in a distributed manner. Specifically, the problem of distributed phase alignment is considered, where neither the transmitters nor the receiver has perfect channel state information, but there is a low-rate feedback link from the receiver to the transmitters. In this setting, a framework is proposed for systematically analyzing the performance of distributed beamforming schemes. To illustrate the advantage of this framework, a simple adaptive distributed beamforming scheme that was recently proposed by Mudambai et al. is studied. Two important properties for the received signal magnitude function are derived. Using these properties and the systematic framework, it is shown that the adaptive distributed beamforming scheme converges both in probability and in mean. Furthermore, it is established that the time required for the adaptive scheme to converge in mean scales linearly with respect to the number of sensor/relay nodes.",
        "published": "2008-06-18T15:56:19Z",
        "link": "http://arxiv.org/abs/0806.3023v3",
        "categories": [
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Algorithmic Based Fault Tolerance Applied to High Performance Computing",
        "authors": [
            "George Bosilca",
            "Remi Delmas",
            "Jack Dongarra",
            "Julien Langou"
        ],
        "summary": "We present a new approach to fault tolerance for High Performance Computing system. Our approach is based on a careful adaptation of the Algorithmic Based Fault Tolerance technique (Huang and Abraham, 1984) to the need of parallel distributed computation. We obtain a strongly scalable mechanism for fault tolerance. We can also detect and correct errors (bit-flip) on the fly of a computation. To assess the viability of our approach, we have developed a fault tolerant matrix-matrix multiplication subroutine and we propose some models to predict its running time. Our parallel fault-tolerant matrix-matrix multiplication scores 1.4 TFLOPS on 484 processors (cluster jacquard.nersc.gov) and returns a correct result while one process failure has happened. This represents 65% of the machine peak efficiency and less than 12% overhead with respect to the fastest failure-free implementation. We predict (and have observed) that, as we increase the processor count, the overhead of the fault tolerance drops significantly.",
        "published": "2008-06-19T02:06:57Z",
        "link": "http://arxiv.org/abs/0806.3121v1",
        "categories": [
            "cs.DC",
            "cs.MS"
        ]
    },
    {
        "title": "TRANS-Net: an Efficient Peer-to-Peer Overlay Network Based on a Full   Transposition Graph",
        "authors": [
            "Stavros Kontopoulos",
            "Athanasios K. Tsakalidis"
        ],
        "summary": "In this paper we propose a new practical P2P system based on a full transposition network topology named TRANS-Net. Full transposition networks achieve higher fault-tolerance and lower congestion among the class of transposition networks. TRANS-Net provides an efficient lookup service i.e. k hops with high probability, where k satisfies Theta(log_n m) less than k less than Theta(log_2 m), where m denotes the number of system nodes and n is a system parameter related to the maximum number that m can take (up to n!). Experiments show that the look-up performance achieves the lower limit of the complexity relation. TRANS-Net also preserves data locality and provides efficient look-up performance for complex queries such as multi-dimensional queries.",
        "published": "2008-06-19T08:28:28Z",
        "link": "http://arxiv.org/abs/0806.3152v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "MOHCS: Towards Mining Overlapping Highly Connected Subgraphs",
        "authors": [
            "Xiahong Lin",
            "Lin Gao",
            "Kefei Chen",
            "David K. Y. Chiu"
        ],
        "summary": "Many networks in real-life typically contain parts in which some nodes are more highly connected to each other than the other nodes of the network. The collection of such nodes are usually called clusters, communities, cohesive groups or modules. In graph terminology, it is called highly connected graph. In this paper, we first prove some properties related to highly connected graph. Based on these properties, we then redefine the highly connected subgraph which results in an algorithm that determines whether a given graph is highly connected in linear time. Then we present a computationally efficient algorithm, called MOHCS, for mining overlapping highly connected subgraphs. We have evaluated experimentally the performance of MOHCS using real and synthetic data sets from computer-generated graph and yeast protein network. Our results show that MOHCS is effective and reliable in finding overlapping highly connected subgraphs. Keywords-component; Highly connected subgraph, clustering algorithms, minimum cut, minimum degree",
        "published": "2008-06-19T15:13:38Z",
        "link": "http://arxiv.org/abs/0806.3215v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Stabilizing Tiny Interaction Protocols",
        "authors": [
            "Davide Canepa",
            "Maria Gradinariu Potop-Butucaru"
        ],
        "summary": "In this paper we present the self-stabilizing implementation of a class of token based algorithms. In the current work we only consider interactions between weak nodes. They are uniform, they do not have unique identifiers, are static and their interactions are restricted to a subset of nodes called neighbours. While interacting, a pair of neighbouring nodes may create mobile agents (that materialize in the current work the token abstraction) that perform traversals of the network and accelerate the system stabilization. In this work we only explore the power of oblivious stateless agents.   Our work shows that the agent paradigm is an elegant distributed tool for achieving self-stabilization in Tiny Interaction Protocols (TIP). Nevertheless, in order to reach the full power of classical self-stabilizing algorithms more complex classes of agents have to be considered (e.g. agents with memory, identifiers or communication skills). Interestingly, our work proposes for the first time a model that unifies the recent studies in mobile robots(agents) that evolve in a discrete space and the already established population protocols paradigm.",
        "published": "2008-06-20T21:01:52Z",
        "link": "http://arxiv.org/abs/0806.3471v2",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Localized Spanners for Wireless Networks",
        "authors": [
            "Mirela Damian",
            "Sriram V. Pemmaraju"
        ],
        "summary": "We present a new efficient localized algorithm to construct, for any given quasi-unit disk graph G=(V,E) and any e > 0, a (1+e)-spanner for G of maximum degree O(1) and total weight O(w(MST)), where w(MST) denotes the weight of a minimum spanning tree for V. We further show that similar localized techniques can be used to construct, for a given unit disk graph G = (V, E), a planar Cdel(1+e)(1+pi/2)-spanner for G of maximum degree O(1) and total weight O(w(MST)). Here Cdel denotes the stretch factor of the unit Delaunay triangulation for V. Both constructions can be completed in O(1) communication rounds, and require each node to know its own coordinates.",
        "published": "2008-06-26T02:09:17Z",
        "link": "http://arxiv.org/abs/0806.4221v1",
        "categories": [
            "cs.DC",
            "C.2.1; F.2.2"
        ]
    },
    {
        "title": "The Dynamics of Probabilistic Population Protocols",
        "authors": [
            "Ioannis Chatzigiannakis",
            "Paul G. Spirakis"
        ],
        "summary": "We study here the dynamics (and stability) of Probabilistic Population Protocols, via the differential equations approach. We provide a quite general model and we show that it includes the model of Angluin et. al. in the case of very large populations. For the general model we give a sufficient condition for stability that can be checked in polynomial time. We also study two interesting subcases: (a) protocols whose specifications (in our terms) are configuration independent. We show that they are always stable and that their eventual subpopulation percentages are actually a Markov Chain stationary distribution. (b) protocols that have dynamics resembling virus spread. We show that their dynamics are actually similar to the well-known Replicator Dynamics of Evolutionary Games. We also provide a sufficient condition for stability in this case.",
        "published": "2008-07-01T12:49:22Z",
        "link": "http://arxiv.org/abs/0807.0140v1",
        "categories": [
            "cs.DC",
            "cs.GT",
            "C.2.4; C.2.1; F.1.1; F.1.2"
        ]
    },
    {
        "title": "Greedy D-Approximation Algorithm for Covering with Arbitrary Constraints   and Submodular Cost",
        "authors": [
            "Christos Koufogiannakis",
            "Neal E. Young"
        ],
        "summary": "This paper describes a simple greedy D-approximation algorithm for any covering problem whose objective function is submodular and non-decreasing, and whose feasible region can be expressed as the intersection of arbitrary (closed upwards) covering constraints, each of which constrains at most D variables of the problem. (A simple example is Vertex Cover, with D = 2.) The algorithm generalizes previous approximation algorithms for fundamental covering problems and online paging and caching problems.",
        "published": "2008-07-04T23:31:29Z",
        "link": "http://arxiv.org/abs/0807.0644v4",
        "categories": [
            "cs.DS",
            "cs.DC",
            "68W25",
            "G.1.6"
        ]
    },
    {
        "title": "Optimal Direct Sum and Privacy Trade-off Results for Quantum and   Classical Communication Complexity",
        "authors": [
            "Rahul Jain",
            "Pranab Sen",
            "Jaikumar Radhakrishnan"
        ],
        "summary": "We show optimal Direct Sum result for the one-way entanglement-assisted quantum communication complexity for any relation f subset of X x Y x Z. We show: Q^{1,pub}(f^m) = Omega(m Q^{1,pub}(f)), where Q^{1,pub}(f), represents the one-way entanglement-assisted quantum communication complexity of f with error at most 1/3 and f^m represents m-copies of f. Similarly for the one-way public-coin classical communication complexity we show: R^{1,pub}(f^m) = Omega(m R^{1,pub}(f)), where R^{1,pub}(f), represents the one-way public-coin classical communication complexity of f with error at most 1/3. We show similar optimal Direct Sum results for the Simultaneous Message Passing quantum and classical models. For two-way protocols we present optimal Privacy Trade-off results leading to a Weak Direct Sum result for such protocols. We show our Direct Sum and Privacy Trade-off results via message compression arguments which also imply a new round elimination lemma in quantum communication. This allows us to extend classical lower bounds on the cell probe complexity of some data structure problems, e.g. Approximate Nearest Neighbor Searching on the Hamming cube {0,1}^n and Predecessor Search to the quantum setting. In a separate result we show that Newman's technique of reducing the number of public-coins in a classical protocol cannot be lifted to the quantum setting. We do this by defining a general notion of black-box reduction of prior entanglement that subsumes Newman's technique. We prove that such a black-box reduction is impossible for quantum protocols. In the final result in the theme of message compression, we provide an upper bound on the problem of Exact Remote State Preparation.",
        "published": "2008-07-08T15:05:45Z",
        "link": "http://arxiv.org/abs/0807.1267v1",
        "categories": [
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Simulations of Large-scale WiFi-based Wireless Networks:   Interdisciplinary Challenges and Applications",
        "authors": [
            "Maziar Nekovee"
        ],
        "summary": "Wireless Fidelity (WiFi) is the fastest growing wireless technology to date. In addition to providing wire-free connectivity to the Internet WiFi technology also enables mobile devices to connect directly to each other and form highly dynamic wireless adhoc networks. Such distributed networks can be used to perform cooperative communication tasks such ad data routing and information dissemination in the absence of a fixed infrastructure. Furthermore, adhoc grids composed of wirelessly networked portable devices are emerging as a new paradigm in grid computing. In this paper we review computational and algorithmic challenges of high-fidelity simulations of such WiFi-based wireless communication and computing networks, including scalable topology maintenance, mobility modelling, parallelisation and synchronisation. We explore similarities and differences between the simulations of these networks and simulations of interacting many-particle systems, such as molecular dynamics (MD) simulations. We show how the cell linked-list algorithm which we have adapted from our MD simulations can be used to greatly improve the computational performance of wireless network simulators in the presence of mobility, and illustrate with an example from our simulation studies of worm attacks on mobile wireless adhoc networks.",
        "published": "2008-07-09T16:04:37Z",
        "link": "http://arxiv.org/abs/0807.1475v1",
        "categories": [
            "cs.CE",
            "cs.DC"
        ]
    },
    {
        "title": "Resource Allocation Strategies for In-Network Stream Processing",
        "authors": [
            "Anne Benoit",
            "Henri Casanova",
            "Veronika Rehn-Sonigo",
            "Yves Robert"
        ],
        "summary": "In this paper we consider the operator mapping problem for in-network stream processing applications. In-network stream processing consists in applying a tree of operators in steady-state to multiple data objects that are continually updated at various locations on a network. Examples of in-network stream processing include the processing of data in a sensor network, or of continuous queries on distributed relational databases. We study the operator mapping problem in a ``constructive'' scenario, i.e., a scenario in which one builds a platform dedicated to the application buy purchasing processing servers with various costs and capabilities. The objective is to minimize the cost of the platform while ensuring that the application achieves a minimum steady-state throughput. The first contribution of this paper is the formalization of a set of relevant operator-placement problems as linear programs, and a proof that even simple versions of the problem are NP-complete. Our second contribution is the design of several polynomial time heuristics, which are evaluated via extensive simulations and compared to theoretical bounds for optimal solutions.",
        "published": "2008-07-10T19:14:14Z",
        "link": "http://arxiv.org/abs/0807.1720v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "The cost of probabilistic gathering in oblivious robot networks",
        "authors": [
            "Julien Clement",
            "Xavier Defago",
            "Maria Gradinariu Potop-Butucaru",
            "Stephane Messika"
        ],
        "summary": "In this paper we address the complexity issues of two agreement problems in oblivious robot networks namely gathering and scattering. These abstractions are fundamental coordination problems in cooperative mobile robotics. Moreover, their oblivious characteristics makes them appealing for self-stabilization since they are self-stabilizing with no extra-cost. Given a set of robots with arbitrary initial location and no initial agreement on a global coordinate system, gathering requires that all robots reach the exact same but not predetermined location while scattering aims at scatter robots such that no two robots share the same location. Both deterministic gathering and scattering have been proved impossible under arbitrary schedulers therefore probabilistic solutions have been recently proposed. The contribution of this paper is twofold. First, we propose a detailed complexity analysis of the existent probabilistic gathering algorithms in both fault-free and fault-prone environments. We consider both crash and byzantine-prone environments. Moreover, using Markov chains tools and additional assumptions on the environment we prove that the gathering convergence time can be reduced from O(n^2) (the best known tight bound) to O(nln(n)). Additionally, we prove that in crash-prone environments gathering is achieved in O(nln(n)+2f). Second, using the same technique we prove that the best known scattering strategy converges in fault-free systems is O(n) (which is one to optimal) while in crash-prone environments it needs O(n-f). Finally, we conclude the paper with a discussion related to different strategies to gather oblivious robots.",
        "published": "2008-07-10T22:43:42Z",
        "link": "http://arxiv.org/abs/0807.1753v1",
        "categories": [
            "cs.DC",
            "cs.CC"
        ]
    },
    {
        "title": "Virtual Transmission Method, A New Distributed Algorithm to Solve Sparse   Linear System",
        "authors": [
            "Fei Wei",
            "Huazhong Yang"
        ],
        "summary": "In this paper, we propose a new parallel algorithm which could work naturally on the parallel computer with arbitrary number of processors. This algorithm is named Virtual Transmission Method (VTM). Its physical backgroud is the lossless transmission line and microwave network. The basic idea of VTM is to insert lossless transmission lines into the sparse linear system to achieve distributed computing.   VTM is proved to be convergent to solve SPD linear system. Preconditioning method and performance model are presented. Numerical experiments show that VTM is efficient, accurate and stable.   Accompanied with VTM, we bring in a new technique to partition the symmetric linear system, which is named Generalized Node & Branch Tearing (GNBT). It is based on Kirchhoff's Current Law from circuit theory. We proved that GNBT is feasible to partition any SPD linear system.",
        "published": "2008-07-12T03:19:51Z",
        "link": "http://arxiv.org/abs/0807.1949v5",
        "categories": [
            "math.NA",
            "cs.DC",
            "65F10, 65F50, 68M14",
            "G.1.0; B.7.2"
        ]
    },
    {
        "title": "The Dark Energy Survey Data Management System",
        "authors": [
            "Joseph J. Mohr",
            "Wayne Barkhouse",
            "Cristina Beldica",
            "Emmanuel Bertin",
            "Y. Dora Cai",
            "Luiz da Costa",
            "J. Anthony Darnell",
            "Gregory E. Daues",
            "Michael Jarvis",
            "Michelle Gower",
            "Huan Lin",
            "leandro Martelli",
            "Eric Neilsen",
            "Chow-Choong Ngeow",
            "Ricardo Ogando",
            "Alex Parga",
            "Erin Sheldon",
            "Douglas Tucker",
            "Nikolay Kuropatkin",
            "Chris Stoughton"
        ],
        "summary": "The Dark Energy Survey collaboration will study cosmic acceleration with a 5000 deg2 griZY survey in the southern sky over 525 nights from 2011-2016. The DES data management (DESDM) system will be used to process and archive these data and the resulting science ready data products. The DESDM system consists of an integrated archive, a processing framework, an ensemble of astronomy codes and a data access framework. We are developing the DESDM system for operation in the high performance computing (HPC) environments at NCSA and Fermilab. Operating the DESDM system in an HPC environment offers both speed and flexibility. We will employ it for our regular nightly processing needs, and for more compute-intensive tasks such as large scale image coaddition campaigns, extraction of weak lensing shear from the full survey dataset, and massive seasonal reprocessing of the DES data. Data products will be available to the Collaboration and later to the public through a virtual-observatory compatible web portal. Our approach leverages investments in publicly available HPC systems, greatly reducing hardware and maintenance costs to the project, which must deploy and maintain only the storage, database platforms and orchestration and web portal nodes that are specific to DESDM. In Fall 2007, we tested the current DESDM system on both simulated and real survey data. We used Teragrid to process 10 simulated DES nights (3TB of raw data), ingesting and calibrating approximately 250 million objects into the DES Archive database. We also used DESDM to process and calibrate over 50 nights of survey data acquired with the Mosaic2 camera. Comparison to truth tables in the case of the simulated data and internal crosschecks in the case of the real data indicate that astrometric and photometric data quality is excellent.",
        "published": "2008-07-16T08:37:43Z",
        "link": "http://arxiv.org/abs/0807.2515v1",
        "categories": [
            "astro-ph",
            "cs.DC"
        ]
    },
    {
        "title": "How to Compute Times of Random Walks based Distributed Algorithms",
        "authors": [
            "Alain Bui",
            "Devan Sohier"
        ],
        "summary": "Random walk based distributed algorithms make use of a token that circulates in the system according to a random walk scheme to achieve their goal. To study their efficiency and compare it to one of the deterministic solutions, one is led to compute certain quantities, namely the hitting times and the cover time. Until now, only bounds on these quantities were defined. First, this paper presents two generalizations of the notions of hitting and cover times to weighted graphs. Indeed, the properties of random walks on symmetrically weighted graphs provide interesting results on random walk based distributed algorithms, such as local load balancing. Both of these generalization are proposed to precisely represent the behaviour of these algorithms, and to take into account what the weights represent. Then, we propose an algorithm to compute the n^2 hitting times on a weighted graph of n vertices, which we improve to obtain a O(n^3) complexity. This complexity is the lowest up to now. This algorithm computes both of the generalizations that we propose for the hitting times on a weighted graph. Finally, we provide the first algorithm to compute the cover time (in both senses) of a graph. We improve it to achieve a complexity of O(n^3 2^n). The algorithms that we present are all robust to a topological change in a limited number of edges. This property allows us to use them on dynamic graphs.",
        "published": "2008-07-23T10:10:59Z",
        "link": "http://arxiv.org/abs/0807.3632v1",
        "categories": [
            "cs.DC",
            "cs.DM"
        ]
    },
    {
        "title": "Analisis Kinerja Sistem Cluster Terhadapa Aplikasi Simulasi Dinamika   Molekular NAMD Memanfaatkan Pustaka CHARM++",
        "authors": [
            "A. B. Mutiara"
        ],
        "summary": "Tingkat kompleksitas dari program simulasi dinamika molekular membutuhkan mesin pemroses dengan kemampuan yang sangat besar. Mesin-mesin paralel terbukti memiliki potensi untuk menjawab tantangan komputasi ini. Untuk memanfaatkan potensi ini secara maksimal, diperlukan suatu program paralel dengan tingkat efisiensi, efektifitas, skalabilitas, dan ekstensibilitas yang maksimal pula. Program NAMD yang dibahas pada penulisan ini dianggap mampu untuk memenuhi semua kriteria yang diinginkan. Program ini dirancang dengan mengimplementasikan pustaka Charm++ untuk pembagian tugas perhitungan secara paralel. NAMD memiliki sistem automatic load balancing secara periodik yang cerdas, sehingga dapat memaksimalkan penggunaan kemampuan mesin yang tersedia. Program ini juga dirancang secara modular, sehingga dapat dimodifikasi dan ditambah dengan sangat mudah. NAMD menggunakan banyak kombinasi algoritma perhitungan dan tehnik-tehnik numerik lainnya dalam melakukan tugasnya. NAMD 2.5 mengimplementasikan semua tehnik dan persamaan perhitungan yang digunakan dalam dunia simulasi dinamika molekular saat ini. NAMD dapat berjalan diatas berbagai mesin paralel termasuk arsitektur cluster, dengan hasil speedup yang mengejutkan. Tulisan ini akan menjelaskan dan membuktikan kemampuan NAMD secara paralel diatas lima buah mesin cluster. Penulisan ini juga akan memaparkan kinerja NAMD pada beberapa.",
        "published": "2008-07-29T09:18:21Z",
        "link": "http://arxiv.org/abs/0807.4609v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A Distributed and Deterministic TDMA Algorithm for   Write-All-With-Collision Model",
        "authors": [
            "Mahesh Arumugam"
        ],
        "summary": "Several self-stabilizing time division multiple access (TDMA) algorithms are proposed for sensor networks. In addition to providing a collision-free communication service, such algorithms enable the transformation of programs written in abstract models considered in distributed computing literature into a model consistent with sensor networks, i.e., write all with collision (WAC) model. Existing TDMA slot assignment algorithms have one or more of the following properties: (i) compute slots using a randomized algorithm, (ii) assume that the topology is known upfront, and/or (iii) assign slots sequentially. If these algorithms are used to transform abstract programs into programs in WAC model then the transformed programs are probabilistically correct, do not allow the addition of new nodes, and/or converge in a sequential fashion. In this paper, we propose a self-stabilizing deterministic TDMA algorithm where a sensor is aware of only its neighbors. We show that the slots are assigned to the sensors in a concurrent fashion and starting from arbitrary initial states, the algorithm converges to states where collision-free communication among the sensors is restored. Moreover, this algorithm facilitates the transformation of abstract programs into programs in WAC model that are deterministically correct.",
        "published": "2008-08-06T20:33:56Z",
        "link": "http://arxiv.org/abs/0808.0920v1",
        "categories": [
            "cs.OS",
            "cs.DC"
        ]
    },
    {
        "title": "Verification of Peterson's Algorithm for Leader Election in a   Unidirectional Asynchronous Ring Using NuSMV",
        "authors": [
            "Amin Ansari"
        ],
        "summary": "The finite intrinsic nature of the most distributed algorithms gives us this ability to use model checking tools for verification of this type of algorithms. In this paper, I attempt to use NuSMV as a model checking tool for verifying necessary properties of Peterson's algorithm for leader election problem in a unidirectional asynchronous ring topology. Peterson's algorithm for an asynchronous ring supposes that each node in the ring has a unique ID and also a queue for dealing with storage problem. By considering that the queue can have any combination of values, a constructed model for a ring with only four nodes will have more than a billion states. Although it seems that model checking is not a feasible approach for this problem, I attempt to use several effective limiting assumptions for hiring formal model checking approach without losing the correct functionality of the Peterson's algorithm. These enforced limiting assumptions target the degree of freedom in the model checking process and significantly decrease the CPU time, memory usage and the total number of page faults. By deploying these limitations, the number of nodes can be increased from four to eight in the model checking process with NuSMV.",
        "published": "2008-08-07T06:02:59Z",
        "link": "http://arxiv.org/abs/0808.0962v1",
        "categories": [
            "cs.LO",
            "cs.DC",
            "D.2.4; C.2.2"
        ]
    },
    {
        "title": "A General Theory of Computational Scalability Based on Rational   Functions",
        "authors": [
            "Neil J. Gunther"
        ],
        "summary": "The universal scalability law of computational capacity is a rational function C_p = P(p)/Q(p) with P(p) a linear polynomial and Q(p) a second-degree polynomial in the number of physical processors p, that has been long used for statistical modeling and prediction of computer system performance. We prove that C_p is equivalent to the synchronous throughput bound for a machine-repairman with state-dependent service rate. Simpler rational functions, such as Amdahl's law and Gustafson speedup, are corollaries of this queue-theoretic bound. C_p is further shown to be both necessary and sufficient for modeling all practical characteristics of computational scalability.",
        "published": "2008-08-11T00:06:16Z",
        "link": "http://arxiv.org/abs/0808.1431v2",
        "categories": [
            "cs.PF",
            "cs.DC",
            "B.8; C.4; C.5.5; D.4.8; F.1.2"
        ]
    },
    {
        "title": "An Almost-Surely Terminating Polynomial Protocol for Asynchronous   Byzantine Agreement with Optimal Resilience",
        "authors": [
            "Ittai Abraham",
            "Danny Dolev",
            "Joseph Y. Halpern"
        ],
        "summary": "Consider an asynchronous system with private channels and $n$ processes, up to $t$ of which may be faulty. We settle a longstanding open question by providing a Byzantine agreement protocol that simultaneously achieves three properties:   1. (optimal) resilience: it works as long as $n>3t$   2. (almost-sure) termination: with probability one, all nonfaulty processes terminate   3. (polynomial) efficiency: the expected computation time, memory consumption, message size, and number of messages sent are all polynomial in $n$.   Earlier protocols have achieved only two of these three properties. In particular, the protocol of Bracha is not polynomially efficient, the protocol of Feldman and Micali is not optimally resilient, and the protocol of Canetti and Rabin does not have almost-sure termination. Our protocol utilizes a new primitive called shunning (asynchronous) verifiable secret sharing (SVSS), which ensures, roughly speaking, that either a secret is successfully shared or a new faulty process is ignored from this point onwards by some nonfaulty process.",
        "published": "2008-08-11T12:22:12Z",
        "link": "http://arxiv.org/abs/0808.1505v1",
        "categories": [
            "cs.DC",
            "C.2.4; D.4.5; D.4.7; C.4"
        ]
    },
    {
        "title": "Our Brothers' Keepers: Secure Routing with High Performance",
        "authors": [
            "Alex Brodsky",
            "Scott Lindenberg"
        ],
        "summary": "The Trinity (Brodsky et al., 2007) spam classification system is based on a distributed hash table that is implemented using a structured peer-to-peer overlay. Such an overlay must be capable of processing hundreds of messages per second, and must be able to route messages to their destination even in the presence of failures and malicious peers that misroute packets or inject fraudulent routing information into the system. Typically there is tension between the requirements to route messages securely and efficiently in the overlay.   We describe a secure and efficient routing extension that we developed within the I3 (Stoica et al. 2004) implementation of the Chord (Stoica et al. 2001) overlay. Secure routing is accomplished through several complementary approaches: First, peers in close proximity form overlapping groups that police themselves to identify and mitigate fraudulent routing information. Second, a form of random routing solves the problem of entire packet flows passing through a malicious peer. Third, a message authentication mechanism links each message to it sender, preventing spoofing. Fourth, each peer's identifier links the peer to its network address, and at the same time uniformly distributes the peers in the key-space.   Lastly, we present our initial evaluation of the system, comprising a 255 peer overlay running on a local cluster. We describe our methodology and show that the overhead of our secure implementation is quite reasonable.",
        "published": "2008-08-12T22:03:52Z",
        "link": "http://arxiv.org/abs/0808.1744v1",
        "categories": [
            "cs.DC",
            "cs.CR",
            "cs.NI",
            "C.2.4"
        ]
    },
    {
        "title": "Compute and Storage Clouds Using Wide Area High Performance Networks",
        "authors": [
            "Robert L. Grossman",
            "Yunhong Gu",
            "Michael Sabala",
            "Wanzhi Zhang"
        ],
        "summary": "We describe a cloud based infrastructure that we have developed that is optimized for wide area, high performance networks and designed to support data mining applications. The infrastructure consists of a storage cloud called Sector and a compute cloud called Sphere. We describe two applications that we have built using the cloud and some experimental studies.",
        "published": "2008-08-13T09:48:37Z",
        "link": "http://arxiv.org/abs/0808.1802v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Data Mining Using High Performance Data Clouds: Experimental Studies   Using Sector and Sphere",
        "authors": [
            "Robert L Grossman",
            "Yunhong Gu"
        ],
        "summary": "We describe the design and implementation of a high performance cloud that we have used to archive, analyze and mine large distributed data sets. By a cloud, we mean an infrastructure that provides resources and/or services over the Internet. A storage cloud provides storage services, while a compute cloud provides compute services. We describe the design of the Sector storage cloud and how it provides the storage services required by the Sphere compute cloud. We also describe the programming paradigm supported by the Sphere compute cloud. Sector and Sphere are designed for analyzing large data sets using computer clusters connected with wide area high performance networks (for example, 10+ Gb/s). We describe a distributed data mining application that we have developed using Sector and Sphere. Finally, we describe some experimental studies comparing Sector/Sphere to Hadoop.",
        "published": "2008-08-22T01:24:06Z",
        "link": "http://arxiv.org/abs/0808.3019v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Data Diffusion: Dynamic Resource Provision and Data-Aware Scheduling for   Data Intensive Applications",
        "authors": [
            "Ioan Raicu",
            "Yong Zhao",
            "Ian Foster",
            "Alex Szalay"
        ],
        "summary": "Data intensive applications often involve the analysis of large datasets that require large amounts of compute and storage resources. While dedicated compute and/or storage farms offer good task/data throughput, they suffer low resource utilization problem under varying workloads conditions. If we instead move such data to distributed computing resources, then we incur expensive data transfer cost. In this paper, we propose a data diffusion approach that combines dynamic resource provisioning, on-demand data replication and caching, and data locality-aware scheduling to achieve improved resource efficiency under varying workloads. We define an abstract \"data diffusion model\" that takes into consideration the workload characteristics, data accessing cost, application throughput and resource utilization; we validate the model using a real-world large-scale astronomy application. Our results show that data diffusion can increase the performance index by as much as 34X, and improve application response time by over 506X, while achieving near-optimal throughputs and execution times.",
        "published": "2008-08-26T15:19:44Z",
        "link": "http://arxiv.org/abs/0808.3535v1",
        "categories": [
            "cs.DC",
            "C.2.4; D.1.3; D.4.2; D.4.7; H.3.4"
        ]
    },
    {
        "title": "Accelerating Large-scale Data Exploration through Data Diffusion",
        "authors": [
            "Ioan Raicu",
            "Yong Zhao",
            "Ian Foster",
            "Alex Szalay"
        ],
        "summary": "Data-intensive applications often require exploratory analysis of large datasets. If analysis is performed on distributed resources, data locality can be crucial to high throughput and performance. We propose a \"data diffusion\" approach that acquires compute and storage resources dynamically, replicates data in response to demand, and schedules computations close to data. As demand increases, more resources are acquired, thus allowing faster response to subsequent requests that refer to the same data; when demand drops, resources are released. This approach can provide the benefits of dedicated hardware without the associated high costs, depending on workload and resource characteristics. The approach is reminiscent of cooperative caching, web-caching, and peer-to-peer storage systems, but addresses different application demands. Other data-aware scheduling approaches assume dedicated resources, which can be expensive and/or inefficient if load varies significantly. To explore the feasibility of the data diffusion approach, we have extended the Falkon resource provisioning and task scheduling system to support data caching and data-aware scheduling. Performance results from both micro-benchmarks and a large scale astronomy application demonstrate that our approach improves performance relative to alternative approaches, as well as provides improved scalability as aggregated I/O bandwidth scales linearly with the number of data cache nodes.",
        "published": "2008-08-26T16:02:50Z",
        "link": "http://arxiv.org/abs/0808.3546v1",
        "categories": [
            "cs.DC",
            "C.2.4; D.4.2; D.4.7; H.3.4"
        ]
    },
    {
        "title": "Realizing Fast, Scalable and Reliable Scientific Computations in Grid   Environments",
        "authors": [
            "Yong Zhao",
            "Ioan Raicu",
            "Ian Foster",
            "Mihael Hategan",
            "Veronika Nefedova",
            "Mike Wilde"
        ],
        "summary": "The practical realization of managing and executing large scale scientific computations efficiently and reliably is quite challenging. Scientific computations often involve thousands or even millions of tasks operating on large quantities of data, such data are often diversely structured and stored in heterogeneous physical formats, and scientists must specify and run such computations over extended periods on collections of compute, storage and network resources that are heterogeneous, distributed and may change constantly. We present the integration of several advanced systems: Swift, Karajan, and Falkon, to address the challenges in running various large scale scientific applications in Grid environments. Swift is a parallel programming tool for rapid and reliable specification, execution, and management of large-scale science and engineering workflows. Swift consists of a simple scripting language called SwiftScript and a powerful runtime system that is based on the CoG Karajan workflow engine and integrates the Falkon light-weight task execution service that uses multi-level scheduling and a streamlined dispatcher. We showcase the scalability, performance and reliability of the integrated system using application examples drawn from astronomy, cognitive neuroscience and molecular dynamics, which all comprise large number of fine-grained jobs. We show that Swift is able to represent dynamic workflows whose structures can only be determined during runtime and reduce largely the code size of various workflow representations using SwiftScript; schedule the execution of hundreds of thousands of parallel computations via the Karajan engine; and achieve up to 90% reduction in execution time when compared to traditional batch schedulers.",
        "published": "2008-08-26T16:15:42Z",
        "link": "http://arxiv.org/abs/0808.3548v1",
        "categories": [
            "cs.DC",
            "cs.PL",
            "D.1.3; D.4.7"
        ]
    },
    {
        "title": "Scientific Workflow Systems for 21st Century e-Science, New Bottle or   New Wine?",
        "authors": [
            "Yong Zhao",
            "Ioan Raicu",
            "Ian Foster"
        ],
        "summary": "With the advances in e-Sciences and the growing complexity of scientific analyses, more and more scientists and researchers are relying on workflow systems for process coordination, derivation automation, provenance tracking, and bookkeeping. While workflow systems have been in use for decades, it is unclear whether scientific workflows can or even should build on existing workflow technologies, or they require fundamentally new approaches. In this paper, we analyze the status and challenges of scientific workflows, investigate both existing technologies and emerging languages, platforms and systems, and identify the key challenges that must be addressed by workflow systems for e-science in the 21st century.",
        "published": "2008-08-26T16:46:49Z",
        "link": "http://arxiv.org/abs/0808.3545v1",
        "categories": [
            "cs.SE",
            "cs.DC",
            "D.1.3; D.4.7"
        ]
    },
    {
        "title": "Towards Loosely-Coupled Programming on Petascale Systems",
        "authors": [
            "Ioan Raicu",
            "Zhao Zhang",
            "Mike Wilde",
            "Ian Foster",
            "Pete Beckman",
            "Kamil Iskra",
            "Ben Clifford"
        ],
        "summary": "We have extended the Falkon lightweight task execution framework to make loosely coupled programming on petascale systems a practical and useful programming model. This work studies and measures the performance factors involved in applying this approach to enable the use of petascale systems by a broader user community, and with greater ease. Our work enables the execution of highly parallel computations composed of loosely coupled serial jobs with no modifications to the respective applications. This approach allows a new-and potentially far larger-class of applications to leverage petascale systems, such as the IBM Blue Gene/P supercomputer. We present the challenges of I/O performance encountered in making this model practical, and show results using both microbenchmarks and real applications from two domains: economic energy modeling and molecular dynamics. Our benchmarks show that we can scale up to 160K processor-cores with high efficiency, and can achieve sustained execution rates of thousands of tasks per second.",
        "published": "2008-08-26T16:48:14Z",
        "link": "http://arxiv.org/abs/0808.3540v2",
        "categories": [
            "cs.DC",
            "C.2.4; D.1.3; D.4.7; H.3.4"
        ]
    },
    {
        "title": "Enabling Loosely-Coupled Serial Job Execution on the IBM BlueGene/P   Supercomputer and the SiCortex SC5832",
        "authors": [
            "Ioan Raicu",
            "Zhao Zhang",
            "Mike Wilde",
            "Ian Foster"
        ],
        "summary": "Our work addresses the enabling of the execution of highly parallel computations composed of loosely coupled serial jobs with no modifications to the respective applications, on large-scale systems. This approach allows new-and potentially far larger-classes of application to leverage systems such as the IBM Blue Gene/P supercomputer and similar emerging petascale architectures. We present here the challenges of I/O performance encountered in making this model practical, and show results using both micro-benchmarks and real applications on two large-scale systems, the BG/P and the SiCortex SC5832. Our preliminary benchmarks show that we can scale to 4096 processors on the Blue Gene/P and 5832 processors on the SiCortex with high efficiency, and can achieve thousands of tasks/sec sustained execution rates for parallel workloads of ordinary serial applications. We measured applications from two domains, economic energy modeling and molecular dynamics.",
        "published": "2008-08-26T16:59:41Z",
        "link": "http://arxiv.org/abs/0808.3536v1",
        "categories": [
            "cs.DC",
            "C.2.4; D.1.3; D.4.7; H.3.4"
        ]
    },
    {
        "title": "Market-Oriented Cloud Computing: Vision, Hype, and Reality for   Delivering IT Services as Computing Utilities",
        "authors": [
            "Rajkumar Buyya",
            "Chee Shin Yeo",
            "Srikumar Venugopal"
        ],
        "summary": "This keynote paper: presents a 21st century vision of computing; identifies various computing paradigms promising to deliver the vision of computing utilities; defines Cloud computing and provides the architecture for creating market-oriented Clouds by leveraging technologies such as VMs; provides thoughts on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain SLA-oriented resource allocation; presents some representative Cloud platforms especially those developed in industries along with our current work towards realising market-oriented resource allocation of Clouds by leveraging the 3rd generation Aneka enterprise Grid technology; reveals our early thoughts on interconnecting Clouds for dynamically creating an atmospheric computing environment along with pointers to future community research; and concludes with the need for convergence of competing IT paradigms for delivering our 21st century vision.",
        "published": "2008-08-26T17:16:11Z",
        "link": "http://arxiv.org/abs/0808.3558v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Providing Virtual Execution Environments: A Twofold Illustration",
        "authors": [
            "Xavier Grehant",
            "J. M. Dana"
        ],
        "summary": "Platform virtualization helps solving major grid computing challenges: share resource with flexible, user-controlled and custom execution environments and in the meanwhile, isolate failures and malicious code. Grid resource management tools will evolve to embrace support for virtual resource.   We present two open source projects that transparently supply virtual execution environments. Tycoon has been developed at HP Labs to optimise resource usage in creating an economy where users bid to access virtual machines and compete for CPU cycles. SmartDomains provides a peer-to-peer layer that automates virtual machines deployment using a description language and deployment engine from HP Labs. These projects demonstrate both client-server and peer-to-peer approaches to virtual resource management. The first case makes extensive use of virtual machines features for dynamic resource allocation. The second translates virtual machines capabilities into a sophisticated language where resource management components can be plugged in configurations and architectures defined at deployment time.   We propose to share our experience at CERN openlab developing SmartDomains and deploying Tycoon to give an illustrative introduction to emerging research in virtual resource management.",
        "published": "2008-08-27T12:48:39Z",
        "link": "http://arxiv.org/abs/0808.3693v1",
        "categories": [
            "cs.DC",
            "C.0; D.2.9; D.4.7"
        ]
    },
    {
        "title": "Amdahl's and Gustafson-Barsis laws revisited",
        "authors": [
            "Andrzej Karbowski"
        ],
        "summary": "The paper presents a simple derivation of the Gustafson-Barsis law from the Amdahl's law. In the computer literature these two laws describing the speedup limits of parallel applications are derived separately. It is shown, that treating the time of the execution of the sequential part of the application as a constant, in few lines the Gustafson-Barsis law can be obtained from the Amdahl's law and that the popular claim, that Gustafson-Barsis law overthrows Amdahl's law is a mistake.",
        "published": "2008-09-06T15:06:53Z",
        "link": "http://arxiv.org/abs/0809.1177v1",
        "categories": [
            "cs.DC",
            "cs.GT",
            "cs.PF"
        ]
    },
    {
        "title": "Sector and Sphere: Towards Simplified Storage and Processing of Large   Scale Distributed Data",
        "authors": [
            "Yunhong Gu",
            "Robert L Grossman"
        ],
        "summary": "Cloud computing has demonstrated that processing very large datasets over commodity clusters can be done simply given the right programming model and infrastructure. In this paper, we describe the design and implementation of the Sector storage cloud and the Sphere compute cloud. In contrast to existing storage and compute clouds, Sector can manage data not only within a data center, but also across geographically distributed data centers. Similarly, the Sphere compute cloud supports User Defined Functions (UDF) over data both within a data center and across data centers. As a special case, MapReduce style programming can be implemented in Sphere by using a Map UDF followed by a Reduce UDF. We describe some experimental studies comparing Sector/Sphere and Hadoop using the Terasort Benchmark. In these studies, Sector is about twice as fast as Hadoop. Sector/Sphere is open source.",
        "published": "2008-09-06T18:37:51Z",
        "link": "http://arxiv.org/abs/0809.1181v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "An optimal local approximation algorithm for max-min linear programs",
        "authors": [
            "Patrik Floréen",
            "Joel Kaasinen",
            "Petteri Kaski",
            "Jukka Suomela"
        ],
        "summary": "We present a local algorithm (constant-time distributed algorithm) for approximating max-min LPs. The objective is to maximise $\\omega$ subject to $Ax \\le 1$, $Cx \\ge \\omega 1$, and $x \\ge 0$ for nonnegative matrices $A$ and $C$. The approximation ratio of our algorithm is the best possible for any local algorithm; there is a matching unconditional lower bound.",
        "published": "2008-09-09T06:11:31Z",
        "link": "http://arxiv.org/abs/0809.1489v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Randomized Distributed Configuration Management of Wireless Networks:   Multi-layer Markov Random Fields and Near-Optimality",
        "authors": [
            "Sung-eok Jeon",
            "Chunayi Ji"
        ],
        "summary": "Distributed configuration management is imperative for wireless infrastructureless networks where each node adjusts locally its physical and logical configuration through information exchange with neighbors. Two issues remain open. The first is the optimality. The second is the complexity. We study these issues through modeling, analysis, and randomized distributed algorithms. Modeling defines the optimality. We first derive a global probabilistic model for a network configuration which characterizes jointly the statistical spatial dependence of a physical- and a logical-configuration. We then show that a local model which approximates the global model is a two-layer Markov Random Field or a random bond model. The complexity of the local model is the communication range among nodes. The local model is near-optimal when the approximation error to the global model is within a given error bound. We analyze the trade-off between an approximation error and complexity, and derive sufficient conditions on the near-optimality of the local model. We validate the model, the analysis and the randomized distributed algorithms also through simulation.",
        "published": "2008-09-11T05:00:51Z",
        "link": "http://arxiv.org/abs/0809.1916v1",
        "categories": [
            "cs.DC",
            "cs.AI"
        ]
    },
    {
        "title": "Getting in the Zone for Successful Scalability",
        "authors": [
            "Jim Holtman",
            "Neil J. Gunther"
        ],
        "summary": "The universal scalability law (USL) is an analytic model used to quantify application scaling. It is universal because it subsumes Amdahl's law and Gustafson linearized scaling as special cases. Using simulation, we show: (i) that the USL is equivalent to synchronous queueing in a load-dependent machine repairman model and (ii) how USL, Amdahl's law, and Gustafson scaling can be regarded as boundaries defining three scalability zones. Typical throughput measurements lie across all three zones. Simulation scenarios provide deeper insight into queueing effects and thus provide a clearer indication of which application features should be tuned to get into the optimal performance zone.",
        "published": "2008-09-15T14:51:04Z",
        "link": "http://arxiv.org/abs/0809.2541v1",
        "categories": [
            "cs.PF",
            "cs.DC",
            "B.8; C.4; C.5.5; D.4.8; F.1.2; G.3"
        ]
    },
    {
        "title": "SWIM: A Simple Model to Generate Small Mobile Worlds",
        "authors": [
            "Alessandro Mei",
            "Julinda Stefa"
        ],
        "summary": "This paper presents small world in motion (SWIM), a new mobility model for ad-hoc networking. SWIM is relatively simple, is easily tuned by setting just a few parameters, and generates traces that look real--synthetic traces have the same statistical properties of real traces. SWIM shows experimentally and theoretically the presence of the power law and exponential decay dichotomy of inter-contact time, and, most importantly, our experiments show that it can predict very accurately the performance of forwarding protocols.",
        "published": "2008-09-16T15:07:58Z",
        "link": "http://arxiv.org/abs/0809.2730v2",
        "categories": [
            "cs.DC",
            "cs.NI",
            "C.2; C.2.1; C.2.2; C.4"
        ]
    },
    {
        "title": "A High Performance Memory Database for Web Application Caches",
        "authors": [
            "Ivan Voras",
            "Danko Basch",
            "Mario Zagar"
        ],
        "summary": "This paper presents the architecture and characteristics of a memory database intended to be used as a cache engine for web applications. Primary goals of this database are speed and efficiency while running on SMP systems with several CPU cores (four and more). A secondary goal is the support for simple metadata structures associated with cached data that can aid in efficient use of the cache. Due to these goals, some data structures and algorithms normally associated with this field of computing needed to be adapted to the new environment.",
        "published": "2008-09-21T01:05:46Z",
        "link": "http://arxiv.org/abs/0809.3542v1",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "Modelling interdependencies between the electricity and information   infrastructures",
        "authors": [
            "Jean-Claude Laprie",
            "Karama Kanoun",
            "Mohamed Kaaniche"
        ],
        "summary": "The aim of this paper is to provide qualitative models characterizing interdependencies related failures of two critical infrastructures: the electricity infrastructure and the associated information infrastructure. The interdependencies of these two infrastructures are increasing due to a growing connection of the power grid networks to the global information infrastructure, as a consequence of market deregulation and opening. These interdependencies increase the risk of failures. We focus on cascading, escalating and common-cause failures, which correspond to the main causes of failures due to interdependencies. We address failures in the electricity infrastructure, in combination with accidental failures in the information infrastructure, then we show briefly how malicious attacks in the information infrastructure can be addressed.",
        "published": "2008-09-24T07:26:09Z",
        "link": "http://arxiv.org/abs/0809.4107v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Content Sharing for Mobile Devices",
        "authors": [
            "Rudi Ball"
        ],
        "summary": "The miniaturisation of computing devices has seen computing devices become increasingly pervasive in society. With this increased pervasiveness, the technologies of small computing devices have also improved. Mobile devices are now capable of capturing various forms of multimedia and able to communicate wirelessly using increasing numbers of communication techniques. The owners and creators of local content are motivated to share this content in ever increasing volume; the conclusion has been that social networks sites are seeing a revolution in the sharing of information between communities of people. As load on centralised systems increases, we present a novel decentralised peer-to-peer approach dubbed the Market Contact Protocol (MCP) to achieve cost effective, scalable and efficient content sharing using opportunistic networking (pocket switched networking), incentive, context-awareness, social contact and mobile devices. Within the report we describe how the MCP is simulated with a superimposed geographic framework on top of the JiST (Java in Simulation Time) framework to evaluate and measure its capability to share content between massively mobile peers. The MCP is shown in conclusion to be a powerful means by which to share content in a massively mobile ad-hoc environment.",
        "published": "2008-09-25T12:58:40Z",
        "link": "http://arxiv.org/abs/0809.4395v1",
        "categories": [
            "cs.DC",
            "cs.NI",
            "C.2.4"
        ]
    },
    {
        "title": "An Analytical Model of Information Dissemination for a Gossip-based   Protocol",
        "authors": [
            "Rena Bakhshi",
            "Daniela Gavidia",
            "Wan Fokkink",
            "Maarten van Steen"
        ],
        "summary": "We develop an analytical model of information dissemination for a gossiping protocol that combines both pull and push approaches. With this model we analyse how fast an item is replicated through a network, and how fast the item spreads in the network, and how fast the item covers the network. We also determine the optimal size of the exchange buffer, to obtain fast replication. Our results are confirmed by large-scale simulation experiments.",
        "published": "2008-10-09T04:42:27Z",
        "link": "http://arxiv.org/abs/0810.1571v1",
        "categories": [
            "cs.DC",
            "cs.DM",
            "cs.IT",
            "cs.PF",
            "math.IT"
        ]
    },
    {
        "title": "Peer-to-Peer Secure Multi-Party Numerical Computation",
        "authors": [
            "Danny Bickson",
            "Genia Bezman",
            "Danny Dolev",
            "Benny Pinkas"
        ],
        "summary": "We propose an efficient framework for enabling secure multi-party numerical computations in a Peer-to-Peer network. This problem arises in a range of applications such as collaborative filtering, distributed computation of trust and reputation, monitoring and numerous other tasks, where the computing nodes would like to preserve the privacy of their inputs while performing a joint computation of a certain function.   Although there is a rich literature in the field of distributed systems security concerning secure multi-party computation, in practice it is hard to deploy those methods in very large scale Peer-to-Peer networks. In this work, we examine several possible approaches and discuss their feasibility. Among the possible approaches, we identify a single approach which is both scalable and theoretically secure.   An additional novel contribution is that we show how to compute the neighborhood based collaborative filtering, a state-of-the-art collaborative filtering algorithm, winner of the Netflix progress prize of the year 2007. Our solution computes this algorithm in a Peer-to-Peer network, using a privacy preserving computation, without loss of accuracy.   Using extensive large scale simulations on top of real Internet topologies, we demonstrate the applicability of our approach. As far as we know, we are the first to implement such a large scale secure multi-party simulation of networks of millions of nodes and hundreds of millions of edges.",
        "published": "2008-10-09T13:07:36Z",
        "link": "http://arxiv.org/abs/0810.1624v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "A Model for Communication in Clusters of Multi-core Machines",
        "authors": [
            "Christine Task",
            "Arun Chauhan"
        ],
        "summary": "A common paradigm for scientific computing is distributed message-passing systems, and a common approach to these systems is to implement them across clusters of high-performance workstations. As multi-core architectures become increasingly mainstream, these clusters are very likely to include multi-core machines. However, the theoretical models which are currently used to develop communication algorithms across these systems do not take into account the unique properties of processes running on shared-memory architectures, including shared external network connections and communication via shared memory locations. Because of this, existing algorithms are far from optimal for modern clusters. Additionally, recent attempts to adapt these algorithms to multicore systems have proceeded without the introduction of a more accurate formal model and have generally neglected to capitalize on the full power these systems offer. We propose a new model which simply and effectively captures the strengths of multi-core machines in collective communications patterns and suggest how it could be used to properly optimize these patterns.",
        "published": "2008-10-13T04:04:42Z",
        "link": "http://arxiv.org/abs/0810.2150v2",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "A simple local 3-approximation algorithm for vertex cover",
        "authors": [
            "Valentin Polishchuk",
            "Jukka Suomela"
        ],
        "summary": "We present a local algorithm (constant-time distributed algorithm) for finding a 3-approximate vertex cover in bounded-degree graphs. The algorithm is deterministic, and no auxiliary information besides port numbering is required.",
        "published": "2008-10-13T12:45:15Z",
        "link": "http://arxiv.org/abs/0810.2175v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Enabling Lock-Free Concurrent Fine-Grain Access to Massive Distributed   Data: Application to Supernovae Detection",
        "authors": [
            "Bogdan Nicolae",
            "Gabriel Antoniu",
            "Luc Bougé"
        ],
        "summary": "We consider the problem of efficiently managing massive data in a large-scale distributed environment. We consider data strings of size in the order of Terabytes, shared and accessed by concurrent clients. On each individual access, a segment of a string, of the order of Megabytes, is read or modified. Our goal is to provide the clients with efficient fine-grain access the data string as concurrently as possible, without locking the string itself. This issue is crucial in the context of applications in the field of astronomy, databases, data mining and multimedia. We illustrate these requiremens with the case of an application for searching supernovae. Our solution relies on distributed, RAM-based data storage, while leveraging a DHT-based, parallel metadata management scheme. The proposed architecture and algorithms have been validated through a software prototype and evaluated in a cluster environment.",
        "published": "2008-10-13T13:07:18Z",
        "link": "http://arxiv.org/abs/0810.2226v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Distributed Management of Massive Data: an Efficient Fine-Grain Data   Access Scheme",
        "authors": [
            "Bogdan Nicolae",
            "Gabriel Antoniu",
            "Luc Bougé"
        ],
        "summary": "This paper addresses the problem of efficiently storing and accessing massive data blocks in a large-scale distributed environment, while providing efficient fine-grain access to data subsets. This issue is crucial in the context of applications in the field of databases, data mining and multimedia. We propose a data sharing service based on distributed, RAM-based storage of data, while leveraging a DHT-based, natively parallel metadata management scheme. As opposed to the most commonly used grid storage infrastructures that provide mechanisms for explicit data localization and transfer, we provide a transparent access model, where data are accessed through global identifiers. Our proposal has been validated through a prototype implementation whose preliminary evaluation provides promising results.",
        "published": "2008-10-13T13:08:14Z",
        "link": "http://arxiv.org/abs/0810.2227v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "The Impact of Mobility on Gossip Algorithms",
        "authors": [
            "Anand D. Sarwate",
            "Alexandros G. Dimakis"
        ],
        "summary": "The influence of node mobility on the convergence time of averaging gossip algorithms in networks is studied. It is shown that a small number of fully mobile nodes can yield a significant decrease in convergence time. A method is developed for deriving lower bounds on the convergence time by merging nodes according to their mobility pattern. This method is used to show that if the agents have one-dimensional mobility in the same direction the convergence time is improved by at most a constant. Upper bounds are obtained on the convergence time using techniques from the theory of Markov chains and show that simple models of mobility can dramatically accelerate gossip as long as the mobility paths significantly overlap. Simulations verify that different mobility patterns can have significantly different effects on the convergence of distributed algorithms.",
        "published": "2008-10-14T18:24:52Z",
        "link": "http://arxiv.org/abs/0810.2513v3",
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Distributed Platform for Mechanism Design",
        "authors": [
            "Krzysztof R. Apt",
            "Farhad Arbab",
            "Huiye Ma"
        ],
        "summary": "We describe a structured system for distributed mechanism design. It consists of a sequence of layers. The lower layers deal with the operations relevant for distributed computing only, while the upper layers are concerned only with communication among players, including broadcasting and multicasting, and distributed decision making. This yields a highly flexible distributed system whose specific applications are realized as instances of its top layer.   This design supports fault-tolerance, prevents manipulations and makes it possible to implement distributed policing. The system is implemented in Java. We illustrate it by discussing a number of implemented examples.",
        "published": "2008-10-17T17:19:54Z",
        "link": "http://arxiv.org/abs/0810.3199v1",
        "categories": [
            "cs.GT",
            "cs.DC",
            "C.2.4; J.4"
        ]
    },
    {
        "title": "Dynamic Approaches to In-Network Aggregation",
        "authors": [
            "Oliver Kennedy",
            "Christoph Koch",
            "Al Demers"
        ],
        "summary": "Collaboration between small-scale wireless devices hinges on their ability to infer properties shared across multiple nearby nodes. Wireless-enabled mobile devices in particular create a highly dynamic environment not conducive to distributed reasoning about such global properties. This paper addresses a specific instance of this problem: distributed aggregation. We present extensions to existing unstructured aggregation protocols that enable estimation of count, sum, and average aggregates in highly dynamic environments. With the modified protocols, devices with only limited connectivity can maintain estimates of the aggregate, despite \\textit{unexpected} peer departures and arrivals. Our analysis of these aggregate maintenance extensions demonstrates their effectiveness in unstructured environments despite high levels of node mobility.",
        "published": "2008-10-17T19:48:38Z",
        "link": "http://arxiv.org/abs/0810.3227v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Grid Computing in the Collider Detector at Fermilab (CDF) scientific   experiment",
        "authors": [
            "Douglas P. Benjamin"
        ],
        "summary": "The computing model for the Collider Detector at Fermilab (CDF) scientific experiment has evolved since the beginning of the experiment. Initially CDF computing was comprised of dedicated resources located in computer farms around the world. With the wide spread acceptance of grid computing in High Energy Physics, CDF computing has migrated to using grid computing extensively. CDF uses computing grids around the world. Each computing grid has required different solutions. The use of portals as interfaces to the collaboration computing resources has proven to be an extremely useful technique allowing the CDF physicists transparently migrate from using dedicated computer farm to using computing located in grid farms often away from Fermilab. Grid computing at CDF continues to evolve as the grid standards and practices change.",
        "published": "2008-10-20T02:19:06Z",
        "link": "http://arxiv.org/abs/0810.3453v1",
        "categories": [
            "cs.DC",
            "hep-ex",
            "physics.data-an"
        ]
    },
    {
        "title": "Experimental Study of Application Specific Source Coding for Wireless   Sensor Networks",
        "authors": [
            "Muthiah Annamalai",
            "Darshan Shrestha",
            "Saibun Tjuatja"
        ],
        "summary": "The energy bottleneck in Wireless Sensor Network(WSN) can be reduced by limiting communication overhead. Application specific source coding schemes for the sensor networks provide fewer bits to represent the same amount of information exploiting the redundancy present in the source model, network architecture and the physical process. This paper reports the performance of representative codes from various families of source coding schemes (lossless, lossy, constant bit-rate, variable bit-rate, distributed and joint encoding/decoding) in terms of energy consumed, bit-rate achieved, quantization-error/reconstruction-error, latency and complexity of encoder-decoder(codec). A reusable frame work for testing source codes is provided. Finally we propose a set of possible applications and suitable source codes in terms of these parameters.",
        "published": "2008-10-20T18:34:22Z",
        "link": "http://arxiv.org/abs/0810.3626v2",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "Distributed Estimation over Wireless Sensor Networks with Packet Losses",
        "authors": [
            "Carlo Fischione",
            "Alberto Speranzon",
            "Karl H. Johansson",
            "Alberto Sangiovanni-Vincentelli"
        ],
        "summary": "A distributed adaptive algorithm to estimate a time-varying signal, measured by a wireless sensor network, is designed and analyzed. One of the major features of the algorithm is that no central coordination among the nodes needs to be assumed. The measurements taken by the nodes of the network are affected by noise, and the communication among the nodes is subject to packet losses. Nodes exchange local estimates and measurements with neighboring nodes. Each node of the network locally computes adaptive weights that minimize the estimation error variance. Decentralized conditions on the weights, needed for the convergence of the estimation error throughout the overall network, are presented. A Lipschitz optimization problem is posed to guarantee stability and the minimization of the variance. An efficient strategy to distribute the computation of the optimal solution is investigated. A theoretical performance analysis of the distributed algorithm is carried out both in the presence of perfect and lossy links. Numerical simulations illustrate performance for various network topologies and packet loss probabilities.",
        "published": "2008-10-21T01:00:26Z",
        "link": "http://arxiv.org/abs/0810.3715v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Directed Transmission Method, A Fully Asynchronous approach to Solve   Sparse Linear Systems in Parallel",
        "authors": [
            "Fei Wei",
            "Huazhong Yang"
        ],
        "summary": "In this paper, we propose a new distributed algorithm, called Directed Transmission Method (DTM). DTM is a fully asynchronous and continuous-time iterative algorithm to solve SPD sparse linear system. As an architecture-aware algorithm, DTM could be freely running on all kinds of heterogeneous parallel computer. We proved that DTM is convergent by making use of the final-value theorem of Laplacian Transformation. Numerical experiments show that DTM is stable and efficient.",
        "published": "2008-10-21T09:37:55Z",
        "link": "http://arxiv.org/abs/0810.3783v5",
        "categories": [
            "math.NA",
            "cs.DC",
            "65F10, 65F50, 68M14"
        ]
    },
    {
        "title": "Best-effort Group Service in Dynamic Networks",
        "authors": [
            "Bertrand Ducourthial",
            "Sofiane Khalfallah",
            "Franck Petit"
        ],
        "summary": "We propose a group membership service for dynamic ad hoc networks. It maintains as long as possible the existing groups and ensures that each group diameter is always smaller than a constant, fixed according to the application using the groups. The proposed protocol is self-stabilizing and works in dynamic distributed systems. Moreover, it ensures a kind of continuity in the service offer to the application while the system is converging, except if too strong topology changes happen. Such a best effort behavior allows applications to rely on the groups while the stabilization has not been reached, which is very useful in dynamic ad hoc networks.",
        "published": "2008-10-21T13:58:50Z",
        "link": "http://arxiv.org/abs/0810.3836v3",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Randomization Adaptive Self-Stabilization",
        "authors": [
            "Shlomi Dolev",
            "Nir Tzachar"
        ],
        "summary": "We present a scheme to convert self-stabilizing algorithms that use randomization during and following convergence to self-stabilizing algorithms that use randomization only during convergence. We thus reduce the number of random bits from an infinite number to a bounded number. The scheme is applicable to the cases in which there exits a local predicate for each node, such that global consistency is implied by the union of the local predicates. We demonstrate our scheme over the token circulation algorithm of Herman and the recent constant time Byzantine self-stabilizing clock synchronization algorithm by Ben-Or, Dolev and Hoch. The application of our scheme results in the first constant time Byzantine self-stabilizing clock synchronization algorithm that uses a bounded number of random bits.",
        "published": "2008-10-24T12:15:40Z",
        "link": "http://arxiv.org/abs/0810.4440v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "The Mob core language and abstract machine (rev 0.2)",
        "authors": [
            "Herve Paulino",
            "Luis Lopes"
        ],
        "summary": "Most current mobile agent systems are based on programming languages whose semantics are difficult to prove correct as they lack an adequate underlying formal theory. In recent years, the development of the theory of concurrent systems, namely of process calculi, has allowed for the first time the modeling of mobile agent systems.Languages directly based on process calculi are, however, very low-level and it is desirable to provide the programmer with higher level abstractions, while keeping the semantics of the base calculus.   In this technical report we present the syntax and the semantics of a scripting language for programming mobile agents called Mob. We describe the language's syntax and semantics. Mob is service-oriented, meaning that agents act both as servers and as clients of services and that this coupling is done dynamically at run-time. The language is implemented on top of a process calculus which allows us to prove that the framework is sound by encoding its semantics into the underlying calculus. This provides a form of language security not available to other mobile agent languages developed using a more ah-doc approach.",
        "published": "2008-10-24T15:02:09Z",
        "link": "http://arxiv.org/abs/0810.4451v1",
        "categories": [
            "cs.PL",
            "cs.DC"
        ]
    },
    {
        "title": "The Multi-Core Era - Trends and Challenges",
        "authors": [
            "Peter Tröger"
        ],
        "summary": "Since the very beginning of hardware development, computer processors were invented with ever-increasing clock frequencies and sophisticated in-build optimization strategies. Due to physical limitations, this 'free lunch' of speedup has come to an end.   The following article gives a summary and bibliography for recent trends and challenges in CMP architectures. It discusses how 40 years of parallel computing research need to be considered in the upcoming multi-core era. We argue that future research must be driven from two sides - a better expression of hardware structures, and a domain-specific understanding of software parallelism.",
        "published": "2008-10-30T08:34:48Z",
        "link": "http://arxiv.org/abs/0810.5439v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Programming languages with algorithmically parallelizing problem",
        "authors": [
            "R. Nuriyev"
        ],
        "summary": "The study consists of two parts. Objective of the first part is modern language constructions responsible for algorithmically insolvability of parallelizing problem. Second part contains several ways to modify the constructions to make the problem algorithmically solvable",
        "published": "2008-10-31T00:25:15Z",
        "link": "http://arxiv.org/abs/0810.5596v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Practical language based on systems of definitions",
        "authors": [
            "R. Nuriyev"
        ],
        "summary": "The article suggests a description of a system of tables with a set of special lists absorbing a semantics of data and reflects a fullness of data. It shows how their parallel processing can be constructed based on the descriptions. The approach also might be used for definition intermediate targets for data mining and unstructured data processing.",
        "published": "2008-10-31T16:42:45Z",
        "link": "http://arxiv.org/abs/0810.5732v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Non procedural language for parallel programs",
        "authors": [
            "Renat Nuriyev"
        ],
        "summary": "Probably building non procedural languages is the most prospective way for parallel programming just because non procedural means no fixed way for execution. The article consists of 3 parts. In first part we consider formal systems for definition a named datasets and studying an expression power of different subclasses. In the second part we consider a complexity of algorithms of building sets by the definitions. In third part we consider a fullness and flexibility of the class of program based data set definitions.",
        "published": "2008-10-31T18:44:38Z",
        "link": "http://arxiv.org/abs/0810.5758v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Optimization of automatically generated multi-core code for the LTE   RACH-PD algorithm",
        "authors": [
            "Maxime Pelcat",
            "Slaheddine Aridhi",
            "Jean François Nezan"
        ],
        "summary": "Embedded real-time applications in communication systems require high processing power. Manual scheduling devel-oped for single-processor applications is not suited to multi-core architectures. The Algorithm Architecture Matching (AAM) methodology optimizes static application implementation on multi-core architectures. The Random Access Channel Preamble Detection (RACH-PD) is an algorithm for non-synchronized access of Long Term Evolu-tion (LTE) wireless networks. LTE aims to improve the spectral efficiency of the next generation cellular system. This paper de-scribes a complete methodology for implementing the RACH-PD. AAM prototyping is applied to the RACH-PD which is modelled as a Synchronous DataFlow graph (SDF). An efficient implemen-tation of the algorithm onto a multi-core DSP, the TI C6487, is then explained. Benchmarks for the solution are given.",
        "published": "2008-11-04T19:36:16Z",
        "link": "http://arxiv.org/abs/0811.0582v1",
        "categories": [
            "cs.MM",
            "cs.DC"
        ]
    },
    {
        "title": "NB-FEB: An Easy-to-Use and Scalable Universal Synchronization Primitive   for Parallel Programming",
        "authors": [
            "Phuong Hoai Ha",
            "Philippas Tsigas",
            "Otto J. Anshus"
        ],
        "summary": "This paper addresses the problem of universal synchronization primitives that can support scalable thread synchronization for large-scale many-core architectures. The universal synchronization primitives that have been deployed widely in conventional architectures like CAS and LL/SC are expected to reach their scalability limits in the evolution to many-core architectures with thousands of cores. We introduce a non-blocking full/empty bit primitive, or NB-FEB for short, as a promising synchronization primitive for parallel programming on may-core architectures. We show that the NB-FEB primitive is universal, scalable, feasible and convenient to use. NB-FEB, together with registers, can solve the consensus problem for an arbitrary number of processes (universality). NB-FEB is combinable, namely its memory requests to the same memory location can be combined into only one memory request, which consequently mitigates performance degradation due to synchronization \"hot spots\" (scalability). Since NB-FEB is a variant of the original full/empty bit that always returns a value instead of waiting for a conditional flag, it is as feasible as the original full/empty bit, which has been implemented in many computer systems (feasibility). The original full/empty bit is well-known as a special-purpose primitive for fast producer-consumer synchronization and has been used extensively in the specific domain of applications. In this paper, we show that NB-FEB can be deployed easily as a general-purpose primitive. Using NB-FEB, we construct a non-blocking software transactional memory system called NBFEB-STM, which can be used to handle concurrent threads conveniently. NBFEB-STM is space efficient: the space complexity of each object updated by $N$ concurrent threads/transactions is $\\Theta(N)$, the optimal.",
        "published": "2008-11-09T00:41:07Z",
        "link": "http://arxiv.org/abs/0811.1304v1",
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.DS"
        ]
    },
    {
        "title": "Distributed Algorithms for Computing Alternate Paths Avoiding Failed   Nodes and Links",
        "authors": [
            "Amit M. Bhosle",
            "Teofilo F. Gonzalez"
        ],
        "summary": "A recent study characterizing failures in computer networks shows that transient single element (node/link) failures are the dominant failures in large communication networks like the Internet. Thus, having the routing paths globally recomputed on a failure does not pay off since the failed element recovers fairly quickly, and the recomputed routing paths need to be discarded. In this paper, we present the first distributed algorithm that computes the alternate paths required by some \"proactive recovery schemes\" for handling transient failures. Our algorithm computes paths that avoid a failed node, and provides an alternate path to a particular destination from an upstream neighbor of the failed node. With minor modifications, we can have the algorithm compute alternate paths that avoid a failed link as well. To the best of our knowledge all previous algorithms proposed for computing alternate paths are centralized, and need complete information of the network graph as input to the algorithm.",
        "published": "2008-11-09T03:34:39Z",
        "link": "http://arxiv.org/abs/0811.1301v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.NI"
        ]
    },
    {
        "title": "Parallel execution of portfolio optimization",
        "authors": [
            "R. Nuriyev"
        ],
        "summary": "Analysis of asset liability management (ALM) strategies especially for long term horizon is a crucial issue for banks, funds and insurance companies.   Modern economic models, investment strategies and optimization criteria make ALM studies computationally very intensive task. It attracts attention to multiprocessor system and especially to the cheapest one: multi core PCs and PC clusters.   In this article we are analyzing problem of parallel organization of portfolio optimization, results of using clusters for optimization and the most efficient cluster architecture for these kinds of tasks.",
        "published": "2008-11-10T15:52:25Z",
        "link": "http://arxiv.org/abs/0811.1504v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Decentralized Overlay for Federation of Enterprise Clouds",
        "authors": [
            "Rajiv Ranjan",
            "Rajkumar Buyya"
        ],
        "summary": "This chapter describes Aneka-Federation, a decentralized and distributed system that combines enterprise Clouds, overlay networking, and structured peer-to-peer techniques to create scalable wide-area networking of compute nodes for high-throughput computing. The Aneka-Federation integrates numerous small scale Aneka Enterprise Cloud services and nodes that are distributed over multiple control and enterprise domains as parts of a single coordinated resource leasing abstraction. The system is designed with the aim of making distributed enterprise Cloud resource integration and application programming flexible, efficient, and scalable. The system is engineered such that it: enables seamless integration of existing Aneka Enterprise Clouds as part of single wide-area resource leasing federation; self-organizes the system components based on a structured peer-to-peer routing methodology; and presents end-users with a distributed application composition environment that can support variety of programming and execution models. This chapter describes the design and implementation of a novel, extensible and decentralized peer-to-peer technique that helps to discover, connect and provision the services of Aneka Enterprise Clouds among the users who can use different programming models to compose their applications. Evaluations of the system with applications that are programmed using the Task and Thread execution models on top of an overlay of Aneka Enterprise Clouds have been described here.",
        "published": "2008-11-16T10:58:25Z",
        "link": "http://arxiv.org/abs/0811.2563v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Self-stabilizing Numerical Iterative Computation",
        "authors": [
            "Ezra N. Hoch",
            "Danny Bickson",
            "Danny Dolev"
        ],
        "summary": "Many challenging tasks in sensor networks, including sensor calibration, ranking of nodes, monitoring, event region detection, collaborative filtering, collaborative signal processing, {\\em etc.}, can be formulated as a problem of solving a linear system of equations. Several recent works propose different distributed algorithms for solving these problems, usually by using linear iterative numerical methods.   In this work, we extend the settings of the above approaches, by adding another dimension to the problem. Specifically, we are interested in {\\em self-stabilizing} algorithms, that continuously run and converge to a solution from any initial state. This aspect of the problem is highly important due to the dynamic nature of the network and the frequent changes in the measured environment.   In this paper, we link together algorithms from two different domains. On the one hand, we use the rich linear algebra literature of linear iterative methods for solving systems of linear equations, which are naturally distributed with rapid convergence properties. On the other hand, we are interested in self-stabilizing algorithms, where the input to the computation is constantly changing, and we would like the algorithms to converge from any initial state. We propose a simple novel method called \\syncAlg as a self-stabilizing variant of the linear iterative methods. We prove that under mild conditions the self-stabilizing algorithm converges to a desired result. We further extend these results to handle the asynchronous case.   As a case study, we discuss the sensor calibration problem and provide simulation results to support the applicability of our approach.",
        "published": "2008-11-19T19:11:46Z",
        "link": "http://arxiv.org/abs/0811.3176v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "SINR Diagrams: Towards Algorithmically Usable SINR Models of Wireless   Networks",
        "authors": [
            "Chen Avin",
            "Yuval Emek",
            "Erez Kantor",
            "Zvi Lotker",
            "David Peleg",
            "Liam Roditty"
        ],
        "summary": "The rules governing the availability and quality of connections in a wireless network are described by physical models such as the signal-to-interference & noise ratio (SINR) model. For a collection of simultaneously transmitting stations in the plane, it is possible to identify a reception zone for each station, consisting of the points where its transmission is received correctly. The resulting SINR diagram partitions the plane into a reception zone per station and the remaining plane where no station can be heard.   SINR diagrams appear to be fundamental to understanding the behavior of wireless networks, and may play a key role in the development of suitable algorithms for such networks, analogous perhaps to the role played by Voronoi diagrams in the study of proximity queries and related issues in computational geometry. So far, however, the properties of SINR diagrams have not been studied systematically, and most algorithmic studies in wireless networking rely on simplified graph-based models such as the unit disk graph (UDG) model, which conveniently abstract away interference-related complications, and make it easier to handle algorithmic issues, but consequently fail to capture accurately some important aspects of wireless networks.   The current paper focuses on obtaining some basic understanding of SINR diagrams, their properties and their usability in algorithmic applications. Specifically, based on some algebraic properties of the polynomials defining the reception zones we show that assuming uniform power transmissions, the reception zones are convex and relatively well-rounded. These results are then used to develop an efficient approximation algorithm for a fundamental point location problem in wireless networks.",
        "published": "2008-11-20T08:49:48Z",
        "link": "http://arxiv.org/abs/0811.3284v2",
        "categories": [
            "cs.NI",
            "cs.DC",
            "C.2.1; F.2.2; G.2.2"
        ]
    },
    {
        "title": "Communication Efficiency in Self-stabilizing Silent Protocols",
        "authors": [
            "Stéphane Devismes",
            "Toshimitsu Masuzawa",
            "Sébastien Tixeuil"
        ],
        "summary": "Self-stabilization is a general paradigm to provide forward recovery capabilities to distributed systems and networks. Intuitively, a protocol is self-stabilizing if it is able to recover without external intervention from any catastrophic transient failure. In this paper, our focus is to lower the communication complexity of self-stabilizing protocols \\emph{below} the need of checking every neighbor forever. In more details, the contribution of the paper is threefold: (i) We provide new complexity measures for communication efficiency of self-stabilizing protocols, especially in the stabilized phase or when there are no faults, (ii) On the negative side, we show that for non-trivial problems such as coloring, maximal matching, and maximal independent set, it is impossible to get (deterministic or probabilistic) self-stabilizing solutions where every participant communicates with less than every neighbor in the stabilized phase, and (iii) On the positive side, we present protocols for coloring, maximal matching, and maximal independent set such that a fraction of the participants communicates with exactly one neighbor in the stabilized phase.",
        "published": "2008-11-23T17:29:25Z",
        "link": "http://arxiv.org/abs/0811.3760v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Fully distributed and fault tolerant task management based on diffusions",
        "authors": [
            "Alain Bui",
            "Olivier Flauzac",
            "Cyril Rabat"
        ],
        "summary": "The task management is a critical component for the computational grids. The aim is to assign tasks on nodes according to a global scheduling policy and a view of local resources of nodes. A peer-to-peer approach for the task management involves a better scalability for the grid and a higher fault tolerance. But some mechanisms have to be proposed to avoid the computation of replicated tasks that can reduce the efficiency and increase the load of nodes. In the same way, these mechanisms have to limit the number of exchanged messages to avoid the overload of the network.   In a previous paper, we have proposed two methods for the task management called active and passive. These methods are based on a random walk: they are fully distributed and fault tolerant. Each node owns a local tasks states set updated thanks to a random walk and each node is in charge of the local assignment. Here, we propose three methods to improve the efficiency of the active method. These new methods are based on a circulating word. The nodes local tasks states sets are updated thanks to periodical diffusions along trees built from the circulating word. Particularly, we show that these methods increase the efficiency of the active method: they produce less replicated tasks. These three methods are also fully distributed and fault tolerant. On the other way, the circulating word can be exploited for other applications like the resources management or the nodes synchronization.",
        "published": "2008-12-03T14:58:19Z",
        "link": "http://arxiv.org/abs/0812.0736v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Distributed (Delta + 1)-coloring in linear (in Delta) time",
        "authors": [
            "Leonid Barenboim",
            "Michael Elkin"
        ],
        "summary": "The distributed (Delta + 1)-coloring problem is one of most fundamental and well-studied problems of Distributed Algorithms. Starting with the work of Cole and Vishkin in 86, there was a long line of gradually improving algorithms published. The current state-of-the-art running time is O(Delta log Delta + log^* n), due to Kuhn and Wattenhofer, PODC'06. Linial (FOCS'87) has proved a lower bound of 1/2 \\log^* n for the problem, and Szegedy and Vishwanathan (STOC'93) provided a heuristic argument that shows that algorithms from a wide family of locally iterative algorithms are unlikely to achieve running time smaller than \\Theta(Delta log Delta).   We present a deterministic (Delta + 1)-coloring distributed algorithm with running time O(Delta) + 1/2 log^* n. We also present a tradeoff between the running time and the number of colors, and devise an O(Delta * t)-coloring algorithm with running time O(Delta / t + \\log^* n), for any parameter t, 1 < t < Delta^{1-epsilon}, for an arbitrarily small constant epsilon, 0 < epsilon < 1.   On the way to this result we study a generalization of the notion of graph coloring, which is called defective coloring. In an m-defective p-coloring the vertices are colored with p colors so that each vertex has up to m neighbors with the same color. We show that an m-defective p-coloring with reasonably small m and p can be computed very efficiently. We also develop a technique to employ multiple defect colorings of various subgraphs of the original graph G for computing a (Delta+1)-coloring of G. We believe that these techniques are of independent interest.",
        "published": "2008-12-07T20:44:28Z",
        "link": "http://arxiv.org/abs/0812.1379v2",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "A simple asynchronous replica-exchange implementation",
        "authors": [
            "Giovanni Bussi"
        ],
        "summary": "We discuss the possibility of implementing asynchronous replica-exchange (or parallel tempering) molecular dynamics. In our scheme, the exchange attempts are driven by asynchronous messages sent by one of the computing nodes, so that different replicas are allowed to perform a different number of time-steps between subsequent attempts. The implementation is simple and based on the message-passing interface (MPI). We illustrate the advantages of our scheme with respect to the standard synchronous algorithm and we benchmark it for a model Lennard-Jones liquid on an IBM-LS21 blade center cluster.",
        "published": "2008-12-09T18:39:58Z",
        "link": "http://arxiv.org/abs/0812.1633v1",
        "categories": [
            "physics.comp-ph",
            "cs.DC"
        ]
    },
    {
        "title": "Optimization of Decentralized Scheduling for Physic Applications in Grid   Environments",
        "authors": [
            "Florin Pop"
        ],
        "summary": "This paper presents a scheduling framework that is configured for, and used in physic systems. Our work addresses the problem of scheduling various computationally intensive and data intensive applications that are required for extracting information from satellite images. The proposed solution allows mapping of image processing applications onto available resources. The scheduling is done at the level of groups of concurrent applications. It demonstrates a very good behavior for scheduling and executing groups of applications, while also achieving a near-optimal utilization of the resources.",
        "published": "2008-12-11T14:52:35Z",
        "link": "http://arxiv.org/abs/0812.2164v1",
        "categories": [
            "cs.DC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "A New Method for Knowledge Representation in Expert System's (XMLKR)",
        "authors": [
            "Mehdi Bahrami"
        ],
        "summary": "Knowledge representation it is an essential section of a Expert Systems, Because in this section we have a framework to establish an expert system then we can modeling and use by this to design an expert system. Many method it is exist for knowledge representation but each method have problems, in this paper we introduce a new method of object oriented by XML language as XMLKR to knowledge representation, and we want to discuss advantage and disadvantage of this method.",
        "published": "2008-12-18T20:39:48Z",
        "link": "http://arxiv.org/abs/0812.3648v1",
        "categories": [
            "cs.DC",
            "cs.AI"
        ]
    },
    {
        "title": "Almost stable matchings in constant time",
        "authors": [
            "Patrik Floréen",
            "Petteri Kaski",
            "Valentin Polishchuk",
            "Jukka Suomela"
        ],
        "summary": "We show that the ratio of matched individuals to blocking pairs grows linearly with the number of propose--accept rounds executed by the Gale--Shapley algorithm for the stable marriage problem. Consequently, the participants can arrive at an almost stable matching even without full information about the problem instance; for each participant, knowing only its local neighbourhood is enough. In distributed-systems parlance, this means that if each person has only a constant number of acceptable partners, an almost stable matching emerges after a constant number of synchronous communication rounds. This holds even if ties are present in the preference lists.   We apply our results to give a distributed $(2+\\epsilon)$-approximation algorithm for maximum-weight matching in bicoloured graphs and a centralised randomised constant-time approximation scheme for estimating the size of a stable matching.",
        "published": "2008-12-29T11:04:46Z",
        "link": "http://arxiv.org/abs/0812.4893v1",
        "categories": [
            "cs.DS",
            "cs.DC"
        ]
    },
    {
        "title": "Symmetric and Asymmetric Asynchronous Interaction",
        "authors": [
            "Rob van Glabbeek",
            "Ursula Goltz",
            "Jens-Wolfhard Schicke"
        ],
        "summary": "We investigate classes of systems based on different interaction patterns with the aim of achieving distributability. As our system model we use Petri nets. In Petri nets, an inherent concept of simultaneity is built in, since when a transition has more than one preplace, it can be crucial that tokens are removed instantaneously. When modelling a system which is intended to be implemented in a distributed way by a Petri net, this built-in concept of synchronous interaction may be problematic. To investigate this we consider asynchronous implementations of nets, in which removing tokens from places can no longer be considered as instantaneous. We model this by inserting silent (unobservable) transitions between transitions and some of their preplaces. We investigate three such implementations, differing in the selection of preplaces of a transition from which the removal of a token is considered time consuming, and the possibility of collecting the tokens in a given order.   We investigate the effect of these different transformations of instantaneous interaction into asynchronous interaction patterns by comparing the behaviours of nets before and after insertion of the silent transitions. We exhibit for which classes of Petri nets we obtain equivalent behaviour with respect to failures equivalence.   It turns out that the resulting hierarchy of Petri net classes can be described by semi-structural properties. For two of the classes we obtain precise characterisations; for the remaining class we obtain lower and upper bounds.   We briefly comment on possible applications of our results to Message Sequence Charts.",
        "published": "2008-12-31T03:43:25Z",
        "link": "http://arxiv.org/abs/0901.0043v1",
        "categories": [
            "cs.LO",
            "cs.DC",
            "F.1.2; B.4.3"
        ]
    },
    {
        "title": "On Synchronous and Asynchronous Interaction in Distributed Systems",
        "authors": [
            "Rob van Glabbeek",
            "Ursula Goltz",
            "Jens-Wolfhard Schicke"
        ],
        "summary": "When considering distributed systems, it is a central issue how to deal with interactions between components. In this paper, we investigate the paradigms of synchronous and asynchronous interaction in the context of distributed systems. We investigate to what extent or under which conditions synchronous interaction is a valid concept for specification and implementation of such systems. We choose Petri nets as our system model and consider different notions of distribution by associating locations to elements of nets. First, we investigate the concept of simultaneity which is inherent in the semantics of Petri nets when transitions have multiple input places. We assume that tokens may only be taken instantaneously by transitions on the same location. We exhibit a hierarchy of `asynchronous' Petri net classes by different assumptions on possible distributions. Alternatively, we assume that the synchronisations specified in a Petri net are crucial system properties. Hence transitions and their preplaces may no longer placed on separate locations. We then answer the question which systems may be implemented in a distributed way without restricting concurrency, assuming that locations are inherently sequential. It turns out that in both settings we find semi-structural properties of Petri nets describing exactly the problematic situations for interactions in distributed systems.",
        "published": "2008-12-31T04:13:35Z",
        "link": "http://arxiv.org/abs/0901.0048v1",
        "categories": [
            "cs.LO",
            "cs.DC",
            "F.1.2; B.4.3"
        ]
    },
    {
        "title": "Cloud Computing and Grid Computing 360-Degree Compared",
        "authors": [
            "Ian Foster",
            "Yong Zhao",
            "Ioan Raicu",
            "Shiyong Lu"
        ],
        "summary": "Cloud Computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for Cloud Computing and there seems to be no consensus on what a Cloud is. On the other hand, Cloud Computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established Grid Computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast Cloud Computing with Grid Computing from various angles and give insights into the essential characteristics of both.",
        "published": "2008-12-31T19:13:05Z",
        "link": "http://arxiv.org/abs/0901.0131v1",
        "categories": [
            "cs.DC",
            "C.2.4; A.1"
        ]
    },
    {
        "title": "Design and Evaluation of a Collective IO Model for Loosely Coupled   Petascale Programming",
        "authors": [
            "Zhao Zhang",
            "Allan Espinosa",
            "Kamil Iskra",
            "Ioan Raicu",
            "Ian Foster",
            "Michael Wilde"
        ],
        "summary": "Loosely coupled programming is a powerful paradigm for rapidly creating higher-level applications from scientific programs on petascale systems, typically using scripting languages. This paradigm is a form of many-task computing (MTC) which focuses on the passing of data between programs as ordinary files rather than messages. While it has the significant benefits of decoupling producer and consumer and allowing existing application programs to be executed in parallel with no recoding, its typical implementation using shared file systems places a high performance burden on the overall system and on the user who will analyze and consume the downstream data. Previous efforts have achieved great speedups with loosely coupled programs, but have done so with careful manual tuning of all shared file system access. In this work, we evaluate a prototype collective IO model for file-based MTC. The model enables efficient and easy distribution of input data files to computing nodes and gathering of output results from them. It eliminates the need for such manual tuning and makes the programming of large-scale clusters using a loosely coupled model easier. Our approach, inspired by in-memory approaches to collective operations for parallel programming, builds on fast local file systems to provide high-speed local file caches for parallel scripts, uses a broadcast approach to handle distribution of common input data, and uses efficient scatter/gather and caching techniques for input and output. We describe the design of the prototype model, its implementation on the Blue Gene/P supercomputer, and present preliminary measurements of its performance on synthetic benchmarks and on a large-scale molecular dynamics application.",
        "published": "2008-12-31T19:35:07Z",
        "link": "http://arxiv.org/abs/0901.0134v1",
        "categories": [
            "cs.DC",
            "C.2.4; D.4.2; D.4.4"
        ]
    },
    {
        "title": "Scientific Computing in the Cloud",
        "authors": [
            "J. J. Rehr",
            "J. P. Gardner",
            "M. Prange",
            "L. Svec",
            "F. Vila"
        ],
        "summary": "We investigate the feasibility of high performance scientific computation using cloud computers as an alternative to traditional computational tools. The availability of these large, virtualized pools of compute resources raises the possibility of a new compute paradigm for scientific research with many advantages. For research groups, cloud computing provides convenient access to reliable, high performance clusters and storage, without the need to purchase and maintain sophisticated hardware. For developers, virtualization allows scientific codes to be optimized and pre-installed on machine images, facilitating control over the computational environment. Preliminary tests are presented for serial and parallelized versions of the widely used x-ray spectroscopy and electronic structure code FEFF on the Amazon Elastic Compute Cloud, including CPU and network performance.",
        "published": "2008-12-31T20:35:40Z",
        "link": "http://arxiv.org/abs/0901.0029v1",
        "categories": [
            "cond-mat.mtrl-sci",
            "cs.DC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Implementation of perception and action at nanoscale",
        "authors": [
            "Sylvain Marlière",
            "Jean Loup Florens",
            "Florence Marchi",
            "Annie Luciani",
            "Joel Chevrier"
        ],
        "summary": "Real time combination of nanosensors and nanoactuators with virtual reality environment and multisensorial interfaces enable us to efficiently act and perceive at nanoscale. Advanced manipulation of nanoobjects and new strategies for scientific education are the key motivations. We have no existing intuitive representation of the nanoworld ruled by laws foreign to our experience. A central challenge is then the construction of nanoworld simulacrum that we can start to visit and to explore. In this nanoworld simulacrum, object identifications will be based on probed entity physical and chemical intrinsic properties, on their interactions with sensors and on the final choices made in building a multisensorial interface so that these objects become coherent elements of the human sphere of action and perception. Here we describe a 1D virtual nanomanipulator, part of the Cit\\'e des Sciences EXPO NANO in Paris, that is the first realization based on this program.",
        "published": "2008-01-04T13:38:39Z",
        "link": "http://arxiv.org/abs/0801.0678v1",
        "categories": [
            "cs.RO",
            "cs.HC"
        ]
    },
    {
        "title": "The What, Who, Where, When, Why and How of Context-Awareness",
        "authors": [
            "George Tsibidis",
            "Theodoros N. Arvanitis",
            "Chris Baber"
        ],
        "summary": "The understanding of context and context-awareness is very important for the areas of handheld and ubiquitous computing. Unfortunately, at present, there has not been a satisfactory definition of these two concepts that would lead to a more effective communication in humancomputer interaction. As a result, on the one hand, application designers are not able to choose what context to use in their applications and on the other, they cannot determine the type of context-awareness behaviours their applications should exhibit. In this work, we aim to provide answers to some fundamental questions that could enlighten us on the definition of context and its functionality.",
        "published": "2008-01-07T16:05:46Z",
        "link": "http://arxiv.org/abs/0801.1033v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Interpretation as a factor in understanding flawed spreadsheets",
        "authors": [
            "David A. Banks",
            "Ann Monday"
        ],
        "summary": "The spreadsheet has been used by the business community for many years and yet still raises a number of significant concerns. As educators our concern is to try to develop the students skills in both the development of spreadsheets and in taking a critical view of their potential defects. In this paper we consider both the problems of mechanical production and the problems of translation of problem to spreadsheet representation.",
        "published": "2008-01-11T21:47:01Z",
        "link": "http://arxiv.org/abs/0801.1856v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1; K.3"
        ]
    },
    {
        "title": "A Framework for Designing Teleconsultation Systems in Africa",
        "authors": [
            "Rowena Luk",
            "Melissa Ho",
            "Paul M. Aoki"
        ],
        "summary": "All of the countries within Africa experience a serious shortage of medical professionals, particularly specialists, a problem that is only exacerbated by high emigration of doctors with better prospects overseas. As a result, those that remain in Africa, particularly those practicing in rural regions, experience a shortage of specialists and other colleagues with whom to exchange ideas. Telemedicine and teleconsultation are key areas that attempt to address this problem by leveraging remote expertise for local problems. This paper presents an overview of teleconsultation in the developing world, with a particular focus on how lessons learned apply to Africa. By teleconsultation, we are addressing non-real-time communication between health care professionals for the purposes of providing expertise and informal recommendations, without the real-time, interactive requirements typical of diagnosis and patient care, which is impractical for the vast majority of existing medical practices. From these previous experiences, we draw a set of guidelines and examine their relevance to Ghana in particular. Based on 6 weeks of needs assessment, we identify key variables that guide our framework, and then illustrate how our framework is used to inform the iterative design of a prototype system.",
        "published": "2008-01-12T23:39:30Z",
        "link": "http://arxiv.org/abs/0801.1925v1",
        "categories": [
            "cs.HC",
            "H.5.3"
        ]
    },
    {
        "title": "Asynchronous Remote Medical Consultation for Ghana",
        "authors": [
            "Rowena Luk",
            "Melissa Ho",
            "Paul M. Aoki"
        ],
        "summary": "Computer-mediated communication systems can be used to bridge the gap between doctors in underserved regions with local shortages of medical expertise and medical specialists worldwide. To this end, we describe the design of a prototype remote consultation system intended to provide the social, institutional and infrastructural context for sustained, self-organizing growth of a globally-distributed Ghanaian medical community. The design is grounded in an iterative design process that included two rounds of extended design fieldwork throughout Ghana and draws on three key design principles (social networks as a framework on which to build incentives within a self-organizing network; optional and incremental integration with existing referral mechanisms; and a weakly-connected, distributed architecture that allows for a highly interactive, responsive system despite failures in connectivity). We discuss initial experiences from an ongoing trial deployment in southern Ghana.",
        "published": "2008-01-12T23:43:18Z",
        "link": "http://arxiv.org/abs/0801.1927v1",
        "categories": [
            "cs.HC",
            "H.5.m"
        ]
    },
    {
        "title": "Multiple Uncertainties in Time-Variant Cosmological Particle Data",
        "authors": [
            "Steve Haroz",
            "Kwan-Liu Ma",
            "Katrin Heitmann"
        ],
        "summary": "Though the mediums for visualization are limited, the potential dimensions of a dataset are not. In many areas of scientific study, understanding the correlations between those dimensions and their uncertainties is pivotal to mining useful information from a dataset. Obtaining this insight can necessitate visualizing the many relationships among temporal, spatial, and other dimensionalities of data and its uncertainties. We utilize multiple views for interactive dataset exploration and selection of important features, and we apply those techniques to the unique challenges of cosmological particle datasets. We show how interactivity and incorporation of multiple visualization techniques help overcome the problem of limited visualization dimensions and allow many types of uncertainty to be seen in correlation with other variables.",
        "published": "2008-01-15T22:57:41Z",
        "link": "http://arxiv.org/abs/0801.2405v2",
        "categories": [
            "astro-ph",
            "cs.GR",
            "cs.HC"
        ]
    },
    {
        "title": "Human Heuristics for Autonomous Agents",
        "authors": [
            "Franco Bagnoli",
            "Andrea Guazzini",
            "Pietro Lio'"
        ],
        "summary": "We investigate the problem of autonomous agents processing pieces of information that may be corrupted (tainted). Agents have the option of contacting a central database for a reliable check of the status of the message, but this procedure is costly and therefore should be used with parsimony. Agents have to evaluate the risk of being infected, and decide if and when communicating partners are affordable. Trustability is implemented as a personal (one-to-one) record of past contacts among agents, and as a mean-field monitoring of the level of message corruption. Moreover, this information is slowly forgotten in time, so that at the end everybody is checked against the database. We explore the behavior of a homogeneous system in the case of a fixed pool of spreaders of corrupted messages, and in the case of spontaneous appearance of corrupted messages.",
        "published": "2008-01-19T19:36:13Z",
        "link": "http://arxiv.org/abs/0801.3048v1",
        "categories": [
            "cs.MA",
            "cs.HC",
            "cs.NI"
        ]
    },
    {
        "title": "Balancing transparency, efficiency and security in pervasive systems",
        "authors": [
            "Mark Wenstrom",
            "Eloisa Bentivegna",
            "Ali Hurson"
        ],
        "summary": "This chapter will survey pervasive computing with a look at how its constraint for transparency affects issues of resource management and security. The goal of pervasive computing is to render computing transparent, such that computing resources are ubiquitously offered to the user and services are proactively performed for a user without his or her intervention. The task of integrating computing infrastructure into everyday life without making it excessively invasive brings about tradeoffs between flexibility and robustness, efficiency and effectiveness, as well as autonomy and reliability. As the feasibility of ubiquitous computing and its real potential for mass applications are still a matter of controversy, this chapter will look into the underlying issues of resource management and authentication to discover how these can be handled in a least invasive fashion. The discussion will be closed by an overview of the solutions proposed by current pervasive computing efforts, both in the area of generic platforms and for dedicated applications such as pervasive education and healthcare.",
        "published": "2008-01-20T19:15:50Z",
        "link": "http://arxiv.org/abs/0801.3102v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.m"
        ]
    },
    {
        "title": "Thinking is Bad: Implications of Human Error Research for Spreadsheet   Research and Practice",
        "authors": [
            "Raymond R. Panko"
        ],
        "summary": "In the spreadsheet error community, both academics and practitioners generally have ignored the rich findings produced by a century of human error research. These findings can suggest ways to reduce errors; we can then test these suggestions empirically. In addition, research on human error seems to suggest that several common prescriptions and expectations for reducing errors are likely to be incorrect. Among the key conclusions from human error research are that thinking is bad, that spreadsheets are not the cause of spreadsheet errors, and that reducing errors is extremely difficult.",
        "published": "2008-01-21T00:33:15Z",
        "link": "http://arxiv.org/abs/0801.3114v1",
        "categories": [
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1; K.3; K.6.3"
        ]
    },
    {
        "title": "Categorisation of Spreadsheet Use within Organisations, Incorporating   Risk: A Progress Report",
        "authors": [
            "Mukul Madahar",
            "Pat Cleary",
            "David Ball"
        ],
        "summary": "There has been a significant amount of research into spreadsheets over the last two decades. Errors in spreadsheets are well documented. Once used mainly for simple functions such as logging, tracking and totalling information, spreadsheets with enhanced formulas are being used for complex calculative models. There are many software packages and tools which assist in detecting errors within spreadsheets. There has been very little evidence of investigation into the spreadsheet risks associated with the main stream operations within an organisation. This study is a part of the investigation into the means of mitigating risks associated with spreadsheet use within organisations. In this paper the authors present and analyse three proposed models for categorisation of spreadsheet use and the level of risks involved. The models are analysed in the light of current knowledge and the general risks associated with organisations.",
        "published": "2008-01-21T00:57:31Z",
        "link": "http://arxiv.org/abs/0801.3119v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "Ensuring Spreadsheet Integrity with Model Master",
        "authors": [
            "Jocelyn Paine"
        ],
        "summary": "We have developed the Model Master (MM) language for describing spreadsheets, and tools for converting MM programs to and from spreadsheets. The MM decompiler translates a spreadsheet into an MM program which gives a concise summary of its calculations, layout, and styling. This is valuable when trying to understand spreadsheets one has not seen before, and when checking for errors. The MM compiler goes the other way, translating an MM program into a spreadsheet. This makes possible a new style of development, in which spreadsheets are generated from textual specifications. This can reduce error rates compared to working directly with the raw spreadsheet, and gives important facilities for code reuse. MM programs also offer advantages over Excel files for the interchange of spreadsheets.",
        "published": "2008-01-24T00:32:29Z",
        "link": "http://arxiv.org/abs/0801.3690v1",
        "categories": [
            "cs.PL",
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.9"
        ]
    },
    {
        "title": "Computational Models of Spreadsheet Development: Basis for Educational   Approaches",
        "authors": [
            "Karin Hodnigg",
            "Markus Clermont",
            "Roland T. Mittermeir"
        ],
        "summary": "Among the multiple causes of high error rates in spreadsheets, lack of proper training and of deep understanding of the computational model upon which spreadsheet computations rest might not be the least issue. The paper addresses this problem by presenting a didactical model focussing on cell interaction, thus exceeding the atomicity of cell computations. The approach is motivated by an investigation how different spreadsheet systems handle certain computational issues implied from moving cells, copy-paste operations, or recursion.",
        "published": "2008-01-28T13:55:55Z",
        "link": "http://arxiv.org/abs/0801.4274v1",
        "categories": [
            "cs.HC",
            "cs.SE"
        ]
    },
    {
        "title": "It's Not What You Have, But How You Use It: Compromises in Mobile Device   Use",
        "authors": [
            "Manas Tungare",
            "Manuel Perez-Quinones"
        ],
        "summary": "As users begin to use many more devices for personal information management (PIM) than just the traditional desktop computer, it is essential for HCI researchers to understand how these devices are being used in the wild and their roles in users' information environments. We conducted a study of 220 knowledge workers about their devices, the activities they performed on each, and the groups of devices used together. Our findings indicate that several devices are often used in groups; integrated multi-function portable devices have begun to replace single-function devices for communication (e.g. email and IM). Users use certain features opportunistically because they happen to be carrying a multi-function device with them. The use of multiple devices and multi-function devices is fraught with compromises as users must choose and make trade-offs among various factors.",
        "published": "2008-01-29T04:42:26Z",
        "link": "http://arxiv.org/abs/0801.4423v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Sign Language Tutoring Tool",
        "authors": [
            "Oya Aran",
            "Ismail Ari",
            "Alexandre Benoit",
            "Ana Huerta Carrillo",
            "François-Xavier Fanard",
            "Pavel Campr",
            "Lale Akarun",
            "Alice Caplier",
            "Michele Rombaut",
            "Bulent Sankur"
        ],
        "summary": "In this project, we have developed a sign language tutor that lets users learn isolated signs by watching recorded videos and by trying the same signs. The system records the user's video and analyses it. If the sign is recognized, both verbal and animated feedback is given to the user. The system is able to recognize complex signs that involve both hand gestures and head movements and expressions. Our performance tests yield a 99% recognition rate on signs involving only manual gestures and 85% recognition rate on signs that involve both manual and non manual components, such as head movement and facial expressions.",
        "published": "2008-02-18T07:28:44Z",
        "link": "http://arxiv.org/abs/0802.2428v1",
        "categories": [
            "cs.LG",
            "cs.HC"
        ]
    },
    {
        "title": "Spreadsheet Errors: What We Know. What We Think We Can Do",
        "authors": [
            "Raymond R. Panko"
        ],
        "summary": "Fifteen years of research studies have concluded unanimously that spreadsheet errors are both common and non-trivial. Now we must seek ways to reduce spreadsheet errors. Several approaches have been suggested, some of which are promising and others, while appealing because they are easy to do, are not likely to be effective. To date, only one technique, cell-by-cell code inspection, has been demonstrated to be effective. We need to conduct further research to determine the degree to which other techniques can reduce spreadsheet errors.",
        "published": "2008-02-23T19:48:15Z",
        "link": "http://arxiv.org/abs/0802.3457v1",
        "categories": [
            "cs.SE",
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "Spreadsheet Development Methodologies using Resolver: Moving   spreadsheets into the 21st Century",
        "authors": [
            "Patrick Kemmis",
            "Giles Thomas"
        ],
        "summary": "We intend to demonstrate the innate problems with existing spreadsheet products and to show how to tackle these issues using a new type of spreadsheet program called Resolver. It addresses the issues head-on and thereby moves the 1980's \"VisiCalc paradigm\" on to match the advances in computer languages and user requirements. Continuous display of the spreadsheet grid and the equivalent computer program, together with the ability to interact and add code through either interface, provides a number of new methodologies for spreadsheet development.",
        "published": "2008-02-24T01:16:52Z",
        "link": "http://arxiv.org/abs/0802.3475v1",
        "categories": [
            "cs.SE",
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.9"
        ]
    },
    {
        "title": "Fun Boy Three Were Wrong: it is what you do, not the way that you do it",
        "authors": [
            "Jocelyn Paine"
        ],
        "summary": "I revisit some classic publications on modularity, to show what problems its pioneers wanted to solve. These problems occur with spreadsheets too: to recognise them may help us avoid them.",
        "published": "2008-02-24T01:34:36Z",
        "link": "http://arxiv.org/abs/0802.3476v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Concerning the Feasibility of Example-driven Modelling Techniques",
        "authors": [
            "Simon R. Thorne",
            "David Ball",
            "Z. Lawson"
        ],
        "summary": "We report on a series of experiments concerning the feasibility of example driven modelling. The main aim was to establish experimentally within an academic environment: the relationship between error and task complexity using a) Traditional spreadsheet modelling; b) example driven techniques. We report on the experimental design, sampling, research methods and the tasks set for both control and treatment groups. Analysis of the completed tasks allows comparison of several different variables. The experimental results compare the performance indicators for the treatment and control groups by comparing accuracy, experience, training, confidence measures, perceived difficulty and perceived completeness. The various results are thoroughly tested for statistical significance using: the Chi squared test, Fisher's exact test for significance, Cochran's Q test and McNemar's test on difficulty.",
        "published": "2008-02-24T01:49:31Z",
        "link": "http://arxiv.org/abs/0802.3477v1",
        "categories": [
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.9"
        ]
    },
    {
        "title": "It Ain't What You View, But The Way That You View It: documenting   spreadsheets with Excelsior, semantic wikis, and literate programming",
        "authors": [
            "Jocelyn Paine"
        ],
        "summary": "I describe preliminary experiments in documenting Excelsior versions of spreadsheets using semantic wikis and literate programming. The objective is to create well-structured and comprehensive documentation, easy to use by those unfamiliar with the spreadsheets documented. I discuss why so much documentation is hard to use, and briefly explain semantic wikis and literate programming; although parts of the paper are Excelsior-specific, these sections may be of more general interest.",
        "published": "2008-02-24T01:57:01Z",
        "link": "http://arxiv.org/abs/0802.3478v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "J.1; H.4.1; K.6.4; D.2.9"
        ]
    },
    {
        "title": "An Empirical Study of End-User Behaviour in Spreadsheet Error Detection   & Correction",
        "authors": [
            "Brian Bishop",
            "Kevin McDaid"
        ],
        "summary": "Very little is known about the process by which end-user developers detect and correct spreadsheet errors. Any research pertaining to the development of spreadsheet testing methodologies or auditing tools would benefit from information on how end-users perform the debugging process in practice. Thirteen industry-based professionals and thirty-four accounting & finance students took part in a current ongoing experiment designed to record and analyse end-user behaviour in spreadsheet error detection and correction. Professionals significantly outperformed students in correcting certain error types. Time-based cell activity analysis showed that a strong correlation exists between the percentage of cells inspected and the number of errors corrected. The cell activity data was gathered through a purpose written VBA Excel plug-in that records the time and detail of all cell selection and cell change actions of individuals.",
        "published": "2008-02-24T02:03:16Z",
        "link": "http://arxiv.org/abs/0802.3479v1",
        "categories": [
            "cs.HC",
            "cs.CY",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Why Task-Based Training is Superior to Traditional Training Methods",
        "authors": [
            "Kath McGuire"
        ],
        "summary": "The risks of spreadsheet use do not just come from the misuse of formulae. As such, training needs to go beyond this technical aspect of spreadsheet use and look at the spreadsheet in its full business context. While standard training is by and large unable to do this, task-based training is perfectly suited to a contextual approach to training.",
        "published": "2008-02-24T02:10:03Z",
        "link": "http://arxiv.org/abs/0802.3480v1",
        "categories": [
            "cs.HC",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Establishing A Minimum Generic Skill Set For Risk Management Teaching In   A Spreadsheet Training Course",
        "authors": [
            "David Chadwick"
        ],
        "summary": "Past research shows that spreadsheet models are prone to such a high frequency of errors and data security implications that the risk management of spreadsheet development and spreadsheet use is of great importance to both industry and academia. The underlying rationale for this paper is that spreadsheet training courses should specifically address risk management in the development process both from a generic and a domain-specific viewpoint. This research specifically focuses on one of these namely those generic issues of risk management that should be present in a training course that attempts to meet good-practice within industry. A pilot questionnaire was constructed showing a possible minimum set of risk management issues and sent to academics and industry practitioners for feedback. The findings from this pilot survey will be used to refine the questionnaire for sending to a larger body of possible respondents. It is expected these findings will form the basis of a risk management teaching approach to be trialled in a number of selected ongoing spreadsheet training courses.",
        "published": "2008-02-24T02:15:49Z",
        "link": "http://arxiv.org/abs/0802.3481v1",
        "categories": [
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1; K.3"
        ]
    },
    {
        "title": "Voice-controlled Debugging of Spreadsheets",
        "authors": [
            "Derek Flood",
            "Kevin Mc Daid"
        ],
        "summary": "Developments in Mobile Computing are putting pressure on the software industry to research new modes of interaction that do not rely on the traditional keyboard and mouse combination. Computer users suffering from Repetitive Strain Injury also seek an alternative to keyboard and mouse devices to reduce suffering in wrist and finger joints. Voice-control is an alternative approach to spreadsheet development and debugging that has been researched and used successfully in other domains. While voice-control technology for spreadsheets is available its effectiveness has not been investigated. This study is the first to compare the performance of a set of expert spreadsheet developers that debugged a spreadsheet using voice-control technology and another set that debugged the same spreadsheet using keyboard and mouse. The study showed that voice, despite its advantages, proved to be slower and less accurate. However, it also revealed ways in which the technology might be improved to redress this imbalance.",
        "published": "2008-02-24T02:21:35Z",
        "link": "http://arxiv.org/abs/0802.3483v1",
        "categories": [
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "A Paradigm for Spreadsheet Engineering Methodologies",
        "authors": [
            "Thomas A. Grossman",
            "Ozgur Ozluk"
        ],
        "summary": "Spreadsheet engineering methodologies are diverse and sometimes contradictory. It is difficult for spreadsheet developers to identify a spreadsheet engineering methodology that is appropriate for their class of spreadsheet, with its unique combination of goals, type of problem, and available time and resources. There is a lack of well-organized, proven methodologies with known costs and benefits for well-defined spreadsheet classes. It is difficult to compare and critically evaluate methodologies. We present a paradigm for organizing and interpreting spreadsheet engineering recommendations. It systematically addresses the myriad choices made when developing a spreadsheet, and explicitly considers resource constraints and other development parameters. This paradigm provides a framework for evaluation, comparison, and selection of methodologies, and a list of essential elements for developers or codifiers of new methodologies. This paradigm identifies gaps in our knowledge that merit further research.",
        "published": "2008-02-26T21:58:26Z",
        "link": "http://arxiv.org/abs/0802.3919v1",
        "categories": [
            "cs.HC",
            "D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "A Toolkit for Scalable Spreadsheet Visualization",
        "authors": [
            "Markus Clermont"
        ],
        "summary": "This paper presents a toolkit for spreadsheet visualization based on logical areas, semantic classes and data modules. Logical areas, semantic classes and data modules are abstract representations of spreadsheet programs that are meant to reduce the auditing and comprehension effort, especially for large and regular spreadsheets. The toolkit is integrated as a plug-in in the Gnumeric spreadsheet system for Linux. It can process large, industry scale spreadsheet programs in reasonable time and is tightly integrated with its host spreadsheet system. Users can generate hierarchical and graph-based representations of their spreadsheets. This allows them to spot conceptual similarities in different regions of the spreadsheet, that would otherwise not fit on a screen. As it is assumed that the learning effort for effective use of such a tool should be kept low, we aim for intuitive handling of most of the tool's functions.",
        "published": "2008-02-26T22:11:24Z",
        "link": "http://arxiv.org/abs/0802.3924v1",
        "categories": [
            "cs.HC",
            "D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Using Layout Information for Spreadsheet Visualization",
        "authors": [
            "Sabine Hipfl"
        ],
        "summary": "This paper extends a spreadsheet visualization technique by using layout information. The original approach identifies logically or semantically related cells by relying exclusively on the content of cells for identifying semantic classes. A disadvantage of semantic classes is that users have to supply parameters which describe the possible shapes of these blocks. The correct parametrization requires a certain degree of experience and is thus not suitable for untrained users. To avoid this constraint, the approach reported in this paper uses row/column-labels as well as common format information for locating areas with common, recurring semantics. Heuristics are provided to distinguish between cell groups with intended common semantics and cell groups related in an ad-hoc manner.",
        "published": "2008-02-27T00:14:34Z",
        "link": "http://arxiv.org/abs/0802.3939v1",
        "categories": [
            "cs.HC",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "HyperSmooth : calcul et visualisation de cartes de potentiel   interactives",
        "authors": [
            "Christine Plumejeaud",
            "Jean-Marc Vincent",
            "Claude Grasland",
            "Jérôme Gensel",
            "Hélène Mathian",
            "Serge Guelton",
            "Joël Boulier"
        ],
        "summary": "The HyperCarte research group wishes to offer a new cartographic tool for spatial analysis of social data, using the potential smoothing method. The purpose of this method is to view the spreading of phenomena's in a continuous way, at a macroscopic scale, basing on data sampled on administrative areas. We aim to offer an interactive tool, accessible via the Web, but guarantying the confidentiality of data. The major difficulty is induced by the high complexity of the calculus, working on a great amount of data. We present our solution to such a technical challenge, and our perspectives of enhancements.",
        "published": "2008-02-28T12:36:41Z",
        "link": "http://arxiv.org/abs/0802.4191v1",
        "categories": [
            "stat.AP",
            "cs.HC"
        ]
    },
    {
        "title": "Considering Functional Spreadsheet Operator Usage Suggests the Value of   Example Driven Modelling for Decision Support Systems",
        "authors": [
            "Simon Thorne",
            "David Ball"
        ],
        "summary": "Most spreadsheet surveys both for reporting use and error focus on the practical application of the spreadsheet in a particular industry. Typically these studies will illustrate that a particular percentage of spreadsheets are used for optimisation and a further percentage are used for 'What if' analysis. Much less common is examining the classes of function, as defined by the vendor, used by modellers to build their spreadsheet models. This alternative analysis allows further insight into the programming nature of spreadsheets and may assist researchers in targeting particular structures in spreadsheet software for further investigation. Further, understanding the functional make-up of spreadsheets allows effective evaluation of novel approaches from a programming point of view. It allows greater insight into studies that report what spreadsheets are used for since it is explicit which functional structures are in use in spreadsheets. We conclude that a deeper understanding of the use of operators and the operator's relationship to error would provide fresh insight into the spreadsheet error problem. Considering functional spreadsheet operator usage suggests the value of Example Driven Modelling for Decision Support Systems",
        "published": "2008-03-03T01:25:41Z",
        "link": "http://arxiv.org/abs/0803.0164v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "Documenting Spreadsheets",
        "authors": [
            "Raymond Payette"
        ],
        "summary": "This paper discusses spreadsheets documentation and new means to achieve this end by using Excel's built-in \"Comment\" function. By structuring comments, they can be used as an essential tool to fully explain spreadsheet. This will greatly facilitate spreadsheet change control, risk management and auditing. It will fill a crucial gap in corporate governance by adding essential information that can be managed in order to satisfy internal controls and accountability standards.",
        "published": "2008-03-03T01:34:30Z",
        "link": "http://arxiv.org/abs/0803.0165v1",
        "categories": [
            "cs.HC",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Spreadsheet Validation and Analysis through Content Visualization",
        "authors": [
            "Richard Brath",
            "Michael Peters"
        ],
        "summary": "Visualizing spreadsheet content provides analytic insight and visual validation of large amounts of spreadsheet data. Oculus Excel Visualizer is a point and click data visualization experiment which directly visualizes Excel data and re-uses the layout and formatting already present in the spreadsheet.",
        "published": "2008-03-03T01:40:31Z",
        "link": "http://arxiv.org/abs/0803.0166v1",
        "categories": [
            "cs.HC",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Does an awareness of differing types of spreadsheet errors aid end-users   in identifying spreadsheets errors?",
        "authors": [
            "Michael Purser",
            "David Chadwick"
        ],
        "summary": "The research presented in this paper establishes a valid, and simplified, revision of previous spreadsheet error classifications. This investigation is concerned with the results of a web survey and two web-based gender and domain-knowledge free spreadsheet error identification exercises. The participants of the survey and exercises were a test group of professionals (all of whom regularly use spreadsheets) and a control group of students from the University of Greenwich (UK). The findings show that over 85% of users are also the spreadsheet's developer, supporting the revised spreadsheet error classification. The findings also show that spreadsheet error identification ability is directly affected both by spreadsheet experience and by error-type awareness. In particular, that spreadsheet error-type awareness significantly improves the user's ability to identify, the more surreptitious, qualitative error.",
        "published": "2008-03-03T01:49:03Z",
        "link": "http://arxiv.org/abs/0803.0167v1",
        "categories": [
            "cs.HC",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Comparison of Characteristics and Practices amongst Spreadsheet Users   with Different Levels of Experience",
        "authors": [
            "Kenneth R. Baker",
            "Stephen G. Powell",
            "Barry Lawson",
            "Lynn Foster-Johnson"
        ],
        "summary": "We developed an internet-based questionnaire on spreadsheet use that we administered to a large number of users in several companies and organizations to document how spreadsheets are currently being developed and used in business. In this paper, we discuss the results drawn from of a comparison of responses from individuals with the most experience and expertise with those from individuals with the least. These results describe two views of spreadsheet design and use in organizations, and reflect gaps between these two groups and between these groups and the entire population of nearly 1600 respondents. Moreover, our results indicate that these gaps have multiple dimensions: they reflect not only the context, skill, and practices of individual users but also the policies of large organizations.",
        "published": "2008-03-03T01:55:40Z",
        "link": "http://arxiv.org/abs/0803.0168v1",
        "categories": [
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "An Investigation of the Incidence and Effect of Spreadsheet Errors   Caused by the Hard Coding of Input Data Values into Formulas",
        "authors": [
            "Paul J. Blayney"
        ],
        "summary": "The hard coding of input data or constants into spreadsheet formulas is widely recognised as poor spreadsheet model design. However, the importance of avoiding such practice appears to be underestimated perhaps in light of the lack of quantitative error at the time of occurrence and the recognition that this design defect may never result in a bottom-line error. The paper examines both the academic and practitioner view of such hard coding design flaws. The practitioner or industry viewpoint is gained indirectly through a review of commercial spreadsheet auditing software. The development of an automated (electronic) means for detecting such hard coding is described together with a discussion of some results obtained through analysis of a number of student and practitioner spreadsheet models.",
        "published": "2008-03-03T02:00:53Z",
        "link": "http://arxiv.org/abs/0803.0169v1",
        "categories": [
            "cs.HC",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Intuitive Source Code Visualization Tools for Improving Student   Comprehension: BRICS",
        "authors": [
            "Christopher Pearson",
            "Celina Gibbs",
            "Yvonne Coady"
        ],
        "summary": "Even relatively simple code analysis can be a daunting task for many first year students. Perceived complexity, coupled with foreign and harsh syntax, often outstrips the ability for students to take in what they are seeing in terms of their verbal memory. That is, first year students often lack the experience to encode critical building blocks in source code, and their interrelationships, into their own words. We believe this argues for the need for IDEs to provide additional support for representations that would appeal directly to visual memory. In this paper, we examine this need for intuitive source code visualization tools that are easily accessible to novice programmers, discuss the requirements for such a tool, and suggest a novel idea that takes advantage of human peripheral vision to achieve stronger overall code structure awareness.",
        "published": "2008-03-04T18:46:49Z",
        "link": "http://arxiv.org/abs/0803.0515v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Optimizing Web Sites for Customer Retention",
        "authors": [
            "Michael Hahsler"
        ],
        "summary": "With customer relationship management (CRM) companies move away from a mainly product-centered view to a customer-centered view. Resulting from this change, the effective management of how to keep contact with customers throughout different channels is one of the key success factors in today's business world. Company Web sites have evolved in many industries into an extremely important channel through which customers can be attracted and retained. To analyze and optimize this channel, accurate models of how customers browse through the Web site and what information within the site they repeatedly view are crucial. Typically, data mining techniques are used for this purpose. However, there already exist numerous models developed in marketing research for traditional channels which could also prove valuable to understanding this new channel. In this paper we propose the application of an extension of the Logarithmic Series Distribution (LSD) model repeat-usage of Web-based information and thus to analyze and optimize a Web Site's capability to support one goal of CRM, to retain customers. As an example, we use the university's blended learning web portal with over a thousand learning resources to demonstrate how the model can be used to evaluate and improve the Web site's effectiveness.",
        "published": "2008-03-07T14:50:21Z",
        "link": "http://arxiv.org/abs/0803.1104v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Citation Counting, Citation Ranking, and h-Index of Human-Computer   Interaction Researchers: A Comparison between Scopus and Web of Science",
        "authors": [
            "Lokman I. Meho",
            "Yvonne Rogers"
        ],
        "summary": "This study examines the differences between Scopus and Web of Science in the citation counting, citation ranking, and h-index of 22 top human-computer interaction (HCI) researchers from EQUATOR--a large British Interdisciplinary Research Collaboration project. Results indicate that Scopus provides significantly more coverage of HCI literature than Web of Science, primarily due to coverage of relevant ACM and IEEE peer-reviewed conference proceedings. No significant differences exist between the two databases if citations in journals only are compared. Although broader coverage of the literature does not significantly alter the relative citation ranking of individual researchers, Scopus helps distinguish between the researchers in a more nuanced fashion than Web of Science in both citation counting and h-index. Scopus also generates significantly different maps of citation networks of individual scholars than those generated by Web of Science. The study also presents a comparison of h-index scores based on Google Scholar with those based on the union of Scopus and Web of Science. The study concludes that Scopus can be used as a sole data source for citation-based research and evaluation in HCI, especially if citations in conference proceedings are sought and that h scores should be manually calculated instead of relying on system calculations.",
        "published": "2008-03-12T08:09:19Z",
        "link": "http://arxiv.org/abs/0803.1716v1",
        "categories": [
            "cs.HC",
            "cs.IR"
        ]
    },
    {
        "title": "A Novel Approach to Formulae Production and Overconfidence Measurement   to Reduce Risk in Spreadsheet Modelling",
        "authors": [
            "Simon Thorne",
            "David Ball",
            "Zoe Lawson"
        ],
        "summary": "Research on formulae production in spreadsheets has established the practice as high risk yet unrecognised as such by industry. There are numerous software applications that are designed to audit formulae and find errors. However these are all post creation, designed to catch errors before the spreadsheet is deployed. As a general conclusion from EuSpRIG 2003 conference it was decided that the time has come to attempt novel solutions based on an understanding of human factors. Hence in this paper we examine one such possibility namely a novel example driven modelling approach. We discuss a control experiment that compares example driven modelling against traditional approaches over several progressively more difficult tests. The results are very interesting and certainly point to the value of further investigation of the example driven potential. Lastly we propose a method for statistically analysing the problem of overconfidence in spreadsheet modellers.",
        "published": "2008-03-12T11:47:41Z",
        "link": "http://arxiv.org/abs/0803.1754v1",
        "categories": [
            "cs.HC",
            "cs.CY",
            "D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Exploring Human Factors in Spreadsheet Development",
        "authors": [
            "Simon Thorne",
            "David Ball"
        ],
        "summary": "In this paper we consider human factors and their impact on spreadsheet development in strategic decision-making. This paper brings forward research from many disciplines both directly related to spreadsheets and a broader spectrum from psychology to industrial processing. We investigate how human factors affect a simplified development cycle and what the potential consequences are.",
        "published": "2008-03-12T22:09:59Z",
        "link": "http://arxiv.org/abs/0803.1862v1",
        "categories": [
            "cs.HC",
            "cs.CY",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Risk Management for Complex Calculations: EuSpRIG Best Practices in   Hybrid Applications",
        "authors": [
            "Deborah Cernauskas",
            "Andrew Kumiega",
            "Ben VanVliet"
        ],
        "summary": "As the need for advanced, interactive mathematical models has increased, user/programmers are increasingly choosing the MatLab scripting language over spreadsheets. However, applications developed in these tools have high error risk, and no best practices exist. We recommend that advanced, highly mathematical applications incorporate these tools with spreadsheets into hybrid applications, where developers can apply EuSpRIG best practices. Development of hybrid applications can reduce the potential for errors, shorten development time, and enable higher level operations. We believe that hybrid applications are the future and over the course of this paper, we apply and extend spreadsheet best practices to reduce or prevent risks in hybrid Excel/MatLab applications.",
        "published": "2008-03-12T22:30:10Z",
        "link": "http://arxiv.org/abs/0803.1866v1",
        "categories": [
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "Breaking Out of the Cell: On The Benefits of a New Spreadsheet   User-Interaction Paradigm",
        "authors": [
            "Ziv Hellman"
        ],
        "summary": "Contemporary spreadsheets are plagued by a profusion of errors, auditing difficulties, lack of uniform development methodologies, and barriers to easy comprehension of the underlying business models they represent. This paper presents a case that most of these difficulties stem from the fact that the standard spreadsheet user-interaction paradigm - the 'cell-matrix' approach - is appropriate for spreadsheet data presentation but has significant drawbacks with respect to spreadsheet creation, maintenance and comprehension when workbooks pass a minimal threshold of complexity. An alternative paradigm for the automated generation of spreadsheets directly from plain-language business model descriptions is presented along with its potential benefits. Sunsight Modeller (TM), a working software system implementing the suggested paradigm, is briefly described.",
        "published": "2008-03-13T00:15:40Z",
        "link": "http://arxiv.org/abs/0803.1875v1",
        "categories": [
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "Controlling the Information Flow in Spreadsheets",
        "authors": [
            "Vipin Samar",
            "Sangeeta Patni"
        ],
        "summary": "There is no denying that spreadsheets have become critical for all operational processes including financial reporting, budgeting, forecasting, and analysis. Microsoft Excel has essentially become a scratch pad and a data browser that can quickly be put to use for information gathering and decision-making. However, there is little control in how data comes into Excel, and how it gets updated. The information supply chain feeding into Excel remains ad hoc and without any centralized IT control. This paper discusses some of the pitfalls of the data collection and maintenance process in Excel. It then suggests service-oriented architecture (SOA) based information gathering and control techniques to ameliorate the pitfalls of this scratch pad while improving the integrity of data, boosting the productivity of the business users, and building controls to satisfy the requirements of Section 404 of the Sarbanes-Oxley Act.",
        "published": "2008-03-17T20:46:12Z",
        "link": "http://arxiv.org/abs/0803.2527v1",
        "categories": [
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.9"
        ]
    },
    {
        "title": "Towards a human eye behavior model by applying Data Mining Techniques on   Gaze Information from IEC",
        "authors": [
            "Denis Pallez",
            "Laurent Brisson",
            "Thierry Baccino"
        ],
        "summary": "In this paper, we firstly present what is Interactive Evolutionary Computation (IEC) and rapidly how we have combined this artificial intelligence technique with an eye-tracker for visual optimization. Next, in order to correctly parameterize our application, we present results from applying data mining techniques on gaze information coming from experiments conducted on about 80 human individuals.",
        "published": "2008-03-21T15:38:25Z",
        "link": "http://arxiv.org/abs/0803.3186v1",
        "categories": [
            "cs.HC",
            "cs.NE"
        ]
    },
    {
        "title": "Facing the Facts",
        "authors": [
            "Patrick O'Beirne"
        ],
        "summary": "Human error research on overconfidence supports the benefits of early visibility of defects and disciplined development. If risk to the enterprise is to be reduced, individuals need to become aware of the reality of the quality of their work. Several cycles of inspection and defect removal are inevitable. Software Quality Management measurements of defect density and removal efficiency are applicable. Research of actual spreadsheet error rates shows data consistent with other software depending on the extent to which the work product was reviewed before inspection. The paper argues that the payback for an investment in early review time is justified by the saving in project delay and expensive errors in use.   'If debugging is the process of removing bugs, then programming must be the process of putting them in' - Anon.",
        "published": "2008-03-24T12:01:16Z",
        "link": "http://arxiv.org/abs/0803.3394v1",
        "categories": [
            "cs.HC",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Diversity of Online Community Activities",
        "authors": [
            "Tad Hogg",
            "Gabor Szabo"
        ],
        "summary": "Web sites where users create and rate content as well as form networks with other users display long-tailed distributions in many aspects of behavior. Using behavior on one such community site, Essembly, we propose and evaluate plausible mechanisms to explain these behaviors. Unlike purely descriptive models, these mechanisms rely on user behaviors based on information available locally to each user. For Essembly, we find the long-tails arise from large differences among user activity rates and qualities of the rated content, as well as the extensive variability in the time users devote to the site. We show that the models not only explain overall behavior but also allow estimating the quality of content from their early behaviors.",
        "published": "2008-03-25T00:23:05Z",
        "link": "http://arxiv.org/abs/0803.3482v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Human dynamics revealed through Web analytics",
        "authors": [
            "Bruno Goncalves",
            "Jose J. Ramasco"
        ],
        "summary": "When the World Wide Web was first conceived as a way to facilitate the sharing of scientific information at the CERN (European Center for Nuclear Research) few could have imagined the role it would come to play in the following decades. Since then, the increasing ubiquity of Internet access and the frequency with which people interact with it raise the possibility of using the Web to better observe, understand, and monitor several aspects of human social behavior. Web sites with large numbers of frequently returning users are ideal for this task. If these sites belong to companies or universities, their usage patterns can furnish information about the working habits of entire populations. In this work, we analyze the properly anonymized logs detailing the access history to Emory University's Web site. Emory is a medium size university located in Atlanta, Georgia. We find interesting structure in the activity patterns of the domain and study in a systematic way the main forces behind the dynamics of the traffic. In particular, we show that both linear preferential linking and priority based queuing are essential ingredients to understand the way users navigate the Web.",
        "published": "2008-03-27T21:19:54Z",
        "link": "http://arxiv.org/abs/0803.4018v2",
        "categories": [
            "cs.HC",
            "cond-mat.stat-mech",
            "cs.CY",
            "physics.soc-ph"
        ]
    },
    {
        "title": "RubberEdge: Reducing Clutching by Combining Position and Rate Control   with Elastic Feedback",
        "authors": [
            "Géry Casiez",
            "Daniel Vogel",
            "Qing Pan",
            "Christophe Chaillou"
        ],
        "summary": "Position control devices enable precise selection, but significant clutching degrades performance. Clutching can be reduced with high control-display gain or pointer acceleration, but there are human and device limits. Elastic rate control eliminates clutching completely, but can make precise selection difficult. We show that hybrid position-rate control can outperform position control by 20% when there is significant clutching, even when using pointer acceleration. Unlike previous work, our RubberEdge technique eliminates trajectory and velocity discontinuities. We derive predictive models for position control with clutching and hybrid control, and present a prototype RubberEdge position-rate control device including initial user feedback.",
        "published": "2008-04-03T13:40:02Z",
        "link": "http://arxiv.org/abs/0804.0556v1",
        "categories": [
            "cs.HC",
            "H.5.2"
        ]
    },
    {
        "title": "Issues in Strategic Decision Modelling",
        "authors": [
            "Paula Jennings"
        ],
        "summary": "[Spreadsheet] Models are invaluable tools for strategic planning. Models help key decision makers develop a shared conceptual understanding of complex decisions, identify sensitivity factors and test management scenarios. Different modelling approaches are specialist areas in themselves. Model development can be onerous, expensive, time consuming, and often bewildering. It is also an iterative process where the true magnitude of the effort, time and data required is often not fully understood until well into the process. This paper explores the traditional approaches to strategic planning modelling commonly used in organisations and considers the application of a real-options approach to match and benefit from the increasing uncertainty in today's rapidly changing world.",
        "published": "2008-04-06T23:20:15Z",
        "link": "http://arxiv.org/abs/0804.0937v1",
        "categories": [
            "cs.HC",
            "D.2.4; D.2.5; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Reducing Overconfidence in Spreadsheet Development",
        "authors": [
            "Raymond R. Panko"
        ],
        "summary": "Despite strong evidence of widespread errors, spreadsheet developers rarely subject their spreadsheets to post-development testing to reduce errors. This may be because spreadsheet developers are overconfident in the accuracy of their spreadsheets. This conjecture is plausible because overconfidence is present in a wide variety of human cognitive domains, even among experts. This paper describes two experiments in overconfidence in spreadsheet development. The first is a pilot study to determine the existence of overconfidence. The second tests a manipulation to reduce overconfidence and errors. The manipulation is modestly successful, indicating that overconfidence reduction is a promising avenue to pursue.",
        "published": "2008-04-07T00:20:24Z",
        "link": "http://arxiv.org/abs/0804.0941v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "The Wall and The Ball: A Study of Domain Referent Spreadsheet Errors",
        "authors": [
            "Richard J. Irons"
        ],
        "summary": "The Cell Error Rate in simple spreadsheets averages about 2% to 5%. This CER has been measured in domain free environments. This paper compares the CERs occurring in domain free and applied domain tasks. The applied domain task requires the application of simple linear algebra to a costing problem. The results show that domain referent knowledge influences participants' approaches to spreadsheet creation and spreadsheet usage. The conclusion is that spreadsheet error making is influenced by domain knowledge and domain perception. Qualitative findings also suggest that spreadsheet error making is a part of overall human behaviour, and ought to be analyzed against this wider canvas.",
        "published": "2008-04-07T00:46:41Z",
        "link": "http://arxiv.org/abs/0804.0943v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "Augmenting Actual Life Through MUVEs",
        "authors": [
            "Laura Anna Ripamonti",
            "Ines Di Loreto",
            "Dario Maggiorini"
        ],
        "summary": "The necessity of supporting more and more social interaction (and not only the mere information sharing) in online environments is the disruptive force upon which phenomena ascribed to the Web2.0 paradigm continuously bud. People interacting in online socio-technical environments mould technology on their needs, seamlessly integrating it into their everyday life. MUVEs (Multi User Virtual Environments) are no exception and, in several cases, represent the new frontier in this field. In this work we analyze if and how MUVEs can be considered a mean for augmenting communities (and more in general people) life. We trace a framework of analysis based on four main observations, and through these lenses we look at Second Life and at several projects we are currently developing in that synthetic world.",
        "published": "2008-04-16T14:43:31Z",
        "link": "http://arxiv.org/abs/0804.2614v1",
        "categories": [
            "cs.HC",
            "cs.CY"
        ]
    },
    {
        "title": "An Analysis of Key Factors for the Success of the Communal Management of   Knowledge",
        "authors": [
            "Isabelle Bourdon",
            "Chris Kimble"
        ],
        "summary": "This paper explores the links between Knowledge Management and new community-based models of the organization from both a theoretical and an empirical perspective. From a theoretical standpoint, we look at Communities of Practice (CoPs) and Knowledge Management (KM) and explore the links between the two as they relate to the use of information systems to manage knowledge. We begin by reviewing technologically supported approaches to KM and introduce the idea of \"Systemes d'Aide a la Gestion des Connaissances\" SAGC (Systems to aid the Management of Knowledge). Following this we examine the contribution that communal structures such as CoPs can make to intraorganizational KM and highlight some of 'success factors' for this approach to KM that are found in the literature. From an empirical standpoint, we present the results of a survey involving the Chief Knowledge Officers (CKOs) of twelve large French businesses; the objective of this study was to identify the factors that might influence the success of such approaches. The survey was analysed using thematic content analysis and the results are presented here with some short illustrative quotes from the CKOs. Finally, the paper concludes with some brief reflections on what can be learnt from looking at this problem from these two perspectives.",
        "published": "2008-04-17T16:00:47Z",
        "link": "http://arxiv.org/abs/0804.2844v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "K.6; H.2"
        ]
    },
    {
        "title": "The Concept of Appropriation as a Heuristic for Conceptualising the   Relationship between Technology, People and Organisations",
        "authors": [
            "Pamela Baillette",
            "Chris Kimble"
        ],
        "summary": "The stated aim of this conference is to debate the continuing evolution of IS in businesses and other organisations. This paper seeks to contribute to this debate by exploring the concept of appropriation from a number of different epistemological, cultural and linguistic viewpoints to allow us to explore 'the black box' of appropriation and to gain a fuller understanding of the term. At the conceptual level, it will examine some of the different ways in which people have attempted to explain the relationship between the objective and concrete features of technology and the subjective and shifting nature of the people and organisation within which that technology is deployed. At the cultural and linguistic level the paper will examine the notion as it is found in the Francophone literature, where the term has a long and rich history, and the Anglophone literature where appropriation is seen as a rather more specialist term. The paper will conclude with some observations on the ongoing nature of the debate, the value of reading beyond the literature with which one is familiar and the rewards that come from exploring different historical (and linguistic) viewpoints.",
        "published": "2008-04-17T16:13:49Z",
        "link": "http://arxiv.org/abs/0804.2847v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "K.4.3; K.6.1"
        ]
    },
    {
        "title": "Identifying 'Hidden' Communities of Practice within Electronic Networks:   Some Preliminary Premises",
        "authors": [
            "Richard Ribeiro",
            "Chris Kimble"
        ],
        "summary": "This paper examines the possibility of discovering 'hidden' (potential) Communities of Practice (CoPs) inside electronic networks, and then using this knowledge to nurture them into a fully developed Virtual Community of Practice (VCoP). Starting from the standpoint of the need to manage knowledge, it discusses several questions related to this subject: the characteristics of 'hidden' communities; the relation between CoPs, Virtual Communities (VCs), Distributed Communities of Practice (DCoPs) and Virtual Communities of Practice (VCoPs); the methods used to search for 'hidden' CoPs; and the possible ways of changing 'hidden' CoPs into fully developed VCoPs. The paper also presents some preliminary findings from a semi-structured interview conducted in The Higher Education Academy Psychology Network (UK). These findings are contrasted against the theory discussed and some additional proposals are suggested at the end.",
        "published": "2008-04-17T16:33:53Z",
        "link": "http://arxiv.org/abs/0804.2851v1",
        "categories": [
            "cs.HC",
            "cs.CY",
            "H.4; H.5.3; K.4"
        ]
    },
    {
        "title": "Size matters: performance declines if your pixels are too big or too   small",
        "authors": [
            "Vassilis Kostakos",
            "Eamonn O'Neill"
        ],
        "summary": "We present a conceptual model that describes the effect of pixel size on target acquisition. We demonstrate the use of our conceptual model by applying it to predict and explain the results of an experiment to evaluate users' performance in a target acquisition task involving three distinct display sizes: standard desktop, small and large displays. The results indicate that users are fastest on standard desktop displays, undershoots are the most common error on small displays and overshoots are the most common error on large displays. We propose heuristics to maintain usability when changing displays. Finally, we contribute to the growing body of evidence that amplitude does affect performance in a display-based pointing task.",
        "published": "2008-04-18T21:02:19Z",
        "link": "http://arxiv.org/abs/0804.3103v1",
        "categories": [
            "cs.GR",
            "cs.HC"
        ]
    },
    {
        "title": "Characterizing Video Responses in Social Networks",
        "authors": [
            "Fabricio Benevenuto",
            "Fernando Duarte",
            "Tiago Rodrigues",
            "Virgilio Almeida",
            "Jussara Almeida",
            "Keith Ross"
        ],
        "summary": "Video sharing sites, such as YouTube, use video responses to enhance the social interactions among their users. The video response feature allows users to interact and converse through video, by creating a video sequence that begins with an opening video and followed by video responses from other users. Our characterization is over 3.4 million videos and 400,000 video responses collected from YouTube during a 7-day period. We first analyze the characteristics of the video responses, such as popularity, duration, and geography. We then examine the social networks that emerge from the video response interactions.",
        "published": "2008-04-30T16:39:32Z",
        "link": "http://arxiv.org/abs/0804.4865v1",
        "categories": [
            "cs.MM",
            "cs.CY",
            "cs.HC",
            "J.4; H.3.5"
        ]
    },
    {
        "title": "SimDialog: A visual game dialog editor",
        "authors": [
            "C. Owen",
            "F. Biocca",
            "C. Bohil",
            "J. Conley"
        ],
        "summary": "SimDialog is a visual editor for dialog in computer games. This paper presents the design of SimDialog, illustrating how script writers and non-programmers can easily create dialog for video games with complex branching structures and dynamic response characteristics. The system creates dialog as a directed graph. This allows for play using the dialog with a state-based cause and effect system that controls selection of non-player character responses and can provide a basic scoring mechanism for games.",
        "published": "2008-04-30T18:44:52Z",
        "link": "http://arxiv.org/abs/0804.4885v1",
        "categories": [
            "cs.HC",
            "cs.AI"
        ]
    },
    {
        "title": "A Spreadsheet Auditing Tool Evaluated in an Industrial Context",
        "authors": [
            "Markus Clermont",
            "Christian Hanin",
            "Roland T. Mittermeir"
        ],
        "summary": "Amongst the large number of write-and-throw-away spreadsheets developed for one-time use there is a rather neglected proportion of spreadsheets that are huge, periodically used, and submitted to regular update-cycles like any conventionally evolving valuable legacy application software. However, due to the very nature of spreadsheets, their evolution is particularly tricky and therefore error-prone. In our strive to develop tools and methodologies to improve spreadsheet quality, we analysed consolidation spreadsheets of an internationally operating company for the errors they contain. The paper presents the results of the field audit, involving 78 spreadsheets with 60,446 non-empty cells. As a by-product, the study performed was also to validate our analysis tools in an industrial context. The evaluated auditing tool offers the auditor a new view on the formula structure of the spreadsheet by grouping similar formulas into equivalence classes. Our auditing approach defines three similarity criteria between formulae, namely copy, logical and structural equivalence. To improve the visualization of large spreadsheets, equivalences and data dependencies are displayed in separated windows that are interlinked with the spreadsheet. The auditing approach helps to find irregularities in the geometrical pattern of similar formulas.",
        "published": "2008-05-12T20:40:27Z",
        "link": "http://arxiv.org/abs/0805.1741v1",
        "categories": [
            "cs.HC",
            "D.1.7; D.2.1; D.2.11; D.3.2; D.3.3; H.4.1; K.6.4; K.8.1"
        ]
    },
    {
        "title": "Visual Checking of Spreadsheets",
        "authors": [
            "Ying Chen",
            "Hock Chuan Chan"
        ],
        "summary": "The difference between surface and deep structures of a spreadsheet is a major cause of difficulty in checking spreadsheets. After a brief survey of current methods of checking (or debugging) spreadsheets, new visual methods of showing the deep structures are presented. Illustrations are given on how these visual methods can be employed in various interactive local and global debugging strategies.",
        "published": "2008-05-15T00:27:15Z",
        "link": "http://arxiv.org/abs/0805.2189v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Managing Critical Spreadsheets in a Compliant Environment",
        "authors": [
            "Soheil Saadat"
        ],
        "summary": "The use of uncontrolled financial spreadsheets can expose organizations to unacceptable business and compliance risks, including errors in the financial reporting process, spreadsheet misuse and fraud, or even significant operational errors. These risks have been well documented and thoroughly researched. With the advent of regulatory mandates such as SOX 404 and FDICIA in the U.S., and MiFID, Basel II and Combined Code in the UK and Europe, leading tax and audit firms are now recommending that organizations automate their internal controls over critical spreadsheets and other end-user computing applications, including Microsoft Access databases. At a minimum, auditors mandate version control, change control and access control for operational spreadsheets, with more advanced controls for critical financial spreadsheets. This paper summarises the key issues regarding the establishment and maintenance of control of Business Critical spreadsheets.",
        "published": "2008-05-27T20:28:10Z",
        "link": "http://arxiv.org/abs/0805.4211v1",
        "categories": [
            "cs.SE",
            "cs.HC",
            "J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
        ]
    },
    {
        "title": "Managing conflicts between users in Wikipedia",
        "authors": [
            "Bernard Jacquemin",
            "Aurélien Lauf",
            "Céline Poudat",
            "Martine Hurault-Plantet",
            "Nicolas Auray"
        ],
        "summary": "Wikipedia is nowadays a widely used encyclopedia, and one of the most visible sites on the Internet. Its strong principle of collaborative work and free editing sometimes generates disputes due to disagreements between users. In this article we study how the wikipedian community resolves the conflicts and which roles do wikipedian choose in this process. We observed the users behavior both in the article talk pages, and in the Arbitration Committee pages specifically dedicated to serious disputes. We first set up a users typology according to their involvement in conflicts and their publishing and management activity in the encyclopedia. We then used those user types to describe users behavior in contributing to articles that are tagged by the wikipedian community as being in conflict with the official guidelines of Wikipedia, or conversely as being well featured.",
        "published": "2008-05-30T13:20:42Z",
        "link": "http://arxiv.org/abs/0805.4754v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "EuSpRIG TEAM work:Tools, Education, Audit, Management",
        "authors": [
            "David Chadwick"
        ],
        "summary": "Research on spreadsheet errors began over fifteen years ago. During that time, there has been ample evidence demonstrating that spreadsheet errors are common and nontrivial. Quite simply, spreadsheet error rates are comparable to error rates in other human cognitive activities and are caused by fundamental limitations in human cognition, not mere sloppiness. Nor does ordinary \"being careful\" eliminate errors or reduce them to acceptable levels.",
        "published": "2008-06-01T21:01:15Z",
        "link": "http://arxiv.org/abs/0806.0172v1",
        "categories": [
            "cs.HC",
            "cs.CY"
        ]
    },
    {
        "title": "Training Gamble leads to Corporate Grumble?",
        "authors": [
            "David R. Chadwick"
        ],
        "summary": "Fifteen years of research studies have concluded unanimously that spreadsheet errors are both common and non-trivial. Now we must seek ways to reduce spreadsheet errors. Several approaches have been suggested, some of which are promising and others, while appealing because they are easy to do, are not likely to be effective. To date, only one technique, cell-by-cell code inspection, has been demonstrated to be effective. We need to conduct further research to determine the degree to which other techniques can reduce spreadsheet errors.",
        "published": "2008-06-01T23:14:22Z",
        "link": "http://arxiv.org/abs/0806.0182v1",
        "categories": [
            "cs.HC",
            "cs.CY"
        ]
    },
    {
        "title": "Investigating the use of Software Agents to Reduce The Risk of   Undetected Errors in Strategic Spreadsheet Applications",
        "authors": [
            "Pat Cleary",
            "Dr David Ball",
            "Mukul Madahar",
            "Simon Thorne",
            "Christopher Gosling",
            "Karen Fernandez"
        ],
        "summary": "There is an overlooked iceberg of problems in end user computing. Spreadsheets are developed by people who are very skilled in their main job function, be it finance, procurement, or production planning, but often have had no formal training in spreadsheet use. IT auditors focus on mainstream information systems but regard spreadsheets as user problems, outside their concerns. Internal auditors review processes, but not the tools that support decision making in these processes.   This paper highlights the gaps between risk management and end user awareness in spreadsheet research. In addition the potential benefits of software agent technologies to the management of risk in spreadsheets are explored. This paper discusses the current research into end user computing and spreadsheet use awareness.",
        "published": "2008-06-01T23:51:12Z",
        "link": "http://arxiv.org/abs/0806.0189v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "GuiLiner: A Configurable and Extensible Graphical User Interface for   Scientific Analysis and Simulation Software",
        "authors": [
            "N. C. Manoukis",
            "E. C. Anderson"
        ],
        "summary": "The computer programs most users interact with daily are driven by a graphical user interface (GUI). However, many scientific applications are used with a command line interface (CLI) for the ease of development and increased flexibility this mode provides. Scientific application developers would benefit from being able to provide a GUI easily for their CLI programs, thus retaining the advantages of both modes of interaction. GuiLiner is a generic, extensible and flexible front-end designed to ``host'' a wide variety of data analysis or simulation programs. Scientific application developers who produce a correctly formatted XML file describing their program's options and some of its documentation can immediately use GuiLiner to produce a carefully implemented GUI for their analysis or simulation programs.",
        "published": "2008-06-02T15:57:55Z",
        "link": "http://arxiv.org/abs/0806.0314v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "D.2.2; H.1.2; I.3.6"
        ]
    },
    {
        "title": "Collaborative model of interaction and Unmanned Vehicle Systems'   interface",
        "authors": [
            "Sylvie Saget",
            "Francois Legras",
            "Gilles Coppin"
        ],
        "summary": "The interface for the next generation of Unmanned Vehicle Systems should be an interface with multi-modal displays and input controls. Then, the role of the interface will not be restricted to be a support of the interactions between the ground operator and vehicles. Interface must take part in the interaction management too. In this paper, we show that recent works in pragmatics and philosophy provide a suitable theoretical framework for the next generation of UV System's interface. We concentrate on two main aspects of the collaborative model of interaction based on acceptance: multi-strategy approach for communicative act generation and interpretation and communicative alignment.",
        "published": "2008-06-04T14:22:38Z",
        "link": "http://arxiv.org/abs/0806.0784v1",
        "categories": [
            "cs.AI",
            "cs.HC",
            "cs.MA"
        ]
    },
    {
        "title": "SPAM over Internet Telephony and how to deal with it",
        "authors": [
            "Andreas U. Schmidt",
            "Nicolai Kuntze",
            "Rachid El Khayari"
        ],
        "summary": "In our modern society telephony has developed to an omnipresent service. People are available at anytime and anywhere. Furthermore the Internet has emerged to an important communication medium. These facts and the raising availability of broadband internet access has led to the fusion of these two services. Voice over IP or short VoIP is the keyword, that describes this combination. The advantages of VoIP in comparison to classic telephony are location independence, simplification of transport networks, ability to establish multimedia communications and the low costs. Nevertheless one can easily see, that combining two technologies, always brings up new challenges and problems that have to be solved. It is undeniable that one of the most annoying facet of the Internet nowadays is email spam. According to different sources email spam is considered to be 80 to 90 percent of the email traffic produced. The threat of so called voice spam or Spam over Internet Telephony (SPIT) is even more fatal, for the annoyance and disturbance factor is much higher. As instance an email that hits the inbox at 4 p.m. is useless but will not disturb the user much. In contrast a ringing phone at 4 p.m. will lead to a much higher disturbance. From the providers point of view both email spam and voice spam produce unwanted traffic and loss of trust of customers into the service. In order to mitigate this threat different approaches from different parties have been developed. This paper focuses on state of the art anti voice spam solutions, analyses them and reveals their weak points. In the end a SPIT producing benchmark tool will be introduced, that attacks the presented anti voice spam solutions. With this tool it is possible for an administrator of a VoIP network to test how vulnerable his system is.",
        "published": "2008-06-10T09:34:30Z",
        "link": "http://arxiv.org/abs/0806.1610v1",
        "categories": [
            "cs.CR",
            "cs.HC"
        ]
    },
    {
        "title": "An Intelligent Multi-Agent Recommender System for Human Capacity   Building",
        "authors": [
            "Vukosi N. Marivate",
            "George Ssali",
            "Tshilidzi Marwala"
        ],
        "summary": "This paper presents a Multi-Agent approach to the problem of recommending training courses to engineering professionals. The recommendation system is built as a proof of concept and limited to the electrical and mechanical engineering disciplines. Through user modelling and data collection from a survey, collaborative filtering recommendation is implemented using intelligent agents. The agents work together in recommending meaningful training courses and updating the course information. The system uses a users profile and keywords from courses to rank courses. A ranking accuracy for courses of 90% is achieved while flexibility is achieved using an agent that retrieves information autonomously using data mining techniques from websites. This manner of recommendation is scalable and adaptable. Further improvements can be made using clustering and recording user feedback.",
        "published": "2008-06-13T09:56:36Z",
        "link": "http://arxiv.org/abs/0806.2216v1",
        "categories": [
            "cs.AI",
            "cs.HC"
        ]
    },
    {
        "title": "In Pursuit of Spreadsheet Excellence",
        "authors": [
            "Grenville J. Croll"
        ],
        "summary": "The first fully-documented study into the quantitative impact of errors in operational spreadsheets identified an interesting anomaly. One of the five participating organisations involved in the study contributed a set of five spreadsheets of such quality that they set the organisation apart in a statistical sense. This virtuoso performance gave rise to a simple sampling test - The Clean Sheet Test - which can be used to objectively evaluate if an organisation is in control of the spreadsheets it is using in important processes such as financial reporting.",
        "published": "2008-06-22T00:14:09Z",
        "link": "http://arxiv.org/abs/0806.3536v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Un cadre de conception pour réunir les modèles d'interaction et   l'ingénierie des interfaces",
        "authors": [
            "Jérôme Lard",
            "Frédéric Landragin",
            "Olivier Grisvard",
            "David Faure"
        ],
        "summary": "We present HIC (Human-system Interaction Container), a general framework for the integration of advanced interaction in the software development process. We show how this framework allows to reconcile the software development methods (such MDA, MDE) with the architectural models of software design such as MVC or PAC. We illustrate our approach thanks to two different types of implementation for this concept in two different business areas: one software design pattern, MVIC (Model View Interaction Control) and one architectural model, IM (Interaction Middleware).",
        "published": "2008-07-16T18:31:14Z",
        "link": "http://arxiv.org/abs/0807.2628v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Ordinateur porté support de réalité augmentée pour des   activités de maintenance et de dépannage",
        "authors": [
            "Olivier Champalle",
            "Bertrand David",
            "René Chalon",
            "Guillaume Masserey"
        ],
        "summary": "In this paper we present a case study of use of wearable computer within the framework of activities of maintenance and repairing. Besides the study of configuration of this wearable computer and its peripherals, we show the integration of context, in-situ storage, traceability and regulation in these activities. This case study is in the scope of a huge project called HMTD (Help Me To Do) which aim is to apply MOCOCO (Mobility, COoperation, COntextualisation) and IMERA (Mobile Interaction in the Augmented Real Environment) principles for better use, maintenance and repairing of equipments in the domestic, public and professional situations.",
        "published": "2008-07-17T17:22:55Z",
        "link": "http://arxiv.org/abs/0807.2836v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Establishing and Measuring Standard Spreadsheet Practices for End-Users",
        "authors": [
            "Garry Cleere"
        ],
        "summary": "This paper offers a brief review of cognitive verbs typically used in the literature to describe standard spreadsheet practices. The verbs identified are then categorised in terms of Bloom's Taxonomy of Hierarchical Levels, and then rated and arranged to distinguish some of their qualities and characteristics. Some measurement items are then evaluated to see how well computerised test question items validate or reinforce training or certification. The paper considers how establishing standard practices in spreadsheet training and certification can help reduce some of the risks associated with spreadsheets, and help promote productivity.",
        "published": "2008-07-18T15:45:47Z",
        "link": "http://arxiv.org/abs/0807.2993v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Reducing Spreadsheet Risk with FormulaDataSleuth",
        "authors": [
            "Bill Bekenn",
            "Ray Hooper"
        ],
        "summary": "A new MS Excel application has been developed which seeks to reduce the risks associated with the development, operation and auditing of Excel spreadsheets. FormulaDataSleuth provides a means of checking spreadsheet formulas and data as they are developed or used, enabling the users to identify actual or potential errors quickly and thereby halt their propagation. In this paper, we will describe, with examples, how the application works and how it can be applied to reduce the risks associated with Excel spreadsheets.",
        "published": "2008-07-18T16:00:34Z",
        "link": "http://arxiv.org/abs/0807.2997v1",
        "categories": [
            "cs.HC",
            "cs.SE"
        ]
    },
    {
        "title": "Audit and Change Analysis of Spreadsheets",
        "authors": [
            "John C. Nash",
            "Neil Smith",
            "Andy Adler"
        ],
        "summary": "Because spreadsheets have a large and growing importance in real-world work, their contents need to be controlled and validated. Generally spreadsheets have been difficult to verify, since data and executable information are stored together. Spreadsheet applications with multiple authors are especially difficult to verify, since controls over access are difficult to enforce. Facing similar problems, traditional software engineering has developed numerous tools and methodologies to control, verify and audit large applications with multiple developers. We present some tools we have developed to enable 1) the audit of selected, filtered, or all changes in a spreadsheet, that is, when a cell was changed, its original and new contents and who made the change, and 2) control of access to the spreadsheet file(s) so that auditing is trustworthy. Our tools apply to OpenOffice.org calc spreadsheets, which can generally be exchanged with Microsoft Excel.",
        "published": "2008-07-20T17:28:25Z",
        "link": "http://arxiv.org/abs/0807.3168v1",
        "categories": [
            "cs.HC",
            "cs.SE"
        ]
    },
    {
        "title": "Accuracy in Spreadsheet Modelling Systems",
        "authors": [
            "Thomas A. Grossman"
        ],
        "summary": "Accuracy in spreadsheet modelling systems can be reduced due to difficulties with the inputs, the model itself, or the spreadsheet implementation of the model. When the \"true\" outputs from the system are unknowable, accuracy is evaluated subjectively. Less than perfect accuracy can be acceptable depending on the purpose of the model, problems with inputs, or resource constraints. Users build modelling systems iteratively, and choose to allocate limited resources to the inputs, the model, the spreadsheet implementation, and to employing the system for business analysis. When making these choices, users can suffer from expectation bias and diagnosis bias. Existing research results tend to focus on errors in the spreadsheet implementation. Because industry has tolerance for system inaccuracy, errors in spreadsheet implementations may not be a serious concern. Spreadsheet productivity may be of more interest.",
        "published": "2008-07-20T20:29:38Z",
        "link": "http://arxiv.org/abs/0807.3183v1",
        "categories": [
            "cs.HC",
            "cs.SE"
        ]
    },
    {
        "title": "Research Strategy and Scoping Survey on Spreadsheet Practices",
        "authors": [
            "Thomas A. Grossman",
            "Ozgur Ozluk"
        ],
        "summary": "We propose a research strategy for creating and deploying prescriptive recommendations for spreadsheet practice. Empirical data on usage can be used to create a taxonomy of spreadsheet classes. Within each class, existing practices and ideal practices can he combined into proposed best practices for deployment. As a first step we propose a scoping survey to gather non-anecdotal data on spreadsheet usage. The scoping survey will interview people who develop spreadsheets. We will investigate the determinants of spreadsheet importance, identify current industry practices, and document existing standards for creation and use of spreadsheets. The survey will provide insight into user attributes, spreadsheet importance, and current practices. Results will be valuable in themselves, and will guide future empirical research.",
        "published": "2008-07-20T20:43:05Z",
        "link": "http://arxiv.org/abs/0807.3184v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "New Guidelines For Spreadsheets",
        "authors": [
            "John F. Raffensperger"
        ],
        "summary": "Current prescriptions for spreadsheet style specify modular separation of data, calcu1ation and output, based on the notion that writing a spreadsheet is like writing a computer program. Instead of a computer programming style, this article examines rules of style for text, graphics, and mathematics. Much 'common wisdom' in spreadsheets contradicts rules for these well-developed arts. A case is made here for a new style for spreadsheets that emphasises readability. The new style is described in detail with an example, and contrasted with the programming style.",
        "published": "2008-07-20T20:58:37Z",
        "link": "http://arxiv.org/abs/0807.3186v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "AceWiki: A Natural and Expressive Semantic Wiki",
        "authors": [
            "Tobias Kuhn"
        ],
        "summary": "We present AceWiki, a prototype of a new kind of semantic wiki using the controlled natural language Attempto Controlled English (ACE) for representing its content. ACE is a subset of English with a restricted grammar and a formal semantics. The use of ACE has two important advantages over existing semantic wikis. First, we can improve the usability and achieve a shallow learning curve. Second, ACE is more expressive than the formal languages of existing semantic wikis. Our evaluation shows that people who are not familiar with the formal foundations of the Semantic Web are able to deal with AceWiki after a very short learning phase and without the help of an expert.",
        "published": "2008-07-29T09:54:44Z",
        "link": "http://arxiv.org/abs/0807.4618v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "H.5.2; I.2.4"
        ]
    },
    {
        "title": "AceWiki: Collaborative Ontology Management in Controlled Natural   Language",
        "authors": [
            "Tobias Kuhn"
        ],
        "summary": "AceWiki is a prototype that shows how a semantic wiki using controlled natural language - Attempto Controlled English (ACE) in our case - can make ontology management easy for everybody. Sentences in ACE can automatically be translated into first-order logic, OWL, or SWRL. AceWiki integrates the OWL reasoner Pellet and ensures that the ontology is always consistent. Previous results have shown that people with no background in logic are able to add formal knowledge to AceWiki without being instructed or trained in advance.",
        "published": "2008-07-29T10:15:38Z",
        "link": "http://arxiv.org/abs/0807.4623v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "H.5.2; I.2.4"
        ]
    },
    {
        "title": "Correctness is not enough",
        "authors": [
            "Louise Pryor"
        ],
        "summary": "The usual aim of spreadsheet audit is to verify correctness. There are two problems with this: first, it is often difficult to tell whether the spreadsheets in question are correct, and second, even if they are, they may still give the wrong results. These problems are explained in this paper, which presents the key criteria for judging a spreadsheet and discusses how those criteria can be achieved",
        "published": "2008-08-14T19:03:10Z",
        "link": "http://arxiv.org/abs/0808.2045v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "The Semiotic Machine",
        "authors": [
            "Eric Engle"
        ],
        "summary": "A semiotic model of the user interface in human-computer interaction. Algorithmic sign, semotics, algorithmic art.",
        "published": "2008-09-02T17:25:25Z",
        "link": "http://arxiv.org/abs/0809.0461v1",
        "categories": [
            "cs.HC",
            "H.5.2"
        ]
    },
    {
        "title": "Proposition of the Interactive Pareto Iterated Local Search Procedure -   Elements and Initial Experiments",
        "authors": [
            "Martin Josef Geiger"
        ],
        "summary": "The article presents an approach to interactively solve multi-objective optimization problems. While the identification of efficient solutions is supported by computational intelligence techniques on the basis of local search, the search is directed by partial preference information obtained from the decision maker.   An application of the approach to biobjective portfolio optimization, modeled as the well-known knapsack problem, is reported, and experimental results are reported for benchmark instances taken from the literature. In brief, we obtain encouraging results that show the applicability of the approach to the described problem.",
        "published": "2008-09-04T06:52:16Z",
        "link": "http://arxiv.org/abs/0809.0753v1",
        "categories": [
            "cs.AI",
            "cs.HC"
        ]
    },
    {
        "title": "On the role of metaphor in information visualization",
        "authors": [
            "John S. Risch"
        ],
        "summary": "The concept of metaphor, in particular graphical (or visual) metaphor, is central to the field of information visualization. Information graphics and interactive information visualization systems employ a variety of metaphorical devices to make abstract, complex, voluminous, or otherwise difficult-to-comprehend information understandable in graphical terms. This paper explores the use of metaphor in information visualization, advancing the theory previously argued by Johnson, Lakoff, Tversky et al. that many information graphics are metaphorically understood in terms of cognitively entrenched spatial patterns known as image schemas. These patterns serve to structure and constrain abstract reasoning processes via metaphorical projection operations that are grounded in everyday perceptual experiences with phenomena such as containment, movement, and force dynamics. Building on previous research, I argue that information graphics promote comprehension of their target information through the use of graphical patterns that invoke these preexisting schematic structures. I further theorize that the degree of structural alignment of a particular graphic with one or more corresponding image schemas accounts for its perceived degree of intuitiveness. Accordingly, image schema theory can provide a powerful explanatory and predictive framework for visualization research. I review relevant theories of analogy and metaphor, and discuss the image schematic properties of several common types of information graphic. I conclude with the proposal that the inventory of image schemas culled from linguistic studies can serve as the basis for an inventory of design elements suitable for developing intuitive and effective new information visualization techniques.",
        "published": "2008-09-04T19:55:40Z",
        "link": "http://arxiv.org/abs/0809.0884v1",
        "categories": [
            "cs.HC",
            "cs.GR"
        ]
    },
    {
        "title": "MOOPPS: An Optimization System for Multi Objective Scheduling",
        "authors": [
            "Martin Josef Geiger"
        ],
        "summary": "In the current paper, we present an optimization system solving multi objective production scheduling problems (MOOPPS). The identification of Pareto optimal alternatives or at least a close approximation of them is possible by a set of implemented metaheuristics. Necessary control parameters can easily be adjusted by the decision maker as the whole software is fully menu driven. This allows the comparison of different metaheuristic algorithms for the considered problem instances. Results are visualized by a graphical user interface showing the distribution of solutions in outcome space as well as their corresponding Gantt chart representation.   The identification of a most preferred solution from the set of efficient solutions is supported by a module based on the aspiration interactive method (AIM). The decision maker successively defines aspiration levels until a single solution is chosen.   After successfully competing in the finals in Ronneby, Sweden, the MOOPPS software has been awarded the European Academic Software Award 2002 (http://www.bth.se/llab/easa_2002.nsf)",
        "published": "2008-09-05T06:22:36Z",
        "link": "http://arxiv.org/abs/0809.0961v1",
        "categories": [
            "cs.AI",
            "cs.HC"
        ]
    },
    {
        "title": "Moving and resizing of the screen objects",
        "authors": [
            "Sergey Andreyev"
        ],
        "summary": "The shape and size of the objects, which we see on the screen, when the application is running, are defined at the design time. By using some sort of adaptive interface, developers give users a chance to resize these objects or on rare occasion even change, but all these changes are predetermined by a developer; user can't go out of the designer's scenario. Making each and all elements moveable / resizable and giving users the full control of these processes, changes the whole idea of applications; programs become user-driven and significantly increase the effectiveness of users' work. This article is about the instrument to turn any screen object into moveable / resizable.",
        "published": "2008-09-05T12:59:06Z",
        "link": "http://arxiv.org/abs/0809.1019v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Metrics-Based Spreadsheet Visualization: Support for Focused Maintenance",
        "authors": [
            "Karin Hodnigg",
            "Roland T. Mittermeir"
        ],
        "summary": "Legacy spreadsheets are both, an asset, and an enduring problem concerning spreadsheets in business. To make spreadsheets stay alive and remain correct, comprehension of a given spreadsheet is highly important. Visualization techniques should ease the complex and mindblowing challenges of finding structures in a huge set of spreadsheet cells for building an adequate mental model of spreadsheet programs. Since spreadsheet programs are as diverse as the purpose they are serving and as inhomogeneous as their programmers, to find an appropriate representation or visualization technique for every spreadsheet program seems futile. We thus propose different visualization and representation methods that may ease spreadsheet comprehension but should not be applied with all kind of spreadsheet programs. Therefore, this paper proposes to use (complexity) measures as indicators for proper visualization.",
        "published": "2008-09-17T20:58:29Z",
        "link": "http://arxiv.org/abs/0809.3009v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Automating Spreadsheet Discovery & Risk Assessment",
        "authors": [
            "Eric Perry"
        ],
        "summary": "There have been many articles and mishaps published about the risks of uncontrolled spreadsheets in today's business environment, including non-compliance, operational risk, errors, and fraud all leading to significant loss events. Spreadsheets fall into the realm of end user developed applications and are often absent the proper safeguards and controls an IT organization would enforce for enterprise applications. There is also an overall lack of software programming discipline enforced in how spreadsheets are developed. However, before an organization can apply proper controls and discipline to critical spreadsheets, an accurate and living inventory of spreadsheets across the enterprise must be created, and all critical spreadsheets must be identified. As such, this paper proposes an automated approach to the initial stages of the spreadsheet management lifecycle - discovery, inventory and risk assessment. Without the use of technology, these phases are often treated as a one-off project. By leveraging technology, they become a sustainable business process.",
        "published": "2008-09-17T21:16:02Z",
        "link": "http://arxiv.org/abs/0809.3016v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "An Exploratory Study of Calendar Use",
        "authors": [
            "Manas Tungare",
            "Manuel Perez-Quinones",
            "Alyssa Sams"
        ],
        "summary": "In this paper, we report on findings from an ethnographic study of how people use their calendars for personal information management (PIM). Our participants were faculty, staff and students who were not required to use or contribute to any specific calendaring solution, but chose to do so anyway. The study was conducted in three parts: first, an initial survey provided broad insights into how calendars were used; second, this was followed up with personal interviews of a few participants which were transcribed and content-analyzed; and third, examples of calendar artifacts were collected to inform our analysis. Findings from our study include the use of multiple reminder alarms, the reliance on paper calendars even among regular users of electronic calendars, and wide use of calendars for reporting and life-archival purposes. We conclude the paper with a discussion of what these imply for designers of interactive calendar systems and future work in PIM research.",
        "published": "2008-09-19T19:56:15Z",
        "link": "http://arxiv.org/abs/0809.3447v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.5.2"
        ]
    },
    {
        "title": "Evaluation of an Intelligent Assistive Technology for Voice Navigation   of Spreadsheets",
        "authors": [
            "Derek Flood",
            "Kevin Mc Daid",
            "Fergal Mc Caffery",
            "Brian Bishop"
        ],
        "summary": "An integral part of spreadsheet auditing is navigation. For sufferers of Repetitive Strain Injury who need to use voice recognition technology this navigation can be highly problematic. To counter this the authors have developed an intelligent voice navigation system, iVoice, which replicates common spreadsheet auditing behaviours through simple voice commands. This paper outlines the iVoice system and summarizes the results of a study to evaluate iVoice when compared to a leading voice recognition technology.",
        "published": "2008-09-21T11:06:23Z",
        "link": "http://arxiv.org/abs/0809.3571v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Spreadsheet modelling for solving combinatorial problems: The vendor   selection problem",
        "authors": [
            "Pandelis G. Ipsilandis"
        ],
        "summary": "Spreadsheets have grown up and became very powerful and easy to use tools in applying analytical techniques for solving business problems. Operations managers, production managers, planners and schedulers can work with them in developing solid and practical Do-It-Yourself Decision Support Systems. Small and Medium size organizations, can apply OR methodologies without the presence of specialized software and trained personnel, which in many cases cannot afford anyway. This paper examines an efficient approach in solving combinatorial programming problems with the use of spreadsheets. A practical application, which demonstrates the approach, concerns the development of a spreadsheet-based DSS for the Multi Item Procurement Problem with Fixed Vendor Cost. The DSS has been build using exclusively standard spreadsheet feature and can solve real problems of substantial size. The benefits and limitations of the approach are also discussed.",
        "published": "2008-09-21T11:21:40Z",
        "link": "http://arxiv.org/abs/0809.3574v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Spreadsheet Components For All",
        "authors": [
            "Jocelyn Paine"
        ],
        "summary": "We have prototyped a \"spreadsheet component repository\" Web site, from which users can copy \"components\" into their own Excel or Google spreadsheets. Components are collections of cells containing formulae: in real life, they would do useful calculations that many practitioners find hard to program, and would be rigorously tested and documented. Crucially, the user can tell the repository which cells in their spreadsheet to use for a componen's inputs and outputs. The repository will then reshape the component to fit. A single component can therefore be used in many different sizes and shapes of spreadsheet. We hope to set up a spreadsheet equivalent of the high-quality numerical subroutine libraries that revolutionised scientific computing, but where instead of subroutines, the library contains such components.",
        "published": "2008-09-21T13:31:35Z",
        "link": "http://arxiv.org/abs/0809.3584v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "A Primer on Spreadsheet Analytics",
        "authors": [
            "Thomas A. Grossman"
        ],
        "summary": "This paper provides guidance to an analyst who wants to extract insight from a spreadsheet model. It discusses the terminology of spreadsheet analytics, how to prepare a spreadsheet model for analysis, and a hierarchy of analytical techniques. These techniques include sensitivity analysis, tornado charts,and backsolving (or goal-seeking). This paper presents native-Excel approaches for automating these techniques, and discusses add-ins that are even more efficient. Spreadsheet optimization and spreadsheet Monte Carlo simulation are briefly discussed. The paper concludes by calling for empirical research, and describing desired features spreadsheet sensitivity analysis and spreadsheet optimization add-ins.",
        "published": "2008-09-21T13:49:23Z",
        "link": "http://arxiv.org/abs/0809.3586v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Spreadsheet End-User Behaviour Analysis",
        "authors": [
            "Brian Bishop",
            "Kevin McDaid"
        ],
        "summary": "To aid the development of spreadsheet debugging tools, a knowledge of end-users natural behaviour within the Excel environment would be advantageous. This paper details the design and application of a novel data acquisition tool, which can be used for the unobtrusive recording of end-users mouse, keyboard and Excel specific actions during the debugging of Excel spreadsheets. A debugging experiment was conducted using this data acquisition tool, and based on analysis of end-users performance and behaviour data, the authors developed a \"spreadsheet cell coverage feedback\" debugging tool. Results from the debugging experiment are presented in terms of enduser debugging performance and behaviour, and the outcomes of an evaluation experiment with the debugging tool are detailed.",
        "published": "2008-09-21T13:59:03Z",
        "link": "http://arxiv.org/abs/0809.3587v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Controlling End User Computing Applications - a case study",
        "authors": [
            "Jamie Chambers",
            "John Hamill"
        ],
        "summary": "We report the results of a project to control the use of end user computing tools for business critical applications in a banking environment. Several workstreams were employed in order to bring about a cultural change within the bank towards the use of spreadsheets and other end-user tools, covering policy development, awareness and skills training, inventory monitoring, user licensing, key risk metrics and mitigation approaches. The outcomes of these activities are discussed, and conclusions are drawn as to the need for appropriate organisational models to guide the use of these tools.",
        "published": "2008-09-21T17:45:08Z",
        "link": "http://arxiv.org/abs/0809.3595v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Spreadsheets: Aiming the Accountant's Hammer to Hit the Nail on the Head",
        "authors": [
            "Mbwana Alliy",
            "Patty Brown"
        ],
        "summary": "Accounting and Finance (A&F) Professionals are arguably the most loyal and concentrated population of spreadsheet users. The work that they perform in spreadsheets has the most significant impact on financial data and business processes within global organizations today. Spreadsheets offer the flexibility and ease of use of a desktop application, combined with the power to perform complex data analysis. They are also the lowest cost business IT tool when stacked up against other functional tools. As a result, spreadsheets are used to support critical business processes in most organizations. In fact, research indicates that over half of financial management reporting is performed with spreadsheets by an accounting and finance professional. A disparity exists in the business world between the importance of spreadsheets on financial data (created by A&F Professionals) and the resources devoted to: The development and oversight of global spreadsheet standards; A recognized and accredited certification in spreadsheet proficiency; Corporate sponsored and required training; Awareness of emerging technologies as it relates to spreadsheet use. This management paper focuses on the current topics relevant to the largest user group (A&F Professionals) of the most widely used financial software application, spreadsheets, also known as the accountant's hammer.",
        "published": "2008-09-21T18:05:42Z",
        "link": "http://arxiv.org/abs/0809.3597v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Information and Data Quality in Spreadsheets",
        "authors": [
            "Patrick O'Beirne"
        ],
        "summary": "The quality of the data in spreadsheets is less discussed than the structural integrity of the formulas. Yet it is an area of great interest to the owners and users of the spreadsheet. This paper provides an overview of Information Quality (IQ) and Data Quality (DQ) with specific reference to how data is sourced, structured, and presented in spreadsheets.",
        "published": "2008-09-21T19:37:28Z",
        "link": "http://arxiv.org/abs/0809.3609v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Overview and main results of the DidaTab project",
        "authors": [
            "Francois-Marie Blondel",
            "Eric Bruillard",
            "Francoise Tort"
        ],
        "summary": "The DidaTab project (Didactics of Spreadsheet, teaching and learning spreadsheets) is a three year project (2005-2007) funded by the French Ministry of Research and dedicated to the study of personal and classroom uses of spreadsheets in the French context, focussing on the processes of appropriation and uses by secondary school students. In this paper, we present an overview of the project, briefly report the studies performed in the framework of the DidaTab project, and give the main results we obtained. We then explore the new research tracks we intend to develop, more in connection with EuSpRIG. Our main result is that the use of spreadsheet during secondary education (grade 6 to 12) is rather sparse for school work (and even more seldom at home) and that student competencies are weak. Curricula have to be reviewed to include more training of dynamics tabular tools (including databases queries) in order to ensure sufficient mastery of computer tools that have became necessary in many educational activities.",
        "published": "2008-09-21T20:16:14Z",
        "link": "http://arxiv.org/abs/0809.3612v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Revisiting the Panko-Halverson Taxonomy of Spreadsheet Errors",
        "authors": [
            "Raymond R. Panko"
        ],
        "summary": "The purpose of this paper is to revisit the Panko-Halverson taxonomy of spreadsheet errors and suggest revisions. There are several reasons for doing so: First, the taxonomy has been widely used. Therefore, it should have scrutiny; Second, the taxonomy has not been widely available in its original form and most users refer to secondary sources. Consequently, they often equate the taxonomy with the simplified extracts used in particular experiments or field studies; Third, perhaps as a consequence, most users use only a fraction of the taxonomy. In particular, they tend not to use the taxonomy's life-cycle dimension; Fourth, the taxonomy has been tested against spreadsheets in experiments and spreadsheets in operational use. It is time to review how it has fared in these tests; Fifth, the taxonomy was based on the types of spreadsheet errors that were known to the authors in the mid-1990s. Subsequent experience has shown that the taxonomy needs to be extended for situations beyond those original experiences; Sixth, the omission category in the taxonomy has proven to be too narrow. Although this paper will focus on the Panko-Halverson taxonomy, this does not mean that that it is the only possible error taxonomy or even the best error taxonomy.",
        "published": "2008-09-21T20:29:53Z",
        "link": "http://arxiv.org/abs/0809.3613v1",
        "categories": [
            "cs.SE",
            "cs.HC"
        ]
    },
    {
        "title": "Combining Semantic Wikis and Controlled Natural Language",
        "authors": [
            "Tobias Kuhn"
        ],
        "summary": "We demonstrate AceWiki that is a semantic wiki using the controlled natural language Attempto Controlled English (ACE). The goal is to enable easy creation and modification of ontologies through the web. Texts in ACE can automatically be translated into first-order logic and other languages, for example OWL. Previous evaluation showed that ordinary people are able to use AceWiki without being instructed.",
        "published": "2008-10-17T07:19:39Z",
        "link": "http://arxiv.org/abs/0810.3076v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "H.5.2; I.2.4"
        ]
    },
    {
        "title": "Bicycle cycles and mobility patterns - Exploring and characterizing data   from a community bicycle program",
        "authors": [
            "Andreas Kaltenbrunner",
            "Rodrigo Meza",
            "Jens Grivolla",
            "Joan Codina",
            "Rafael Banchs"
        ],
        "summary": "This paper provides an analysis of human mobility data in an urban area using the amount of available bikes in the stations of the community bicycle program Bicing in Barcelona. The data was obtained by periodic mining of a KML-file accessible through the Bicing website. Although in principle very noisy, after some preprocessing and filtering steps the data allows to detect temporal patterns in mobility as well as identify residential, university, business and leisure areas of the city. The results lead to a proposal for an improvement of the bicing website, including a prediction of the number of available bikes in a certain station within the next minutes/hours. Furthermore a model for identifying the most probable routes between stations is briefly sketched.",
        "published": "2008-10-23T00:15:10Z",
        "link": "http://arxiv.org/abs/0810.4187v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "G.3; H.3.3"
        ]
    },
    {
        "title": "An Eye Tracking Study into the Effects of Graph Layout",
        "authors": [
            "Weidong Huang"
        ],
        "summary": "Graphs are typically visualized as node-link diagrams. Although there is a fair amount of research focusing on crossing minimization to improve readability, little attention has been paid on how to handle crossings when they are an essential part of the final visualizations. This requires us to understand how people read graphs and how crossings affect reading performance.   As an initial step to this end, a preliminary eye tracking experiment was conducted. The specific purpose of this experiment was to test the effects of crossing angles and geometric-path tendency on eye movements and performance. Sixteen subjects performed both path search and node locating tasks with six drawings. The results showed that small angles can slow down and trigger extra eye movements, causing delays for path search tasks, whereas crossings have little impact on node locating tasks. Geometric-path tendency indicates that a path between two nodes can become harder to follow when many branches of the path go toward the target node. The insights obtained are discussed with a view to further confirmation in future work.",
        "published": "2008-10-24T11:50:03Z",
        "link": "http://arxiv.org/abs/0810.4431v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "The adaptability of physiological systems optimizes performance: new   directions in augmentation",
        "authors": [
            "Bradly Alicea"
        ],
        "summary": "This paper contributes to the human-machine interface community in two ways: as a critique of the closed-loop AC (augmented cognition) approach, and as a way to introduce concepts from complex systems and systems physiology into the field. Of particular relevance is a comparison of the inverted-U (or Gaussian) model of optimal performance and multidimensional fitness landscape model. Hypothetical examples will be given from human physiology and learning and memory. In particular, a four-step model will be introduced that is proposed as a better means to characterize multivariate systems during behavioral processes with complex dynamics such as learning. Finally, the alternate approach presented herein is considered as a preferable design alternate in human-machine systems. It is within this context that future directions are discussed.",
        "published": "2008-10-27T17:42:05Z",
        "link": "http://arxiv.org/abs/0810.4884v2",
        "categories": [
            "cs.HC",
            "cs.NE"
        ]
    },
    {
        "title": "Edhibou: a Customizable Interface for Decision Support in a Semantic   Portal",
        "authors": [
            "Fadi Badra",
            "Mathieu D'Aquin",
            "Jean Lieber",
            "Thomas Meilender"
        ],
        "summary": "The Semantic Web is becoming more and more a reality, as the required technologies have reached an appropriate level of maturity. However, at this stage, it is important to provide tools facilitating the use and deployment of these technologies by end-users. In this paper, we describe EdHibou, an automatically generated, ontology-based graphical user interface that integrates in a semantic portal. The particularity of EdHibou is that it makes use of OWL reasoning capabilities to provide intelligent features, such as decision support, upon the underlying ontology. We present an application of EdHibou to medical decision support based on a formalization of clinical guidelines in OWL and show how it can be customized thanks to an ontology of graphical components.",
        "published": "2008-11-03T14:49:44Z",
        "link": "http://arxiv.org/abs/0811.0310v1",
        "categories": [
            "cs.AI",
            "cs.HC"
        ]
    },
    {
        "title": "Cooperative interface of a swarm of UAVs",
        "authors": [
            "Sylvie Saget",
            "Francois Legras",
            "Gilles Coppin"
        ],
        "summary": "After presenting the broad context of authority sharing, we outline how introducing more natural interaction in the design of the ground operator interface of UV systems should help in allowing a single operator to manage the complexity of his/her task. Introducing new modalities is one one of the means in the realization of our vision of next- generation GOI. A more fundamental aspect resides in the interaction manager which should help balance the workload of the operator between mission and interaction, notably by applying a multi-strategy approach to generation and interpretation. We intend to apply these principles to the context of the Smaart prototype, and in this perspective, we illustrate how to characterize the workload associated with a particular operational situation.",
        "published": "2008-11-03T16:54:10Z",
        "link": "http://arxiv.org/abs/0811.0335v1",
        "categories": [
            "cs.AI",
            "cs.HC",
            "cs.MA"
        ]
    },
    {
        "title": "Magic Fairy Tales as Source for Interface Metaphors",
        "authors": [
            "Vladimir L. Averbukh"
        ],
        "summary": "The work is devoted to a problem of search of metaphors for interactive systems and systems based on Virtual Reality (VR) environments. The analysis of magic fairy tales as a source of metaphors for interface and virtual reality is offered. Some results of design process based on magic metaphors are considered.",
        "published": "2008-11-12T20:31:34Z",
        "link": "http://arxiv.org/abs/0811.1974v1",
        "categories": [
            "cs.HC",
            "H.1.m"
        ]
    },
    {
        "title": "High resolution dynamical mapping of social interactions with active   RFID",
        "authors": [
            "Alain Barrat",
            "Ciro Cattuto",
            "Vittoria Colizza",
            "Jean-Francois Pinton",
            "Wouter Van den Broeck",
            "Alessandro Vespignani"
        ],
        "summary": "In this paper we present an experimental framework to gather data on face-to-face social interactions between individuals, with a high spatial and temporal resolution. We use active Radio Frequency Identification (RFID) devices that assess contacts with one another by exchanging low-power radio packets. When individuals wear the beacons as a badge, a persistent radio contact between the RFID devices can be used as a proxy for a social interaction between individuals. We present the results of a pilot study recently performed during a conference, and a subsequent preliminary data analysis, that provides an assessment of our method and highlights its versatility and applicability in many areas concerned with human dynamics.",
        "published": "2008-11-25T20:54:34Z",
        "link": "http://arxiv.org/abs/0811.4170v2",
        "categories": [
            "cs.CY",
            "cs.HC",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Emergent Community Structure in Social Tagging Systems",
        "authors": [
            "Ciro Cattuto",
            "Andrea Baldassarri",
            "Vito D. P. Servedio",
            "Vittorio Loreto"
        ],
        "summary": "A distributed classification paradigm known as collaborative tagging has been widely adopted in new Web applications designed to manage and share online resources. Users of these applications organize resources (Web pages, digital photographs, academic papers) by associating with them freely chosen text labels, or tags. Here we leverage the social aspects of collaborative tagging and introduce a notion of resource distance based on the collective tagging activity of users. We collect data from a popular system and perform experiments showing that our definition of distance can be used to build a weighted network of resources with a detectable community structure. We show that this community structure clearly exposes the semantic relations among resources. The communities of resources that we observe are a genuinely emergent feature, resulting from the uncoordinated activity of a large number of users, and their detection paves the way for mapping emergent semantics in social tagging systems.",
        "published": "2008-12-03T11:40:46Z",
        "link": "http://arxiv.org/abs/0812.0698v1",
        "categories": [
            "cs.IR",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "Stroke Fragmentation based on Geometry Features and HMM",
        "authors": [
            "Guihuan Feng",
            "Christian Viard-Gaudin"
        ],
        "summary": "Stroke fragmentation is one of the key steps in pen-based interaction. In this letter, we present a unified HMM-based stroke fragmentation technique that can do segment point location and primitive type determination simultaneously. The geometry features included are used to evaluate local features, and the HMM model is utilized to measure the global drawing context. Experiments prove that the model can efficiently represent smooth curves as well as strokes made up of arbitrary lines and circular arcs.",
        "published": "2008-12-04T07:35:48Z",
        "link": "http://arxiv.org/abs/0812.0874v1",
        "categories": [
            "cs.HC",
            "cs.CV"
        ]
    }
]